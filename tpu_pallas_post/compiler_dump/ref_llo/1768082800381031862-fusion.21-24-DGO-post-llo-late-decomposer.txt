
%s0 = inlined_call_operand.<no memory space> [shape: u32[], index: 0, kind: input, shape index: {}] /* operand 0 */
%s1 = inlined_call_operand.<no memory space> [shape: u32[], index: 1, kind: input, shape index: {}] /* operand 1 */
%s2 = inlined_call_operand.<no memory space> [shape: u32[], index: 2, kind: input, shape index: {}] /* operand 2 */
%s3 = inlined_call_operand.vmem [shape: u32[2048], index: 3, kind: input, shape index: {}] /* operand 3 */
%s4 = inlined_call_operand.vmem [shape: u32[8], index: 4, kind: input, shape index: {}]
%s5 = inlined_call_operand.vmem [shape: u32[2048], index: 5, kind: input, shape index: {}] /* operand 5 */
%s6 = inlined_call_operand.vmem [shape: u32[8], index: 6, kind: input, shape index: {}]
%s7 = inlined_call_operand.hbm [shape: bf16[8,2048,128], index: 7, kind: output, shape index: {}] /* operand 7 */
%v8 = vstv %s0
%v9 = vstv %s1
%v10 = vstv %s2

%11 = vsyncpa [#allocation2], 0

%13 = vsyncpa [#allocation2 + $0x1], 0

%s120396 = smov 0 /* copy for cssa */
%s120400 = smov 0 /* copy for cssa */
%s120404 = smov 0 /* copy for cssa */
%s120421 = smov %s120404 /* copy for cssa */
%s120425 = smov %s120400 /* copy for cssa */
%s120429 = smov %s120396 /* copy for cssa */

%s120424 = smov %s120423 /* phi copy :: iteration index, stage = 1 iter bound = 1 */
%s120428 = smov %s120427 /* phi copy :: iteration index, stage = 0 iter bound = 1 */
%s120432 = smov %s120431 /* phi copy :: iteration index, stage = 0 */
%s120399 = smov %s120432 /* phi copy :: iteration index, stage = 0 */
%s120403 = smov %s120428 /* phi copy :: iteration index, stage = 0 iter bound = 1 */
%s120407 = smov %s120424 /* phi copy :: iteration index, stage = 1 iter bound = 1 */
%s34 = sadd.s32 1, %s120403
%p36 = scmp.ge.s32.totalorder %s34, 8
%s37 = scalar_select /*predicate=*/%p36, /*on_true=*/0, /*on_false=*/%s34
%p119738 = scmp.ge.s32.totalorder %s120399, 1
%p232 = scmp.lt.s32.totalorder %s120399, 9
%p233 = pnand %p119738, %p232

%236 = sbr.rel (%p233) target = $region36

%v879 = vld [vmem:[%s4] ss:$0 sm:$0xff]
%880 = vbcast.lane.b32.xlu1 %v879, 1
%379 = vbcast.lane.b32.xlu0 %v879, 0
%1367 = vbcast.lane.b32.xlu1 %v879, 2
%1854 = vbcast.lane.b32.xlu0 %v879, 3
%2341 = vbcast.lane.b32.xlu1 %v879, 4
%s120390 = sshll.u32 %s120407, 8
%s353 = sshrl.u32 %s120390, 10
%p119745 = scmp.gt.s32.totalorder %s353, 1
%s355 = scalar_select /*predicate=*/%p119745, /*on_true=*/1, /*on_false=*/%s353
%s356 = sand.u32 1023, %s120390 /* smod.u32 w/div 1024 */
%s357 = sshrl.u32 %s356, 7
%s358 = sand.u32 127, %s356 /* smod.u32 w/div 128 */
%s119746 = sshll.u32 %s355, 3
%s360 = scalar_lea.vmem %s3, %s119746
%s362 = scalar_lea.vmem %s360, %s357
%v363 = vld [vmem:[%s362] ss:$0 sm:$0xff]
%s364 = sand.u32 255, %s358
%s366 = sor.u32 256, %s364
%367 = vbcast.lane.b32.xlu0 %v363, %s366
%3315 = vbcast.lane.b32.xlu1 %v879, 6
%2828 = vbcast.lane.b32.xlu0 %v879, 5
%s4278 = sadd.s32 8, %s120390
%s4279 = sshrl.u32 %s4278, 10
%p119764 = scmp.gt.s32.totalorder %s4279, 1
%s4281 = scalar_select /*predicate=*/%p119764, /*on_true=*/1, /*on_false=*/%s4279
%s4282 = sand.u32 1023, %s4278 /* smod.u32 w/div 1024 */
%s4283 = sshrl.u32 %s4282, 7
%s4284 = sand.u32 127, %s4282 /* smod.u32 w/div 128 */
%s119765 = sshll.u32 %s4281, 3
%s4286 = scalar_lea.vmem %s3, %s119765
%s4288 = scalar_lea.vmem %s4286, %s4283
%v4289 = vld [vmem:[%s4288] ss:$0 sm:$0xff]
%s4290 = sand.u32 255, %s4284
%s4292 = sor.u32 256, %s4290
%4293 = vbcast.lane.b32.xlu1 %v4289, %s4292
%3802 = vbcast.lane.b32.xlu0 %v879, 7
%s11722 = sadd.s32 24, %s120390
%s11723 = sshrl.u32 %s11722, 10
%p119804 = scmp.gt.s32.totalorder %s11723, 1
%s11725 = scalar_select /*predicate=*/%p119804, /*on_true=*/1, /*on_false=*/%s11723
%s11726 = sand.u32 1023, %s11722 /* smod.u32 w/div 1024 */
%s11727 = sshrl.u32 %s11726, 7
%s11728 = sand.u32 127, %s11726 /* smod.u32 w/div 128 */
%s119805 = sshll.u32 %s11725, 3
%s11730 = scalar_lea.vmem %s3, %s119805
%s11732 = scalar_lea.vmem %s11730, %s11727
%v11733 = vld [vmem:[%s11732] ss:$0 sm:$0xff]
%s11734 = sand.u32 255, %s11728
%s11736 = sor.u32 256, %s11734
%11737 = vbcast.lane.b32.xlu1 %v11733, %s11736
%s8000 = sadd.s32 16, %s120390
%s8001 = sshrl.u32 %s8000, 10
%p119784 = scmp.gt.s32.totalorder %s8001, 1
%s8003 = scalar_select /*predicate=*/%p119784, /*on_true=*/1, /*on_false=*/%s8001
%s8004 = sand.u32 1023, %s8000 /* smod.u32 w/div 1024 */
%s8005 = sshrl.u32 %s8004, 7
%s8006 = sand.u32 127, %s8004 /* smod.u32 w/div 128 */
%s119785 = sshll.u32 %s8003, 3
%s8008 = scalar_lea.vmem %s3, %s119785
%s8010 = scalar_lea.vmem %s8008, %s8005
%v8011 = vld [vmem:[%s8010] ss:$0 sm:$0xff]
%s8012 = sand.u32 255, %s8006
%s8014 = sor.u32 256, %s8012
%8015 = vbcast.lane.b32.xlu0 %v8011, %s8014
%s19166 = sadd.s32 40, %s120390
%s19167 = sshrl.u32 %s19166, 10
%p119844 = scmp.gt.s32.totalorder %s19167, 1
%s19169 = scalar_select /*predicate=*/%p119844, /*on_true=*/1, /*on_false=*/%s19167
%s19170 = sand.u32 1023, %s19166 /* smod.u32 w/div 1024 */
%s19171 = sshrl.u32 %s19170, 7
%s19172 = sand.u32 127, %s19170 /* smod.u32 w/div 128 */
%s119845 = sshll.u32 %s19169, 3
%s19174 = scalar_lea.vmem %s3, %s119845
%s19176 = scalar_lea.vmem %s19174, %s19171
%v19177 = vld [vmem:[%s19176] ss:$0 sm:$0xff]
%s19178 = sand.u32 255, %s19172
%s19180 = sor.u32 256, %s19178
%19181 = vbcast.lane.b32.xlu1 %v19177, %s19180
%s15444 = sadd.s32 32, %s120390
%s15445 = sshrl.u32 %s15444, 10
%p119824 = scmp.gt.s32.totalorder %s15445, 1
%s15447 = scalar_select /*predicate=*/%p119824, /*on_true=*/1, /*on_false=*/%s15445
%s15448 = sand.u32 1023, %s15444 /* smod.u32 w/div 1024 */
%s15449 = sshrl.u32 %s15448, 7
%s15450 = sand.u32 127, %s15448 /* smod.u32 w/div 128 */
%s119825 = sshll.u32 %s15447, 3
%s15452 = scalar_lea.vmem %s3, %s119825
%s15454 = scalar_lea.vmem %s15452, %s15449
%v15455 = vld [vmem:[%s15454] ss:$0 sm:$0xff]
%s15456 = sand.u32 255, %s15450
%s15458 = sor.u32 256, %s15456
%15459 = vbcast.lane.b32.xlu0 %v15455, %s15458
%s26610 = sadd.s32 56, %s120390
%s26611 = sshrl.u32 %s26610, 10
%p119884 = scmp.gt.s32.totalorder %s26611, 1
%s26613 = scalar_select /*predicate=*/%p119884, /*on_true=*/1, /*on_false=*/%s26611
%s26614 = sand.u32 1023, %s26610 /* smod.u32 w/div 1024 */
%s26615 = sshrl.u32 %s26614, 7
%s26616 = sand.u32 127, %s26614 /* smod.u32 w/div 128 */
%s119885 = sshll.u32 %s26613, 3
%s26618 = scalar_lea.vmem %s3, %s119885
%s26620 = scalar_lea.vmem %s26618, %s26615
%v26621 = vld [vmem:[%s26620] ss:$0 sm:$0xff]
%s26622 = sand.u32 255, %s26616
%s26624 = sor.u32 256, %s26622
%26625 = vbcast.lane.b32.xlu1 %v26621, %s26624
%s22888 = sadd.s32 48, %s120390
%s22889 = sshrl.u32 %s22888, 10
%p119864 = scmp.gt.s32.totalorder %s22889, 1
%s22891 = scalar_select /*predicate=*/%p119864, /*on_true=*/1, /*on_false=*/%s22889
%s22892 = sand.u32 1023, %s22888 /* smod.u32 w/div 1024 */
%s22893 = sshrl.u32 %s22892, 7
%s22894 = sand.u32 127, %s22892 /* smod.u32 w/div 128 */
%s119865 = sshll.u32 %s22891, 3
%s22896 = scalar_lea.vmem %s3, %s119865
%s22898 = scalar_lea.vmem %s22896, %s22893
%v22899 = vld [vmem:[%s22898] ss:$0 sm:$0xff]
%s22900 = sand.u32 255, %s22894
%s22902 = sor.u32 256, %s22900
%22903 = vbcast.lane.b32.xlu0 %v22899, %s22902
%s34054 = sadd.s32 72, %s120390
%s34055 = sshrl.u32 %s34054, 10
%p119924 = scmp.gt.s32.totalorder %s34055, 1
%s34057 = scalar_select /*predicate=*/%p119924, /*on_true=*/1, /*on_false=*/%s34055
%s34058 = sand.u32 1023, %s34054 /* smod.u32 w/div 1024 */
%s34059 = sshrl.u32 %s34058, 7
%s34060 = sand.u32 127, %s34058 /* smod.u32 w/div 128 */
%s119925 = sshll.u32 %s34057, 3
%s34062 = scalar_lea.vmem %s3, %s119925
%s34064 = scalar_lea.vmem %s34062, %s34059
%v34065 = vld [vmem:[%s34064] ss:$0 sm:$0xff]
%s34066 = sand.u32 255, %s34060
%s34068 = sor.u32 256, %s34066
%34069 = vbcast.lane.b32.xlu1 %v34065, %s34068
%s30332 = sadd.s32 64, %s120390
%s30333 = sshrl.u32 %s30332, 10
%p119904 = scmp.gt.s32.totalorder %s30333, 1
%s30335 = scalar_select /*predicate=*/%p119904, /*on_true=*/1, /*on_false=*/%s30333
%s30336 = sand.u32 1023, %s30332 /* smod.u32 w/div 1024 */
%s30337 = sshrl.u32 %s30336, 7
%s30338 = sand.u32 127, %s30336 /* smod.u32 w/div 128 */
%s119905 = sshll.u32 %s30335, 3
%s30340 = scalar_lea.vmem %s3, %s119905
%s30342 = scalar_lea.vmem %s30340, %s30337
%v30343 = vld [vmem:[%s30342] ss:$0 sm:$0xff]
%s30344 = sand.u32 255, %s30338
%s30346 = sor.u32 256, %s30344
%30347 = vbcast.lane.b32.xlu0 %v30343, %s30346
%s41498 = sadd.s32 88, %s120390
%s41499 = sshrl.u32 %s41498, 10
%p119964 = scmp.gt.s32.totalorder %s41499, 1
%s41501 = scalar_select /*predicate=*/%p119964, /*on_true=*/1, /*on_false=*/%s41499
%s41502 = sand.u32 1023, %s41498 /* smod.u32 w/div 1024 */
%s41503 = sshrl.u32 %s41502, 7
%s41504 = sand.u32 127, %s41502 /* smod.u32 w/div 128 */
%s119965 = sshll.u32 %s41501, 3
%s41506 = scalar_lea.vmem %s3, %s119965
%s41508 = scalar_lea.vmem %s41506, %s41503
%v41509 = vld [vmem:[%s41508] ss:$0 sm:$0xff]
%s41510 = sand.u32 255, %s41504
%s41512 = sor.u32 256, %s41510
%41513 = vbcast.lane.b32.xlu1 %v41509, %s41512
%s37776 = sadd.s32 80, %s120390
%s37777 = sshrl.u32 %s37776, 10
%p119944 = scmp.gt.s32.totalorder %s37777, 1
%s37779 = scalar_select /*predicate=*/%p119944, /*on_true=*/1, /*on_false=*/%s37777
%s37780 = sand.u32 1023, %s37776 /* smod.u32 w/div 1024 */
%s37781 = sshrl.u32 %s37780, 7
%s37782 = sand.u32 127, %s37780 /* smod.u32 w/div 128 */
%s119945 = sshll.u32 %s37779, 3
%s37784 = scalar_lea.vmem %s3, %s119945
%s37786 = scalar_lea.vmem %s37784, %s37781
%v37787 = vld [vmem:[%s37786] ss:$0 sm:$0xff]
%s37788 = sand.u32 255, %s37782
%s37790 = sor.u32 256, %s37788
%37791 = vbcast.lane.b32.xlu0 %v37787, %s37790
%s48942 = sadd.s32 104, %s120390
%s48943 = sshrl.u32 %s48942, 10
%p120004 = scmp.gt.s32.totalorder %s48943, 1
%s48945 = scalar_select /*predicate=*/%p120004, /*on_true=*/1, /*on_false=*/%s48943
%s48946 = sand.u32 1023, %s48942 /* smod.u32 w/div 1024 */
%s48947 = sshrl.u32 %s48946, 7
%s48948 = sand.u32 127, %s48946 /* smod.u32 w/div 128 */
%s120005 = sshll.u32 %s48945, 3
%s48950 = scalar_lea.vmem %s3, %s120005
%s48952 = scalar_lea.vmem %s48950, %s48947
%v48953 = vld [vmem:[%s48952] ss:$0 sm:$0xff]
%s48954 = sand.u32 255, %s48948
%s48956 = sor.u32 256, %s48954
%48957 = vbcast.lane.b32.xlu1 %v48953, %s48956
%s45220 = sadd.s32 96, %s120390
%s45221 = sshrl.u32 %s45220, 10
%p119984 = scmp.gt.s32.totalorder %s45221, 1
%s45223 = scalar_select /*predicate=*/%p119984, /*on_true=*/1, /*on_false=*/%s45221
%s45224 = sand.u32 1023, %s45220 /* smod.u32 w/div 1024 */
%s45225 = sshrl.u32 %s45224, 7
%s45226 = sand.u32 127, %s45224 /* smod.u32 w/div 128 */
%s119985 = sshll.u32 %s45223, 3
%s45228 = scalar_lea.vmem %s3, %s119985
%s45230 = scalar_lea.vmem %s45228, %s45225
%v45231 = vld [vmem:[%s45230] ss:$0 sm:$0xff]
%s45232 = sand.u32 255, %s45226
%s45234 = sor.u32 256, %s45232
%45235 = vbcast.lane.b32.xlu0 %v45231, %s45234
%s56386 = sadd.s32 120, %s120390
%s56387 = sshrl.u32 %s56386, 10
%p120044 = scmp.gt.s32.totalorder %s56387, 1
%s56389 = scalar_select /*predicate=*/%p120044, /*on_true=*/1, /*on_false=*/%s56387
%s56390 = sand.u32 1023, %s56386 /* smod.u32 w/div 1024 */
%s56391 = sshrl.u32 %s56390, 7
%s56392 = sand.u32 127, %s56390 /* smod.u32 w/div 128 */
%s120045 = sshll.u32 %s56389, 3
%s56394 = scalar_lea.vmem %s3, %s120045
%s56396 = scalar_lea.vmem %s56394, %s56391
%v56397 = vld [vmem:[%s56396] ss:$0 sm:$0xff]
%s56398 = sand.u32 255, %s56392
%s56400 = sor.u32 256, %s56398
%56401 = vbcast.lane.b32.xlu1 %v56397, %s56400
%s52664 = sadd.s32 112, %s120390
%s52665 = sshrl.u32 %s52664, 10
%p120024 = scmp.gt.s32.totalorder %s52665, 1
%s52667 = scalar_select /*predicate=*/%p120024, /*on_true=*/1, /*on_false=*/%s52665
%s52668 = sand.u32 1023, %s52664 /* smod.u32 w/div 1024 */
%s52669 = sshrl.u32 %s52668, 7
%s52670 = sand.u32 127, %s52668 /* smod.u32 w/div 128 */
%s120025 = sshll.u32 %s52667, 3
%s52672 = scalar_lea.vmem %s3, %s120025
%s52674 = scalar_lea.vmem %s52672, %s52669
%v52675 = vld [vmem:[%s52674] ss:$0 sm:$0xff]
%s52676 = sand.u32 255, %s52670
%s52678 = sor.u32 256, %s52676
%52679 = vbcast.lane.b32.xlu0 %v52675, %s52678
%s63830 = sadd.s32 136, %s120390
%s63831 = sshrl.u32 %s63830, 10
%p120084 = scmp.gt.s32.totalorder %s63831, 1
%s63833 = scalar_select /*predicate=*/%p120084, /*on_true=*/1, /*on_false=*/%s63831
%s63834 = sand.u32 1023, %s63830 /* smod.u32 w/div 1024 */
%s63835 = sshrl.u32 %s63834, 7
%s63836 = sand.u32 127, %s63834 /* smod.u32 w/div 128 */
%s120085 = sshll.u32 %s63833, 3
%s63838 = scalar_lea.vmem %s3, %s120085
%s63840 = scalar_lea.vmem %s63838, %s63835
%v63841 = vld [vmem:[%s63840] ss:$0 sm:$0xff]
%s63842 = sand.u32 255, %s63836
%s63844 = sor.u32 256, %s63842
%63845 = vbcast.lane.b32.xlu1 %v63841, %s63844
%s60108 = sadd.s32 128, %s120390
%s60109 = sshrl.u32 %s60108, 10
%p120064 = scmp.gt.s32.totalorder %s60109, 1
%s60111 = scalar_select /*predicate=*/%p120064, /*on_true=*/1, /*on_false=*/%s60109
%s60112 = sand.u32 1023, %s60108 /* smod.u32 w/div 1024 */
%s60113 = sshrl.u32 %s60112, 7
%s60114 = sand.u32 127, %s60112 /* smod.u32 w/div 128 */
%s120065 = sshll.u32 %s60111, 3
%s60116 = scalar_lea.vmem %s3, %s120065
%s60118 = scalar_lea.vmem %s60116, %s60113
%v60119 = vld [vmem:[%s60118] ss:$0 sm:$0xff]
%s60120 = sand.u32 255, %s60114
%s60122 = sor.u32 256, %s60120
%60123 = vbcast.lane.b32.xlu0 %v60119, %s60122
%s71274 = sadd.s32 152, %s120390
%s71275 = sshrl.u32 %s71274, 10
%p120124 = scmp.gt.s32.totalorder %s71275, 1
%s71277 = scalar_select /*predicate=*/%p120124, /*on_true=*/1, /*on_false=*/%s71275
%s71278 = sand.u32 1023, %s71274 /* smod.u32 w/div 1024 */
%s71279 = sshrl.u32 %s71278, 7
%s71280 = sand.u32 127, %s71278 /* smod.u32 w/div 128 */
%s120125 = sshll.u32 %s71277, 3
%s71282 = scalar_lea.vmem %s3, %s120125
%s71284 = scalar_lea.vmem %s71282, %s71279
%v71285 = vld [vmem:[%s71284] ss:$0 sm:$0xff]
%s71286 = sand.u32 255, %s71280
%s71288 = sor.u32 256, %s71286
%71289 = vbcast.lane.b32.xlu1 %v71285, %s71288
%s67552 = sadd.s32 144, %s120390
%s67553 = sshrl.u32 %s67552, 10
%p120104 = scmp.gt.s32.totalorder %s67553, 1
%s67555 = scalar_select /*predicate=*/%p120104, /*on_true=*/1, /*on_false=*/%s67553
%s67556 = sand.u32 1023, %s67552 /* smod.u32 w/div 1024 */
%s67557 = sshrl.u32 %s67556, 7
%s67558 = sand.u32 127, %s67556 /* smod.u32 w/div 128 */
%s120105 = sshll.u32 %s67555, 3
%s67560 = scalar_lea.vmem %s3, %s120105
%s67562 = scalar_lea.vmem %s67560, %s67557
%v67563 = vld [vmem:[%s67562] ss:$0 sm:$0xff]
%s67564 = sand.u32 255, %s67558
%s67566 = sor.u32 256, %s67564
%67567 = vbcast.lane.b32.xlu0 %v67563, %s67566
%s78718 = sadd.s32 168, %s120390
%s78719 = sshrl.u32 %s78718, 10
%p120164 = scmp.gt.s32.totalorder %s78719, 1
%s78721 = scalar_select /*predicate=*/%p120164, /*on_true=*/1, /*on_false=*/%s78719
%s78722 = sand.u32 1023, %s78718 /* smod.u32 w/div 1024 */
%s78723 = sshrl.u32 %s78722, 7
%s78724 = sand.u32 127, %s78722 /* smod.u32 w/div 128 */
%s120165 = sshll.u32 %s78721, 3
%s78726 = scalar_lea.vmem %s3, %s120165
%s78728 = scalar_lea.vmem %s78726, %s78723
%v78729 = vld [vmem:[%s78728] ss:$0 sm:$0xff]
%s78730 = sand.u32 255, %s78724
%s78732 = sor.u32 256, %s78730
%78733 = vbcast.lane.b32.xlu1 %v78729, %s78732
%s74996 = sadd.s32 160, %s120390
%s74997 = sshrl.u32 %s74996, 10
%p120144 = scmp.gt.s32.totalorder %s74997, 1
%s74999 = scalar_select /*predicate=*/%p120144, /*on_true=*/1, /*on_false=*/%s74997
%s75000 = sand.u32 1023, %s74996 /* smod.u32 w/div 1024 */
%s75001 = sshrl.u32 %s75000, 7
%s75002 = sand.u32 127, %s75000 /* smod.u32 w/div 128 */
%s120145 = sshll.u32 %s74999, 3
%s75004 = scalar_lea.vmem %s3, %s120145
%s75006 = scalar_lea.vmem %s75004, %s75001
%v75007 = vld [vmem:[%s75006] ss:$0 sm:$0xff]
%s75008 = sand.u32 255, %s75002
%s75010 = sor.u32 256, %s75008
%75011 = vbcast.lane.b32.xlu0 %v75007, %s75010
%s86162 = sadd.s32 184, %s120390
%s86163 = sshrl.u32 %s86162, 10
%p120204 = scmp.gt.s32.totalorder %s86163, 1
%s86165 = scalar_select /*predicate=*/%p120204, /*on_true=*/1, /*on_false=*/%s86163
%s86166 = sand.u32 1023, %s86162 /* smod.u32 w/div 1024 */
%s86167 = sshrl.u32 %s86166, 7
%s86168 = sand.u32 127, %s86166 /* smod.u32 w/div 128 */
%s120205 = sshll.u32 %s86165, 3
%s86170 = scalar_lea.vmem %s3, %s120205
%s86172 = scalar_lea.vmem %s86170, %s86167
%v86173 = vld [vmem:[%s86172] ss:$0 sm:$0xff]
%s86174 = sand.u32 255, %s86168
%s86176 = sor.u32 256, %s86174
%86177 = vbcast.lane.b32.xlu1 %v86173, %s86176
%s82440 = sadd.s32 176, %s120390
%s82441 = sshrl.u32 %s82440, 10
%p120184 = scmp.gt.s32.totalorder %s82441, 1
%s82443 = scalar_select /*predicate=*/%p120184, /*on_true=*/1, /*on_false=*/%s82441
%s82444 = sand.u32 1023, %s82440 /* smod.u32 w/div 1024 */
%s82445 = sshrl.u32 %s82444, 7
%s82446 = sand.u32 127, %s82444 /* smod.u32 w/div 128 */
%s120185 = sshll.u32 %s82443, 3
%s82448 = scalar_lea.vmem %s3, %s120185
%s82450 = scalar_lea.vmem %s82448, %s82445
%v82451 = vld [vmem:[%s82450] ss:$0 sm:$0xff]
%s82452 = sand.u32 255, %s82446
%s82454 = sor.u32 256, %s82452
%82455 = vbcast.lane.b32.xlu0 %v82451, %s82454
%s93606 = sadd.s32 200, %s120390
%s93607 = sshrl.u32 %s93606, 10
%p120244 = scmp.gt.s32.totalorder %s93607, 1
%s93609 = scalar_select /*predicate=*/%p120244, /*on_true=*/1, /*on_false=*/%s93607
%s93610 = sand.u32 1023, %s93606 /* smod.u32 w/div 1024 */
%s93611 = sshrl.u32 %s93610, 7
%s93612 = sand.u32 127, %s93610 /* smod.u32 w/div 128 */
%s120245 = sshll.u32 %s93609, 3
%s93614 = scalar_lea.vmem %s3, %s120245
%s93616 = scalar_lea.vmem %s93614, %s93611
%v93617 = vld [vmem:[%s93616] ss:$0 sm:$0xff]
%s93618 = sand.u32 255, %s93612
%s93620 = sor.u32 256, %s93618
%93621 = vbcast.lane.b32.xlu1 %v93617, %s93620
%s89884 = sadd.s32 192, %s120390
%s89885 = sshrl.u32 %s89884, 10
%p120224 = scmp.gt.s32.totalorder %s89885, 1
%s89887 = scalar_select /*predicate=*/%p120224, /*on_true=*/1, /*on_false=*/%s89885
%s89888 = sand.u32 1023, %s89884 /* smod.u32 w/div 1024 */
%s89889 = sshrl.u32 %s89888, 7
%s89890 = sand.u32 127, %s89888 /* smod.u32 w/div 128 */
%s120225 = sshll.u32 %s89887, 3
%s89892 = scalar_lea.vmem %s3, %s120225
%s89894 = scalar_lea.vmem %s89892, %s89889
%v89895 = vld [vmem:[%s89894] ss:$0 sm:$0xff]
%s89896 = sand.u32 255, %s89890
%s89898 = sor.u32 256, %s89896
%89899 = vbcast.lane.b32.xlu0 %v89895, %s89898
%s101050 = sadd.s32 216, %s120390
%s101051 = sshrl.u32 %s101050, 10
%p120284 = scmp.gt.s32.totalorder %s101051, 1
%s101053 = scalar_select /*predicate=*/%p120284, /*on_true=*/1, /*on_false=*/%s101051
%s101054 = sand.u32 1023, %s101050 /* smod.u32 w/div 1024 */
%s101055 = sshrl.u32 %s101054, 7
%s101056 = sand.u32 127, %s101054 /* smod.u32 w/div 128 */
%s120285 = sshll.u32 %s101053, 3
%s101058 = scalar_lea.vmem %s3, %s120285
%s101060 = scalar_lea.vmem %s101058, %s101055
%v101061 = vld [vmem:[%s101060] ss:$0 sm:$0xff]
%s101062 = sand.u32 255, %s101056
%s101064 = sor.u32 256, %s101062
%101065 = vbcast.lane.b32.xlu1 %v101061, %s101064
%s97328 = sadd.s32 208, %s120390
%s97329 = sshrl.u32 %s97328, 10
%p120264 = scmp.gt.s32.totalorder %s97329, 1
%s97331 = scalar_select /*predicate=*/%p120264, /*on_true=*/1, /*on_false=*/%s97329
%s97332 = sand.u32 1023, %s97328 /* smod.u32 w/div 1024 */
%s97333 = sshrl.u32 %s97332, 7
%s97334 = sand.u32 127, %s97332 /* smod.u32 w/div 128 */
%s120265 = sshll.u32 %s97331, 3
%s97336 = scalar_lea.vmem %s3, %s120265
%s97338 = scalar_lea.vmem %s97336, %s97333
%v97339 = vld [vmem:[%s97338] ss:$0 sm:$0xff]
%s97340 = sand.u32 255, %s97334
%s97342 = sor.u32 256, %s97340
%97343 = vbcast.lane.b32.xlu0 %v97339, %s97342
%s108494 = sadd.s32 232, %s120390
%s108495 = sshrl.u32 %s108494, 10
%p120324 = scmp.gt.s32.totalorder %s108495, 1
%s108497 = scalar_select /*predicate=*/%p120324, /*on_true=*/1, /*on_false=*/%s108495
%s108498 = sand.u32 1023, %s108494 /* smod.u32 w/div 1024 */
%s108499 = sshrl.u32 %s108498, 7
%s108500 = sand.u32 127, %s108498 /* smod.u32 w/div 128 */
%s120325 = sshll.u32 %s108497, 3
%s108502 = scalar_lea.vmem %s3, %s120325
%s108504 = scalar_lea.vmem %s108502, %s108499
%v108505 = vld [vmem:[%s108504] ss:$0 sm:$0xff]
%s108506 = sand.u32 255, %s108500
%s108508 = sor.u32 256, %s108506
%108509 = vbcast.lane.b32.xlu1 %v108505, %s108508
%s104772 = sadd.s32 224, %s120390
%s104773 = sshrl.u32 %s104772, 10
%p120304 = scmp.gt.s32.totalorder %s104773, 1
%s104775 = scalar_select /*predicate=*/%p120304, /*on_true=*/1, /*on_false=*/%s104773
%s104776 = sand.u32 1023, %s104772 /* smod.u32 w/div 1024 */
%s104777 = sshrl.u32 %s104776, 7
%s104778 = sand.u32 127, %s104776 /* smod.u32 w/div 128 */
%s120305 = sshll.u32 %s104775, 3
%s104780 = scalar_lea.vmem %s3, %s120305
%s104782 = scalar_lea.vmem %s104780, %s104777
%v104783 = vld [vmem:[%s104782] ss:$0 sm:$0xff]
%s104784 = sand.u32 255, %s104778
%s104786 = sor.u32 256, %s104784
%104787 = vbcast.lane.b32.xlu0 %v104783, %s104786
%s115938 = sadd.s32 248, %s120390
%s115939 = sshrl.u32 %s115938, 10
%p120364 = scmp.gt.s32.totalorder %s115939, 1
%s115941 = scalar_select /*predicate=*/%p120364, /*on_true=*/1, /*on_false=*/%s115939
%s115942 = sand.u32 1023, %s115938 /* smod.u32 w/div 1024 */
%s115943 = sshrl.u32 %s115942, 7
%s115944 = sand.u32 127, %s115942 /* smod.u32 w/div 128 */
%s120365 = sshll.u32 %s115941, 3
%s115946 = scalar_lea.vmem %s3, %s120365
%s115948 = scalar_lea.vmem %s115946, %s115943
%v115949 = vld [vmem:[%s115948] ss:$0 sm:$0xff]
%s115950 = sand.u32 255, %s115944
%s115952 = sor.u32 256, %s115950
%115953 = vbcast.lane.b32.xlu1 %v115949, %s115952
%s112216 = sadd.s32 240, %s120390
%s112217 = sshrl.u32 %s112216, 10
%p120344 = scmp.gt.s32.totalorder %s112217, 1
%s112219 = scalar_select /*predicate=*/%p120344, /*on_true=*/1, /*on_false=*/%s112217
%s112220 = sand.u32 1023, %s112216 /* smod.u32 w/div 1024 */
%s112221 = sshrl.u32 %s112220, 7
%s112222 = sand.u32 127, %s112220 /* smod.u32 w/div 128 */
%s120345 = sshll.u32 %s112219, 3
%s112224 = scalar_lea.vmem %s3, %s120345
%s112226 = scalar_lea.vmem %s112224, %s112221
%v112227 = vld [vmem:[%s112226] ss:$0 sm:$0xff]
%s112228 = sand.u32 255, %s112222
%s112230 = sor.u32 256, %s112228
%112231 = vbcast.lane.b32.xlu0 %v112227, %s112230
%v406 = vld [vmem:[%s6] ss:$0 sm:$0xff]
%407 = vbcast.lane.b32.xlu1 %v406, 0
%s388 = scalar_lea.vmem %s5, %s119746
%s390 = scalar_lea.vmem %s388, %s357
%v391 = vld [vmem:[%s390] ss:$0 sm:$0xff]
%395 = vbcast.lane.b32.xlu0 %v391, %s366
%1380 = vbcast.lane.b32.xlu1 %v406, 2
%893 = vbcast.lane.b32.xlu0 %v406, 1
%2354 = vbcast.lane.b32.xlu1 %v406, 4
%1867 = vbcast.lane.b32.xlu0 %v406, 3
%3328 = vbcast.lane.b32.xlu1 %v406, 6
%2841 = vbcast.lane.b32.xlu0 %v406, 5
%s4303 = scalar_lea.vmem %s5, %s119765
%s4305 = scalar_lea.vmem %s4303, %s4283
%v4306 = vld [vmem:[%s4305] ss:$0 sm:$0xff]
%4310 = vbcast.lane.b32.xlu1 %v4306, %s4292
%3815 = vbcast.lane.b32.xlu0 %v406, 7
%s11747 = scalar_lea.vmem %s5, %s119805
%s11749 = scalar_lea.vmem %s11747, %s11727
%v11750 = vld [vmem:[%s11749] ss:$0 sm:$0xff]
%11754 = vbcast.lane.b32.xlu1 %v11750, %s11736
%s8025 = scalar_lea.vmem %s5, %s119785
%s8027 = scalar_lea.vmem %s8025, %s8005
%v8028 = vld [vmem:[%s8027] ss:$0 sm:$0xff]
%8032 = vbcast.lane.b32.xlu0 %v8028, %s8014
%s19191 = scalar_lea.vmem %s5, %s119845
%s19193 = scalar_lea.vmem %s19191, %s19171
%v19194 = vld [vmem:[%s19193] ss:$0 sm:$0xff]
%19198 = vbcast.lane.b32.xlu1 %v19194, %s19180
%s15469 = scalar_lea.vmem %s5, %s119825
%s15471 = scalar_lea.vmem %s15469, %s15449
%v15472 = vld [vmem:[%s15471] ss:$0 sm:$0xff]
%15476 = vbcast.lane.b32.xlu0 %v15472, %s15458
%s26635 = scalar_lea.vmem %s5, %s119885
%s26637 = scalar_lea.vmem %s26635, %s26615
%v26638 = vld [vmem:[%s26637] ss:$0 sm:$0xff]
%26642 = vbcast.lane.b32.xlu1 %v26638, %s26624
%s22913 = scalar_lea.vmem %s5, %s119865
%s22915 = scalar_lea.vmem %s22913, %s22893
%v22916 = vld [vmem:[%s22915] ss:$0 sm:$0xff]
%22920 = vbcast.lane.b32.xlu0 %v22916, %s22902
%s34079 = scalar_lea.vmem %s5, %s119925
%s34081 = scalar_lea.vmem %s34079, %s34059
%v34082 = vld [vmem:[%s34081] ss:$0 sm:$0xff]
%34086 = vbcast.lane.b32.xlu1 %v34082, %s34068
%s30357 = scalar_lea.vmem %s5, %s119905
%s30359 = scalar_lea.vmem %s30357, %s30337
%v30360 = vld [vmem:[%s30359] ss:$0 sm:$0xff]
%30364 = vbcast.lane.b32.xlu0 %v30360, %s30346
%v881 = vpop.permute.xlu1 %880
%v380 = vpop.permute.xlu0 %379
%s41523 = scalar_lea.vmem %s5, %s119965
%s41525 = scalar_lea.vmem %s41523, %s41503
%v41526 = vld [vmem:[%s41525] ss:$0 sm:$0xff]
%41530 = vbcast.lane.b32.xlu1 %v41526, %s41512
%s37801 = scalar_lea.vmem %s5, %s119945
%s37803 = scalar_lea.vmem %s37801, %s37781
%v37804 = vld [vmem:[%s37803] ss:$0 sm:$0xff]
%37808 = vbcast.lane.b32.xlu0 %v37804, %s37790
%v1368 = vpop.permute.xlu1 %1367
%v1855 = vpop.permute.xlu0 %1854
%s48967 = scalar_lea.vmem %s5, %s120005
%s48969 = scalar_lea.vmem %s48967, %s48947
%v48970 = vld [vmem:[%s48969] ss:$0 sm:$0xff]
%48974 = vbcast.lane.b32.xlu1 %v48970, %s48956
%s45245 = scalar_lea.vmem %s5, %s119985
%s45247 = scalar_lea.vmem %s45245, %s45225
%v45248 = vld [vmem:[%s45247] ss:$0 sm:$0xff]
%45252 = vbcast.lane.b32.xlu0 %v45248, %s45234
%v2342 = vpop.permute.xlu1 %2341
%v368 = vpop.permute.xlu0 %367
%s56411 = scalar_lea.vmem %s5, %s120045
%s56413 = scalar_lea.vmem %s56411, %s56391
%v56414 = vld [vmem:[%s56413] ss:$0 sm:$0xff]
%56418 = vbcast.lane.b32.xlu1 %v56414, %s56400
%s52689 = scalar_lea.vmem %s5, %s120025
%s52691 = scalar_lea.vmem %s52689, %s52669
%v52692 = vld [vmem:[%s52691] ss:$0 sm:$0xff]
%52696 = vbcast.lane.b32.xlu0 %v52692, %s52678
%v3316 = vpop.permute.xlu1 %3315
%v2829 = vpop.permute.xlu0 %2828
%s63855 = scalar_lea.vmem %s5, %s120085
%s63857 = scalar_lea.vmem %s63855, %s63835
%v63858 = vld [vmem:[%s63857] ss:$0 sm:$0xff]
%63862 = vbcast.lane.b32.xlu1 %v63858, %s63844
%s60133 = scalar_lea.vmem %s5, %s120065
%s60135 = scalar_lea.vmem %s60133, %s60113
%v60136 = vld [vmem:[%s60135] ss:$0 sm:$0xff]
%60140 = vbcast.lane.b32.xlu0 %v60136, %s60122
%v4294 = vpop.permute.xlu1 %4293
%v3803 = vpop.permute.xlu0 %3802
%s71299 = scalar_lea.vmem %s5, %s120125
%s71301 = scalar_lea.vmem %s71299, %s71279
%v71302 = vld [vmem:[%s71301] ss:$0 sm:$0xff]
%71306 = vbcast.lane.b32.xlu1 %v71302, %s71288
%s67577 = scalar_lea.vmem %s5, %s120105
%s67579 = scalar_lea.vmem %s67577, %s67557
%v67580 = vld [vmem:[%s67579] ss:$0 sm:$0xff]
%67584 = vbcast.lane.b32.xlu0 %v67580, %s67566
%v11738 = vpop.permute.xlu1 %11737
%v8016 = vpop.permute.xlu0 %8015
%s78743 = scalar_lea.vmem %s5, %s120165
%s78745 = scalar_lea.vmem %s78743, %s78723
%v78746 = vld [vmem:[%s78745] ss:$0 sm:$0xff]
%78750 = vbcast.lane.b32.xlu1 %v78746, %s78732
%s75021 = scalar_lea.vmem %s5, %s120145
%s75023 = scalar_lea.vmem %s75021, %s75001
%v75024 = vld [vmem:[%s75023] ss:$0 sm:$0xff]
%75028 = vbcast.lane.b32.xlu0 %v75024, %s75010
%v19182 = vpop.permute.xlu1 %19181
%v15460 = vpop.permute.xlu0 %15459
%s86187 = scalar_lea.vmem %s5, %s120205
%s86189 = scalar_lea.vmem %s86187, %s86167
%v86190 = vld [vmem:[%s86189] ss:$0 sm:$0xff]
%86194 = vbcast.lane.b32.xlu1 %v86190, %s86176
%s82465 = scalar_lea.vmem %s5, %s120185
%s82467 = scalar_lea.vmem %s82465, %s82445
%v82468 = vld [vmem:[%s82467] ss:$0 sm:$0xff]
%82472 = vbcast.lane.b32.xlu0 %v82468, %s82454
%v26626 = vpop.permute.xlu1 %26625
%v22904 = vpop.permute.xlu0 %22903
%s93631 = scalar_lea.vmem %s5, %s120245
%s93633 = scalar_lea.vmem %s93631, %s93611
%v93634 = vld [vmem:[%s93633] ss:$0 sm:$0xff]
%93638 = vbcast.lane.b32.xlu1 %v93634, %s93620
%s89909 = scalar_lea.vmem %s5, %s120225
%s89911 = scalar_lea.vmem %s89909, %s89889
%v89912 = vld [vmem:[%s89911] ss:$0 sm:$0xff]
%89916 = vbcast.lane.b32.xlu0 %v89912, %s89898
%v34070 = vpop.permute.xlu1 %34069
%v30348 = vpop.permute.xlu0 %30347
%s101075 = scalar_lea.vmem %s5, %s120285
%s101077 = scalar_lea.vmem %s101075, %s101055
%v101078 = vld [vmem:[%s101077] ss:$0 sm:$0xff]
%101082 = vbcast.lane.b32.xlu1 %v101078, %s101064
%s97353 = scalar_lea.vmem %s5, %s120265
%s97355 = scalar_lea.vmem %s97353, %s97333
%v97356 = vld [vmem:[%s97355] ss:$0 sm:$0xff]
%97360 = vbcast.lane.b32.xlu0 %v97356, %s97342
%v41514 = vpop.permute.xlu1 %41513
%v37792 = vpop.permute.xlu0 %37791
%s108519 = scalar_lea.vmem %s5, %s120325
%s108521 = scalar_lea.vmem %s108519, %s108499
%v108522 = vld [vmem:[%s108521] ss:$0 sm:$0xff]
%108526 = vbcast.lane.b32.xlu1 %v108522, %s108508
%s104797 = scalar_lea.vmem %s5, %s120305
%s104799 = scalar_lea.vmem %s104797, %s104777
%v104800 = vld [vmem:[%s104799] ss:$0 sm:$0xff]
%104804 = vbcast.lane.b32.xlu0 %v104800, %s104786
%v48958 = vpop.permute.xlu1 %48957
%v45236 = vpop.permute.xlu0 %45235
%s115963 = scalar_lea.vmem %s5, %s120365
%s115965 = scalar_lea.vmem %s115963, %s115943
%v115966 = vld [vmem:[%s115965] ss:$0 sm:$0xff]
%115970 = vbcast.lane.b32.xlu1 %v115966, %s115952
%s112241 = scalar_lea.vmem %s5, %s120345
%s112243 = scalar_lea.vmem %s112241, %s112221
%v112244 = vld [vmem:[%s112243] ss:$0 sm:$0xff]
%112248 = vbcast.lane.b32.xlu0 %v112244, %s112230
%v56402 = vpop.permute.xlu1 %56401
%v52680 = vpop.permute.xlu0 %52679
%v63846 = vpop.permute.xlu1 %63845
%v60124 = vpop.permute.xlu0 %60123
%v71290 = vpop.permute.xlu1 %71289
%v67568 = vpop.permute.xlu0 %67567
%v78734 = vpop.permute.xlu1 %78733
%v75012 = vpop.permute.xlu0 %75011
%v86178 = vpop.permute.xlu1 %86177
%v82456 = vpop.permute.xlu0 %82455
%v93622 = vpop.permute.xlu1 %93621
%v89900 = vpop.permute.xlu0 %89899
%v101066 = vpop.permute.xlu1 %101065
%v97344 = vpop.permute.xlu0 %97343
%v108510 = vpop.permute.xlu1 %108509
%v104788 = vpop.permute.xlu0 %104787
%v115954 = vpop.permute.xlu1 %115953
%v112232 = vpop.permute.xlu0 %112231
%v408 = vpop.permute.xlu1 %407
%v396 = vpop.permute.xlu0 %395
%v1381 = vpop.permute.xlu1 %1380
%v894 = vpop.permute.xlu0 %893
%v2355 = vpop.permute.xlu1 %2354
%v1868 = vpop.permute.xlu0 %1867
%v3329 = vpop.permute.xlu1 %3328
%v2842 = vpop.permute.xlu0 %2841
%v4311 = vpop.permute.xlu1 %4310
%v3816 = vpop.permute.xlu0 %3815
%v11755 = vpop.permute.xlu1 %11754
%v8033 = vpop.permute.xlu0 %8032
%v19199 = vpop.permute.xlu1 %19198
%v15477 = vpop.permute.xlu0 %15476
%v26643 = vpop.permute.xlu1 %26642
%v22921 = vpop.permute.xlu0 %22920
%v34087 = vpop.permute.xlu1 %34086
%v30365 = vpop.permute.xlu0 %30364
%v41531 = vpop.permute.xlu1 %41530
%v37809 = vpop.permute.xlu0 %37808
%v48975 = vpop.permute.xlu1 %48974
%v45253 = vpop.permute.xlu0 %45252
%v56419 = vpop.permute.xlu1 %56418
%v52697 = vpop.permute.xlu0 %52696
%v63863 = vpop.permute.xlu1 %63862
%v60141 = vpop.permute.xlu0 %60140
%v71307 = vpop.permute.xlu1 %71306
%v67585 = vpop.permute.xlu0 %67584
%v78751 = vpop.permute.xlu1 %78750
%v75029 = vpop.permute.xlu0 %75028
%v86195 = vpop.permute.xlu1 %86194
%v82473 = vpop.permute.xlu0 %82472
%v93639 = vpop.permute.xlu1 %93638
%v89917 = vpop.permute.xlu0 %89916
%v101083 = vpop.permute.xlu1 %101082
%v97361 = vpop.permute.xlu0 %97360
%v108527 = vpop.permute.xlu1 %108526
%v104805 = vpop.permute.xlu0 %104804
%v115971 = vpop.permute.xlu1 %115970
%v112249 = vpop.permute.xlu0 %112248
%s119739 = sadd.s32 4294967295, %s120399
%s278 = sand.u32 1, %s119739 /* smod.u32 w/div 2 */
%s119740 = sshll.u32 %s278, 10
%s280 = scalar_lea.vmem [#allocation0], %s119740
%v411 = vadd.s32 %v408, %v396
%v414 = vlaneseq
%v415 = vand.u32 127, %v414
%v421 = vadd.s32 %v415, %v411
%vm425 = vcmp.lt.u32.totalorder %v421, %v411
%vm430 = vcmp.lt.u32.totalorder %v411, %v408
%v435 = vadd.s32 %v380, %v368
%v439 = vadd.s32 1, %v435
%v443 = vsel /*vm=*/%vm430, /*on_true_vy=*/%v439, /*on_false_vx=*/%v435
%v447 = vadd.s32 1, %v443
%v451 = vsel /*vm=*/%vm425, /*on_true_vy=*/%v447, /*on_false_vx=*/%v443
%v456 = vadd.s32 %v451, %v10
%v460 = vadd.s32 %v421, %v9
%v464 = vadd.s32 %v460, %v456
%v466 = vshll.u32 %v460, 13
%v467 = vshrl.u32 %v460, 19
%v468 = vor.u32 %v467, %v466
%v469 = vxor.u32 %v468, %v464
%v472 = vadd.s32 %v469, %v464
%v474 = vshll.u32 %v469, 15
%v475 = vshrl.u32 %v469, 17
%v476 = vor.u32 %v475, %v474
%v477 = vxor.u32 %v476, %v472
%v480 = vadd.s32 %v477, %v472
%v482 = vshll.u32 %v477, 26
%v483 = vshrl.u32 %v477, 6
%v484 = vor.u32 %v483, %v482
%v485 = vxor.u32 %v484, %v480
%v488 = vadd.s32 %v485, %v480
%v492 = vadd.s32 %v488, %v9
%v494 = vshll.u32 %v485, 6
%v495 = vshrl.u32 %v485, 26
%v496 = vor.u32 %v495, %v494
%v497 = vxor.u32 %v496, %v488
%v500 = vadd.s32 %v497, %v8
%v504 = vadd.s32 1, %v500
%v508 = vadd.s32 %v504, %v492
%v510 = vshll.u32 %v504, 17
%v511 = vshrl.u32 %v504, 15
%v512 = vor.u32 %v511, %v510
%v513 = vxor.u32 %v512, %v508
%v516 = vadd.s32 %v513, %v508
%v518 = vshll.u32 %v513, 29
%v519 = vshrl.u32 %v513, 3
%v520 = vor.u32 %v519, %v518
%v521 = vxor.u32 %v520, %v516
%v524 = vadd.s32 %v521, %v516
%v526 = vshll.u32 %v521, 16
%v527 = vshrl.u32 %v521, 16
%v528 = vor.u32 %v527, %v526
%v529 = vxor.u32 %v528, %v524
%v532 = vadd.s32 %v529, %v524
%v536 = vadd.s32 %v532, %v8
%v538 = vshll.u32 %v529, 24
%v539 = vshrl.u32 %v529, 8
%v540 = vor.u32 %v539, %v538
%v541 = vxor.u32 %v540, %v532
%v544 = vadd.s32 %v541, %v10
%v548 = vadd.s32 2, %v544
%v552 = vadd.s32 %v548, %v536
%v554 = vshll.u32 %v548, 13
%v555 = vshrl.u32 %v548, 19
%v556 = vor.u32 %v555, %v554
%v557 = vxor.u32 %v556, %v552
%v560 = vadd.s32 %v557, %v552
%v562 = vshll.u32 %v557, 15
%v563 = vshrl.u32 %v557, 17
%v564 = vor.u32 %v563, %v562
%v565 = vxor.u32 %v564, %v560
%v568 = vadd.s32 %v565, %v560
%v570 = vshll.u32 %v565, 26
%v571 = vshrl.u32 %v565, 6
%v572 = vor.u32 %v571, %v570
%v573 = vxor.u32 %v572, %v568
%v576 = vadd.s32 %v573, %v568
%v580 = vadd.s32 %v576, %v10
%v582 = vshll.u32 %v573, 6
%v583 = vshrl.u32 %v573, 26
%v584 = vor.u32 %v583, %v582
%v585 = vxor.u32 %v584, %v576
%v588 = vadd.s32 %v585, %v9
%v592 = vadd.s32 3, %v588
%v596 = vadd.s32 %v592, %v580
%v598 = vshll.u32 %v592, 17
%v599 = vshrl.u32 %v592, 15
%v600 = vor.u32 %v599, %v598
%v601 = vxor.u32 %v600, %v596
%v604 = vadd.s32 %v601, %v596
%v606 = vshll.u32 %v601, 29
%v607 = vshrl.u32 %v601, 3
%v608 = vor.u32 %v607, %v606
%v609 = vxor.u32 %v608, %v604
%v612 = vadd.s32 %v609, %v604
%v614 = vshll.u32 %v609, 16
%v615 = vshrl.u32 %v609, 16
%v616 = vor.u32 %v615, %v614
%v617 = vxor.u32 %v616, %v612
%v620 = vadd.s32 %v617, %v612
%v624 = vadd.s32 %v620, %v9
%v626 = vshll.u32 %v617, 24
%v627 = vshrl.u32 %v617, 8
%v628 = vor.u32 %v627, %v626
%v629 = vxor.u32 %v628, %v620
%v632 = vadd.s32 %v629, %v8
%v636 = vadd.s32 4, %v632
%v640 = vadd.s32 %v636, %v624
%v642 = vshll.u32 %v636, 13
%v643 = vshrl.u32 %v636, 19
%v644 = vor.u32 %v643, %v642
%v645 = vxor.u32 %v644, %v640
%v648 = vadd.s32 %v645, %v640
%v650 = vshll.u32 %v645, 15
%v651 = vshrl.u32 %v645, 17
%v652 = vor.u32 %v651, %v650
%v653 = vxor.u32 %v652, %v648
%v656 = vadd.s32 %v653, %v648
%v658 = vshll.u32 %v653, 26
%v659 = vshrl.u32 %v653, 6
%v660 = vor.u32 %v659, %v658
%v661 = vxor.u32 %v660, %v656
%v664 = vadd.s32 %v661, %v656
%v668 = vadd.s32 %v664, %v8
%v670 = vshll.u32 %v661, 6
%v671 = vshrl.u32 %v661, 26
%v672 = vor.u32 %v671, %v670
%v673 = vxor.u32 %v672, %v664
%v676 = vadd.s32 %v673, %v10
%v680 = vadd.s32 5, %v676
%v682 = vxor.u32 %v680, %v668
%v683 = vand.u32.u8 255, %v682
%v684 = vand.u32 65535, %v683
%v685 = vshrl.u32 %v684, 1
%v686 = vor.u32 16256, %v685
%v687 = vand.u32.u16 65535, %v686
%v119749 = vadd.low.f32.bf16 -1.0, %v687
%v696 = vmul.f32 2.0, %v119749
%v700 = vadd.f32 -0.99609375, %v696
%v704 = vmax.f32 %v700, -0.99609375
%v706 = vand.u32 2147483647, %v704
%vm709 = vcmp.eq.f32.partialorder %v706, 1.0
%v714 = vmul.f32 inf, %v704
%v716 = vxor.u32 2147483648, %v704
%v719 = vmul.f32 %v716, %v704
%v721 = vadd.f32 1.0, %v719
%v722 = vlog2.pop %v721
%v723 = vmul.f32 0.6931472, %v722
%v724 = vmul.f32 -0.5, %v719
%v725 = vadd.f32 1.0, %v724
%v726 = vmul.f32 %v725, %v719
%v727 = vand.u32 2147483647, %v719
%vm728 = vcmp.lt.f32.partialorder %v727, 0.0004427343
%v729 = vsel /*vm=*/%vm728, /*on_true_vy=*/%v726, /*on_false_vx=*/%v723
%v730 = vxor.u32 2147483648, %v729
%vm733 = vcmp.lt.f32.partialorder %v730, 5.0
%v120408 = vmov 2.8329768 /* materialized constant */
%v738 = vsel /*vm=*/%vm733, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v120409 = vmov 1.001674 /* materialized constant */
%v742 = vsel /*vm=*/%vm733, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v120410 = vmov 0.0094388705 /* materialized constant */
%v746 = vsel /*vm=*/%vm733, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v120411 = vmov -0.0076224613 /* materialized constant */
%v750 = vsel /*vm=*/%vm733, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v120412 = vmov 0.0057395077 /* materialized constant */
%v754 = vsel /*vm=*/%vm733, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v120413 = vmov -0.0036734284 /* materialized constant */
%v758 = vsel /*vm=*/%vm733, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v120414 = vmov 0.0013493432 /* materialized constant */
%v762 = vsel /*vm=*/%vm733, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v120415 = vmov 0.00010095056 /* materialized constant */
%v766 = vsel /*vm=*/%vm733, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v120416 = vmov -0.00020021426 /* materialized constant */
%v770 = vsel /*vm=*/%vm733, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v774 = vadd.f32 -2.5, %v730
%v776 = vrsqrt.pop %v730
%v777 = vmul.f32 %v776, %v730
%vm778 = vcmp.eq.f32.partialorder %v730, inf
%v779 = vsel /*vm=*/%vm778, /*on_true_vy=*/%v730, /*on_false_vx=*/%v777
%vm780 = vcmp.eq.f32.partialorder %v730, 0.0
%v781 = vand.u32 2147483648, %v730
%v782 = vsel /*vm=*/%vm780, /*on_true_vy=*/%v781, /*on_false_vx=*/%v779
%v785 = vadd.f32 -3.0, %v782
%v789 = vsel /*vm=*/%vm733, /*on_true_vy=*/%v774, /*on_false_vx=*/%v785
%v793 = vmul.f32 %v789, %v770
%v797 = vadd.f32 %v793, %v766
%v801 = vmul.f32 %v797, %v789
%v805 = vadd.f32 %v801, %v762
%v809 = vmul.f32 %v805, %v789
%v813 = vadd.f32 %v809, %v758
%v817 = vmul.f32 %v813, %v789
%v821 = vadd.f32 %v817, %v754
%v825 = vmul.f32 %v821, %v789
%v829 = vadd.f32 %v825, %v750
%v833 = vmul.f32 %v829, %v789
%v837 = vadd.f32 %v833, %v746
%v841 = vmul.f32 %v837, %v789
%v845 = vadd.f32 %v841, %v742
%v849 = vmul.f32 %v845, %v789
%v853 = vadd.f32 %v849, %v738
%v857 = vmul.f32 %v853, %v704
%v861 = vsel /*vm=*/%vm709, /*on_true_vy=*/%v714, /*on_false_vx=*/%v857
%v865 = vmul.f32 1.4140625, %v861
%v120417 = vmov 0.0 /* materialized constant */
%v867 = vpack.c.bf16 %v120417, %v865
%868 = vst [vmem:[%s280] sm:$0xf] /*vst_source=*/%v867
%v897 = vadd.s32 %v894, %v396
%v907 = vadd.s32 %v897, %v415
%vm911 = vcmp.lt.u32.totalorder %v907, %v897
%vm916 = vcmp.lt.u32.totalorder %v897, %v894
%v921 = vadd.s32 %v881, %v368
%v925 = vadd.s32 1, %v921
%v929 = vsel /*vm=*/%vm916, /*on_true_vy=*/%v925, /*on_false_vx=*/%v921
%v933 = vadd.s32 1, %v929
%v937 = vsel /*vm=*/%vm911, /*on_true_vy=*/%v933, /*on_false_vx=*/%v929
%v942 = vadd.s32 %v937, %v10
%v946 = vadd.s32 %v907, %v9
%v950 = vadd.s32 %v946, %v942
%v952 = vshll.u32 %v946, 13
%v953 = vshrl.u32 %v946, 19
%v954 = vor.u32 %v953, %v952
%v955 = vxor.u32 %v954, %v950
%v958 = vadd.s32 %v955, %v950
%v960 = vshll.u32 %v955, 15
%v961 = vshrl.u32 %v955, 17
%v962 = vor.u32 %v961, %v960
%v963 = vxor.u32 %v962, %v958
%v966 = vadd.s32 %v963, %v958
%v968 = vshll.u32 %v963, 26
%v969 = vshrl.u32 %v963, 6
%v970 = vor.u32 %v969, %v968
%v971 = vxor.u32 %v970, %v966
%v974 = vadd.s32 %v971, %v966
%v978 = vadd.s32 %v974, %v9
%v980 = vshll.u32 %v971, 6
%v981 = vshrl.u32 %v971, 26
%v982 = vor.u32 %v981, %v980
%v983 = vxor.u32 %v982, %v974
%v986 = vadd.s32 %v983, %v8
%v990 = vadd.s32 1, %v986
%v994 = vadd.s32 %v990, %v978
%v996 = vshll.u32 %v990, 17
%v997 = vshrl.u32 %v990, 15
%v998 = vor.u32 %v997, %v996
%v999 = vxor.u32 %v998, %v994
%v1002 = vadd.s32 %v999, %v994
%v1004 = vshll.u32 %v999, 29
%v1005 = vshrl.u32 %v999, 3
%v1006 = vor.u32 %v1005, %v1004
%v1007 = vxor.u32 %v1006, %v1002
%v1010 = vadd.s32 %v1007, %v1002
%v1012 = vshll.u32 %v1007, 16
%v1013 = vshrl.u32 %v1007, 16
%v1014 = vor.u32 %v1013, %v1012
%v1015 = vxor.u32 %v1014, %v1010
%v1018 = vadd.s32 %v1015, %v1010
%v1022 = vadd.s32 %v1018, %v8
%v1024 = vshll.u32 %v1015, 24
%v1025 = vshrl.u32 %v1015, 8
%v1026 = vor.u32 %v1025, %v1024
%v1027 = vxor.u32 %v1026, %v1018
%v1030 = vadd.s32 %v1027, %v10
%v1034 = vadd.s32 2, %v1030
%v1038 = vadd.s32 %v1034, %v1022
%v1040 = vshll.u32 %v1034, 13
%v1041 = vshrl.u32 %v1034, 19
%v1042 = vor.u32 %v1041, %v1040
%v1043 = vxor.u32 %v1042, %v1038
%v1046 = vadd.s32 %v1043, %v1038
%v1048 = vshll.u32 %v1043, 15
%v1049 = vshrl.u32 %v1043, 17
%v1050 = vor.u32 %v1049, %v1048
%v1051 = vxor.u32 %v1050, %v1046
%v1054 = vadd.s32 %v1051, %v1046
%v1056 = vshll.u32 %v1051, 26
%v1057 = vshrl.u32 %v1051, 6
%v1058 = vor.u32 %v1057, %v1056
%v1059 = vxor.u32 %v1058, %v1054
%v1062 = vadd.s32 %v1059, %v1054
%v1066 = vadd.s32 %v1062, %v10
%v1068 = vshll.u32 %v1059, 6
%v1069 = vshrl.u32 %v1059, 26
%v1070 = vor.u32 %v1069, %v1068
%v1071 = vxor.u32 %v1070, %v1062
%v1074 = vadd.s32 %v1071, %v9
%v1078 = vadd.s32 3, %v1074
%v1082 = vadd.s32 %v1078, %v1066
%v1084 = vshll.u32 %v1078, 17
%v1085 = vshrl.u32 %v1078, 15
%v1086 = vor.u32 %v1085, %v1084
%v1087 = vxor.u32 %v1086, %v1082
%v1090 = vadd.s32 %v1087, %v1082
%v1092 = vshll.u32 %v1087, 29
%v1093 = vshrl.u32 %v1087, 3
%v1094 = vor.u32 %v1093, %v1092
%v1095 = vxor.u32 %v1094, %v1090
%v1098 = vadd.s32 %v1095, %v1090
%v1100 = vshll.u32 %v1095, 16
%v1101 = vshrl.u32 %v1095, 16
%v1102 = vor.u32 %v1101, %v1100
%v1103 = vxor.u32 %v1102, %v1098
%v1106 = vadd.s32 %v1103, %v1098
%v1110 = vadd.s32 %v1106, %v9
%v1112 = vshll.u32 %v1103, 24
%v1113 = vshrl.u32 %v1103, 8
%v1114 = vor.u32 %v1113, %v1112
%v1115 = vxor.u32 %v1114, %v1106
%v1118 = vadd.s32 %v1115, %v8
%v1122 = vadd.s32 4, %v1118
%v1126 = vadd.s32 %v1122, %v1110
%v1128 = vshll.u32 %v1122, 13
%v1129 = vshrl.u32 %v1122, 19
%v1130 = vor.u32 %v1129, %v1128
%v1131 = vxor.u32 %v1130, %v1126
%v1134 = vadd.s32 %v1131, %v1126
%v1136 = vshll.u32 %v1131, 15
%v1137 = vshrl.u32 %v1131, 17
%v1138 = vor.u32 %v1137, %v1136
%v1139 = vxor.u32 %v1138, %v1134
%v1142 = vadd.s32 %v1139, %v1134
%v1144 = vshll.u32 %v1139, 26
%v1145 = vshrl.u32 %v1139, 6
%v1146 = vor.u32 %v1145, %v1144
%v1147 = vxor.u32 %v1146, %v1142
%v1150 = vadd.s32 %v1147, %v1142
%v1154 = vadd.s32 %v1150, %v8
%v1156 = vshll.u32 %v1147, 6
%v1157 = vshrl.u32 %v1147, 26
%v1158 = vor.u32 %v1157, %v1156
%v1159 = vxor.u32 %v1158, %v1150
%v1162 = vadd.s32 %v1159, %v10
%v1166 = vadd.s32 5, %v1162
%v1168 = vxor.u32 %v1166, %v1154
%v1169 = vand.u32.u8 255, %v1168
%v1170 = vand.u32 65535, %v1169
%v1171 = vshrl.u32 %v1170, 1
%v1172 = vor.u32 16256, %v1171
%v1173 = vand.u32.u16 65535, %v1172
%v119750 = vadd.low.f32.bf16 -1.0, %v1173
%v1182 = vmul.f32 2.0, %v119750
%v1186 = vadd.f32 -0.99609375, %v1182
%v1190 = vmax.f32 %v1186, -0.99609375
%v1192 = vand.u32 2147483647, %v1190
%vm1195 = vcmp.eq.f32.partialorder %v1192, 1.0
%v1200 = vmul.f32 inf, %v1190
%v1202 = vxor.u32 2147483648, %v1190
%v1205 = vmul.f32 %v1202, %v1190
%v1207 = vadd.f32 1.0, %v1205
%v1208 = vlog2.pop %v1207
%v1209 = vmul.f32 0.6931472, %v1208
%v1210 = vmul.f32 -0.5, %v1205
%v1211 = vadd.f32 1.0, %v1210
%v1212 = vmul.f32 %v1211, %v1205
%v1213 = vand.u32 2147483647, %v1205
%vm1214 = vcmp.lt.f32.partialorder %v1213, 0.0004427343
%v1215 = vsel /*vm=*/%vm1214, /*on_true_vy=*/%v1212, /*on_false_vx=*/%v1209
%v1216 = vxor.u32 2147483648, %v1215
%vm1219 = vcmp.lt.f32.partialorder %v1216, 5.0
%v1224 = vsel /*vm=*/%vm1219, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v1228 = vsel /*vm=*/%vm1219, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v1232 = vsel /*vm=*/%vm1219, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v1236 = vsel /*vm=*/%vm1219, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v1240 = vsel /*vm=*/%vm1219, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v1244 = vsel /*vm=*/%vm1219, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v1248 = vsel /*vm=*/%vm1219, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v1252 = vsel /*vm=*/%vm1219, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v1256 = vsel /*vm=*/%vm1219, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v1260 = vadd.f32 -2.5, %v1216
%v1262 = vrsqrt.pop %v1216
%v1263 = vmul.f32 %v1262, %v1216
%vm1264 = vcmp.eq.f32.partialorder %v1216, inf
%v1265 = vsel /*vm=*/%vm1264, /*on_true_vy=*/%v1216, /*on_false_vx=*/%v1263
%vm1266 = vcmp.eq.f32.partialorder %v1216, 0.0
%v1267 = vand.u32 2147483648, %v1216
%v1268 = vsel /*vm=*/%vm1266, /*on_true_vy=*/%v1267, /*on_false_vx=*/%v1265
%v1271 = vadd.f32 -3.0, %v1268
%v1275 = vsel /*vm=*/%vm1219, /*on_true_vy=*/%v1260, /*on_false_vx=*/%v1271
%v1279 = vmul.f32 %v1275, %v1256
%v1283 = vadd.f32 %v1279, %v1252
%v1287 = vmul.f32 %v1283, %v1275
%v1291 = vadd.f32 %v1287, %v1248
%v1295 = vmul.f32 %v1291, %v1275
%v1299 = vadd.f32 %v1295, %v1244
%v1303 = vmul.f32 %v1299, %v1275
%v1307 = vadd.f32 %v1303, %v1240
%v1311 = vmul.f32 %v1307, %v1275
%v1315 = vadd.f32 %v1311, %v1236
%v1319 = vmul.f32 %v1315, %v1275
%v1323 = vadd.f32 %v1319, %v1232
%v1327 = vmul.f32 %v1323, %v1275
%v1331 = vadd.f32 %v1327, %v1228
%v1335 = vmul.f32 %v1331, %v1275
%v1339 = vadd.f32 %v1335, %v1224
%v1343 = vmul.f32 %v1339, %v1190
%v1347 = vsel /*vm=*/%vm1195, /*on_true_vy=*/%v1200, /*on_false_vx=*/%v1343
%v1351 = vmul.f32 1.4140625, %v1347
%v1354 = vpack.c.bf16 %v120417, %v1351
%119751 = vst [vmem:[%s280 + $0x80] sm:$0xf] /*vst_source=*/%v1354
%v1384 = vadd.s32 %v1381, %v396
%v1394 = vadd.s32 %v1384, %v415
%vm1398 = vcmp.lt.u32.totalorder %v1394, %v1384
%vm1403 = vcmp.lt.u32.totalorder %v1384, %v1381
%v1408 = vadd.s32 %v1368, %v368
%v1412 = vadd.s32 1, %v1408
%v1416 = vsel /*vm=*/%vm1403, /*on_true_vy=*/%v1412, /*on_false_vx=*/%v1408
%v1420 = vadd.s32 1, %v1416
%v1424 = vsel /*vm=*/%vm1398, /*on_true_vy=*/%v1420, /*on_false_vx=*/%v1416
%v1429 = vadd.s32 %v1424, %v10
%v1433 = vadd.s32 %v1394, %v9
%v1437 = vadd.s32 %v1433, %v1429
%v1439 = vshll.u32 %v1433, 13
%v1440 = vshrl.u32 %v1433, 19
%v1441 = vor.u32 %v1440, %v1439
%v1442 = vxor.u32 %v1441, %v1437
%v1445 = vadd.s32 %v1442, %v1437
%v1447 = vshll.u32 %v1442, 15
%v1448 = vshrl.u32 %v1442, 17
%v1449 = vor.u32 %v1448, %v1447
%v1450 = vxor.u32 %v1449, %v1445
%v1453 = vadd.s32 %v1450, %v1445
%v1455 = vshll.u32 %v1450, 26
%v1456 = vshrl.u32 %v1450, 6
%v1457 = vor.u32 %v1456, %v1455
%v1458 = vxor.u32 %v1457, %v1453
%v1461 = vadd.s32 %v1458, %v1453
%v1465 = vadd.s32 %v1461, %v9
%v1467 = vshll.u32 %v1458, 6
%v1468 = vshrl.u32 %v1458, 26
%v1469 = vor.u32 %v1468, %v1467
%v1470 = vxor.u32 %v1469, %v1461
%v1473 = vadd.s32 %v1470, %v8
%v1477 = vadd.s32 1, %v1473
%v1481 = vadd.s32 %v1477, %v1465
%v1483 = vshll.u32 %v1477, 17
%v1484 = vshrl.u32 %v1477, 15
%v1485 = vor.u32 %v1484, %v1483
%v1486 = vxor.u32 %v1485, %v1481
%v1489 = vadd.s32 %v1486, %v1481
%v1491 = vshll.u32 %v1486, 29
%v1492 = vshrl.u32 %v1486, 3
%v1493 = vor.u32 %v1492, %v1491
%v1494 = vxor.u32 %v1493, %v1489
%v1497 = vadd.s32 %v1494, %v1489
%v1499 = vshll.u32 %v1494, 16
%v1500 = vshrl.u32 %v1494, 16
%v1501 = vor.u32 %v1500, %v1499
%v1502 = vxor.u32 %v1501, %v1497
%v1505 = vadd.s32 %v1502, %v1497
%v1509 = vadd.s32 %v1505, %v8
%v1511 = vshll.u32 %v1502, 24
%v1512 = vshrl.u32 %v1502, 8
%v1513 = vor.u32 %v1512, %v1511
%v1514 = vxor.u32 %v1513, %v1505
%v1517 = vadd.s32 %v1514, %v10
%v1521 = vadd.s32 2, %v1517
%v1525 = vadd.s32 %v1521, %v1509
%v1527 = vshll.u32 %v1521, 13
%v1528 = vshrl.u32 %v1521, 19
%v1529 = vor.u32 %v1528, %v1527
%v1530 = vxor.u32 %v1529, %v1525
%v1533 = vadd.s32 %v1530, %v1525
%v1535 = vshll.u32 %v1530, 15
%v1536 = vshrl.u32 %v1530, 17
%v1537 = vor.u32 %v1536, %v1535
%v1538 = vxor.u32 %v1537, %v1533
%v1541 = vadd.s32 %v1538, %v1533
%v1543 = vshll.u32 %v1538, 26
%v1544 = vshrl.u32 %v1538, 6
%v1545 = vor.u32 %v1544, %v1543
%v1546 = vxor.u32 %v1545, %v1541
%v1549 = vadd.s32 %v1546, %v1541
%v1553 = vadd.s32 %v1549, %v10
%v1555 = vshll.u32 %v1546, 6
%v1556 = vshrl.u32 %v1546, 26
%v1557 = vor.u32 %v1556, %v1555
%v1558 = vxor.u32 %v1557, %v1549
%v1561 = vadd.s32 %v1558, %v9
%v1565 = vadd.s32 3, %v1561
%v1569 = vadd.s32 %v1565, %v1553
%v1571 = vshll.u32 %v1565, 17
%v1572 = vshrl.u32 %v1565, 15
%v1573 = vor.u32 %v1572, %v1571
%v1574 = vxor.u32 %v1573, %v1569
%v1577 = vadd.s32 %v1574, %v1569
%v1579 = vshll.u32 %v1574, 29
%v1580 = vshrl.u32 %v1574, 3
%v1581 = vor.u32 %v1580, %v1579
%v1582 = vxor.u32 %v1581, %v1577
%v1585 = vadd.s32 %v1582, %v1577
%v1587 = vshll.u32 %v1582, 16
%v1588 = vshrl.u32 %v1582, 16
%v1589 = vor.u32 %v1588, %v1587
%v1590 = vxor.u32 %v1589, %v1585
%v1593 = vadd.s32 %v1590, %v1585
%v1597 = vadd.s32 %v1593, %v9
%v1599 = vshll.u32 %v1590, 24
%v1600 = vshrl.u32 %v1590, 8
%v1601 = vor.u32 %v1600, %v1599
%v1602 = vxor.u32 %v1601, %v1593
%v1605 = vadd.s32 %v1602, %v8
%v1609 = vadd.s32 4, %v1605
%v1613 = vadd.s32 %v1609, %v1597
%v1615 = vshll.u32 %v1609, 13
%v1616 = vshrl.u32 %v1609, 19
%v1617 = vor.u32 %v1616, %v1615
%v1618 = vxor.u32 %v1617, %v1613
%v1621 = vadd.s32 %v1618, %v1613
%v1623 = vshll.u32 %v1618, 15
%v1624 = vshrl.u32 %v1618, 17
%v1625 = vor.u32 %v1624, %v1623
%v1626 = vxor.u32 %v1625, %v1621
%v1629 = vadd.s32 %v1626, %v1621
%v1631 = vshll.u32 %v1626, 26
%v1632 = vshrl.u32 %v1626, 6
%v1633 = vor.u32 %v1632, %v1631
%v1634 = vxor.u32 %v1633, %v1629
%v1637 = vadd.s32 %v1634, %v1629
%v1641 = vadd.s32 %v1637, %v8
%v1643 = vshll.u32 %v1634, 6
%v1644 = vshrl.u32 %v1634, 26
%v1645 = vor.u32 %v1644, %v1643
%v1646 = vxor.u32 %v1645, %v1637
%v1649 = vadd.s32 %v1646, %v10
%v1653 = vadd.s32 5, %v1649
%v1655 = vxor.u32 %v1653, %v1641
%v1656 = vand.u32.u8 255, %v1655
%v1657 = vand.u32 65535, %v1656
%v1658 = vshrl.u32 %v1657, 1
%v1659 = vor.u32 16256, %v1658
%v1660 = vand.u32.u16 65535, %v1659
%v119752 = vadd.low.f32.bf16 -1.0, %v1660
%v1669 = vmul.f32 2.0, %v119752
%v1673 = vadd.f32 -0.99609375, %v1669
%v1677 = vmax.f32 %v1673, -0.99609375
%v1679 = vand.u32 2147483647, %v1677
%vm1682 = vcmp.eq.f32.partialorder %v1679, 1.0
%v1687 = vmul.f32 inf, %v1677
%v1689 = vxor.u32 2147483648, %v1677
%v1692 = vmul.f32 %v1689, %v1677
%v1694 = vadd.f32 1.0, %v1692
%v1695 = vlog2.pop %v1694
%v1696 = vmul.f32 0.6931472, %v1695
%v1697 = vmul.f32 -0.5, %v1692
%v1698 = vadd.f32 1.0, %v1697
%v1699 = vmul.f32 %v1698, %v1692
%v1700 = vand.u32 2147483647, %v1692
%vm1701 = vcmp.lt.f32.partialorder %v1700, 0.0004427343
%v1702 = vsel /*vm=*/%vm1701, /*on_true_vy=*/%v1699, /*on_false_vx=*/%v1696
%v1703 = vxor.u32 2147483648, %v1702
%vm1706 = vcmp.lt.f32.partialorder %v1703, 5.0
%v1711 = vsel /*vm=*/%vm1706, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v1715 = vsel /*vm=*/%vm1706, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v1719 = vsel /*vm=*/%vm1706, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v1723 = vsel /*vm=*/%vm1706, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v1727 = vsel /*vm=*/%vm1706, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v1731 = vsel /*vm=*/%vm1706, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v1735 = vsel /*vm=*/%vm1706, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v1739 = vsel /*vm=*/%vm1706, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v1743 = vsel /*vm=*/%vm1706, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v1747 = vadd.f32 -2.5, %v1703
%v1749 = vrsqrt.pop %v1703
%v1750 = vmul.f32 %v1749, %v1703
%vm1751 = vcmp.eq.f32.partialorder %v1703, inf
%v1752 = vsel /*vm=*/%vm1751, /*on_true_vy=*/%v1703, /*on_false_vx=*/%v1750
%vm1753 = vcmp.eq.f32.partialorder %v1703, 0.0
%v1754 = vand.u32 2147483648, %v1703
%v1755 = vsel /*vm=*/%vm1753, /*on_true_vy=*/%v1754, /*on_false_vx=*/%v1752
%v1758 = vadd.f32 -3.0, %v1755
%v1762 = vsel /*vm=*/%vm1706, /*on_true_vy=*/%v1747, /*on_false_vx=*/%v1758
%v1766 = vmul.f32 %v1762, %v1743
%v1770 = vadd.f32 %v1766, %v1739
%v1774 = vmul.f32 %v1770, %v1762
%v1778 = vadd.f32 %v1774, %v1735
%v1782 = vmul.f32 %v1778, %v1762
%v1786 = vadd.f32 %v1782, %v1731
%v1790 = vmul.f32 %v1786, %v1762
%v1794 = vadd.f32 %v1790, %v1727
%v1798 = vmul.f32 %v1794, %v1762
%v1802 = vadd.f32 %v1798, %v1723
%v1806 = vmul.f32 %v1802, %v1762
%v1810 = vadd.f32 %v1806, %v1719
%v1814 = vmul.f32 %v1810, %v1762
%v1818 = vadd.f32 %v1814, %v1715
%v1822 = vmul.f32 %v1818, %v1762
%v1826 = vadd.f32 %v1822, %v1711
%v1830 = vmul.f32 %v1826, %v1677
%v1834 = vsel /*vm=*/%vm1682, /*on_true_vy=*/%v1687, /*on_false_vx=*/%v1830
%v1838 = vmul.f32 1.4140625, %v1834
%v1841 = vpack.c.bf16 %v120417, %v1838
%119753 = vst [vmem:[%s280 + $0x100] sm:$0xf] /*vst_source=*/%v1841
%v1871 = vadd.s32 %v1868, %v396
%v1881 = vadd.s32 %v1871, %v415
%vm1885 = vcmp.lt.u32.totalorder %v1881, %v1871
%vm1890 = vcmp.lt.u32.totalorder %v1871, %v1868
%v1895 = vadd.s32 %v1855, %v368
%v1899 = vadd.s32 1, %v1895
%v1903 = vsel /*vm=*/%vm1890, /*on_true_vy=*/%v1899, /*on_false_vx=*/%v1895
%v1907 = vadd.s32 1, %v1903
%v1911 = vsel /*vm=*/%vm1885, /*on_true_vy=*/%v1907, /*on_false_vx=*/%v1903
%v1916 = vadd.s32 %v1911, %v10
%v1920 = vadd.s32 %v1881, %v9
%v1924 = vadd.s32 %v1920, %v1916
%v1926 = vshll.u32 %v1920, 13
%v1927 = vshrl.u32 %v1920, 19
%v1928 = vor.u32 %v1927, %v1926
%v1929 = vxor.u32 %v1928, %v1924
%v1932 = vadd.s32 %v1929, %v1924
%v1934 = vshll.u32 %v1929, 15
%v1935 = vshrl.u32 %v1929, 17
%v1936 = vor.u32 %v1935, %v1934
%v1937 = vxor.u32 %v1936, %v1932
%v1940 = vadd.s32 %v1937, %v1932
%v1942 = vshll.u32 %v1937, 26
%v1943 = vshrl.u32 %v1937, 6
%v1944 = vor.u32 %v1943, %v1942
%v1945 = vxor.u32 %v1944, %v1940
%v1948 = vadd.s32 %v1945, %v1940
%v1952 = vadd.s32 %v1948, %v9
%v1954 = vshll.u32 %v1945, 6
%v1955 = vshrl.u32 %v1945, 26
%v1956 = vor.u32 %v1955, %v1954
%v1957 = vxor.u32 %v1956, %v1948
%v1960 = vadd.s32 %v1957, %v8
%v1964 = vadd.s32 1, %v1960
%v1968 = vadd.s32 %v1964, %v1952
%v1970 = vshll.u32 %v1964, 17
%v1971 = vshrl.u32 %v1964, 15
%v1972 = vor.u32 %v1971, %v1970
%v1973 = vxor.u32 %v1972, %v1968
%v1976 = vadd.s32 %v1973, %v1968
%v1978 = vshll.u32 %v1973, 29
%v1979 = vshrl.u32 %v1973, 3
%v1980 = vor.u32 %v1979, %v1978
%v1981 = vxor.u32 %v1980, %v1976
%v1984 = vadd.s32 %v1981, %v1976
%v1986 = vshll.u32 %v1981, 16
%v1987 = vshrl.u32 %v1981, 16
%v1988 = vor.u32 %v1987, %v1986
%v1989 = vxor.u32 %v1988, %v1984
%v1992 = vadd.s32 %v1989, %v1984
%v1996 = vadd.s32 %v1992, %v8
%v1998 = vshll.u32 %v1989, 24
%v1999 = vshrl.u32 %v1989, 8
%v2000 = vor.u32 %v1999, %v1998
%v2001 = vxor.u32 %v2000, %v1992
%v2004 = vadd.s32 %v2001, %v10
%v2008 = vadd.s32 2, %v2004
%v2012 = vadd.s32 %v2008, %v1996
%v2014 = vshll.u32 %v2008, 13
%v2015 = vshrl.u32 %v2008, 19
%v2016 = vor.u32 %v2015, %v2014
%v2017 = vxor.u32 %v2016, %v2012
%v2020 = vadd.s32 %v2017, %v2012
%v2022 = vshll.u32 %v2017, 15
%v2023 = vshrl.u32 %v2017, 17
%v2024 = vor.u32 %v2023, %v2022
%v2025 = vxor.u32 %v2024, %v2020
%v2028 = vadd.s32 %v2025, %v2020
%v2030 = vshll.u32 %v2025, 26
%v2031 = vshrl.u32 %v2025, 6
%v2032 = vor.u32 %v2031, %v2030
%v2033 = vxor.u32 %v2032, %v2028
%v2036 = vadd.s32 %v2033, %v2028
%v2040 = vadd.s32 %v2036, %v10
%v2042 = vshll.u32 %v2033, 6
%v2043 = vshrl.u32 %v2033, 26
%v2044 = vor.u32 %v2043, %v2042
%v2045 = vxor.u32 %v2044, %v2036
%v2048 = vadd.s32 %v2045, %v9
%v2052 = vadd.s32 3, %v2048
%v2056 = vadd.s32 %v2052, %v2040
%v2058 = vshll.u32 %v2052, 17
%v2059 = vshrl.u32 %v2052, 15
%v2060 = vor.u32 %v2059, %v2058
%v2061 = vxor.u32 %v2060, %v2056
%v2064 = vadd.s32 %v2061, %v2056
%v2066 = vshll.u32 %v2061, 29
%v2067 = vshrl.u32 %v2061, 3
%v2068 = vor.u32 %v2067, %v2066
%v2069 = vxor.u32 %v2068, %v2064
%v2072 = vadd.s32 %v2069, %v2064
%v2074 = vshll.u32 %v2069, 16
%v2075 = vshrl.u32 %v2069, 16
%v2076 = vor.u32 %v2075, %v2074
%v2077 = vxor.u32 %v2076, %v2072
%v2080 = vadd.s32 %v2077, %v2072
%v2084 = vadd.s32 %v2080, %v9
%v2086 = vshll.u32 %v2077, 24
%v2087 = vshrl.u32 %v2077, 8
%v2088 = vor.u32 %v2087, %v2086
%v2089 = vxor.u32 %v2088, %v2080
%v2092 = vadd.s32 %v2089, %v8
%v2096 = vadd.s32 4, %v2092
%v2100 = vadd.s32 %v2096, %v2084
%v2102 = vshll.u32 %v2096, 13
%v2103 = vshrl.u32 %v2096, 19
%v2104 = vor.u32 %v2103, %v2102
%v2105 = vxor.u32 %v2104, %v2100
%v2108 = vadd.s32 %v2105, %v2100
%v2110 = vshll.u32 %v2105, 15
%v2111 = vshrl.u32 %v2105, 17
%v2112 = vor.u32 %v2111, %v2110
%v2113 = vxor.u32 %v2112, %v2108
%v2116 = vadd.s32 %v2113, %v2108
%v2118 = vshll.u32 %v2113, 26
%v2119 = vshrl.u32 %v2113, 6
%v2120 = vor.u32 %v2119, %v2118
%v2121 = vxor.u32 %v2120, %v2116
%v2124 = vadd.s32 %v2121, %v2116
%v2128 = vadd.s32 %v2124, %v8
%v2130 = vshll.u32 %v2121, 6
%v2131 = vshrl.u32 %v2121, 26
%v2132 = vor.u32 %v2131, %v2130
%v2133 = vxor.u32 %v2132, %v2124
%v2136 = vadd.s32 %v2133, %v10
%v2140 = vadd.s32 5, %v2136
%v2142 = vxor.u32 %v2140, %v2128
%v2143 = vand.u32.u8 255, %v2142
%v2144 = vand.u32 65535, %v2143
%v2145 = vshrl.u32 %v2144, 1
%v2146 = vor.u32 16256, %v2145
%v2147 = vand.u32.u16 65535, %v2146
%v119754 = vadd.low.f32.bf16 -1.0, %v2147
%v2156 = vmul.f32 2.0, %v119754
%v2160 = vadd.f32 -0.99609375, %v2156
%v2164 = vmax.f32 %v2160, -0.99609375
%v2166 = vand.u32 2147483647, %v2164
%vm2169 = vcmp.eq.f32.partialorder %v2166, 1.0
%v2174 = vmul.f32 inf, %v2164
%v2176 = vxor.u32 2147483648, %v2164
%v2179 = vmul.f32 %v2176, %v2164
%v2181 = vadd.f32 1.0, %v2179
%v2182 = vlog2.pop %v2181
%v2183 = vmul.f32 0.6931472, %v2182
%v2184 = vmul.f32 -0.5, %v2179
%v2185 = vadd.f32 1.0, %v2184
%v2186 = vmul.f32 %v2185, %v2179
%v2187 = vand.u32 2147483647, %v2179
%vm2188 = vcmp.lt.f32.partialorder %v2187, 0.0004427343
%v2189 = vsel /*vm=*/%vm2188, /*on_true_vy=*/%v2186, /*on_false_vx=*/%v2183
%v2190 = vxor.u32 2147483648, %v2189
%vm2193 = vcmp.lt.f32.partialorder %v2190, 5.0
%v2198 = vsel /*vm=*/%vm2193, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v2202 = vsel /*vm=*/%vm2193, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v2206 = vsel /*vm=*/%vm2193, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v2210 = vsel /*vm=*/%vm2193, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v2214 = vsel /*vm=*/%vm2193, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v2218 = vsel /*vm=*/%vm2193, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v2222 = vsel /*vm=*/%vm2193, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v2226 = vsel /*vm=*/%vm2193, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v2230 = vsel /*vm=*/%vm2193, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v2234 = vadd.f32 -2.5, %v2190
%v2236 = vrsqrt.pop %v2190
%v2237 = vmul.f32 %v2236, %v2190
%vm2238 = vcmp.eq.f32.partialorder %v2190, inf
%v2239 = vsel /*vm=*/%vm2238, /*on_true_vy=*/%v2190, /*on_false_vx=*/%v2237
%vm2240 = vcmp.eq.f32.partialorder %v2190, 0.0
%v2241 = vand.u32 2147483648, %v2190
%v2242 = vsel /*vm=*/%vm2240, /*on_true_vy=*/%v2241, /*on_false_vx=*/%v2239
%v2245 = vadd.f32 -3.0, %v2242
%v2249 = vsel /*vm=*/%vm2193, /*on_true_vy=*/%v2234, /*on_false_vx=*/%v2245
%v2253 = vmul.f32 %v2249, %v2230
%v2257 = vadd.f32 %v2253, %v2226
%v2261 = vmul.f32 %v2257, %v2249
%v2265 = vadd.f32 %v2261, %v2222
%v2269 = vmul.f32 %v2265, %v2249
%v2273 = vadd.f32 %v2269, %v2218
%v2277 = vmul.f32 %v2273, %v2249
%v2281 = vadd.f32 %v2277, %v2214
%v2285 = vmul.f32 %v2281, %v2249
%v2289 = vadd.f32 %v2285, %v2210
%v2293 = vmul.f32 %v2289, %v2249
%v2297 = vadd.f32 %v2293, %v2206
%v2301 = vmul.f32 %v2297, %v2249
%v2305 = vadd.f32 %v2301, %v2202
%v2309 = vmul.f32 %v2305, %v2249
%v2313 = vadd.f32 %v2309, %v2198
%v2317 = vmul.f32 %v2313, %v2164
%v2321 = vsel /*vm=*/%vm2169, /*on_true_vy=*/%v2174, /*on_false_vx=*/%v2317
%v2325 = vmul.f32 1.4140625, %v2321
%v2328 = vpack.c.bf16 %v120417, %v2325
%119755 = vst [vmem:[%s280 + $0x180] sm:$0xf] /*vst_source=*/%v2328
%v2358 = vadd.s32 %v2355, %v396
%v2368 = vadd.s32 %v2358, %v415
%vm2372 = vcmp.lt.u32.totalorder %v2368, %v2358
%vm2377 = vcmp.lt.u32.totalorder %v2358, %v2355
%v2382 = vadd.s32 %v2342, %v368
%v2386 = vadd.s32 1, %v2382
%v2390 = vsel /*vm=*/%vm2377, /*on_true_vy=*/%v2386, /*on_false_vx=*/%v2382
%v2394 = vadd.s32 1, %v2390
%v2398 = vsel /*vm=*/%vm2372, /*on_true_vy=*/%v2394, /*on_false_vx=*/%v2390
%v2403 = vadd.s32 %v2398, %v10
%v2407 = vadd.s32 %v2368, %v9
%v2411 = vadd.s32 %v2407, %v2403
%v2413 = vshll.u32 %v2407, 13
%v2414 = vshrl.u32 %v2407, 19
%v2415 = vor.u32 %v2414, %v2413
%v2416 = vxor.u32 %v2415, %v2411
%v2419 = vadd.s32 %v2416, %v2411
%v2421 = vshll.u32 %v2416, 15
%v2422 = vshrl.u32 %v2416, 17
%v2423 = vor.u32 %v2422, %v2421
%v2424 = vxor.u32 %v2423, %v2419
%v2427 = vadd.s32 %v2424, %v2419
%v2429 = vshll.u32 %v2424, 26
%v2430 = vshrl.u32 %v2424, 6
%v2431 = vor.u32 %v2430, %v2429
%v2432 = vxor.u32 %v2431, %v2427
%v2435 = vadd.s32 %v2432, %v2427
%v2439 = vadd.s32 %v2435, %v9
%v2441 = vshll.u32 %v2432, 6
%v2442 = vshrl.u32 %v2432, 26
%v2443 = vor.u32 %v2442, %v2441
%v2444 = vxor.u32 %v2443, %v2435
%v2447 = vadd.s32 %v2444, %v8
%v2451 = vadd.s32 1, %v2447
%v2455 = vadd.s32 %v2451, %v2439
%v2457 = vshll.u32 %v2451, 17
%v2458 = vshrl.u32 %v2451, 15
%v2459 = vor.u32 %v2458, %v2457
%v2460 = vxor.u32 %v2459, %v2455
%v2463 = vadd.s32 %v2460, %v2455
%v2465 = vshll.u32 %v2460, 29
%v2466 = vshrl.u32 %v2460, 3
%v2467 = vor.u32 %v2466, %v2465
%v2468 = vxor.u32 %v2467, %v2463
%v2471 = vadd.s32 %v2468, %v2463
%v2473 = vshll.u32 %v2468, 16
%v2474 = vshrl.u32 %v2468, 16
%v2475 = vor.u32 %v2474, %v2473
%v2476 = vxor.u32 %v2475, %v2471
%v2479 = vadd.s32 %v2476, %v2471
%v2483 = vadd.s32 %v2479, %v8
%v2485 = vshll.u32 %v2476, 24
%v2486 = vshrl.u32 %v2476, 8
%v2487 = vor.u32 %v2486, %v2485
%v2488 = vxor.u32 %v2487, %v2479
%v2491 = vadd.s32 %v2488, %v10
%v2495 = vadd.s32 2, %v2491
%v2499 = vadd.s32 %v2495, %v2483
%v2501 = vshll.u32 %v2495, 13
%v2502 = vshrl.u32 %v2495, 19
%v2503 = vor.u32 %v2502, %v2501
%v2504 = vxor.u32 %v2503, %v2499
%v2507 = vadd.s32 %v2504, %v2499
%v2509 = vshll.u32 %v2504, 15
%v2510 = vshrl.u32 %v2504, 17
%v2511 = vor.u32 %v2510, %v2509
%v2512 = vxor.u32 %v2511, %v2507
%v2515 = vadd.s32 %v2512, %v2507
%v2517 = vshll.u32 %v2512, 26
%v2518 = vshrl.u32 %v2512, 6
%v2519 = vor.u32 %v2518, %v2517
%v2520 = vxor.u32 %v2519, %v2515
%v2523 = vadd.s32 %v2520, %v2515
%v2527 = vadd.s32 %v2523, %v10
%v2529 = vshll.u32 %v2520, 6
%v2530 = vshrl.u32 %v2520, 26
%v2531 = vor.u32 %v2530, %v2529
%v2532 = vxor.u32 %v2531, %v2523
%v2535 = vadd.s32 %v2532, %v9
%v2539 = vadd.s32 3, %v2535
%v2543 = vadd.s32 %v2539, %v2527
%v2545 = vshll.u32 %v2539, 17
%v2546 = vshrl.u32 %v2539, 15
%v2547 = vor.u32 %v2546, %v2545
%v2548 = vxor.u32 %v2547, %v2543
%v2551 = vadd.s32 %v2548, %v2543
%v2553 = vshll.u32 %v2548, 29
%v2554 = vshrl.u32 %v2548, 3
%v2555 = vor.u32 %v2554, %v2553
%v2556 = vxor.u32 %v2555, %v2551
%v2559 = vadd.s32 %v2556, %v2551
%v2561 = vshll.u32 %v2556, 16
%v2562 = vshrl.u32 %v2556, 16
%v2563 = vor.u32 %v2562, %v2561
%v2564 = vxor.u32 %v2563, %v2559
%v2567 = vadd.s32 %v2564, %v2559
%v2571 = vadd.s32 %v2567, %v9
%v2573 = vshll.u32 %v2564, 24
%v2574 = vshrl.u32 %v2564, 8
%v2575 = vor.u32 %v2574, %v2573
%v2576 = vxor.u32 %v2575, %v2567
%v2579 = vadd.s32 %v2576, %v8
%v2583 = vadd.s32 4, %v2579
%v2587 = vadd.s32 %v2583, %v2571
%v2589 = vshll.u32 %v2583, 13
%v2590 = vshrl.u32 %v2583, 19
%v2591 = vor.u32 %v2590, %v2589
%v2592 = vxor.u32 %v2591, %v2587
%v2595 = vadd.s32 %v2592, %v2587
%v2597 = vshll.u32 %v2592, 15
%v2598 = vshrl.u32 %v2592, 17
%v2599 = vor.u32 %v2598, %v2597
%v2600 = vxor.u32 %v2599, %v2595
%v2603 = vadd.s32 %v2600, %v2595
%v2605 = vshll.u32 %v2600, 26
%v2606 = vshrl.u32 %v2600, 6
%v2607 = vor.u32 %v2606, %v2605
%v2608 = vxor.u32 %v2607, %v2603
%v2611 = vadd.s32 %v2608, %v2603
%v2615 = vadd.s32 %v2611, %v8
%v2617 = vshll.u32 %v2608, 6
%v2618 = vshrl.u32 %v2608, 26
%v2619 = vor.u32 %v2618, %v2617
%v2620 = vxor.u32 %v2619, %v2611
%v2623 = vadd.s32 %v2620, %v10
%v2627 = vadd.s32 5, %v2623
%v2629 = vxor.u32 %v2627, %v2615
%v2630 = vand.u32.u8 255, %v2629
%v2631 = vand.u32 65535, %v2630
%v2632 = vshrl.u32 %v2631, 1
%v2633 = vor.u32 16256, %v2632
%v2634 = vand.u32.u16 65535, %v2633
%v119756 = vadd.low.f32.bf16 -1.0, %v2634
%v2643 = vmul.f32 2.0, %v119756
%v2647 = vadd.f32 -0.99609375, %v2643
%v2651 = vmax.f32 %v2647, -0.99609375
%v2653 = vand.u32 2147483647, %v2651
%vm2656 = vcmp.eq.f32.partialorder %v2653, 1.0
%v2661 = vmul.f32 inf, %v2651
%v2663 = vxor.u32 2147483648, %v2651
%v2666 = vmul.f32 %v2663, %v2651
%v2668 = vadd.f32 1.0, %v2666
%v2669 = vlog2.pop %v2668
%v2670 = vmul.f32 0.6931472, %v2669
%v2671 = vmul.f32 -0.5, %v2666
%v2672 = vadd.f32 1.0, %v2671
%v2673 = vmul.f32 %v2672, %v2666
%v2674 = vand.u32 2147483647, %v2666
%vm2675 = vcmp.lt.f32.partialorder %v2674, 0.0004427343
%v2676 = vsel /*vm=*/%vm2675, /*on_true_vy=*/%v2673, /*on_false_vx=*/%v2670
%v2677 = vxor.u32 2147483648, %v2676
%vm2680 = vcmp.lt.f32.partialorder %v2677, 5.0
%v2685 = vsel /*vm=*/%vm2680, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v2689 = vsel /*vm=*/%vm2680, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v2693 = vsel /*vm=*/%vm2680, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v2697 = vsel /*vm=*/%vm2680, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v2701 = vsel /*vm=*/%vm2680, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v2705 = vsel /*vm=*/%vm2680, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v2709 = vsel /*vm=*/%vm2680, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v2713 = vsel /*vm=*/%vm2680, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v2717 = vsel /*vm=*/%vm2680, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v2721 = vadd.f32 -2.5, %v2677
%v2723 = vrsqrt.pop %v2677
%v2724 = vmul.f32 %v2723, %v2677
%vm2725 = vcmp.eq.f32.partialorder %v2677, inf
%v2726 = vsel /*vm=*/%vm2725, /*on_true_vy=*/%v2677, /*on_false_vx=*/%v2724
%vm2727 = vcmp.eq.f32.partialorder %v2677, 0.0
%v2728 = vand.u32 2147483648, %v2677
%v2729 = vsel /*vm=*/%vm2727, /*on_true_vy=*/%v2728, /*on_false_vx=*/%v2726
%v2732 = vadd.f32 -3.0, %v2729
%v2736 = vsel /*vm=*/%vm2680, /*on_true_vy=*/%v2721, /*on_false_vx=*/%v2732
%v2740 = vmul.f32 %v2736, %v2717
%v2744 = vadd.f32 %v2740, %v2713
%v2748 = vmul.f32 %v2744, %v2736
%v2752 = vadd.f32 %v2748, %v2709
%v2756 = vmul.f32 %v2752, %v2736
%v2760 = vadd.f32 %v2756, %v2705
%v2764 = vmul.f32 %v2760, %v2736
%v2768 = vadd.f32 %v2764, %v2701
%v2772 = vmul.f32 %v2768, %v2736
%v2776 = vadd.f32 %v2772, %v2697
%v2780 = vmul.f32 %v2776, %v2736
%v2784 = vadd.f32 %v2780, %v2693
%v2788 = vmul.f32 %v2784, %v2736
%v2792 = vadd.f32 %v2788, %v2689
%v2796 = vmul.f32 %v2792, %v2736
%v2800 = vadd.f32 %v2796, %v2685
%v2804 = vmul.f32 %v2800, %v2651
%v2808 = vsel /*vm=*/%vm2656, /*on_true_vy=*/%v2661, /*on_false_vx=*/%v2804
%v2812 = vmul.f32 1.4140625, %v2808
%v2815 = vpack.c.bf16 %v120417, %v2812
%119757 = vst [vmem:[%s280 + $0x200] sm:$0xf] /*vst_source=*/%v2815
%v2845 = vadd.s32 %v2842, %v396
%v2855 = vadd.s32 %v2845, %v415
%vm2859 = vcmp.lt.u32.totalorder %v2855, %v2845
%vm2864 = vcmp.lt.u32.totalorder %v2845, %v2842
%v2869 = vadd.s32 %v2829, %v368
%v2873 = vadd.s32 1, %v2869
%v2877 = vsel /*vm=*/%vm2864, /*on_true_vy=*/%v2873, /*on_false_vx=*/%v2869
%v2881 = vadd.s32 1, %v2877
%v2885 = vsel /*vm=*/%vm2859, /*on_true_vy=*/%v2881, /*on_false_vx=*/%v2877
%v2890 = vadd.s32 %v2885, %v10
%v2894 = vadd.s32 %v2855, %v9
%v2898 = vadd.s32 %v2894, %v2890
%v2900 = vshll.u32 %v2894, 13
%v2901 = vshrl.u32 %v2894, 19
%v2902 = vor.u32 %v2901, %v2900
%v2903 = vxor.u32 %v2902, %v2898
%v2906 = vadd.s32 %v2903, %v2898
%v2908 = vshll.u32 %v2903, 15
%v2909 = vshrl.u32 %v2903, 17
%v2910 = vor.u32 %v2909, %v2908
%v2911 = vxor.u32 %v2910, %v2906
%v2914 = vadd.s32 %v2911, %v2906
%v2916 = vshll.u32 %v2911, 26
%v2917 = vshrl.u32 %v2911, 6
%v2918 = vor.u32 %v2917, %v2916
%v2919 = vxor.u32 %v2918, %v2914
%v2922 = vadd.s32 %v2919, %v2914
%v2926 = vadd.s32 %v2922, %v9
%v2928 = vshll.u32 %v2919, 6
%v2929 = vshrl.u32 %v2919, 26
%v2930 = vor.u32 %v2929, %v2928
%v2931 = vxor.u32 %v2930, %v2922
%v2934 = vadd.s32 %v2931, %v8
%v2938 = vadd.s32 1, %v2934
%v2942 = vadd.s32 %v2938, %v2926
%v2944 = vshll.u32 %v2938, 17
%v2945 = vshrl.u32 %v2938, 15
%v2946 = vor.u32 %v2945, %v2944
%v2947 = vxor.u32 %v2946, %v2942
%v2950 = vadd.s32 %v2947, %v2942
%v2952 = vshll.u32 %v2947, 29
%v2953 = vshrl.u32 %v2947, 3
%v2954 = vor.u32 %v2953, %v2952
%v2955 = vxor.u32 %v2954, %v2950
%v2958 = vadd.s32 %v2955, %v2950
%v2960 = vshll.u32 %v2955, 16
%v2961 = vshrl.u32 %v2955, 16
%v2962 = vor.u32 %v2961, %v2960
%v2963 = vxor.u32 %v2962, %v2958
%v2966 = vadd.s32 %v2963, %v2958
%v2970 = vadd.s32 %v2966, %v8
%v2972 = vshll.u32 %v2963, 24
%v2973 = vshrl.u32 %v2963, 8
%v2974 = vor.u32 %v2973, %v2972
%v2975 = vxor.u32 %v2974, %v2966
%v2978 = vadd.s32 %v2975, %v10
%v2982 = vadd.s32 2, %v2978
%v2986 = vadd.s32 %v2982, %v2970
%v2988 = vshll.u32 %v2982, 13
%v2989 = vshrl.u32 %v2982, 19
%v2990 = vor.u32 %v2989, %v2988
%v2991 = vxor.u32 %v2990, %v2986
%v2994 = vadd.s32 %v2991, %v2986
%v2996 = vshll.u32 %v2991, 15
%v2997 = vshrl.u32 %v2991, 17
%v2998 = vor.u32 %v2997, %v2996
%v2999 = vxor.u32 %v2998, %v2994
%v3002 = vadd.s32 %v2999, %v2994
%v3004 = vshll.u32 %v2999, 26
%v3005 = vshrl.u32 %v2999, 6
%v3006 = vor.u32 %v3005, %v3004
%v3007 = vxor.u32 %v3006, %v3002
%v3010 = vadd.s32 %v3007, %v3002
%v3014 = vadd.s32 %v3010, %v10
%v3016 = vshll.u32 %v3007, 6
%v3017 = vshrl.u32 %v3007, 26
%v3018 = vor.u32 %v3017, %v3016
%v3019 = vxor.u32 %v3018, %v3010
%v3022 = vadd.s32 %v3019, %v9
%v3026 = vadd.s32 3, %v3022
%v3030 = vadd.s32 %v3026, %v3014
%v3032 = vshll.u32 %v3026, 17
%v3033 = vshrl.u32 %v3026, 15
%v3034 = vor.u32 %v3033, %v3032
%v3035 = vxor.u32 %v3034, %v3030
%v3038 = vadd.s32 %v3035, %v3030
%v3040 = vshll.u32 %v3035, 29
%v3041 = vshrl.u32 %v3035, 3
%v3042 = vor.u32 %v3041, %v3040
%v3043 = vxor.u32 %v3042, %v3038
%v3046 = vadd.s32 %v3043, %v3038
%v3048 = vshll.u32 %v3043, 16
%v3049 = vshrl.u32 %v3043, 16
%v3050 = vor.u32 %v3049, %v3048
%v3051 = vxor.u32 %v3050, %v3046
%v3054 = vadd.s32 %v3051, %v3046
%v3058 = vadd.s32 %v3054, %v9
%v3060 = vshll.u32 %v3051, 24
%v3061 = vshrl.u32 %v3051, 8
%v3062 = vor.u32 %v3061, %v3060
%v3063 = vxor.u32 %v3062, %v3054
%v3066 = vadd.s32 %v3063, %v8
%v3070 = vadd.s32 4, %v3066
%v3074 = vadd.s32 %v3070, %v3058
%v3076 = vshll.u32 %v3070, 13
%v3077 = vshrl.u32 %v3070, 19
%v3078 = vor.u32 %v3077, %v3076
%v3079 = vxor.u32 %v3078, %v3074
%v3082 = vadd.s32 %v3079, %v3074
%v3084 = vshll.u32 %v3079, 15
%v3085 = vshrl.u32 %v3079, 17
%v3086 = vor.u32 %v3085, %v3084
%v3087 = vxor.u32 %v3086, %v3082
%v3090 = vadd.s32 %v3087, %v3082
%v3092 = vshll.u32 %v3087, 26
%v3093 = vshrl.u32 %v3087, 6
%v3094 = vor.u32 %v3093, %v3092
%v3095 = vxor.u32 %v3094, %v3090
%v3098 = vadd.s32 %v3095, %v3090
%v3102 = vadd.s32 %v3098, %v8
%v3104 = vshll.u32 %v3095, 6
%v3105 = vshrl.u32 %v3095, 26
%v3106 = vor.u32 %v3105, %v3104
%v3107 = vxor.u32 %v3106, %v3098
%v3110 = vadd.s32 %v3107, %v10
%v3114 = vadd.s32 5, %v3110
%v3116 = vxor.u32 %v3114, %v3102
%v3117 = vand.u32.u8 255, %v3116
%v3118 = vand.u32 65535, %v3117
%v3119 = vshrl.u32 %v3118, 1
%v3120 = vor.u32 16256, %v3119
%v3121 = vand.u32.u16 65535, %v3120
%v119758 = vadd.low.f32.bf16 -1.0, %v3121
%v3130 = vmul.f32 2.0, %v119758
%v3134 = vadd.f32 -0.99609375, %v3130
%v3138 = vmax.f32 %v3134, -0.99609375
%v3140 = vand.u32 2147483647, %v3138
%vm3143 = vcmp.eq.f32.partialorder %v3140, 1.0
%v3148 = vmul.f32 inf, %v3138
%v3150 = vxor.u32 2147483648, %v3138
%v3153 = vmul.f32 %v3150, %v3138
%v3155 = vadd.f32 1.0, %v3153
%v3156 = vlog2.pop %v3155
%v3157 = vmul.f32 0.6931472, %v3156
%v3158 = vmul.f32 -0.5, %v3153
%v3159 = vadd.f32 1.0, %v3158
%v3160 = vmul.f32 %v3159, %v3153
%v3161 = vand.u32 2147483647, %v3153
%vm3162 = vcmp.lt.f32.partialorder %v3161, 0.0004427343
%v3163 = vsel /*vm=*/%vm3162, /*on_true_vy=*/%v3160, /*on_false_vx=*/%v3157
%v3164 = vxor.u32 2147483648, %v3163
%vm3167 = vcmp.lt.f32.partialorder %v3164, 5.0
%v3172 = vsel /*vm=*/%vm3167, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v3176 = vsel /*vm=*/%vm3167, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v3180 = vsel /*vm=*/%vm3167, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v3184 = vsel /*vm=*/%vm3167, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v3188 = vsel /*vm=*/%vm3167, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v3192 = vsel /*vm=*/%vm3167, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v3196 = vsel /*vm=*/%vm3167, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v3200 = vsel /*vm=*/%vm3167, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v3204 = vsel /*vm=*/%vm3167, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v3208 = vadd.f32 -2.5, %v3164
%v3210 = vrsqrt.pop %v3164
%v3211 = vmul.f32 %v3210, %v3164
%vm3212 = vcmp.eq.f32.partialorder %v3164, inf
%v3213 = vsel /*vm=*/%vm3212, /*on_true_vy=*/%v3164, /*on_false_vx=*/%v3211
%vm3214 = vcmp.eq.f32.partialorder %v3164, 0.0
%v3215 = vand.u32 2147483648, %v3164
%v3216 = vsel /*vm=*/%vm3214, /*on_true_vy=*/%v3215, /*on_false_vx=*/%v3213
%v3219 = vadd.f32 -3.0, %v3216
%v3223 = vsel /*vm=*/%vm3167, /*on_true_vy=*/%v3208, /*on_false_vx=*/%v3219
%v3227 = vmul.f32 %v3223, %v3204
%v3231 = vadd.f32 %v3227, %v3200
%v3235 = vmul.f32 %v3231, %v3223
%v3239 = vadd.f32 %v3235, %v3196
%v3243 = vmul.f32 %v3239, %v3223
%v3247 = vadd.f32 %v3243, %v3192
%v3251 = vmul.f32 %v3247, %v3223
%v3255 = vadd.f32 %v3251, %v3188
%v3259 = vmul.f32 %v3255, %v3223
%v3263 = vadd.f32 %v3259, %v3184
%v3267 = vmul.f32 %v3263, %v3223
%v3271 = vadd.f32 %v3267, %v3180
%v3275 = vmul.f32 %v3271, %v3223
%v3279 = vadd.f32 %v3275, %v3176
%v3283 = vmul.f32 %v3279, %v3223
%v3287 = vadd.f32 %v3283, %v3172
%v3291 = vmul.f32 %v3287, %v3138
%v3295 = vsel /*vm=*/%vm3143, /*on_true_vy=*/%v3148, /*on_false_vx=*/%v3291
%v3299 = vmul.f32 1.4140625, %v3295
%v3302 = vpack.c.bf16 %v120417, %v3299
%119759 = vst [vmem:[%s280 + $0x280] sm:$0xf] /*vst_source=*/%v3302
%v3332 = vadd.s32 %v3329, %v396
%v3342 = vadd.s32 %v3332, %v415
%vm3346 = vcmp.lt.u32.totalorder %v3342, %v3332
%vm3351 = vcmp.lt.u32.totalorder %v3332, %v3329
%v3356 = vadd.s32 %v3316, %v368
%v3360 = vadd.s32 1, %v3356
%v3364 = vsel /*vm=*/%vm3351, /*on_true_vy=*/%v3360, /*on_false_vx=*/%v3356
%v3368 = vadd.s32 1, %v3364
%v3372 = vsel /*vm=*/%vm3346, /*on_true_vy=*/%v3368, /*on_false_vx=*/%v3364
%v3377 = vadd.s32 %v3372, %v10
%v3381 = vadd.s32 %v3342, %v9
%v3385 = vadd.s32 %v3381, %v3377
%v3387 = vshll.u32 %v3381, 13
%v3388 = vshrl.u32 %v3381, 19
%v3389 = vor.u32 %v3388, %v3387
%v3390 = vxor.u32 %v3389, %v3385
%v3393 = vadd.s32 %v3390, %v3385
%v3395 = vshll.u32 %v3390, 15
%v3396 = vshrl.u32 %v3390, 17
%v3397 = vor.u32 %v3396, %v3395
%v3398 = vxor.u32 %v3397, %v3393
%v3401 = vadd.s32 %v3398, %v3393
%v3403 = vshll.u32 %v3398, 26
%v3404 = vshrl.u32 %v3398, 6
%v3405 = vor.u32 %v3404, %v3403
%v3406 = vxor.u32 %v3405, %v3401
%v3409 = vadd.s32 %v3406, %v3401
%v3413 = vadd.s32 %v3409, %v9
%v3415 = vshll.u32 %v3406, 6
%v3416 = vshrl.u32 %v3406, 26
%v3417 = vor.u32 %v3416, %v3415
%v3418 = vxor.u32 %v3417, %v3409
%v3421 = vadd.s32 %v3418, %v8
%v3425 = vadd.s32 1, %v3421
%v3429 = vadd.s32 %v3425, %v3413
%v3431 = vshll.u32 %v3425, 17
%v3432 = vshrl.u32 %v3425, 15
%v3433 = vor.u32 %v3432, %v3431
%v3434 = vxor.u32 %v3433, %v3429
%v3437 = vadd.s32 %v3434, %v3429
%v3439 = vshll.u32 %v3434, 29
%v3440 = vshrl.u32 %v3434, 3
%v3441 = vor.u32 %v3440, %v3439
%v3442 = vxor.u32 %v3441, %v3437
%v3445 = vadd.s32 %v3442, %v3437
%v3447 = vshll.u32 %v3442, 16
%v3448 = vshrl.u32 %v3442, 16
%v3449 = vor.u32 %v3448, %v3447
%v3450 = vxor.u32 %v3449, %v3445
%v3453 = vadd.s32 %v3450, %v3445
%v3457 = vadd.s32 %v3453, %v8
%v3459 = vshll.u32 %v3450, 24
%v3460 = vshrl.u32 %v3450, 8
%v3461 = vor.u32 %v3460, %v3459
%v3462 = vxor.u32 %v3461, %v3453
%v3465 = vadd.s32 %v3462, %v10
%v3469 = vadd.s32 2, %v3465
%v3473 = vadd.s32 %v3469, %v3457
%v3475 = vshll.u32 %v3469, 13
%v3476 = vshrl.u32 %v3469, 19
%v3477 = vor.u32 %v3476, %v3475
%v3478 = vxor.u32 %v3477, %v3473
%v3481 = vadd.s32 %v3478, %v3473
%v3483 = vshll.u32 %v3478, 15
%v3484 = vshrl.u32 %v3478, 17
%v3485 = vor.u32 %v3484, %v3483
%v3486 = vxor.u32 %v3485, %v3481
%v3489 = vadd.s32 %v3486, %v3481
%v3491 = vshll.u32 %v3486, 26
%v3492 = vshrl.u32 %v3486, 6
%v3493 = vor.u32 %v3492, %v3491
%v3494 = vxor.u32 %v3493, %v3489
%v3497 = vadd.s32 %v3494, %v3489
%v3501 = vadd.s32 %v3497, %v10
%v3503 = vshll.u32 %v3494, 6
%v3504 = vshrl.u32 %v3494, 26
%v3505 = vor.u32 %v3504, %v3503
%v3506 = vxor.u32 %v3505, %v3497
%v3509 = vadd.s32 %v3506, %v9
%v3513 = vadd.s32 3, %v3509
%v3517 = vadd.s32 %v3513, %v3501
%v3519 = vshll.u32 %v3513, 17
%v3520 = vshrl.u32 %v3513, 15
%v3521 = vor.u32 %v3520, %v3519
%v3522 = vxor.u32 %v3521, %v3517
%v3525 = vadd.s32 %v3522, %v3517
%v3527 = vshll.u32 %v3522, 29
%v3528 = vshrl.u32 %v3522, 3
%v3529 = vor.u32 %v3528, %v3527
%v3530 = vxor.u32 %v3529, %v3525
%v3533 = vadd.s32 %v3530, %v3525
%v3535 = vshll.u32 %v3530, 16
%v3536 = vshrl.u32 %v3530, 16
%v3537 = vor.u32 %v3536, %v3535
%v3538 = vxor.u32 %v3537, %v3533
%v3541 = vadd.s32 %v3538, %v3533
%v3545 = vadd.s32 %v3541, %v9
%v3547 = vshll.u32 %v3538, 24
%v3548 = vshrl.u32 %v3538, 8
%v3549 = vor.u32 %v3548, %v3547
%v3550 = vxor.u32 %v3549, %v3541
%v3553 = vadd.s32 %v3550, %v8
%v3557 = vadd.s32 4, %v3553
%v3561 = vadd.s32 %v3557, %v3545
%v3563 = vshll.u32 %v3557, 13
%v3564 = vshrl.u32 %v3557, 19
%v3565 = vor.u32 %v3564, %v3563
%v3566 = vxor.u32 %v3565, %v3561
%v3569 = vadd.s32 %v3566, %v3561
%v3571 = vshll.u32 %v3566, 15
%v3572 = vshrl.u32 %v3566, 17
%v3573 = vor.u32 %v3572, %v3571
%v3574 = vxor.u32 %v3573, %v3569
%v3577 = vadd.s32 %v3574, %v3569
%v3579 = vshll.u32 %v3574, 26
%v3580 = vshrl.u32 %v3574, 6
%v3581 = vor.u32 %v3580, %v3579
%v3582 = vxor.u32 %v3581, %v3577
%v3585 = vadd.s32 %v3582, %v3577
%v3589 = vadd.s32 %v3585, %v8
%v3591 = vshll.u32 %v3582, 6
%v3592 = vshrl.u32 %v3582, 26
%v3593 = vor.u32 %v3592, %v3591
%v3594 = vxor.u32 %v3593, %v3585
%v3597 = vadd.s32 %v3594, %v10
%v3601 = vadd.s32 5, %v3597
%v3603 = vxor.u32 %v3601, %v3589
%v3604 = vand.u32.u8 255, %v3603
%v3605 = vand.u32 65535, %v3604
%v3606 = vshrl.u32 %v3605, 1
%v3607 = vor.u32 16256, %v3606
%v3608 = vand.u32.u16 65535, %v3607
%v119760 = vadd.low.f32.bf16 -1.0, %v3608
%v3617 = vmul.f32 2.0, %v119760
%v3621 = vadd.f32 -0.99609375, %v3617
%v3625 = vmax.f32 %v3621, -0.99609375
%v3627 = vand.u32 2147483647, %v3625
%vm3630 = vcmp.eq.f32.partialorder %v3627, 1.0
%v3635 = vmul.f32 inf, %v3625
%v3637 = vxor.u32 2147483648, %v3625
%v3640 = vmul.f32 %v3637, %v3625
%v3642 = vadd.f32 1.0, %v3640
%v3643 = vlog2.pop %v3642
%v3644 = vmul.f32 0.6931472, %v3643
%v3645 = vmul.f32 -0.5, %v3640
%v3646 = vadd.f32 1.0, %v3645
%v3647 = vmul.f32 %v3646, %v3640
%v3648 = vand.u32 2147483647, %v3640
%vm3649 = vcmp.lt.f32.partialorder %v3648, 0.0004427343
%v3650 = vsel /*vm=*/%vm3649, /*on_true_vy=*/%v3647, /*on_false_vx=*/%v3644
%v3651 = vxor.u32 2147483648, %v3650
%vm3654 = vcmp.lt.f32.partialorder %v3651, 5.0
%v3659 = vsel /*vm=*/%vm3654, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v3663 = vsel /*vm=*/%vm3654, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v3667 = vsel /*vm=*/%vm3654, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v3671 = vsel /*vm=*/%vm3654, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v3675 = vsel /*vm=*/%vm3654, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v3679 = vsel /*vm=*/%vm3654, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v3683 = vsel /*vm=*/%vm3654, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v3687 = vsel /*vm=*/%vm3654, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v3691 = vsel /*vm=*/%vm3654, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v3695 = vadd.f32 -2.5, %v3651
%v3697 = vrsqrt.pop %v3651
%v3698 = vmul.f32 %v3697, %v3651
%vm3699 = vcmp.eq.f32.partialorder %v3651, inf
%v3700 = vsel /*vm=*/%vm3699, /*on_true_vy=*/%v3651, /*on_false_vx=*/%v3698
%vm3701 = vcmp.eq.f32.partialorder %v3651, 0.0
%v3702 = vand.u32 2147483648, %v3651
%v3703 = vsel /*vm=*/%vm3701, /*on_true_vy=*/%v3702, /*on_false_vx=*/%v3700
%v3706 = vadd.f32 -3.0, %v3703
%v3710 = vsel /*vm=*/%vm3654, /*on_true_vy=*/%v3695, /*on_false_vx=*/%v3706
%v3714 = vmul.f32 %v3710, %v3691
%v3718 = vadd.f32 %v3714, %v3687
%v3722 = vmul.f32 %v3718, %v3710
%v3726 = vadd.f32 %v3722, %v3683
%v3730 = vmul.f32 %v3726, %v3710
%v3734 = vadd.f32 %v3730, %v3679
%v3738 = vmul.f32 %v3734, %v3710
%v3742 = vadd.f32 %v3738, %v3675
%v3746 = vmul.f32 %v3742, %v3710
%v3750 = vadd.f32 %v3746, %v3671
%v3754 = vmul.f32 %v3750, %v3710
%v3758 = vadd.f32 %v3754, %v3667
%v3762 = vmul.f32 %v3758, %v3710
%v3766 = vadd.f32 %v3762, %v3663
%v3770 = vmul.f32 %v3766, %v3710
%v3774 = vadd.f32 %v3770, %v3659
%v3778 = vmul.f32 %v3774, %v3625
%v3782 = vsel /*vm=*/%vm3630, /*on_true_vy=*/%v3635, /*on_false_vx=*/%v3778
%v3786 = vmul.f32 1.4140625, %v3782
%v3789 = vpack.c.bf16 %v120417, %v3786
%119761 = vst [vmem:[%s280 + $0x300] sm:$0xf] /*vst_source=*/%v3789
%v3819 = vadd.s32 %v3816, %v396
%v3829 = vadd.s32 %v3819, %v415
%vm3833 = vcmp.lt.u32.totalorder %v3829, %v3819
%vm3838 = vcmp.lt.u32.totalorder %v3819, %v3816
%v3843 = vadd.s32 %v3803, %v368
%v3847 = vadd.s32 1, %v3843
%v3851 = vsel /*vm=*/%vm3838, /*on_true_vy=*/%v3847, /*on_false_vx=*/%v3843
%v3855 = vadd.s32 1, %v3851
%v3859 = vsel /*vm=*/%vm3833, /*on_true_vy=*/%v3855, /*on_false_vx=*/%v3851
%v3864 = vadd.s32 %v3859, %v10
%v3868 = vadd.s32 %v3829, %v9
%v3872 = vadd.s32 %v3868, %v3864
%v3874 = vshll.u32 %v3868, 13
%v3875 = vshrl.u32 %v3868, 19
%v3876 = vor.u32 %v3875, %v3874
%v3877 = vxor.u32 %v3876, %v3872
%v3880 = vadd.s32 %v3877, %v3872
%v3882 = vshll.u32 %v3877, 15
%v3883 = vshrl.u32 %v3877, 17
%v3884 = vor.u32 %v3883, %v3882
%v3885 = vxor.u32 %v3884, %v3880
%v3888 = vadd.s32 %v3885, %v3880
%v3890 = vshll.u32 %v3885, 26
%v3891 = vshrl.u32 %v3885, 6
%v3892 = vor.u32 %v3891, %v3890
%v3893 = vxor.u32 %v3892, %v3888
%v3896 = vadd.s32 %v3893, %v3888
%v3900 = vadd.s32 %v3896, %v9
%v3902 = vshll.u32 %v3893, 6
%v3903 = vshrl.u32 %v3893, 26
%v3904 = vor.u32 %v3903, %v3902
%v3905 = vxor.u32 %v3904, %v3896
%v3908 = vadd.s32 %v3905, %v8
%v3912 = vadd.s32 1, %v3908
%v3916 = vadd.s32 %v3912, %v3900
%v3918 = vshll.u32 %v3912, 17
%v3919 = vshrl.u32 %v3912, 15
%v3920 = vor.u32 %v3919, %v3918
%v3921 = vxor.u32 %v3920, %v3916
%v3924 = vadd.s32 %v3921, %v3916
%v3926 = vshll.u32 %v3921, 29
%v3927 = vshrl.u32 %v3921, 3
%v3928 = vor.u32 %v3927, %v3926
%v3929 = vxor.u32 %v3928, %v3924
%v3932 = vadd.s32 %v3929, %v3924
%v3934 = vshll.u32 %v3929, 16
%v3935 = vshrl.u32 %v3929, 16
%v3936 = vor.u32 %v3935, %v3934
%v3937 = vxor.u32 %v3936, %v3932
%v3940 = vadd.s32 %v3937, %v3932
%v3944 = vadd.s32 %v3940, %v8
%v3946 = vshll.u32 %v3937, 24
%v3947 = vshrl.u32 %v3937, 8
%v3948 = vor.u32 %v3947, %v3946
%v3949 = vxor.u32 %v3948, %v3940
%v3952 = vadd.s32 %v3949, %v10
%v3956 = vadd.s32 2, %v3952
%v3960 = vadd.s32 %v3956, %v3944
%v3962 = vshll.u32 %v3956, 13
%v3963 = vshrl.u32 %v3956, 19
%v3964 = vor.u32 %v3963, %v3962
%v3965 = vxor.u32 %v3964, %v3960
%v3968 = vadd.s32 %v3965, %v3960
%v3970 = vshll.u32 %v3965, 15
%v3971 = vshrl.u32 %v3965, 17
%v3972 = vor.u32 %v3971, %v3970
%v3973 = vxor.u32 %v3972, %v3968
%v3976 = vadd.s32 %v3973, %v3968
%v3978 = vshll.u32 %v3973, 26
%v3979 = vshrl.u32 %v3973, 6
%v3980 = vor.u32 %v3979, %v3978
%v3981 = vxor.u32 %v3980, %v3976
%v3984 = vadd.s32 %v3981, %v3976
%v3988 = vadd.s32 %v3984, %v10
%v3990 = vshll.u32 %v3981, 6
%v3991 = vshrl.u32 %v3981, 26
%v3992 = vor.u32 %v3991, %v3990
%v3993 = vxor.u32 %v3992, %v3984
%v3996 = vadd.s32 %v3993, %v9
%v4000 = vadd.s32 3, %v3996
%v4004 = vadd.s32 %v4000, %v3988
%v4006 = vshll.u32 %v4000, 17
%v4007 = vshrl.u32 %v4000, 15
%v4008 = vor.u32 %v4007, %v4006
%v4009 = vxor.u32 %v4008, %v4004
%v4012 = vadd.s32 %v4009, %v4004
%v4014 = vshll.u32 %v4009, 29
%v4015 = vshrl.u32 %v4009, 3
%v4016 = vor.u32 %v4015, %v4014
%v4017 = vxor.u32 %v4016, %v4012
%v4020 = vadd.s32 %v4017, %v4012
%v4022 = vshll.u32 %v4017, 16
%v4023 = vshrl.u32 %v4017, 16
%v4024 = vor.u32 %v4023, %v4022
%v4025 = vxor.u32 %v4024, %v4020
%v4028 = vadd.s32 %v4025, %v4020
%v4032 = vadd.s32 %v4028, %v9
%v4034 = vshll.u32 %v4025, 24
%v4035 = vshrl.u32 %v4025, 8
%v4036 = vor.u32 %v4035, %v4034
%v4037 = vxor.u32 %v4036, %v4028
%v4040 = vadd.s32 %v4037, %v8
%v4044 = vadd.s32 4, %v4040
%v4048 = vadd.s32 %v4044, %v4032
%v4050 = vshll.u32 %v4044, 13
%v4051 = vshrl.u32 %v4044, 19
%v4052 = vor.u32 %v4051, %v4050
%v4053 = vxor.u32 %v4052, %v4048
%v4056 = vadd.s32 %v4053, %v4048
%v4058 = vshll.u32 %v4053, 15
%v4059 = vshrl.u32 %v4053, 17
%v4060 = vor.u32 %v4059, %v4058
%v4061 = vxor.u32 %v4060, %v4056
%v4064 = vadd.s32 %v4061, %v4056
%v4066 = vshll.u32 %v4061, 26
%v4067 = vshrl.u32 %v4061, 6
%v4068 = vor.u32 %v4067, %v4066
%v4069 = vxor.u32 %v4068, %v4064
%v4072 = vadd.s32 %v4069, %v4064
%v4076 = vadd.s32 %v4072, %v8
%v4078 = vshll.u32 %v4069, 6
%v4079 = vshrl.u32 %v4069, 26
%v4080 = vor.u32 %v4079, %v4078
%v4081 = vxor.u32 %v4080, %v4072
%v4084 = vadd.s32 %v4081, %v10
%v4088 = vadd.s32 5, %v4084
%v4090 = vxor.u32 %v4088, %v4076
%v4091 = vand.u32.u8 255, %v4090
%v4092 = vand.u32 65535, %v4091
%v4093 = vshrl.u32 %v4092, 1
%v4094 = vor.u32 16256, %v4093
%v4095 = vand.u32.u16 65535, %v4094
%v119762 = vadd.low.f32.bf16 -1.0, %v4095
%v4104 = vmul.f32 2.0, %v119762
%v4108 = vadd.f32 -0.99609375, %v4104
%v4112 = vmax.f32 %v4108, -0.99609375
%v4114 = vand.u32 2147483647, %v4112
%vm4117 = vcmp.eq.f32.partialorder %v4114, 1.0
%v4122 = vmul.f32 inf, %v4112
%v4124 = vxor.u32 2147483648, %v4112
%v4127 = vmul.f32 %v4124, %v4112
%v4129 = vadd.f32 1.0, %v4127
%v4130 = vlog2.pop %v4129
%v4131 = vmul.f32 0.6931472, %v4130
%v4132 = vmul.f32 -0.5, %v4127
%v4133 = vadd.f32 1.0, %v4132
%v4134 = vmul.f32 %v4133, %v4127
%v4135 = vand.u32 2147483647, %v4127
%vm4136 = vcmp.lt.f32.partialorder %v4135, 0.0004427343
%v4137 = vsel /*vm=*/%vm4136, /*on_true_vy=*/%v4134, /*on_false_vx=*/%v4131
%v4138 = vxor.u32 2147483648, %v4137
%vm4141 = vcmp.lt.f32.partialorder %v4138, 5.0
%v4146 = vsel /*vm=*/%vm4141, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v4150 = vsel /*vm=*/%vm4141, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v4154 = vsel /*vm=*/%vm4141, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v4158 = vsel /*vm=*/%vm4141, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v4162 = vsel /*vm=*/%vm4141, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v4166 = vsel /*vm=*/%vm4141, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v4170 = vsel /*vm=*/%vm4141, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v4174 = vsel /*vm=*/%vm4141, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v4178 = vsel /*vm=*/%vm4141, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v4182 = vadd.f32 -2.5, %v4138
%v4184 = vrsqrt.pop %v4138
%v4185 = vmul.f32 %v4184, %v4138
%vm4186 = vcmp.eq.f32.partialorder %v4138, inf
%v4187 = vsel /*vm=*/%vm4186, /*on_true_vy=*/%v4138, /*on_false_vx=*/%v4185
%vm4188 = vcmp.eq.f32.partialorder %v4138, 0.0
%v4189 = vand.u32 2147483648, %v4138
%v4190 = vsel /*vm=*/%vm4188, /*on_true_vy=*/%v4189, /*on_false_vx=*/%v4187
%v4193 = vadd.f32 -3.0, %v4190
%v4197 = vsel /*vm=*/%vm4141, /*on_true_vy=*/%v4182, /*on_false_vx=*/%v4193
%v4201 = vmul.f32 %v4197, %v4178
%v4205 = vadd.f32 %v4201, %v4174
%v4209 = vmul.f32 %v4205, %v4197
%v4213 = vadd.f32 %v4209, %v4170
%v4217 = vmul.f32 %v4213, %v4197
%v4221 = vadd.f32 %v4217, %v4166
%v4225 = vmul.f32 %v4221, %v4197
%v4229 = vadd.f32 %v4225, %v4162
%v4233 = vmul.f32 %v4229, %v4197
%v4237 = vadd.f32 %v4233, %v4158
%v4241 = vmul.f32 %v4237, %v4197
%v4245 = vadd.f32 %v4241, %v4154
%v4249 = vmul.f32 %v4245, %v4197
%v4253 = vadd.f32 %v4249, %v4150
%v4257 = vmul.f32 %v4253, %v4197
%v4261 = vadd.f32 %v4257, %v4146
%v4265 = vmul.f32 %v4261, %v4112
%v4269 = vsel /*vm=*/%vm4117, /*on_true_vy=*/%v4122, /*on_false_vx=*/%v4265
%v4273 = vmul.f32 1.4140625, %v4269
%v4276 = vpack.c.bf16 %v120417, %v4273
%119763 = vst [vmem:[%s280 + $0x380] sm:$0xf] /*vst_source=*/%v4276
%v4314 = vadd.s32 %v4311, %v408
%v4324 = vadd.s32 %v4314, %v415
%vm4328 = vcmp.lt.u32.totalorder %v4324, %v4314
%vm4333 = vcmp.lt.u32.totalorder %v4314, %v408
%v4338 = vadd.s32 %v4294, %v380
%v4342 = vadd.s32 1, %v4338
%v4346 = vsel /*vm=*/%vm4333, /*on_true_vy=*/%v4342, /*on_false_vx=*/%v4338
%v4350 = vadd.s32 1, %v4346
%v4354 = vsel /*vm=*/%vm4328, /*on_true_vy=*/%v4350, /*on_false_vx=*/%v4346
%v4359 = vadd.s32 %v4354, %v10
%v4363 = vadd.s32 %v4324, %v9
%v4367 = vadd.s32 %v4363, %v4359
%v4369 = vshll.u32 %v4363, 13
%v4370 = vshrl.u32 %v4363, 19
%v4371 = vor.u32 %v4370, %v4369
%v4372 = vxor.u32 %v4371, %v4367
%v4375 = vadd.s32 %v4372, %v4367
%v4377 = vshll.u32 %v4372, 15
%v4378 = vshrl.u32 %v4372, 17
%v4379 = vor.u32 %v4378, %v4377
%v4380 = vxor.u32 %v4379, %v4375
%v4383 = vadd.s32 %v4380, %v4375
%v4385 = vshll.u32 %v4380, 26
%v4386 = vshrl.u32 %v4380, 6
%v4387 = vor.u32 %v4386, %v4385
%v4388 = vxor.u32 %v4387, %v4383
%v4391 = vadd.s32 %v4388, %v4383
%v4395 = vadd.s32 %v4391, %v9
%v4397 = vshll.u32 %v4388, 6
%v4398 = vshrl.u32 %v4388, 26
%v4399 = vor.u32 %v4398, %v4397
%v4400 = vxor.u32 %v4399, %v4391
%v4403 = vadd.s32 %v4400, %v8
%v4407 = vadd.s32 1, %v4403
%v4411 = vadd.s32 %v4407, %v4395
%v4413 = vshll.u32 %v4407, 17
%v4414 = vshrl.u32 %v4407, 15
%v4415 = vor.u32 %v4414, %v4413
%v4416 = vxor.u32 %v4415, %v4411
%v4419 = vadd.s32 %v4416, %v4411
%v4421 = vshll.u32 %v4416, 29
%v4422 = vshrl.u32 %v4416, 3
%v4423 = vor.u32 %v4422, %v4421
%v4424 = vxor.u32 %v4423, %v4419
%v4427 = vadd.s32 %v4424, %v4419
%v4429 = vshll.u32 %v4424, 16
%v4430 = vshrl.u32 %v4424, 16
%v4431 = vor.u32 %v4430, %v4429
%v4432 = vxor.u32 %v4431, %v4427
%v4435 = vadd.s32 %v4432, %v4427
%v4439 = vadd.s32 %v4435, %v8
%v4441 = vshll.u32 %v4432, 24
%v4442 = vshrl.u32 %v4432, 8
%v4443 = vor.u32 %v4442, %v4441
%v4444 = vxor.u32 %v4443, %v4435
%v4447 = vadd.s32 %v4444, %v10
%v4451 = vadd.s32 2, %v4447
%v4455 = vadd.s32 %v4451, %v4439
%v4457 = vshll.u32 %v4451, 13
%v4458 = vshrl.u32 %v4451, 19
%v4459 = vor.u32 %v4458, %v4457
%v4460 = vxor.u32 %v4459, %v4455
%v4463 = vadd.s32 %v4460, %v4455
%v4465 = vshll.u32 %v4460, 15
%v4466 = vshrl.u32 %v4460, 17
%v4467 = vor.u32 %v4466, %v4465
%v4468 = vxor.u32 %v4467, %v4463
%v4471 = vadd.s32 %v4468, %v4463
%v4473 = vshll.u32 %v4468, 26
%v4474 = vshrl.u32 %v4468, 6
%v4475 = vor.u32 %v4474, %v4473
%v4476 = vxor.u32 %v4475, %v4471
%v4479 = vadd.s32 %v4476, %v4471
%v4483 = vadd.s32 %v4479, %v10
%v4485 = vshll.u32 %v4476, 6
%v4486 = vshrl.u32 %v4476, 26
%v4487 = vor.u32 %v4486, %v4485
%v4488 = vxor.u32 %v4487, %v4479
%v4491 = vadd.s32 %v4488, %v9
%v4495 = vadd.s32 3, %v4491
%v4499 = vadd.s32 %v4495, %v4483
%v4501 = vshll.u32 %v4495, 17
%v4502 = vshrl.u32 %v4495, 15
%v4503 = vor.u32 %v4502, %v4501
%v4504 = vxor.u32 %v4503, %v4499
%v4507 = vadd.s32 %v4504, %v4499
%v4509 = vshll.u32 %v4504, 29
%v4510 = vshrl.u32 %v4504, 3
%v4511 = vor.u32 %v4510, %v4509
%v4512 = vxor.u32 %v4511, %v4507
%v4515 = vadd.s32 %v4512, %v4507
%v4517 = vshll.u32 %v4512, 16
%v4518 = vshrl.u32 %v4512, 16
%v4519 = vor.u32 %v4518, %v4517
%v4520 = vxor.u32 %v4519, %v4515
%v4523 = vadd.s32 %v4520, %v4515
%v4527 = vadd.s32 %v4523, %v9
%v4529 = vshll.u32 %v4520, 24
%v4530 = vshrl.u32 %v4520, 8
%v4531 = vor.u32 %v4530, %v4529
%v4532 = vxor.u32 %v4531, %v4523
%v4535 = vadd.s32 %v4532, %v8
%v4539 = vadd.s32 4, %v4535
%v4543 = vadd.s32 %v4539, %v4527
%v4545 = vshll.u32 %v4539, 13
%v4546 = vshrl.u32 %v4539, 19
%v4547 = vor.u32 %v4546, %v4545
%v4548 = vxor.u32 %v4547, %v4543
%v4551 = vadd.s32 %v4548, %v4543
%v4553 = vshll.u32 %v4548, 15
%v4554 = vshrl.u32 %v4548, 17
%v4555 = vor.u32 %v4554, %v4553
%v4556 = vxor.u32 %v4555, %v4551
%v4559 = vadd.s32 %v4556, %v4551
%v4561 = vshll.u32 %v4556, 26
%v4562 = vshrl.u32 %v4556, 6
%v4563 = vor.u32 %v4562, %v4561
%v4564 = vxor.u32 %v4563, %v4559
%v4567 = vadd.s32 %v4564, %v4559
%v4571 = vadd.s32 %v4567, %v8
%v4573 = vshll.u32 %v4564, 6
%v4574 = vshrl.u32 %v4564, 26
%v4575 = vor.u32 %v4574, %v4573
%v4576 = vxor.u32 %v4575, %v4567
%v4579 = vadd.s32 %v4576, %v10
%v4583 = vadd.s32 5, %v4579
%v4585 = vxor.u32 %v4583, %v4571
%v4586 = vand.u32.u8 255, %v4585
%v4587 = vand.u32 65535, %v4586
%v4588 = vshrl.u32 %v4587, 1
%v4589 = vor.u32 16256, %v4588
%v4590 = vand.u32.u16 65535, %v4589
%v119768 = vadd.low.f32.bf16 -1.0, %v4590
%v4599 = vmul.f32 2.0, %v119768
%v4603 = vadd.f32 -0.99609375, %v4599
%v4607 = vmax.f32 %v4603, -0.99609375
%v4609 = vand.u32 2147483647, %v4607
%vm4612 = vcmp.eq.f32.partialorder %v4609, 1.0
%v4617 = vmul.f32 inf, %v4607
%v4619 = vxor.u32 2147483648, %v4607
%v4622 = vmul.f32 %v4619, %v4607
%v4624 = vadd.f32 1.0, %v4622
%v4625 = vlog2.pop %v4624
%v4626 = vmul.f32 0.6931472, %v4625
%v4627 = vmul.f32 -0.5, %v4622
%v4628 = vadd.f32 1.0, %v4627
%v4629 = vmul.f32 %v4628, %v4622
%v4630 = vand.u32 2147483647, %v4622
%vm4631 = vcmp.lt.f32.partialorder %v4630, 0.0004427343
%v4632 = vsel /*vm=*/%vm4631, /*on_true_vy=*/%v4629, /*on_false_vx=*/%v4626
%v4633 = vxor.u32 2147483648, %v4632
%vm4636 = vcmp.lt.f32.partialorder %v4633, 5.0
%v4641 = vsel /*vm=*/%vm4636, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v4645 = vsel /*vm=*/%vm4636, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v4649 = vsel /*vm=*/%vm4636, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v4653 = vsel /*vm=*/%vm4636, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v4657 = vsel /*vm=*/%vm4636, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v4661 = vsel /*vm=*/%vm4636, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v4665 = vsel /*vm=*/%vm4636, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v4669 = vsel /*vm=*/%vm4636, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v4673 = vsel /*vm=*/%vm4636, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v4677 = vadd.f32 -2.5, %v4633
%v4679 = vrsqrt.pop %v4633
%v4680 = vmul.f32 %v4679, %v4633
%vm4681 = vcmp.eq.f32.partialorder %v4633, inf
%v4682 = vsel /*vm=*/%vm4681, /*on_true_vy=*/%v4633, /*on_false_vx=*/%v4680
%vm4683 = vcmp.eq.f32.partialorder %v4633, 0.0
%v4684 = vand.u32 2147483648, %v4633
%v4685 = vsel /*vm=*/%vm4683, /*on_true_vy=*/%v4684, /*on_false_vx=*/%v4682
%v4688 = vadd.f32 -3.0, %v4685
%v4692 = vsel /*vm=*/%vm4636, /*on_true_vy=*/%v4677, /*on_false_vx=*/%v4688
%v4696 = vmul.f32 %v4692, %v4673
%v4700 = vadd.f32 %v4696, %v4669
%v4704 = vmul.f32 %v4700, %v4692
%v4708 = vadd.f32 %v4704, %v4665
%v4712 = vmul.f32 %v4708, %v4692
%v4716 = vadd.f32 %v4712, %v4661
%v4720 = vmul.f32 %v4716, %v4692
%v4724 = vadd.f32 %v4720, %v4657
%v4728 = vmul.f32 %v4724, %v4692
%v4732 = vadd.f32 %v4728, %v4653
%v4736 = vmul.f32 %v4732, %v4692
%v4740 = vadd.f32 %v4736, %v4649
%v4744 = vmul.f32 %v4740, %v4692
%v4748 = vadd.f32 %v4744, %v4645
%v4752 = vmul.f32 %v4748, %v4692
%v4756 = vadd.f32 %v4752, %v4641
%v4760 = vmul.f32 %v4756, %v4607
%v4764 = vsel /*vm=*/%vm4612, /*on_true_vy=*/%v4617, /*on_false_vx=*/%v4760
%v4768 = vmul.f32 1.4140625, %v4764
%v4771 = vpack.c.bf16 %v120417, %v4768
%119769 = vst [vmem:[%s280 + $0x4] sm:$0xf] /*vst_source=*/%v4771
%v4775 = vadd.s32 %v4311, %v894
%v4785 = vadd.s32 %v4775, %v415
%vm4789 = vcmp.lt.u32.totalorder %v4785, %v4775
%vm4794 = vcmp.lt.u32.totalorder %v4775, %v894
%v4799 = vadd.s32 %v4294, %v881
%v4803 = vadd.s32 1, %v4799
%v4807 = vsel /*vm=*/%vm4794, /*on_true_vy=*/%v4803, /*on_false_vx=*/%v4799
%v4811 = vadd.s32 1, %v4807
%v4815 = vsel /*vm=*/%vm4789, /*on_true_vy=*/%v4811, /*on_false_vx=*/%v4807
%v4820 = vadd.s32 %v4815, %v10
%v4824 = vadd.s32 %v4785, %v9
%v4828 = vadd.s32 %v4824, %v4820
%v4830 = vshll.u32 %v4824, 13
%v4831 = vshrl.u32 %v4824, 19
%v4832 = vor.u32 %v4831, %v4830
%v4833 = vxor.u32 %v4832, %v4828
%v4836 = vadd.s32 %v4833, %v4828
%v4838 = vshll.u32 %v4833, 15
%v4839 = vshrl.u32 %v4833, 17
%v4840 = vor.u32 %v4839, %v4838
%v4841 = vxor.u32 %v4840, %v4836
%v4844 = vadd.s32 %v4841, %v4836
%v4846 = vshll.u32 %v4841, 26
%v4847 = vshrl.u32 %v4841, 6
%v4848 = vor.u32 %v4847, %v4846
%v4849 = vxor.u32 %v4848, %v4844
%v4852 = vadd.s32 %v4849, %v4844
%v4856 = vadd.s32 %v4852, %v9
%v4858 = vshll.u32 %v4849, 6
%v4859 = vshrl.u32 %v4849, 26
%v4860 = vor.u32 %v4859, %v4858
%v4861 = vxor.u32 %v4860, %v4852
%v4864 = vadd.s32 %v4861, %v8
%v4868 = vadd.s32 1, %v4864
%v4872 = vadd.s32 %v4868, %v4856
%v4874 = vshll.u32 %v4868, 17
%v4875 = vshrl.u32 %v4868, 15
%v4876 = vor.u32 %v4875, %v4874
%v4877 = vxor.u32 %v4876, %v4872
%v4880 = vadd.s32 %v4877, %v4872
%v4882 = vshll.u32 %v4877, 29
%v4883 = vshrl.u32 %v4877, 3
%v4884 = vor.u32 %v4883, %v4882
%v4885 = vxor.u32 %v4884, %v4880
%v4888 = vadd.s32 %v4885, %v4880
%v4890 = vshll.u32 %v4885, 16
%v4891 = vshrl.u32 %v4885, 16
%v4892 = vor.u32 %v4891, %v4890
%v4893 = vxor.u32 %v4892, %v4888
%v4896 = vadd.s32 %v4893, %v4888
%v4900 = vadd.s32 %v4896, %v8
%v4902 = vshll.u32 %v4893, 24
%v4903 = vshrl.u32 %v4893, 8
%v4904 = vor.u32 %v4903, %v4902
%v4905 = vxor.u32 %v4904, %v4896
%v4908 = vadd.s32 %v4905, %v10
%v4912 = vadd.s32 2, %v4908
%v4916 = vadd.s32 %v4912, %v4900
%v4918 = vshll.u32 %v4912, 13
%v4919 = vshrl.u32 %v4912, 19
%v4920 = vor.u32 %v4919, %v4918
%v4921 = vxor.u32 %v4920, %v4916
%v4924 = vadd.s32 %v4921, %v4916
%v4926 = vshll.u32 %v4921, 15
%v4927 = vshrl.u32 %v4921, 17
%v4928 = vor.u32 %v4927, %v4926
%v4929 = vxor.u32 %v4928, %v4924
%v4932 = vadd.s32 %v4929, %v4924
%v4934 = vshll.u32 %v4929, 26
%v4935 = vshrl.u32 %v4929, 6
%v4936 = vor.u32 %v4935, %v4934
%v4937 = vxor.u32 %v4936, %v4932
%v4940 = vadd.s32 %v4937, %v4932
%v4944 = vadd.s32 %v4940, %v10
%v4946 = vshll.u32 %v4937, 6
%v4947 = vshrl.u32 %v4937, 26
%v4948 = vor.u32 %v4947, %v4946
%v4949 = vxor.u32 %v4948, %v4940
%v4952 = vadd.s32 %v4949, %v9
%v4956 = vadd.s32 3, %v4952
%v4960 = vadd.s32 %v4956, %v4944
%v4962 = vshll.u32 %v4956, 17
%v4963 = vshrl.u32 %v4956, 15
%v4964 = vor.u32 %v4963, %v4962
%v4965 = vxor.u32 %v4964, %v4960
%v4968 = vadd.s32 %v4965, %v4960
%v4970 = vshll.u32 %v4965, 29
%v4971 = vshrl.u32 %v4965, 3
%v4972 = vor.u32 %v4971, %v4970
%v4973 = vxor.u32 %v4972, %v4968
%v4976 = vadd.s32 %v4973, %v4968
%v4978 = vshll.u32 %v4973, 16
%v4979 = vshrl.u32 %v4973, 16
%v4980 = vor.u32 %v4979, %v4978
%v4981 = vxor.u32 %v4980, %v4976
%v4984 = vadd.s32 %v4981, %v4976
%v4988 = vadd.s32 %v4984, %v9
%v4990 = vshll.u32 %v4981, 24
%v4991 = vshrl.u32 %v4981, 8
%v4992 = vor.u32 %v4991, %v4990
%v4993 = vxor.u32 %v4992, %v4984
%v4996 = vadd.s32 %v4993, %v8
%v5000 = vadd.s32 4, %v4996
%v5004 = vadd.s32 %v5000, %v4988
%v5006 = vshll.u32 %v5000, 13
%v5007 = vshrl.u32 %v5000, 19
%v5008 = vor.u32 %v5007, %v5006
%v5009 = vxor.u32 %v5008, %v5004
%v5012 = vadd.s32 %v5009, %v5004
%v5014 = vshll.u32 %v5009, 15
%v5015 = vshrl.u32 %v5009, 17
%v5016 = vor.u32 %v5015, %v5014
%v5017 = vxor.u32 %v5016, %v5012
%v5020 = vadd.s32 %v5017, %v5012
%v5022 = vshll.u32 %v5017, 26
%v5023 = vshrl.u32 %v5017, 6
%v5024 = vor.u32 %v5023, %v5022
%v5025 = vxor.u32 %v5024, %v5020
%v5028 = vadd.s32 %v5025, %v5020
%v5032 = vadd.s32 %v5028, %v8
%v5034 = vshll.u32 %v5025, 6
%v5035 = vshrl.u32 %v5025, 26
%v5036 = vor.u32 %v5035, %v5034
%v5037 = vxor.u32 %v5036, %v5028
%v5040 = vadd.s32 %v5037, %v10
%v5044 = vadd.s32 5, %v5040
%v5046 = vxor.u32 %v5044, %v5032
%v5047 = vand.u32.u8 255, %v5046
%v5048 = vand.u32 65535, %v5047
%v5049 = vshrl.u32 %v5048, 1
%v5050 = vor.u32 16256, %v5049
%v5051 = vand.u32.u16 65535, %v5050
%v119770 = vadd.low.f32.bf16 -1.0, %v5051
%v5060 = vmul.f32 2.0, %v119770
%v5064 = vadd.f32 -0.99609375, %v5060
%v5068 = vmax.f32 %v5064, -0.99609375
%v5070 = vand.u32 2147483647, %v5068
%vm5073 = vcmp.eq.f32.partialorder %v5070, 1.0
%v5078 = vmul.f32 inf, %v5068
%v5080 = vxor.u32 2147483648, %v5068
%v5083 = vmul.f32 %v5080, %v5068
%v5085 = vadd.f32 1.0, %v5083
%v5086 = vlog2.pop %v5085
%v5087 = vmul.f32 0.6931472, %v5086
%v5088 = vmul.f32 -0.5, %v5083
%v5089 = vadd.f32 1.0, %v5088
%v5090 = vmul.f32 %v5089, %v5083
%v5091 = vand.u32 2147483647, %v5083
%vm5092 = vcmp.lt.f32.partialorder %v5091, 0.0004427343
%v5093 = vsel /*vm=*/%vm5092, /*on_true_vy=*/%v5090, /*on_false_vx=*/%v5087
%v5094 = vxor.u32 2147483648, %v5093
%vm5097 = vcmp.lt.f32.partialorder %v5094, 5.0
%v5102 = vsel /*vm=*/%vm5097, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v5106 = vsel /*vm=*/%vm5097, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v5110 = vsel /*vm=*/%vm5097, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v5114 = vsel /*vm=*/%vm5097, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v5118 = vsel /*vm=*/%vm5097, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v5122 = vsel /*vm=*/%vm5097, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v5126 = vsel /*vm=*/%vm5097, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v5130 = vsel /*vm=*/%vm5097, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v5134 = vsel /*vm=*/%vm5097, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v5138 = vadd.f32 -2.5, %v5094
%v5140 = vrsqrt.pop %v5094
%v5141 = vmul.f32 %v5140, %v5094
%vm5142 = vcmp.eq.f32.partialorder %v5094, inf
%v5143 = vsel /*vm=*/%vm5142, /*on_true_vy=*/%v5094, /*on_false_vx=*/%v5141
%vm5144 = vcmp.eq.f32.partialorder %v5094, 0.0
%v5145 = vand.u32 2147483648, %v5094
%v5146 = vsel /*vm=*/%vm5144, /*on_true_vy=*/%v5145, /*on_false_vx=*/%v5143
%v5149 = vadd.f32 -3.0, %v5146
%v5153 = vsel /*vm=*/%vm5097, /*on_true_vy=*/%v5138, /*on_false_vx=*/%v5149
%v5157 = vmul.f32 %v5153, %v5134
%v5161 = vadd.f32 %v5157, %v5130
%v5165 = vmul.f32 %v5161, %v5153
%v5169 = vadd.f32 %v5165, %v5126
%v5173 = vmul.f32 %v5169, %v5153
%v5177 = vadd.f32 %v5173, %v5122
%v5181 = vmul.f32 %v5177, %v5153
%v5185 = vadd.f32 %v5181, %v5118
%v5189 = vmul.f32 %v5185, %v5153
%v5193 = vadd.f32 %v5189, %v5114
%v5197 = vmul.f32 %v5193, %v5153
%v5201 = vadd.f32 %v5197, %v5110
%v5205 = vmul.f32 %v5201, %v5153
%v5209 = vadd.f32 %v5205, %v5106
%v5213 = vmul.f32 %v5209, %v5153
%v5217 = vadd.f32 %v5213, %v5102
%v5221 = vmul.f32 %v5217, %v5068
%v5225 = vsel /*vm=*/%vm5073, /*on_true_vy=*/%v5078, /*on_false_vx=*/%v5221
%v5229 = vmul.f32 1.4140625, %v5225
%v5232 = vpack.c.bf16 %v120417, %v5229
%119771 = vst [vmem:[%s280 + $0x84] sm:$0xf] /*vst_source=*/%v5232
%v5236 = vadd.s32 %v4311, %v1381
%v5246 = vadd.s32 %v5236, %v415
%vm5250 = vcmp.lt.u32.totalorder %v5246, %v5236
%vm5255 = vcmp.lt.u32.totalorder %v5236, %v1381
%v5260 = vadd.s32 %v4294, %v1368
%v5264 = vadd.s32 1, %v5260
%v5268 = vsel /*vm=*/%vm5255, /*on_true_vy=*/%v5264, /*on_false_vx=*/%v5260
%v5272 = vadd.s32 1, %v5268
%v5276 = vsel /*vm=*/%vm5250, /*on_true_vy=*/%v5272, /*on_false_vx=*/%v5268
%v5281 = vadd.s32 %v5276, %v10
%v5285 = vadd.s32 %v5246, %v9
%v5289 = vadd.s32 %v5285, %v5281
%v5291 = vshll.u32 %v5285, 13
%v5292 = vshrl.u32 %v5285, 19
%v5293 = vor.u32 %v5292, %v5291
%v5294 = vxor.u32 %v5293, %v5289
%v5297 = vadd.s32 %v5294, %v5289
%v5299 = vshll.u32 %v5294, 15
%v5300 = vshrl.u32 %v5294, 17
%v5301 = vor.u32 %v5300, %v5299
%v5302 = vxor.u32 %v5301, %v5297
%v5305 = vadd.s32 %v5302, %v5297
%v5307 = vshll.u32 %v5302, 26
%v5308 = vshrl.u32 %v5302, 6
%v5309 = vor.u32 %v5308, %v5307
%v5310 = vxor.u32 %v5309, %v5305
%v5313 = vadd.s32 %v5310, %v5305
%v5317 = vadd.s32 %v5313, %v9
%v5319 = vshll.u32 %v5310, 6
%v5320 = vshrl.u32 %v5310, 26
%v5321 = vor.u32 %v5320, %v5319
%v5322 = vxor.u32 %v5321, %v5313
%v5325 = vadd.s32 %v5322, %v8
%v5329 = vadd.s32 1, %v5325
%v5333 = vadd.s32 %v5329, %v5317
%v5335 = vshll.u32 %v5329, 17
%v5336 = vshrl.u32 %v5329, 15
%v5337 = vor.u32 %v5336, %v5335
%v5338 = vxor.u32 %v5337, %v5333
%v5341 = vadd.s32 %v5338, %v5333
%v5343 = vshll.u32 %v5338, 29
%v5344 = vshrl.u32 %v5338, 3
%v5345 = vor.u32 %v5344, %v5343
%v5346 = vxor.u32 %v5345, %v5341
%v5349 = vadd.s32 %v5346, %v5341
%v5351 = vshll.u32 %v5346, 16
%v5352 = vshrl.u32 %v5346, 16
%v5353 = vor.u32 %v5352, %v5351
%v5354 = vxor.u32 %v5353, %v5349
%v5357 = vadd.s32 %v5354, %v5349
%v5361 = vadd.s32 %v5357, %v8
%v5363 = vshll.u32 %v5354, 24
%v5364 = vshrl.u32 %v5354, 8
%v5365 = vor.u32 %v5364, %v5363
%v5366 = vxor.u32 %v5365, %v5357
%v5369 = vadd.s32 %v5366, %v10
%v5373 = vadd.s32 2, %v5369
%v5377 = vadd.s32 %v5373, %v5361
%v5379 = vshll.u32 %v5373, 13
%v5380 = vshrl.u32 %v5373, 19
%v5381 = vor.u32 %v5380, %v5379
%v5382 = vxor.u32 %v5381, %v5377
%v5385 = vadd.s32 %v5382, %v5377
%v5387 = vshll.u32 %v5382, 15
%v5388 = vshrl.u32 %v5382, 17
%v5389 = vor.u32 %v5388, %v5387
%v5390 = vxor.u32 %v5389, %v5385
%v5393 = vadd.s32 %v5390, %v5385
%v5395 = vshll.u32 %v5390, 26
%v5396 = vshrl.u32 %v5390, 6
%v5397 = vor.u32 %v5396, %v5395
%v5398 = vxor.u32 %v5397, %v5393
%v5401 = vadd.s32 %v5398, %v5393
%v5405 = vadd.s32 %v5401, %v10
%v5407 = vshll.u32 %v5398, 6
%v5408 = vshrl.u32 %v5398, 26
%v5409 = vor.u32 %v5408, %v5407
%v5410 = vxor.u32 %v5409, %v5401
%v5413 = vadd.s32 %v5410, %v9
%v5417 = vadd.s32 3, %v5413
%v5421 = vadd.s32 %v5417, %v5405
%v5423 = vshll.u32 %v5417, 17
%v5424 = vshrl.u32 %v5417, 15
%v5425 = vor.u32 %v5424, %v5423
%v5426 = vxor.u32 %v5425, %v5421
%v5429 = vadd.s32 %v5426, %v5421
%v5431 = vshll.u32 %v5426, 29
%v5432 = vshrl.u32 %v5426, 3
%v5433 = vor.u32 %v5432, %v5431
%v5434 = vxor.u32 %v5433, %v5429
%v5437 = vadd.s32 %v5434, %v5429
%v5439 = vshll.u32 %v5434, 16
%v5440 = vshrl.u32 %v5434, 16
%v5441 = vor.u32 %v5440, %v5439
%v5442 = vxor.u32 %v5441, %v5437
%v5445 = vadd.s32 %v5442, %v5437
%v5449 = vadd.s32 %v5445, %v9
%v5451 = vshll.u32 %v5442, 24
%v5452 = vshrl.u32 %v5442, 8
%v5453 = vor.u32 %v5452, %v5451
%v5454 = vxor.u32 %v5453, %v5445
%v5457 = vadd.s32 %v5454, %v8
%v5461 = vadd.s32 4, %v5457
%v5465 = vadd.s32 %v5461, %v5449
%v5467 = vshll.u32 %v5461, 13
%v5468 = vshrl.u32 %v5461, 19
%v5469 = vor.u32 %v5468, %v5467
%v5470 = vxor.u32 %v5469, %v5465
%v5473 = vadd.s32 %v5470, %v5465
%v5475 = vshll.u32 %v5470, 15
%v5476 = vshrl.u32 %v5470, 17
%v5477 = vor.u32 %v5476, %v5475
%v5478 = vxor.u32 %v5477, %v5473
%v5481 = vadd.s32 %v5478, %v5473
%v5483 = vshll.u32 %v5478, 26
%v5484 = vshrl.u32 %v5478, 6
%v5485 = vor.u32 %v5484, %v5483
%v5486 = vxor.u32 %v5485, %v5481
%v5489 = vadd.s32 %v5486, %v5481
%v5493 = vadd.s32 %v5489, %v8
%v5495 = vshll.u32 %v5486, 6
%v5496 = vshrl.u32 %v5486, 26
%v5497 = vor.u32 %v5496, %v5495
%v5498 = vxor.u32 %v5497, %v5489
%v5501 = vadd.s32 %v5498, %v10
%v5505 = vadd.s32 5, %v5501
%v5507 = vxor.u32 %v5505, %v5493
%v5508 = vand.u32.u8 255, %v5507
%v5509 = vand.u32 65535, %v5508
%v5510 = vshrl.u32 %v5509, 1
%v5511 = vor.u32 16256, %v5510
%v5512 = vand.u32.u16 65535, %v5511
%v119772 = vadd.low.f32.bf16 -1.0, %v5512
%v5521 = vmul.f32 2.0, %v119772
%v5525 = vadd.f32 -0.99609375, %v5521
%v5529 = vmax.f32 %v5525, -0.99609375
%v5531 = vand.u32 2147483647, %v5529
%vm5534 = vcmp.eq.f32.partialorder %v5531, 1.0
%v5539 = vmul.f32 inf, %v5529
%v5541 = vxor.u32 2147483648, %v5529
%v5544 = vmul.f32 %v5541, %v5529
%v5546 = vadd.f32 1.0, %v5544
%v5547 = vlog2.pop %v5546
%v5548 = vmul.f32 0.6931472, %v5547
%v5549 = vmul.f32 -0.5, %v5544
%v5550 = vadd.f32 1.0, %v5549
%v5551 = vmul.f32 %v5550, %v5544
%v5552 = vand.u32 2147483647, %v5544
%vm5553 = vcmp.lt.f32.partialorder %v5552, 0.0004427343
%v5554 = vsel /*vm=*/%vm5553, /*on_true_vy=*/%v5551, /*on_false_vx=*/%v5548
%v5555 = vxor.u32 2147483648, %v5554
%vm5558 = vcmp.lt.f32.partialorder %v5555, 5.0
%v5563 = vsel /*vm=*/%vm5558, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v5567 = vsel /*vm=*/%vm5558, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v5571 = vsel /*vm=*/%vm5558, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v5575 = vsel /*vm=*/%vm5558, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v5579 = vsel /*vm=*/%vm5558, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v5583 = vsel /*vm=*/%vm5558, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v5587 = vsel /*vm=*/%vm5558, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v5591 = vsel /*vm=*/%vm5558, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v5595 = vsel /*vm=*/%vm5558, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v5599 = vadd.f32 -2.5, %v5555
%v5601 = vrsqrt.pop %v5555
%v5602 = vmul.f32 %v5601, %v5555
%vm5603 = vcmp.eq.f32.partialorder %v5555, inf
%v5604 = vsel /*vm=*/%vm5603, /*on_true_vy=*/%v5555, /*on_false_vx=*/%v5602
%vm5605 = vcmp.eq.f32.partialorder %v5555, 0.0
%v5606 = vand.u32 2147483648, %v5555
%v5607 = vsel /*vm=*/%vm5605, /*on_true_vy=*/%v5606, /*on_false_vx=*/%v5604
%v5610 = vadd.f32 -3.0, %v5607
%v5614 = vsel /*vm=*/%vm5558, /*on_true_vy=*/%v5599, /*on_false_vx=*/%v5610
%v5618 = vmul.f32 %v5614, %v5595
%v5622 = vadd.f32 %v5618, %v5591
%v5626 = vmul.f32 %v5622, %v5614
%v5630 = vadd.f32 %v5626, %v5587
%v5634 = vmul.f32 %v5630, %v5614
%v5638 = vadd.f32 %v5634, %v5583
%v5642 = vmul.f32 %v5638, %v5614
%v5646 = vadd.f32 %v5642, %v5579
%v5650 = vmul.f32 %v5646, %v5614
%v5654 = vadd.f32 %v5650, %v5575
%v5658 = vmul.f32 %v5654, %v5614
%v5662 = vadd.f32 %v5658, %v5571
%v5666 = vmul.f32 %v5662, %v5614
%v5670 = vadd.f32 %v5666, %v5567
%v5674 = vmul.f32 %v5670, %v5614
%v5678 = vadd.f32 %v5674, %v5563
%v5682 = vmul.f32 %v5678, %v5529
%v5686 = vsel /*vm=*/%vm5534, /*on_true_vy=*/%v5539, /*on_false_vx=*/%v5682
%v5690 = vmul.f32 1.4140625, %v5686
%v5693 = vpack.c.bf16 %v120417, %v5690
%119773 = vst [vmem:[%s280 + $0x104] sm:$0xf] /*vst_source=*/%v5693
%v5697 = vadd.s32 %v4311, %v1868
%v5707 = vadd.s32 %v5697, %v415
%vm5711 = vcmp.lt.u32.totalorder %v5707, %v5697
%vm5716 = vcmp.lt.u32.totalorder %v5697, %v1868
%v5721 = vadd.s32 %v4294, %v1855
%v5725 = vadd.s32 1, %v5721
%v5729 = vsel /*vm=*/%vm5716, /*on_true_vy=*/%v5725, /*on_false_vx=*/%v5721
%v5733 = vadd.s32 1, %v5729
%v5737 = vsel /*vm=*/%vm5711, /*on_true_vy=*/%v5733, /*on_false_vx=*/%v5729
%v5742 = vadd.s32 %v5737, %v10
%v5746 = vadd.s32 %v5707, %v9
%v5750 = vadd.s32 %v5746, %v5742
%v5752 = vshll.u32 %v5746, 13
%v5753 = vshrl.u32 %v5746, 19
%v5754 = vor.u32 %v5753, %v5752
%v5755 = vxor.u32 %v5754, %v5750
%v5758 = vadd.s32 %v5755, %v5750
%v5760 = vshll.u32 %v5755, 15
%v5761 = vshrl.u32 %v5755, 17
%v5762 = vor.u32 %v5761, %v5760
%v5763 = vxor.u32 %v5762, %v5758
%v5766 = vadd.s32 %v5763, %v5758
%v5768 = vshll.u32 %v5763, 26
%v5769 = vshrl.u32 %v5763, 6
%v5770 = vor.u32 %v5769, %v5768
%v5771 = vxor.u32 %v5770, %v5766
%v5774 = vadd.s32 %v5771, %v5766
%v5778 = vadd.s32 %v5774, %v9
%v5780 = vshll.u32 %v5771, 6
%v5781 = vshrl.u32 %v5771, 26
%v5782 = vor.u32 %v5781, %v5780
%v5783 = vxor.u32 %v5782, %v5774
%v5786 = vadd.s32 %v5783, %v8
%v5790 = vadd.s32 1, %v5786
%v5794 = vadd.s32 %v5790, %v5778
%v5796 = vshll.u32 %v5790, 17
%v5797 = vshrl.u32 %v5790, 15
%v5798 = vor.u32 %v5797, %v5796
%v5799 = vxor.u32 %v5798, %v5794
%v5802 = vadd.s32 %v5799, %v5794
%v5804 = vshll.u32 %v5799, 29
%v5805 = vshrl.u32 %v5799, 3
%v5806 = vor.u32 %v5805, %v5804
%v5807 = vxor.u32 %v5806, %v5802
%v5810 = vadd.s32 %v5807, %v5802
%v5812 = vshll.u32 %v5807, 16
%v5813 = vshrl.u32 %v5807, 16
%v5814 = vor.u32 %v5813, %v5812
%v5815 = vxor.u32 %v5814, %v5810
%v5818 = vadd.s32 %v5815, %v5810
%v5822 = vadd.s32 %v5818, %v8
%v5824 = vshll.u32 %v5815, 24
%v5825 = vshrl.u32 %v5815, 8
%v5826 = vor.u32 %v5825, %v5824
%v5827 = vxor.u32 %v5826, %v5818
%v5830 = vadd.s32 %v5827, %v10
%v5834 = vadd.s32 2, %v5830
%v5838 = vadd.s32 %v5834, %v5822
%v5840 = vshll.u32 %v5834, 13
%v5841 = vshrl.u32 %v5834, 19
%v5842 = vor.u32 %v5841, %v5840
%v5843 = vxor.u32 %v5842, %v5838
%v5846 = vadd.s32 %v5843, %v5838
%v5848 = vshll.u32 %v5843, 15
%v5849 = vshrl.u32 %v5843, 17
%v5850 = vor.u32 %v5849, %v5848
%v5851 = vxor.u32 %v5850, %v5846
%v5854 = vadd.s32 %v5851, %v5846
%v5856 = vshll.u32 %v5851, 26
%v5857 = vshrl.u32 %v5851, 6
%v5858 = vor.u32 %v5857, %v5856
%v5859 = vxor.u32 %v5858, %v5854
%v5862 = vadd.s32 %v5859, %v5854
%v5866 = vadd.s32 %v5862, %v10
%v5868 = vshll.u32 %v5859, 6
%v5869 = vshrl.u32 %v5859, 26
%v5870 = vor.u32 %v5869, %v5868
%v5871 = vxor.u32 %v5870, %v5862
%v5874 = vadd.s32 %v5871, %v9
%v5878 = vadd.s32 3, %v5874
%v5882 = vadd.s32 %v5878, %v5866
%v5884 = vshll.u32 %v5878, 17
%v5885 = vshrl.u32 %v5878, 15
%v5886 = vor.u32 %v5885, %v5884
%v5887 = vxor.u32 %v5886, %v5882
%v5890 = vadd.s32 %v5887, %v5882
%v5892 = vshll.u32 %v5887, 29
%v5893 = vshrl.u32 %v5887, 3
%v5894 = vor.u32 %v5893, %v5892
%v5895 = vxor.u32 %v5894, %v5890
%v5898 = vadd.s32 %v5895, %v5890
%v5900 = vshll.u32 %v5895, 16
%v5901 = vshrl.u32 %v5895, 16
%v5902 = vor.u32 %v5901, %v5900
%v5903 = vxor.u32 %v5902, %v5898
%v5906 = vadd.s32 %v5903, %v5898
%v5910 = vadd.s32 %v5906, %v9
%v5912 = vshll.u32 %v5903, 24
%v5913 = vshrl.u32 %v5903, 8
%v5914 = vor.u32 %v5913, %v5912
%v5915 = vxor.u32 %v5914, %v5906
%v5918 = vadd.s32 %v5915, %v8
%v5922 = vadd.s32 4, %v5918
%v5926 = vadd.s32 %v5922, %v5910
%v5928 = vshll.u32 %v5922, 13
%v5929 = vshrl.u32 %v5922, 19
%v5930 = vor.u32 %v5929, %v5928
%v5931 = vxor.u32 %v5930, %v5926
%v5934 = vadd.s32 %v5931, %v5926
%v5936 = vshll.u32 %v5931, 15
%v5937 = vshrl.u32 %v5931, 17
%v5938 = vor.u32 %v5937, %v5936
%v5939 = vxor.u32 %v5938, %v5934
%v5942 = vadd.s32 %v5939, %v5934
%v5944 = vshll.u32 %v5939, 26
%v5945 = vshrl.u32 %v5939, 6
%v5946 = vor.u32 %v5945, %v5944
%v5947 = vxor.u32 %v5946, %v5942
%v5950 = vadd.s32 %v5947, %v5942
%v5954 = vadd.s32 %v5950, %v8
%v5956 = vshll.u32 %v5947, 6
%v5957 = vshrl.u32 %v5947, 26
%v5958 = vor.u32 %v5957, %v5956
%v5959 = vxor.u32 %v5958, %v5950
%v5962 = vadd.s32 %v5959, %v10
%v5966 = vadd.s32 5, %v5962
%v5968 = vxor.u32 %v5966, %v5954
%v5969 = vand.u32.u8 255, %v5968
%v5970 = vand.u32 65535, %v5969
%v5971 = vshrl.u32 %v5970, 1
%v5972 = vor.u32 16256, %v5971
%v5973 = vand.u32.u16 65535, %v5972
%v119774 = vadd.low.f32.bf16 -1.0, %v5973
%v5982 = vmul.f32 2.0, %v119774
%v5986 = vadd.f32 -0.99609375, %v5982
%v5990 = vmax.f32 %v5986, -0.99609375
%v5992 = vand.u32 2147483647, %v5990
%vm5995 = vcmp.eq.f32.partialorder %v5992, 1.0
%v6000 = vmul.f32 inf, %v5990
%v6002 = vxor.u32 2147483648, %v5990
%v6005 = vmul.f32 %v6002, %v5990
%v6007 = vadd.f32 1.0, %v6005
%v6008 = vlog2.pop %v6007
%v6009 = vmul.f32 0.6931472, %v6008
%v6010 = vmul.f32 -0.5, %v6005
%v6011 = vadd.f32 1.0, %v6010
%v6012 = vmul.f32 %v6011, %v6005
%v6013 = vand.u32 2147483647, %v6005
%vm6014 = vcmp.lt.f32.partialorder %v6013, 0.0004427343
%v6015 = vsel /*vm=*/%vm6014, /*on_true_vy=*/%v6012, /*on_false_vx=*/%v6009
%v6016 = vxor.u32 2147483648, %v6015
%vm6019 = vcmp.lt.f32.partialorder %v6016, 5.0
%v6024 = vsel /*vm=*/%vm6019, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v6028 = vsel /*vm=*/%vm6019, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v6032 = vsel /*vm=*/%vm6019, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v6036 = vsel /*vm=*/%vm6019, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v6040 = vsel /*vm=*/%vm6019, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v6044 = vsel /*vm=*/%vm6019, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v6048 = vsel /*vm=*/%vm6019, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v6052 = vsel /*vm=*/%vm6019, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v6056 = vsel /*vm=*/%vm6019, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v6060 = vadd.f32 -2.5, %v6016
%v6062 = vrsqrt.pop %v6016
%v6063 = vmul.f32 %v6062, %v6016
%vm6064 = vcmp.eq.f32.partialorder %v6016, inf
%v6065 = vsel /*vm=*/%vm6064, /*on_true_vy=*/%v6016, /*on_false_vx=*/%v6063
%vm6066 = vcmp.eq.f32.partialorder %v6016, 0.0
%v6067 = vand.u32 2147483648, %v6016
%v6068 = vsel /*vm=*/%vm6066, /*on_true_vy=*/%v6067, /*on_false_vx=*/%v6065
%v6071 = vadd.f32 -3.0, %v6068
%v6075 = vsel /*vm=*/%vm6019, /*on_true_vy=*/%v6060, /*on_false_vx=*/%v6071
%v6079 = vmul.f32 %v6075, %v6056
%v6083 = vadd.f32 %v6079, %v6052
%v6087 = vmul.f32 %v6083, %v6075
%v6091 = vadd.f32 %v6087, %v6048
%v6095 = vmul.f32 %v6091, %v6075
%v6099 = vadd.f32 %v6095, %v6044
%v6103 = vmul.f32 %v6099, %v6075
%v6107 = vadd.f32 %v6103, %v6040
%v6111 = vmul.f32 %v6107, %v6075
%v6115 = vadd.f32 %v6111, %v6036
%v6119 = vmul.f32 %v6115, %v6075
%v6123 = vadd.f32 %v6119, %v6032
%v6127 = vmul.f32 %v6123, %v6075
%v6131 = vadd.f32 %v6127, %v6028
%v6135 = vmul.f32 %v6131, %v6075
%v6139 = vadd.f32 %v6135, %v6024
%v6143 = vmul.f32 %v6139, %v5990
%v6147 = vsel /*vm=*/%vm5995, /*on_true_vy=*/%v6000, /*on_false_vx=*/%v6143
%v6151 = vmul.f32 1.4140625, %v6147
%v6154 = vpack.c.bf16 %v120417, %v6151
%119775 = vst [vmem:[%s280 + $0x184] sm:$0xf] /*vst_source=*/%v6154
%v6158 = vadd.s32 %v4311, %v2355
%v6168 = vadd.s32 %v6158, %v415
%vm6172 = vcmp.lt.u32.totalorder %v6168, %v6158
%vm6177 = vcmp.lt.u32.totalorder %v6158, %v2355
%v6182 = vadd.s32 %v4294, %v2342
%v6186 = vadd.s32 1, %v6182
%v6190 = vsel /*vm=*/%vm6177, /*on_true_vy=*/%v6186, /*on_false_vx=*/%v6182
%v6194 = vadd.s32 1, %v6190
%v6198 = vsel /*vm=*/%vm6172, /*on_true_vy=*/%v6194, /*on_false_vx=*/%v6190
%v6203 = vadd.s32 %v6198, %v10
%v6207 = vadd.s32 %v6168, %v9
%v6211 = vadd.s32 %v6207, %v6203
%v6213 = vshll.u32 %v6207, 13
%v6214 = vshrl.u32 %v6207, 19
%v6215 = vor.u32 %v6214, %v6213
%v6216 = vxor.u32 %v6215, %v6211
%v6219 = vadd.s32 %v6216, %v6211
%v6221 = vshll.u32 %v6216, 15
%v6222 = vshrl.u32 %v6216, 17
%v6223 = vor.u32 %v6222, %v6221
%v6224 = vxor.u32 %v6223, %v6219
%v6227 = vadd.s32 %v6224, %v6219
%v6229 = vshll.u32 %v6224, 26
%v6230 = vshrl.u32 %v6224, 6
%v6231 = vor.u32 %v6230, %v6229
%v6232 = vxor.u32 %v6231, %v6227
%v6235 = vadd.s32 %v6232, %v6227
%v6239 = vadd.s32 %v6235, %v9
%v6241 = vshll.u32 %v6232, 6
%v6242 = vshrl.u32 %v6232, 26
%v6243 = vor.u32 %v6242, %v6241
%v6244 = vxor.u32 %v6243, %v6235
%v6247 = vadd.s32 %v6244, %v8
%v6251 = vadd.s32 1, %v6247
%v6255 = vadd.s32 %v6251, %v6239
%v6257 = vshll.u32 %v6251, 17
%v6258 = vshrl.u32 %v6251, 15
%v6259 = vor.u32 %v6258, %v6257
%v6260 = vxor.u32 %v6259, %v6255
%v6263 = vadd.s32 %v6260, %v6255
%v6265 = vshll.u32 %v6260, 29
%v6266 = vshrl.u32 %v6260, 3
%v6267 = vor.u32 %v6266, %v6265
%v6268 = vxor.u32 %v6267, %v6263
%v6271 = vadd.s32 %v6268, %v6263
%v6273 = vshll.u32 %v6268, 16
%v6274 = vshrl.u32 %v6268, 16
%v6275 = vor.u32 %v6274, %v6273
%v6276 = vxor.u32 %v6275, %v6271
%v6279 = vadd.s32 %v6276, %v6271
%v6283 = vadd.s32 %v6279, %v8
%v6285 = vshll.u32 %v6276, 24
%v6286 = vshrl.u32 %v6276, 8
%v6287 = vor.u32 %v6286, %v6285
%v6288 = vxor.u32 %v6287, %v6279
%v6291 = vadd.s32 %v6288, %v10
%v6295 = vadd.s32 2, %v6291
%v6299 = vadd.s32 %v6295, %v6283
%v6301 = vshll.u32 %v6295, 13
%v6302 = vshrl.u32 %v6295, 19
%v6303 = vor.u32 %v6302, %v6301
%v6304 = vxor.u32 %v6303, %v6299
%v6307 = vadd.s32 %v6304, %v6299
%v6309 = vshll.u32 %v6304, 15
%v6310 = vshrl.u32 %v6304, 17
%v6311 = vor.u32 %v6310, %v6309
%v6312 = vxor.u32 %v6311, %v6307
%v6315 = vadd.s32 %v6312, %v6307
%v6317 = vshll.u32 %v6312, 26
%v6318 = vshrl.u32 %v6312, 6
%v6319 = vor.u32 %v6318, %v6317
%v6320 = vxor.u32 %v6319, %v6315
%v6323 = vadd.s32 %v6320, %v6315
%v6327 = vadd.s32 %v6323, %v10
%v6329 = vshll.u32 %v6320, 6
%v6330 = vshrl.u32 %v6320, 26
%v6331 = vor.u32 %v6330, %v6329
%v6332 = vxor.u32 %v6331, %v6323
%v6335 = vadd.s32 %v6332, %v9
%v6339 = vadd.s32 3, %v6335
%v6343 = vadd.s32 %v6339, %v6327
%v6345 = vshll.u32 %v6339, 17
%v6346 = vshrl.u32 %v6339, 15
%v6347 = vor.u32 %v6346, %v6345
%v6348 = vxor.u32 %v6347, %v6343
%v6351 = vadd.s32 %v6348, %v6343
%v6353 = vshll.u32 %v6348, 29
%v6354 = vshrl.u32 %v6348, 3
%v6355 = vor.u32 %v6354, %v6353
%v6356 = vxor.u32 %v6355, %v6351
%v6359 = vadd.s32 %v6356, %v6351
%v6361 = vshll.u32 %v6356, 16
%v6362 = vshrl.u32 %v6356, 16
%v6363 = vor.u32 %v6362, %v6361
%v6364 = vxor.u32 %v6363, %v6359
%v6367 = vadd.s32 %v6364, %v6359
%v6371 = vadd.s32 %v6367, %v9
%v6373 = vshll.u32 %v6364, 24
%v6374 = vshrl.u32 %v6364, 8
%v6375 = vor.u32 %v6374, %v6373
%v6376 = vxor.u32 %v6375, %v6367
%v6379 = vadd.s32 %v6376, %v8
%v6383 = vadd.s32 4, %v6379
%v6387 = vadd.s32 %v6383, %v6371
%v6389 = vshll.u32 %v6383, 13
%v6390 = vshrl.u32 %v6383, 19
%v6391 = vor.u32 %v6390, %v6389
%v6392 = vxor.u32 %v6391, %v6387
%v6395 = vadd.s32 %v6392, %v6387
%v6397 = vshll.u32 %v6392, 15
%v6398 = vshrl.u32 %v6392, 17
%v6399 = vor.u32 %v6398, %v6397
%v6400 = vxor.u32 %v6399, %v6395
%v6403 = vadd.s32 %v6400, %v6395
%v6405 = vshll.u32 %v6400, 26
%v6406 = vshrl.u32 %v6400, 6
%v6407 = vor.u32 %v6406, %v6405
%v6408 = vxor.u32 %v6407, %v6403
%v6411 = vadd.s32 %v6408, %v6403
%v6415 = vadd.s32 %v6411, %v8
%v6417 = vshll.u32 %v6408, 6
%v6418 = vshrl.u32 %v6408, 26
%v6419 = vor.u32 %v6418, %v6417
%v6420 = vxor.u32 %v6419, %v6411
%v6423 = vadd.s32 %v6420, %v10
%v6427 = vadd.s32 5, %v6423
%v6429 = vxor.u32 %v6427, %v6415
%v6430 = vand.u32.u8 255, %v6429
%v6431 = vand.u32 65535, %v6430
%v6432 = vshrl.u32 %v6431, 1
%v6433 = vor.u32 16256, %v6432
%v6434 = vand.u32.u16 65535, %v6433
%v119776 = vadd.low.f32.bf16 -1.0, %v6434
%v6443 = vmul.f32 2.0, %v119776
%v6447 = vadd.f32 -0.99609375, %v6443
%v6451 = vmax.f32 %v6447, -0.99609375
%v6453 = vand.u32 2147483647, %v6451
%vm6456 = vcmp.eq.f32.partialorder %v6453, 1.0
%v6461 = vmul.f32 inf, %v6451
%v6463 = vxor.u32 2147483648, %v6451
%v6466 = vmul.f32 %v6463, %v6451
%v6468 = vadd.f32 1.0, %v6466
%v6469 = vlog2.pop %v6468
%v6470 = vmul.f32 0.6931472, %v6469
%v6471 = vmul.f32 -0.5, %v6466
%v6472 = vadd.f32 1.0, %v6471
%v6473 = vmul.f32 %v6472, %v6466
%v6474 = vand.u32 2147483647, %v6466
%vm6475 = vcmp.lt.f32.partialorder %v6474, 0.0004427343
%v6476 = vsel /*vm=*/%vm6475, /*on_true_vy=*/%v6473, /*on_false_vx=*/%v6470
%v6477 = vxor.u32 2147483648, %v6476
%vm6480 = vcmp.lt.f32.partialorder %v6477, 5.0
%v6485 = vsel /*vm=*/%vm6480, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v6489 = vsel /*vm=*/%vm6480, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v6493 = vsel /*vm=*/%vm6480, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v6497 = vsel /*vm=*/%vm6480, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v6501 = vsel /*vm=*/%vm6480, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v6505 = vsel /*vm=*/%vm6480, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v6509 = vsel /*vm=*/%vm6480, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v6513 = vsel /*vm=*/%vm6480, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v6517 = vsel /*vm=*/%vm6480, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v6521 = vadd.f32 -2.5, %v6477
%v6523 = vrsqrt.pop %v6477
%v6524 = vmul.f32 %v6523, %v6477
%vm6525 = vcmp.eq.f32.partialorder %v6477, inf
%v6526 = vsel /*vm=*/%vm6525, /*on_true_vy=*/%v6477, /*on_false_vx=*/%v6524
%vm6527 = vcmp.eq.f32.partialorder %v6477, 0.0
%v6528 = vand.u32 2147483648, %v6477
%v6529 = vsel /*vm=*/%vm6527, /*on_true_vy=*/%v6528, /*on_false_vx=*/%v6526
%v6532 = vadd.f32 -3.0, %v6529
%v6536 = vsel /*vm=*/%vm6480, /*on_true_vy=*/%v6521, /*on_false_vx=*/%v6532
%v6540 = vmul.f32 %v6536, %v6517
%v6544 = vadd.f32 %v6540, %v6513
%v6548 = vmul.f32 %v6544, %v6536
%v6552 = vadd.f32 %v6548, %v6509
%v6556 = vmul.f32 %v6552, %v6536
%v6560 = vadd.f32 %v6556, %v6505
%v6564 = vmul.f32 %v6560, %v6536
%v6568 = vadd.f32 %v6564, %v6501
%v6572 = vmul.f32 %v6568, %v6536
%v6576 = vadd.f32 %v6572, %v6497
%v6580 = vmul.f32 %v6576, %v6536
%v6584 = vadd.f32 %v6580, %v6493
%v6588 = vmul.f32 %v6584, %v6536
%v6592 = vadd.f32 %v6588, %v6489
%v6596 = vmul.f32 %v6592, %v6536
%v6600 = vadd.f32 %v6596, %v6485
%v6604 = vmul.f32 %v6600, %v6451
%v6608 = vsel /*vm=*/%vm6456, /*on_true_vy=*/%v6461, /*on_false_vx=*/%v6604
%v6612 = vmul.f32 1.4140625, %v6608
%v6615 = vpack.c.bf16 %v120417, %v6612
%119777 = vst [vmem:[%s280 + $0x204] sm:$0xf] /*vst_source=*/%v6615
%v6619 = vadd.s32 %v4311, %v2842
%v6629 = vadd.s32 %v6619, %v415
%vm6633 = vcmp.lt.u32.totalorder %v6629, %v6619
%vm6638 = vcmp.lt.u32.totalorder %v6619, %v2842
%v6643 = vadd.s32 %v4294, %v2829
%v6647 = vadd.s32 1, %v6643
%v6651 = vsel /*vm=*/%vm6638, /*on_true_vy=*/%v6647, /*on_false_vx=*/%v6643
%v6655 = vadd.s32 1, %v6651
%v6659 = vsel /*vm=*/%vm6633, /*on_true_vy=*/%v6655, /*on_false_vx=*/%v6651
%v6664 = vadd.s32 %v6659, %v10
%v6668 = vadd.s32 %v6629, %v9
%v6672 = vadd.s32 %v6668, %v6664
%v6674 = vshll.u32 %v6668, 13
%v6675 = vshrl.u32 %v6668, 19
%v6676 = vor.u32 %v6675, %v6674
%v6677 = vxor.u32 %v6676, %v6672
%v6680 = vadd.s32 %v6677, %v6672
%v6682 = vshll.u32 %v6677, 15
%v6683 = vshrl.u32 %v6677, 17
%v6684 = vor.u32 %v6683, %v6682
%v6685 = vxor.u32 %v6684, %v6680
%v6688 = vadd.s32 %v6685, %v6680
%v6690 = vshll.u32 %v6685, 26
%v6691 = vshrl.u32 %v6685, 6
%v6692 = vor.u32 %v6691, %v6690
%v6693 = vxor.u32 %v6692, %v6688
%v6696 = vadd.s32 %v6693, %v6688
%v6700 = vadd.s32 %v6696, %v9
%v6702 = vshll.u32 %v6693, 6
%v6703 = vshrl.u32 %v6693, 26
%v6704 = vor.u32 %v6703, %v6702
%v6705 = vxor.u32 %v6704, %v6696
%v6708 = vadd.s32 %v6705, %v8
%v6712 = vadd.s32 1, %v6708
%v6716 = vadd.s32 %v6712, %v6700
%v6718 = vshll.u32 %v6712, 17
%v6719 = vshrl.u32 %v6712, 15
%v6720 = vor.u32 %v6719, %v6718
%v6721 = vxor.u32 %v6720, %v6716
%v6724 = vadd.s32 %v6721, %v6716
%v6726 = vshll.u32 %v6721, 29
%v6727 = vshrl.u32 %v6721, 3
%v6728 = vor.u32 %v6727, %v6726
%v6729 = vxor.u32 %v6728, %v6724
%v6732 = vadd.s32 %v6729, %v6724
%v6734 = vshll.u32 %v6729, 16
%v6735 = vshrl.u32 %v6729, 16
%v6736 = vor.u32 %v6735, %v6734
%v6737 = vxor.u32 %v6736, %v6732
%v6740 = vadd.s32 %v6737, %v6732
%v6744 = vadd.s32 %v6740, %v8
%v6746 = vshll.u32 %v6737, 24
%v6747 = vshrl.u32 %v6737, 8
%v6748 = vor.u32 %v6747, %v6746
%v6749 = vxor.u32 %v6748, %v6740
%v6752 = vadd.s32 %v6749, %v10
%v6756 = vadd.s32 2, %v6752
%v6760 = vadd.s32 %v6756, %v6744
%v6762 = vshll.u32 %v6756, 13
%v6763 = vshrl.u32 %v6756, 19
%v6764 = vor.u32 %v6763, %v6762
%v6765 = vxor.u32 %v6764, %v6760
%v6768 = vadd.s32 %v6765, %v6760
%v6770 = vshll.u32 %v6765, 15
%v6771 = vshrl.u32 %v6765, 17
%v6772 = vor.u32 %v6771, %v6770
%v6773 = vxor.u32 %v6772, %v6768
%v6776 = vadd.s32 %v6773, %v6768
%v6778 = vshll.u32 %v6773, 26
%v6779 = vshrl.u32 %v6773, 6
%v6780 = vor.u32 %v6779, %v6778
%v6781 = vxor.u32 %v6780, %v6776
%v6784 = vadd.s32 %v6781, %v6776
%v6788 = vadd.s32 %v6784, %v10
%v6790 = vshll.u32 %v6781, 6
%v6791 = vshrl.u32 %v6781, 26
%v6792 = vor.u32 %v6791, %v6790
%v6793 = vxor.u32 %v6792, %v6784
%v6796 = vadd.s32 %v6793, %v9
%v6800 = vadd.s32 3, %v6796
%v6804 = vadd.s32 %v6800, %v6788
%v6806 = vshll.u32 %v6800, 17
%v6807 = vshrl.u32 %v6800, 15
%v6808 = vor.u32 %v6807, %v6806
%v6809 = vxor.u32 %v6808, %v6804
%v6812 = vadd.s32 %v6809, %v6804
%v6814 = vshll.u32 %v6809, 29
%v6815 = vshrl.u32 %v6809, 3
%v6816 = vor.u32 %v6815, %v6814
%v6817 = vxor.u32 %v6816, %v6812
%v6820 = vadd.s32 %v6817, %v6812
%v6822 = vshll.u32 %v6817, 16
%v6823 = vshrl.u32 %v6817, 16
%v6824 = vor.u32 %v6823, %v6822
%v6825 = vxor.u32 %v6824, %v6820
%v6828 = vadd.s32 %v6825, %v6820
%v6832 = vadd.s32 %v6828, %v9
%v6834 = vshll.u32 %v6825, 24
%v6835 = vshrl.u32 %v6825, 8
%v6836 = vor.u32 %v6835, %v6834
%v6837 = vxor.u32 %v6836, %v6828
%v6840 = vadd.s32 %v6837, %v8
%v6844 = vadd.s32 4, %v6840
%v6848 = vadd.s32 %v6844, %v6832
%v6850 = vshll.u32 %v6844, 13
%v6851 = vshrl.u32 %v6844, 19
%v6852 = vor.u32 %v6851, %v6850
%v6853 = vxor.u32 %v6852, %v6848
%v6856 = vadd.s32 %v6853, %v6848
%v6858 = vshll.u32 %v6853, 15
%v6859 = vshrl.u32 %v6853, 17
%v6860 = vor.u32 %v6859, %v6858
%v6861 = vxor.u32 %v6860, %v6856
%v6864 = vadd.s32 %v6861, %v6856
%v6866 = vshll.u32 %v6861, 26
%v6867 = vshrl.u32 %v6861, 6
%v6868 = vor.u32 %v6867, %v6866
%v6869 = vxor.u32 %v6868, %v6864
%v6872 = vadd.s32 %v6869, %v6864
%v6876 = vadd.s32 %v6872, %v8
%v6878 = vshll.u32 %v6869, 6
%v6879 = vshrl.u32 %v6869, 26
%v6880 = vor.u32 %v6879, %v6878
%v6881 = vxor.u32 %v6880, %v6872
%v6884 = vadd.s32 %v6881, %v10
%v6888 = vadd.s32 5, %v6884
%v6890 = vxor.u32 %v6888, %v6876
%v6891 = vand.u32.u8 255, %v6890
%v6892 = vand.u32 65535, %v6891
%v6893 = vshrl.u32 %v6892, 1
%v6894 = vor.u32 16256, %v6893
%v6895 = vand.u32.u16 65535, %v6894
%v119778 = vadd.low.f32.bf16 -1.0, %v6895
%v6904 = vmul.f32 2.0, %v119778
%v6908 = vadd.f32 -0.99609375, %v6904
%v6912 = vmax.f32 %v6908, -0.99609375
%v6914 = vand.u32 2147483647, %v6912
%vm6917 = vcmp.eq.f32.partialorder %v6914, 1.0
%v6922 = vmul.f32 inf, %v6912
%v6924 = vxor.u32 2147483648, %v6912
%v6927 = vmul.f32 %v6924, %v6912
%v6929 = vadd.f32 1.0, %v6927
%v6930 = vlog2.pop %v6929
%v6931 = vmul.f32 0.6931472, %v6930
%v6932 = vmul.f32 -0.5, %v6927
%v6933 = vadd.f32 1.0, %v6932
%v6934 = vmul.f32 %v6933, %v6927
%v6935 = vand.u32 2147483647, %v6927
%vm6936 = vcmp.lt.f32.partialorder %v6935, 0.0004427343
%v6937 = vsel /*vm=*/%vm6936, /*on_true_vy=*/%v6934, /*on_false_vx=*/%v6931
%v6938 = vxor.u32 2147483648, %v6937
%vm6941 = vcmp.lt.f32.partialorder %v6938, 5.0
%v6946 = vsel /*vm=*/%vm6941, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v6950 = vsel /*vm=*/%vm6941, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v6954 = vsel /*vm=*/%vm6941, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v6958 = vsel /*vm=*/%vm6941, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v6962 = vsel /*vm=*/%vm6941, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v6966 = vsel /*vm=*/%vm6941, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v6970 = vsel /*vm=*/%vm6941, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v6974 = vsel /*vm=*/%vm6941, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v6978 = vsel /*vm=*/%vm6941, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v6982 = vadd.f32 -2.5, %v6938
%v6984 = vrsqrt.pop %v6938
%v6985 = vmul.f32 %v6984, %v6938
%vm6986 = vcmp.eq.f32.partialorder %v6938, inf
%v6987 = vsel /*vm=*/%vm6986, /*on_true_vy=*/%v6938, /*on_false_vx=*/%v6985
%vm6988 = vcmp.eq.f32.partialorder %v6938, 0.0
%v6989 = vand.u32 2147483648, %v6938
%v6990 = vsel /*vm=*/%vm6988, /*on_true_vy=*/%v6989, /*on_false_vx=*/%v6987
%v6993 = vadd.f32 -3.0, %v6990
%v6997 = vsel /*vm=*/%vm6941, /*on_true_vy=*/%v6982, /*on_false_vx=*/%v6993
%v7001 = vmul.f32 %v6997, %v6978
%v7005 = vadd.f32 %v7001, %v6974
%v7009 = vmul.f32 %v7005, %v6997
%v7013 = vadd.f32 %v7009, %v6970
%v7017 = vmul.f32 %v7013, %v6997
%v7021 = vadd.f32 %v7017, %v6966
%v7025 = vmul.f32 %v7021, %v6997
%v7029 = vadd.f32 %v7025, %v6962
%v7033 = vmul.f32 %v7029, %v6997
%v7037 = vadd.f32 %v7033, %v6958
%v7041 = vmul.f32 %v7037, %v6997
%v7045 = vadd.f32 %v7041, %v6954
%v7049 = vmul.f32 %v7045, %v6997
%v7053 = vadd.f32 %v7049, %v6950
%v7057 = vmul.f32 %v7053, %v6997
%v7061 = vadd.f32 %v7057, %v6946
%v7065 = vmul.f32 %v7061, %v6912
%v7069 = vsel /*vm=*/%vm6917, /*on_true_vy=*/%v6922, /*on_false_vx=*/%v7065
%v7073 = vmul.f32 1.4140625, %v7069
%v7076 = vpack.c.bf16 %v120417, %v7073
%119779 = vst [vmem:[%s280 + $0x284] sm:$0xf] /*vst_source=*/%v7076
%v7080 = vadd.s32 %v4311, %v3329
%v7090 = vadd.s32 %v7080, %v415
%vm7094 = vcmp.lt.u32.totalorder %v7090, %v7080
%vm7099 = vcmp.lt.u32.totalorder %v7080, %v3329
%v7104 = vadd.s32 %v4294, %v3316
%v7108 = vadd.s32 1, %v7104
%v7112 = vsel /*vm=*/%vm7099, /*on_true_vy=*/%v7108, /*on_false_vx=*/%v7104
%v7116 = vadd.s32 1, %v7112
%v7120 = vsel /*vm=*/%vm7094, /*on_true_vy=*/%v7116, /*on_false_vx=*/%v7112
%v7125 = vadd.s32 %v7120, %v10
%v7129 = vadd.s32 %v7090, %v9
%v7133 = vadd.s32 %v7129, %v7125
%v7135 = vshll.u32 %v7129, 13
%v7136 = vshrl.u32 %v7129, 19
%v7137 = vor.u32 %v7136, %v7135
%v7138 = vxor.u32 %v7137, %v7133
%v7141 = vadd.s32 %v7138, %v7133
%v7143 = vshll.u32 %v7138, 15
%v7144 = vshrl.u32 %v7138, 17
%v7145 = vor.u32 %v7144, %v7143
%v7146 = vxor.u32 %v7145, %v7141
%v7149 = vadd.s32 %v7146, %v7141
%v7151 = vshll.u32 %v7146, 26
%v7152 = vshrl.u32 %v7146, 6
%v7153 = vor.u32 %v7152, %v7151
%v7154 = vxor.u32 %v7153, %v7149
%v7157 = vadd.s32 %v7154, %v7149
%v7161 = vadd.s32 %v7157, %v9
%v7163 = vshll.u32 %v7154, 6
%v7164 = vshrl.u32 %v7154, 26
%v7165 = vor.u32 %v7164, %v7163
%v7166 = vxor.u32 %v7165, %v7157
%v7169 = vadd.s32 %v7166, %v8
%v7173 = vadd.s32 1, %v7169
%v7177 = vadd.s32 %v7173, %v7161
%v7179 = vshll.u32 %v7173, 17
%v7180 = vshrl.u32 %v7173, 15
%v7181 = vor.u32 %v7180, %v7179
%v7182 = vxor.u32 %v7181, %v7177
%v7185 = vadd.s32 %v7182, %v7177
%v7187 = vshll.u32 %v7182, 29
%v7188 = vshrl.u32 %v7182, 3
%v7189 = vor.u32 %v7188, %v7187
%v7190 = vxor.u32 %v7189, %v7185
%v7193 = vadd.s32 %v7190, %v7185
%v7195 = vshll.u32 %v7190, 16
%v7196 = vshrl.u32 %v7190, 16
%v7197 = vor.u32 %v7196, %v7195
%v7198 = vxor.u32 %v7197, %v7193
%v7201 = vadd.s32 %v7198, %v7193
%v7205 = vadd.s32 %v7201, %v8
%v7207 = vshll.u32 %v7198, 24
%v7208 = vshrl.u32 %v7198, 8
%v7209 = vor.u32 %v7208, %v7207
%v7210 = vxor.u32 %v7209, %v7201
%v7213 = vadd.s32 %v7210, %v10
%v7217 = vadd.s32 2, %v7213
%v7221 = vadd.s32 %v7217, %v7205
%v7223 = vshll.u32 %v7217, 13
%v7224 = vshrl.u32 %v7217, 19
%v7225 = vor.u32 %v7224, %v7223
%v7226 = vxor.u32 %v7225, %v7221
%v7229 = vadd.s32 %v7226, %v7221
%v7231 = vshll.u32 %v7226, 15
%v7232 = vshrl.u32 %v7226, 17
%v7233 = vor.u32 %v7232, %v7231
%v7234 = vxor.u32 %v7233, %v7229
%v7237 = vadd.s32 %v7234, %v7229
%v7239 = vshll.u32 %v7234, 26
%v7240 = vshrl.u32 %v7234, 6
%v7241 = vor.u32 %v7240, %v7239
%v7242 = vxor.u32 %v7241, %v7237
%v7245 = vadd.s32 %v7242, %v7237
%v7249 = vadd.s32 %v7245, %v10
%v7251 = vshll.u32 %v7242, 6
%v7252 = vshrl.u32 %v7242, 26
%v7253 = vor.u32 %v7252, %v7251
%v7254 = vxor.u32 %v7253, %v7245
%v7257 = vadd.s32 %v7254, %v9
%v7261 = vadd.s32 3, %v7257
%v7265 = vadd.s32 %v7261, %v7249
%v7267 = vshll.u32 %v7261, 17
%v7268 = vshrl.u32 %v7261, 15
%v7269 = vor.u32 %v7268, %v7267
%v7270 = vxor.u32 %v7269, %v7265
%v7273 = vadd.s32 %v7270, %v7265
%v7275 = vshll.u32 %v7270, 29
%v7276 = vshrl.u32 %v7270, 3
%v7277 = vor.u32 %v7276, %v7275
%v7278 = vxor.u32 %v7277, %v7273
%v7281 = vadd.s32 %v7278, %v7273
%v7283 = vshll.u32 %v7278, 16
%v7284 = vshrl.u32 %v7278, 16
%v7285 = vor.u32 %v7284, %v7283
%v7286 = vxor.u32 %v7285, %v7281
%v7289 = vadd.s32 %v7286, %v7281
%v7293 = vadd.s32 %v7289, %v9
%v7295 = vshll.u32 %v7286, 24
%v7296 = vshrl.u32 %v7286, 8
%v7297 = vor.u32 %v7296, %v7295
%v7298 = vxor.u32 %v7297, %v7289
%v7301 = vadd.s32 %v7298, %v8
%v7305 = vadd.s32 4, %v7301
%v7309 = vadd.s32 %v7305, %v7293
%v7311 = vshll.u32 %v7305, 13
%v7312 = vshrl.u32 %v7305, 19
%v7313 = vor.u32 %v7312, %v7311
%v7314 = vxor.u32 %v7313, %v7309
%v7317 = vadd.s32 %v7314, %v7309
%v7319 = vshll.u32 %v7314, 15
%v7320 = vshrl.u32 %v7314, 17
%v7321 = vor.u32 %v7320, %v7319
%v7322 = vxor.u32 %v7321, %v7317
%v7325 = vadd.s32 %v7322, %v7317
%v7327 = vshll.u32 %v7322, 26
%v7328 = vshrl.u32 %v7322, 6
%v7329 = vor.u32 %v7328, %v7327
%v7330 = vxor.u32 %v7329, %v7325
%v7333 = vadd.s32 %v7330, %v7325
%v7337 = vadd.s32 %v7333, %v8
%v7339 = vshll.u32 %v7330, 6
%v7340 = vshrl.u32 %v7330, 26
%v7341 = vor.u32 %v7340, %v7339
%v7342 = vxor.u32 %v7341, %v7333
%v7345 = vadd.s32 %v7342, %v10
%v7349 = vadd.s32 5, %v7345
%v7351 = vxor.u32 %v7349, %v7337
%v7352 = vand.u32.u8 255, %v7351
%v7353 = vand.u32 65535, %v7352
%v7354 = vshrl.u32 %v7353, 1
%v7355 = vor.u32 16256, %v7354
%v7356 = vand.u32.u16 65535, %v7355
%v119780 = vadd.low.f32.bf16 -1.0, %v7356
%v7365 = vmul.f32 2.0, %v119780
%v7369 = vadd.f32 -0.99609375, %v7365
%v7373 = vmax.f32 %v7369, -0.99609375
%v7375 = vand.u32 2147483647, %v7373
%vm7378 = vcmp.eq.f32.partialorder %v7375, 1.0
%v7383 = vmul.f32 inf, %v7373
%v7385 = vxor.u32 2147483648, %v7373
%v7388 = vmul.f32 %v7385, %v7373
%v7390 = vadd.f32 1.0, %v7388
%v7391 = vlog2.pop %v7390
%v7392 = vmul.f32 0.6931472, %v7391
%v7393 = vmul.f32 -0.5, %v7388
%v7394 = vadd.f32 1.0, %v7393
%v7395 = vmul.f32 %v7394, %v7388
%v7396 = vand.u32 2147483647, %v7388
%vm7397 = vcmp.lt.f32.partialorder %v7396, 0.0004427343
%v7398 = vsel /*vm=*/%vm7397, /*on_true_vy=*/%v7395, /*on_false_vx=*/%v7392
%v7399 = vxor.u32 2147483648, %v7398
%vm7402 = vcmp.lt.f32.partialorder %v7399, 5.0
%v7407 = vsel /*vm=*/%vm7402, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v7411 = vsel /*vm=*/%vm7402, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v7415 = vsel /*vm=*/%vm7402, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v7419 = vsel /*vm=*/%vm7402, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v7423 = vsel /*vm=*/%vm7402, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v7427 = vsel /*vm=*/%vm7402, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v7431 = vsel /*vm=*/%vm7402, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v7435 = vsel /*vm=*/%vm7402, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v7439 = vsel /*vm=*/%vm7402, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v7443 = vadd.f32 -2.5, %v7399
%v7445 = vrsqrt.pop %v7399
%v7446 = vmul.f32 %v7445, %v7399
%vm7447 = vcmp.eq.f32.partialorder %v7399, inf
%v7448 = vsel /*vm=*/%vm7447, /*on_true_vy=*/%v7399, /*on_false_vx=*/%v7446
%vm7449 = vcmp.eq.f32.partialorder %v7399, 0.0
%v7450 = vand.u32 2147483648, %v7399
%v7451 = vsel /*vm=*/%vm7449, /*on_true_vy=*/%v7450, /*on_false_vx=*/%v7448
%v7454 = vadd.f32 -3.0, %v7451
%v7458 = vsel /*vm=*/%vm7402, /*on_true_vy=*/%v7443, /*on_false_vx=*/%v7454
%v7462 = vmul.f32 %v7458, %v7439
%v7466 = vadd.f32 %v7462, %v7435
%v7470 = vmul.f32 %v7466, %v7458
%v7474 = vadd.f32 %v7470, %v7431
%v7478 = vmul.f32 %v7474, %v7458
%v7482 = vadd.f32 %v7478, %v7427
%v7486 = vmul.f32 %v7482, %v7458
%v7490 = vadd.f32 %v7486, %v7423
%v7494 = vmul.f32 %v7490, %v7458
%v7498 = vadd.f32 %v7494, %v7419
%v7502 = vmul.f32 %v7498, %v7458
%v7506 = vadd.f32 %v7502, %v7415
%v7510 = vmul.f32 %v7506, %v7458
%v7514 = vadd.f32 %v7510, %v7411
%v7518 = vmul.f32 %v7514, %v7458
%v7522 = vadd.f32 %v7518, %v7407
%v7526 = vmul.f32 %v7522, %v7373
%v7530 = vsel /*vm=*/%vm7378, /*on_true_vy=*/%v7383, /*on_false_vx=*/%v7526
%v7534 = vmul.f32 1.4140625, %v7530
%v7537 = vpack.c.bf16 %v120417, %v7534
%119781 = vst [vmem:[%s280 + $0x304] sm:$0xf] /*vst_source=*/%v7537
%v7541 = vadd.s32 %v4311, %v3816
%v7551 = vadd.s32 %v7541, %v415
%vm7555 = vcmp.lt.u32.totalorder %v7551, %v7541
%vm7560 = vcmp.lt.u32.totalorder %v7541, %v3816
%v7565 = vadd.s32 %v4294, %v3803
%v7569 = vadd.s32 1, %v7565
%v7573 = vsel /*vm=*/%vm7560, /*on_true_vy=*/%v7569, /*on_false_vx=*/%v7565
%v7577 = vadd.s32 1, %v7573
%v7581 = vsel /*vm=*/%vm7555, /*on_true_vy=*/%v7577, /*on_false_vx=*/%v7573
%v7586 = vadd.s32 %v7581, %v10
%v7590 = vadd.s32 %v7551, %v9
%v7594 = vadd.s32 %v7590, %v7586
%v7596 = vshll.u32 %v7590, 13
%v7597 = vshrl.u32 %v7590, 19
%v7598 = vor.u32 %v7597, %v7596
%v7599 = vxor.u32 %v7598, %v7594
%v7602 = vadd.s32 %v7599, %v7594
%v7604 = vshll.u32 %v7599, 15
%v7605 = vshrl.u32 %v7599, 17
%v7606 = vor.u32 %v7605, %v7604
%v7607 = vxor.u32 %v7606, %v7602
%v7610 = vadd.s32 %v7607, %v7602
%v7612 = vshll.u32 %v7607, 26
%v7613 = vshrl.u32 %v7607, 6
%v7614 = vor.u32 %v7613, %v7612
%v7615 = vxor.u32 %v7614, %v7610
%v7618 = vadd.s32 %v7615, %v7610
%v7622 = vadd.s32 %v7618, %v9
%v7624 = vshll.u32 %v7615, 6
%v7625 = vshrl.u32 %v7615, 26
%v7626 = vor.u32 %v7625, %v7624
%v7627 = vxor.u32 %v7626, %v7618
%v7630 = vadd.s32 %v7627, %v8
%v7634 = vadd.s32 1, %v7630
%v7638 = vadd.s32 %v7634, %v7622
%v7640 = vshll.u32 %v7634, 17
%v7641 = vshrl.u32 %v7634, 15
%v7642 = vor.u32 %v7641, %v7640
%v7643 = vxor.u32 %v7642, %v7638
%v7646 = vadd.s32 %v7643, %v7638
%v7648 = vshll.u32 %v7643, 29
%v7649 = vshrl.u32 %v7643, 3
%v7650 = vor.u32 %v7649, %v7648
%v7651 = vxor.u32 %v7650, %v7646
%v7654 = vadd.s32 %v7651, %v7646
%v7656 = vshll.u32 %v7651, 16
%v7657 = vshrl.u32 %v7651, 16
%v7658 = vor.u32 %v7657, %v7656
%v7659 = vxor.u32 %v7658, %v7654
%v7662 = vadd.s32 %v7659, %v7654
%v7666 = vadd.s32 %v7662, %v8
%v7668 = vshll.u32 %v7659, 24
%v7669 = vshrl.u32 %v7659, 8
%v7670 = vor.u32 %v7669, %v7668
%v7671 = vxor.u32 %v7670, %v7662
%v7674 = vadd.s32 %v7671, %v10
%v7678 = vadd.s32 2, %v7674
%v7682 = vadd.s32 %v7678, %v7666
%v7684 = vshll.u32 %v7678, 13
%v7685 = vshrl.u32 %v7678, 19
%v7686 = vor.u32 %v7685, %v7684
%v7687 = vxor.u32 %v7686, %v7682
%v7690 = vadd.s32 %v7687, %v7682
%v7692 = vshll.u32 %v7687, 15
%v7693 = vshrl.u32 %v7687, 17
%v7694 = vor.u32 %v7693, %v7692
%v7695 = vxor.u32 %v7694, %v7690
%v7698 = vadd.s32 %v7695, %v7690
%v7700 = vshll.u32 %v7695, 26
%v7701 = vshrl.u32 %v7695, 6
%v7702 = vor.u32 %v7701, %v7700
%v7703 = vxor.u32 %v7702, %v7698
%v7706 = vadd.s32 %v7703, %v7698
%v7710 = vadd.s32 %v7706, %v10
%v7712 = vshll.u32 %v7703, 6
%v7713 = vshrl.u32 %v7703, 26
%v7714 = vor.u32 %v7713, %v7712
%v7715 = vxor.u32 %v7714, %v7706
%v7718 = vadd.s32 %v7715, %v9
%v7722 = vadd.s32 3, %v7718
%v7726 = vadd.s32 %v7722, %v7710
%v7728 = vshll.u32 %v7722, 17
%v7729 = vshrl.u32 %v7722, 15
%v7730 = vor.u32 %v7729, %v7728
%v7731 = vxor.u32 %v7730, %v7726
%v7734 = vadd.s32 %v7731, %v7726
%v7736 = vshll.u32 %v7731, 29
%v7737 = vshrl.u32 %v7731, 3
%v7738 = vor.u32 %v7737, %v7736
%v7739 = vxor.u32 %v7738, %v7734
%v7742 = vadd.s32 %v7739, %v7734
%v7744 = vshll.u32 %v7739, 16
%v7745 = vshrl.u32 %v7739, 16
%v7746 = vor.u32 %v7745, %v7744
%v7747 = vxor.u32 %v7746, %v7742
%v7750 = vadd.s32 %v7747, %v7742
%v7754 = vadd.s32 %v7750, %v9
%v7756 = vshll.u32 %v7747, 24
%v7757 = vshrl.u32 %v7747, 8
%v7758 = vor.u32 %v7757, %v7756
%v7759 = vxor.u32 %v7758, %v7750
%v7762 = vadd.s32 %v7759, %v8
%v7766 = vadd.s32 4, %v7762
%v7770 = vadd.s32 %v7766, %v7754
%v7772 = vshll.u32 %v7766, 13
%v7773 = vshrl.u32 %v7766, 19
%v7774 = vor.u32 %v7773, %v7772
%v7775 = vxor.u32 %v7774, %v7770
%v7778 = vadd.s32 %v7775, %v7770
%v7780 = vshll.u32 %v7775, 15
%v7781 = vshrl.u32 %v7775, 17
%v7782 = vor.u32 %v7781, %v7780
%v7783 = vxor.u32 %v7782, %v7778
%v7786 = vadd.s32 %v7783, %v7778
%v7788 = vshll.u32 %v7783, 26
%v7789 = vshrl.u32 %v7783, 6
%v7790 = vor.u32 %v7789, %v7788
%v7791 = vxor.u32 %v7790, %v7786
%v7794 = vadd.s32 %v7791, %v7786
%v7798 = vadd.s32 %v7794, %v8
%v7800 = vshll.u32 %v7791, 6
%v7801 = vshrl.u32 %v7791, 26
%v7802 = vor.u32 %v7801, %v7800
%v7803 = vxor.u32 %v7802, %v7794
%v7806 = vadd.s32 %v7803, %v10
%v7810 = vadd.s32 5, %v7806
%v7812 = vxor.u32 %v7810, %v7798
%v7813 = vand.u32.u8 255, %v7812
%v7814 = vand.u32 65535, %v7813
%v7815 = vshrl.u32 %v7814, 1
%v7816 = vor.u32 16256, %v7815
%v7817 = vand.u32.u16 65535, %v7816
%v119782 = vadd.low.f32.bf16 -1.0, %v7817
%v7826 = vmul.f32 2.0, %v119782
%v7830 = vadd.f32 -0.99609375, %v7826
%v7834 = vmax.f32 %v7830, -0.99609375
%v7836 = vand.u32 2147483647, %v7834
%vm7839 = vcmp.eq.f32.partialorder %v7836, 1.0
%v7844 = vmul.f32 inf, %v7834
%v7846 = vxor.u32 2147483648, %v7834
%v7849 = vmul.f32 %v7846, %v7834
%v7851 = vadd.f32 1.0, %v7849
%v7852 = vlog2.pop %v7851
%v7853 = vmul.f32 0.6931472, %v7852
%v7854 = vmul.f32 -0.5, %v7849
%v7855 = vadd.f32 1.0, %v7854
%v7856 = vmul.f32 %v7855, %v7849
%v7857 = vand.u32 2147483647, %v7849
%vm7858 = vcmp.lt.f32.partialorder %v7857, 0.0004427343
%v7859 = vsel /*vm=*/%vm7858, /*on_true_vy=*/%v7856, /*on_false_vx=*/%v7853
%v7860 = vxor.u32 2147483648, %v7859
%vm7863 = vcmp.lt.f32.partialorder %v7860, 5.0
%v7868 = vsel /*vm=*/%vm7863, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v7872 = vsel /*vm=*/%vm7863, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v7876 = vsel /*vm=*/%vm7863, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v7880 = vsel /*vm=*/%vm7863, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v7884 = vsel /*vm=*/%vm7863, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v7888 = vsel /*vm=*/%vm7863, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v7892 = vsel /*vm=*/%vm7863, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v7896 = vsel /*vm=*/%vm7863, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v7900 = vsel /*vm=*/%vm7863, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v7904 = vadd.f32 -2.5, %v7860
%v7906 = vrsqrt.pop %v7860
%v7907 = vmul.f32 %v7906, %v7860
%vm7908 = vcmp.eq.f32.partialorder %v7860, inf
%v7909 = vsel /*vm=*/%vm7908, /*on_true_vy=*/%v7860, /*on_false_vx=*/%v7907
%vm7910 = vcmp.eq.f32.partialorder %v7860, 0.0
%v7911 = vand.u32 2147483648, %v7860
%v7912 = vsel /*vm=*/%vm7910, /*on_true_vy=*/%v7911, /*on_false_vx=*/%v7909
%v7915 = vadd.f32 -3.0, %v7912
%v7919 = vsel /*vm=*/%vm7863, /*on_true_vy=*/%v7904, /*on_false_vx=*/%v7915
%v7923 = vmul.f32 %v7919, %v7900
%v7927 = vadd.f32 %v7923, %v7896
%v7931 = vmul.f32 %v7927, %v7919
%v7935 = vadd.f32 %v7931, %v7892
%v7939 = vmul.f32 %v7935, %v7919
%v7943 = vadd.f32 %v7939, %v7888
%v7947 = vmul.f32 %v7943, %v7919
%v7951 = vadd.f32 %v7947, %v7884
%v7955 = vmul.f32 %v7951, %v7919
%v7959 = vadd.f32 %v7955, %v7880
%v7963 = vmul.f32 %v7959, %v7919
%v7967 = vadd.f32 %v7963, %v7876
%v7971 = vmul.f32 %v7967, %v7919
%v7975 = vadd.f32 %v7971, %v7872
%v7979 = vmul.f32 %v7975, %v7919
%v7983 = vadd.f32 %v7979, %v7868
%v7987 = vmul.f32 %v7983, %v7834
%v7991 = vsel /*vm=*/%vm7839, /*on_true_vy=*/%v7844, /*on_false_vx=*/%v7987
%v7995 = vmul.f32 1.4140625, %v7991
%v7998 = vpack.c.bf16 %v120417, %v7995
%119783 = vst [vmem:[%s280 + $0x384] sm:$0xf] /*vst_source=*/%v7998
%v8036 = vadd.s32 %v8033, %v408
%v8046 = vadd.s32 %v8036, %v415
%vm8050 = vcmp.lt.u32.totalorder %v8046, %v8036
%vm8055 = vcmp.lt.u32.totalorder %v8036, %v408
%v8060 = vadd.s32 %v8016, %v380
%v8064 = vadd.s32 1, %v8060
%v8068 = vsel /*vm=*/%vm8055, /*on_true_vy=*/%v8064, /*on_false_vx=*/%v8060
%v8072 = vadd.s32 1, %v8068
%v8076 = vsel /*vm=*/%vm8050, /*on_true_vy=*/%v8072, /*on_false_vx=*/%v8068
%v8081 = vadd.s32 %v8076, %v10
%v8085 = vadd.s32 %v8046, %v9
%v8089 = vadd.s32 %v8085, %v8081
%v8091 = vshll.u32 %v8085, 13
%v8092 = vshrl.u32 %v8085, 19
%v8093 = vor.u32 %v8092, %v8091
%v8094 = vxor.u32 %v8093, %v8089
%v8097 = vadd.s32 %v8094, %v8089
%v8099 = vshll.u32 %v8094, 15
%v8100 = vshrl.u32 %v8094, 17
%v8101 = vor.u32 %v8100, %v8099
%v8102 = vxor.u32 %v8101, %v8097
%v8105 = vadd.s32 %v8102, %v8097
%v8107 = vshll.u32 %v8102, 26
%v8108 = vshrl.u32 %v8102, 6
%v8109 = vor.u32 %v8108, %v8107
%v8110 = vxor.u32 %v8109, %v8105
%v8113 = vadd.s32 %v8110, %v8105
%v8117 = vadd.s32 %v8113, %v9
%v8119 = vshll.u32 %v8110, 6
%v8120 = vshrl.u32 %v8110, 26
%v8121 = vor.u32 %v8120, %v8119
%v8122 = vxor.u32 %v8121, %v8113
%v8125 = vadd.s32 %v8122, %v8
%v8129 = vadd.s32 1, %v8125
%v8133 = vadd.s32 %v8129, %v8117
%v8135 = vshll.u32 %v8129, 17
%v8136 = vshrl.u32 %v8129, 15
%v8137 = vor.u32 %v8136, %v8135
%v8138 = vxor.u32 %v8137, %v8133
%v8141 = vadd.s32 %v8138, %v8133
%v8143 = vshll.u32 %v8138, 29
%v8144 = vshrl.u32 %v8138, 3
%v8145 = vor.u32 %v8144, %v8143
%v8146 = vxor.u32 %v8145, %v8141
%v8149 = vadd.s32 %v8146, %v8141
%v8151 = vshll.u32 %v8146, 16
%v8152 = vshrl.u32 %v8146, 16
%v8153 = vor.u32 %v8152, %v8151
%v8154 = vxor.u32 %v8153, %v8149
%v8157 = vadd.s32 %v8154, %v8149
%v8161 = vadd.s32 %v8157, %v8
%v8163 = vshll.u32 %v8154, 24
%v8164 = vshrl.u32 %v8154, 8
%v8165 = vor.u32 %v8164, %v8163
%v8166 = vxor.u32 %v8165, %v8157
%v8169 = vadd.s32 %v8166, %v10
%v8173 = vadd.s32 2, %v8169
%v8177 = vadd.s32 %v8173, %v8161
%v8179 = vshll.u32 %v8173, 13
%v8180 = vshrl.u32 %v8173, 19
%v8181 = vor.u32 %v8180, %v8179
%v8182 = vxor.u32 %v8181, %v8177
%v8185 = vadd.s32 %v8182, %v8177
%v8187 = vshll.u32 %v8182, 15
%v8188 = vshrl.u32 %v8182, 17
%v8189 = vor.u32 %v8188, %v8187
%v8190 = vxor.u32 %v8189, %v8185
%v8193 = vadd.s32 %v8190, %v8185
%v8195 = vshll.u32 %v8190, 26
%v8196 = vshrl.u32 %v8190, 6
%v8197 = vor.u32 %v8196, %v8195
%v8198 = vxor.u32 %v8197, %v8193
%v8201 = vadd.s32 %v8198, %v8193
%v8205 = vadd.s32 %v8201, %v10
%v8207 = vshll.u32 %v8198, 6
%v8208 = vshrl.u32 %v8198, 26
%v8209 = vor.u32 %v8208, %v8207
%v8210 = vxor.u32 %v8209, %v8201
%v8213 = vadd.s32 %v8210, %v9
%v8217 = vadd.s32 3, %v8213
%v8221 = vadd.s32 %v8217, %v8205
%v8223 = vshll.u32 %v8217, 17
%v8224 = vshrl.u32 %v8217, 15
%v8225 = vor.u32 %v8224, %v8223
%v8226 = vxor.u32 %v8225, %v8221
%v8229 = vadd.s32 %v8226, %v8221
%v8231 = vshll.u32 %v8226, 29
%v8232 = vshrl.u32 %v8226, 3
%v8233 = vor.u32 %v8232, %v8231
%v8234 = vxor.u32 %v8233, %v8229
%v8237 = vadd.s32 %v8234, %v8229
%v8239 = vshll.u32 %v8234, 16
%v8240 = vshrl.u32 %v8234, 16
%v8241 = vor.u32 %v8240, %v8239
%v8242 = vxor.u32 %v8241, %v8237
%v8245 = vadd.s32 %v8242, %v8237
%v8249 = vadd.s32 %v8245, %v9
%v8251 = vshll.u32 %v8242, 24
%v8252 = vshrl.u32 %v8242, 8
%v8253 = vor.u32 %v8252, %v8251
%v8254 = vxor.u32 %v8253, %v8245
%v8257 = vadd.s32 %v8254, %v8
%v8261 = vadd.s32 4, %v8257
%v8265 = vadd.s32 %v8261, %v8249
%v8267 = vshll.u32 %v8261, 13
%v8268 = vshrl.u32 %v8261, 19
%v8269 = vor.u32 %v8268, %v8267
%v8270 = vxor.u32 %v8269, %v8265
%v8273 = vadd.s32 %v8270, %v8265
%v8275 = vshll.u32 %v8270, 15
%v8276 = vshrl.u32 %v8270, 17
%v8277 = vor.u32 %v8276, %v8275
%v8278 = vxor.u32 %v8277, %v8273
%v8281 = vadd.s32 %v8278, %v8273
%v8283 = vshll.u32 %v8278, 26
%v8284 = vshrl.u32 %v8278, 6
%v8285 = vor.u32 %v8284, %v8283
%v8286 = vxor.u32 %v8285, %v8281
%v8289 = vadd.s32 %v8286, %v8281
%v8293 = vadd.s32 %v8289, %v8
%v8295 = vshll.u32 %v8286, 6
%v8296 = vshrl.u32 %v8286, 26
%v8297 = vor.u32 %v8296, %v8295
%v8298 = vxor.u32 %v8297, %v8289
%v8301 = vadd.s32 %v8298, %v10
%v8305 = vadd.s32 5, %v8301
%v8307 = vxor.u32 %v8305, %v8293
%v8308 = vand.u32.u8 255, %v8307
%v8309 = vand.u32 65535, %v8308
%v8310 = vshrl.u32 %v8309, 1
%v8311 = vor.u32 16256, %v8310
%v8312 = vand.u32.u16 65535, %v8311
%v119788 = vadd.low.f32.bf16 -1.0, %v8312
%v8321 = vmul.f32 2.0, %v119788
%v8325 = vadd.f32 -0.99609375, %v8321
%v8329 = vmax.f32 %v8325, -0.99609375
%v8331 = vand.u32 2147483647, %v8329
%vm8334 = vcmp.eq.f32.partialorder %v8331, 1.0
%v8339 = vmul.f32 inf, %v8329
%v8341 = vxor.u32 2147483648, %v8329
%v8344 = vmul.f32 %v8341, %v8329
%v8346 = vadd.f32 1.0, %v8344
%v8347 = vlog2.pop %v8346
%v8348 = vmul.f32 0.6931472, %v8347
%v8349 = vmul.f32 -0.5, %v8344
%v8350 = vadd.f32 1.0, %v8349
%v8351 = vmul.f32 %v8350, %v8344
%v8352 = vand.u32 2147483647, %v8344
%vm8353 = vcmp.lt.f32.partialorder %v8352, 0.0004427343
%v8354 = vsel /*vm=*/%vm8353, /*on_true_vy=*/%v8351, /*on_false_vx=*/%v8348
%v8355 = vxor.u32 2147483648, %v8354
%vm8358 = vcmp.lt.f32.partialorder %v8355, 5.0
%v8363 = vsel /*vm=*/%vm8358, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v8367 = vsel /*vm=*/%vm8358, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v8371 = vsel /*vm=*/%vm8358, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v8375 = vsel /*vm=*/%vm8358, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v8379 = vsel /*vm=*/%vm8358, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v8383 = vsel /*vm=*/%vm8358, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v8387 = vsel /*vm=*/%vm8358, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v8391 = vsel /*vm=*/%vm8358, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v8395 = vsel /*vm=*/%vm8358, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v8399 = vadd.f32 -2.5, %v8355
%v8401 = vrsqrt.pop %v8355
%v8402 = vmul.f32 %v8401, %v8355
%vm8403 = vcmp.eq.f32.partialorder %v8355, inf
%v8404 = vsel /*vm=*/%vm8403, /*on_true_vy=*/%v8355, /*on_false_vx=*/%v8402
%vm8405 = vcmp.eq.f32.partialorder %v8355, 0.0
%v8406 = vand.u32 2147483648, %v8355
%v8407 = vsel /*vm=*/%vm8405, /*on_true_vy=*/%v8406, /*on_false_vx=*/%v8404
%v8410 = vadd.f32 -3.0, %v8407
%v8414 = vsel /*vm=*/%vm8358, /*on_true_vy=*/%v8399, /*on_false_vx=*/%v8410
%v8418 = vmul.f32 %v8414, %v8395
%v8422 = vadd.f32 %v8418, %v8391
%v8426 = vmul.f32 %v8422, %v8414
%v8430 = vadd.f32 %v8426, %v8387
%v8434 = vmul.f32 %v8430, %v8414
%v8438 = vadd.f32 %v8434, %v8383
%v8442 = vmul.f32 %v8438, %v8414
%v8446 = vadd.f32 %v8442, %v8379
%v8450 = vmul.f32 %v8446, %v8414
%v8454 = vadd.f32 %v8450, %v8375
%v8458 = vmul.f32 %v8454, %v8414
%v8462 = vadd.f32 %v8458, %v8371
%v8466 = vmul.f32 %v8462, %v8414
%v8470 = vadd.f32 %v8466, %v8367
%v8474 = vmul.f32 %v8470, %v8414
%v8478 = vadd.f32 %v8474, %v8363
%v8482 = vmul.f32 %v8478, %v8329
%v8486 = vsel /*vm=*/%vm8334, /*on_true_vy=*/%v8339, /*on_false_vx=*/%v8482
%v8490 = vmul.f32 1.4140625, %v8486
%v8493 = vpack.c.bf16 %v120417, %v8490
%119789 = vst [vmem:[%s280 + $0x8] sm:$0xf] /*vst_source=*/%v8493
%v8497 = vadd.s32 %v8033, %v894
%v8507 = vadd.s32 %v8497, %v415
%vm8511 = vcmp.lt.u32.totalorder %v8507, %v8497
%vm8516 = vcmp.lt.u32.totalorder %v8497, %v894
%v8521 = vadd.s32 %v8016, %v881
%v8525 = vadd.s32 1, %v8521
%v8529 = vsel /*vm=*/%vm8516, /*on_true_vy=*/%v8525, /*on_false_vx=*/%v8521
%v8533 = vadd.s32 1, %v8529
%v8537 = vsel /*vm=*/%vm8511, /*on_true_vy=*/%v8533, /*on_false_vx=*/%v8529
%v8542 = vadd.s32 %v8537, %v10
%v8546 = vadd.s32 %v8507, %v9
%v8550 = vadd.s32 %v8546, %v8542
%v8552 = vshll.u32 %v8546, 13
%v8553 = vshrl.u32 %v8546, 19
%v8554 = vor.u32 %v8553, %v8552
%v8555 = vxor.u32 %v8554, %v8550
%v8558 = vadd.s32 %v8555, %v8550
%v8560 = vshll.u32 %v8555, 15
%v8561 = vshrl.u32 %v8555, 17
%v8562 = vor.u32 %v8561, %v8560
%v8563 = vxor.u32 %v8562, %v8558
%v8566 = vadd.s32 %v8563, %v8558
%v8568 = vshll.u32 %v8563, 26
%v8569 = vshrl.u32 %v8563, 6
%v8570 = vor.u32 %v8569, %v8568
%v8571 = vxor.u32 %v8570, %v8566
%v8574 = vadd.s32 %v8571, %v8566
%v8578 = vadd.s32 %v8574, %v9
%v8580 = vshll.u32 %v8571, 6
%v8581 = vshrl.u32 %v8571, 26
%v8582 = vor.u32 %v8581, %v8580
%v8583 = vxor.u32 %v8582, %v8574
%v8586 = vadd.s32 %v8583, %v8
%v8590 = vadd.s32 1, %v8586
%v8594 = vadd.s32 %v8590, %v8578
%v8596 = vshll.u32 %v8590, 17
%v8597 = vshrl.u32 %v8590, 15
%v8598 = vor.u32 %v8597, %v8596
%v8599 = vxor.u32 %v8598, %v8594
%v8602 = vadd.s32 %v8599, %v8594
%v8604 = vshll.u32 %v8599, 29
%v8605 = vshrl.u32 %v8599, 3
%v8606 = vor.u32 %v8605, %v8604
%v8607 = vxor.u32 %v8606, %v8602
%v8610 = vadd.s32 %v8607, %v8602
%v8612 = vshll.u32 %v8607, 16
%v8613 = vshrl.u32 %v8607, 16
%v8614 = vor.u32 %v8613, %v8612
%v8615 = vxor.u32 %v8614, %v8610
%v8618 = vadd.s32 %v8615, %v8610
%v8622 = vadd.s32 %v8618, %v8
%v8624 = vshll.u32 %v8615, 24
%v8625 = vshrl.u32 %v8615, 8
%v8626 = vor.u32 %v8625, %v8624
%v8627 = vxor.u32 %v8626, %v8618
%v8630 = vadd.s32 %v8627, %v10
%v8634 = vadd.s32 2, %v8630
%v8638 = vadd.s32 %v8634, %v8622
%v8640 = vshll.u32 %v8634, 13
%v8641 = vshrl.u32 %v8634, 19
%v8642 = vor.u32 %v8641, %v8640
%v8643 = vxor.u32 %v8642, %v8638
%v8646 = vadd.s32 %v8643, %v8638
%v8648 = vshll.u32 %v8643, 15
%v8649 = vshrl.u32 %v8643, 17
%v8650 = vor.u32 %v8649, %v8648
%v8651 = vxor.u32 %v8650, %v8646
%v8654 = vadd.s32 %v8651, %v8646
%v8656 = vshll.u32 %v8651, 26
%v8657 = vshrl.u32 %v8651, 6
%v8658 = vor.u32 %v8657, %v8656
%v8659 = vxor.u32 %v8658, %v8654
%v8662 = vadd.s32 %v8659, %v8654
%v8666 = vadd.s32 %v8662, %v10
%v8668 = vshll.u32 %v8659, 6
%v8669 = vshrl.u32 %v8659, 26
%v8670 = vor.u32 %v8669, %v8668
%v8671 = vxor.u32 %v8670, %v8662
%v8674 = vadd.s32 %v8671, %v9
%v8678 = vadd.s32 3, %v8674
%v8682 = vadd.s32 %v8678, %v8666
%v8684 = vshll.u32 %v8678, 17
%v8685 = vshrl.u32 %v8678, 15
%v8686 = vor.u32 %v8685, %v8684
%v8687 = vxor.u32 %v8686, %v8682
%v8690 = vadd.s32 %v8687, %v8682
%v8692 = vshll.u32 %v8687, 29
%v8693 = vshrl.u32 %v8687, 3
%v8694 = vor.u32 %v8693, %v8692
%v8695 = vxor.u32 %v8694, %v8690
%v8698 = vadd.s32 %v8695, %v8690
%v8700 = vshll.u32 %v8695, 16
%v8701 = vshrl.u32 %v8695, 16
%v8702 = vor.u32 %v8701, %v8700
%v8703 = vxor.u32 %v8702, %v8698
%v8706 = vadd.s32 %v8703, %v8698
%v8710 = vadd.s32 %v8706, %v9
%v8712 = vshll.u32 %v8703, 24
%v8713 = vshrl.u32 %v8703, 8
%v8714 = vor.u32 %v8713, %v8712
%v8715 = vxor.u32 %v8714, %v8706
%v8718 = vadd.s32 %v8715, %v8
%v8722 = vadd.s32 4, %v8718
%v8726 = vadd.s32 %v8722, %v8710
%v8728 = vshll.u32 %v8722, 13
%v8729 = vshrl.u32 %v8722, 19
%v8730 = vor.u32 %v8729, %v8728
%v8731 = vxor.u32 %v8730, %v8726
%v8734 = vadd.s32 %v8731, %v8726
%v8736 = vshll.u32 %v8731, 15
%v8737 = vshrl.u32 %v8731, 17
%v8738 = vor.u32 %v8737, %v8736
%v8739 = vxor.u32 %v8738, %v8734
%v8742 = vadd.s32 %v8739, %v8734
%v8744 = vshll.u32 %v8739, 26
%v8745 = vshrl.u32 %v8739, 6
%v8746 = vor.u32 %v8745, %v8744
%v8747 = vxor.u32 %v8746, %v8742
%v8750 = vadd.s32 %v8747, %v8742
%v8754 = vadd.s32 %v8750, %v8
%v8756 = vshll.u32 %v8747, 6
%v8757 = vshrl.u32 %v8747, 26
%v8758 = vor.u32 %v8757, %v8756
%v8759 = vxor.u32 %v8758, %v8750
%v8762 = vadd.s32 %v8759, %v10
%v8766 = vadd.s32 5, %v8762
%v8768 = vxor.u32 %v8766, %v8754
%v8769 = vand.u32.u8 255, %v8768
%v8770 = vand.u32 65535, %v8769
%v8771 = vshrl.u32 %v8770, 1
%v8772 = vor.u32 16256, %v8771
%v8773 = vand.u32.u16 65535, %v8772
%v119790 = vadd.low.f32.bf16 -1.0, %v8773
%v8782 = vmul.f32 2.0, %v119790
%v8786 = vadd.f32 -0.99609375, %v8782
%v8790 = vmax.f32 %v8786, -0.99609375
%v8792 = vand.u32 2147483647, %v8790
%vm8795 = vcmp.eq.f32.partialorder %v8792, 1.0
%v8800 = vmul.f32 inf, %v8790
%v8802 = vxor.u32 2147483648, %v8790
%v8805 = vmul.f32 %v8802, %v8790
%v8807 = vadd.f32 1.0, %v8805
%v8808 = vlog2.pop %v8807
%v8809 = vmul.f32 0.6931472, %v8808
%v8810 = vmul.f32 -0.5, %v8805
%v8811 = vadd.f32 1.0, %v8810
%v8812 = vmul.f32 %v8811, %v8805
%v8813 = vand.u32 2147483647, %v8805
%vm8814 = vcmp.lt.f32.partialorder %v8813, 0.0004427343
%v8815 = vsel /*vm=*/%vm8814, /*on_true_vy=*/%v8812, /*on_false_vx=*/%v8809
%v8816 = vxor.u32 2147483648, %v8815
%vm8819 = vcmp.lt.f32.partialorder %v8816, 5.0
%v8824 = vsel /*vm=*/%vm8819, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v8828 = vsel /*vm=*/%vm8819, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v8832 = vsel /*vm=*/%vm8819, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v8836 = vsel /*vm=*/%vm8819, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v8840 = vsel /*vm=*/%vm8819, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v8844 = vsel /*vm=*/%vm8819, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v8848 = vsel /*vm=*/%vm8819, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v8852 = vsel /*vm=*/%vm8819, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v8856 = vsel /*vm=*/%vm8819, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v8860 = vadd.f32 -2.5, %v8816
%v8862 = vrsqrt.pop %v8816
%v8863 = vmul.f32 %v8862, %v8816
%vm8864 = vcmp.eq.f32.partialorder %v8816, inf
%v8865 = vsel /*vm=*/%vm8864, /*on_true_vy=*/%v8816, /*on_false_vx=*/%v8863
%vm8866 = vcmp.eq.f32.partialorder %v8816, 0.0
%v8867 = vand.u32 2147483648, %v8816
%v8868 = vsel /*vm=*/%vm8866, /*on_true_vy=*/%v8867, /*on_false_vx=*/%v8865
%v8871 = vadd.f32 -3.0, %v8868
%v8875 = vsel /*vm=*/%vm8819, /*on_true_vy=*/%v8860, /*on_false_vx=*/%v8871
%v8879 = vmul.f32 %v8875, %v8856
%v8883 = vadd.f32 %v8879, %v8852
%v8887 = vmul.f32 %v8883, %v8875
%v8891 = vadd.f32 %v8887, %v8848
%v8895 = vmul.f32 %v8891, %v8875
%v8899 = vadd.f32 %v8895, %v8844
%v8903 = vmul.f32 %v8899, %v8875
%v8907 = vadd.f32 %v8903, %v8840
%v8911 = vmul.f32 %v8907, %v8875
%v8915 = vadd.f32 %v8911, %v8836
%v8919 = vmul.f32 %v8915, %v8875
%v8923 = vadd.f32 %v8919, %v8832
%v8927 = vmul.f32 %v8923, %v8875
%v8931 = vadd.f32 %v8927, %v8828
%v8935 = vmul.f32 %v8931, %v8875
%v8939 = vadd.f32 %v8935, %v8824
%v8943 = vmul.f32 %v8939, %v8790
%v8947 = vsel /*vm=*/%vm8795, /*on_true_vy=*/%v8800, /*on_false_vx=*/%v8943
%v8951 = vmul.f32 1.4140625, %v8947
%v8954 = vpack.c.bf16 %v120417, %v8951
%119791 = vst [vmem:[%s280 + $0x88] sm:$0xf] /*vst_source=*/%v8954
%v8958 = vadd.s32 %v8033, %v1381
%v8968 = vadd.s32 %v8958, %v415
%vm8972 = vcmp.lt.u32.totalorder %v8968, %v8958
%vm8977 = vcmp.lt.u32.totalorder %v8958, %v1381
%v8982 = vadd.s32 %v8016, %v1368
%v8986 = vadd.s32 1, %v8982
%v8990 = vsel /*vm=*/%vm8977, /*on_true_vy=*/%v8986, /*on_false_vx=*/%v8982
%v8994 = vadd.s32 1, %v8990
%v8998 = vsel /*vm=*/%vm8972, /*on_true_vy=*/%v8994, /*on_false_vx=*/%v8990
%v9003 = vadd.s32 %v8998, %v10
%v9007 = vadd.s32 %v8968, %v9
%v9011 = vadd.s32 %v9007, %v9003
%v9013 = vshll.u32 %v9007, 13
%v9014 = vshrl.u32 %v9007, 19
%v9015 = vor.u32 %v9014, %v9013
%v9016 = vxor.u32 %v9015, %v9011
%v9019 = vadd.s32 %v9016, %v9011
%v9021 = vshll.u32 %v9016, 15
%v9022 = vshrl.u32 %v9016, 17
%v9023 = vor.u32 %v9022, %v9021
%v9024 = vxor.u32 %v9023, %v9019
%v9027 = vadd.s32 %v9024, %v9019
%v9029 = vshll.u32 %v9024, 26
%v9030 = vshrl.u32 %v9024, 6
%v9031 = vor.u32 %v9030, %v9029
%v9032 = vxor.u32 %v9031, %v9027
%v9035 = vadd.s32 %v9032, %v9027
%v9039 = vadd.s32 %v9035, %v9
%v9041 = vshll.u32 %v9032, 6
%v9042 = vshrl.u32 %v9032, 26
%v9043 = vor.u32 %v9042, %v9041
%v9044 = vxor.u32 %v9043, %v9035
%v9047 = vadd.s32 %v9044, %v8
%v9051 = vadd.s32 1, %v9047
%v9055 = vadd.s32 %v9051, %v9039
%v9057 = vshll.u32 %v9051, 17
%v9058 = vshrl.u32 %v9051, 15
%v9059 = vor.u32 %v9058, %v9057
%v9060 = vxor.u32 %v9059, %v9055
%v9063 = vadd.s32 %v9060, %v9055
%v9065 = vshll.u32 %v9060, 29
%v9066 = vshrl.u32 %v9060, 3
%v9067 = vor.u32 %v9066, %v9065
%v9068 = vxor.u32 %v9067, %v9063
%v9071 = vadd.s32 %v9068, %v9063
%v9073 = vshll.u32 %v9068, 16
%v9074 = vshrl.u32 %v9068, 16
%v9075 = vor.u32 %v9074, %v9073
%v9076 = vxor.u32 %v9075, %v9071
%v9079 = vadd.s32 %v9076, %v9071
%v9083 = vadd.s32 %v9079, %v8
%v9085 = vshll.u32 %v9076, 24
%v9086 = vshrl.u32 %v9076, 8
%v9087 = vor.u32 %v9086, %v9085
%v9088 = vxor.u32 %v9087, %v9079
%v9091 = vadd.s32 %v9088, %v10
%v9095 = vadd.s32 2, %v9091
%v9099 = vadd.s32 %v9095, %v9083
%v9101 = vshll.u32 %v9095, 13
%v9102 = vshrl.u32 %v9095, 19
%v9103 = vor.u32 %v9102, %v9101
%v9104 = vxor.u32 %v9103, %v9099
%v9107 = vadd.s32 %v9104, %v9099
%v9109 = vshll.u32 %v9104, 15
%v9110 = vshrl.u32 %v9104, 17
%v9111 = vor.u32 %v9110, %v9109
%v9112 = vxor.u32 %v9111, %v9107
%v9115 = vadd.s32 %v9112, %v9107
%v9117 = vshll.u32 %v9112, 26
%v9118 = vshrl.u32 %v9112, 6
%v9119 = vor.u32 %v9118, %v9117
%v9120 = vxor.u32 %v9119, %v9115
%v9123 = vadd.s32 %v9120, %v9115
%v9127 = vadd.s32 %v9123, %v10
%v9129 = vshll.u32 %v9120, 6
%v9130 = vshrl.u32 %v9120, 26
%v9131 = vor.u32 %v9130, %v9129
%v9132 = vxor.u32 %v9131, %v9123
%v9135 = vadd.s32 %v9132, %v9
%v9139 = vadd.s32 3, %v9135
%v9143 = vadd.s32 %v9139, %v9127
%v9145 = vshll.u32 %v9139, 17
%v9146 = vshrl.u32 %v9139, 15
%v9147 = vor.u32 %v9146, %v9145
%v9148 = vxor.u32 %v9147, %v9143
%v9151 = vadd.s32 %v9148, %v9143
%v9153 = vshll.u32 %v9148, 29
%v9154 = vshrl.u32 %v9148, 3
%v9155 = vor.u32 %v9154, %v9153
%v9156 = vxor.u32 %v9155, %v9151
%v9159 = vadd.s32 %v9156, %v9151
%v9161 = vshll.u32 %v9156, 16
%v9162 = vshrl.u32 %v9156, 16
%v9163 = vor.u32 %v9162, %v9161
%v9164 = vxor.u32 %v9163, %v9159
%v9167 = vadd.s32 %v9164, %v9159
%v9171 = vadd.s32 %v9167, %v9
%v9173 = vshll.u32 %v9164, 24
%v9174 = vshrl.u32 %v9164, 8
%v9175 = vor.u32 %v9174, %v9173
%v9176 = vxor.u32 %v9175, %v9167
%v9179 = vadd.s32 %v9176, %v8
%v9183 = vadd.s32 4, %v9179
%v9187 = vadd.s32 %v9183, %v9171
%v9189 = vshll.u32 %v9183, 13
%v9190 = vshrl.u32 %v9183, 19
%v9191 = vor.u32 %v9190, %v9189
%v9192 = vxor.u32 %v9191, %v9187
%v9195 = vadd.s32 %v9192, %v9187
%v9197 = vshll.u32 %v9192, 15
%v9198 = vshrl.u32 %v9192, 17
%v9199 = vor.u32 %v9198, %v9197
%v9200 = vxor.u32 %v9199, %v9195
%v9203 = vadd.s32 %v9200, %v9195
%v9205 = vshll.u32 %v9200, 26
%v9206 = vshrl.u32 %v9200, 6
%v9207 = vor.u32 %v9206, %v9205
%v9208 = vxor.u32 %v9207, %v9203
%v9211 = vadd.s32 %v9208, %v9203
%v9215 = vadd.s32 %v9211, %v8
%v9217 = vshll.u32 %v9208, 6
%v9218 = vshrl.u32 %v9208, 26
%v9219 = vor.u32 %v9218, %v9217
%v9220 = vxor.u32 %v9219, %v9211
%v9223 = vadd.s32 %v9220, %v10
%v9227 = vadd.s32 5, %v9223
%v9229 = vxor.u32 %v9227, %v9215
%v9230 = vand.u32.u8 255, %v9229
%v9231 = vand.u32 65535, %v9230
%v9232 = vshrl.u32 %v9231, 1
%v9233 = vor.u32 16256, %v9232
%v9234 = vand.u32.u16 65535, %v9233
%v119792 = vadd.low.f32.bf16 -1.0, %v9234
%v9243 = vmul.f32 2.0, %v119792
%v9247 = vadd.f32 -0.99609375, %v9243
%v9251 = vmax.f32 %v9247, -0.99609375
%v9253 = vand.u32 2147483647, %v9251
%vm9256 = vcmp.eq.f32.partialorder %v9253, 1.0
%v9261 = vmul.f32 inf, %v9251
%v9263 = vxor.u32 2147483648, %v9251
%v9266 = vmul.f32 %v9263, %v9251
%v9268 = vadd.f32 1.0, %v9266
%v9269 = vlog2.pop %v9268
%v9270 = vmul.f32 0.6931472, %v9269
%v9271 = vmul.f32 -0.5, %v9266
%v9272 = vadd.f32 1.0, %v9271
%v9273 = vmul.f32 %v9272, %v9266
%v9274 = vand.u32 2147483647, %v9266
%vm9275 = vcmp.lt.f32.partialorder %v9274, 0.0004427343
%v9276 = vsel /*vm=*/%vm9275, /*on_true_vy=*/%v9273, /*on_false_vx=*/%v9270
%v9277 = vxor.u32 2147483648, %v9276
%vm9280 = vcmp.lt.f32.partialorder %v9277, 5.0
%v9285 = vsel /*vm=*/%vm9280, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v9289 = vsel /*vm=*/%vm9280, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v9293 = vsel /*vm=*/%vm9280, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v9297 = vsel /*vm=*/%vm9280, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v9301 = vsel /*vm=*/%vm9280, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v9305 = vsel /*vm=*/%vm9280, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v9309 = vsel /*vm=*/%vm9280, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v9313 = vsel /*vm=*/%vm9280, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v9317 = vsel /*vm=*/%vm9280, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v9321 = vadd.f32 -2.5, %v9277
%v9323 = vrsqrt.pop %v9277
%v9324 = vmul.f32 %v9323, %v9277
%vm9325 = vcmp.eq.f32.partialorder %v9277, inf
%v9326 = vsel /*vm=*/%vm9325, /*on_true_vy=*/%v9277, /*on_false_vx=*/%v9324
%vm9327 = vcmp.eq.f32.partialorder %v9277, 0.0
%v9328 = vand.u32 2147483648, %v9277
%v9329 = vsel /*vm=*/%vm9327, /*on_true_vy=*/%v9328, /*on_false_vx=*/%v9326
%v9332 = vadd.f32 -3.0, %v9329
%v9336 = vsel /*vm=*/%vm9280, /*on_true_vy=*/%v9321, /*on_false_vx=*/%v9332
%v9340 = vmul.f32 %v9336, %v9317
%v9344 = vadd.f32 %v9340, %v9313
%v9348 = vmul.f32 %v9344, %v9336
%v9352 = vadd.f32 %v9348, %v9309
%v9356 = vmul.f32 %v9352, %v9336
%v9360 = vadd.f32 %v9356, %v9305
%v9364 = vmul.f32 %v9360, %v9336
%v9368 = vadd.f32 %v9364, %v9301
%v9372 = vmul.f32 %v9368, %v9336
%v9376 = vadd.f32 %v9372, %v9297
%v9380 = vmul.f32 %v9376, %v9336
%v9384 = vadd.f32 %v9380, %v9293
%v9388 = vmul.f32 %v9384, %v9336
%v9392 = vadd.f32 %v9388, %v9289
%v9396 = vmul.f32 %v9392, %v9336
%v9400 = vadd.f32 %v9396, %v9285
%v9404 = vmul.f32 %v9400, %v9251
%v9408 = vsel /*vm=*/%vm9256, /*on_true_vy=*/%v9261, /*on_false_vx=*/%v9404
%v9412 = vmul.f32 1.4140625, %v9408
%v9415 = vpack.c.bf16 %v120417, %v9412
%119793 = vst [vmem:[%s280 + $0x108] sm:$0xf] /*vst_source=*/%v9415
%v9419 = vadd.s32 %v8033, %v1868
%v9429 = vadd.s32 %v9419, %v415
%vm9433 = vcmp.lt.u32.totalorder %v9429, %v9419
%vm9438 = vcmp.lt.u32.totalorder %v9419, %v1868
%v9443 = vadd.s32 %v8016, %v1855
%v9447 = vadd.s32 1, %v9443
%v9451 = vsel /*vm=*/%vm9438, /*on_true_vy=*/%v9447, /*on_false_vx=*/%v9443
%v9455 = vadd.s32 1, %v9451
%v9459 = vsel /*vm=*/%vm9433, /*on_true_vy=*/%v9455, /*on_false_vx=*/%v9451
%v9464 = vadd.s32 %v9459, %v10
%v9468 = vadd.s32 %v9429, %v9
%v9472 = vadd.s32 %v9468, %v9464
%v9474 = vshll.u32 %v9468, 13
%v9475 = vshrl.u32 %v9468, 19
%v9476 = vor.u32 %v9475, %v9474
%v9477 = vxor.u32 %v9476, %v9472
%v9480 = vadd.s32 %v9477, %v9472
%v9482 = vshll.u32 %v9477, 15
%v9483 = vshrl.u32 %v9477, 17
%v9484 = vor.u32 %v9483, %v9482
%v9485 = vxor.u32 %v9484, %v9480
%v9488 = vadd.s32 %v9485, %v9480
%v9490 = vshll.u32 %v9485, 26
%v9491 = vshrl.u32 %v9485, 6
%v9492 = vor.u32 %v9491, %v9490
%v9493 = vxor.u32 %v9492, %v9488
%v9496 = vadd.s32 %v9493, %v9488
%v9500 = vadd.s32 %v9496, %v9
%v9502 = vshll.u32 %v9493, 6
%v9503 = vshrl.u32 %v9493, 26
%v9504 = vor.u32 %v9503, %v9502
%v9505 = vxor.u32 %v9504, %v9496
%v9508 = vadd.s32 %v9505, %v8
%v9512 = vadd.s32 1, %v9508
%v9516 = vadd.s32 %v9512, %v9500
%v9518 = vshll.u32 %v9512, 17
%v9519 = vshrl.u32 %v9512, 15
%v9520 = vor.u32 %v9519, %v9518
%v9521 = vxor.u32 %v9520, %v9516
%v9524 = vadd.s32 %v9521, %v9516
%v9526 = vshll.u32 %v9521, 29
%v9527 = vshrl.u32 %v9521, 3
%v9528 = vor.u32 %v9527, %v9526
%v9529 = vxor.u32 %v9528, %v9524
%v9532 = vadd.s32 %v9529, %v9524
%v9534 = vshll.u32 %v9529, 16
%v9535 = vshrl.u32 %v9529, 16
%v9536 = vor.u32 %v9535, %v9534
%v9537 = vxor.u32 %v9536, %v9532
%v9540 = vadd.s32 %v9537, %v9532
%v9544 = vadd.s32 %v9540, %v8
%v9546 = vshll.u32 %v9537, 24
%v9547 = vshrl.u32 %v9537, 8
%v9548 = vor.u32 %v9547, %v9546
%v9549 = vxor.u32 %v9548, %v9540
%v9552 = vadd.s32 %v9549, %v10
%v9556 = vadd.s32 2, %v9552
%v9560 = vadd.s32 %v9556, %v9544
%v9562 = vshll.u32 %v9556, 13
%v9563 = vshrl.u32 %v9556, 19
%v9564 = vor.u32 %v9563, %v9562
%v9565 = vxor.u32 %v9564, %v9560
%v9568 = vadd.s32 %v9565, %v9560
%v9570 = vshll.u32 %v9565, 15
%v9571 = vshrl.u32 %v9565, 17
%v9572 = vor.u32 %v9571, %v9570
%v9573 = vxor.u32 %v9572, %v9568
%v9576 = vadd.s32 %v9573, %v9568
%v9578 = vshll.u32 %v9573, 26
%v9579 = vshrl.u32 %v9573, 6
%v9580 = vor.u32 %v9579, %v9578
%v9581 = vxor.u32 %v9580, %v9576
%v9584 = vadd.s32 %v9581, %v9576
%v9588 = vadd.s32 %v9584, %v10
%v9590 = vshll.u32 %v9581, 6
%v9591 = vshrl.u32 %v9581, 26
%v9592 = vor.u32 %v9591, %v9590
%v9593 = vxor.u32 %v9592, %v9584
%v9596 = vadd.s32 %v9593, %v9
%v9600 = vadd.s32 3, %v9596
%v9604 = vadd.s32 %v9600, %v9588
%v9606 = vshll.u32 %v9600, 17
%v9607 = vshrl.u32 %v9600, 15
%v9608 = vor.u32 %v9607, %v9606
%v9609 = vxor.u32 %v9608, %v9604
%v9612 = vadd.s32 %v9609, %v9604
%v9614 = vshll.u32 %v9609, 29
%v9615 = vshrl.u32 %v9609, 3
%v9616 = vor.u32 %v9615, %v9614
%v9617 = vxor.u32 %v9616, %v9612
%v9620 = vadd.s32 %v9617, %v9612
%v9622 = vshll.u32 %v9617, 16
%v9623 = vshrl.u32 %v9617, 16
%v9624 = vor.u32 %v9623, %v9622
%v9625 = vxor.u32 %v9624, %v9620
%v9628 = vadd.s32 %v9625, %v9620
%v9632 = vadd.s32 %v9628, %v9
%v9634 = vshll.u32 %v9625, 24
%v9635 = vshrl.u32 %v9625, 8
%v9636 = vor.u32 %v9635, %v9634
%v9637 = vxor.u32 %v9636, %v9628
%v9640 = vadd.s32 %v9637, %v8
%v9644 = vadd.s32 4, %v9640
%v9648 = vadd.s32 %v9644, %v9632
%v9650 = vshll.u32 %v9644, 13
%v9651 = vshrl.u32 %v9644, 19
%v9652 = vor.u32 %v9651, %v9650
%v9653 = vxor.u32 %v9652, %v9648
%v9656 = vadd.s32 %v9653, %v9648
%v9658 = vshll.u32 %v9653, 15
%v9659 = vshrl.u32 %v9653, 17
%v9660 = vor.u32 %v9659, %v9658
%v9661 = vxor.u32 %v9660, %v9656
%v9664 = vadd.s32 %v9661, %v9656
%v9666 = vshll.u32 %v9661, 26
%v9667 = vshrl.u32 %v9661, 6
%v9668 = vor.u32 %v9667, %v9666
%v9669 = vxor.u32 %v9668, %v9664
%v9672 = vadd.s32 %v9669, %v9664
%v9676 = vadd.s32 %v9672, %v8
%v9678 = vshll.u32 %v9669, 6
%v9679 = vshrl.u32 %v9669, 26
%v9680 = vor.u32 %v9679, %v9678
%v9681 = vxor.u32 %v9680, %v9672
%v9684 = vadd.s32 %v9681, %v10
%v9688 = vadd.s32 5, %v9684
%v9690 = vxor.u32 %v9688, %v9676
%v9691 = vand.u32.u8 255, %v9690
%v9692 = vand.u32 65535, %v9691
%v9693 = vshrl.u32 %v9692, 1
%v9694 = vor.u32 16256, %v9693
%v9695 = vand.u32.u16 65535, %v9694
%v119794 = vadd.low.f32.bf16 -1.0, %v9695
%v9704 = vmul.f32 2.0, %v119794
%v9708 = vadd.f32 -0.99609375, %v9704
%v9712 = vmax.f32 %v9708, -0.99609375
%v9714 = vand.u32 2147483647, %v9712
%vm9717 = vcmp.eq.f32.partialorder %v9714, 1.0
%v9722 = vmul.f32 inf, %v9712
%v9724 = vxor.u32 2147483648, %v9712
%v9727 = vmul.f32 %v9724, %v9712
%v9729 = vadd.f32 1.0, %v9727
%v9730 = vlog2.pop %v9729
%v9731 = vmul.f32 0.6931472, %v9730
%v9732 = vmul.f32 -0.5, %v9727
%v9733 = vadd.f32 1.0, %v9732
%v9734 = vmul.f32 %v9733, %v9727
%v9735 = vand.u32 2147483647, %v9727
%vm9736 = vcmp.lt.f32.partialorder %v9735, 0.0004427343
%v9737 = vsel /*vm=*/%vm9736, /*on_true_vy=*/%v9734, /*on_false_vx=*/%v9731
%v9738 = vxor.u32 2147483648, %v9737
%vm9741 = vcmp.lt.f32.partialorder %v9738, 5.0
%v9746 = vsel /*vm=*/%vm9741, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v9750 = vsel /*vm=*/%vm9741, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v9754 = vsel /*vm=*/%vm9741, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v9758 = vsel /*vm=*/%vm9741, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v9762 = vsel /*vm=*/%vm9741, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v9766 = vsel /*vm=*/%vm9741, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v9770 = vsel /*vm=*/%vm9741, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v9774 = vsel /*vm=*/%vm9741, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v9778 = vsel /*vm=*/%vm9741, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v9782 = vadd.f32 -2.5, %v9738
%v9784 = vrsqrt.pop %v9738
%v9785 = vmul.f32 %v9784, %v9738
%vm9786 = vcmp.eq.f32.partialorder %v9738, inf
%v9787 = vsel /*vm=*/%vm9786, /*on_true_vy=*/%v9738, /*on_false_vx=*/%v9785
%vm9788 = vcmp.eq.f32.partialorder %v9738, 0.0
%v9789 = vand.u32 2147483648, %v9738
%v9790 = vsel /*vm=*/%vm9788, /*on_true_vy=*/%v9789, /*on_false_vx=*/%v9787
%v9793 = vadd.f32 -3.0, %v9790
%v9797 = vsel /*vm=*/%vm9741, /*on_true_vy=*/%v9782, /*on_false_vx=*/%v9793
%v9801 = vmul.f32 %v9797, %v9778
%v9805 = vadd.f32 %v9801, %v9774
%v9809 = vmul.f32 %v9805, %v9797
%v9813 = vadd.f32 %v9809, %v9770
%v9817 = vmul.f32 %v9813, %v9797
%v9821 = vadd.f32 %v9817, %v9766
%v9825 = vmul.f32 %v9821, %v9797
%v9829 = vadd.f32 %v9825, %v9762
%v9833 = vmul.f32 %v9829, %v9797
%v9837 = vadd.f32 %v9833, %v9758
%v9841 = vmul.f32 %v9837, %v9797
%v9845 = vadd.f32 %v9841, %v9754
%v9849 = vmul.f32 %v9845, %v9797
%v9853 = vadd.f32 %v9849, %v9750
%v9857 = vmul.f32 %v9853, %v9797
%v9861 = vadd.f32 %v9857, %v9746
%v9865 = vmul.f32 %v9861, %v9712
%v9869 = vsel /*vm=*/%vm9717, /*on_true_vy=*/%v9722, /*on_false_vx=*/%v9865
%v9873 = vmul.f32 1.4140625, %v9869
%v9876 = vpack.c.bf16 %v120417, %v9873
%119795 = vst [vmem:[%s280 + $0x188] sm:$0xf] /*vst_source=*/%v9876
%v9880 = vadd.s32 %v8033, %v2355
%v9890 = vadd.s32 %v9880, %v415
%vm9894 = vcmp.lt.u32.totalorder %v9890, %v9880
%vm9899 = vcmp.lt.u32.totalorder %v9880, %v2355
%v9904 = vadd.s32 %v8016, %v2342
%v9908 = vadd.s32 1, %v9904
%v9912 = vsel /*vm=*/%vm9899, /*on_true_vy=*/%v9908, /*on_false_vx=*/%v9904
%v9916 = vadd.s32 1, %v9912
%v9920 = vsel /*vm=*/%vm9894, /*on_true_vy=*/%v9916, /*on_false_vx=*/%v9912
%v9925 = vadd.s32 %v9920, %v10
%v9929 = vadd.s32 %v9890, %v9
%v9933 = vadd.s32 %v9929, %v9925
%v9935 = vshll.u32 %v9929, 13
%v9936 = vshrl.u32 %v9929, 19
%v9937 = vor.u32 %v9936, %v9935
%v9938 = vxor.u32 %v9937, %v9933
%v9941 = vadd.s32 %v9938, %v9933
%v9943 = vshll.u32 %v9938, 15
%v9944 = vshrl.u32 %v9938, 17
%v9945 = vor.u32 %v9944, %v9943
%v9946 = vxor.u32 %v9945, %v9941
%v9949 = vadd.s32 %v9946, %v9941
%v9951 = vshll.u32 %v9946, 26
%v9952 = vshrl.u32 %v9946, 6
%v9953 = vor.u32 %v9952, %v9951
%v9954 = vxor.u32 %v9953, %v9949
%v9957 = vadd.s32 %v9954, %v9949
%v9961 = vadd.s32 %v9957, %v9
%v9963 = vshll.u32 %v9954, 6
%v9964 = vshrl.u32 %v9954, 26
%v9965 = vor.u32 %v9964, %v9963
%v9966 = vxor.u32 %v9965, %v9957
%v9969 = vadd.s32 %v9966, %v8
%v9973 = vadd.s32 1, %v9969
%v9977 = vadd.s32 %v9973, %v9961
%v9979 = vshll.u32 %v9973, 17
%v9980 = vshrl.u32 %v9973, 15
%v9981 = vor.u32 %v9980, %v9979
%v9982 = vxor.u32 %v9981, %v9977
%v9985 = vadd.s32 %v9982, %v9977
%v9987 = vshll.u32 %v9982, 29
%v9988 = vshrl.u32 %v9982, 3
%v9989 = vor.u32 %v9988, %v9987
%v9990 = vxor.u32 %v9989, %v9985
%v9993 = vadd.s32 %v9990, %v9985
%v9995 = vshll.u32 %v9990, 16
%v9996 = vshrl.u32 %v9990, 16
%v9997 = vor.u32 %v9996, %v9995
%v9998 = vxor.u32 %v9997, %v9993
%v10001 = vadd.s32 %v9998, %v9993
%v10005 = vadd.s32 %v10001, %v8
%v10007 = vshll.u32 %v9998, 24
%v10008 = vshrl.u32 %v9998, 8
%v10009 = vor.u32 %v10008, %v10007
%v10010 = vxor.u32 %v10009, %v10001
%v10013 = vadd.s32 %v10010, %v10
%v10017 = vadd.s32 2, %v10013
%v10021 = vadd.s32 %v10017, %v10005
%v10023 = vshll.u32 %v10017, 13
%v10024 = vshrl.u32 %v10017, 19
%v10025 = vor.u32 %v10024, %v10023
%v10026 = vxor.u32 %v10025, %v10021
%v10029 = vadd.s32 %v10026, %v10021
%v10031 = vshll.u32 %v10026, 15
%v10032 = vshrl.u32 %v10026, 17
%v10033 = vor.u32 %v10032, %v10031
%v10034 = vxor.u32 %v10033, %v10029
%v10037 = vadd.s32 %v10034, %v10029
%v10039 = vshll.u32 %v10034, 26
%v10040 = vshrl.u32 %v10034, 6
%v10041 = vor.u32 %v10040, %v10039
%v10042 = vxor.u32 %v10041, %v10037
%v10045 = vadd.s32 %v10042, %v10037
%v10049 = vadd.s32 %v10045, %v10
%v10051 = vshll.u32 %v10042, 6
%v10052 = vshrl.u32 %v10042, 26
%v10053 = vor.u32 %v10052, %v10051
%v10054 = vxor.u32 %v10053, %v10045
%v10057 = vadd.s32 %v10054, %v9
%v10061 = vadd.s32 3, %v10057
%v10065 = vadd.s32 %v10061, %v10049
%v10067 = vshll.u32 %v10061, 17
%v10068 = vshrl.u32 %v10061, 15
%v10069 = vor.u32 %v10068, %v10067
%v10070 = vxor.u32 %v10069, %v10065
%v10073 = vadd.s32 %v10070, %v10065
%v10075 = vshll.u32 %v10070, 29
%v10076 = vshrl.u32 %v10070, 3
%v10077 = vor.u32 %v10076, %v10075
%v10078 = vxor.u32 %v10077, %v10073
%v10081 = vadd.s32 %v10078, %v10073
%v10083 = vshll.u32 %v10078, 16
%v10084 = vshrl.u32 %v10078, 16
%v10085 = vor.u32 %v10084, %v10083
%v10086 = vxor.u32 %v10085, %v10081
%v10089 = vadd.s32 %v10086, %v10081
%v10093 = vadd.s32 %v10089, %v9
%v10095 = vshll.u32 %v10086, 24
%v10096 = vshrl.u32 %v10086, 8
%v10097 = vor.u32 %v10096, %v10095
%v10098 = vxor.u32 %v10097, %v10089
%v10101 = vadd.s32 %v10098, %v8
%v10105 = vadd.s32 4, %v10101
%v10109 = vadd.s32 %v10105, %v10093
%v10111 = vshll.u32 %v10105, 13
%v10112 = vshrl.u32 %v10105, 19
%v10113 = vor.u32 %v10112, %v10111
%v10114 = vxor.u32 %v10113, %v10109
%v10117 = vadd.s32 %v10114, %v10109
%v10119 = vshll.u32 %v10114, 15
%v10120 = vshrl.u32 %v10114, 17
%v10121 = vor.u32 %v10120, %v10119
%v10122 = vxor.u32 %v10121, %v10117
%v10125 = vadd.s32 %v10122, %v10117
%v10127 = vshll.u32 %v10122, 26
%v10128 = vshrl.u32 %v10122, 6
%v10129 = vor.u32 %v10128, %v10127
%v10130 = vxor.u32 %v10129, %v10125
%v10133 = vadd.s32 %v10130, %v10125
%v10137 = vadd.s32 %v10133, %v8
%v10139 = vshll.u32 %v10130, 6
%v10140 = vshrl.u32 %v10130, 26
%v10141 = vor.u32 %v10140, %v10139
%v10142 = vxor.u32 %v10141, %v10133
%v10145 = vadd.s32 %v10142, %v10
%v10149 = vadd.s32 5, %v10145
%v10151 = vxor.u32 %v10149, %v10137
%v10152 = vand.u32.u8 255, %v10151
%v10153 = vand.u32 65535, %v10152
%v10154 = vshrl.u32 %v10153, 1
%v10155 = vor.u32 16256, %v10154
%v10156 = vand.u32.u16 65535, %v10155
%v119796 = vadd.low.f32.bf16 -1.0, %v10156
%v10165 = vmul.f32 2.0, %v119796
%v10169 = vadd.f32 -0.99609375, %v10165
%v10173 = vmax.f32 %v10169, -0.99609375
%v10175 = vand.u32 2147483647, %v10173
%vm10178 = vcmp.eq.f32.partialorder %v10175, 1.0
%v10183 = vmul.f32 inf, %v10173
%v10185 = vxor.u32 2147483648, %v10173
%v10188 = vmul.f32 %v10185, %v10173
%v10190 = vadd.f32 1.0, %v10188
%v10191 = vlog2.pop %v10190
%v10192 = vmul.f32 0.6931472, %v10191
%v10193 = vmul.f32 -0.5, %v10188
%v10194 = vadd.f32 1.0, %v10193
%v10195 = vmul.f32 %v10194, %v10188
%v10196 = vand.u32 2147483647, %v10188
%vm10197 = vcmp.lt.f32.partialorder %v10196, 0.0004427343
%v10198 = vsel /*vm=*/%vm10197, /*on_true_vy=*/%v10195, /*on_false_vx=*/%v10192
%v10199 = vxor.u32 2147483648, %v10198
%vm10202 = vcmp.lt.f32.partialorder %v10199, 5.0
%v10207 = vsel /*vm=*/%vm10202, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v10211 = vsel /*vm=*/%vm10202, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v10215 = vsel /*vm=*/%vm10202, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v10219 = vsel /*vm=*/%vm10202, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v10223 = vsel /*vm=*/%vm10202, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v10227 = vsel /*vm=*/%vm10202, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v10231 = vsel /*vm=*/%vm10202, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v10235 = vsel /*vm=*/%vm10202, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v10239 = vsel /*vm=*/%vm10202, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v10243 = vadd.f32 -2.5, %v10199
%v10245 = vrsqrt.pop %v10199
%v10246 = vmul.f32 %v10245, %v10199
%vm10247 = vcmp.eq.f32.partialorder %v10199, inf
%v10248 = vsel /*vm=*/%vm10247, /*on_true_vy=*/%v10199, /*on_false_vx=*/%v10246
%vm10249 = vcmp.eq.f32.partialorder %v10199, 0.0
%v10250 = vand.u32 2147483648, %v10199
%v10251 = vsel /*vm=*/%vm10249, /*on_true_vy=*/%v10250, /*on_false_vx=*/%v10248
%v10254 = vadd.f32 -3.0, %v10251
%v10258 = vsel /*vm=*/%vm10202, /*on_true_vy=*/%v10243, /*on_false_vx=*/%v10254
%v10262 = vmul.f32 %v10258, %v10239
%v10266 = vadd.f32 %v10262, %v10235
%v10270 = vmul.f32 %v10266, %v10258
%v10274 = vadd.f32 %v10270, %v10231
%v10278 = vmul.f32 %v10274, %v10258
%v10282 = vadd.f32 %v10278, %v10227
%v10286 = vmul.f32 %v10282, %v10258
%v10290 = vadd.f32 %v10286, %v10223
%v10294 = vmul.f32 %v10290, %v10258
%v10298 = vadd.f32 %v10294, %v10219
%v10302 = vmul.f32 %v10298, %v10258
%v10306 = vadd.f32 %v10302, %v10215
%v10310 = vmul.f32 %v10306, %v10258
%v10314 = vadd.f32 %v10310, %v10211
%v10318 = vmul.f32 %v10314, %v10258
%v10322 = vadd.f32 %v10318, %v10207
%v10326 = vmul.f32 %v10322, %v10173
%v10330 = vsel /*vm=*/%vm10178, /*on_true_vy=*/%v10183, /*on_false_vx=*/%v10326
%v10334 = vmul.f32 1.4140625, %v10330
%v10337 = vpack.c.bf16 %v120417, %v10334
%119797 = vst [vmem:[%s280 + $0x208] sm:$0xf] /*vst_source=*/%v10337
%v10341 = vadd.s32 %v8033, %v2842
%v10351 = vadd.s32 %v10341, %v415
%vm10355 = vcmp.lt.u32.totalorder %v10351, %v10341
%vm10360 = vcmp.lt.u32.totalorder %v10341, %v2842
%v10365 = vadd.s32 %v8016, %v2829
%v10369 = vadd.s32 1, %v10365
%v10373 = vsel /*vm=*/%vm10360, /*on_true_vy=*/%v10369, /*on_false_vx=*/%v10365
%v10377 = vadd.s32 1, %v10373
%v10381 = vsel /*vm=*/%vm10355, /*on_true_vy=*/%v10377, /*on_false_vx=*/%v10373
%v10386 = vadd.s32 %v10381, %v10
%v10390 = vadd.s32 %v10351, %v9
%v10394 = vadd.s32 %v10390, %v10386
%v10396 = vshll.u32 %v10390, 13
%v10397 = vshrl.u32 %v10390, 19
%v10398 = vor.u32 %v10397, %v10396
%v10399 = vxor.u32 %v10398, %v10394
%v10402 = vadd.s32 %v10399, %v10394
%v10404 = vshll.u32 %v10399, 15
%v10405 = vshrl.u32 %v10399, 17
%v10406 = vor.u32 %v10405, %v10404
%v10407 = vxor.u32 %v10406, %v10402
%v10410 = vadd.s32 %v10407, %v10402
%v10412 = vshll.u32 %v10407, 26
%v10413 = vshrl.u32 %v10407, 6
%v10414 = vor.u32 %v10413, %v10412
%v10415 = vxor.u32 %v10414, %v10410
%v10418 = vadd.s32 %v10415, %v10410
%v10422 = vadd.s32 %v10418, %v9
%v10424 = vshll.u32 %v10415, 6
%v10425 = vshrl.u32 %v10415, 26
%v10426 = vor.u32 %v10425, %v10424
%v10427 = vxor.u32 %v10426, %v10418
%v10430 = vadd.s32 %v10427, %v8
%v10434 = vadd.s32 1, %v10430
%v10438 = vadd.s32 %v10434, %v10422
%v10440 = vshll.u32 %v10434, 17
%v10441 = vshrl.u32 %v10434, 15
%v10442 = vor.u32 %v10441, %v10440
%v10443 = vxor.u32 %v10442, %v10438
%v10446 = vadd.s32 %v10443, %v10438
%v10448 = vshll.u32 %v10443, 29
%v10449 = vshrl.u32 %v10443, 3
%v10450 = vor.u32 %v10449, %v10448
%v10451 = vxor.u32 %v10450, %v10446
%v10454 = vadd.s32 %v10451, %v10446
%v10456 = vshll.u32 %v10451, 16
%v10457 = vshrl.u32 %v10451, 16
%v10458 = vor.u32 %v10457, %v10456
%v10459 = vxor.u32 %v10458, %v10454
%v10462 = vadd.s32 %v10459, %v10454
%v10466 = vadd.s32 %v10462, %v8
%v10468 = vshll.u32 %v10459, 24
%v10469 = vshrl.u32 %v10459, 8
%v10470 = vor.u32 %v10469, %v10468
%v10471 = vxor.u32 %v10470, %v10462
%v10474 = vadd.s32 %v10471, %v10
%v10478 = vadd.s32 2, %v10474
%v10482 = vadd.s32 %v10478, %v10466
%v10484 = vshll.u32 %v10478, 13
%v10485 = vshrl.u32 %v10478, 19
%v10486 = vor.u32 %v10485, %v10484
%v10487 = vxor.u32 %v10486, %v10482
%v10490 = vadd.s32 %v10487, %v10482
%v10492 = vshll.u32 %v10487, 15
%v10493 = vshrl.u32 %v10487, 17
%v10494 = vor.u32 %v10493, %v10492
%v10495 = vxor.u32 %v10494, %v10490
%v10498 = vadd.s32 %v10495, %v10490
%v10500 = vshll.u32 %v10495, 26
%v10501 = vshrl.u32 %v10495, 6
%v10502 = vor.u32 %v10501, %v10500
%v10503 = vxor.u32 %v10502, %v10498
%v10506 = vadd.s32 %v10503, %v10498
%v10510 = vadd.s32 %v10506, %v10
%v10512 = vshll.u32 %v10503, 6
%v10513 = vshrl.u32 %v10503, 26
%v10514 = vor.u32 %v10513, %v10512
%v10515 = vxor.u32 %v10514, %v10506
%v10518 = vadd.s32 %v10515, %v9
%v10522 = vadd.s32 3, %v10518
%v10526 = vadd.s32 %v10522, %v10510
%v10528 = vshll.u32 %v10522, 17
%v10529 = vshrl.u32 %v10522, 15
%v10530 = vor.u32 %v10529, %v10528
%v10531 = vxor.u32 %v10530, %v10526
%v10534 = vadd.s32 %v10531, %v10526
%v10536 = vshll.u32 %v10531, 29
%v10537 = vshrl.u32 %v10531, 3
%v10538 = vor.u32 %v10537, %v10536
%v10539 = vxor.u32 %v10538, %v10534
%v10542 = vadd.s32 %v10539, %v10534
%v10544 = vshll.u32 %v10539, 16
%v10545 = vshrl.u32 %v10539, 16
%v10546 = vor.u32 %v10545, %v10544
%v10547 = vxor.u32 %v10546, %v10542
%v10550 = vadd.s32 %v10547, %v10542
%v10554 = vadd.s32 %v10550, %v9
%v10556 = vshll.u32 %v10547, 24
%v10557 = vshrl.u32 %v10547, 8
%v10558 = vor.u32 %v10557, %v10556
%v10559 = vxor.u32 %v10558, %v10550
%v10562 = vadd.s32 %v10559, %v8
%v10566 = vadd.s32 4, %v10562
%v10570 = vadd.s32 %v10566, %v10554
%v10572 = vshll.u32 %v10566, 13
%v10573 = vshrl.u32 %v10566, 19
%v10574 = vor.u32 %v10573, %v10572
%v10575 = vxor.u32 %v10574, %v10570
%v10578 = vadd.s32 %v10575, %v10570
%v10580 = vshll.u32 %v10575, 15
%v10581 = vshrl.u32 %v10575, 17
%v10582 = vor.u32 %v10581, %v10580
%v10583 = vxor.u32 %v10582, %v10578
%v10586 = vadd.s32 %v10583, %v10578
%v10588 = vshll.u32 %v10583, 26
%v10589 = vshrl.u32 %v10583, 6
%v10590 = vor.u32 %v10589, %v10588
%v10591 = vxor.u32 %v10590, %v10586
%v10594 = vadd.s32 %v10591, %v10586
%v10598 = vadd.s32 %v10594, %v8
%v10600 = vshll.u32 %v10591, 6
%v10601 = vshrl.u32 %v10591, 26
%v10602 = vor.u32 %v10601, %v10600
%v10603 = vxor.u32 %v10602, %v10594
%v10606 = vadd.s32 %v10603, %v10
%v10610 = vadd.s32 5, %v10606
%v10612 = vxor.u32 %v10610, %v10598
%v10613 = vand.u32.u8 255, %v10612
%v10614 = vand.u32 65535, %v10613
%v10615 = vshrl.u32 %v10614, 1
%v10616 = vor.u32 16256, %v10615
%v10617 = vand.u32.u16 65535, %v10616
%v119798 = vadd.low.f32.bf16 -1.0, %v10617
%v10626 = vmul.f32 2.0, %v119798
%v10630 = vadd.f32 -0.99609375, %v10626
%v10634 = vmax.f32 %v10630, -0.99609375
%v10636 = vand.u32 2147483647, %v10634
%vm10639 = vcmp.eq.f32.partialorder %v10636, 1.0
%v10644 = vmul.f32 inf, %v10634
%v10646 = vxor.u32 2147483648, %v10634
%v10649 = vmul.f32 %v10646, %v10634
%v10651 = vadd.f32 1.0, %v10649
%v10652 = vlog2.pop %v10651
%v10653 = vmul.f32 0.6931472, %v10652
%v10654 = vmul.f32 -0.5, %v10649
%v10655 = vadd.f32 1.0, %v10654
%v10656 = vmul.f32 %v10655, %v10649
%v10657 = vand.u32 2147483647, %v10649
%vm10658 = vcmp.lt.f32.partialorder %v10657, 0.0004427343
%v10659 = vsel /*vm=*/%vm10658, /*on_true_vy=*/%v10656, /*on_false_vx=*/%v10653
%v10660 = vxor.u32 2147483648, %v10659
%vm10663 = vcmp.lt.f32.partialorder %v10660, 5.0
%v10668 = vsel /*vm=*/%vm10663, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v10672 = vsel /*vm=*/%vm10663, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v10676 = vsel /*vm=*/%vm10663, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v10680 = vsel /*vm=*/%vm10663, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v10684 = vsel /*vm=*/%vm10663, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v10688 = vsel /*vm=*/%vm10663, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v10692 = vsel /*vm=*/%vm10663, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v10696 = vsel /*vm=*/%vm10663, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v10700 = vsel /*vm=*/%vm10663, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v10704 = vadd.f32 -2.5, %v10660
%v10706 = vrsqrt.pop %v10660
%v10707 = vmul.f32 %v10706, %v10660
%vm10708 = vcmp.eq.f32.partialorder %v10660, inf
%v10709 = vsel /*vm=*/%vm10708, /*on_true_vy=*/%v10660, /*on_false_vx=*/%v10707
%vm10710 = vcmp.eq.f32.partialorder %v10660, 0.0
%v10711 = vand.u32 2147483648, %v10660
%v10712 = vsel /*vm=*/%vm10710, /*on_true_vy=*/%v10711, /*on_false_vx=*/%v10709
%v10715 = vadd.f32 -3.0, %v10712
%v10719 = vsel /*vm=*/%vm10663, /*on_true_vy=*/%v10704, /*on_false_vx=*/%v10715
%v10723 = vmul.f32 %v10719, %v10700
%v10727 = vadd.f32 %v10723, %v10696
%v10731 = vmul.f32 %v10727, %v10719
%v10735 = vadd.f32 %v10731, %v10692
%v10739 = vmul.f32 %v10735, %v10719
%v10743 = vadd.f32 %v10739, %v10688
%v10747 = vmul.f32 %v10743, %v10719
%v10751 = vadd.f32 %v10747, %v10684
%v10755 = vmul.f32 %v10751, %v10719
%v10759 = vadd.f32 %v10755, %v10680
%v10763 = vmul.f32 %v10759, %v10719
%v10767 = vadd.f32 %v10763, %v10676
%v10771 = vmul.f32 %v10767, %v10719
%v10775 = vadd.f32 %v10771, %v10672
%v10779 = vmul.f32 %v10775, %v10719
%v10783 = vadd.f32 %v10779, %v10668
%v10787 = vmul.f32 %v10783, %v10634
%v10791 = vsel /*vm=*/%vm10639, /*on_true_vy=*/%v10644, /*on_false_vx=*/%v10787
%v10795 = vmul.f32 1.4140625, %v10791
%v10798 = vpack.c.bf16 %v120417, %v10795
%119799 = vst [vmem:[%s280 + $0x288] sm:$0xf] /*vst_source=*/%v10798
%v10802 = vadd.s32 %v8033, %v3329
%v10812 = vadd.s32 %v10802, %v415
%vm10816 = vcmp.lt.u32.totalorder %v10812, %v10802
%vm10821 = vcmp.lt.u32.totalorder %v10802, %v3329
%v10826 = vadd.s32 %v8016, %v3316
%v10830 = vadd.s32 1, %v10826
%v10834 = vsel /*vm=*/%vm10821, /*on_true_vy=*/%v10830, /*on_false_vx=*/%v10826
%v10838 = vadd.s32 1, %v10834
%v10842 = vsel /*vm=*/%vm10816, /*on_true_vy=*/%v10838, /*on_false_vx=*/%v10834
%v10847 = vadd.s32 %v10842, %v10
%v10851 = vadd.s32 %v10812, %v9
%v10855 = vadd.s32 %v10851, %v10847
%v10857 = vshll.u32 %v10851, 13
%v10858 = vshrl.u32 %v10851, 19
%v10859 = vor.u32 %v10858, %v10857
%v10860 = vxor.u32 %v10859, %v10855
%v10863 = vadd.s32 %v10860, %v10855
%v10865 = vshll.u32 %v10860, 15
%v10866 = vshrl.u32 %v10860, 17
%v10867 = vor.u32 %v10866, %v10865
%v10868 = vxor.u32 %v10867, %v10863
%v10871 = vadd.s32 %v10868, %v10863
%v10873 = vshll.u32 %v10868, 26
%v10874 = vshrl.u32 %v10868, 6
%v10875 = vor.u32 %v10874, %v10873
%v10876 = vxor.u32 %v10875, %v10871
%v10879 = vadd.s32 %v10876, %v10871
%v10883 = vadd.s32 %v10879, %v9
%v10885 = vshll.u32 %v10876, 6
%v10886 = vshrl.u32 %v10876, 26
%v10887 = vor.u32 %v10886, %v10885
%v10888 = vxor.u32 %v10887, %v10879
%v10891 = vadd.s32 %v10888, %v8
%v10895 = vadd.s32 1, %v10891
%v10899 = vadd.s32 %v10895, %v10883
%v10901 = vshll.u32 %v10895, 17
%v10902 = vshrl.u32 %v10895, 15
%v10903 = vor.u32 %v10902, %v10901
%v10904 = vxor.u32 %v10903, %v10899
%v10907 = vadd.s32 %v10904, %v10899
%v10909 = vshll.u32 %v10904, 29
%v10910 = vshrl.u32 %v10904, 3
%v10911 = vor.u32 %v10910, %v10909
%v10912 = vxor.u32 %v10911, %v10907
%v10915 = vadd.s32 %v10912, %v10907
%v10917 = vshll.u32 %v10912, 16
%v10918 = vshrl.u32 %v10912, 16
%v10919 = vor.u32 %v10918, %v10917
%v10920 = vxor.u32 %v10919, %v10915
%v10923 = vadd.s32 %v10920, %v10915
%v10927 = vadd.s32 %v10923, %v8
%v10929 = vshll.u32 %v10920, 24
%v10930 = vshrl.u32 %v10920, 8
%v10931 = vor.u32 %v10930, %v10929
%v10932 = vxor.u32 %v10931, %v10923
%v10935 = vadd.s32 %v10932, %v10
%v10939 = vadd.s32 2, %v10935
%v10943 = vadd.s32 %v10939, %v10927
%v10945 = vshll.u32 %v10939, 13
%v10946 = vshrl.u32 %v10939, 19
%v10947 = vor.u32 %v10946, %v10945
%v10948 = vxor.u32 %v10947, %v10943
%v10951 = vadd.s32 %v10948, %v10943
%v10953 = vshll.u32 %v10948, 15
%v10954 = vshrl.u32 %v10948, 17
%v10955 = vor.u32 %v10954, %v10953
%v10956 = vxor.u32 %v10955, %v10951
%v10959 = vadd.s32 %v10956, %v10951
%v10961 = vshll.u32 %v10956, 26
%v10962 = vshrl.u32 %v10956, 6
%v10963 = vor.u32 %v10962, %v10961
%v10964 = vxor.u32 %v10963, %v10959
%v10967 = vadd.s32 %v10964, %v10959
%v10971 = vadd.s32 %v10967, %v10
%v10973 = vshll.u32 %v10964, 6
%v10974 = vshrl.u32 %v10964, 26
%v10975 = vor.u32 %v10974, %v10973
%v10976 = vxor.u32 %v10975, %v10967
%v10979 = vadd.s32 %v10976, %v9
%v10983 = vadd.s32 3, %v10979
%v10987 = vadd.s32 %v10983, %v10971
%v10989 = vshll.u32 %v10983, 17
%v10990 = vshrl.u32 %v10983, 15
%v10991 = vor.u32 %v10990, %v10989
%v10992 = vxor.u32 %v10991, %v10987
%v10995 = vadd.s32 %v10992, %v10987
%v10997 = vshll.u32 %v10992, 29
%v10998 = vshrl.u32 %v10992, 3
%v10999 = vor.u32 %v10998, %v10997
%v11000 = vxor.u32 %v10999, %v10995
%v11003 = vadd.s32 %v11000, %v10995
%v11005 = vshll.u32 %v11000, 16
%v11006 = vshrl.u32 %v11000, 16
%v11007 = vor.u32 %v11006, %v11005
%v11008 = vxor.u32 %v11007, %v11003
%v11011 = vadd.s32 %v11008, %v11003
%v11015 = vadd.s32 %v11011, %v9
%v11017 = vshll.u32 %v11008, 24
%v11018 = vshrl.u32 %v11008, 8
%v11019 = vor.u32 %v11018, %v11017
%v11020 = vxor.u32 %v11019, %v11011
%v11023 = vadd.s32 %v11020, %v8
%v11027 = vadd.s32 4, %v11023
%v11031 = vadd.s32 %v11027, %v11015
%v11033 = vshll.u32 %v11027, 13
%v11034 = vshrl.u32 %v11027, 19
%v11035 = vor.u32 %v11034, %v11033
%v11036 = vxor.u32 %v11035, %v11031
%v11039 = vadd.s32 %v11036, %v11031
%v11041 = vshll.u32 %v11036, 15
%v11042 = vshrl.u32 %v11036, 17
%v11043 = vor.u32 %v11042, %v11041
%v11044 = vxor.u32 %v11043, %v11039
%v11047 = vadd.s32 %v11044, %v11039
%v11049 = vshll.u32 %v11044, 26
%v11050 = vshrl.u32 %v11044, 6
%v11051 = vor.u32 %v11050, %v11049
%v11052 = vxor.u32 %v11051, %v11047
%v11055 = vadd.s32 %v11052, %v11047
%v11059 = vadd.s32 %v11055, %v8
%v11061 = vshll.u32 %v11052, 6
%v11062 = vshrl.u32 %v11052, 26
%v11063 = vor.u32 %v11062, %v11061
%v11064 = vxor.u32 %v11063, %v11055
%v11067 = vadd.s32 %v11064, %v10
%v11071 = vadd.s32 5, %v11067
%v11073 = vxor.u32 %v11071, %v11059
%v11074 = vand.u32.u8 255, %v11073
%v11075 = vand.u32 65535, %v11074
%v11076 = vshrl.u32 %v11075, 1
%v11077 = vor.u32 16256, %v11076
%v11078 = vand.u32.u16 65535, %v11077
%v119800 = vadd.low.f32.bf16 -1.0, %v11078
%v11087 = vmul.f32 2.0, %v119800
%v11091 = vadd.f32 -0.99609375, %v11087
%v11095 = vmax.f32 %v11091, -0.99609375
%v11097 = vand.u32 2147483647, %v11095
%vm11100 = vcmp.eq.f32.partialorder %v11097, 1.0
%v11105 = vmul.f32 inf, %v11095
%v11107 = vxor.u32 2147483648, %v11095
%v11110 = vmul.f32 %v11107, %v11095
%v11112 = vadd.f32 1.0, %v11110
%v11113 = vlog2.pop %v11112
%v11114 = vmul.f32 0.6931472, %v11113
%v11115 = vmul.f32 -0.5, %v11110
%v11116 = vadd.f32 1.0, %v11115
%v11117 = vmul.f32 %v11116, %v11110
%v11118 = vand.u32 2147483647, %v11110
%vm11119 = vcmp.lt.f32.partialorder %v11118, 0.0004427343
%v11120 = vsel /*vm=*/%vm11119, /*on_true_vy=*/%v11117, /*on_false_vx=*/%v11114
%v11121 = vxor.u32 2147483648, %v11120
%vm11124 = vcmp.lt.f32.partialorder %v11121, 5.0
%v11129 = vsel /*vm=*/%vm11124, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v11133 = vsel /*vm=*/%vm11124, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v11137 = vsel /*vm=*/%vm11124, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v11141 = vsel /*vm=*/%vm11124, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v11145 = vsel /*vm=*/%vm11124, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v11149 = vsel /*vm=*/%vm11124, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v11153 = vsel /*vm=*/%vm11124, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v11157 = vsel /*vm=*/%vm11124, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v11161 = vsel /*vm=*/%vm11124, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v11165 = vadd.f32 -2.5, %v11121
%v11167 = vrsqrt.pop %v11121
%v11168 = vmul.f32 %v11167, %v11121
%vm11169 = vcmp.eq.f32.partialorder %v11121, inf
%v11170 = vsel /*vm=*/%vm11169, /*on_true_vy=*/%v11121, /*on_false_vx=*/%v11168
%vm11171 = vcmp.eq.f32.partialorder %v11121, 0.0
%v11172 = vand.u32 2147483648, %v11121
%v11173 = vsel /*vm=*/%vm11171, /*on_true_vy=*/%v11172, /*on_false_vx=*/%v11170
%v11176 = vadd.f32 -3.0, %v11173
%v11180 = vsel /*vm=*/%vm11124, /*on_true_vy=*/%v11165, /*on_false_vx=*/%v11176
%v11184 = vmul.f32 %v11180, %v11161
%v11188 = vadd.f32 %v11184, %v11157
%v11192 = vmul.f32 %v11188, %v11180
%v11196 = vadd.f32 %v11192, %v11153
%v11200 = vmul.f32 %v11196, %v11180
%v11204 = vadd.f32 %v11200, %v11149
%v11208 = vmul.f32 %v11204, %v11180
%v11212 = vadd.f32 %v11208, %v11145
%v11216 = vmul.f32 %v11212, %v11180
%v11220 = vadd.f32 %v11216, %v11141
%v11224 = vmul.f32 %v11220, %v11180
%v11228 = vadd.f32 %v11224, %v11137
%v11232 = vmul.f32 %v11228, %v11180
%v11236 = vadd.f32 %v11232, %v11133
%v11240 = vmul.f32 %v11236, %v11180
%v11244 = vadd.f32 %v11240, %v11129
%v11248 = vmul.f32 %v11244, %v11095
%v11252 = vsel /*vm=*/%vm11100, /*on_true_vy=*/%v11105, /*on_false_vx=*/%v11248
%v11256 = vmul.f32 1.4140625, %v11252
%v11259 = vpack.c.bf16 %v120417, %v11256
%119801 = vst [vmem:[%s280 + $0x308] sm:$0xf] /*vst_source=*/%v11259
%v11263 = vadd.s32 %v8033, %v3816
%v11273 = vadd.s32 %v11263, %v415
%vm11277 = vcmp.lt.u32.totalorder %v11273, %v11263
%vm11282 = vcmp.lt.u32.totalorder %v11263, %v3816
%v11287 = vadd.s32 %v8016, %v3803
%v11291 = vadd.s32 1, %v11287
%v11295 = vsel /*vm=*/%vm11282, /*on_true_vy=*/%v11291, /*on_false_vx=*/%v11287
%v11299 = vadd.s32 1, %v11295
%v11303 = vsel /*vm=*/%vm11277, /*on_true_vy=*/%v11299, /*on_false_vx=*/%v11295
%v11308 = vadd.s32 %v11303, %v10
%v11312 = vadd.s32 %v11273, %v9
%v11316 = vadd.s32 %v11312, %v11308
%v11318 = vshll.u32 %v11312, 13
%v11319 = vshrl.u32 %v11312, 19
%v11320 = vor.u32 %v11319, %v11318
%v11321 = vxor.u32 %v11320, %v11316
%v11324 = vadd.s32 %v11321, %v11316
%v11326 = vshll.u32 %v11321, 15
%v11327 = vshrl.u32 %v11321, 17
%v11328 = vor.u32 %v11327, %v11326
%v11329 = vxor.u32 %v11328, %v11324
%v11332 = vadd.s32 %v11329, %v11324
%v11334 = vshll.u32 %v11329, 26
%v11335 = vshrl.u32 %v11329, 6
%v11336 = vor.u32 %v11335, %v11334
%v11337 = vxor.u32 %v11336, %v11332
%v11340 = vadd.s32 %v11337, %v11332
%v11344 = vadd.s32 %v11340, %v9
%v11346 = vshll.u32 %v11337, 6
%v11347 = vshrl.u32 %v11337, 26
%v11348 = vor.u32 %v11347, %v11346
%v11349 = vxor.u32 %v11348, %v11340
%v11352 = vadd.s32 %v11349, %v8
%v11356 = vadd.s32 1, %v11352
%v11360 = vadd.s32 %v11356, %v11344
%v11362 = vshll.u32 %v11356, 17
%v11363 = vshrl.u32 %v11356, 15
%v11364 = vor.u32 %v11363, %v11362
%v11365 = vxor.u32 %v11364, %v11360
%v11368 = vadd.s32 %v11365, %v11360
%v11370 = vshll.u32 %v11365, 29
%v11371 = vshrl.u32 %v11365, 3
%v11372 = vor.u32 %v11371, %v11370
%v11373 = vxor.u32 %v11372, %v11368
%v11376 = vadd.s32 %v11373, %v11368
%v11378 = vshll.u32 %v11373, 16
%v11379 = vshrl.u32 %v11373, 16
%v11380 = vor.u32 %v11379, %v11378
%v11381 = vxor.u32 %v11380, %v11376
%v11384 = vadd.s32 %v11381, %v11376
%v11388 = vadd.s32 %v11384, %v8
%v11390 = vshll.u32 %v11381, 24
%v11391 = vshrl.u32 %v11381, 8
%v11392 = vor.u32 %v11391, %v11390
%v11393 = vxor.u32 %v11392, %v11384
%v11396 = vadd.s32 %v11393, %v10
%v11400 = vadd.s32 2, %v11396
%v11404 = vadd.s32 %v11400, %v11388
%v11406 = vshll.u32 %v11400, 13
%v11407 = vshrl.u32 %v11400, 19
%v11408 = vor.u32 %v11407, %v11406
%v11409 = vxor.u32 %v11408, %v11404
%v11412 = vadd.s32 %v11409, %v11404
%v11414 = vshll.u32 %v11409, 15
%v11415 = vshrl.u32 %v11409, 17
%v11416 = vor.u32 %v11415, %v11414
%v11417 = vxor.u32 %v11416, %v11412
%v11420 = vadd.s32 %v11417, %v11412
%v11422 = vshll.u32 %v11417, 26
%v11423 = vshrl.u32 %v11417, 6
%v11424 = vor.u32 %v11423, %v11422
%v11425 = vxor.u32 %v11424, %v11420
%v11428 = vadd.s32 %v11425, %v11420
%v11432 = vadd.s32 %v11428, %v10
%v11434 = vshll.u32 %v11425, 6
%v11435 = vshrl.u32 %v11425, 26
%v11436 = vor.u32 %v11435, %v11434
%v11437 = vxor.u32 %v11436, %v11428
%v11440 = vadd.s32 %v11437, %v9
%v11444 = vadd.s32 3, %v11440
%v11448 = vadd.s32 %v11444, %v11432
%v11450 = vshll.u32 %v11444, 17
%v11451 = vshrl.u32 %v11444, 15
%v11452 = vor.u32 %v11451, %v11450
%v11453 = vxor.u32 %v11452, %v11448
%v11456 = vadd.s32 %v11453, %v11448
%v11458 = vshll.u32 %v11453, 29
%v11459 = vshrl.u32 %v11453, 3
%v11460 = vor.u32 %v11459, %v11458
%v11461 = vxor.u32 %v11460, %v11456
%v11464 = vadd.s32 %v11461, %v11456
%v11466 = vshll.u32 %v11461, 16
%v11467 = vshrl.u32 %v11461, 16
%v11468 = vor.u32 %v11467, %v11466
%v11469 = vxor.u32 %v11468, %v11464
%v11472 = vadd.s32 %v11469, %v11464
%v11476 = vadd.s32 %v11472, %v9
%v11478 = vshll.u32 %v11469, 24
%v11479 = vshrl.u32 %v11469, 8
%v11480 = vor.u32 %v11479, %v11478
%v11481 = vxor.u32 %v11480, %v11472
%v11484 = vadd.s32 %v11481, %v8
%v11488 = vadd.s32 4, %v11484
%v11492 = vadd.s32 %v11488, %v11476
%v11494 = vshll.u32 %v11488, 13
%v11495 = vshrl.u32 %v11488, 19
%v11496 = vor.u32 %v11495, %v11494
%v11497 = vxor.u32 %v11496, %v11492
%v11500 = vadd.s32 %v11497, %v11492
%v11502 = vshll.u32 %v11497, 15
%v11503 = vshrl.u32 %v11497, 17
%v11504 = vor.u32 %v11503, %v11502
%v11505 = vxor.u32 %v11504, %v11500
%v11508 = vadd.s32 %v11505, %v11500
%v11510 = vshll.u32 %v11505, 26
%v11511 = vshrl.u32 %v11505, 6
%v11512 = vor.u32 %v11511, %v11510
%v11513 = vxor.u32 %v11512, %v11508
%v11516 = vadd.s32 %v11513, %v11508
%v11520 = vadd.s32 %v11516, %v8
%v11522 = vshll.u32 %v11513, 6
%v11523 = vshrl.u32 %v11513, 26
%v11524 = vor.u32 %v11523, %v11522
%v11525 = vxor.u32 %v11524, %v11516
%v11528 = vadd.s32 %v11525, %v10
%v11532 = vadd.s32 5, %v11528
%v11534 = vxor.u32 %v11532, %v11520
%v11535 = vand.u32.u8 255, %v11534
%v11536 = vand.u32 65535, %v11535
%v11537 = vshrl.u32 %v11536, 1
%v11538 = vor.u32 16256, %v11537
%v11539 = vand.u32.u16 65535, %v11538
%v119802 = vadd.low.f32.bf16 -1.0, %v11539
%v11548 = vmul.f32 2.0, %v119802
%v11552 = vadd.f32 -0.99609375, %v11548
%v11556 = vmax.f32 %v11552, -0.99609375
%v11558 = vand.u32 2147483647, %v11556
%vm11561 = vcmp.eq.f32.partialorder %v11558, 1.0
%v11566 = vmul.f32 inf, %v11556
%v11568 = vxor.u32 2147483648, %v11556
%v11571 = vmul.f32 %v11568, %v11556
%v11573 = vadd.f32 1.0, %v11571
%v11574 = vlog2.pop %v11573
%v11575 = vmul.f32 0.6931472, %v11574
%v11576 = vmul.f32 -0.5, %v11571
%v11577 = vadd.f32 1.0, %v11576
%v11578 = vmul.f32 %v11577, %v11571
%v11579 = vand.u32 2147483647, %v11571
%vm11580 = vcmp.lt.f32.partialorder %v11579, 0.0004427343
%v11581 = vsel /*vm=*/%vm11580, /*on_true_vy=*/%v11578, /*on_false_vx=*/%v11575
%v11582 = vxor.u32 2147483648, %v11581
%vm11585 = vcmp.lt.f32.partialorder %v11582, 5.0
%v11590 = vsel /*vm=*/%vm11585, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v11594 = vsel /*vm=*/%vm11585, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v11598 = vsel /*vm=*/%vm11585, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v11602 = vsel /*vm=*/%vm11585, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v11606 = vsel /*vm=*/%vm11585, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v11610 = vsel /*vm=*/%vm11585, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v11614 = vsel /*vm=*/%vm11585, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v11618 = vsel /*vm=*/%vm11585, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v11622 = vsel /*vm=*/%vm11585, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v11626 = vadd.f32 -2.5, %v11582
%v11628 = vrsqrt.pop %v11582
%v11629 = vmul.f32 %v11628, %v11582
%vm11630 = vcmp.eq.f32.partialorder %v11582, inf
%v11631 = vsel /*vm=*/%vm11630, /*on_true_vy=*/%v11582, /*on_false_vx=*/%v11629
%vm11632 = vcmp.eq.f32.partialorder %v11582, 0.0
%v11633 = vand.u32 2147483648, %v11582
%v11634 = vsel /*vm=*/%vm11632, /*on_true_vy=*/%v11633, /*on_false_vx=*/%v11631
%v11637 = vadd.f32 -3.0, %v11634
%v11641 = vsel /*vm=*/%vm11585, /*on_true_vy=*/%v11626, /*on_false_vx=*/%v11637
%v11645 = vmul.f32 %v11641, %v11622
%v11649 = vadd.f32 %v11645, %v11618
%v11653 = vmul.f32 %v11649, %v11641
%v11657 = vadd.f32 %v11653, %v11614
%v11661 = vmul.f32 %v11657, %v11641
%v11665 = vadd.f32 %v11661, %v11610
%v11669 = vmul.f32 %v11665, %v11641
%v11673 = vadd.f32 %v11669, %v11606
%v11677 = vmul.f32 %v11673, %v11641
%v11681 = vadd.f32 %v11677, %v11602
%v11685 = vmul.f32 %v11681, %v11641
%v11689 = vadd.f32 %v11685, %v11598
%v11693 = vmul.f32 %v11689, %v11641
%v11697 = vadd.f32 %v11693, %v11594
%v11701 = vmul.f32 %v11697, %v11641
%v11705 = vadd.f32 %v11701, %v11590
%v11709 = vmul.f32 %v11705, %v11556
%v11713 = vsel /*vm=*/%vm11561, /*on_true_vy=*/%v11566, /*on_false_vx=*/%v11709
%v11717 = vmul.f32 1.4140625, %v11713
%v11720 = vpack.c.bf16 %v120417, %v11717
%119803 = vst [vmem:[%s280 + $0x388] sm:$0xf] /*vst_source=*/%v11720
%v11758 = vadd.s32 %v11755, %v408
%v11768 = vadd.s32 %v11758, %v415
%vm11772 = vcmp.lt.u32.totalorder %v11768, %v11758
%vm11777 = vcmp.lt.u32.totalorder %v11758, %v408
%v11782 = vadd.s32 %v11738, %v380
%v11786 = vadd.s32 1, %v11782
%v11790 = vsel /*vm=*/%vm11777, /*on_true_vy=*/%v11786, /*on_false_vx=*/%v11782
%v11794 = vadd.s32 1, %v11790
%v11798 = vsel /*vm=*/%vm11772, /*on_true_vy=*/%v11794, /*on_false_vx=*/%v11790
%v11803 = vadd.s32 %v11798, %v10
%v11807 = vadd.s32 %v11768, %v9
%v11811 = vadd.s32 %v11807, %v11803
%v11813 = vshll.u32 %v11807, 13
%v11814 = vshrl.u32 %v11807, 19
%v11815 = vor.u32 %v11814, %v11813
%v11816 = vxor.u32 %v11815, %v11811
%v11819 = vadd.s32 %v11816, %v11811
%v11821 = vshll.u32 %v11816, 15
%v11822 = vshrl.u32 %v11816, 17
%v11823 = vor.u32 %v11822, %v11821
%v11824 = vxor.u32 %v11823, %v11819
%v11827 = vadd.s32 %v11824, %v11819
%v11829 = vshll.u32 %v11824, 26
%v11830 = vshrl.u32 %v11824, 6
%v11831 = vor.u32 %v11830, %v11829
%v11832 = vxor.u32 %v11831, %v11827
%v11835 = vadd.s32 %v11832, %v11827
%v11839 = vadd.s32 %v11835, %v9
%v11841 = vshll.u32 %v11832, 6
%v11842 = vshrl.u32 %v11832, 26
%v11843 = vor.u32 %v11842, %v11841
%v11844 = vxor.u32 %v11843, %v11835
%v11847 = vadd.s32 %v11844, %v8
%v11851 = vadd.s32 1, %v11847
%v11855 = vadd.s32 %v11851, %v11839
%v11857 = vshll.u32 %v11851, 17
%v11858 = vshrl.u32 %v11851, 15
%v11859 = vor.u32 %v11858, %v11857
%v11860 = vxor.u32 %v11859, %v11855
%v11863 = vadd.s32 %v11860, %v11855
%v11865 = vshll.u32 %v11860, 29
%v11866 = vshrl.u32 %v11860, 3
%v11867 = vor.u32 %v11866, %v11865
%v11868 = vxor.u32 %v11867, %v11863
%v11871 = vadd.s32 %v11868, %v11863
%v11873 = vshll.u32 %v11868, 16
%v11874 = vshrl.u32 %v11868, 16
%v11875 = vor.u32 %v11874, %v11873
%v11876 = vxor.u32 %v11875, %v11871
%v11879 = vadd.s32 %v11876, %v11871
%v11883 = vadd.s32 %v11879, %v8
%v11885 = vshll.u32 %v11876, 24
%v11886 = vshrl.u32 %v11876, 8
%v11887 = vor.u32 %v11886, %v11885
%v11888 = vxor.u32 %v11887, %v11879
%v11891 = vadd.s32 %v11888, %v10
%v11895 = vadd.s32 2, %v11891
%v11899 = vadd.s32 %v11895, %v11883
%v11901 = vshll.u32 %v11895, 13
%v11902 = vshrl.u32 %v11895, 19
%v11903 = vor.u32 %v11902, %v11901
%v11904 = vxor.u32 %v11903, %v11899
%v11907 = vadd.s32 %v11904, %v11899
%v11909 = vshll.u32 %v11904, 15
%v11910 = vshrl.u32 %v11904, 17
%v11911 = vor.u32 %v11910, %v11909
%v11912 = vxor.u32 %v11911, %v11907
%v11915 = vadd.s32 %v11912, %v11907
%v11917 = vshll.u32 %v11912, 26
%v11918 = vshrl.u32 %v11912, 6
%v11919 = vor.u32 %v11918, %v11917
%v11920 = vxor.u32 %v11919, %v11915
%v11923 = vadd.s32 %v11920, %v11915
%v11927 = vadd.s32 %v11923, %v10
%v11929 = vshll.u32 %v11920, 6
%v11930 = vshrl.u32 %v11920, 26
%v11931 = vor.u32 %v11930, %v11929
%v11932 = vxor.u32 %v11931, %v11923
%v11935 = vadd.s32 %v11932, %v9
%v11939 = vadd.s32 3, %v11935
%v11943 = vadd.s32 %v11939, %v11927
%v11945 = vshll.u32 %v11939, 17
%v11946 = vshrl.u32 %v11939, 15
%v11947 = vor.u32 %v11946, %v11945
%v11948 = vxor.u32 %v11947, %v11943
%v11951 = vadd.s32 %v11948, %v11943
%v11953 = vshll.u32 %v11948, 29
%v11954 = vshrl.u32 %v11948, 3
%v11955 = vor.u32 %v11954, %v11953
%v11956 = vxor.u32 %v11955, %v11951
%v11959 = vadd.s32 %v11956, %v11951
%v11961 = vshll.u32 %v11956, 16
%v11962 = vshrl.u32 %v11956, 16
%v11963 = vor.u32 %v11962, %v11961
%v11964 = vxor.u32 %v11963, %v11959
%v11967 = vadd.s32 %v11964, %v11959
%v11971 = vadd.s32 %v11967, %v9
%v11973 = vshll.u32 %v11964, 24
%v11974 = vshrl.u32 %v11964, 8
%v11975 = vor.u32 %v11974, %v11973
%v11976 = vxor.u32 %v11975, %v11967
%v11979 = vadd.s32 %v11976, %v8
%v11983 = vadd.s32 4, %v11979
%v11987 = vadd.s32 %v11983, %v11971
%v11989 = vshll.u32 %v11983, 13
%v11990 = vshrl.u32 %v11983, 19
%v11991 = vor.u32 %v11990, %v11989
%v11992 = vxor.u32 %v11991, %v11987
%v11995 = vadd.s32 %v11992, %v11987
%v11997 = vshll.u32 %v11992, 15
%v11998 = vshrl.u32 %v11992, 17
%v11999 = vor.u32 %v11998, %v11997
%v12000 = vxor.u32 %v11999, %v11995
%v12003 = vadd.s32 %v12000, %v11995
%v12005 = vshll.u32 %v12000, 26
%v12006 = vshrl.u32 %v12000, 6
%v12007 = vor.u32 %v12006, %v12005
%v12008 = vxor.u32 %v12007, %v12003
%v12011 = vadd.s32 %v12008, %v12003
%v12015 = vadd.s32 %v12011, %v8
%v12017 = vshll.u32 %v12008, 6
%v12018 = vshrl.u32 %v12008, 26
%v12019 = vor.u32 %v12018, %v12017
%v12020 = vxor.u32 %v12019, %v12011
%v12023 = vadd.s32 %v12020, %v10
%v12027 = vadd.s32 5, %v12023
%v12029 = vxor.u32 %v12027, %v12015
%v12030 = vand.u32.u8 255, %v12029
%v12031 = vand.u32 65535, %v12030
%v12032 = vshrl.u32 %v12031, 1
%v12033 = vor.u32 16256, %v12032
%v12034 = vand.u32.u16 65535, %v12033
%v119808 = vadd.low.f32.bf16 -1.0, %v12034
%v12043 = vmul.f32 2.0, %v119808
%v12047 = vadd.f32 -0.99609375, %v12043
%v12051 = vmax.f32 %v12047, -0.99609375
%v12053 = vand.u32 2147483647, %v12051
%vm12056 = vcmp.eq.f32.partialorder %v12053, 1.0
%v12061 = vmul.f32 inf, %v12051
%v12063 = vxor.u32 2147483648, %v12051
%v12066 = vmul.f32 %v12063, %v12051
%v12068 = vadd.f32 1.0, %v12066
%v12069 = vlog2.pop %v12068
%v12070 = vmul.f32 0.6931472, %v12069
%v12071 = vmul.f32 -0.5, %v12066
%v12072 = vadd.f32 1.0, %v12071
%v12073 = vmul.f32 %v12072, %v12066
%v12074 = vand.u32 2147483647, %v12066
%vm12075 = vcmp.lt.f32.partialorder %v12074, 0.0004427343
%v12076 = vsel /*vm=*/%vm12075, /*on_true_vy=*/%v12073, /*on_false_vx=*/%v12070
%v12077 = vxor.u32 2147483648, %v12076
%vm12080 = vcmp.lt.f32.partialorder %v12077, 5.0
%v12085 = vsel /*vm=*/%vm12080, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v12089 = vsel /*vm=*/%vm12080, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v12093 = vsel /*vm=*/%vm12080, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v12097 = vsel /*vm=*/%vm12080, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v12101 = vsel /*vm=*/%vm12080, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v12105 = vsel /*vm=*/%vm12080, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v12109 = vsel /*vm=*/%vm12080, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v12113 = vsel /*vm=*/%vm12080, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v12117 = vsel /*vm=*/%vm12080, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v12121 = vadd.f32 -2.5, %v12077
%v12123 = vrsqrt.pop %v12077
%v12124 = vmul.f32 %v12123, %v12077
%vm12125 = vcmp.eq.f32.partialorder %v12077, inf
%v12126 = vsel /*vm=*/%vm12125, /*on_true_vy=*/%v12077, /*on_false_vx=*/%v12124
%vm12127 = vcmp.eq.f32.partialorder %v12077, 0.0
%v12128 = vand.u32 2147483648, %v12077
%v12129 = vsel /*vm=*/%vm12127, /*on_true_vy=*/%v12128, /*on_false_vx=*/%v12126
%v12132 = vadd.f32 -3.0, %v12129
%v12136 = vsel /*vm=*/%vm12080, /*on_true_vy=*/%v12121, /*on_false_vx=*/%v12132
%v12140 = vmul.f32 %v12136, %v12117
%v12144 = vadd.f32 %v12140, %v12113
%v12148 = vmul.f32 %v12144, %v12136
%v12152 = vadd.f32 %v12148, %v12109
%v12156 = vmul.f32 %v12152, %v12136
%v12160 = vadd.f32 %v12156, %v12105
%v12164 = vmul.f32 %v12160, %v12136
%v12168 = vadd.f32 %v12164, %v12101
%v12172 = vmul.f32 %v12168, %v12136
%v12176 = vadd.f32 %v12172, %v12097
%v12180 = vmul.f32 %v12176, %v12136
%v12184 = vadd.f32 %v12180, %v12093
%v12188 = vmul.f32 %v12184, %v12136
%v12192 = vadd.f32 %v12188, %v12089
%v12196 = vmul.f32 %v12192, %v12136
%v12200 = vadd.f32 %v12196, %v12085
%v12204 = vmul.f32 %v12200, %v12051
%v12208 = vsel /*vm=*/%vm12056, /*on_true_vy=*/%v12061, /*on_false_vx=*/%v12204
%v12212 = vmul.f32 1.4140625, %v12208
%v12215 = vpack.c.bf16 %v120417, %v12212
%119809 = vst [vmem:[%s280 + $0xc] sm:$0xf] /*vst_source=*/%v12215
%v12219 = vadd.s32 %v11755, %v894
%v12229 = vadd.s32 %v12219, %v415
%vm12233 = vcmp.lt.u32.totalorder %v12229, %v12219
%vm12238 = vcmp.lt.u32.totalorder %v12219, %v894
%v12243 = vadd.s32 %v11738, %v881
%v12247 = vadd.s32 1, %v12243
%v12251 = vsel /*vm=*/%vm12238, /*on_true_vy=*/%v12247, /*on_false_vx=*/%v12243
%v12255 = vadd.s32 1, %v12251
%v12259 = vsel /*vm=*/%vm12233, /*on_true_vy=*/%v12255, /*on_false_vx=*/%v12251
%v12264 = vadd.s32 %v12259, %v10
%v12268 = vadd.s32 %v12229, %v9
%v12272 = vadd.s32 %v12268, %v12264
%v12274 = vshll.u32 %v12268, 13
%v12275 = vshrl.u32 %v12268, 19
%v12276 = vor.u32 %v12275, %v12274
%v12277 = vxor.u32 %v12276, %v12272
%v12280 = vadd.s32 %v12277, %v12272
%v12282 = vshll.u32 %v12277, 15
%v12283 = vshrl.u32 %v12277, 17
%v12284 = vor.u32 %v12283, %v12282
%v12285 = vxor.u32 %v12284, %v12280
%v12288 = vadd.s32 %v12285, %v12280
%v12290 = vshll.u32 %v12285, 26
%v12291 = vshrl.u32 %v12285, 6
%v12292 = vor.u32 %v12291, %v12290
%v12293 = vxor.u32 %v12292, %v12288
%v12296 = vadd.s32 %v12293, %v12288
%v12300 = vadd.s32 %v12296, %v9
%v12302 = vshll.u32 %v12293, 6
%v12303 = vshrl.u32 %v12293, 26
%v12304 = vor.u32 %v12303, %v12302
%v12305 = vxor.u32 %v12304, %v12296
%v12308 = vadd.s32 %v12305, %v8
%v12312 = vadd.s32 1, %v12308
%v12316 = vadd.s32 %v12312, %v12300
%v12318 = vshll.u32 %v12312, 17
%v12319 = vshrl.u32 %v12312, 15
%v12320 = vor.u32 %v12319, %v12318
%v12321 = vxor.u32 %v12320, %v12316
%v12324 = vadd.s32 %v12321, %v12316
%v12326 = vshll.u32 %v12321, 29
%v12327 = vshrl.u32 %v12321, 3
%v12328 = vor.u32 %v12327, %v12326
%v12329 = vxor.u32 %v12328, %v12324
%v12332 = vadd.s32 %v12329, %v12324
%v12334 = vshll.u32 %v12329, 16
%v12335 = vshrl.u32 %v12329, 16
%v12336 = vor.u32 %v12335, %v12334
%v12337 = vxor.u32 %v12336, %v12332
%v12340 = vadd.s32 %v12337, %v12332
%v12344 = vadd.s32 %v12340, %v8
%v12346 = vshll.u32 %v12337, 24
%v12347 = vshrl.u32 %v12337, 8
%v12348 = vor.u32 %v12347, %v12346
%v12349 = vxor.u32 %v12348, %v12340
%v12352 = vadd.s32 %v12349, %v10
%v12356 = vadd.s32 2, %v12352
%v12360 = vadd.s32 %v12356, %v12344
%v12362 = vshll.u32 %v12356, 13
%v12363 = vshrl.u32 %v12356, 19
%v12364 = vor.u32 %v12363, %v12362
%v12365 = vxor.u32 %v12364, %v12360
%v12368 = vadd.s32 %v12365, %v12360
%v12370 = vshll.u32 %v12365, 15
%v12371 = vshrl.u32 %v12365, 17
%v12372 = vor.u32 %v12371, %v12370
%v12373 = vxor.u32 %v12372, %v12368
%v12376 = vadd.s32 %v12373, %v12368
%v12378 = vshll.u32 %v12373, 26
%v12379 = vshrl.u32 %v12373, 6
%v12380 = vor.u32 %v12379, %v12378
%v12381 = vxor.u32 %v12380, %v12376
%v12384 = vadd.s32 %v12381, %v12376
%v12388 = vadd.s32 %v12384, %v10
%v12390 = vshll.u32 %v12381, 6
%v12391 = vshrl.u32 %v12381, 26
%v12392 = vor.u32 %v12391, %v12390
%v12393 = vxor.u32 %v12392, %v12384
%v12396 = vadd.s32 %v12393, %v9
%v12400 = vadd.s32 3, %v12396
%v12404 = vadd.s32 %v12400, %v12388
%v12406 = vshll.u32 %v12400, 17
%v12407 = vshrl.u32 %v12400, 15
%v12408 = vor.u32 %v12407, %v12406
%v12409 = vxor.u32 %v12408, %v12404
%v12412 = vadd.s32 %v12409, %v12404
%v12414 = vshll.u32 %v12409, 29
%v12415 = vshrl.u32 %v12409, 3
%v12416 = vor.u32 %v12415, %v12414
%v12417 = vxor.u32 %v12416, %v12412
%v12420 = vadd.s32 %v12417, %v12412
%v12422 = vshll.u32 %v12417, 16
%v12423 = vshrl.u32 %v12417, 16
%v12424 = vor.u32 %v12423, %v12422
%v12425 = vxor.u32 %v12424, %v12420
%v12428 = vadd.s32 %v12425, %v12420
%v12432 = vadd.s32 %v12428, %v9
%v12434 = vshll.u32 %v12425, 24
%v12435 = vshrl.u32 %v12425, 8
%v12436 = vor.u32 %v12435, %v12434
%v12437 = vxor.u32 %v12436, %v12428
%v12440 = vadd.s32 %v12437, %v8
%v12444 = vadd.s32 4, %v12440
%v12448 = vadd.s32 %v12444, %v12432
%v12450 = vshll.u32 %v12444, 13
%v12451 = vshrl.u32 %v12444, 19
%v12452 = vor.u32 %v12451, %v12450
%v12453 = vxor.u32 %v12452, %v12448
%v12456 = vadd.s32 %v12453, %v12448
%v12458 = vshll.u32 %v12453, 15
%v12459 = vshrl.u32 %v12453, 17
%v12460 = vor.u32 %v12459, %v12458
%v12461 = vxor.u32 %v12460, %v12456
%v12464 = vadd.s32 %v12461, %v12456
%v12466 = vshll.u32 %v12461, 26
%v12467 = vshrl.u32 %v12461, 6
%v12468 = vor.u32 %v12467, %v12466
%v12469 = vxor.u32 %v12468, %v12464
%v12472 = vadd.s32 %v12469, %v12464
%v12476 = vadd.s32 %v12472, %v8
%v12478 = vshll.u32 %v12469, 6
%v12479 = vshrl.u32 %v12469, 26
%v12480 = vor.u32 %v12479, %v12478
%v12481 = vxor.u32 %v12480, %v12472
%v12484 = vadd.s32 %v12481, %v10
%v12488 = vadd.s32 5, %v12484
%v12490 = vxor.u32 %v12488, %v12476
%v12491 = vand.u32.u8 255, %v12490
%v12492 = vand.u32 65535, %v12491
%v12493 = vshrl.u32 %v12492, 1
%v12494 = vor.u32 16256, %v12493
%v12495 = vand.u32.u16 65535, %v12494
%v119810 = vadd.low.f32.bf16 -1.0, %v12495
%v12504 = vmul.f32 2.0, %v119810
%v12508 = vadd.f32 -0.99609375, %v12504
%v12512 = vmax.f32 %v12508, -0.99609375
%v12514 = vand.u32 2147483647, %v12512
%vm12517 = vcmp.eq.f32.partialorder %v12514, 1.0
%v12522 = vmul.f32 inf, %v12512
%v12524 = vxor.u32 2147483648, %v12512
%v12527 = vmul.f32 %v12524, %v12512
%v12529 = vadd.f32 1.0, %v12527
%v12530 = vlog2.pop %v12529
%v12531 = vmul.f32 0.6931472, %v12530
%v12532 = vmul.f32 -0.5, %v12527
%v12533 = vadd.f32 1.0, %v12532
%v12534 = vmul.f32 %v12533, %v12527
%v12535 = vand.u32 2147483647, %v12527
%vm12536 = vcmp.lt.f32.partialorder %v12535, 0.0004427343
%v12537 = vsel /*vm=*/%vm12536, /*on_true_vy=*/%v12534, /*on_false_vx=*/%v12531
%v12538 = vxor.u32 2147483648, %v12537
%vm12541 = vcmp.lt.f32.partialorder %v12538, 5.0
%v12546 = vsel /*vm=*/%vm12541, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v12550 = vsel /*vm=*/%vm12541, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v12554 = vsel /*vm=*/%vm12541, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v12558 = vsel /*vm=*/%vm12541, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v12562 = vsel /*vm=*/%vm12541, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v12566 = vsel /*vm=*/%vm12541, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v12570 = vsel /*vm=*/%vm12541, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v12574 = vsel /*vm=*/%vm12541, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v12578 = vsel /*vm=*/%vm12541, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v12582 = vadd.f32 -2.5, %v12538
%v12584 = vrsqrt.pop %v12538
%v12585 = vmul.f32 %v12584, %v12538
%vm12586 = vcmp.eq.f32.partialorder %v12538, inf
%v12587 = vsel /*vm=*/%vm12586, /*on_true_vy=*/%v12538, /*on_false_vx=*/%v12585
%vm12588 = vcmp.eq.f32.partialorder %v12538, 0.0
%v12589 = vand.u32 2147483648, %v12538
%v12590 = vsel /*vm=*/%vm12588, /*on_true_vy=*/%v12589, /*on_false_vx=*/%v12587
%v12593 = vadd.f32 -3.0, %v12590
%v12597 = vsel /*vm=*/%vm12541, /*on_true_vy=*/%v12582, /*on_false_vx=*/%v12593
%v12601 = vmul.f32 %v12597, %v12578
%v12605 = vadd.f32 %v12601, %v12574
%v12609 = vmul.f32 %v12605, %v12597
%v12613 = vadd.f32 %v12609, %v12570
%v12617 = vmul.f32 %v12613, %v12597
%v12621 = vadd.f32 %v12617, %v12566
%v12625 = vmul.f32 %v12621, %v12597
%v12629 = vadd.f32 %v12625, %v12562
%v12633 = vmul.f32 %v12629, %v12597
%v12637 = vadd.f32 %v12633, %v12558
%v12641 = vmul.f32 %v12637, %v12597
%v12645 = vadd.f32 %v12641, %v12554
%v12649 = vmul.f32 %v12645, %v12597
%v12653 = vadd.f32 %v12649, %v12550
%v12657 = vmul.f32 %v12653, %v12597
%v12661 = vadd.f32 %v12657, %v12546
%v12665 = vmul.f32 %v12661, %v12512
%v12669 = vsel /*vm=*/%vm12517, /*on_true_vy=*/%v12522, /*on_false_vx=*/%v12665
%v12673 = vmul.f32 1.4140625, %v12669
%v12676 = vpack.c.bf16 %v120417, %v12673
%119811 = vst [vmem:[%s280 + $0x8c] sm:$0xf] /*vst_source=*/%v12676
%v12680 = vadd.s32 %v11755, %v1381
%v12690 = vadd.s32 %v12680, %v415
%vm12694 = vcmp.lt.u32.totalorder %v12690, %v12680
%vm12699 = vcmp.lt.u32.totalorder %v12680, %v1381
%v12704 = vadd.s32 %v11738, %v1368
%v12708 = vadd.s32 1, %v12704
%v12712 = vsel /*vm=*/%vm12699, /*on_true_vy=*/%v12708, /*on_false_vx=*/%v12704
%v12716 = vadd.s32 1, %v12712
%v12720 = vsel /*vm=*/%vm12694, /*on_true_vy=*/%v12716, /*on_false_vx=*/%v12712
%v12725 = vadd.s32 %v12720, %v10
%v12729 = vadd.s32 %v12690, %v9
%v12733 = vadd.s32 %v12729, %v12725
%v12735 = vshll.u32 %v12729, 13
%v12736 = vshrl.u32 %v12729, 19
%v12737 = vor.u32 %v12736, %v12735
%v12738 = vxor.u32 %v12737, %v12733
%v12741 = vadd.s32 %v12738, %v12733
%v12743 = vshll.u32 %v12738, 15
%v12744 = vshrl.u32 %v12738, 17
%v12745 = vor.u32 %v12744, %v12743
%v12746 = vxor.u32 %v12745, %v12741
%v12749 = vadd.s32 %v12746, %v12741
%v12751 = vshll.u32 %v12746, 26
%v12752 = vshrl.u32 %v12746, 6
%v12753 = vor.u32 %v12752, %v12751
%v12754 = vxor.u32 %v12753, %v12749
%v12757 = vadd.s32 %v12754, %v12749
%v12761 = vadd.s32 %v12757, %v9
%v12763 = vshll.u32 %v12754, 6
%v12764 = vshrl.u32 %v12754, 26
%v12765 = vor.u32 %v12764, %v12763
%v12766 = vxor.u32 %v12765, %v12757
%v12769 = vadd.s32 %v12766, %v8
%v12773 = vadd.s32 1, %v12769
%v12777 = vadd.s32 %v12773, %v12761
%v12779 = vshll.u32 %v12773, 17
%v12780 = vshrl.u32 %v12773, 15
%v12781 = vor.u32 %v12780, %v12779
%v12782 = vxor.u32 %v12781, %v12777
%v12785 = vadd.s32 %v12782, %v12777
%v12787 = vshll.u32 %v12782, 29
%v12788 = vshrl.u32 %v12782, 3
%v12789 = vor.u32 %v12788, %v12787
%v12790 = vxor.u32 %v12789, %v12785
%v12793 = vadd.s32 %v12790, %v12785
%v12795 = vshll.u32 %v12790, 16
%v12796 = vshrl.u32 %v12790, 16
%v12797 = vor.u32 %v12796, %v12795
%v12798 = vxor.u32 %v12797, %v12793
%v12801 = vadd.s32 %v12798, %v12793
%v12805 = vadd.s32 %v12801, %v8
%v12807 = vshll.u32 %v12798, 24
%v12808 = vshrl.u32 %v12798, 8
%v12809 = vor.u32 %v12808, %v12807
%v12810 = vxor.u32 %v12809, %v12801
%v12813 = vadd.s32 %v12810, %v10
%v12817 = vadd.s32 2, %v12813
%v12821 = vadd.s32 %v12817, %v12805
%v12823 = vshll.u32 %v12817, 13
%v12824 = vshrl.u32 %v12817, 19
%v12825 = vor.u32 %v12824, %v12823
%v12826 = vxor.u32 %v12825, %v12821
%v12829 = vadd.s32 %v12826, %v12821
%v12831 = vshll.u32 %v12826, 15
%v12832 = vshrl.u32 %v12826, 17
%v12833 = vor.u32 %v12832, %v12831
%v12834 = vxor.u32 %v12833, %v12829
%v12837 = vadd.s32 %v12834, %v12829
%v12839 = vshll.u32 %v12834, 26
%v12840 = vshrl.u32 %v12834, 6
%v12841 = vor.u32 %v12840, %v12839
%v12842 = vxor.u32 %v12841, %v12837
%v12845 = vadd.s32 %v12842, %v12837
%v12849 = vadd.s32 %v12845, %v10
%v12851 = vshll.u32 %v12842, 6
%v12852 = vshrl.u32 %v12842, 26
%v12853 = vor.u32 %v12852, %v12851
%v12854 = vxor.u32 %v12853, %v12845
%v12857 = vadd.s32 %v12854, %v9
%v12861 = vadd.s32 3, %v12857
%v12865 = vadd.s32 %v12861, %v12849
%v12867 = vshll.u32 %v12861, 17
%v12868 = vshrl.u32 %v12861, 15
%v12869 = vor.u32 %v12868, %v12867
%v12870 = vxor.u32 %v12869, %v12865
%v12873 = vadd.s32 %v12870, %v12865
%v12875 = vshll.u32 %v12870, 29
%v12876 = vshrl.u32 %v12870, 3
%v12877 = vor.u32 %v12876, %v12875
%v12878 = vxor.u32 %v12877, %v12873
%v12881 = vadd.s32 %v12878, %v12873
%v12883 = vshll.u32 %v12878, 16
%v12884 = vshrl.u32 %v12878, 16
%v12885 = vor.u32 %v12884, %v12883
%v12886 = vxor.u32 %v12885, %v12881
%v12889 = vadd.s32 %v12886, %v12881
%v12893 = vadd.s32 %v12889, %v9
%v12895 = vshll.u32 %v12886, 24
%v12896 = vshrl.u32 %v12886, 8
%v12897 = vor.u32 %v12896, %v12895
%v12898 = vxor.u32 %v12897, %v12889
%v12901 = vadd.s32 %v12898, %v8
%v12905 = vadd.s32 4, %v12901
%v12909 = vadd.s32 %v12905, %v12893
%v12911 = vshll.u32 %v12905, 13
%v12912 = vshrl.u32 %v12905, 19
%v12913 = vor.u32 %v12912, %v12911
%v12914 = vxor.u32 %v12913, %v12909
%v12917 = vadd.s32 %v12914, %v12909
%v12919 = vshll.u32 %v12914, 15
%v12920 = vshrl.u32 %v12914, 17
%v12921 = vor.u32 %v12920, %v12919
%v12922 = vxor.u32 %v12921, %v12917
%v12925 = vadd.s32 %v12922, %v12917
%v12927 = vshll.u32 %v12922, 26
%v12928 = vshrl.u32 %v12922, 6
%v12929 = vor.u32 %v12928, %v12927
%v12930 = vxor.u32 %v12929, %v12925
%v12933 = vadd.s32 %v12930, %v12925
%v12937 = vadd.s32 %v12933, %v8
%v12939 = vshll.u32 %v12930, 6
%v12940 = vshrl.u32 %v12930, 26
%v12941 = vor.u32 %v12940, %v12939
%v12942 = vxor.u32 %v12941, %v12933
%v12945 = vadd.s32 %v12942, %v10
%v12949 = vadd.s32 5, %v12945
%v12951 = vxor.u32 %v12949, %v12937
%v12952 = vand.u32.u8 255, %v12951
%v12953 = vand.u32 65535, %v12952
%v12954 = vshrl.u32 %v12953, 1
%v12955 = vor.u32 16256, %v12954
%v12956 = vand.u32.u16 65535, %v12955
%v119812 = vadd.low.f32.bf16 -1.0, %v12956
%v12965 = vmul.f32 2.0, %v119812
%v12969 = vadd.f32 -0.99609375, %v12965
%v12973 = vmax.f32 %v12969, -0.99609375
%v12975 = vand.u32 2147483647, %v12973
%vm12978 = vcmp.eq.f32.partialorder %v12975, 1.0
%v12983 = vmul.f32 inf, %v12973
%v12985 = vxor.u32 2147483648, %v12973
%v12988 = vmul.f32 %v12985, %v12973
%v12990 = vadd.f32 1.0, %v12988
%v12991 = vlog2.pop %v12990
%v12992 = vmul.f32 0.6931472, %v12991
%v12993 = vmul.f32 -0.5, %v12988
%v12994 = vadd.f32 1.0, %v12993
%v12995 = vmul.f32 %v12994, %v12988
%v12996 = vand.u32 2147483647, %v12988
%vm12997 = vcmp.lt.f32.partialorder %v12996, 0.0004427343
%v12998 = vsel /*vm=*/%vm12997, /*on_true_vy=*/%v12995, /*on_false_vx=*/%v12992
%v12999 = vxor.u32 2147483648, %v12998
%vm13002 = vcmp.lt.f32.partialorder %v12999, 5.0
%v13007 = vsel /*vm=*/%vm13002, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v13011 = vsel /*vm=*/%vm13002, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v13015 = vsel /*vm=*/%vm13002, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v13019 = vsel /*vm=*/%vm13002, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v13023 = vsel /*vm=*/%vm13002, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v13027 = vsel /*vm=*/%vm13002, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v13031 = vsel /*vm=*/%vm13002, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v13035 = vsel /*vm=*/%vm13002, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v13039 = vsel /*vm=*/%vm13002, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v13043 = vadd.f32 -2.5, %v12999
%v13045 = vrsqrt.pop %v12999
%v13046 = vmul.f32 %v13045, %v12999
%vm13047 = vcmp.eq.f32.partialorder %v12999, inf
%v13048 = vsel /*vm=*/%vm13047, /*on_true_vy=*/%v12999, /*on_false_vx=*/%v13046
%vm13049 = vcmp.eq.f32.partialorder %v12999, 0.0
%v13050 = vand.u32 2147483648, %v12999
%v13051 = vsel /*vm=*/%vm13049, /*on_true_vy=*/%v13050, /*on_false_vx=*/%v13048
%v13054 = vadd.f32 -3.0, %v13051
%v13058 = vsel /*vm=*/%vm13002, /*on_true_vy=*/%v13043, /*on_false_vx=*/%v13054
%v13062 = vmul.f32 %v13058, %v13039
%v13066 = vadd.f32 %v13062, %v13035
%v13070 = vmul.f32 %v13066, %v13058
%v13074 = vadd.f32 %v13070, %v13031
%v13078 = vmul.f32 %v13074, %v13058
%v13082 = vadd.f32 %v13078, %v13027
%v13086 = vmul.f32 %v13082, %v13058
%v13090 = vadd.f32 %v13086, %v13023
%v13094 = vmul.f32 %v13090, %v13058
%v13098 = vadd.f32 %v13094, %v13019
%v13102 = vmul.f32 %v13098, %v13058
%v13106 = vadd.f32 %v13102, %v13015
%v13110 = vmul.f32 %v13106, %v13058
%v13114 = vadd.f32 %v13110, %v13011
%v13118 = vmul.f32 %v13114, %v13058
%v13122 = vadd.f32 %v13118, %v13007
%v13126 = vmul.f32 %v13122, %v12973
%v13130 = vsel /*vm=*/%vm12978, /*on_true_vy=*/%v12983, /*on_false_vx=*/%v13126
%v13134 = vmul.f32 1.4140625, %v13130
%v13137 = vpack.c.bf16 %v120417, %v13134
%119813 = vst [vmem:[%s280 + $0x10c] sm:$0xf] /*vst_source=*/%v13137
%v13141 = vadd.s32 %v11755, %v1868
%v13151 = vadd.s32 %v13141, %v415
%vm13155 = vcmp.lt.u32.totalorder %v13151, %v13141
%vm13160 = vcmp.lt.u32.totalorder %v13141, %v1868
%v13165 = vadd.s32 %v11738, %v1855
%v13169 = vadd.s32 1, %v13165
%v13173 = vsel /*vm=*/%vm13160, /*on_true_vy=*/%v13169, /*on_false_vx=*/%v13165
%v13177 = vadd.s32 1, %v13173
%v13181 = vsel /*vm=*/%vm13155, /*on_true_vy=*/%v13177, /*on_false_vx=*/%v13173
%v13186 = vadd.s32 %v13181, %v10
%v13190 = vadd.s32 %v13151, %v9
%v13194 = vadd.s32 %v13190, %v13186
%v13196 = vshll.u32 %v13190, 13
%v13197 = vshrl.u32 %v13190, 19
%v13198 = vor.u32 %v13197, %v13196
%v13199 = vxor.u32 %v13198, %v13194
%v13202 = vadd.s32 %v13199, %v13194
%v13204 = vshll.u32 %v13199, 15
%v13205 = vshrl.u32 %v13199, 17
%v13206 = vor.u32 %v13205, %v13204
%v13207 = vxor.u32 %v13206, %v13202
%v13210 = vadd.s32 %v13207, %v13202
%v13212 = vshll.u32 %v13207, 26
%v13213 = vshrl.u32 %v13207, 6
%v13214 = vor.u32 %v13213, %v13212
%v13215 = vxor.u32 %v13214, %v13210
%v13218 = vadd.s32 %v13215, %v13210
%v13222 = vadd.s32 %v13218, %v9
%v13224 = vshll.u32 %v13215, 6
%v13225 = vshrl.u32 %v13215, 26
%v13226 = vor.u32 %v13225, %v13224
%v13227 = vxor.u32 %v13226, %v13218
%v13230 = vadd.s32 %v13227, %v8
%v13234 = vadd.s32 1, %v13230
%v13238 = vadd.s32 %v13234, %v13222
%v13240 = vshll.u32 %v13234, 17
%v13241 = vshrl.u32 %v13234, 15
%v13242 = vor.u32 %v13241, %v13240
%v13243 = vxor.u32 %v13242, %v13238
%v13246 = vadd.s32 %v13243, %v13238
%v13248 = vshll.u32 %v13243, 29
%v13249 = vshrl.u32 %v13243, 3
%v13250 = vor.u32 %v13249, %v13248
%v13251 = vxor.u32 %v13250, %v13246
%v13254 = vadd.s32 %v13251, %v13246
%v13256 = vshll.u32 %v13251, 16
%v13257 = vshrl.u32 %v13251, 16
%v13258 = vor.u32 %v13257, %v13256
%v13259 = vxor.u32 %v13258, %v13254
%v13262 = vadd.s32 %v13259, %v13254
%v13266 = vadd.s32 %v13262, %v8
%v13268 = vshll.u32 %v13259, 24
%v13269 = vshrl.u32 %v13259, 8
%v13270 = vor.u32 %v13269, %v13268
%v13271 = vxor.u32 %v13270, %v13262
%v13274 = vadd.s32 %v13271, %v10
%v13278 = vadd.s32 2, %v13274
%v13282 = vadd.s32 %v13278, %v13266
%v13284 = vshll.u32 %v13278, 13
%v13285 = vshrl.u32 %v13278, 19
%v13286 = vor.u32 %v13285, %v13284
%v13287 = vxor.u32 %v13286, %v13282
%v13290 = vadd.s32 %v13287, %v13282
%v13292 = vshll.u32 %v13287, 15
%v13293 = vshrl.u32 %v13287, 17
%v13294 = vor.u32 %v13293, %v13292
%v13295 = vxor.u32 %v13294, %v13290
%v13298 = vadd.s32 %v13295, %v13290
%v13300 = vshll.u32 %v13295, 26
%v13301 = vshrl.u32 %v13295, 6
%v13302 = vor.u32 %v13301, %v13300
%v13303 = vxor.u32 %v13302, %v13298
%v13306 = vadd.s32 %v13303, %v13298
%v13310 = vadd.s32 %v13306, %v10
%v13312 = vshll.u32 %v13303, 6
%v13313 = vshrl.u32 %v13303, 26
%v13314 = vor.u32 %v13313, %v13312
%v13315 = vxor.u32 %v13314, %v13306
%v13318 = vadd.s32 %v13315, %v9
%v13322 = vadd.s32 3, %v13318
%v13326 = vadd.s32 %v13322, %v13310
%v13328 = vshll.u32 %v13322, 17
%v13329 = vshrl.u32 %v13322, 15
%v13330 = vor.u32 %v13329, %v13328
%v13331 = vxor.u32 %v13330, %v13326
%v13334 = vadd.s32 %v13331, %v13326
%v13336 = vshll.u32 %v13331, 29
%v13337 = vshrl.u32 %v13331, 3
%v13338 = vor.u32 %v13337, %v13336
%v13339 = vxor.u32 %v13338, %v13334
%v13342 = vadd.s32 %v13339, %v13334
%v13344 = vshll.u32 %v13339, 16
%v13345 = vshrl.u32 %v13339, 16
%v13346 = vor.u32 %v13345, %v13344
%v13347 = vxor.u32 %v13346, %v13342
%v13350 = vadd.s32 %v13347, %v13342
%v13354 = vadd.s32 %v13350, %v9
%v13356 = vshll.u32 %v13347, 24
%v13357 = vshrl.u32 %v13347, 8
%v13358 = vor.u32 %v13357, %v13356
%v13359 = vxor.u32 %v13358, %v13350
%v13362 = vadd.s32 %v13359, %v8
%v13366 = vadd.s32 4, %v13362
%v13370 = vadd.s32 %v13366, %v13354
%v13372 = vshll.u32 %v13366, 13
%v13373 = vshrl.u32 %v13366, 19
%v13374 = vor.u32 %v13373, %v13372
%v13375 = vxor.u32 %v13374, %v13370
%v13378 = vadd.s32 %v13375, %v13370
%v13380 = vshll.u32 %v13375, 15
%v13381 = vshrl.u32 %v13375, 17
%v13382 = vor.u32 %v13381, %v13380
%v13383 = vxor.u32 %v13382, %v13378
%v13386 = vadd.s32 %v13383, %v13378
%v13388 = vshll.u32 %v13383, 26
%v13389 = vshrl.u32 %v13383, 6
%v13390 = vor.u32 %v13389, %v13388
%v13391 = vxor.u32 %v13390, %v13386
%v13394 = vadd.s32 %v13391, %v13386
%v13398 = vadd.s32 %v13394, %v8
%v13400 = vshll.u32 %v13391, 6
%v13401 = vshrl.u32 %v13391, 26
%v13402 = vor.u32 %v13401, %v13400
%v13403 = vxor.u32 %v13402, %v13394
%v13406 = vadd.s32 %v13403, %v10
%v13410 = vadd.s32 5, %v13406
%v13412 = vxor.u32 %v13410, %v13398
%v13413 = vand.u32.u8 255, %v13412
%v13414 = vand.u32 65535, %v13413
%v13415 = vshrl.u32 %v13414, 1
%v13416 = vor.u32 16256, %v13415
%v13417 = vand.u32.u16 65535, %v13416
%v119814 = vadd.low.f32.bf16 -1.0, %v13417
%v13426 = vmul.f32 2.0, %v119814
%v13430 = vadd.f32 -0.99609375, %v13426
%v13434 = vmax.f32 %v13430, -0.99609375
%v13436 = vand.u32 2147483647, %v13434
%vm13439 = vcmp.eq.f32.partialorder %v13436, 1.0
%v13444 = vmul.f32 inf, %v13434
%v13446 = vxor.u32 2147483648, %v13434
%v13449 = vmul.f32 %v13446, %v13434
%v13451 = vadd.f32 1.0, %v13449
%v13452 = vlog2.pop %v13451
%v13453 = vmul.f32 0.6931472, %v13452
%v13454 = vmul.f32 -0.5, %v13449
%v13455 = vadd.f32 1.0, %v13454
%v13456 = vmul.f32 %v13455, %v13449
%v13457 = vand.u32 2147483647, %v13449
%vm13458 = vcmp.lt.f32.partialorder %v13457, 0.0004427343
%v13459 = vsel /*vm=*/%vm13458, /*on_true_vy=*/%v13456, /*on_false_vx=*/%v13453
%v13460 = vxor.u32 2147483648, %v13459
%vm13463 = vcmp.lt.f32.partialorder %v13460, 5.0
%v13468 = vsel /*vm=*/%vm13463, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v13472 = vsel /*vm=*/%vm13463, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v13476 = vsel /*vm=*/%vm13463, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v13480 = vsel /*vm=*/%vm13463, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v13484 = vsel /*vm=*/%vm13463, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v13488 = vsel /*vm=*/%vm13463, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v13492 = vsel /*vm=*/%vm13463, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v13496 = vsel /*vm=*/%vm13463, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v13500 = vsel /*vm=*/%vm13463, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v13504 = vadd.f32 -2.5, %v13460
%v13506 = vrsqrt.pop %v13460
%v13507 = vmul.f32 %v13506, %v13460
%vm13508 = vcmp.eq.f32.partialorder %v13460, inf
%v13509 = vsel /*vm=*/%vm13508, /*on_true_vy=*/%v13460, /*on_false_vx=*/%v13507
%vm13510 = vcmp.eq.f32.partialorder %v13460, 0.0
%v13511 = vand.u32 2147483648, %v13460
%v13512 = vsel /*vm=*/%vm13510, /*on_true_vy=*/%v13511, /*on_false_vx=*/%v13509
%v13515 = vadd.f32 -3.0, %v13512
%v13519 = vsel /*vm=*/%vm13463, /*on_true_vy=*/%v13504, /*on_false_vx=*/%v13515
%v13523 = vmul.f32 %v13519, %v13500
%v13527 = vadd.f32 %v13523, %v13496
%v13531 = vmul.f32 %v13527, %v13519
%v13535 = vadd.f32 %v13531, %v13492
%v13539 = vmul.f32 %v13535, %v13519
%v13543 = vadd.f32 %v13539, %v13488
%v13547 = vmul.f32 %v13543, %v13519
%v13551 = vadd.f32 %v13547, %v13484
%v13555 = vmul.f32 %v13551, %v13519
%v13559 = vadd.f32 %v13555, %v13480
%v13563 = vmul.f32 %v13559, %v13519
%v13567 = vadd.f32 %v13563, %v13476
%v13571 = vmul.f32 %v13567, %v13519
%v13575 = vadd.f32 %v13571, %v13472
%v13579 = vmul.f32 %v13575, %v13519
%v13583 = vadd.f32 %v13579, %v13468
%v13587 = vmul.f32 %v13583, %v13434
%v13591 = vsel /*vm=*/%vm13439, /*on_true_vy=*/%v13444, /*on_false_vx=*/%v13587
%v13595 = vmul.f32 1.4140625, %v13591
%v13598 = vpack.c.bf16 %v120417, %v13595
%119815 = vst [vmem:[%s280 + $0x18c] sm:$0xf] /*vst_source=*/%v13598
%v13602 = vadd.s32 %v11755, %v2355
%v13612 = vadd.s32 %v13602, %v415
%vm13616 = vcmp.lt.u32.totalorder %v13612, %v13602
%vm13621 = vcmp.lt.u32.totalorder %v13602, %v2355
%v13626 = vadd.s32 %v11738, %v2342
%v13630 = vadd.s32 1, %v13626
%v13634 = vsel /*vm=*/%vm13621, /*on_true_vy=*/%v13630, /*on_false_vx=*/%v13626
%v13638 = vadd.s32 1, %v13634
%v13642 = vsel /*vm=*/%vm13616, /*on_true_vy=*/%v13638, /*on_false_vx=*/%v13634
%v13647 = vadd.s32 %v13642, %v10
%v13651 = vadd.s32 %v13612, %v9
%v13655 = vadd.s32 %v13651, %v13647
%v13657 = vshll.u32 %v13651, 13
%v13658 = vshrl.u32 %v13651, 19
%v13659 = vor.u32 %v13658, %v13657
%v13660 = vxor.u32 %v13659, %v13655
%v13663 = vadd.s32 %v13660, %v13655
%v13665 = vshll.u32 %v13660, 15
%v13666 = vshrl.u32 %v13660, 17
%v13667 = vor.u32 %v13666, %v13665
%v13668 = vxor.u32 %v13667, %v13663
%v13671 = vadd.s32 %v13668, %v13663
%v13673 = vshll.u32 %v13668, 26
%v13674 = vshrl.u32 %v13668, 6
%v13675 = vor.u32 %v13674, %v13673
%v13676 = vxor.u32 %v13675, %v13671
%v13679 = vadd.s32 %v13676, %v13671
%v13683 = vadd.s32 %v13679, %v9
%v13685 = vshll.u32 %v13676, 6
%v13686 = vshrl.u32 %v13676, 26
%v13687 = vor.u32 %v13686, %v13685
%v13688 = vxor.u32 %v13687, %v13679
%v13691 = vadd.s32 %v13688, %v8
%v13695 = vadd.s32 1, %v13691
%v13699 = vadd.s32 %v13695, %v13683
%v13701 = vshll.u32 %v13695, 17
%v13702 = vshrl.u32 %v13695, 15
%v13703 = vor.u32 %v13702, %v13701
%v13704 = vxor.u32 %v13703, %v13699
%v13707 = vadd.s32 %v13704, %v13699
%v13709 = vshll.u32 %v13704, 29
%v13710 = vshrl.u32 %v13704, 3
%v13711 = vor.u32 %v13710, %v13709
%v13712 = vxor.u32 %v13711, %v13707
%v13715 = vadd.s32 %v13712, %v13707
%v13717 = vshll.u32 %v13712, 16
%v13718 = vshrl.u32 %v13712, 16
%v13719 = vor.u32 %v13718, %v13717
%v13720 = vxor.u32 %v13719, %v13715
%v13723 = vadd.s32 %v13720, %v13715
%v13727 = vadd.s32 %v13723, %v8
%v13729 = vshll.u32 %v13720, 24
%v13730 = vshrl.u32 %v13720, 8
%v13731 = vor.u32 %v13730, %v13729
%v13732 = vxor.u32 %v13731, %v13723
%v13735 = vadd.s32 %v13732, %v10
%v13739 = vadd.s32 2, %v13735
%v13743 = vadd.s32 %v13739, %v13727
%v13745 = vshll.u32 %v13739, 13
%v13746 = vshrl.u32 %v13739, 19
%v13747 = vor.u32 %v13746, %v13745
%v13748 = vxor.u32 %v13747, %v13743
%v13751 = vadd.s32 %v13748, %v13743
%v13753 = vshll.u32 %v13748, 15
%v13754 = vshrl.u32 %v13748, 17
%v13755 = vor.u32 %v13754, %v13753
%v13756 = vxor.u32 %v13755, %v13751
%v13759 = vadd.s32 %v13756, %v13751
%v13761 = vshll.u32 %v13756, 26
%v13762 = vshrl.u32 %v13756, 6
%v13763 = vor.u32 %v13762, %v13761
%v13764 = vxor.u32 %v13763, %v13759
%v13767 = vadd.s32 %v13764, %v13759
%v13771 = vadd.s32 %v13767, %v10
%v13773 = vshll.u32 %v13764, 6
%v13774 = vshrl.u32 %v13764, 26
%v13775 = vor.u32 %v13774, %v13773
%v13776 = vxor.u32 %v13775, %v13767
%v13779 = vadd.s32 %v13776, %v9
%v13783 = vadd.s32 3, %v13779
%v13787 = vadd.s32 %v13783, %v13771
%v13789 = vshll.u32 %v13783, 17
%v13790 = vshrl.u32 %v13783, 15
%v13791 = vor.u32 %v13790, %v13789
%v13792 = vxor.u32 %v13791, %v13787
%v13795 = vadd.s32 %v13792, %v13787
%v13797 = vshll.u32 %v13792, 29
%v13798 = vshrl.u32 %v13792, 3
%v13799 = vor.u32 %v13798, %v13797
%v13800 = vxor.u32 %v13799, %v13795
%v13803 = vadd.s32 %v13800, %v13795
%v13805 = vshll.u32 %v13800, 16
%v13806 = vshrl.u32 %v13800, 16
%v13807 = vor.u32 %v13806, %v13805
%v13808 = vxor.u32 %v13807, %v13803
%v13811 = vadd.s32 %v13808, %v13803
%v13815 = vadd.s32 %v13811, %v9
%v13817 = vshll.u32 %v13808, 24
%v13818 = vshrl.u32 %v13808, 8
%v13819 = vor.u32 %v13818, %v13817
%v13820 = vxor.u32 %v13819, %v13811
%v13823 = vadd.s32 %v13820, %v8
%v13827 = vadd.s32 4, %v13823
%v13831 = vadd.s32 %v13827, %v13815
%v13833 = vshll.u32 %v13827, 13
%v13834 = vshrl.u32 %v13827, 19
%v13835 = vor.u32 %v13834, %v13833
%v13836 = vxor.u32 %v13835, %v13831
%v13839 = vadd.s32 %v13836, %v13831
%v13841 = vshll.u32 %v13836, 15
%v13842 = vshrl.u32 %v13836, 17
%v13843 = vor.u32 %v13842, %v13841
%v13844 = vxor.u32 %v13843, %v13839
%v13847 = vadd.s32 %v13844, %v13839
%v13849 = vshll.u32 %v13844, 26
%v13850 = vshrl.u32 %v13844, 6
%v13851 = vor.u32 %v13850, %v13849
%v13852 = vxor.u32 %v13851, %v13847
%v13855 = vadd.s32 %v13852, %v13847
%v13859 = vadd.s32 %v13855, %v8
%v13861 = vshll.u32 %v13852, 6
%v13862 = vshrl.u32 %v13852, 26
%v13863 = vor.u32 %v13862, %v13861
%v13864 = vxor.u32 %v13863, %v13855
%v13867 = vadd.s32 %v13864, %v10
%v13871 = vadd.s32 5, %v13867
%v13873 = vxor.u32 %v13871, %v13859
%v13874 = vand.u32.u8 255, %v13873
%v13875 = vand.u32 65535, %v13874
%v13876 = vshrl.u32 %v13875, 1
%v13877 = vor.u32 16256, %v13876
%v13878 = vand.u32.u16 65535, %v13877
%v119816 = vadd.low.f32.bf16 -1.0, %v13878
%v13887 = vmul.f32 2.0, %v119816
%v13891 = vadd.f32 -0.99609375, %v13887
%v13895 = vmax.f32 %v13891, -0.99609375
%v13897 = vand.u32 2147483647, %v13895
%vm13900 = vcmp.eq.f32.partialorder %v13897, 1.0
%v13905 = vmul.f32 inf, %v13895
%v13907 = vxor.u32 2147483648, %v13895
%v13910 = vmul.f32 %v13907, %v13895
%v13912 = vadd.f32 1.0, %v13910
%v13913 = vlog2.pop %v13912
%v13914 = vmul.f32 0.6931472, %v13913
%v13915 = vmul.f32 -0.5, %v13910
%v13916 = vadd.f32 1.0, %v13915
%v13917 = vmul.f32 %v13916, %v13910
%v13918 = vand.u32 2147483647, %v13910
%vm13919 = vcmp.lt.f32.partialorder %v13918, 0.0004427343
%v13920 = vsel /*vm=*/%vm13919, /*on_true_vy=*/%v13917, /*on_false_vx=*/%v13914
%v13921 = vxor.u32 2147483648, %v13920
%vm13924 = vcmp.lt.f32.partialorder %v13921, 5.0
%v13929 = vsel /*vm=*/%vm13924, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v13933 = vsel /*vm=*/%vm13924, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v13937 = vsel /*vm=*/%vm13924, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v13941 = vsel /*vm=*/%vm13924, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v13945 = vsel /*vm=*/%vm13924, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v13949 = vsel /*vm=*/%vm13924, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v13953 = vsel /*vm=*/%vm13924, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v13957 = vsel /*vm=*/%vm13924, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v13961 = vsel /*vm=*/%vm13924, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v13965 = vadd.f32 -2.5, %v13921
%v13967 = vrsqrt.pop %v13921
%v13968 = vmul.f32 %v13967, %v13921
%vm13969 = vcmp.eq.f32.partialorder %v13921, inf
%v13970 = vsel /*vm=*/%vm13969, /*on_true_vy=*/%v13921, /*on_false_vx=*/%v13968
%vm13971 = vcmp.eq.f32.partialorder %v13921, 0.0
%v13972 = vand.u32 2147483648, %v13921
%v13973 = vsel /*vm=*/%vm13971, /*on_true_vy=*/%v13972, /*on_false_vx=*/%v13970
%v13976 = vadd.f32 -3.0, %v13973
%v13980 = vsel /*vm=*/%vm13924, /*on_true_vy=*/%v13965, /*on_false_vx=*/%v13976
%v13984 = vmul.f32 %v13980, %v13961
%v13988 = vadd.f32 %v13984, %v13957
%v13992 = vmul.f32 %v13988, %v13980
%v13996 = vadd.f32 %v13992, %v13953
%v14000 = vmul.f32 %v13996, %v13980
%v14004 = vadd.f32 %v14000, %v13949
%v14008 = vmul.f32 %v14004, %v13980
%v14012 = vadd.f32 %v14008, %v13945
%v14016 = vmul.f32 %v14012, %v13980
%v14020 = vadd.f32 %v14016, %v13941
%v14024 = vmul.f32 %v14020, %v13980
%v14028 = vadd.f32 %v14024, %v13937
%v14032 = vmul.f32 %v14028, %v13980
%v14036 = vadd.f32 %v14032, %v13933
%v14040 = vmul.f32 %v14036, %v13980
%v14044 = vadd.f32 %v14040, %v13929
%v14048 = vmul.f32 %v14044, %v13895
%v14052 = vsel /*vm=*/%vm13900, /*on_true_vy=*/%v13905, /*on_false_vx=*/%v14048
%v14056 = vmul.f32 1.4140625, %v14052
%v14059 = vpack.c.bf16 %v120417, %v14056
%119817 = vst [vmem:[%s280 + $0x20c] sm:$0xf] /*vst_source=*/%v14059
%v14063 = vadd.s32 %v11755, %v2842
%v14073 = vadd.s32 %v14063, %v415
%vm14077 = vcmp.lt.u32.totalorder %v14073, %v14063
%vm14082 = vcmp.lt.u32.totalorder %v14063, %v2842
%v14087 = vadd.s32 %v11738, %v2829
%v14091 = vadd.s32 1, %v14087
%v14095 = vsel /*vm=*/%vm14082, /*on_true_vy=*/%v14091, /*on_false_vx=*/%v14087
%v14099 = vadd.s32 1, %v14095
%v14103 = vsel /*vm=*/%vm14077, /*on_true_vy=*/%v14099, /*on_false_vx=*/%v14095
%v14108 = vadd.s32 %v14103, %v10
%v14112 = vadd.s32 %v14073, %v9
%v14116 = vadd.s32 %v14112, %v14108
%v14118 = vshll.u32 %v14112, 13
%v14119 = vshrl.u32 %v14112, 19
%v14120 = vor.u32 %v14119, %v14118
%v14121 = vxor.u32 %v14120, %v14116
%v14124 = vadd.s32 %v14121, %v14116
%v14126 = vshll.u32 %v14121, 15
%v14127 = vshrl.u32 %v14121, 17
%v14128 = vor.u32 %v14127, %v14126
%v14129 = vxor.u32 %v14128, %v14124
%v14132 = vadd.s32 %v14129, %v14124
%v14134 = vshll.u32 %v14129, 26
%v14135 = vshrl.u32 %v14129, 6
%v14136 = vor.u32 %v14135, %v14134
%v14137 = vxor.u32 %v14136, %v14132
%v14140 = vadd.s32 %v14137, %v14132
%v14144 = vadd.s32 %v14140, %v9
%v14146 = vshll.u32 %v14137, 6
%v14147 = vshrl.u32 %v14137, 26
%v14148 = vor.u32 %v14147, %v14146
%v14149 = vxor.u32 %v14148, %v14140
%v14152 = vadd.s32 %v14149, %v8
%v14156 = vadd.s32 1, %v14152
%v14160 = vadd.s32 %v14156, %v14144
%v14162 = vshll.u32 %v14156, 17
%v14163 = vshrl.u32 %v14156, 15
%v14164 = vor.u32 %v14163, %v14162
%v14165 = vxor.u32 %v14164, %v14160
%v14168 = vadd.s32 %v14165, %v14160
%v14170 = vshll.u32 %v14165, 29
%v14171 = vshrl.u32 %v14165, 3
%v14172 = vor.u32 %v14171, %v14170
%v14173 = vxor.u32 %v14172, %v14168
%v14176 = vadd.s32 %v14173, %v14168
%v14178 = vshll.u32 %v14173, 16
%v14179 = vshrl.u32 %v14173, 16
%v14180 = vor.u32 %v14179, %v14178
%v14181 = vxor.u32 %v14180, %v14176
%v14184 = vadd.s32 %v14181, %v14176
%v14188 = vadd.s32 %v14184, %v8
%v14190 = vshll.u32 %v14181, 24
%v14191 = vshrl.u32 %v14181, 8
%v14192 = vor.u32 %v14191, %v14190
%v14193 = vxor.u32 %v14192, %v14184
%v14196 = vadd.s32 %v14193, %v10
%v14200 = vadd.s32 2, %v14196
%v14204 = vadd.s32 %v14200, %v14188
%v14206 = vshll.u32 %v14200, 13
%v14207 = vshrl.u32 %v14200, 19
%v14208 = vor.u32 %v14207, %v14206
%v14209 = vxor.u32 %v14208, %v14204
%v14212 = vadd.s32 %v14209, %v14204
%v14214 = vshll.u32 %v14209, 15
%v14215 = vshrl.u32 %v14209, 17
%v14216 = vor.u32 %v14215, %v14214
%v14217 = vxor.u32 %v14216, %v14212
%v14220 = vadd.s32 %v14217, %v14212
%v14222 = vshll.u32 %v14217, 26
%v14223 = vshrl.u32 %v14217, 6
%v14224 = vor.u32 %v14223, %v14222
%v14225 = vxor.u32 %v14224, %v14220
%v14228 = vadd.s32 %v14225, %v14220
%v14232 = vadd.s32 %v14228, %v10
%v14234 = vshll.u32 %v14225, 6
%v14235 = vshrl.u32 %v14225, 26
%v14236 = vor.u32 %v14235, %v14234
%v14237 = vxor.u32 %v14236, %v14228
%v14240 = vadd.s32 %v14237, %v9
%v14244 = vadd.s32 3, %v14240
%v14248 = vadd.s32 %v14244, %v14232
%v14250 = vshll.u32 %v14244, 17
%v14251 = vshrl.u32 %v14244, 15
%v14252 = vor.u32 %v14251, %v14250
%v14253 = vxor.u32 %v14252, %v14248
%v14256 = vadd.s32 %v14253, %v14248
%v14258 = vshll.u32 %v14253, 29
%v14259 = vshrl.u32 %v14253, 3
%v14260 = vor.u32 %v14259, %v14258
%v14261 = vxor.u32 %v14260, %v14256
%v14264 = vadd.s32 %v14261, %v14256
%v14266 = vshll.u32 %v14261, 16
%v14267 = vshrl.u32 %v14261, 16
%v14268 = vor.u32 %v14267, %v14266
%v14269 = vxor.u32 %v14268, %v14264
%v14272 = vadd.s32 %v14269, %v14264
%v14276 = vadd.s32 %v14272, %v9
%v14278 = vshll.u32 %v14269, 24
%v14279 = vshrl.u32 %v14269, 8
%v14280 = vor.u32 %v14279, %v14278
%v14281 = vxor.u32 %v14280, %v14272
%v14284 = vadd.s32 %v14281, %v8
%v14288 = vadd.s32 4, %v14284
%v14292 = vadd.s32 %v14288, %v14276
%v14294 = vshll.u32 %v14288, 13
%v14295 = vshrl.u32 %v14288, 19
%v14296 = vor.u32 %v14295, %v14294
%v14297 = vxor.u32 %v14296, %v14292
%v14300 = vadd.s32 %v14297, %v14292
%v14302 = vshll.u32 %v14297, 15
%v14303 = vshrl.u32 %v14297, 17
%v14304 = vor.u32 %v14303, %v14302
%v14305 = vxor.u32 %v14304, %v14300
%v14308 = vadd.s32 %v14305, %v14300
%v14310 = vshll.u32 %v14305, 26
%v14311 = vshrl.u32 %v14305, 6
%v14312 = vor.u32 %v14311, %v14310
%v14313 = vxor.u32 %v14312, %v14308
%v14316 = vadd.s32 %v14313, %v14308
%v14320 = vadd.s32 %v14316, %v8
%v14322 = vshll.u32 %v14313, 6
%v14323 = vshrl.u32 %v14313, 26
%v14324 = vor.u32 %v14323, %v14322
%v14325 = vxor.u32 %v14324, %v14316
%v14328 = vadd.s32 %v14325, %v10
%v14332 = vadd.s32 5, %v14328
%v14334 = vxor.u32 %v14332, %v14320
%v14335 = vand.u32.u8 255, %v14334
%v14336 = vand.u32 65535, %v14335
%v14337 = vshrl.u32 %v14336, 1
%v14338 = vor.u32 16256, %v14337
%v14339 = vand.u32.u16 65535, %v14338
%v119818 = vadd.low.f32.bf16 -1.0, %v14339
%v14348 = vmul.f32 2.0, %v119818
%v14352 = vadd.f32 -0.99609375, %v14348
%v14356 = vmax.f32 %v14352, -0.99609375
%v14358 = vand.u32 2147483647, %v14356
%vm14361 = vcmp.eq.f32.partialorder %v14358, 1.0
%v14366 = vmul.f32 inf, %v14356
%v14368 = vxor.u32 2147483648, %v14356
%v14371 = vmul.f32 %v14368, %v14356
%v14373 = vadd.f32 1.0, %v14371
%v14374 = vlog2.pop %v14373
%v14375 = vmul.f32 0.6931472, %v14374
%v14376 = vmul.f32 -0.5, %v14371
%v14377 = vadd.f32 1.0, %v14376
%v14378 = vmul.f32 %v14377, %v14371
%v14379 = vand.u32 2147483647, %v14371
%vm14380 = vcmp.lt.f32.partialorder %v14379, 0.0004427343
%v14381 = vsel /*vm=*/%vm14380, /*on_true_vy=*/%v14378, /*on_false_vx=*/%v14375
%v14382 = vxor.u32 2147483648, %v14381
%vm14385 = vcmp.lt.f32.partialorder %v14382, 5.0
%v14390 = vsel /*vm=*/%vm14385, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v14394 = vsel /*vm=*/%vm14385, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v14398 = vsel /*vm=*/%vm14385, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v14402 = vsel /*vm=*/%vm14385, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v14406 = vsel /*vm=*/%vm14385, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v14410 = vsel /*vm=*/%vm14385, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v14414 = vsel /*vm=*/%vm14385, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v14418 = vsel /*vm=*/%vm14385, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v14422 = vsel /*vm=*/%vm14385, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v14426 = vadd.f32 -2.5, %v14382
%v14428 = vrsqrt.pop %v14382
%v14429 = vmul.f32 %v14428, %v14382
%vm14430 = vcmp.eq.f32.partialorder %v14382, inf
%v14431 = vsel /*vm=*/%vm14430, /*on_true_vy=*/%v14382, /*on_false_vx=*/%v14429
%vm14432 = vcmp.eq.f32.partialorder %v14382, 0.0
%v14433 = vand.u32 2147483648, %v14382
%v14434 = vsel /*vm=*/%vm14432, /*on_true_vy=*/%v14433, /*on_false_vx=*/%v14431
%v14437 = vadd.f32 -3.0, %v14434
%v14441 = vsel /*vm=*/%vm14385, /*on_true_vy=*/%v14426, /*on_false_vx=*/%v14437
%v14445 = vmul.f32 %v14441, %v14422
%v14449 = vadd.f32 %v14445, %v14418
%v14453 = vmul.f32 %v14449, %v14441
%v14457 = vadd.f32 %v14453, %v14414
%v14461 = vmul.f32 %v14457, %v14441
%v14465 = vadd.f32 %v14461, %v14410
%v14469 = vmul.f32 %v14465, %v14441
%v14473 = vadd.f32 %v14469, %v14406
%v14477 = vmul.f32 %v14473, %v14441
%v14481 = vadd.f32 %v14477, %v14402
%v14485 = vmul.f32 %v14481, %v14441
%v14489 = vadd.f32 %v14485, %v14398
%v14493 = vmul.f32 %v14489, %v14441
%v14497 = vadd.f32 %v14493, %v14394
%v14501 = vmul.f32 %v14497, %v14441
%v14505 = vadd.f32 %v14501, %v14390
%v14509 = vmul.f32 %v14505, %v14356
%v14513 = vsel /*vm=*/%vm14361, /*on_true_vy=*/%v14366, /*on_false_vx=*/%v14509
%v14517 = vmul.f32 1.4140625, %v14513
%v14520 = vpack.c.bf16 %v120417, %v14517
%119819 = vst [vmem:[%s280 + $0x28c] sm:$0xf] /*vst_source=*/%v14520
%v14524 = vadd.s32 %v11755, %v3329
%v14534 = vadd.s32 %v14524, %v415
%vm14538 = vcmp.lt.u32.totalorder %v14534, %v14524
%vm14543 = vcmp.lt.u32.totalorder %v14524, %v3329
%v14548 = vadd.s32 %v11738, %v3316
%v14552 = vadd.s32 1, %v14548
%v14556 = vsel /*vm=*/%vm14543, /*on_true_vy=*/%v14552, /*on_false_vx=*/%v14548
%v14560 = vadd.s32 1, %v14556
%v14564 = vsel /*vm=*/%vm14538, /*on_true_vy=*/%v14560, /*on_false_vx=*/%v14556
%v14569 = vadd.s32 %v14564, %v10
%v14573 = vadd.s32 %v14534, %v9
%v14577 = vadd.s32 %v14573, %v14569
%v14579 = vshll.u32 %v14573, 13
%v14580 = vshrl.u32 %v14573, 19
%v14581 = vor.u32 %v14580, %v14579
%v14582 = vxor.u32 %v14581, %v14577
%v14585 = vadd.s32 %v14582, %v14577
%v14587 = vshll.u32 %v14582, 15
%v14588 = vshrl.u32 %v14582, 17
%v14589 = vor.u32 %v14588, %v14587
%v14590 = vxor.u32 %v14589, %v14585
%v14593 = vadd.s32 %v14590, %v14585
%v14595 = vshll.u32 %v14590, 26
%v14596 = vshrl.u32 %v14590, 6
%v14597 = vor.u32 %v14596, %v14595
%v14598 = vxor.u32 %v14597, %v14593
%v14601 = vadd.s32 %v14598, %v14593
%v14605 = vadd.s32 %v14601, %v9
%v14607 = vshll.u32 %v14598, 6
%v14608 = vshrl.u32 %v14598, 26
%v14609 = vor.u32 %v14608, %v14607
%v14610 = vxor.u32 %v14609, %v14601
%v14613 = vadd.s32 %v14610, %v8
%v14617 = vadd.s32 1, %v14613
%v14621 = vadd.s32 %v14617, %v14605
%v14623 = vshll.u32 %v14617, 17
%v14624 = vshrl.u32 %v14617, 15
%v14625 = vor.u32 %v14624, %v14623
%v14626 = vxor.u32 %v14625, %v14621
%v14629 = vadd.s32 %v14626, %v14621
%v14631 = vshll.u32 %v14626, 29
%v14632 = vshrl.u32 %v14626, 3
%v14633 = vor.u32 %v14632, %v14631
%v14634 = vxor.u32 %v14633, %v14629
%v14637 = vadd.s32 %v14634, %v14629
%v14639 = vshll.u32 %v14634, 16
%v14640 = vshrl.u32 %v14634, 16
%v14641 = vor.u32 %v14640, %v14639
%v14642 = vxor.u32 %v14641, %v14637
%v14645 = vadd.s32 %v14642, %v14637
%v14649 = vadd.s32 %v14645, %v8
%v14651 = vshll.u32 %v14642, 24
%v14652 = vshrl.u32 %v14642, 8
%v14653 = vor.u32 %v14652, %v14651
%v14654 = vxor.u32 %v14653, %v14645
%v14657 = vadd.s32 %v14654, %v10
%v14661 = vadd.s32 2, %v14657
%v14665 = vadd.s32 %v14661, %v14649
%v14667 = vshll.u32 %v14661, 13
%v14668 = vshrl.u32 %v14661, 19
%v14669 = vor.u32 %v14668, %v14667
%v14670 = vxor.u32 %v14669, %v14665
%v14673 = vadd.s32 %v14670, %v14665
%v14675 = vshll.u32 %v14670, 15
%v14676 = vshrl.u32 %v14670, 17
%v14677 = vor.u32 %v14676, %v14675
%v14678 = vxor.u32 %v14677, %v14673
%v14681 = vadd.s32 %v14678, %v14673
%v14683 = vshll.u32 %v14678, 26
%v14684 = vshrl.u32 %v14678, 6
%v14685 = vor.u32 %v14684, %v14683
%v14686 = vxor.u32 %v14685, %v14681
%v14689 = vadd.s32 %v14686, %v14681
%v14693 = vadd.s32 %v14689, %v10
%v14695 = vshll.u32 %v14686, 6
%v14696 = vshrl.u32 %v14686, 26
%v14697 = vor.u32 %v14696, %v14695
%v14698 = vxor.u32 %v14697, %v14689
%v14701 = vadd.s32 %v14698, %v9
%v14705 = vadd.s32 3, %v14701
%v14709 = vadd.s32 %v14705, %v14693
%v14711 = vshll.u32 %v14705, 17
%v14712 = vshrl.u32 %v14705, 15
%v14713 = vor.u32 %v14712, %v14711
%v14714 = vxor.u32 %v14713, %v14709
%v14717 = vadd.s32 %v14714, %v14709
%v14719 = vshll.u32 %v14714, 29
%v14720 = vshrl.u32 %v14714, 3
%v14721 = vor.u32 %v14720, %v14719
%v14722 = vxor.u32 %v14721, %v14717
%v14725 = vadd.s32 %v14722, %v14717
%v14727 = vshll.u32 %v14722, 16
%v14728 = vshrl.u32 %v14722, 16
%v14729 = vor.u32 %v14728, %v14727
%v14730 = vxor.u32 %v14729, %v14725
%v14733 = vadd.s32 %v14730, %v14725
%v14737 = vadd.s32 %v14733, %v9
%v14739 = vshll.u32 %v14730, 24
%v14740 = vshrl.u32 %v14730, 8
%v14741 = vor.u32 %v14740, %v14739
%v14742 = vxor.u32 %v14741, %v14733
%v14745 = vadd.s32 %v14742, %v8
%v14749 = vadd.s32 4, %v14745
%v14753 = vadd.s32 %v14749, %v14737
%v14755 = vshll.u32 %v14749, 13
%v14756 = vshrl.u32 %v14749, 19
%v14757 = vor.u32 %v14756, %v14755
%v14758 = vxor.u32 %v14757, %v14753
%v14761 = vadd.s32 %v14758, %v14753
%v14763 = vshll.u32 %v14758, 15
%v14764 = vshrl.u32 %v14758, 17
%v14765 = vor.u32 %v14764, %v14763
%v14766 = vxor.u32 %v14765, %v14761
%v14769 = vadd.s32 %v14766, %v14761
%v14771 = vshll.u32 %v14766, 26
%v14772 = vshrl.u32 %v14766, 6
%v14773 = vor.u32 %v14772, %v14771
%v14774 = vxor.u32 %v14773, %v14769
%v14777 = vadd.s32 %v14774, %v14769
%v14781 = vadd.s32 %v14777, %v8
%v14783 = vshll.u32 %v14774, 6
%v14784 = vshrl.u32 %v14774, 26
%v14785 = vor.u32 %v14784, %v14783
%v14786 = vxor.u32 %v14785, %v14777
%v14789 = vadd.s32 %v14786, %v10
%v14793 = vadd.s32 5, %v14789
%v14795 = vxor.u32 %v14793, %v14781
%v14796 = vand.u32.u8 255, %v14795
%v14797 = vand.u32 65535, %v14796
%v14798 = vshrl.u32 %v14797, 1
%v14799 = vor.u32 16256, %v14798
%v14800 = vand.u32.u16 65535, %v14799
%v119820 = vadd.low.f32.bf16 -1.0, %v14800
%v14809 = vmul.f32 2.0, %v119820
%v14813 = vadd.f32 -0.99609375, %v14809
%v14817 = vmax.f32 %v14813, -0.99609375
%v14819 = vand.u32 2147483647, %v14817
%vm14822 = vcmp.eq.f32.partialorder %v14819, 1.0
%v14827 = vmul.f32 inf, %v14817
%v14829 = vxor.u32 2147483648, %v14817
%v14832 = vmul.f32 %v14829, %v14817
%v14834 = vadd.f32 1.0, %v14832
%v14835 = vlog2.pop %v14834
%v14836 = vmul.f32 0.6931472, %v14835
%v14837 = vmul.f32 -0.5, %v14832
%v14838 = vadd.f32 1.0, %v14837
%v14839 = vmul.f32 %v14838, %v14832
%v14840 = vand.u32 2147483647, %v14832
%vm14841 = vcmp.lt.f32.partialorder %v14840, 0.0004427343
%v14842 = vsel /*vm=*/%vm14841, /*on_true_vy=*/%v14839, /*on_false_vx=*/%v14836
%v14843 = vxor.u32 2147483648, %v14842
%vm14846 = vcmp.lt.f32.partialorder %v14843, 5.0
%v14851 = vsel /*vm=*/%vm14846, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v14855 = vsel /*vm=*/%vm14846, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v14859 = vsel /*vm=*/%vm14846, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v14863 = vsel /*vm=*/%vm14846, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v14867 = vsel /*vm=*/%vm14846, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v14871 = vsel /*vm=*/%vm14846, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v14875 = vsel /*vm=*/%vm14846, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v14879 = vsel /*vm=*/%vm14846, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v14883 = vsel /*vm=*/%vm14846, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v14887 = vadd.f32 -2.5, %v14843
%v14889 = vrsqrt.pop %v14843
%v14890 = vmul.f32 %v14889, %v14843
%vm14891 = vcmp.eq.f32.partialorder %v14843, inf
%v14892 = vsel /*vm=*/%vm14891, /*on_true_vy=*/%v14843, /*on_false_vx=*/%v14890
%vm14893 = vcmp.eq.f32.partialorder %v14843, 0.0
%v14894 = vand.u32 2147483648, %v14843
%v14895 = vsel /*vm=*/%vm14893, /*on_true_vy=*/%v14894, /*on_false_vx=*/%v14892
%v14898 = vadd.f32 -3.0, %v14895
%v14902 = vsel /*vm=*/%vm14846, /*on_true_vy=*/%v14887, /*on_false_vx=*/%v14898
%v14906 = vmul.f32 %v14902, %v14883
%v14910 = vadd.f32 %v14906, %v14879
%v14914 = vmul.f32 %v14910, %v14902
%v14918 = vadd.f32 %v14914, %v14875
%v14922 = vmul.f32 %v14918, %v14902
%v14926 = vadd.f32 %v14922, %v14871
%v14930 = vmul.f32 %v14926, %v14902
%v14934 = vadd.f32 %v14930, %v14867
%v14938 = vmul.f32 %v14934, %v14902
%v14942 = vadd.f32 %v14938, %v14863
%v14946 = vmul.f32 %v14942, %v14902
%v14950 = vadd.f32 %v14946, %v14859
%v14954 = vmul.f32 %v14950, %v14902
%v14958 = vadd.f32 %v14954, %v14855
%v14962 = vmul.f32 %v14958, %v14902
%v14966 = vadd.f32 %v14962, %v14851
%v14970 = vmul.f32 %v14966, %v14817
%v14974 = vsel /*vm=*/%vm14822, /*on_true_vy=*/%v14827, /*on_false_vx=*/%v14970
%v14978 = vmul.f32 1.4140625, %v14974
%v14981 = vpack.c.bf16 %v120417, %v14978
%119821 = vst [vmem:[%s280 + $0x30c] sm:$0xf] /*vst_source=*/%v14981
%v14985 = vadd.s32 %v11755, %v3816
%v14995 = vadd.s32 %v14985, %v415
%vm14999 = vcmp.lt.u32.totalorder %v14995, %v14985
%vm15004 = vcmp.lt.u32.totalorder %v14985, %v3816
%v15009 = vadd.s32 %v11738, %v3803
%v15013 = vadd.s32 1, %v15009
%v15017 = vsel /*vm=*/%vm15004, /*on_true_vy=*/%v15013, /*on_false_vx=*/%v15009
%v15021 = vadd.s32 1, %v15017
%v15025 = vsel /*vm=*/%vm14999, /*on_true_vy=*/%v15021, /*on_false_vx=*/%v15017
%v15030 = vadd.s32 %v15025, %v10
%v15034 = vadd.s32 %v14995, %v9
%v15038 = vadd.s32 %v15034, %v15030
%v15040 = vshll.u32 %v15034, 13
%v15041 = vshrl.u32 %v15034, 19
%v15042 = vor.u32 %v15041, %v15040
%v15043 = vxor.u32 %v15042, %v15038
%v15046 = vadd.s32 %v15043, %v15038
%v15048 = vshll.u32 %v15043, 15
%v15049 = vshrl.u32 %v15043, 17
%v15050 = vor.u32 %v15049, %v15048
%v15051 = vxor.u32 %v15050, %v15046
%v15054 = vadd.s32 %v15051, %v15046
%v15056 = vshll.u32 %v15051, 26
%v15057 = vshrl.u32 %v15051, 6
%v15058 = vor.u32 %v15057, %v15056
%v15059 = vxor.u32 %v15058, %v15054
%v15062 = vadd.s32 %v15059, %v15054
%v15066 = vadd.s32 %v15062, %v9
%v15068 = vshll.u32 %v15059, 6
%v15069 = vshrl.u32 %v15059, 26
%v15070 = vor.u32 %v15069, %v15068
%v15071 = vxor.u32 %v15070, %v15062
%v15074 = vadd.s32 %v15071, %v8
%v15078 = vadd.s32 1, %v15074
%v15082 = vadd.s32 %v15078, %v15066
%v15084 = vshll.u32 %v15078, 17
%v15085 = vshrl.u32 %v15078, 15
%v15086 = vor.u32 %v15085, %v15084
%v15087 = vxor.u32 %v15086, %v15082
%v15090 = vadd.s32 %v15087, %v15082
%v15092 = vshll.u32 %v15087, 29
%v15093 = vshrl.u32 %v15087, 3
%v15094 = vor.u32 %v15093, %v15092
%v15095 = vxor.u32 %v15094, %v15090
%v15098 = vadd.s32 %v15095, %v15090
%v15100 = vshll.u32 %v15095, 16
%v15101 = vshrl.u32 %v15095, 16
%v15102 = vor.u32 %v15101, %v15100
%v15103 = vxor.u32 %v15102, %v15098
%v15106 = vadd.s32 %v15103, %v15098
%v15110 = vadd.s32 %v15106, %v8
%v15112 = vshll.u32 %v15103, 24
%v15113 = vshrl.u32 %v15103, 8
%v15114 = vor.u32 %v15113, %v15112
%v15115 = vxor.u32 %v15114, %v15106
%v15118 = vadd.s32 %v15115, %v10
%v15122 = vadd.s32 2, %v15118
%v15126 = vadd.s32 %v15122, %v15110
%v15128 = vshll.u32 %v15122, 13
%v15129 = vshrl.u32 %v15122, 19
%v15130 = vor.u32 %v15129, %v15128
%v15131 = vxor.u32 %v15130, %v15126
%v15134 = vadd.s32 %v15131, %v15126
%v15136 = vshll.u32 %v15131, 15
%v15137 = vshrl.u32 %v15131, 17
%v15138 = vor.u32 %v15137, %v15136
%v15139 = vxor.u32 %v15138, %v15134
%v15142 = vadd.s32 %v15139, %v15134
%v15144 = vshll.u32 %v15139, 26
%v15145 = vshrl.u32 %v15139, 6
%v15146 = vor.u32 %v15145, %v15144
%v15147 = vxor.u32 %v15146, %v15142
%v15150 = vadd.s32 %v15147, %v15142
%v15154 = vadd.s32 %v15150, %v10
%v15156 = vshll.u32 %v15147, 6
%v15157 = vshrl.u32 %v15147, 26
%v15158 = vor.u32 %v15157, %v15156
%v15159 = vxor.u32 %v15158, %v15150
%v15162 = vadd.s32 %v15159, %v9
%v15166 = vadd.s32 3, %v15162
%v15170 = vadd.s32 %v15166, %v15154
%v15172 = vshll.u32 %v15166, 17
%v15173 = vshrl.u32 %v15166, 15
%v15174 = vor.u32 %v15173, %v15172
%v15175 = vxor.u32 %v15174, %v15170
%v15178 = vadd.s32 %v15175, %v15170
%v15180 = vshll.u32 %v15175, 29
%v15181 = vshrl.u32 %v15175, 3
%v15182 = vor.u32 %v15181, %v15180
%v15183 = vxor.u32 %v15182, %v15178
%v15186 = vadd.s32 %v15183, %v15178
%v15188 = vshll.u32 %v15183, 16
%v15189 = vshrl.u32 %v15183, 16
%v15190 = vor.u32 %v15189, %v15188
%v15191 = vxor.u32 %v15190, %v15186
%v15194 = vadd.s32 %v15191, %v15186
%v15198 = vadd.s32 %v15194, %v9
%v15200 = vshll.u32 %v15191, 24
%v15201 = vshrl.u32 %v15191, 8
%v15202 = vor.u32 %v15201, %v15200
%v15203 = vxor.u32 %v15202, %v15194
%v15206 = vadd.s32 %v15203, %v8
%v15210 = vadd.s32 4, %v15206
%v15214 = vadd.s32 %v15210, %v15198
%v15216 = vshll.u32 %v15210, 13
%v15217 = vshrl.u32 %v15210, 19
%v15218 = vor.u32 %v15217, %v15216
%v15219 = vxor.u32 %v15218, %v15214
%v15222 = vadd.s32 %v15219, %v15214
%v15224 = vshll.u32 %v15219, 15
%v15225 = vshrl.u32 %v15219, 17
%v15226 = vor.u32 %v15225, %v15224
%v15227 = vxor.u32 %v15226, %v15222
%v15230 = vadd.s32 %v15227, %v15222
%v15232 = vshll.u32 %v15227, 26
%v15233 = vshrl.u32 %v15227, 6
%v15234 = vor.u32 %v15233, %v15232
%v15235 = vxor.u32 %v15234, %v15230
%v15238 = vadd.s32 %v15235, %v15230
%v15242 = vadd.s32 %v15238, %v8
%v15244 = vshll.u32 %v15235, 6
%v15245 = vshrl.u32 %v15235, 26
%v15246 = vor.u32 %v15245, %v15244
%v15247 = vxor.u32 %v15246, %v15238
%v15250 = vadd.s32 %v15247, %v10
%v15254 = vadd.s32 5, %v15250
%v15256 = vxor.u32 %v15254, %v15242
%v15257 = vand.u32.u8 255, %v15256
%v15258 = vand.u32 65535, %v15257
%v15259 = vshrl.u32 %v15258, 1
%v15260 = vor.u32 16256, %v15259
%v15261 = vand.u32.u16 65535, %v15260
%v119822 = vadd.low.f32.bf16 -1.0, %v15261
%v15270 = vmul.f32 2.0, %v119822
%v15274 = vadd.f32 -0.99609375, %v15270
%v15278 = vmax.f32 %v15274, -0.99609375
%v15280 = vand.u32 2147483647, %v15278
%vm15283 = vcmp.eq.f32.partialorder %v15280, 1.0
%v15288 = vmul.f32 inf, %v15278
%v15290 = vxor.u32 2147483648, %v15278
%v15293 = vmul.f32 %v15290, %v15278
%v15295 = vadd.f32 1.0, %v15293
%v15296 = vlog2.pop %v15295
%v15297 = vmul.f32 0.6931472, %v15296
%v15298 = vmul.f32 -0.5, %v15293
%v15299 = vadd.f32 1.0, %v15298
%v15300 = vmul.f32 %v15299, %v15293
%v15301 = vand.u32 2147483647, %v15293
%vm15302 = vcmp.lt.f32.partialorder %v15301, 0.0004427343
%v15303 = vsel /*vm=*/%vm15302, /*on_true_vy=*/%v15300, /*on_false_vx=*/%v15297
%v15304 = vxor.u32 2147483648, %v15303
%vm15307 = vcmp.lt.f32.partialorder %v15304, 5.0
%v15312 = vsel /*vm=*/%vm15307, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v15316 = vsel /*vm=*/%vm15307, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v15320 = vsel /*vm=*/%vm15307, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v15324 = vsel /*vm=*/%vm15307, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v15328 = vsel /*vm=*/%vm15307, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v15332 = vsel /*vm=*/%vm15307, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v15336 = vsel /*vm=*/%vm15307, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v15340 = vsel /*vm=*/%vm15307, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v15344 = vsel /*vm=*/%vm15307, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v15348 = vadd.f32 -2.5, %v15304
%v15350 = vrsqrt.pop %v15304
%v15351 = vmul.f32 %v15350, %v15304
%vm15352 = vcmp.eq.f32.partialorder %v15304, inf
%v15353 = vsel /*vm=*/%vm15352, /*on_true_vy=*/%v15304, /*on_false_vx=*/%v15351
%vm15354 = vcmp.eq.f32.partialorder %v15304, 0.0
%v15355 = vand.u32 2147483648, %v15304
%v15356 = vsel /*vm=*/%vm15354, /*on_true_vy=*/%v15355, /*on_false_vx=*/%v15353
%v15359 = vadd.f32 -3.0, %v15356
%v15363 = vsel /*vm=*/%vm15307, /*on_true_vy=*/%v15348, /*on_false_vx=*/%v15359
%v15367 = vmul.f32 %v15363, %v15344
%v15371 = vadd.f32 %v15367, %v15340
%v15375 = vmul.f32 %v15371, %v15363
%v15379 = vadd.f32 %v15375, %v15336
%v15383 = vmul.f32 %v15379, %v15363
%v15387 = vadd.f32 %v15383, %v15332
%v15391 = vmul.f32 %v15387, %v15363
%v15395 = vadd.f32 %v15391, %v15328
%v15399 = vmul.f32 %v15395, %v15363
%v15403 = vadd.f32 %v15399, %v15324
%v15407 = vmul.f32 %v15403, %v15363
%v15411 = vadd.f32 %v15407, %v15320
%v15415 = vmul.f32 %v15411, %v15363
%v15419 = vadd.f32 %v15415, %v15316
%v15423 = vmul.f32 %v15419, %v15363
%v15427 = vadd.f32 %v15423, %v15312
%v15431 = vmul.f32 %v15427, %v15278
%v15435 = vsel /*vm=*/%vm15283, /*on_true_vy=*/%v15288, /*on_false_vx=*/%v15431
%v15439 = vmul.f32 1.4140625, %v15435
%v15442 = vpack.c.bf16 %v120417, %v15439
%119823 = vst [vmem:[%s280 + $0x38c] sm:$0xf] /*vst_source=*/%v15442
%v15480 = vadd.s32 %v15477, %v408
%v15490 = vadd.s32 %v15480, %v415
%vm15494 = vcmp.lt.u32.totalorder %v15490, %v15480
%vm15499 = vcmp.lt.u32.totalorder %v15480, %v408
%v15504 = vadd.s32 %v15460, %v380
%v15508 = vadd.s32 1, %v15504
%v15512 = vsel /*vm=*/%vm15499, /*on_true_vy=*/%v15508, /*on_false_vx=*/%v15504
%v15516 = vadd.s32 1, %v15512
%v15520 = vsel /*vm=*/%vm15494, /*on_true_vy=*/%v15516, /*on_false_vx=*/%v15512
%v15525 = vadd.s32 %v15520, %v10
%v15529 = vadd.s32 %v15490, %v9
%v15533 = vadd.s32 %v15529, %v15525
%v15535 = vshll.u32 %v15529, 13
%v15536 = vshrl.u32 %v15529, 19
%v15537 = vor.u32 %v15536, %v15535
%v15538 = vxor.u32 %v15537, %v15533
%v15541 = vadd.s32 %v15538, %v15533
%v15543 = vshll.u32 %v15538, 15
%v15544 = vshrl.u32 %v15538, 17
%v15545 = vor.u32 %v15544, %v15543
%v15546 = vxor.u32 %v15545, %v15541
%v15549 = vadd.s32 %v15546, %v15541
%v15551 = vshll.u32 %v15546, 26
%v15552 = vshrl.u32 %v15546, 6
%v15553 = vor.u32 %v15552, %v15551
%v15554 = vxor.u32 %v15553, %v15549
%v15557 = vadd.s32 %v15554, %v15549
%v15561 = vadd.s32 %v15557, %v9
%v15563 = vshll.u32 %v15554, 6
%v15564 = vshrl.u32 %v15554, 26
%v15565 = vor.u32 %v15564, %v15563
%v15566 = vxor.u32 %v15565, %v15557
%v15569 = vadd.s32 %v15566, %v8
%v15573 = vadd.s32 1, %v15569
%v15577 = vadd.s32 %v15573, %v15561
%v15579 = vshll.u32 %v15573, 17
%v15580 = vshrl.u32 %v15573, 15
%v15581 = vor.u32 %v15580, %v15579
%v15582 = vxor.u32 %v15581, %v15577
%v15585 = vadd.s32 %v15582, %v15577
%v15587 = vshll.u32 %v15582, 29
%v15588 = vshrl.u32 %v15582, 3
%v15589 = vor.u32 %v15588, %v15587
%v15590 = vxor.u32 %v15589, %v15585
%v15593 = vadd.s32 %v15590, %v15585
%v15595 = vshll.u32 %v15590, 16
%v15596 = vshrl.u32 %v15590, 16
%v15597 = vor.u32 %v15596, %v15595
%v15598 = vxor.u32 %v15597, %v15593
%v15601 = vadd.s32 %v15598, %v15593
%v15605 = vadd.s32 %v15601, %v8
%v15607 = vshll.u32 %v15598, 24
%v15608 = vshrl.u32 %v15598, 8
%v15609 = vor.u32 %v15608, %v15607
%v15610 = vxor.u32 %v15609, %v15601
%v15613 = vadd.s32 %v15610, %v10
%v15617 = vadd.s32 2, %v15613
%v15621 = vadd.s32 %v15617, %v15605
%v15623 = vshll.u32 %v15617, 13
%v15624 = vshrl.u32 %v15617, 19
%v15625 = vor.u32 %v15624, %v15623
%v15626 = vxor.u32 %v15625, %v15621
%v15629 = vadd.s32 %v15626, %v15621
%v15631 = vshll.u32 %v15626, 15
%v15632 = vshrl.u32 %v15626, 17
%v15633 = vor.u32 %v15632, %v15631
%v15634 = vxor.u32 %v15633, %v15629
%v15637 = vadd.s32 %v15634, %v15629
%v15639 = vshll.u32 %v15634, 26
%v15640 = vshrl.u32 %v15634, 6
%v15641 = vor.u32 %v15640, %v15639
%v15642 = vxor.u32 %v15641, %v15637
%v15645 = vadd.s32 %v15642, %v15637
%v15649 = vadd.s32 %v15645, %v10
%v15651 = vshll.u32 %v15642, 6
%v15652 = vshrl.u32 %v15642, 26
%v15653 = vor.u32 %v15652, %v15651
%v15654 = vxor.u32 %v15653, %v15645
%v15657 = vadd.s32 %v15654, %v9
%v15661 = vadd.s32 3, %v15657
%v15665 = vadd.s32 %v15661, %v15649
%v15667 = vshll.u32 %v15661, 17
%v15668 = vshrl.u32 %v15661, 15
%v15669 = vor.u32 %v15668, %v15667
%v15670 = vxor.u32 %v15669, %v15665
%v15673 = vadd.s32 %v15670, %v15665
%v15675 = vshll.u32 %v15670, 29
%v15676 = vshrl.u32 %v15670, 3
%v15677 = vor.u32 %v15676, %v15675
%v15678 = vxor.u32 %v15677, %v15673
%v15681 = vadd.s32 %v15678, %v15673
%v15683 = vshll.u32 %v15678, 16
%v15684 = vshrl.u32 %v15678, 16
%v15685 = vor.u32 %v15684, %v15683
%v15686 = vxor.u32 %v15685, %v15681
%v15689 = vadd.s32 %v15686, %v15681
%v15693 = vadd.s32 %v15689, %v9
%v15695 = vshll.u32 %v15686, 24
%v15696 = vshrl.u32 %v15686, 8
%v15697 = vor.u32 %v15696, %v15695
%v15698 = vxor.u32 %v15697, %v15689
%v15701 = vadd.s32 %v15698, %v8
%v15705 = vadd.s32 4, %v15701
%v15709 = vadd.s32 %v15705, %v15693
%v15711 = vshll.u32 %v15705, 13
%v15712 = vshrl.u32 %v15705, 19
%v15713 = vor.u32 %v15712, %v15711
%v15714 = vxor.u32 %v15713, %v15709
%v15717 = vadd.s32 %v15714, %v15709
%v15719 = vshll.u32 %v15714, 15
%v15720 = vshrl.u32 %v15714, 17
%v15721 = vor.u32 %v15720, %v15719
%v15722 = vxor.u32 %v15721, %v15717
%v15725 = vadd.s32 %v15722, %v15717
%v15727 = vshll.u32 %v15722, 26
%v15728 = vshrl.u32 %v15722, 6
%v15729 = vor.u32 %v15728, %v15727
%v15730 = vxor.u32 %v15729, %v15725
%v15733 = vadd.s32 %v15730, %v15725
%v15737 = vadd.s32 %v15733, %v8
%v15739 = vshll.u32 %v15730, 6
%v15740 = vshrl.u32 %v15730, 26
%v15741 = vor.u32 %v15740, %v15739
%v15742 = vxor.u32 %v15741, %v15733
%v15745 = vadd.s32 %v15742, %v10
%v15749 = vadd.s32 5, %v15745
%v15751 = vxor.u32 %v15749, %v15737
%v15752 = vand.u32.u8 255, %v15751
%v15753 = vand.u32 65535, %v15752
%v15754 = vshrl.u32 %v15753, 1
%v15755 = vor.u32 16256, %v15754
%v15756 = vand.u32.u16 65535, %v15755
%v119828 = vadd.low.f32.bf16 -1.0, %v15756
%v15765 = vmul.f32 2.0, %v119828
%v15769 = vadd.f32 -0.99609375, %v15765
%v15773 = vmax.f32 %v15769, -0.99609375
%v15775 = vand.u32 2147483647, %v15773
%vm15778 = vcmp.eq.f32.partialorder %v15775, 1.0
%v15783 = vmul.f32 inf, %v15773
%v15785 = vxor.u32 2147483648, %v15773
%v15788 = vmul.f32 %v15785, %v15773
%v15790 = vadd.f32 1.0, %v15788
%v15791 = vlog2.pop %v15790
%v15792 = vmul.f32 0.6931472, %v15791
%v15793 = vmul.f32 -0.5, %v15788
%v15794 = vadd.f32 1.0, %v15793
%v15795 = vmul.f32 %v15794, %v15788
%v15796 = vand.u32 2147483647, %v15788
%vm15797 = vcmp.lt.f32.partialorder %v15796, 0.0004427343
%v15798 = vsel /*vm=*/%vm15797, /*on_true_vy=*/%v15795, /*on_false_vx=*/%v15792
%v15799 = vxor.u32 2147483648, %v15798
%vm15802 = vcmp.lt.f32.partialorder %v15799, 5.0
%v15807 = vsel /*vm=*/%vm15802, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v15811 = vsel /*vm=*/%vm15802, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v15815 = vsel /*vm=*/%vm15802, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v15819 = vsel /*vm=*/%vm15802, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v15823 = vsel /*vm=*/%vm15802, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v15827 = vsel /*vm=*/%vm15802, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v15831 = vsel /*vm=*/%vm15802, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v15835 = vsel /*vm=*/%vm15802, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v15839 = vsel /*vm=*/%vm15802, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v15843 = vadd.f32 -2.5, %v15799
%v15845 = vrsqrt.pop %v15799
%v15846 = vmul.f32 %v15845, %v15799
%vm15847 = vcmp.eq.f32.partialorder %v15799, inf
%v15848 = vsel /*vm=*/%vm15847, /*on_true_vy=*/%v15799, /*on_false_vx=*/%v15846
%vm15849 = vcmp.eq.f32.partialorder %v15799, 0.0
%v15850 = vand.u32 2147483648, %v15799
%v15851 = vsel /*vm=*/%vm15849, /*on_true_vy=*/%v15850, /*on_false_vx=*/%v15848
%v15854 = vadd.f32 -3.0, %v15851
%v15858 = vsel /*vm=*/%vm15802, /*on_true_vy=*/%v15843, /*on_false_vx=*/%v15854
%v15862 = vmul.f32 %v15858, %v15839
%v15866 = vadd.f32 %v15862, %v15835
%v15870 = vmul.f32 %v15866, %v15858
%v15874 = vadd.f32 %v15870, %v15831
%v15878 = vmul.f32 %v15874, %v15858
%v15882 = vadd.f32 %v15878, %v15827
%v15886 = vmul.f32 %v15882, %v15858
%v15890 = vadd.f32 %v15886, %v15823
%v15894 = vmul.f32 %v15890, %v15858
%v15898 = vadd.f32 %v15894, %v15819
%v15902 = vmul.f32 %v15898, %v15858
%v15906 = vadd.f32 %v15902, %v15815
%v15910 = vmul.f32 %v15906, %v15858
%v15914 = vadd.f32 %v15910, %v15811
%v15918 = vmul.f32 %v15914, %v15858
%v15922 = vadd.f32 %v15918, %v15807
%v15926 = vmul.f32 %v15922, %v15773
%v15930 = vsel /*vm=*/%vm15778, /*on_true_vy=*/%v15783, /*on_false_vx=*/%v15926
%v15934 = vmul.f32 1.4140625, %v15930
%v15937 = vpack.c.bf16 %v120417, %v15934
%119829 = vst [vmem:[%s280 + $0x10] sm:$0xf] /*vst_source=*/%v15937
%v15941 = vadd.s32 %v15477, %v894
%v15951 = vadd.s32 %v15941, %v415
%vm15955 = vcmp.lt.u32.totalorder %v15951, %v15941
%vm15960 = vcmp.lt.u32.totalorder %v15941, %v894
%v15965 = vadd.s32 %v15460, %v881
%v15969 = vadd.s32 1, %v15965
%v15973 = vsel /*vm=*/%vm15960, /*on_true_vy=*/%v15969, /*on_false_vx=*/%v15965
%v15977 = vadd.s32 1, %v15973
%v15981 = vsel /*vm=*/%vm15955, /*on_true_vy=*/%v15977, /*on_false_vx=*/%v15973
%v15986 = vadd.s32 %v15981, %v10
%v15990 = vadd.s32 %v15951, %v9
%v15994 = vadd.s32 %v15990, %v15986
%v15996 = vshll.u32 %v15990, 13
%v15997 = vshrl.u32 %v15990, 19
%v15998 = vor.u32 %v15997, %v15996
%v15999 = vxor.u32 %v15998, %v15994
%v16002 = vadd.s32 %v15999, %v15994
%v16004 = vshll.u32 %v15999, 15
%v16005 = vshrl.u32 %v15999, 17
%v16006 = vor.u32 %v16005, %v16004
%v16007 = vxor.u32 %v16006, %v16002
%v16010 = vadd.s32 %v16007, %v16002
%v16012 = vshll.u32 %v16007, 26
%v16013 = vshrl.u32 %v16007, 6
%v16014 = vor.u32 %v16013, %v16012
%v16015 = vxor.u32 %v16014, %v16010
%v16018 = vadd.s32 %v16015, %v16010
%v16022 = vadd.s32 %v16018, %v9
%v16024 = vshll.u32 %v16015, 6
%v16025 = vshrl.u32 %v16015, 26
%v16026 = vor.u32 %v16025, %v16024
%v16027 = vxor.u32 %v16026, %v16018
%v16030 = vadd.s32 %v16027, %v8
%v16034 = vadd.s32 1, %v16030
%v16038 = vadd.s32 %v16034, %v16022
%v16040 = vshll.u32 %v16034, 17
%v16041 = vshrl.u32 %v16034, 15
%v16042 = vor.u32 %v16041, %v16040
%v16043 = vxor.u32 %v16042, %v16038
%v16046 = vadd.s32 %v16043, %v16038
%v16048 = vshll.u32 %v16043, 29
%v16049 = vshrl.u32 %v16043, 3
%v16050 = vor.u32 %v16049, %v16048
%v16051 = vxor.u32 %v16050, %v16046
%v16054 = vadd.s32 %v16051, %v16046
%v16056 = vshll.u32 %v16051, 16
%v16057 = vshrl.u32 %v16051, 16
%v16058 = vor.u32 %v16057, %v16056
%v16059 = vxor.u32 %v16058, %v16054
%v16062 = vadd.s32 %v16059, %v16054
%v16066 = vadd.s32 %v16062, %v8
%v16068 = vshll.u32 %v16059, 24
%v16069 = vshrl.u32 %v16059, 8
%v16070 = vor.u32 %v16069, %v16068
%v16071 = vxor.u32 %v16070, %v16062
%v16074 = vadd.s32 %v16071, %v10
%v16078 = vadd.s32 2, %v16074
%v16082 = vadd.s32 %v16078, %v16066
%v16084 = vshll.u32 %v16078, 13
%v16085 = vshrl.u32 %v16078, 19
%v16086 = vor.u32 %v16085, %v16084
%v16087 = vxor.u32 %v16086, %v16082
%v16090 = vadd.s32 %v16087, %v16082
%v16092 = vshll.u32 %v16087, 15
%v16093 = vshrl.u32 %v16087, 17
%v16094 = vor.u32 %v16093, %v16092
%v16095 = vxor.u32 %v16094, %v16090
%v16098 = vadd.s32 %v16095, %v16090
%v16100 = vshll.u32 %v16095, 26
%v16101 = vshrl.u32 %v16095, 6
%v16102 = vor.u32 %v16101, %v16100
%v16103 = vxor.u32 %v16102, %v16098
%v16106 = vadd.s32 %v16103, %v16098
%v16110 = vadd.s32 %v16106, %v10
%v16112 = vshll.u32 %v16103, 6
%v16113 = vshrl.u32 %v16103, 26
%v16114 = vor.u32 %v16113, %v16112
%v16115 = vxor.u32 %v16114, %v16106
%v16118 = vadd.s32 %v16115, %v9
%v16122 = vadd.s32 3, %v16118
%v16126 = vadd.s32 %v16122, %v16110
%v16128 = vshll.u32 %v16122, 17
%v16129 = vshrl.u32 %v16122, 15
%v16130 = vor.u32 %v16129, %v16128
%v16131 = vxor.u32 %v16130, %v16126
%v16134 = vadd.s32 %v16131, %v16126
%v16136 = vshll.u32 %v16131, 29
%v16137 = vshrl.u32 %v16131, 3
%v16138 = vor.u32 %v16137, %v16136
%v16139 = vxor.u32 %v16138, %v16134
%v16142 = vadd.s32 %v16139, %v16134
%v16144 = vshll.u32 %v16139, 16
%v16145 = vshrl.u32 %v16139, 16
%v16146 = vor.u32 %v16145, %v16144
%v16147 = vxor.u32 %v16146, %v16142
%v16150 = vadd.s32 %v16147, %v16142
%v16154 = vadd.s32 %v16150, %v9
%v16156 = vshll.u32 %v16147, 24
%v16157 = vshrl.u32 %v16147, 8
%v16158 = vor.u32 %v16157, %v16156
%v16159 = vxor.u32 %v16158, %v16150
%v16162 = vadd.s32 %v16159, %v8
%v16166 = vadd.s32 4, %v16162
%v16170 = vadd.s32 %v16166, %v16154
%v16172 = vshll.u32 %v16166, 13
%v16173 = vshrl.u32 %v16166, 19
%v16174 = vor.u32 %v16173, %v16172
%v16175 = vxor.u32 %v16174, %v16170
%v16178 = vadd.s32 %v16175, %v16170
%v16180 = vshll.u32 %v16175, 15
%v16181 = vshrl.u32 %v16175, 17
%v16182 = vor.u32 %v16181, %v16180
%v16183 = vxor.u32 %v16182, %v16178
%v16186 = vadd.s32 %v16183, %v16178
%v16188 = vshll.u32 %v16183, 26
%v16189 = vshrl.u32 %v16183, 6
%v16190 = vor.u32 %v16189, %v16188
%v16191 = vxor.u32 %v16190, %v16186
%v16194 = vadd.s32 %v16191, %v16186
%v16198 = vadd.s32 %v16194, %v8
%v16200 = vshll.u32 %v16191, 6
%v16201 = vshrl.u32 %v16191, 26
%v16202 = vor.u32 %v16201, %v16200
%v16203 = vxor.u32 %v16202, %v16194
%v16206 = vadd.s32 %v16203, %v10
%v16210 = vadd.s32 5, %v16206
%v16212 = vxor.u32 %v16210, %v16198
%v16213 = vand.u32.u8 255, %v16212
%v16214 = vand.u32 65535, %v16213
%v16215 = vshrl.u32 %v16214, 1
%v16216 = vor.u32 16256, %v16215
%v16217 = vand.u32.u16 65535, %v16216
%v119830 = vadd.low.f32.bf16 -1.0, %v16217
%v16226 = vmul.f32 2.0, %v119830
%v16230 = vadd.f32 -0.99609375, %v16226
%v16234 = vmax.f32 %v16230, -0.99609375
%v16236 = vand.u32 2147483647, %v16234
%vm16239 = vcmp.eq.f32.partialorder %v16236, 1.0
%v16244 = vmul.f32 inf, %v16234
%v16246 = vxor.u32 2147483648, %v16234
%v16249 = vmul.f32 %v16246, %v16234
%v16251 = vadd.f32 1.0, %v16249
%v16252 = vlog2.pop %v16251
%v16253 = vmul.f32 0.6931472, %v16252
%v16254 = vmul.f32 -0.5, %v16249
%v16255 = vadd.f32 1.0, %v16254
%v16256 = vmul.f32 %v16255, %v16249
%v16257 = vand.u32 2147483647, %v16249
%vm16258 = vcmp.lt.f32.partialorder %v16257, 0.0004427343
%v16259 = vsel /*vm=*/%vm16258, /*on_true_vy=*/%v16256, /*on_false_vx=*/%v16253
%v16260 = vxor.u32 2147483648, %v16259
%vm16263 = vcmp.lt.f32.partialorder %v16260, 5.0
%v16268 = vsel /*vm=*/%vm16263, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v16272 = vsel /*vm=*/%vm16263, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v16276 = vsel /*vm=*/%vm16263, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v16280 = vsel /*vm=*/%vm16263, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v16284 = vsel /*vm=*/%vm16263, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v16288 = vsel /*vm=*/%vm16263, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v16292 = vsel /*vm=*/%vm16263, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v16296 = vsel /*vm=*/%vm16263, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v16300 = vsel /*vm=*/%vm16263, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v16304 = vadd.f32 -2.5, %v16260
%v16306 = vrsqrt.pop %v16260
%v16307 = vmul.f32 %v16306, %v16260
%vm16308 = vcmp.eq.f32.partialorder %v16260, inf
%v16309 = vsel /*vm=*/%vm16308, /*on_true_vy=*/%v16260, /*on_false_vx=*/%v16307
%vm16310 = vcmp.eq.f32.partialorder %v16260, 0.0
%v16311 = vand.u32 2147483648, %v16260
%v16312 = vsel /*vm=*/%vm16310, /*on_true_vy=*/%v16311, /*on_false_vx=*/%v16309
%v16315 = vadd.f32 -3.0, %v16312
%v16319 = vsel /*vm=*/%vm16263, /*on_true_vy=*/%v16304, /*on_false_vx=*/%v16315
%v16323 = vmul.f32 %v16319, %v16300
%v16327 = vadd.f32 %v16323, %v16296
%v16331 = vmul.f32 %v16327, %v16319
%v16335 = vadd.f32 %v16331, %v16292
%v16339 = vmul.f32 %v16335, %v16319
%v16343 = vadd.f32 %v16339, %v16288
%v16347 = vmul.f32 %v16343, %v16319
%v16351 = vadd.f32 %v16347, %v16284
%v16355 = vmul.f32 %v16351, %v16319
%v16359 = vadd.f32 %v16355, %v16280
%v16363 = vmul.f32 %v16359, %v16319
%v16367 = vadd.f32 %v16363, %v16276
%v16371 = vmul.f32 %v16367, %v16319
%v16375 = vadd.f32 %v16371, %v16272
%v16379 = vmul.f32 %v16375, %v16319
%v16383 = vadd.f32 %v16379, %v16268
%v16387 = vmul.f32 %v16383, %v16234
%v16391 = vsel /*vm=*/%vm16239, /*on_true_vy=*/%v16244, /*on_false_vx=*/%v16387
%v16395 = vmul.f32 1.4140625, %v16391
%v16398 = vpack.c.bf16 %v120417, %v16395
%119831 = vst [vmem:[%s280 + $0x90] sm:$0xf] /*vst_source=*/%v16398
%v16402 = vadd.s32 %v15477, %v1381
%v16412 = vadd.s32 %v16402, %v415
%vm16416 = vcmp.lt.u32.totalorder %v16412, %v16402
%vm16421 = vcmp.lt.u32.totalorder %v16402, %v1381
%v16426 = vadd.s32 %v15460, %v1368
%v16430 = vadd.s32 1, %v16426
%v16434 = vsel /*vm=*/%vm16421, /*on_true_vy=*/%v16430, /*on_false_vx=*/%v16426
%v16438 = vadd.s32 1, %v16434
%v16442 = vsel /*vm=*/%vm16416, /*on_true_vy=*/%v16438, /*on_false_vx=*/%v16434
%v16447 = vadd.s32 %v16442, %v10
%v16451 = vadd.s32 %v16412, %v9
%v16455 = vadd.s32 %v16451, %v16447
%v16457 = vshll.u32 %v16451, 13
%v16458 = vshrl.u32 %v16451, 19
%v16459 = vor.u32 %v16458, %v16457
%v16460 = vxor.u32 %v16459, %v16455
%v16463 = vadd.s32 %v16460, %v16455
%v16465 = vshll.u32 %v16460, 15
%v16466 = vshrl.u32 %v16460, 17
%v16467 = vor.u32 %v16466, %v16465
%v16468 = vxor.u32 %v16467, %v16463
%v16471 = vadd.s32 %v16468, %v16463
%v16473 = vshll.u32 %v16468, 26
%v16474 = vshrl.u32 %v16468, 6
%v16475 = vor.u32 %v16474, %v16473
%v16476 = vxor.u32 %v16475, %v16471
%v16479 = vadd.s32 %v16476, %v16471
%v16483 = vadd.s32 %v16479, %v9
%v16485 = vshll.u32 %v16476, 6
%v16486 = vshrl.u32 %v16476, 26
%v16487 = vor.u32 %v16486, %v16485
%v16488 = vxor.u32 %v16487, %v16479
%v16491 = vadd.s32 %v16488, %v8
%v16495 = vadd.s32 1, %v16491
%v16499 = vadd.s32 %v16495, %v16483
%v16501 = vshll.u32 %v16495, 17
%v16502 = vshrl.u32 %v16495, 15
%v16503 = vor.u32 %v16502, %v16501
%v16504 = vxor.u32 %v16503, %v16499
%v16507 = vadd.s32 %v16504, %v16499
%v16509 = vshll.u32 %v16504, 29
%v16510 = vshrl.u32 %v16504, 3
%v16511 = vor.u32 %v16510, %v16509
%v16512 = vxor.u32 %v16511, %v16507
%v16515 = vadd.s32 %v16512, %v16507
%v16517 = vshll.u32 %v16512, 16
%v16518 = vshrl.u32 %v16512, 16
%v16519 = vor.u32 %v16518, %v16517
%v16520 = vxor.u32 %v16519, %v16515
%v16523 = vadd.s32 %v16520, %v16515
%v16527 = vadd.s32 %v16523, %v8
%v16529 = vshll.u32 %v16520, 24
%v16530 = vshrl.u32 %v16520, 8
%v16531 = vor.u32 %v16530, %v16529
%v16532 = vxor.u32 %v16531, %v16523
%v16535 = vadd.s32 %v16532, %v10
%v16539 = vadd.s32 2, %v16535
%v16543 = vadd.s32 %v16539, %v16527
%v16545 = vshll.u32 %v16539, 13
%v16546 = vshrl.u32 %v16539, 19
%v16547 = vor.u32 %v16546, %v16545
%v16548 = vxor.u32 %v16547, %v16543
%v16551 = vadd.s32 %v16548, %v16543
%v16553 = vshll.u32 %v16548, 15
%v16554 = vshrl.u32 %v16548, 17
%v16555 = vor.u32 %v16554, %v16553
%v16556 = vxor.u32 %v16555, %v16551
%v16559 = vadd.s32 %v16556, %v16551
%v16561 = vshll.u32 %v16556, 26
%v16562 = vshrl.u32 %v16556, 6
%v16563 = vor.u32 %v16562, %v16561
%v16564 = vxor.u32 %v16563, %v16559
%v16567 = vadd.s32 %v16564, %v16559
%v16571 = vadd.s32 %v16567, %v10
%v16573 = vshll.u32 %v16564, 6
%v16574 = vshrl.u32 %v16564, 26
%v16575 = vor.u32 %v16574, %v16573
%v16576 = vxor.u32 %v16575, %v16567
%v16579 = vadd.s32 %v16576, %v9
%v16583 = vadd.s32 3, %v16579
%v16587 = vadd.s32 %v16583, %v16571
%v16589 = vshll.u32 %v16583, 17
%v16590 = vshrl.u32 %v16583, 15
%v16591 = vor.u32 %v16590, %v16589
%v16592 = vxor.u32 %v16591, %v16587
%v16595 = vadd.s32 %v16592, %v16587
%v16597 = vshll.u32 %v16592, 29
%v16598 = vshrl.u32 %v16592, 3
%v16599 = vor.u32 %v16598, %v16597
%v16600 = vxor.u32 %v16599, %v16595
%v16603 = vadd.s32 %v16600, %v16595
%v16605 = vshll.u32 %v16600, 16
%v16606 = vshrl.u32 %v16600, 16
%v16607 = vor.u32 %v16606, %v16605
%v16608 = vxor.u32 %v16607, %v16603
%v16611 = vadd.s32 %v16608, %v16603
%v16615 = vadd.s32 %v16611, %v9
%v16617 = vshll.u32 %v16608, 24
%v16618 = vshrl.u32 %v16608, 8
%v16619 = vor.u32 %v16618, %v16617
%v16620 = vxor.u32 %v16619, %v16611
%v16623 = vadd.s32 %v16620, %v8
%v16627 = vadd.s32 4, %v16623
%v16631 = vadd.s32 %v16627, %v16615
%v16633 = vshll.u32 %v16627, 13
%v16634 = vshrl.u32 %v16627, 19
%v16635 = vor.u32 %v16634, %v16633
%v16636 = vxor.u32 %v16635, %v16631
%v16639 = vadd.s32 %v16636, %v16631
%v16641 = vshll.u32 %v16636, 15
%v16642 = vshrl.u32 %v16636, 17
%v16643 = vor.u32 %v16642, %v16641
%v16644 = vxor.u32 %v16643, %v16639
%v16647 = vadd.s32 %v16644, %v16639
%v16649 = vshll.u32 %v16644, 26
%v16650 = vshrl.u32 %v16644, 6
%v16651 = vor.u32 %v16650, %v16649
%v16652 = vxor.u32 %v16651, %v16647
%v16655 = vadd.s32 %v16652, %v16647
%v16659 = vadd.s32 %v16655, %v8
%v16661 = vshll.u32 %v16652, 6
%v16662 = vshrl.u32 %v16652, 26
%v16663 = vor.u32 %v16662, %v16661
%v16664 = vxor.u32 %v16663, %v16655
%v16667 = vadd.s32 %v16664, %v10
%v16671 = vadd.s32 5, %v16667
%v16673 = vxor.u32 %v16671, %v16659
%v16674 = vand.u32.u8 255, %v16673
%v16675 = vand.u32 65535, %v16674
%v16676 = vshrl.u32 %v16675, 1
%v16677 = vor.u32 16256, %v16676
%v16678 = vand.u32.u16 65535, %v16677
%v119832 = vadd.low.f32.bf16 -1.0, %v16678
%v16687 = vmul.f32 2.0, %v119832
%v16691 = vadd.f32 -0.99609375, %v16687
%v16695 = vmax.f32 %v16691, -0.99609375
%v16697 = vand.u32 2147483647, %v16695
%vm16700 = vcmp.eq.f32.partialorder %v16697, 1.0
%v16705 = vmul.f32 inf, %v16695
%v16707 = vxor.u32 2147483648, %v16695
%v16710 = vmul.f32 %v16707, %v16695
%v16712 = vadd.f32 1.0, %v16710
%v16713 = vlog2.pop %v16712
%v16714 = vmul.f32 0.6931472, %v16713
%v16715 = vmul.f32 -0.5, %v16710
%v16716 = vadd.f32 1.0, %v16715
%v16717 = vmul.f32 %v16716, %v16710
%v16718 = vand.u32 2147483647, %v16710
%vm16719 = vcmp.lt.f32.partialorder %v16718, 0.0004427343
%v16720 = vsel /*vm=*/%vm16719, /*on_true_vy=*/%v16717, /*on_false_vx=*/%v16714
%v16721 = vxor.u32 2147483648, %v16720
%vm16724 = vcmp.lt.f32.partialorder %v16721, 5.0
%v16729 = vsel /*vm=*/%vm16724, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v16733 = vsel /*vm=*/%vm16724, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v16737 = vsel /*vm=*/%vm16724, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v16741 = vsel /*vm=*/%vm16724, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v16745 = vsel /*vm=*/%vm16724, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v16749 = vsel /*vm=*/%vm16724, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v16753 = vsel /*vm=*/%vm16724, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v16757 = vsel /*vm=*/%vm16724, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v16761 = vsel /*vm=*/%vm16724, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v16765 = vadd.f32 -2.5, %v16721
%v16767 = vrsqrt.pop %v16721
%v16768 = vmul.f32 %v16767, %v16721
%vm16769 = vcmp.eq.f32.partialorder %v16721, inf
%v16770 = vsel /*vm=*/%vm16769, /*on_true_vy=*/%v16721, /*on_false_vx=*/%v16768
%vm16771 = vcmp.eq.f32.partialorder %v16721, 0.0
%v16772 = vand.u32 2147483648, %v16721
%v16773 = vsel /*vm=*/%vm16771, /*on_true_vy=*/%v16772, /*on_false_vx=*/%v16770
%v16776 = vadd.f32 -3.0, %v16773
%v16780 = vsel /*vm=*/%vm16724, /*on_true_vy=*/%v16765, /*on_false_vx=*/%v16776
%v16784 = vmul.f32 %v16780, %v16761
%v16788 = vadd.f32 %v16784, %v16757
%v16792 = vmul.f32 %v16788, %v16780
%v16796 = vadd.f32 %v16792, %v16753
%v16800 = vmul.f32 %v16796, %v16780
%v16804 = vadd.f32 %v16800, %v16749
%v16808 = vmul.f32 %v16804, %v16780
%v16812 = vadd.f32 %v16808, %v16745
%v16816 = vmul.f32 %v16812, %v16780
%v16820 = vadd.f32 %v16816, %v16741
%v16824 = vmul.f32 %v16820, %v16780
%v16828 = vadd.f32 %v16824, %v16737
%v16832 = vmul.f32 %v16828, %v16780
%v16836 = vadd.f32 %v16832, %v16733
%v16840 = vmul.f32 %v16836, %v16780
%v16844 = vadd.f32 %v16840, %v16729
%v16848 = vmul.f32 %v16844, %v16695
%v16852 = vsel /*vm=*/%vm16700, /*on_true_vy=*/%v16705, /*on_false_vx=*/%v16848
%v16856 = vmul.f32 1.4140625, %v16852
%v16859 = vpack.c.bf16 %v120417, %v16856
%119833 = vst [vmem:[%s280 + $0x110] sm:$0xf] /*vst_source=*/%v16859
%v16863 = vadd.s32 %v15477, %v1868
%v16873 = vadd.s32 %v16863, %v415
%vm16877 = vcmp.lt.u32.totalorder %v16873, %v16863
%vm16882 = vcmp.lt.u32.totalorder %v16863, %v1868
%v16887 = vadd.s32 %v15460, %v1855
%v16891 = vadd.s32 1, %v16887
%v16895 = vsel /*vm=*/%vm16882, /*on_true_vy=*/%v16891, /*on_false_vx=*/%v16887
%v16899 = vadd.s32 1, %v16895
%v16903 = vsel /*vm=*/%vm16877, /*on_true_vy=*/%v16899, /*on_false_vx=*/%v16895
%v16908 = vadd.s32 %v16903, %v10
%v16912 = vadd.s32 %v16873, %v9
%v16916 = vadd.s32 %v16912, %v16908
%v16918 = vshll.u32 %v16912, 13
%v16919 = vshrl.u32 %v16912, 19
%v16920 = vor.u32 %v16919, %v16918
%v16921 = vxor.u32 %v16920, %v16916
%v16924 = vadd.s32 %v16921, %v16916
%v16926 = vshll.u32 %v16921, 15
%v16927 = vshrl.u32 %v16921, 17
%v16928 = vor.u32 %v16927, %v16926
%v16929 = vxor.u32 %v16928, %v16924
%v16932 = vadd.s32 %v16929, %v16924
%v16934 = vshll.u32 %v16929, 26
%v16935 = vshrl.u32 %v16929, 6
%v16936 = vor.u32 %v16935, %v16934
%v16937 = vxor.u32 %v16936, %v16932
%v16940 = vadd.s32 %v16937, %v16932
%v16944 = vadd.s32 %v16940, %v9
%v16946 = vshll.u32 %v16937, 6
%v16947 = vshrl.u32 %v16937, 26
%v16948 = vor.u32 %v16947, %v16946
%v16949 = vxor.u32 %v16948, %v16940
%v16952 = vadd.s32 %v16949, %v8
%v16956 = vadd.s32 1, %v16952
%v16960 = vadd.s32 %v16956, %v16944
%v16962 = vshll.u32 %v16956, 17
%v16963 = vshrl.u32 %v16956, 15
%v16964 = vor.u32 %v16963, %v16962
%v16965 = vxor.u32 %v16964, %v16960
%v16968 = vadd.s32 %v16965, %v16960
%v16970 = vshll.u32 %v16965, 29
%v16971 = vshrl.u32 %v16965, 3
%v16972 = vor.u32 %v16971, %v16970
%v16973 = vxor.u32 %v16972, %v16968
%v16976 = vadd.s32 %v16973, %v16968
%v16978 = vshll.u32 %v16973, 16
%v16979 = vshrl.u32 %v16973, 16
%v16980 = vor.u32 %v16979, %v16978
%v16981 = vxor.u32 %v16980, %v16976
%v16984 = vadd.s32 %v16981, %v16976
%v16988 = vadd.s32 %v16984, %v8
%v16990 = vshll.u32 %v16981, 24
%v16991 = vshrl.u32 %v16981, 8
%v16992 = vor.u32 %v16991, %v16990
%v16993 = vxor.u32 %v16992, %v16984
%v16996 = vadd.s32 %v16993, %v10
%v17000 = vadd.s32 2, %v16996
%v17004 = vadd.s32 %v17000, %v16988
%v17006 = vshll.u32 %v17000, 13
%v17007 = vshrl.u32 %v17000, 19
%v17008 = vor.u32 %v17007, %v17006
%v17009 = vxor.u32 %v17008, %v17004
%v17012 = vadd.s32 %v17009, %v17004
%v17014 = vshll.u32 %v17009, 15
%v17015 = vshrl.u32 %v17009, 17
%v17016 = vor.u32 %v17015, %v17014
%v17017 = vxor.u32 %v17016, %v17012
%v17020 = vadd.s32 %v17017, %v17012
%v17022 = vshll.u32 %v17017, 26
%v17023 = vshrl.u32 %v17017, 6
%v17024 = vor.u32 %v17023, %v17022
%v17025 = vxor.u32 %v17024, %v17020
%v17028 = vadd.s32 %v17025, %v17020
%v17032 = vadd.s32 %v17028, %v10
%v17034 = vshll.u32 %v17025, 6
%v17035 = vshrl.u32 %v17025, 26
%v17036 = vor.u32 %v17035, %v17034
%v17037 = vxor.u32 %v17036, %v17028
%v17040 = vadd.s32 %v17037, %v9
%v17044 = vadd.s32 3, %v17040
%v17048 = vadd.s32 %v17044, %v17032
%v17050 = vshll.u32 %v17044, 17
%v17051 = vshrl.u32 %v17044, 15
%v17052 = vor.u32 %v17051, %v17050
%v17053 = vxor.u32 %v17052, %v17048
%v17056 = vadd.s32 %v17053, %v17048
%v17058 = vshll.u32 %v17053, 29
%v17059 = vshrl.u32 %v17053, 3
%v17060 = vor.u32 %v17059, %v17058
%v17061 = vxor.u32 %v17060, %v17056
%v17064 = vadd.s32 %v17061, %v17056
%v17066 = vshll.u32 %v17061, 16
%v17067 = vshrl.u32 %v17061, 16
%v17068 = vor.u32 %v17067, %v17066
%v17069 = vxor.u32 %v17068, %v17064
%v17072 = vadd.s32 %v17069, %v17064
%v17076 = vadd.s32 %v17072, %v9
%v17078 = vshll.u32 %v17069, 24
%v17079 = vshrl.u32 %v17069, 8
%v17080 = vor.u32 %v17079, %v17078
%v17081 = vxor.u32 %v17080, %v17072
%v17084 = vadd.s32 %v17081, %v8
%v17088 = vadd.s32 4, %v17084
%v17092 = vadd.s32 %v17088, %v17076
%v17094 = vshll.u32 %v17088, 13
%v17095 = vshrl.u32 %v17088, 19
%v17096 = vor.u32 %v17095, %v17094
%v17097 = vxor.u32 %v17096, %v17092
%v17100 = vadd.s32 %v17097, %v17092
%v17102 = vshll.u32 %v17097, 15
%v17103 = vshrl.u32 %v17097, 17
%v17104 = vor.u32 %v17103, %v17102
%v17105 = vxor.u32 %v17104, %v17100
%v17108 = vadd.s32 %v17105, %v17100
%v17110 = vshll.u32 %v17105, 26
%v17111 = vshrl.u32 %v17105, 6
%v17112 = vor.u32 %v17111, %v17110
%v17113 = vxor.u32 %v17112, %v17108
%v17116 = vadd.s32 %v17113, %v17108
%v17120 = vadd.s32 %v17116, %v8
%v17122 = vshll.u32 %v17113, 6
%v17123 = vshrl.u32 %v17113, 26
%v17124 = vor.u32 %v17123, %v17122
%v17125 = vxor.u32 %v17124, %v17116
%v17128 = vadd.s32 %v17125, %v10
%v17132 = vadd.s32 5, %v17128
%v17134 = vxor.u32 %v17132, %v17120
%v17135 = vand.u32.u8 255, %v17134
%v17136 = vand.u32 65535, %v17135
%v17137 = vshrl.u32 %v17136, 1
%v17138 = vor.u32 16256, %v17137
%v17139 = vand.u32.u16 65535, %v17138
%v119834 = vadd.low.f32.bf16 -1.0, %v17139
%v17148 = vmul.f32 2.0, %v119834
%v17152 = vadd.f32 -0.99609375, %v17148
%v17156 = vmax.f32 %v17152, -0.99609375
%v17158 = vand.u32 2147483647, %v17156
%vm17161 = vcmp.eq.f32.partialorder %v17158, 1.0
%v17166 = vmul.f32 inf, %v17156
%v17168 = vxor.u32 2147483648, %v17156
%v17171 = vmul.f32 %v17168, %v17156
%v17173 = vadd.f32 1.0, %v17171
%v17174 = vlog2.pop %v17173
%v17175 = vmul.f32 0.6931472, %v17174
%v17176 = vmul.f32 -0.5, %v17171
%v17177 = vadd.f32 1.0, %v17176
%v17178 = vmul.f32 %v17177, %v17171
%v17179 = vand.u32 2147483647, %v17171
%vm17180 = vcmp.lt.f32.partialorder %v17179, 0.0004427343
%v17181 = vsel /*vm=*/%vm17180, /*on_true_vy=*/%v17178, /*on_false_vx=*/%v17175
%v17182 = vxor.u32 2147483648, %v17181
%vm17185 = vcmp.lt.f32.partialorder %v17182, 5.0
%v17190 = vsel /*vm=*/%vm17185, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v17194 = vsel /*vm=*/%vm17185, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v17198 = vsel /*vm=*/%vm17185, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v17202 = vsel /*vm=*/%vm17185, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v17206 = vsel /*vm=*/%vm17185, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v17210 = vsel /*vm=*/%vm17185, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v17214 = vsel /*vm=*/%vm17185, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v17218 = vsel /*vm=*/%vm17185, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v17222 = vsel /*vm=*/%vm17185, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v17226 = vadd.f32 -2.5, %v17182
%v17228 = vrsqrt.pop %v17182
%v17229 = vmul.f32 %v17228, %v17182
%vm17230 = vcmp.eq.f32.partialorder %v17182, inf
%v17231 = vsel /*vm=*/%vm17230, /*on_true_vy=*/%v17182, /*on_false_vx=*/%v17229
%vm17232 = vcmp.eq.f32.partialorder %v17182, 0.0
%v17233 = vand.u32 2147483648, %v17182
%v17234 = vsel /*vm=*/%vm17232, /*on_true_vy=*/%v17233, /*on_false_vx=*/%v17231
%v17237 = vadd.f32 -3.0, %v17234
%v17241 = vsel /*vm=*/%vm17185, /*on_true_vy=*/%v17226, /*on_false_vx=*/%v17237
%v17245 = vmul.f32 %v17241, %v17222
%v17249 = vadd.f32 %v17245, %v17218
%v17253 = vmul.f32 %v17249, %v17241
%v17257 = vadd.f32 %v17253, %v17214
%v17261 = vmul.f32 %v17257, %v17241
%v17265 = vadd.f32 %v17261, %v17210
%v17269 = vmul.f32 %v17265, %v17241
%v17273 = vadd.f32 %v17269, %v17206
%v17277 = vmul.f32 %v17273, %v17241
%v17281 = vadd.f32 %v17277, %v17202
%v17285 = vmul.f32 %v17281, %v17241
%v17289 = vadd.f32 %v17285, %v17198
%v17293 = vmul.f32 %v17289, %v17241
%v17297 = vadd.f32 %v17293, %v17194
%v17301 = vmul.f32 %v17297, %v17241
%v17305 = vadd.f32 %v17301, %v17190
%v17309 = vmul.f32 %v17305, %v17156
%v17313 = vsel /*vm=*/%vm17161, /*on_true_vy=*/%v17166, /*on_false_vx=*/%v17309
%v17317 = vmul.f32 1.4140625, %v17313
%v17320 = vpack.c.bf16 %v120417, %v17317
%119835 = vst [vmem:[%s280 + $0x190] sm:$0xf] /*vst_source=*/%v17320
%v17324 = vadd.s32 %v15477, %v2355
%v17334 = vadd.s32 %v17324, %v415
%vm17338 = vcmp.lt.u32.totalorder %v17334, %v17324
%vm17343 = vcmp.lt.u32.totalorder %v17324, %v2355
%v17348 = vadd.s32 %v15460, %v2342
%v17352 = vadd.s32 1, %v17348
%v17356 = vsel /*vm=*/%vm17343, /*on_true_vy=*/%v17352, /*on_false_vx=*/%v17348
%v17360 = vadd.s32 1, %v17356
%v17364 = vsel /*vm=*/%vm17338, /*on_true_vy=*/%v17360, /*on_false_vx=*/%v17356
%v17369 = vadd.s32 %v17364, %v10
%v17373 = vadd.s32 %v17334, %v9
%v17377 = vadd.s32 %v17373, %v17369
%v17379 = vshll.u32 %v17373, 13
%v17380 = vshrl.u32 %v17373, 19
%v17381 = vor.u32 %v17380, %v17379
%v17382 = vxor.u32 %v17381, %v17377
%v17385 = vadd.s32 %v17382, %v17377
%v17387 = vshll.u32 %v17382, 15
%v17388 = vshrl.u32 %v17382, 17
%v17389 = vor.u32 %v17388, %v17387
%v17390 = vxor.u32 %v17389, %v17385
%v17393 = vadd.s32 %v17390, %v17385
%v17395 = vshll.u32 %v17390, 26
%v17396 = vshrl.u32 %v17390, 6
%v17397 = vor.u32 %v17396, %v17395
%v17398 = vxor.u32 %v17397, %v17393
%v17401 = vadd.s32 %v17398, %v17393
%v17405 = vadd.s32 %v17401, %v9
%v17407 = vshll.u32 %v17398, 6
%v17408 = vshrl.u32 %v17398, 26
%v17409 = vor.u32 %v17408, %v17407
%v17410 = vxor.u32 %v17409, %v17401
%v17413 = vadd.s32 %v17410, %v8
%v17417 = vadd.s32 1, %v17413
%v17421 = vadd.s32 %v17417, %v17405
%v17423 = vshll.u32 %v17417, 17
%v17424 = vshrl.u32 %v17417, 15
%v17425 = vor.u32 %v17424, %v17423
%v17426 = vxor.u32 %v17425, %v17421
%v17429 = vadd.s32 %v17426, %v17421
%v17431 = vshll.u32 %v17426, 29
%v17432 = vshrl.u32 %v17426, 3
%v17433 = vor.u32 %v17432, %v17431
%v17434 = vxor.u32 %v17433, %v17429
%v17437 = vadd.s32 %v17434, %v17429
%v17439 = vshll.u32 %v17434, 16
%v17440 = vshrl.u32 %v17434, 16
%v17441 = vor.u32 %v17440, %v17439
%v17442 = vxor.u32 %v17441, %v17437
%v17445 = vadd.s32 %v17442, %v17437
%v17449 = vadd.s32 %v17445, %v8
%v17451 = vshll.u32 %v17442, 24
%v17452 = vshrl.u32 %v17442, 8
%v17453 = vor.u32 %v17452, %v17451
%v17454 = vxor.u32 %v17453, %v17445
%v17457 = vadd.s32 %v17454, %v10
%v17461 = vadd.s32 2, %v17457
%v17465 = vadd.s32 %v17461, %v17449
%v17467 = vshll.u32 %v17461, 13
%v17468 = vshrl.u32 %v17461, 19
%v17469 = vor.u32 %v17468, %v17467
%v17470 = vxor.u32 %v17469, %v17465
%v17473 = vadd.s32 %v17470, %v17465
%v17475 = vshll.u32 %v17470, 15
%v17476 = vshrl.u32 %v17470, 17
%v17477 = vor.u32 %v17476, %v17475
%v17478 = vxor.u32 %v17477, %v17473
%v17481 = vadd.s32 %v17478, %v17473
%v17483 = vshll.u32 %v17478, 26
%v17484 = vshrl.u32 %v17478, 6
%v17485 = vor.u32 %v17484, %v17483
%v17486 = vxor.u32 %v17485, %v17481
%v17489 = vadd.s32 %v17486, %v17481
%v17493 = vadd.s32 %v17489, %v10
%v17495 = vshll.u32 %v17486, 6
%v17496 = vshrl.u32 %v17486, 26
%v17497 = vor.u32 %v17496, %v17495
%v17498 = vxor.u32 %v17497, %v17489
%v17501 = vadd.s32 %v17498, %v9
%v17505 = vadd.s32 3, %v17501
%v17509 = vadd.s32 %v17505, %v17493
%v17511 = vshll.u32 %v17505, 17
%v17512 = vshrl.u32 %v17505, 15
%v17513 = vor.u32 %v17512, %v17511
%v17514 = vxor.u32 %v17513, %v17509
%v17517 = vadd.s32 %v17514, %v17509
%v17519 = vshll.u32 %v17514, 29
%v17520 = vshrl.u32 %v17514, 3
%v17521 = vor.u32 %v17520, %v17519
%v17522 = vxor.u32 %v17521, %v17517
%v17525 = vadd.s32 %v17522, %v17517
%v17527 = vshll.u32 %v17522, 16
%v17528 = vshrl.u32 %v17522, 16
%v17529 = vor.u32 %v17528, %v17527
%v17530 = vxor.u32 %v17529, %v17525
%v17533 = vadd.s32 %v17530, %v17525
%v17537 = vadd.s32 %v17533, %v9
%v17539 = vshll.u32 %v17530, 24
%v17540 = vshrl.u32 %v17530, 8
%v17541 = vor.u32 %v17540, %v17539
%v17542 = vxor.u32 %v17541, %v17533
%v17545 = vadd.s32 %v17542, %v8
%v17549 = vadd.s32 4, %v17545
%v17553 = vadd.s32 %v17549, %v17537
%v17555 = vshll.u32 %v17549, 13
%v17556 = vshrl.u32 %v17549, 19
%v17557 = vor.u32 %v17556, %v17555
%v17558 = vxor.u32 %v17557, %v17553
%v17561 = vadd.s32 %v17558, %v17553
%v17563 = vshll.u32 %v17558, 15
%v17564 = vshrl.u32 %v17558, 17
%v17565 = vor.u32 %v17564, %v17563
%v17566 = vxor.u32 %v17565, %v17561
%v17569 = vadd.s32 %v17566, %v17561
%v17571 = vshll.u32 %v17566, 26
%v17572 = vshrl.u32 %v17566, 6
%v17573 = vor.u32 %v17572, %v17571
%v17574 = vxor.u32 %v17573, %v17569
%v17577 = vadd.s32 %v17574, %v17569
%v17581 = vadd.s32 %v17577, %v8
%v17583 = vshll.u32 %v17574, 6
%v17584 = vshrl.u32 %v17574, 26
%v17585 = vor.u32 %v17584, %v17583
%v17586 = vxor.u32 %v17585, %v17577
%v17589 = vadd.s32 %v17586, %v10
%v17593 = vadd.s32 5, %v17589
%v17595 = vxor.u32 %v17593, %v17581
%v17596 = vand.u32.u8 255, %v17595
%v17597 = vand.u32 65535, %v17596
%v17598 = vshrl.u32 %v17597, 1
%v17599 = vor.u32 16256, %v17598
%v17600 = vand.u32.u16 65535, %v17599
%v119836 = vadd.low.f32.bf16 -1.0, %v17600
%v17609 = vmul.f32 2.0, %v119836
%v17613 = vadd.f32 -0.99609375, %v17609
%v17617 = vmax.f32 %v17613, -0.99609375
%v17619 = vand.u32 2147483647, %v17617
%vm17622 = vcmp.eq.f32.partialorder %v17619, 1.0
%v17627 = vmul.f32 inf, %v17617
%v17629 = vxor.u32 2147483648, %v17617
%v17632 = vmul.f32 %v17629, %v17617
%v17634 = vadd.f32 1.0, %v17632
%v17635 = vlog2.pop %v17634
%v17636 = vmul.f32 0.6931472, %v17635
%v17637 = vmul.f32 -0.5, %v17632
%v17638 = vadd.f32 1.0, %v17637
%v17639 = vmul.f32 %v17638, %v17632
%v17640 = vand.u32 2147483647, %v17632
%vm17641 = vcmp.lt.f32.partialorder %v17640, 0.0004427343
%v17642 = vsel /*vm=*/%vm17641, /*on_true_vy=*/%v17639, /*on_false_vx=*/%v17636
%v17643 = vxor.u32 2147483648, %v17642
%vm17646 = vcmp.lt.f32.partialorder %v17643, 5.0
%v17651 = vsel /*vm=*/%vm17646, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v17655 = vsel /*vm=*/%vm17646, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v17659 = vsel /*vm=*/%vm17646, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v17663 = vsel /*vm=*/%vm17646, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v17667 = vsel /*vm=*/%vm17646, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v17671 = vsel /*vm=*/%vm17646, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v17675 = vsel /*vm=*/%vm17646, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v17679 = vsel /*vm=*/%vm17646, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v17683 = vsel /*vm=*/%vm17646, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v17687 = vadd.f32 -2.5, %v17643
%v17689 = vrsqrt.pop %v17643
%v17690 = vmul.f32 %v17689, %v17643
%vm17691 = vcmp.eq.f32.partialorder %v17643, inf
%v17692 = vsel /*vm=*/%vm17691, /*on_true_vy=*/%v17643, /*on_false_vx=*/%v17690
%vm17693 = vcmp.eq.f32.partialorder %v17643, 0.0
%v17694 = vand.u32 2147483648, %v17643
%v17695 = vsel /*vm=*/%vm17693, /*on_true_vy=*/%v17694, /*on_false_vx=*/%v17692
%v17698 = vadd.f32 -3.0, %v17695
%v17702 = vsel /*vm=*/%vm17646, /*on_true_vy=*/%v17687, /*on_false_vx=*/%v17698
%v17706 = vmul.f32 %v17702, %v17683
%v17710 = vadd.f32 %v17706, %v17679
%v17714 = vmul.f32 %v17710, %v17702
%v17718 = vadd.f32 %v17714, %v17675
%v17722 = vmul.f32 %v17718, %v17702
%v17726 = vadd.f32 %v17722, %v17671
%v17730 = vmul.f32 %v17726, %v17702
%v17734 = vadd.f32 %v17730, %v17667
%v17738 = vmul.f32 %v17734, %v17702
%v17742 = vadd.f32 %v17738, %v17663
%v17746 = vmul.f32 %v17742, %v17702
%v17750 = vadd.f32 %v17746, %v17659
%v17754 = vmul.f32 %v17750, %v17702
%v17758 = vadd.f32 %v17754, %v17655
%v17762 = vmul.f32 %v17758, %v17702
%v17766 = vadd.f32 %v17762, %v17651
%v17770 = vmul.f32 %v17766, %v17617
%v17774 = vsel /*vm=*/%vm17622, /*on_true_vy=*/%v17627, /*on_false_vx=*/%v17770
%v17778 = vmul.f32 1.4140625, %v17774
%v17781 = vpack.c.bf16 %v120417, %v17778
%119837 = vst [vmem:[%s280 + $0x210] sm:$0xf] /*vst_source=*/%v17781
%v17785 = vadd.s32 %v15477, %v2842
%v17795 = vadd.s32 %v17785, %v415
%vm17799 = vcmp.lt.u32.totalorder %v17795, %v17785
%vm17804 = vcmp.lt.u32.totalorder %v17785, %v2842
%v17809 = vadd.s32 %v15460, %v2829
%v17813 = vadd.s32 1, %v17809
%v17817 = vsel /*vm=*/%vm17804, /*on_true_vy=*/%v17813, /*on_false_vx=*/%v17809
%v17821 = vadd.s32 1, %v17817
%v17825 = vsel /*vm=*/%vm17799, /*on_true_vy=*/%v17821, /*on_false_vx=*/%v17817
%v17830 = vadd.s32 %v17825, %v10
%v17834 = vadd.s32 %v17795, %v9
%v17838 = vadd.s32 %v17834, %v17830
%v17840 = vshll.u32 %v17834, 13
%v17841 = vshrl.u32 %v17834, 19
%v17842 = vor.u32 %v17841, %v17840
%v17843 = vxor.u32 %v17842, %v17838
%v17846 = vadd.s32 %v17843, %v17838
%v17848 = vshll.u32 %v17843, 15
%v17849 = vshrl.u32 %v17843, 17
%v17850 = vor.u32 %v17849, %v17848
%v17851 = vxor.u32 %v17850, %v17846
%v17854 = vadd.s32 %v17851, %v17846
%v17856 = vshll.u32 %v17851, 26
%v17857 = vshrl.u32 %v17851, 6
%v17858 = vor.u32 %v17857, %v17856
%v17859 = vxor.u32 %v17858, %v17854
%v17862 = vadd.s32 %v17859, %v17854
%v17866 = vadd.s32 %v17862, %v9
%v17868 = vshll.u32 %v17859, 6
%v17869 = vshrl.u32 %v17859, 26
%v17870 = vor.u32 %v17869, %v17868
%v17871 = vxor.u32 %v17870, %v17862
%v17874 = vadd.s32 %v17871, %v8
%v17878 = vadd.s32 1, %v17874
%v17882 = vadd.s32 %v17878, %v17866
%v17884 = vshll.u32 %v17878, 17
%v17885 = vshrl.u32 %v17878, 15
%v17886 = vor.u32 %v17885, %v17884
%v17887 = vxor.u32 %v17886, %v17882
%v17890 = vadd.s32 %v17887, %v17882
%v17892 = vshll.u32 %v17887, 29
%v17893 = vshrl.u32 %v17887, 3
%v17894 = vor.u32 %v17893, %v17892
%v17895 = vxor.u32 %v17894, %v17890
%v17898 = vadd.s32 %v17895, %v17890
%v17900 = vshll.u32 %v17895, 16
%v17901 = vshrl.u32 %v17895, 16
%v17902 = vor.u32 %v17901, %v17900
%v17903 = vxor.u32 %v17902, %v17898
%v17906 = vadd.s32 %v17903, %v17898
%v17910 = vadd.s32 %v17906, %v8
%v17912 = vshll.u32 %v17903, 24
%v17913 = vshrl.u32 %v17903, 8
%v17914 = vor.u32 %v17913, %v17912
%v17915 = vxor.u32 %v17914, %v17906
%v17918 = vadd.s32 %v17915, %v10
%v17922 = vadd.s32 2, %v17918
%v17926 = vadd.s32 %v17922, %v17910
%v17928 = vshll.u32 %v17922, 13
%v17929 = vshrl.u32 %v17922, 19
%v17930 = vor.u32 %v17929, %v17928
%v17931 = vxor.u32 %v17930, %v17926
%v17934 = vadd.s32 %v17931, %v17926
%v17936 = vshll.u32 %v17931, 15
%v17937 = vshrl.u32 %v17931, 17
%v17938 = vor.u32 %v17937, %v17936
%v17939 = vxor.u32 %v17938, %v17934
%v17942 = vadd.s32 %v17939, %v17934
%v17944 = vshll.u32 %v17939, 26
%v17945 = vshrl.u32 %v17939, 6
%v17946 = vor.u32 %v17945, %v17944
%v17947 = vxor.u32 %v17946, %v17942
%v17950 = vadd.s32 %v17947, %v17942
%v17954 = vadd.s32 %v17950, %v10
%v17956 = vshll.u32 %v17947, 6
%v17957 = vshrl.u32 %v17947, 26
%v17958 = vor.u32 %v17957, %v17956
%v17959 = vxor.u32 %v17958, %v17950
%v17962 = vadd.s32 %v17959, %v9
%v17966 = vadd.s32 3, %v17962
%v17970 = vadd.s32 %v17966, %v17954
%v17972 = vshll.u32 %v17966, 17
%v17973 = vshrl.u32 %v17966, 15
%v17974 = vor.u32 %v17973, %v17972
%v17975 = vxor.u32 %v17974, %v17970
%v17978 = vadd.s32 %v17975, %v17970
%v17980 = vshll.u32 %v17975, 29
%v17981 = vshrl.u32 %v17975, 3
%v17982 = vor.u32 %v17981, %v17980
%v17983 = vxor.u32 %v17982, %v17978
%v17986 = vadd.s32 %v17983, %v17978
%v17988 = vshll.u32 %v17983, 16
%v17989 = vshrl.u32 %v17983, 16
%v17990 = vor.u32 %v17989, %v17988
%v17991 = vxor.u32 %v17990, %v17986
%v17994 = vadd.s32 %v17991, %v17986
%v17998 = vadd.s32 %v17994, %v9
%v18000 = vshll.u32 %v17991, 24
%v18001 = vshrl.u32 %v17991, 8
%v18002 = vor.u32 %v18001, %v18000
%v18003 = vxor.u32 %v18002, %v17994
%v18006 = vadd.s32 %v18003, %v8
%v18010 = vadd.s32 4, %v18006
%v18014 = vadd.s32 %v18010, %v17998
%v18016 = vshll.u32 %v18010, 13
%v18017 = vshrl.u32 %v18010, 19
%v18018 = vor.u32 %v18017, %v18016
%v18019 = vxor.u32 %v18018, %v18014
%v18022 = vadd.s32 %v18019, %v18014
%v18024 = vshll.u32 %v18019, 15
%v18025 = vshrl.u32 %v18019, 17
%v18026 = vor.u32 %v18025, %v18024
%v18027 = vxor.u32 %v18026, %v18022
%v18030 = vadd.s32 %v18027, %v18022
%v18032 = vshll.u32 %v18027, 26
%v18033 = vshrl.u32 %v18027, 6
%v18034 = vor.u32 %v18033, %v18032
%v18035 = vxor.u32 %v18034, %v18030
%v18038 = vadd.s32 %v18035, %v18030
%v18042 = vadd.s32 %v18038, %v8
%v18044 = vshll.u32 %v18035, 6
%v18045 = vshrl.u32 %v18035, 26
%v18046 = vor.u32 %v18045, %v18044
%v18047 = vxor.u32 %v18046, %v18038
%v18050 = vadd.s32 %v18047, %v10
%v18054 = vadd.s32 5, %v18050
%v18056 = vxor.u32 %v18054, %v18042
%v18057 = vand.u32.u8 255, %v18056
%v18058 = vand.u32 65535, %v18057
%v18059 = vshrl.u32 %v18058, 1
%v18060 = vor.u32 16256, %v18059
%v18061 = vand.u32.u16 65535, %v18060
%v119838 = vadd.low.f32.bf16 -1.0, %v18061
%v18070 = vmul.f32 2.0, %v119838
%v18074 = vadd.f32 -0.99609375, %v18070
%v18078 = vmax.f32 %v18074, -0.99609375
%v18080 = vand.u32 2147483647, %v18078
%vm18083 = vcmp.eq.f32.partialorder %v18080, 1.0
%v18088 = vmul.f32 inf, %v18078
%v18090 = vxor.u32 2147483648, %v18078
%v18093 = vmul.f32 %v18090, %v18078
%v18095 = vadd.f32 1.0, %v18093
%v18096 = vlog2.pop %v18095
%v18097 = vmul.f32 0.6931472, %v18096
%v18098 = vmul.f32 -0.5, %v18093
%v18099 = vadd.f32 1.0, %v18098
%v18100 = vmul.f32 %v18099, %v18093
%v18101 = vand.u32 2147483647, %v18093
%vm18102 = vcmp.lt.f32.partialorder %v18101, 0.0004427343
%v18103 = vsel /*vm=*/%vm18102, /*on_true_vy=*/%v18100, /*on_false_vx=*/%v18097
%v18104 = vxor.u32 2147483648, %v18103
%vm18107 = vcmp.lt.f32.partialorder %v18104, 5.0
%v18112 = vsel /*vm=*/%vm18107, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v18116 = vsel /*vm=*/%vm18107, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v18120 = vsel /*vm=*/%vm18107, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v18124 = vsel /*vm=*/%vm18107, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v18128 = vsel /*vm=*/%vm18107, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v18132 = vsel /*vm=*/%vm18107, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v18136 = vsel /*vm=*/%vm18107, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v18140 = vsel /*vm=*/%vm18107, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v18144 = vsel /*vm=*/%vm18107, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v18148 = vadd.f32 -2.5, %v18104
%v18150 = vrsqrt.pop %v18104
%v18151 = vmul.f32 %v18150, %v18104
%vm18152 = vcmp.eq.f32.partialorder %v18104, inf
%v18153 = vsel /*vm=*/%vm18152, /*on_true_vy=*/%v18104, /*on_false_vx=*/%v18151
%vm18154 = vcmp.eq.f32.partialorder %v18104, 0.0
%v18155 = vand.u32 2147483648, %v18104
%v18156 = vsel /*vm=*/%vm18154, /*on_true_vy=*/%v18155, /*on_false_vx=*/%v18153
%v18159 = vadd.f32 -3.0, %v18156
%v18163 = vsel /*vm=*/%vm18107, /*on_true_vy=*/%v18148, /*on_false_vx=*/%v18159
%v18167 = vmul.f32 %v18163, %v18144
%v18171 = vadd.f32 %v18167, %v18140
%v18175 = vmul.f32 %v18171, %v18163
%v18179 = vadd.f32 %v18175, %v18136
%v18183 = vmul.f32 %v18179, %v18163
%v18187 = vadd.f32 %v18183, %v18132
%v18191 = vmul.f32 %v18187, %v18163
%v18195 = vadd.f32 %v18191, %v18128
%v18199 = vmul.f32 %v18195, %v18163
%v18203 = vadd.f32 %v18199, %v18124
%v18207 = vmul.f32 %v18203, %v18163
%v18211 = vadd.f32 %v18207, %v18120
%v18215 = vmul.f32 %v18211, %v18163
%v18219 = vadd.f32 %v18215, %v18116
%v18223 = vmul.f32 %v18219, %v18163
%v18227 = vadd.f32 %v18223, %v18112
%v18231 = vmul.f32 %v18227, %v18078
%v18235 = vsel /*vm=*/%vm18083, /*on_true_vy=*/%v18088, /*on_false_vx=*/%v18231
%v18239 = vmul.f32 1.4140625, %v18235
%v18242 = vpack.c.bf16 %v120417, %v18239
%119839 = vst [vmem:[%s280 + $0x290] sm:$0xf] /*vst_source=*/%v18242
%v18246 = vadd.s32 %v15477, %v3329
%v18256 = vadd.s32 %v18246, %v415
%vm18260 = vcmp.lt.u32.totalorder %v18256, %v18246
%vm18265 = vcmp.lt.u32.totalorder %v18246, %v3329
%v18270 = vadd.s32 %v15460, %v3316
%v18274 = vadd.s32 1, %v18270
%v18278 = vsel /*vm=*/%vm18265, /*on_true_vy=*/%v18274, /*on_false_vx=*/%v18270
%v18282 = vadd.s32 1, %v18278
%v18286 = vsel /*vm=*/%vm18260, /*on_true_vy=*/%v18282, /*on_false_vx=*/%v18278
%v18291 = vadd.s32 %v18286, %v10
%v18295 = vadd.s32 %v18256, %v9
%v18299 = vadd.s32 %v18295, %v18291
%v18301 = vshll.u32 %v18295, 13
%v18302 = vshrl.u32 %v18295, 19
%v18303 = vor.u32 %v18302, %v18301
%v18304 = vxor.u32 %v18303, %v18299
%v18307 = vadd.s32 %v18304, %v18299
%v18309 = vshll.u32 %v18304, 15
%v18310 = vshrl.u32 %v18304, 17
%v18311 = vor.u32 %v18310, %v18309
%v18312 = vxor.u32 %v18311, %v18307
%v18315 = vadd.s32 %v18312, %v18307
%v18317 = vshll.u32 %v18312, 26
%v18318 = vshrl.u32 %v18312, 6
%v18319 = vor.u32 %v18318, %v18317
%v18320 = vxor.u32 %v18319, %v18315
%v18323 = vadd.s32 %v18320, %v18315
%v18327 = vadd.s32 %v18323, %v9
%v18329 = vshll.u32 %v18320, 6
%v18330 = vshrl.u32 %v18320, 26
%v18331 = vor.u32 %v18330, %v18329
%v18332 = vxor.u32 %v18331, %v18323
%v18335 = vadd.s32 %v18332, %v8
%v18339 = vadd.s32 1, %v18335
%v18343 = vadd.s32 %v18339, %v18327
%v18345 = vshll.u32 %v18339, 17
%v18346 = vshrl.u32 %v18339, 15
%v18347 = vor.u32 %v18346, %v18345
%v18348 = vxor.u32 %v18347, %v18343
%v18351 = vadd.s32 %v18348, %v18343
%v18353 = vshll.u32 %v18348, 29
%v18354 = vshrl.u32 %v18348, 3
%v18355 = vor.u32 %v18354, %v18353
%v18356 = vxor.u32 %v18355, %v18351
%v18359 = vadd.s32 %v18356, %v18351
%v18361 = vshll.u32 %v18356, 16
%v18362 = vshrl.u32 %v18356, 16
%v18363 = vor.u32 %v18362, %v18361
%v18364 = vxor.u32 %v18363, %v18359
%v18367 = vadd.s32 %v18364, %v18359
%v18371 = vadd.s32 %v18367, %v8
%v18373 = vshll.u32 %v18364, 24
%v18374 = vshrl.u32 %v18364, 8
%v18375 = vor.u32 %v18374, %v18373
%v18376 = vxor.u32 %v18375, %v18367
%v18379 = vadd.s32 %v18376, %v10
%v18383 = vadd.s32 2, %v18379
%v18387 = vadd.s32 %v18383, %v18371
%v18389 = vshll.u32 %v18383, 13
%v18390 = vshrl.u32 %v18383, 19
%v18391 = vor.u32 %v18390, %v18389
%v18392 = vxor.u32 %v18391, %v18387
%v18395 = vadd.s32 %v18392, %v18387
%v18397 = vshll.u32 %v18392, 15
%v18398 = vshrl.u32 %v18392, 17
%v18399 = vor.u32 %v18398, %v18397
%v18400 = vxor.u32 %v18399, %v18395
%v18403 = vadd.s32 %v18400, %v18395
%v18405 = vshll.u32 %v18400, 26
%v18406 = vshrl.u32 %v18400, 6
%v18407 = vor.u32 %v18406, %v18405
%v18408 = vxor.u32 %v18407, %v18403
%v18411 = vadd.s32 %v18408, %v18403
%v18415 = vadd.s32 %v18411, %v10
%v18417 = vshll.u32 %v18408, 6
%v18418 = vshrl.u32 %v18408, 26
%v18419 = vor.u32 %v18418, %v18417
%v18420 = vxor.u32 %v18419, %v18411
%v18423 = vadd.s32 %v18420, %v9
%v18427 = vadd.s32 3, %v18423
%v18431 = vadd.s32 %v18427, %v18415
%v18433 = vshll.u32 %v18427, 17
%v18434 = vshrl.u32 %v18427, 15
%v18435 = vor.u32 %v18434, %v18433
%v18436 = vxor.u32 %v18435, %v18431
%v18439 = vadd.s32 %v18436, %v18431
%v18441 = vshll.u32 %v18436, 29
%v18442 = vshrl.u32 %v18436, 3
%v18443 = vor.u32 %v18442, %v18441
%v18444 = vxor.u32 %v18443, %v18439
%v18447 = vadd.s32 %v18444, %v18439
%v18449 = vshll.u32 %v18444, 16
%v18450 = vshrl.u32 %v18444, 16
%v18451 = vor.u32 %v18450, %v18449
%v18452 = vxor.u32 %v18451, %v18447
%v18455 = vadd.s32 %v18452, %v18447
%v18459 = vadd.s32 %v18455, %v9
%v18461 = vshll.u32 %v18452, 24
%v18462 = vshrl.u32 %v18452, 8
%v18463 = vor.u32 %v18462, %v18461
%v18464 = vxor.u32 %v18463, %v18455
%v18467 = vadd.s32 %v18464, %v8
%v18471 = vadd.s32 4, %v18467
%v18475 = vadd.s32 %v18471, %v18459
%v18477 = vshll.u32 %v18471, 13
%v18478 = vshrl.u32 %v18471, 19
%v18479 = vor.u32 %v18478, %v18477
%v18480 = vxor.u32 %v18479, %v18475
%v18483 = vadd.s32 %v18480, %v18475
%v18485 = vshll.u32 %v18480, 15
%v18486 = vshrl.u32 %v18480, 17
%v18487 = vor.u32 %v18486, %v18485
%v18488 = vxor.u32 %v18487, %v18483
%v18491 = vadd.s32 %v18488, %v18483
%v18493 = vshll.u32 %v18488, 26
%v18494 = vshrl.u32 %v18488, 6
%v18495 = vor.u32 %v18494, %v18493
%v18496 = vxor.u32 %v18495, %v18491
%v18499 = vadd.s32 %v18496, %v18491
%v18503 = vadd.s32 %v18499, %v8
%v18505 = vshll.u32 %v18496, 6
%v18506 = vshrl.u32 %v18496, 26
%v18507 = vor.u32 %v18506, %v18505
%v18508 = vxor.u32 %v18507, %v18499
%v18511 = vadd.s32 %v18508, %v10
%v18515 = vadd.s32 5, %v18511
%v18517 = vxor.u32 %v18515, %v18503
%v18518 = vand.u32.u8 255, %v18517
%v18519 = vand.u32 65535, %v18518
%v18520 = vshrl.u32 %v18519, 1
%v18521 = vor.u32 16256, %v18520
%v18522 = vand.u32.u16 65535, %v18521
%v119840 = vadd.low.f32.bf16 -1.0, %v18522
%v18531 = vmul.f32 2.0, %v119840
%v18535 = vadd.f32 -0.99609375, %v18531
%v18539 = vmax.f32 %v18535, -0.99609375
%v18541 = vand.u32 2147483647, %v18539
%vm18544 = vcmp.eq.f32.partialorder %v18541, 1.0
%v18549 = vmul.f32 inf, %v18539
%v18551 = vxor.u32 2147483648, %v18539
%v18554 = vmul.f32 %v18551, %v18539
%v18556 = vadd.f32 1.0, %v18554
%v18557 = vlog2.pop %v18556
%v18558 = vmul.f32 0.6931472, %v18557
%v18559 = vmul.f32 -0.5, %v18554
%v18560 = vadd.f32 1.0, %v18559
%v18561 = vmul.f32 %v18560, %v18554
%v18562 = vand.u32 2147483647, %v18554
%vm18563 = vcmp.lt.f32.partialorder %v18562, 0.0004427343
%v18564 = vsel /*vm=*/%vm18563, /*on_true_vy=*/%v18561, /*on_false_vx=*/%v18558
%v18565 = vxor.u32 2147483648, %v18564
%vm18568 = vcmp.lt.f32.partialorder %v18565, 5.0
%v18573 = vsel /*vm=*/%vm18568, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v18577 = vsel /*vm=*/%vm18568, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v18581 = vsel /*vm=*/%vm18568, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v18585 = vsel /*vm=*/%vm18568, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v18589 = vsel /*vm=*/%vm18568, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v18593 = vsel /*vm=*/%vm18568, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v18597 = vsel /*vm=*/%vm18568, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v18601 = vsel /*vm=*/%vm18568, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v18605 = vsel /*vm=*/%vm18568, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v18609 = vadd.f32 -2.5, %v18565
%v18611 = vrsqrt.pop %v18565
%v18612 = vmul.f32 %v18611, %v18565
%vm18613 = vcmp.eq.f32.partialorder %v18565, inf
%v18614 = vsel /*vm=*/%vm18613, /*on_true_vy=*/%v18565, /*on_false_vx=*/%v18612
%vm18615 = vcmp.eq.f32.partialorder %v18565, 0.0
%v18616 = vand.u32 2147483648, %v18565
%v18617 = vsel /*vm=*/%vm18615, /*on_true_vy=*/%v18616, /*on_false_vx=*/%v18614
%v18620 = vadd.f32 -3.0, %v18617
%v18624 = vsel /*vm=*/%vm18568, /*on_true_vy=*/%v18609, /*on_false_vx=*/%v18620
%v18628 = vmul.f32 %v18624, %v18605
%v18632 = vadd.f32 %v18628, %v18601
%v18636 = vmul.f32 %v18632, %v18624
%v18640 = vadd.f32 %v18636, %v18597
%v18644 = vmul.f32 %v18640, %v18624
%v18648 = vadd.f32 %v18644, %v18593
%v18652 = vmul.f32 %v18648, %v18624
%v18656 = vadd.f32 %v18652, %v18589
%v18660 = vmul.f32 %v18656, %v18624
%v18664 = vadd.f32 %v18660, %v18585
%v18668 = vmul.f32 %v18664, %v18624
%v18672 = vadd.f32 %v18668, %v18581
%v18676 = vmul.f32 %v18672, %v18624
%v18680 = vadd.f32 %v18676, %v18577
%v18684 = vmul.f32 %v18680, %v18624
%v18688 = vadd.f32 %v18684, %v18573
%v18692 = vmul.f32 %v18688, %v18539
%v18696 = vsel /*vm=*/%vm18544, /*on_true_vy=*/%v18549, /*on_false_vx=*/%v18692
%v18700 = vmul.f32 1.4140625, %v18696
%v18703 = vpack.c.bf16 %v120417, %v18700
%119841 = vst [vmem:[%s280 + $0x310] sm:$0xf] /*vst_source=*/%v18703
%v18707 = vadd.s32 %v15477, %v3816
%v18717 = vadd.s32 %v18707, %v415
%vm18721 = vcmp.lt.u32.totalorder %v18717, %v18707
%vm18726 = vcmp.lt.u32.totalorder %v18707, %v3816
%v18731 = vadd.s32 %v15460, %v3803
%v18735 = vadd.s32 1, %v18731
%v18739 = vsel /*vm=*/%vm18726, /*on_true_vy=*/%v18735, /*on_false_vx=*/%v18731
%v18743 = vadd.s32 1, %v18739
%v18747 = vsel /*vm=*/%vm18721, /*on_true_vy=*/%v18743, /*on_false_vx=*/%v18739
%v18752 = vadd.s32 %v18747, %v10
%v18756 = vadd.s32 %v18717, %v9
%v18760 = vadd.s32 %v18756, %v18752
%v18762 = vshll.u32 %v18756, 13
%v18763 = vshrl.u32 %v18756, 19
%v18764 = vor.u32 %v18763, %v18762
%v18765 = vxor.u32 %v18764, %v18760
%v18768 = vadd.s32 %v18765, %v18760
%v18770 = vshll.u32 %v18765, 15
%v18771 = vshrl.u32 %v18765, 17
%v18772 = vor.u32 %v18771, %v18770
%v18773 = vxor.u32 %v18772, %v18768
%v18776 = vadd.s32 %v18773, %v18768
%v18778 = vshll.u32 %v18773, 26
%v18779 = vshrl.u32 %v18773, 6
%v18780 = vor.u32 %v18779, %v18778
%v18781 = vxor.u32 %v18780, %v18776
%v18784 = vadd.s32 %v18781, %v18776
%v18788 = vadd.s32 %v18784, %v9
%v18790 = vshll.u32 %v18781, 6
%v18791 = vshrl.u32 %v18781, 26
%v18792 = vor.u32 %v18791, %v18790
%v18793 = vxor.u32 %v18792, %v18784
%v18796 = vadd.s32 %v18793, %v8
%v18800 = vadd.s32 1, %v18796
%v18804 = vadd.s32 %v18800, %v18788
%v18806 = vshll.u32 %v18800, 17
%v18807 = vshrl.u32 %v18800, 15
%v18808 = vor.u32 %v18807, %v18806
%v18809 = vxor.u32 %v18808, %v18804
%v18812 = vadd.s32 %v18809, %v18804
%v18814 = vshll.u32 %v18809, 29
%v18815 = vshrl.u32 %v18809, 3
%v18816 = vor.u32 %v18815, %v18814
%v18817 = vxor.u32 %v18816, %v18812
%v18820 = vadd.s32 %v18817, %v18812
%v18822 = vshll.u32 %v18817, 16
%v18823 = vshrl.u32 %v18817, 16
%v18824 = vor.u32 %v18823, %v18822
%v18825 = vxor.u32 %v18824, %v18820
%v18828 = vadd.s32 %v18825, %v18820
%v18832 = vadd.s32 %v18828, %v8
%v18834 = vshll.u32 %v18825, 24
%v18835 = vshrl.u32 %v18825, 8
%v18836 = vor.u32 %v18835, %v18834
%v18837 = vxor.u32 %v18836, %v18828
%v18840 = vadd.s32 %v18837, %v10
%v18844 = vadd.s32 2, %v18840
%v18848 = vadd.s32 %v18844, %v18832
%v18850 = vshll.u32 %v18844, 13
%v18851 = vshrl.u32 %v18844, 19
%v18852 = vor.u32 %v18851, %v18850
%v18853 = vxor.u32 %v18852, %v18848
%v18856 = vadd.s32 %v18853, %v18848
%v18858 = vshll.u32 %v18853, 15
%v18859 = vshrl.u32 %v18853, 17
%v18860 = vor.u32 %v18859, %v18858
%v18861 = vxor.u32 %v18860, %v18856
%v18864 = vadd.s32 %v18861, %v18856
%v18866 = vshll.u32 %v18861, 26
%v18867 = vshrl.u32 %v18861, 6
%v18868 = vor.u32 %v18867, %v18866
%v18869 = vxor.u32 %v18868, %v18864
%v18872 = vadd.s32 %v18869, %v18864
%v18876 = vadd.s32 %v18872, %v10
%v18878 = vshll.u32 %v18869, 6
%v18879 = vshrl.u32 %v18869, 26
%v18880 = vor.u32 %v18879, %v18878
%v18881 = vxor.u32 %v18880, %v18872
%v18884 = vadd.s32 %v18881, %v9
%v18888 = vadd.s32 3, %v18884
%v18892 = vadd.s32 %v18888, %v18876
%v18894 = vshll.u32 %v18888, 17
%v18895 = vshrl.u32 %v18888, 15
%v18896 = vor.u32 %v18895, %v18894
%v18897 = vxor.u32 %v18896, %v18892
%v18900 = vadd.s32 %v18897, %v18892
%v18902 = vshll.u32 %v18897, 29
%v18903 = vshrl.u32 %v18897, 3
%v18904 = vor.u32 %v18903, %v18902
%v18905 = vxor.u32 %v18904, %v18900
%v18908 = vadd.s32 %v18905, %v18900
%v18910 = vshll.u32 %v18905, 16
%v18911 = vshrl.u32 %v18905, 16
%v18912 = vor.u32 %v18911, %v18910
%v18913 = vxor.u32 %v18912, %v18908
%v18916 = vadd.s32 %v18913, %v18908
%v18920 = vadd.s32 %v18916, %v9
%v18922 = vshll.u32 %v18913, 24
%v18923 = vshrl.u32 %v18913, 8
%v18924 = vor.u32 %v18923, %v18922
%v18925 = vxor.u32 %v18924, %v18916
%v18928 = vadd.s32 %v18925, %v8
%v18932 = vadd.s32 4, %v18928
%v18936 = vadd.s32 %v18932, %v18920
%v18938 = vshll.u32 %v18932, 13
%v18939 = vshrl.u32 %v18932, 19
%v18940 = vor.u32 %v18939, %v18938
%v18941 = vxor.u32 %v18940, %v18936
%v18944 = vadd.s32 %v18941, %v18936
%v18946 = vshll.u32 %v18941, 15
%v18947 = vshrl.u32 %v18941, 17
%v18948 = vor.u32 %v18947, %v18946
%v18949 = vxor.u32 %v18948, %v18944
%v18952 = vadd.s32 %v18949, %v18944
%v18954 = vshll.u32 %v18949, 26
%v18955 = vshrl.u32 %v18949, 6
%v18956 = vor.u32 %v18955, %v18954
%v18957 = vxor.u32 %v18956, %v18952
%v18960 = vadd.s32 %v18957, %v18952
%v18964 = vadd.s32 %v18960, %v8
%v18966 = vshll.u32 %v18957, 6
%v18967 = vshrl.u32 %v18957, 26
%v18968 = vor.u32 %v18967, %v18966
%v18969 = vxor.u32 %v18968, %v18960
%v18972 = vadd.s32 %v18969, %v10
%v18976 = vadd.s32 5, %v18972
%v18978 = vxor.u32 %v18976, %v18964
%v18979 = vand.u32.u8 255, %v18978
%v18980 = vand.u32 65535, %v18979
%v18981 = vshrl.u32 %v18980, 1
%v18982 = vor.u32 16256, %v18981
%v18983 = vand.u32.u16 65535, %v18982
%v119842 = vadd.low.f32.bf16 -1.0, %v18983
%v18992 = vmul.f32 2.0, %v119842
%v18996 = vadd.f32 -0.99609375, %v18992
%v19000 = vmax.f32 %v18996, -0.99609375
%v19002 = vand.u32 2147483647, %v19000
%vm19005 = vcmp.eq.f32.partialorder %v19002, 1.0
%v19010 = vmul.f32 inf, %v19000
%v19012 = vxor.u32 2147483648, %v19000
%v19015 = vmul.f32 %v19012, %v19000
%v19017 = vadd.f32 1.0, %v19015
%v19018 = vlog2.pop %v19017
%v19019 = vmul.f32 0.6931472, %v19018
%v19020 = vmul.f32 -0.5, %v19015
%v19021 = vadd.f32 1.0, %v19020
%v19022 = vmul.f32 %v19021, %v19015
%v19023 = vand.u32 2147483647, %v19015
%vm19024 = vcmp.lt.f32.partialorder %v19023, 0.0004427343
%v19025 = vsel /*vm=*/%vm19024, /*on_true_vy=*/%v19022, /*on_false_vx=*/%v19019
%v19026 = vxor.u32 2147483648, %v19025
%vm19029 = vcmp.lt.f32.partialorder %v19026, 5.0
%v19034 = vsel /*vm=*/%vm19029, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v19038 = vsel /*vm=*/%vm19029, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v19042 = vsel /*vm=*/%vm19029, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v19046 = vsel /*vm=*/%vm19029, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v19050 = vsel /*vm=*/%vm19029, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v19054 = vsel /*vm=*/%vm19029, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v19058 = vsel /*vm=*/%vm19029, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v19062 = vsel /*vm=*/%vm19029, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v19066 = vsel /*vm=*/%vm19029, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v19070 = vadd.f32 -2.5, %v19026
%v19072 = vrsqrt.pop %v19026
%v19073 = vmul.f32 %v19072, %v19026
%vm19074 = vcmp.eq.f32.partialorder %v19026, inf
%v19075 = vsel /*vm=*/%vm19074, /*on_true_vy=*/%v19026, /*on_false_vx=*/%v19073
%vm19076 = vcmp.eq.f32.partialorder %v19026, 0.0
%v19077 = vand.u32 2147483648, %v19026
%v19078 = vsel /*vm=*/%vm19076, /*on_true_vy=*/%v19077, /*on_false_vx=*/%v19075
%v19081 = vadd.f32 -3.0, %v19078
%v19085 = vsel /*vm=*/%vm19029, /*on_true_vy=*/%v19070, /*on_false_vx=*/%v19081
%v19089 = vmul.f32 %v19085, %v19066
%v19093 = vadd.f32 %v19089, %v19062
%v19097 = vmul.f32 %v19093, %v19085
%v19101 = vadd.f32 %v19097, %v19058
%v19105 = vmul.f32 %v19101, %v19085
%v19109 = vadd.f32 %v19105, %v19054
%v19113 = vmul.f32 %v19109, %v19085
%v19117 = vadd.f32 %v19113, %v19050
%v19121 = vmul.f32 %v19117, %v19085
%v19125 = vadd.f32 %v19121, %v19046
%v19129 = vmul.f32 %v19125, %v19085
%v19133 = vadd.f32 %v19129, %v19042
%v19137 = vmul.f32 %v19133, %v19085
%v19141 = vadd.f32 %v19137, %v19038
%v19145 = vmul.f32 %v19141, %v19085
%v19149 = vadd.f32 %v19145, %v19034
%v19153 = vmul.f32 %v19149, %v19000
%v19157 = vsel /*vm=*/%vm19005, /*on_true_vy=*/%v19010, /*on_false_vx=*/%v19153
%v19161 = vmul.f32 1.4140625, %v19157
%v19164 = vpack.c.bf16 %v120417, %v19161
%119843 = vst [vmem:[%s280 + $0x390] sm:$0xf] /*vst_source=*/%v19164
%v19202 = vadd.s32 %v19199, %v408
%v19212 = vadd.s32 %v19202, %v415
%vm19216 = vcmp.lt.u32.totalorder %v19212, %v19202
%vm19221 = vcmp.lt.u32.totalorder %v19202, %v408
%v19226 = vadd.s32 %v19182, %v380
%v19230 = vadd.s32 1, %v19226
%v19234 = vsel /*vm=*/%vm19221, /*on_true_vy=*/%v19230, /*on_false_vx=*/%v19226
%v19238 = vadd.s32 1, %v19234
%v19242 = vsel /*vm=*/%vm19216, /*on_true_vy=*/%v19238, /*on_false_vx=*/%v19234
%v19247 = vadd.s32 %v19242, %v10
%v19251 = vadd.s32 %v19212, %v9
%v19255 = vadd.s32 %v19251, %v19247
%v19257 = vshll.u32 %v19251, 13
%v19258 = vshrl.u32 %v19251, 19
%v19259 = vor.u32 %v19258, %v19257
%v19260 = vxor.u32 %v19259, %v19255
%v19263 = vadd.s32 %v19260, %v19255
%v19265 = vshll.u32 %v19260, 15
%v19266 = vshrl.u32 %v19260, 17
%v19267 = vor.u32 %v19266, %v19265
%v19268 = vxor.u32 %v19267, %v19263
%v19271 = vadd.s32 %v19268, %v19263
%v19273 = vshll.u32 %v19268, 26
%v19274 = vshrl.u32 %v19268, 6
%v19275 = vor.u32 %v19274, %v19273
%v19276 = vxor.u32 %v19275, %v19271
%v19279 = vadd.s32 %v19276, %v19271
%v19283 = vadd.s32 %v19279, %v9
%v19285 = vshll.u32 %v19276, 6
%v19286 = vshrl.u32 %v19276, 26
%v19287 = vor.u32 %v19286, %v19285
%v19288 = vxor.u32 %v19287, %v19279
%v19291 = vadd.s32 %v19288, %v8
%v19295 = vadd.s32 1, %v19291
%v19299 = vadd.s32 %v19295, %v19283
%v19301 = vshll.u32 %v19295, 17
%v19302 = vshrl.u32 %v19295, 15
%v19303 = vor.u32 %v19302, %v19301
%v19304 = vxor.u32 %v19303, %v19299
%v19307 = vadd.s32 %v19304, %v19299
%v19309 = vshll.u32 %v19304, 29
%v19310 = vshrl.u32 %v19304, 3
%v19311 = vor.u32 %v19310, %v19309
%v19312 = vxor.u32 %v19311, %v19307
%v19315 = vadd.s32 %v19312, %v19307
%v19317 = vshll.u32 %v19312, 16
%v19318 = vshrl.u32 %v19312, 16
%v19319 = vor.u32 %v19318, %v19317
%v19320 = vxor.u32 %v19319, %v19315
%v19323 = vadd.s32 %v19320, %v19315
%v19327 = vadd.s32 %v19323, %v8
%v19329 = vshll.u32 %v19320, 24
%v19330 = vshrl.u32 %v19320, 8
%v19331 = vor.u32 %v19330, %v19329
%v19332 = vxor.u32 %v19331, %v19323
%v19335 = vadd.s32 %v19332, %v10
%v19339 = vadd.s32 2, %v19335
%v19343 = vadd.s32 %v19339, %v19327
%v19345 = vshll.u32 %v19339, 13
%v19346 = vshrl.u32 %v19339, 19
%v19347 = vor.u32 %v19346, %v19345
%v19348 = vxor.u32 %v19347, %v19343
%v19351 = vadd.s32 %v19348, %v19343
%v19353 = vshll.u32 %v19348, 15
%v19354 = vshrl.u32 %v19348, 17
%v19355 = vor.u32 %v19354, %v19353
%v19356 = vxor.u32 %v19355, %v19351
%v19359 = vadd.s32 %v19356, %v19351
%v19361 = vshll.u32 %v19356, 26
%v19362 = vshrl.u32 %v19356, 6
%v19363 = vor.u32 %v19362, %v19361
%v19364 = vxor.u32 %v19363, %v19359
%v19367 = vadd.s32 %v19364, %v19359
%v19371 = vadd.s32 %v19367, %v10
%v19373 = vshll.u32 %v19364, 6
%v19374 = vshrl.u32 %v19364, 26
%v19375 = vor.u32 %v19374, %v19373
%v19376 = vxor.u32 %v19375, %v19367
%v19379 = vadd.s32 %v19376, %v9
%v19383 = vadd.s32 3, %v19379
%v19387 = vadd.s32 %v19383, %v19371
%v19389 = vshll.u32 %v19383, 17
%v19390 = vshrl.u32 %v19383, 15
%v19391 = vor.u32 %v19390, %v19389
%v19392 = vxor.u32 %v19391, %v19387
%v19395 = vadd.s32 %v19392, %v19387
%v19397 = vshll.u32 %v19392, 29
%v19398 = vshrl.u32 %v19392, 3
%v19399 = vor.u32 %v19398, %v19397
%v19400 = vxor.u32 %v19399, %v19395
%v19403 = vadd.s32 %v19400, %v19395
%v19405 = vshll.u32 %v19400, 16
%v19406 = vshrl.u32 %v19400, 16
%v19407 = vor.u32 %v19406, %v19405
%v19408 = vxor.u32 %v19407, %v19403
%v19411 = vadd.s32 %v19408, %v19403
%v19415 = vadd.s32 %v19411, %v9
%v19417 = vshll.u32 %v19408, 24
%v19418 = vshrl.u32 %v19408, 8
%v19419 = vor.u32 %v19418, %v19417
%v19420 = vxor.u32 %v19419, %v19411
%v19423 = vadd.s32 %v19420, %v8
%v19427 = vadd.s32 4, %v19423
%v19431 = vadd.s32 %v19427, %v19415
%v19433 = vshll.u32 %v19427, 13
%v19434 = vshrl.u32 %v19427, 19
%v19435 = vor.u32 %v19434, %v19433
%v19436 = vxor.u32 %v19435, %v19431
%v19439 = vadd.s32 %v19436, %v19431
%v19441 = vshll.u32 %v19436, 15
%v19442 = vshrl.u32 %v19436, 17
%v19443 = vor.u32 %v19442, %v19441
%v19444 = vxor.u32 %v19443, %v19439
%v19447 = vadd.s32 %v19444, %v19439
%v19449 = vshll.u32 %v19444, 26
%v19450 = vshrl.u32 %v19444, 6
%v19451 = vor.u32 %v19450, %v19449
%v19452 = vxor.u32 %v19451, %v19447
%v19455 = vadd.s32 %v19452, %v19447
%v19459 = vadd.s32 %v19455, %v8
%v19461 = vshll.u32 %v19452, 6
%v19462 = vshrl.u32 %v19452, 26
%v19463 = vor.u32 %v19462, %v19461
%v19464 = vxor.u32 %v19463, %v19455
%v19467 = vadd.s32 %v19464, %v10
%v19471 = vadd.s32 5, %v19467
%v19473 = vxor.u32 %v19471, %v19459
%v19474 = vand.u32.u8 255, %v19473
%v19475 = vand.u32 65535, %v19474
%v19476 = vshrl.u32 %v19475, 1
%v19477 = vor.u32 16256, %v19476
%v19478 = vand.u32.u16 65535, %v19477
%v119848 = vadd.low.f32.bf16 -1.0, %v19478
%v19487 = vmul.f32 2.0, %v119848
%v19491 = vadd.f32 -0.99609375, %v19487
%v19495 = vmax.f32 %v19491, -0.99609375
%v19497 = vand.u32 2147483647, %v19495
%vm19500 = vcmp.eq.f32.partialorder %v19497, 1.0
%v19505 = vmul.f32 inf, %v19495
%v19507 = vxor.u32 2147483648, %v19495
%v19510 = vmul.f32 %v19507, %v19495
%v19512 = vadd.f32 1.0, %v19510
%v19513 = vlog2.pop %v19512
%v19514 = vmul.f32 0.6931472, %v19513
%v19515 = vmul.f32 -0.5, %v19510
%v19516 = vadd.f32 1.0, %v19515
%v19517 = vmul.f32 %v19516, %v19510
%v19518 = vand.u32 2147483647, %v19510
%vm19519 = vcmp.lt.f32.partialorder %v19518, 0.0004427343
%v19520 = vsel /*vm=*/%vm19519, /*on_true_vy=*/%v19517, /*on_false_vx=*/%v19514
%v19521 = vxor.u32 2147483648, %v19520
%vm19524 = vcmp.lt.f32.partialorder %v19521, 5.0
%v19529 = vsel /*vm=*/%vm19524, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v19533 = vsel /*vm=*/%vm19524, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v19537 = vsel /*vm=*/%vm19524, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v19541 = vsel /*vm=*/%vm19524, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v19545 = vsel /*vm=*/%vm19524, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v19549 = vsel /*vm=*/%vm19524, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v19553 = vsel /*vm=*/%vm19524, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v19557 = vsel /*vm=*/%vm19524, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v19561 = vsel /*vm=*/%vm19524, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v19565 = vadd.f32 -2.5, %v19521
%v19567 = vrsqrt.pop %v19521
%v19568 = vmul.f32 %v19567, %v19521
%vm19569 = vcmp.eq.f32.partialorder %v19521, inf
%v19570 = vsel /*vm=*/%vm19569, /*on_true_vy=*/%v19521, /*on_false_vx=*/%v19568
%vm19571 = vcmp.eq.f32.partialorder %v19521, 0.0
%v19572 = vand.u32 2147483648, %v19521
%v19573 = vsel /*vm=*/%vm19571, /*on_true_vy=*/%v19572, /*on_false_vx=*/%v19570
%v19576 = vadd.f32 -3.0, %v19573
%v19580 = vsel /*vm=*/%vm19524, /*on_true_vy=*/%v19565, /*on_false_vx=*/%v19576
%v19584 = vmul.f32 %v19580, %v19561
%v19588 = vadd.f32 %v19584, %v19557
%v19592 = vmul.f32 %v19588, %v19580
%v19596 = vadd.f32 %v19592, %v19553
%v19600 = vmul.f32 %v19596, %v19580
%v19604 = vadd.f32 %v19600, %v19549
%v19608 = vmul.f32 %v19604, %v19580
%v19612 = vadd.f32 %v19608, %v19545
%v19616 = vmul.f32 %v19612, %v19580
%v19620 = vadd.f32 %v19616, %v19541
%v19624 = vmul.f32 %v19620, %v19580
%v19628 = vadd.f32 %v19624, %v19537
%v19632 = vmul.f32 %v19628, %v19580
%v19636 = vadd.f32 %v19632, %v19533
%v19640 = vmul.f32 %v19636, %v19580
%v19644 = vadd.f32 %v19640, %v19529
%v19648 = vmul.f32 %v19644, %v19495
%v19652 = vsel /*vm=*/%vm19500, /*on_true_vy=*/%v19505, /*on_false_vx=*/%v19648
%v19656 = vmul.f32 1.4140625, %v19652
%v19659 = vpack.c.bf16 %v120417, %v19656
%119849 = vst [vmem:[%s280 + $0x14] sm:$0xf] /*vst_source=*/%v19659
%v19663 = vadd.s32 %v19199, %v894
%v19673 = vadd.s32 %v19663, %v415
%vm19677 = vcmp.lt.u32.totalorder %v19673, %v19663
%vm19682 = vcmp.lt.u32.totalorder %v19663, %v894
%v19687 = vadd.s32 %v19182, %v881
%v19691 = vadd.s32 1, %v19687
%v19695 = vsel /*vm=*/%vm19682, /*on_true_vy=*/%v19691, /*on_false_vx=*/%v19687
%v19699 = vadd.s32 1, %v19695
%v19703 = vsel /*vm=*/%vm19677, /*on_true_vy=*/%v19699, /*on_false_vx=*/%v19695
%v19708 = vadd.s32 %v19703, %v10
%v19712 = vadd.s32 %v19673, %v9
%v19716 = vadd.s32 %v19712, %v19708
%v19718 = vshll.u32 %v19712, 13
%v19719 = vshrl.u32 %v19712, 19
%v19720 = vor.u32 %v19719, %v19718
%v19721 = vxor.u32 %v19720, %v19716
%v19724 = vadd.s32 %v19721, %v19716
%v19726 = vshll.u32 %v19721, 15
%v19727 = vshrl.u32 %v19721, 17
%v19728 = vor.u32 %v19727, %v19726
%v19729 = vxor.u32 %v19728, %v19724
%v19732 = vadd.s32 %v19729, %v19724
%v19734 = vshll.u32 %v19729, 26
%v19735 = vshrl.u32 %v19729, 6
%v19736 = vor.u32 %v19735, %v19734
%v19737 = vxor.u32 %v19736, %v19732
%v19740 = vadd.s32 %v19737, %v19732
%v19744 = vadd.s32 %v19740, %v9
%v19746 = vshll.u32 %v19737, 6
%v19747 = vshrl.u32 %v19737, 26
%v19748 = vor.u32 %v19747, %v19746
%v19749 = vxor.u32 %v19748, %v19740
%v19752 = vadd.s32 %v19749, %v8
%v19756 = vadd.s32 1, %v19752
%v19760 = vadd.s32 %v19756, %v19744
%v19762 = vshll.u32 %v19756, 17
%v19763 = vshrl.u32 %v19756, 15
%v19764 = vor.u32 %v19763, %v19762
%v19765 = vxor.u32 %v19764, %v19760
%v19768 = vadd.s32 %v19765, %v19760
%v19770 = vshll.u32 %v19765, 29
%v19771 = vshrl.u32 %v19765, 3
%v19772 = vor.u32 %v19771, %v19770
%v19773 = vxor.u32 %v19772, %v19768
%v19776 = vadd.s32 %v19773, %v19768
%v19778 = vshll.u32 %v19773, 16
%v19779 = vshrl.u32 %v19773, 16
%v19780 = vor.u32 %v19779, %v19778
%v19781 = vxor.u32 %v19780, %v19776
%v19784 = vadd.s32 %v19781, %v19776
%v19788 = vadd.s32 %v19784, %v8
%v19790 = vshll.u32 %v19781, 24
%v19791 = vshrl.u32 %v19781, 8
%v19792 = vor.u32 %v19791, %v19790
%v19793 = vxor.u32 %v19792, %v19784
%v19796 = vadd.s32 %v19793, %v10
%v19800 = vadd.s32 2, %v19796
%v19804 = vadd.s32 %v19800, %v19788
%v19806 = vshll.u32 %v19800, 13
%v19807 = vshrl.u32 %v19800, 19
%v19808 = vor.u32 %v19807, %v19806
%v19809 = vxor.u32 %v19808, %v19804
%v19812 = vadd.s32 %v19809, %v19804
%v19814 = vshll.u32 %v19809, 15
%v19815 = vshrl.u32 %v19809, 17
%v19816 = vor.u32 %v19815, %v19814
%v19817 = vxor.u32 %v19816, %v19812
%v19820 = vadd.s32 %v19817, %v19812
%v19822 = vshll.u32 %v19817, 26
%v19823 = vshrl.u32 %v19817, 6
%v19824 = vor.u32 %v19823, %v19822
%v19825 = vxor.u32 %v19824, %v19820
%v19828 = vadd.s32 %v19825, %v19820
%v19832 = vadd.s32 %v19828, %v10
%v19834 = vshll.u32 %v19825, 6
%v19835 = vshrl.u32 %v19825, 26
%v19836 = vor.u32 %v19835, %v19834
%v19837 = vxor.u32 %v19836, %v19828
%v19840 = vadd.s32 %v19837, %v9
%v19844 = vadd.s32 3, %v19840
%v19848 = vadd.s32 %v19844, %v19832
%v19850 = vshll.u32 %v19844, 17
%v19851 = vshrl.u32 %v19844, 15
%v19852 = vor.u32 %v19851, %v19850
%v19853 = vxor.u32 %v19852, %v19848
%v19856 = vadd.s32 %v19853, %v19848
%v19858 = vshll.u32 %v19853, 29
%v19859 = vshrl.u32 %v19853, 3
%v19860 = vor.u32 %v19859, %v19858
%v19861 = vxor.u32 %v19860, %v19856
%v19864 = vadd.s32 %v19861, %v19856
%v19866 = vshll.u32 %v19861, 16
%v19867 = vshrl.u32 %v19861, 16
%v19868 = vor.u32 %v19867, %v19866
%v19869 = vxor.u32 %v19868, %v19864
%v19872 = vadd.s32 %v19869, %v19864
%v19876 = vadd.s32 %v19872, %v9
%v19878 = vshll.u32 %v19869, 24
%v19879 = vshrl.u32 %v19869, 8
%v19880 = vor.u32 %v19879, %v19878
%v19881 = vxor.u32 %v19880, %v19872
%v19884 = vadd.s32 %v19881, %v8
%v19888 = vadd.s32 4, %v19884
%v19892 = vadd.s32 %v19888, %v19876
%v19894 = vshll.u32 %v19888, 13
%v19895 = vshrl.u32 %v19888, 19
%v19896 = vor.u32 %v19895, %v19894
%v19897 = vxor.u32 %v19896, %v19892
%v19900 = vadd.s32 %v19897, %v19892
%v19902 = vshll.u32 %v19897, 15
%v19903 = vshrl.u32 %v19897, 17
%v19904 = vor.u32 %v19903, %v19902
%v19905 = vxor.u32 %v19904, %v19900
%v19908 = vadd.s32 %v19905, %v19900
%v19910 = vshll.u32 %v19905, 26
%v19911 = vshrl.u32 %v19905, 6
%v19912 = vor.u32 %v19911, %v19910
%v19913 = vxor.u32 %v19912, %v19908
%v19916 = vadd.s32 %v19913, %v19908
%v19920 = vadd.s32 %v19916, %v8
%v19922 = vshll.u32 %v19913, 6
%v19923 = vshrl.u32 %v19913, 26
%v19924 = vor.u32 %v19923, %v19922
%v19925 = vxor.u32 %v19924, %v19916
%v19928 = vadd.s32 %v19925, %v10
%v19932 = vadd.s32 5, %v19928
%v19934 = vxor.u32 %v19932, %v19920
%v19935 = vand.u32.u8 255, %v19934
%v19936 = vand.u32 65535, %v19935
%v19937 = vshrl.u32 %v19936, 1
%v19938 = vor.u32 16256, %v19937
%v19939 = vand.u32.u16 65535, %v19938
%v119850 = vadd.low.f32.bf16 -1.0, %v19939
%v19948 = vmul.f32 2.0, %v119850
%v19952 = vadd.f32 -0.99609375, %v19948
%v19956 = vmax.f32 %v19952, -0.99609375
%v19958 = vand.u32 2147483647, %v19956
%vm19961 = vcmp.eq.f32.partialorder %v19958, 1.0
%v19966 = vmul.f32 inf, %v19956
%v19968 = vxor.u32 2147483648, %v19956
%v19971 = vmul.f32 %v19968, %v19956
%v19973 = vadd.f32 1.0, %v19971
%v19974 = vlog2.pop %v19973
%v19975 = vmul.f32 0.6931472, %v19974
%v19976 = vmul.f32 -0.5, %v19971
%v19977 = vadd.f32 1.0, %v19976
%v19978 = vmul.f32 %v19977, %v19971
%v19979 = vand.u32 2147483647, %v19971
%vm19980 = vcmp.lt.f32.partialorder %v19979, 0.0004427343
%v19981 = vsel /*vm=*/%vm19980, /*on_true_vy=*/%v19978, /*on_false_vx=*/%v19975
%v19982 = vxor.u32 2147483648, %v19981
%vm19985 = vcmp.lt.f32.partialorder %v19982, 5.0
%v19990 = vsel /*vm=*/%vm19985, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v19994 = vsel /*vm=*/%vm19985, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v19998 = vsel /*vm=*/%vm19985, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v20002 = vsel /*vm=*/%vm19985, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v20006 = vsel /*vm=*/%vm19985, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v20010 = vsel /*vm=*/%vm19985, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v20014 = vsel /*vm=*/%vm19985, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v20018 = vsel /*vm=*/%vm19985, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v20022 = vsel /*vm=*/%vm19985, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v20026 = vadd.f32 -2.5, %v19982
%v20028 = vrsqrt.pop %v19982
%v20029 = vmul.f32 %v20028, %v19982
%vm20030 = vcmp.eq.f32.partialorder %v19982, inf
%v20031 = vsel /*vm=*/%vm20030, /*on_true_vy=*/%v19982, /*on_false_vx=*/%v20029
%vm20032 = vcmp.eq.f32.partialorder %v19982, 0.0
%v20033 = vand.u32 2147483648, %v19982
%v20034 = vsel /*vm=*/%vm20032, /*on_true_vy=*/%v20033, /*on_false_vx=*/%v20031
%v20037 = vadd.f32 -3.0, %v20034
%v20041 = vsel /*vm=*/%vm19985, /*on_true_vy=*/%v20026, /*on_false_vx=*/%v20037
%v20045 = vmul.f32 %v20041, %v20022
%v20049 = vadd.f32 %v20045, %v20018
%v20053 = vmul.f32 %v20049, %v20041
%v20057 = vadd.f32 %v20053, %v20014
%v20061 = vmul.f32 %v20057, %v20041
%v20065 = vadd.f32 %v20061, %v20010
%v20069 = vmul.f32 %v20065, %v20041
%v20073 = vadd.f32 %v20069, %v20006
%v20077 = vmul.f32 %v20073, %v20041
%v20081 = vadd.f32 %v20077, %v20002
%v20085 = vmul.f32 %v20081, %v20041
%v20089 = vadd.f32 %v20085, %v19998
%v20093 = vmul.f32 %v20089, %v20041
%v20097 = vadd.f32 %v20093, %v19994
%v20101 = vmul.f32 %v20097, %v20041
%v20105 = vadd.f32 %v20101, %v19990
%v20109 = vmul.f32 %v20105, %v19956
%v20113 = vsel /*vm=*/%vm19961, /*on_true_vy=*/%v19966, /*on_false_vx=*/%v20109
%v20117 = vmul.f32 1.4140625, %v20113
%v20120 = vpack.c.bf16 %v120417, %v20117
%119851 = vst [vmem:[%s280 + $0x94] sm:$0xf] /*vst_source=*/%v20120
%v20124 = vadd.s32 %v19199, %v1381
%v20134 = vadd.s32 %v20124, %v415
%vm20138 = vcmp.lt.u32.totalorder %v20134, %v20124
%vm20143 = vcmp.lt.u32.totalorder %v20124, %v1381
%v20148 = vadd.s32 %v19182, %v1368
%v20152 = vadd.s32 1, %v20148
%v20156 = vsel /*vm=*/%vm20143, /*on_true_vy=*/%v20152, /*on_false_vx=*/%v20148
%v20160 = vadd.s32 1, %v20156
%v20164 = vsel /*vm=*/%vm20138, /*on_true_vy=*/%v20160, /*on_false_vx=*/%v20156
%v20169 = vadd.s32 %v20164, %v10
%v20173 = vadd.s32 %v20134, %v9
%v20177 = vadd.s32 %v20173, %v20169
%v20179 = vshll.u32 %v20173, 13
%v20180 = vshrl.u32 %v20173, 19
%v20181 = vor.u32 %v20180, %v20179
%v20182 = vxor.u32 %v20181, %v20177
%v20185 = vadd.s32 %v20182, %v20177
%v20187 = vshll.u32 %v20182, 15
%v20188 = vshrl.u32 %v20182, 17
%v20189 = vor.u32 %v20188, %v20187
%v20190 = vxor.u32 %v20189, %v20185
%v20193 = vadd.s32 %v20190, %v20185
%v20195 = vshll.u32 %v20190, 26
%v20196 = vshrl.u32 %v20190, 6
%v20197 = vor.u32 %v20196, %v20195
%v20198 = vxor.u32 %v20197, %v20193
%v20201 = vadd.s32 %v20198, %v20193
%v20205 = vadd.s32 %v20201, %v9
%v20207 = vshll.u32 %v20198, 6
%v20208 = vshrl.u32 %v20198, 26
%v20209 = vor.u32 %v20208, %v20207
%v20210 = vxor.u32 %v20209, %v20201
%v20213 = vadd.s32 %v20210, %v8
%v20217 = vadd.s32 1, %v20213
%v20221 = vadd.s32 %v20217, %v20205
%v20223 = vshll.u32 %v20217, 17
%v20224 = vshrl.u32 %v20217, 15
%v20225 = vor.u32 %v20224, %v20223
%v20226 = vxor.u32 %v20225, %v20221
%v20229 = vadd.s32 %v20226, %v20221
%v20231 = vshll.u32 %v20226, 29
%v20232 = vshrl.u32 %v20226, 3
%v20233 = vor.u32 %v20232, %v20231
%v20234 = vxor.u32 %v20233, %v20229
%v20237 = vadd.s32 %v20234, %v20229
%v20239 = vshll.u32 %v20234, 16
%v20240 = vshrl.u32 %v20234, 16
%v20241 = vor.u32 %v20240, %v20239
%v20242 = vxor.u32 %v20241, %v20237
%v20245 = vadd.s32 %v20242, %v20237
%v20249 = vadd.s32 %v20245, %v8
%v20251 = vshll.u32 %v20242, 24
%v20252 = vshrl.u32 %v20242, 8
%v20253 = vor.u32 %v20252, %v20251
%v20254 = vxor.u32 %v20253, %v20245
%v20257 = vadd.s32 %v20254, %v10
%v20261 = vadd.s32 2, %v20257
%v20265 = vadd.s32 %v20261, %v20249
%v20267 = vshll.u32 %v20261, 13
%v20268 = vshrl.u32 %v20261, 19
%v20269 = vor.u32 %v20268, %v20267
%v20270 = vxor.u32 %v20269, %v20265
%v20273 = vadd.s32 %v20270, %v20265
%v20275 = vshll.u32 %v20270, 15
%v20276 = vshrl.u32 %v20270, 17
%v20277 = vor.u32 %v20276, %v20275
%v20278 = vxor.u32 %v20277, %v20273
%v20281 = vadd.s32 %v20278, %v20273
%v20283 = vshll.u32 %v20278, 26
%v20284 = vshrl.u32 %v20278, 6
%v20285 = vor.u32 %v20284, %v20283
%v20286 = vxor.u32 %v20285, %v20281
%v20289 = vadd.s32 %v20286, %v20281
%v20293 = vadd.s32 %v20289, %v10
%v20295 = vshll.u32 %v20286, 6
%v20296 = vshrl.u32 %v20286, 26
%v20297 = vor.u32 %v20296, %v20295
%v20298 = vxor.u32 %v20297, %v20289
%v20301 = vadd.s32 %v20298, %v9
%v20305 = vadd.s32 3, %v20301
%v20309 = vadd.s32 %v20305, %v20293
%v20311 = vshll.u32 %v20305, 17
%v20312 = vshrl.u32 %v20305, 15
%v20313 = vor.u32 %v20312, %v20311
%v20314 = vxor.u32 %v20313, %v20309
%v20317 = vadd.s32 %v20314, %v20309
%v20319 = vshll.u32 %v20314, 29
%v20320 = vshrl.u32 %v20314, 3
%v20321 = vor.u32 %v20320, %v20319
%v20322 = vxor.u32 %v20321, %v20317
%v20325 = vadd.s32 %v20322, %v20317
%v20327 = vshll.u32 %v20322, 16
%v20328 = vshrl.u32 %v20322, 16
%v20329 = vor.u32 %v20328, %v20327
%v20330 = vxor.u32 %v20329, %v20325
%v20333 = vadd.s32 %v20330, %v20325
%v20337 = vadd.s32 %v20333, %v9
%v20339 = vshll.u32 %v20330, 24
%v20340 = vshrl.u32 %v20330, 8
%v20341 = vor.u32 %v20340, %v20339
%v20342 = vxor.u32 %v20341, %v20333
%v20345 = vadd.s32 %v20342, %v8
%v20349 = vadd.s32 4, %v20345
%v20353 = vadd.s32 %v20349, %v20337
%v20355 = vshll.u32 %v20349, 13
%v20356 = vshrl.u32 %v20349, 19
%v20357 = vor.u32 %v20356, %v20355
%v20358 = vxor.u32 %v20357, %v20353
%v20361 = vadd.s32 %v20358, %v20353
%v20363 = vshll.u32 %v20358, 15
%v20364 = vshrl.u32 %v20358, 17
%v20365 = vor.u32 %v20364, %v20363
%v20366 = vxor.u32 %v20365, %v20361
%v20369 = vadd.s32 %v20366, %v20361
%v20371 = vshll.u32 %v20366, 26
%v20372 = vshrl.u32 %v20366, 6
%v20373 = vor.u32 %v20372, %v20371
%v20374 = vxor.u32 %v20373, %v20369
%v20377 = vadd.s32 %v20374, %v20369
%v20381 = vadd.s32 %v20377, %v8
%v20383 = vshll.u32 %v20374, 6
%v20384 = vshrl.u32 %v20374, 26
%v20385 = vor.u32 %v20384, %v20383
%v20386 = vxor.u32 %v20385, %v20377
%v20389 = vadd.s32 %v20386, %v10
%v20393 = vadd.s32 5, %v20389
%v20395 = vxor.u32 %v20393, %v20381
%v20396 = vand.u32.u8 255, %v20395
%v20397 = vand.u32 65535, %v20396
%v20398 = vshrl.u32 %v20397, 1
%v20399 = vor.u32 16256, %v20398
%v20400 = vand.u32.u16 65535, %v20399
%v119852 = vadd.low.f32.bf16 -1.0, %v20400
%v20409 = vmul.f32 2.0, %v119852
%v20413 = vadd.f32 -0.99609375, %v20409
%v20417 = vmax.f32 %v20413, -0.99609375
%v20419 = vand.u32 2147483647, %v20417
%vm20422 = vcmp.eq.f32.partialorder %v20419, 1.0
%v20427 = vmul.f32 inf, %v20417
%v20429 = vxor.u32 2147483648, %v20417
%v20432 = vmul.f32 %v20429, %v20417
%v20434 = vadd.f32 1.0, %v20432
%v20435 = vlog2.pop %v20434
%v20436 = vmul.f32 0.6931472, %v20435
%v20437 = vmul.f32 -0.5, %v20432
%v20438 = vadd.f32 1.0, %v20437
%v20439 = vmul.f32 %v20438, %v20432
%v20440 = vand.u32 2147483647, %v20432
%vm20441 = vcmp.lt.f32.partialorder %v20440, 0.0004427343
%v20442 = vsel /*vm=*/%vm20441, /*on_true_vy=*/%v20439, /*on_false_vx=*/%v20436
%v20443 = vxor.u32 2147483648, %v20442
%vm20446 = vcmp.lt.f32.partialorder %v20443, 5.0
%v20451 = vsel /*vm=*/%vm20446, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v20455 = vsel /*vm=*/%vm20446, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v20459 = vsel /*vm=*/%vm20446, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v20463 = vsel /*vm=*/%vm20446, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v20467 = vsel /*vm=*/%vm20446, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v20471 = vsel /*vm=*/%vm20446, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v20475 = vsel /*vm=*/%vm20446, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v20479 = vsel /*vm=*/%vm20446, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v20483 = vsel /*vm=*/%vm20446, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v20487 = vadd.f32 -2.5, %v20443
%v20489 = vrsqrt.pop %v20443
%v20490 = vmul.f32 %v20489, %v20443
%vm20491 = vcmp.eq.f32.partialorder %v20443, inf
%v20492 = vsel /*vm=*/%vm20491, /*on_true_vy=*/%v20443, /*on_false_vx=*/%v20490
%vm20493 = vcmp.eq.f32.partialorder %v20443, 0.0
%v20494 = vand.u32 2147483648, %v20443
%v20495 = vsel /*vm=*/%vm20493, /*on_true_vy=*/%v20494, /*on_false_vx=*/%v20492
%v20498 = vadd.f32 -3.0, %v20495
%v20502 = vsel /*vm=*/%vm20446, /*on_true_vy=*/%v20487, /*on_false_vx=*/%v20498
%v20506 = vmul.f32 %v20502, %v20483
%v20510 = vadd.f32 %v20506, %v20479
%v20514 = vmul.f32 %v20510, %v20502
%v20518 = vadd.f32 %v20514, %v20475
%v20522 = vmul.f32 %v20518, %v20502
%v20526 = vadd.f32 %v20522, %v20471
%v20530 = vmul.f32 %v20526, %v20502
%v20534 = vadd.f32 %v20530, %v20467
%v20538 = vmul.f32 %v20534, %v20502
%v20542 = vadd.f32 %v20538, %v20463
%v20546 = vmul.f32 %v20542, %v20502
%v20550 = vadd.f32 %v20546, %v20459
%v20554 = vmul.f32 %v20550, %v20502
%v20558 = vadd.f32 %v20554, %v20455
%v20562 = vmul.f32 %v20558, %v20502
%v20566 = vadd.f32 %v20562, %v20451
%v20570 = vmul.f32 %v20566, %v20417
%v20574 = vsel /*vm=*/%vm20422, /*on_true_vy=*/%v20427, /*on_false_vx=*/%v20570
%v20578 = vmul.f32 1.4140625, %v20574
%v20581 = vpack.c.bf16 %v120417, %v20578
%119853 = vst [vmem:[%s280 + $0x114] sm:$0xf] /*vst_source=*/%v20581
%v20585 = vadd.s32 %v19199, %v1868
%v20595 = vadd.s32 %v20585, %v415
%vm20599 = vcmp.lt.u32.totalorder %v20595, %v20585
%vm20604 = vcmp.lt.u32.totalorder %v20585, %v1868
%v20609 = vadd.s32 %v19182, %v1855
%v20613 = vadd.s32 1, %v20609
%v20617 = vsel /*vm=*/%vm20604, /*on_true_vy=*/%v20613, /*on_false_vx=*/%v20609
%v20621 = vadd.s32 1, %v20617
%v20625 = vsel /*vm=*/%vm20599, /*on_true_vy=*/%v20621, /*on_false_vx=*/%v20617
%v20630 = vadd.s32 %v20625, %v10
%v20634 = vadd.s32 %v20595, %v9
%v20638 = vadd.s32 %v20634, %v20630
%v20640 = vshll.u32 %v20634, 13
%v20641 = vshrl.u32 %v20634, 19
%v20642 = vor.u32 %v20641, %v20640
%v20643 = vxor.u32 %v20642, %v20638
%v20646 = vadd.s32 %v20643, %v20638
%v20648 = vshll.u32 %v20643, 15
%v20649 = vshrl.u32 %v20643, 17
%v20650 = vor.u32 %v20649, %v20648
%v20651 = vxor.u32 %v20650, %v20646
%v20654 = vadd.s32 %v20651, %v20646
%v20656 = vshll.u32 %v20651, 26
%v20657 = vshrl.u32 %v20651, 6
%v20658 = vor.u32 %v20657, %v20656
%v20659 = vxor.u32 %v20658, %v20654
%v20662 = vadd.s32 %v20659, %v20654
%v20666 = vadd.s32 %v20662, %v9
%v20668 = vshll.u32 %v20659, 6
%v20669 = vshrl.u32 %v20659, 26
%v20670 = vor.u32 %v20669, %v20668
%v20671 = vxor.u32 %v20670, %v20662
%v20674 = vadd.s32 %v20671, %v8
%v20678 = vadd.s32 1, %v20674
%v20682 = vadd.s32 %v20678, %v20666
%v20684 = vshll.u32 %v20678, 17
%v20685 = vshrl.u32 %v20678, 15
%v20686 = vor.u32 %v20685, %v20684
%v20687 = vxor.u32 %v20686, %v20682
%v20690 = vadd.s32 %v20687, %v20682
%v20692 = vshll.u32 %v20687, 29
%v20693 = vshrl.u32 %v20687, 3
%v20694 = vor.u32 %v20693, %v20692
%v20695 = vxor.u32 %v20694, %v20690
%v20698 = vadd.s32 %v20695, %v20690
%v20700 = vshll.u32 %v20695, 16
%v20701 = vshrl.u32 %v20695, 16
%v20702 = vor.u32 %v20701, %v20700
%v20703 = vxor.u32 %v20702, %v20698
%v20706 = vadd.s32 %v20703, %v20698
%v20710 = vadd.s32 %v20706, %v8
%v20712 = vshll.u32 %v20703, 24
%v20713 = vshrl.u32 %v20703, 8
%v20714 = vor.u32 %v20713, %v20712
%v20715 = vxor.u32 %v20714, %v20706
%v20718 = vadd.s32 %v20715, %v10
%v20722 = vadd.s32 2, %v20718
%v20726 = vadd.s32 %v20722, %v20710
%v20728 = vshll.u32 %v20722, 13
%v20729 = vshrl.u32 %v20722, 19
%v20730 = vor.u32 %v20729, %v20728
%v20731 = vxor.u32 %v20730, %v20726
%v20734 = vadd.s32 %v20731, %v20726
%v20736 = vshll.u32 %v20731, 15
%v20737 = vshrl.u32 %v20731, 17
%v20738 = vor.u32 %v20737, %v20736
%v20739 = vxor.u32 %v20738, %v20734
%v20742 = vadd.s32 %v20739, %v20734
%v20744 = vshll.u32 %v20739, 26
%v20745 = vshrl.u32 %v20739, 6
%v20746 = vor.u32 %v20745, %v20744
%v20747 = vxor.u32 %v20746, %v20742
%v20750 = vadd.s32 %v20747, %v20742
%v20754 = vadd.s32 %v20750, %v10
%v20756 = vshll.u32 %v20747, 6
%v20757 = vshrl.u32 %v20747, 26
%v20758 = vor.u32 %v20757, %v20756
%v20759 = vxor.u32 %v20758, %v20750
%v20762 = vadd.s32 %v20759, %v9
%v20766 = vadd.s32 3, %v20762
%v20770 = vadd.s32 %v20766, %v20754
%v20772 = vshll.u32 %v20766, 17
%v20773 = vshrl.u32 %v20766, 15
%v20774 = vor.u32 %v20773, %v20772
%v20775 = vxor.u32 %v20774, %v20770
%v20778 = vadd.s32 %v20775, %v20770
%v20780 = vshll.u32 %v20775, 29
%v20781 = vshrl.u32 %v20775, 3
%v20782 = vor.u32 %v20781, %v20780
%v20783 = vxor.u32 %v20782, %v20778
%v20786 = vadd.s32 %v20783, %v20778
%v20788 = vshll.u32 %v20783, 16
%v20789 = vshrl.u32 %v20783, 16
%v20790 = vor.u32 %v20789, %v20788
%v20791 = vxor.u32 %v20790, %v20786
%v20794 = vadd.s32 %v20791, %v20786
%v20798 = vadd.s32 %v20794, %v9
%v20800 = vshll.u32 %v20791, 24
%v20801 = vshrl.u32 %v20791, 8
%v20802 = vor.u32 %v20801, %v20800
%v20803 = vxor.u32 %v20802, %v20794
%v20806 = vadd.s32 %v20803, %v8
%v20810 = vadd.s32 4, %v20806
%v20814 = vadd.s32 %v20810, %v20798
%v20816 = vshll.u32 %v20810, 13
%v20817 = vshrl.u32 %v20810, 19
%v20818 = vor.u32 %v20817, %v20816
%v20819 = vxor.u32 %v20818, %v20814
%v20822 = vadd.s32 %v20819, %v20814
%v20824 = vshll.u32 %v20819, 15
%v20825 = vshrl.u32 %v20819, 17
%v20826 = vor.u32 %v20825, %v20824
%v20827 = vxor.u32 %v20826, %v20822
%v20830 = vadd.s32 %v20827, %v20822
%v20832 = vshll.u32 %v20827, 26
%v20833 = vshrl.u32 %v20827, 6
%v20834 = vor.u32 %v20833, %v20832
%v20835 = vxor.u32 %v20834, %v20830
%v20838 = vadd.s32 %v20835, %v20830
%v20842 = vadd.s32 %v20838, %v8
%v20844 = vshll.u32 %v20835, 6
%v20845 = vshrl.u32 %v20835, 26
%v20846 = vor.u32 %v20845, %v20844
%v20847 = vxor.u32 %v20846, %v20838
%v20850 = vadd.s32 %v20847, %v10
%v20854 = vadd.s32 5, %v20850
%v20856 = vxor.u32 %v20854, %v20842
%v20857 = vand.u32.u8 255, %v20856
%v20858 = vand.u32 65535, %v20857
%v20859 = vshrl.u32 %v20858, 1
%v20860 = vor.u32 16256, %v20859
%v20861 = vand.u32.u16 65535, %v20860
%v119854 = vadd.low.f32.bf16 -1.0, %v20861
%v20870 = vmul.f32 2.0, %v119854
%v20874 = vadd.f32 -0.99609375, %v20870
%v20878 = vmax.f32 %v20874, -0.99609375
%v20880 = vand.u32 2147483647, %v20878
%vm20883 = vcmp.eq.f32.partialorder %v20880, 1.0
%v20888 = vmul.f32 inf, %v20878
%v20890 = vxor.u32 2147483648, %v20878
%v20893 = vmul.f32 %v20890, %v20878
%v20895 = vadd.f32 1.0, %v20893
%v20896 = vlog2.pop %v20895
%v20897 = vmul.f32 0.6931472, %v20896
%v20898 = vmul.f32 -0.5, %v20893
%v20899 = vadd.f32 1.0, %v20898
%v20900 = vmul.f32 %v20899, %v20893
%v20901 = vand.u32 2147483647, %v20893
%vm20902 = vcmp.lt.f32.partialorder %v20901, 0.0004427343
%v20903 = vsel /*vm=*/%vm20902, /*on_true_vy=*/%v20900, /*on_false_vx=*/%v20897
%v20904 = vxor.u32 2147483648, %v20903
%vm20907 = vcmp.lt.f32.partialorder %v20904, 5.0
%v20912 = vsel /*vm=*/%vm20907, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v20916 = vsel /*vm=*/%vm20907, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v20920 = vsel /*vm=*/%vm20907, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v20924 = vsel /*vm=*/%vm20907, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v20928 = vsel /*vm=*/%vm20907, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v20932 = vsel /*vm=*/%vm20907, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v20936 = vsel /*vm=*/%vm20907, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v20940 = vsel /*vm=*/%vm20907, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v20944 = vsel /*vm=*/%vm20907, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v20948 = vadd.f32 -2.5, %v20904
%v20950 = vrsqrt.pop %v20904
%v20951 = vmul.f32 %v20950, %v20904
%vm20952 = vcmp.eq.f32.partialorder %v20904, inf
%v20953 = vsel /*vm=*/%vm20952, /*on_true_vy=*/%v20904, /*on_false_vx=*/%v20951
%vm20954 = vcmp.eq.f32.partialorder %v20904, 0.0
%v20955 = vand.u32 2147483648, %v20904
%v20956 = vsel /*vm=*/%vm20954, /*on_true_vy=*/%v20955, /*on_false_vx=*/%v20953
%v20959 = vadd.f32 -3.0, %v20956
%v20963 = vsel /*vm=*/%vm20907, /*on_true_vy=*/%v20948, /*on_false_vx=*/%v20959
%v20967 = vmul.f32 %v20963, %v20944
%v20971 = vadd.f32 %v20967, %v20940
%v20975 = vmul.f32 %v20971, %v20963
%v20979 = vadd.f32 %v20975, %v20936
%v20983 = vmul.f32 %v20979, %v20963
%v20987 = vadd.f32 %v20983, %v20932
%v20991 = vmul.f32 %v20987, %v20963
%v20995 = vadd.f32 %v20991, %v20928
%v20999 = vmul.f32 %v20995, %v20963
%v21003 = vadd.f32 %v20999, %v20924
%v21007 = vmul.f32 %v21003, %v20963
%v21011 = vadd.f32 %v21007, %v20920
%v21015 = vmul.f32 %v21011, %v20963
%v21019 = vadd.f32 %v21015, %v20916
%v21023 = vmul.f32 %v21019, %v20963
%v21027 = vadd.f32 %v21023, %v20912
%v21031 = vmul.f32 %v21027, %v20878
%v21035 = vsel /*vm=*/%vm20883, /*on_true_vy=*/%v20888, /*on_false_vx=*/%v21031
%v21039 = vmul.f32 1.4140625, %v21035
%v21042 = vpack.c.bf16 %v120417, %v21039
%119855 = vst [vmem:[%s280 + $0x194] sm:$0xf] /*vst_source=*/%v21042
%v21046 = vadd.s32 %v19199, %v2355
%v21056 = vadd.s32 %v21046, %v415
%vm21060 = vcmp.lt.u32.totalorder %v21056, %v21046
%vm21065 = vcmp.lt.u32.totalorder %v21046, %v2355
%v21070 = vadd.s32 %v19182, %v2342
%v21074 = vadd.s32 1, %v21070
%v21078 = vsel /*vm=*/%vm21065, /*on_true_vy=*/%v21074, /*on_false_vx=*/%v21070
%v21082 = vadd.s32 1, %v21078
%v21086 = vsel /*vm=*/%vm21060, /*on_true_vy=*/%v21082, /*on_false_vx=*/%v21078
%v21091 = vadd.s32 %v21086, %v10
%v21095 = vadd.s32 %v21056, %v9
%v21099 = vadd.s32 %v21095, %v21091
%v21101 = vshll.u32 %v21095, 13
%v21102 = vshrl.u32 %v21095, 19
%v21103 = vor.u32 %v21102, %v21101
%v21104 = vxor.u32 %v21103, %v21099
%v21107 = vadd.s32 %v21104, %v21099
%v21109 = vshll.u32 %v21104, 15
%v21110 = vshrl.u32 %v21104, 17
%v21111 = vor.u32 %v21110, %v21109
%v21112 = vxor.u32 %v21111, %v21107
%v21115 = vadd.s32 %v21112, %v21107
%v21117 = vshll.u32 %v21112, 26
%v21118 = vshrl.u32 %v21112, 6
%v21119 = vor.u32 %v21118, %v21117
%v21120 = vxor.u32 %v21119, %v21115
%v21123 = vadd.s32 %v21120, %v21115
%v21127 = vadd.s32 %v21123, %v9
%v21129 = vshll.u32 %v21120, 6
%v21130 = vshrl.u32 %v21120, 26
%v21131 = vor.u32 %v21130, %v21129
%v21132 = vxor.u32 %v21131, %v21123
%v21135 = vadd.s32 %v21132, %v8
%v21139 = vadd.s32 1, %v21135
%v21143 = vadd.s32 %v21139, %v21127
%v21145 = vshll.u32 %v21139, 17
%v21146 = vshrl.u32 %v21139, 15
%v21147 = vor.u32 %v21146, %v21145
%v21148 = vxor.u32 %v21147, %v21143
%v21151 = vadd.s32 %v21148, %v21143
%v21153 = vshll.u32 %v21148, 29
%v21154 = vshrl.u32 %v21148, 3
%v21155 = vor.u32 %v21154, %v21153
%v21156 = vxor.u32 %v21155, %v21151
%v21159 = vadd.s32 %v21156, %v21151
%v21161 = vshll.u32 %v21156, 16
%v21162 = vshrl.u32 %v21156, 16
%v21163 = vor.u32 %v21162, %v21161
%v21164 = vxor.u32 %v21163, %v21159
%v21167 = vadd.s32 %v21164, %v21159
%v21171 = vadd.s32 %v21167, %v8
%v21173 = vshll.u32 %v21164, 24
%v21174 = vshrl.u32 %v21164, 8
%v21175 = vor.u32 %v21174, %v21173
%v21176 = vxor.u32 %v21175, %v21167
%v21179 = vadd.s32 %v21176, %v10
%v21183 = vadd.s32 2, %v21179
%v21187 = vadd.s32 %v21183, %v21171
%v21189 = vshll.u32 %v21183, 13
%v21190 = vshrl.u32 %v21183, 19
%v21191 = vor.u32 %v21190, %v21189
%v21192 = vxor.u32 %v21191, %v21187
%v21195 = vadd.s32 %v21192, %v21187
%v21197 = vshll.u32 %v21192, 15
%v21198 = vshrl.u32 %v21192, 17
%v21199 = vor.u32 %v21198, %v21197
%v21200 = vxor.u32 %v21199, %v21195
%v21203 = vadd.s32 %v21200, %v21195
%v21205 = vshll.u32 %v21200, 26
%v21206 = vshrl.u32 %v21200, 6
%v21207 = vor.u32 %v21206, %v21205
%v21208 = vxor.u32 %v21207, %v21203
%v21211 = vadd.s32 %v21208, %v21203
%v21215 = vadd.s32 %v21211, %v10
%v21217 = vshll.u32 %v21208, 6
%v21218 = vshrl.u32 %v21208, 26
%v21219 = vor.u32 %v21218, %v21217
%v21220 = vxor.u32 %v21219, %v21211
%v21223 = vadd.s32 %v21220, %v9
%v21227 = vadd.s32 3, %v21223
%v21231 = vadd.s32 %v21227, %v21215
%v21233 = vshll.u32 %v21227, 17
%v21234 = vshrl.u32 %v21227, 15
%v21235 = vor.u32 %v21234, %v21233
%v21236 = vxor.u32 %v21235, %v21231
%v21239 = vadd.s32 %v21236, %v21231
%v21241 = vshll.u32 %v21236, 29
%v21242 = vshrl.u32 %v21236, 3
%v21243 = vor.u32 %v21242, %v21241
%v21244 = vxor.u32 %v21243, %v21239
%v21247 = vadd.s32 %v21244, %v21239
%v21249 = vshll.u32 %v21244, 16
%v21250 = vshrl.u32 %v21244, 16
%v21251 = vor.u32 %v21250, %v21249
%v21252 = vxor.u32 %v21251, %v21247
%v21255 = vadd.s32 %v21252, %v21247
%v21259 = vadd.s32 %v21255, %v9
%v21261 = vshll.u32 %v21252, 24
%v21262 = vshrl.u32 %v21252, 8
%v21263 = vor.u32 %v21262, %v21261
%v21264 = vxor.u32 %v21263, %v21255
%v21267 = vadd.s32 %v21264, %v8
%v21271 = vadd.s32 4, %v21267
%v21275 = vadd.s32 %v21271, %v21259
%v21277 = vshll.u32 %v21271, 13
%v21278 = vshrl.u32 %v21271, 19
%v21279 = vor.u32 %v21278, %v21277
%v21280 = vxor.u32 %v21279, %v21275
%v21283 = vadd.s32 %v21280, %v21275
%v21285 = vshll.u32 %v21280, 15
%v21286 = vshrl.u32 %v21280, 17
%v21287 = vor.u32 %v21286, %v21285
%v21288 = vxor.u32 %v21287, %v21283
%v21291 = vadd.s32 %v21288, %v21283
%v21293 = vshll.u32 %v21288, 26
%v21294 = vshrl.u32 %v21288, 6
%v21295 = vor.u32 %v21294, %v21293
%v21296 = vxor.u32 %v21295, %v21291
%v21299 = vadd.s32 %v21296, %v21291
%v21303 = vadd.s32 %v21299, %v8
%v21305 = vshll.u32 %v21296, 6
%v21306 = vshrl.u32 %v21296, 26
%v21307 = vor.u32 %v21306, %v21305
%v21308 = vxor.u32 %v21307, %v21299
%v21311 = vadd.s32 %v21308, %v10
%v21315 = vadd.s32 5, %v21311
%v21317 = vxor.u32 %v21315, %v21303
%v21318 = vand.u32.u8 255, %v21317
%v21319 = vand.u32 65535, %v21318
%v21320 = vshrl.u32 %v21319, 1
%v21321 = vor.u32 16256, %v21320
%v21322 = vand.u32.u16 65535, %v21321
%v119856 = vadd.low.f32.bf16 -1.0, %v21322
%v21331 = vmul.f32 2.0, %v119856
%v21335 = vadd.f32 -0.99609375, %v21331
%v21339 = vmax.f32 %v21335, -0.99609375
%v21341 = vand.u32 2147483647, %v21339
%vm21344 = vcmp.eq.f32.partialorder %v21341, 1.0
%v21349 = vmul.f32 inf, %v21339
%v21351 = vxor.u32 2147483648, %v21339
%v21354 = vmul.f32 %v21351, %v21339
%v21356 = vadd.f32 1.0, %v21354
%v21357 = vlog2.pop %v21356
%v21358 = vmul.f32 0.6931472, %v21357
%v21359 = vmul.f32 -0.5, %v21354
%v21360 = vadd.f32 1.0, %v21359
%v21361 = vmul.f32 %v21360, %v21354
%v21362 = vand.u32 2147483647, %v21354
%vm21363 = vcmp.lt.f32.partialorder %v21362, 0.0004427343
%v21364 = vsel /*vm=*/%vm21363, /*on_true_vy=*/%v21361, /*on_false_vx=*/%v21358
%v21365 = vxor.u32 2147483648, %v21364
%vm21368 = vcmp.lt.f32.partialorder %v21365, 5.0
%v21373 = vsel /*vm=*/%vm21368, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v21377 = vsel /*vm=*/%vm21368, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v21381 = vsel /*vm=*/%vm21368, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v21385 = vsel /*vm=*/%vm21368, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v21389 = vsel /*vm=*/%vm21368, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v21393 = vsel /*vm=*/%vm21368, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v21397 = vsel /*vm=*/%vm21368, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v21401 = vsel /*vm=*/%vm21368, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v21405 = vsel /*vm=*/%vm21368, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v21409 = vadd.f32 -2.5, %v21365
%v21411 = vrsqrt.pop %v21365
%v21412 = vmul.f32 %v21411, %v21365
%vm21413 = vcmp.eq.f32.partialorder %v21365, inf
%v21414 = vsel /*vm=*/%vm21413, /*on_true_vy=*/%v21365, /*on_false_vx=*/%v21412
%vm21415 = vcmp.eq.f32.partialorder %v21365, 0.0
%v21416 = vand.u32 2147483648, %v21365
%v21417 = vsel /*vm=*/%vm21415, /*on_true_vy=*/%v21416, /*on_false_vx=*/%v21414
%v21420 = vadd.f32 -3.0, %v21417
%v21424 = vsel /*vm=*/%vm21368, /*on_true_vy=*/%v21409, /*on_false_vx=*/%v21420
%v21428 = vmul.f32 %v21424, %v21405
%v21432 = vadd.f32 %v21428, %v21401
%v21436 = vmul.f32 %v21432, %v21424
%v21440 = vadd.f32 %v21436, %v21397
%v21444 = vmul.f32 %v21440, %v21424
%v21448 = vadd.f32 %v21444, %v21393
%v21452 = vmul.f32 %v21448, %v21424
%v21456 = vadd.f32 %v21452, %v21389
%v21460 = vmul.f32 %v21456, %v21424
%v21464 = vadd.f32 %v21460, %v21385
%v21468 = vmul.f32 %v21464, %v21424
%v21472 = vadd.f32 %v21468, %v21381
%v21476 = vmul.f32 %v21472, %v21424
%v21480 = vadd.f32 %v21476, %v21377
%v21484 = vmul.f32 %v21480, %v21424
%v21488 = vadd.f32 %v21484, %v21373
%v21492 = vmul.f32 %v21488, %v21339
%v21496 = vsel /*vm=*/%vm21344, /*on_true_vy=*/%v21349, /*on_false_vx=*/%v21492
%v21500 = vmul.f32 1.4140625, %v21496
%v21503 = vpack.c.bf16 %v120417, %v21500
%119857 = vst [vmem:[%s280 + $0x214] sm:$0xf] /*vst_source=*/%v21503
%v21507 = vadd.s32 %v19199, %v2842
%v21517 = vadd.s32 %v21507, %v415
%vm21521 = vcmp.lt.u32.totalorder %v21517, %v21507
%vm21526 = vcmp.lt.u32.totalorder %v21507, %v2842
%v21531 = vadd.s32 %v19182, %v2829
%v21535 = vadd.s32 1, %v21531
%v21539 = vsel /*vm=*/%vm21526, /*on_true_vy=*/%v21535, /*on_false_vx=*/%v21531
%v21543 = vadd.s32 1, %v21539
%v21547 = vsel /*vm=*/%vm21521, /*on_true_vy=*/%v21543, /*on_false_vx=*/%v21539
%v21552 = vadd.s32 %v21547, %v10
%v21556 = vadd.s32 %v21517, %v9
%v21560 = vadd.s32 %v21556, %v21552
%v21562 = vshll.u32 %v21556, 13
%v21563 = vshrl.u32 %v21556, 19
%v21564 = vor.u32 %v21563, %v21562
%v21565 = vxor.u32 %v21564, %v21560
%v21568 = vadd.s32 %v21565, %v21560
%v21570 = vshll.u32 %v21565, 15
%v21571 = vshrl.u32 %v21565, 17
%v21572 = vor.u32 %v21571, %v21570
%v21573 = vxor.u32 %v21572, %v21568
%v21576 = vadd.s32 %v21573, %v21568
%v21578 = vshll.u32 %v21573, 26
%v21579 = vshrl.u32 %v21573, 6
%v21580 = vor.u32 %v21579, %v21578
%v21581 = vxor.u32 %v21580, %v21576
%v21584 = vadd.s32 %v21581, %v21576
%v21588 = vadd.s32 %v21584, %v9
%v21590 = vshll.u32 %v21581, 6
%v21591 = vshrl.u32 %v21581, 26
%v21592 = vor.u32 %v21591, %v21590
%v21593 = vxor.u32 %v21592, %v21584
%v21596 = vadd.s32 %v21593, %v8
%v21600 = vadd.s32 1, %v21596
%v21604 = vadd.s32 %v21600, %v21588
%v21606 = vshll.u32 %v21600, 17
%v21607 = vshrl.u32 %v21600, 15
%v21608 = vor.u32 %v21607, %v21606
%v21609 = vxor.u32 %v21608, %v21604
%v21612 = vadd.s32 %v21609, %v21604
%v21614 = vshll.u32 %v21609, 29
%v21615 = vshrl.u32 %v21609, 3
%v21616 = vor.u32 %v21615, %v21614
%v21617 = vxor.u32 %v21616, %v21612
%v21620 = vadd.s32 %v21617, %v21612
%v21622 = vshll.u32 %v21617, 16
%v21623 = vshrl.u32 %v21617, 16
%v21624 = vor.u32 %v21623, %v21622
%v21625 = vxor.u32 %v21624, %v21620
%v21628 = vadd.s32 %v21625, %v21620
%v21632 = vadd.s32 %v21628, %v8
%v21634 = vshll.u32 %v21625, 24
%v21635 = vshrl.u32 %v21625, 8
%v21636 = vor.u32 %v21635, %v21634
%v21637 = vxor.u32 %v21636, %v21628
%v21640 = vadd.s32 %v21637, %v10
%v21644 = vadd.s32 2, %v21640
%v21648 = vadd.s32 %v21644, %v21632
%v21650 = vshll.u32 %v21644, 13
%v21651 = vshrl.u32 %v21644, 19
%v21652 = vor.u32 %v21651, %v21650
%v21653 = vxor.u32 %v21652, %v21648
%v21656 = vadd.s32 %v21653, %v21648
%v21658 = vshll.u32 %v21653, 15
%v21659 = vshrl.u32 %v21653, 17
%v21660 = vor.u32 %v21659, %v21658
%v21661 = vxor.u32 %v21660, %v21656
%v21664 = vadd.s32 %v21661, %v21656
%v21666 = vshll.u32 %v21661, 26
%v21667 = vshrl.u32 %v21661, 6
%v21668 = vor.u32 %v21667, %v21666
%v21669 = vxor.u32 %v21668, %v21664
%v21672 = vadd.s32 %v21669, %v21664
%v21676 = vadd.s32 %v21672, %v10
%v21678 = vshll.u32 %v21669, 6
%v21679 = vshrl.u32 %v21669, 26
%v21680 = vor.u32 %v21679, %v21678
%v21681 = vxor.u32 %v21680, %v21672
%v21684 = vadd.s32 %v21681, %v9
%v21688 = vadd.s32 3, %v21684
%v21692 = vadd.s32 %v21688, %v21676
%v21694 = vshll.u32 %v21688, 17
%v21695 = vshrl.u32 %v21688, 15
%v21696 = vor.u32 %v21695, %v21694
%v21697 = vxor.u32 %v21696, %v21692
%v21700 = vadd.s32 %v21697, %v21692
%v21702 = vshll.u32 %v21697, 29
%v21703 = vshrl.u32 %v21697, 3
%v21704 = vor.u32 %v21703, %v21702
%v21705 = vxor.u32 %v21704, %v21700
%v21708 = vadd.s32 %v21705, %v21700
%v21710 = vshll.u32 %v21705, 16
%v21711 = vshrl.u32 %v21705, 16
%v21712 = vor.u32 %v21711, %v21710
%v21713 = vxor.u32 %v21712, %v21708
%v21716 = vadd.s32 %v21713, %v21708
%v21720 = vadd.s32 %v21716, %v9
%v21722 = vshll.u32 %v21713, 24
%v21723 = vshrl.u32 %v21713, 8
%v21724 = vor.u32 %v21723, %v21722
%v21725 = vxor.u32 %v21724, %v21716
%v21728 = vadd.s32 %v21725, %v8
%v21732 = vadd.s32 4, %v21728
%v21736 = vadd.s32 %v21732, %v21720
%v21738 = vshll.u32 %v21732, 13
%v21739 = vshrl.u32 %v21732, 19
%v21740 = vor.u32 %v21739, %v21738
%v21741 = vxor.u32 %v21740, %v21736
%v21744 = vadd.s32 %v21741, %v21736
%v21746 = vshll.u32 %v21741, 15
%v21747 = vshrl.u32 %v21741, 17
%v21748 = vor.u32 %v21747, %v21746
%v21749 = vxor.u32 %v21748, %v21744
%v21752 = vadd.s32 %v21749, %v21744
%v21754 = vshll.u32 %v21749, 26
%v21755 = vshrl.u32 %v21749, 6
%v21756 = vor.u32 %v21755, %v21754
%v21757 = vxor.u32 %v21756, %v21752
%v21760 = vadd.s32 %v21757, %v21752
%v21764 = vadd.s32 %v21760, %v8
%v21766 = vshll.u32 %v21757, 6
%v21767 = vshrl.u32 %v21757, 26
%v21768 = vor.u32 %v21767, %v21766
%v21769 = vxor.u32 %v21768, %v21760
%v21772 = vadd.s32 %v21769, %v10
%v21776 = vadd.s32 5, %v21772
%v21778 = vxor.u32 %v21776, %v21764
%v21779 = vand.u32.u8 255, %v21778
%v21780 = vand.u32 65535, %v21779
%v21781 = vshrl.u32 %v21780, 1
%v21782 = vor.u32 16256, %v21781
%v21783 = vand.u32.u16 65535, %v21782
%v119858 = vadd.low.f32.bf16 -1.0, %v21783
%v21792 = vmul.f32 2.0, %v119858
%v21796 = vadd.f32 -0.99609375, %v21792
%v21800 = vmax.f32 %v21796, -0.99609375
%v21802 = vand.u32 2147483647, %v21800
%vm21805 = vcmp.eq.f32.partialorder %v21802, 1.0
%v21810 = vmul.f32 inf, %v21800
%v21812 = vxor.u32 2147483648, %v21800
%v21815 = vmul.f32 %v21812, %v21800
%v21817 = vadd.f32 1.0, %v21815
%v21818 = vlog2.pop %v21817
%v21819 = vmul.f32 0.6931472, %v21818
%v21820 = vmul.f32 -0.5, %v21815
%v21821 = vadd.f32 1.0, %v21820
%v21822 = vmul.f32 %v21821, %v21815
%v21823 = vand.u32 2147483647, %v21815
%vm21824 = vcmp.lt.f32.partialorder %v21823, 0.0004427343
%v21825 = vsel /*vm=*/%vm21824, /*on_true_vy=*/%v21822, /*on_false_vx=*/%v21819
%v21826 = vxor.u32 2147483648, %v21825
%vm21829 = vcmp.lt.f32.partialorder %v21826, 5.0
%v21834 = vsel /*vm=*/%vm21829, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v21838 = vsel /*vm=*/%vm21829, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v21842 = vsel /*vm=*/%vm21829, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v21846 = vsel /*vm=*/%vm21829, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v21850 = vsel /*vm=*/%vm21829, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v21854 = vsel /*vm=*/%vm21829, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v21858 = vsel /*vm=*/%vm21829, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v21862 = vsel /*vm=*/%vm21829, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v21866 = vsel /*vm=*/%vm21829, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v21870 = vadd.f32 -2.5, %v21826
%v21872 = vrsqrt.pop %v21826
%v21873 = vmul.f32 %v21872, %v21826
%vm21874 = vcmp.eq.f32.partialorder %v21826, inf
%v21875 = vsel /*vm=*/%vm21874, /*on_true_vy=*/%v21826, /*on_false_vx=*/%v21873
%vm21876 = vcmp.eq.f32.partialorder %v21826, 0.0
%v21877 = vand.u32 2147483648, %v21826
%v21878 = vsel /*vm=*/%vm21876, /*on_true_vy=*/%v21877, /*on_false_vx=*/%v21875
%v21881 = vadd.f32 -3.0, %v21878
%v21885 = vsel /*vm=*/%vm21829, /*on_true_vy=*/%v21870, /*on_false_vx=*/%v21881
%v21889 = vmul.f32 %v21885, %v21866
%v21893 = vadd.f32 %v21889, %v21862
%v21897 = vmul.f32 %v21893, %v21885
%v21901 = vadd.f32 %v21897, %v21858
%v21905 = vmul.f32 %v21901, %v21885
%v21909 = vadd.f32 %v21905, %v21854
%v21913 = vmul.f32 %v21909, %v21885
%v21917 = vadd.f32 %v21913, %v21850
%v21921 = vmul.f32 %v21917, %v21885
%v21925 = vadd.f32 %v21921, %v21846
%v21929 = vmul.f32 %v21925, %v21885
%v21933 = vadd.f32 %v21929, %v21842
%v21937 = vmul.f32 %v21933, %v21885
%v21941 = vadd.f32 %v21937, %v21838
%v21945 = vmul.f32 %v21941, %v21885
%v21949 = vadd.f32 %v21945, %v21834
%v21953 = vmul.f32 %v21949, %v21800
%v21957 = vsel /*vm=*/%vm21805, /*on_true_vy=*/%v21810, /*on_false_vx=*/%v21953
%v21961 = vmul.f32 1.4140625, %v21957
%v21964 = vpack.c.bf16 %v120417, %v21961
%119859 = vst [vmem:[%s280 + $0x294] sm:$0xf] /*vst_source=*/%v21964
%v21968 = vadd.s32 %v19199, %v3329
%v21978 = vadd.s32 %v21968, %v415
%vm21982 = vcmp.lt.u32.totalorder %v21978, %v21968
%vm21987 = vcmp.lt.u32.totalorder %v21968, %v3329
%v21992 = vadd.s32 %v19182, %v3316
%v21996 = vadd.s32 1, %v21992
%v22000 = vsel /*vm=*/%vm21987, /*on_true_vy=*/%v21996, /*on_false_vx=*/%v21992
%v22004 = vadd.s32 1, %v22000
%v22008 = vsel /*vm=*/%vm21982, /*on_true_vy=*/%v22004, /*on_false_vx=*/%v22000
%v22013 = vadd.s32 %v22008, %v10
%v22017 = vadd.s32 %v21978, %v9
%v22021 = vadd.s32 %v22017, %v22013
%v22023 = vshll.u32 %v22017, 13
%v22024 = vshrl.u32 %v22017, 19
%v22025 = vor.u32 %v22024, %v22023
%v22026 = vxor.u32 %v22025, %v22021
%v22029 = vadd.s32 %v22026, %v22021
%v22031 = vshll.u32 %v22026, 15
%v22032 = vshrl.u32 %v22026, 17
%v22033 = vor.u32 %v22032, %v22031
%v22034 = vxor.u32 %v22033, %v22029
%v22037 = vadd.s32 %v22034, %v22029
%v22039 = vshll.u32 %v22034, 26
%v22040 = vshrl.u32 %v22034, 6
%v22041 = vor.u32 %v22040, %v22039
%v22042 = vxor.u32 %v22041, %v22037
%v22045 = vadd.s32 %v22042, %v22037
%v22049 = vadd.s32 %v22045, %v9
%v22051 = vshll.u32 %v22042, 6
%v22052 = vshrl.u32 %v22042, 26
%v22053 = vor.u32 %v22052, %v22051
%v22054 = vxor.u32 %v22053, %v22045
%v22057 = vadd.s32 %v22054, %v8
%v22061 = vadd.s32 1, %v22057
%v22065 = vadd.s32 %v22061, %v22049
%v22067 = vshll.u32 %v22061, 17
%v22068 = vshrl.u32 %v22061, 15
%v22069 = vor.u32 %v22068, %v22067
%v22070 = vxor.u32 %v22069, %v22065
%v22073 = vadd.s32 %v22070, %v22065
%v22075 = vshll.u32 %v22070, 29
%v22076 = vshrl.u32 %v22070, 3
%v22077 = vor.u32 %v22076, %v22075
%v22078 = vxor.u32 %v22077, %v22073
%v22081 = vadd.s32 %v22078, %v22073
%v22083 = vshll.u32 %v22078, 16
%v22084 = vshrl.u32 %v22078, 16
%v22085 = vor.u32 %v22084, %v22083
%v22086 = vxor.u32 %v22085, %v22081
%v22089 = vadd.s32 %v22086, %v22081
%v22093 = vadd.s32 %v22089, %v8
%v22095 = vshll.u32 %v22086, 24
%v22096 = vshrl.u32 %v22086, 8
%v22097 = vor.u32 %v22096, %v22095
%v22098 = vxor.u32 %v22097, %v22089
%v22101 = vadd.s32 %v22098, %v10
%v22105 = vadd.s32 2, %v22101
%v22109 = vadd.s32 %v22105, %v22093
%v22111 = vshll.u32 %v22105, 13
%v22112 = vshrl.u32 %v22105, 19
%v22113 = vor.u32 %v22112, %v22111
%v22114 = vxor.u32 %v22113, %v22109
%v22117 = vadd.s32 %v22114, %v22109
%v22119 = vshll.u32 %v22114, 15
%v22120 = vshrl.u32 %v22114, 17
%v22121 = vor.u32 %v22120, %v22119
%v22122 = vxor.u32 %v22121, %v22117
%v22125 = vadd.s32 %v22122, %v22117
%v22127 = vshll.u32 %v22122, 26
%v22128 = vshrl.u32 %v22122, 6
%v22129 = vor.u32 %v22128, %v22127
%v22130 = vxor.u32 %v22129, %v22125
%v22133 = vadd.s32 %v22130, %v22125
%v22137 = vadd.s32 %v22133, %v10
%v22139 = vshll.u32 %v22130, 6
%v22140 = vshrl.u32 %v22130, 26
%v22141 = vor.u32 %v22140, %v22139
%v22142 = vxor.u32 %v22141, %v22133
%v22145 = vadd.s32 %v22142, %v9
%v22149 = vadd.s32 3, %v22145
%v22153 = vadd.s32 %v22149, %v22137
%v22155 = vshll.u32 %v22149, 17
%v22156 = vshrl.u32 %v22149, 15
%v22157 = vor.u32 %v22156, %v22155
%v22158 = vxor.u32 %v22157, %v22153
%v22161 = vadd.s32 %v22158, %v22153
%v22163 = vshll.u32 %v22158, 29
%v22164 = vshrl.u32 %v22158, 3
%v22165 = vor.u32 %v22164, %v22163
%v22166 = vxor.u32 %v22165, %v22161
%v22169 = vadd.s32 %v22166, %v22161
%v22171 = vshll.u32 %v22166, 16
%v22172 = vshrl.u32 %v22166, 16
%v22173 = vor.u32 %v22172, %v22171
%v22174 = vxor.u32 %v22173, %v22169
%v22177 = vadd.s32 %v22174, %v22169
%v22181 = vadd.s32 %v22177, %v9
%v22183 = vshll.u32 %v22174, 24
%v22184 = vshrl.u32 %v22174, 8
%v22185 = vor.u32 %v22184, %v22183
%v22186 = vxor.u32 %v22185, %v22177
%v22189 = vadd.s32 %v22186, %v8
%v22193 = vadd.s32 4, %v22189
%v22197 = vadd.s32 %v22193, %v22181
%v22199 = vshll.u32 %v22193, 13
%v22200 = vshrl.u32 %v22193, 19
%v22201 = vor.u32 %v22200, %v22199
%v22202 = vxor.u32 %v22201, %v22197
%v22205 = vadd.s32 %v22202, %v22197
%v22207 = vshll.u32 %v22202, 15
%v22208 = vshrl.u32 %v22202, 17
%v22209 = vor.u32 %v22208, %v22207
%v22210 = vxor.u32 %v22209, %v22205
%v22213 = vadd.s32 %v22210, %v22205
%v22215 = vshll.u32 %v22210, 26
%v22216 = vshrl.u32 %v22210, 6
%v22217 = vor.u32 %v22216, %v22215
%v22218 = vxor.u32 %v22217, %v22213
%v22221 = vadd.s32 %v22218, %v22213
%v22225 = vadd.s32 %v22221, %v8
%v22227 = vshll.u32 %v22218, 6
%v22228 = vshrl.u32 %v22218, 26
%v22229 = vor.u32 %v22228, %v22227
%v22230 = vxor.u32 %v22229, %v22221
%v22233 = vadd.s32 %v22230, %v10
%v22237 = vadd.s32 5, %v22233
%v22239 = vxor.u32 %v22237, %v22225
%v22240 = vand.u32.u8 255, %v22239
%v22241 = vand.u32 65535, %v22240
%v22242 = vshrl.u32 %v22241, 1
%v22243 = vor.u32 16256, %v22242
%v22244 = vand.u32.u16 65535, %v22243
%v119860 = vadd.low.f32.bf16 -1.0, %v22244
%v22253 = vmul.f32 2.0, %v119860
%v22257 = vadd.f32 -0.99609375, %v22253
%v22261 = vmax.f32 %v22257, -0.99609375
%v22263 = vand.u32 2147483647, %v22261
%vm22266 = vcmp.eq.f32.partialorder %v22263, 1.0
%v22271 = vmul.f32 inf, %v22261
%v22273 = vxor.u32 2147483648, %v22261
%v22276 = vmul.f32 %v22273, %v22261
%v22278 = vadd.f32 1.0, %v22276
%v22279 = vlog2.pop %v22278
%v22280 = vmul.f32 0.6931472, %v22279
%v22281 = vmul.f32 -0.5, %v22276
%v22282 = vadd.f32 1.0, %v22281
%v22283 = vmul.f32 %v22282, %v22276
%v22284 = vand.u32 2147483647, %v22276
%vm22285 = vcmp.lt.f32.partialorder %v22284, 0.0004427343
%v22286 = vsel /*vm=*/%vm22285, /*on_true_vy=*/%v22283, /*on_false_vx=*/%v22280
%v22287 = vxor.u32 2147483648, %v22286
%vm22290 = vcmp.lt.f32.partialorder %v22287, 5.0
%v22295 = vsel /*vm=*/%vm22290, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v22299 = vsel /*vm=*/%vm22290, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v22303 = vsel /*vm=*/%vm22290, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v22307 = vsel /*vm=*/%vm22290, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v22311 = vsel /*vm=*/%vm22290, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v22315 = vsel /*vm=*/%vm22290, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v22319 = vsel /*vm=*/%vm22290, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v22323 = vsel /*vm=*/%vm22290, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v22327 = vsel /*vm=*/%vm22290, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v22331 = vadd.f32 -2.5, %v22287
%v22333 = vrsqrt.pop %v22287
%v22334 = vmul.f32 %v22333, %v22287
%vm22335 = vcmp.eq.f32.partialorder %v22287, inf
%v22336 = vsel /*vm=*/%vm22335, /*on_true_vy=*/%v22287, /*on_false_vx=*/%v22334
%vm22337 = vcmp.eq.f32.partialorder %v22287, 0.0
%v22338 = vand.u32 2147483648, %v22287
%v22339 = vsel /*vm=*/%vm22337, /*on_true_vy=*/%v22338, /*on_false_vx=*/%v22336
%v22342 = vadd.f32 -3.0, %v22339
%v22346 = vsel /*vm=*/%vm22290, /*on_true_vy=*/%v22331, /*on_false_vx=*/%v22342
%v22350 = vmul.f32 %v22346, %v22327
%v22354 = vadd.f32 %v22350, %v22323
%v22358 = vmul.f32 %v22354, %v22346
%v22362 = vadd.f32 %v22358, %v22319
%v22366 = vmul.f32 %v22362, %v22346
%v22370 = vadd.f32 %v22366, %v22315
%v22374 = vmul.f32 %v22370, %v22346
%v22378 = vadd.f32 %v22374, %v22311
%v22382 = vmul.f32 %v22378, %v22346
%v22386 = vadd.f32 %v22382, %v22307
%v22390 = vmul.f32 %v22386, %v22346
%v22394 = vadd.f32 %v22390, %v22303
%v22398 = vmul.f32 %v22394, %v22346
%v22402 = vadd.f32 %v22398, %v22299
%v22406 = vmul.f32 %v22402, %v22346
%v22410 = vadd.f32 %v22406, %v22295
%v22414 = vmul.f32 %v22410, %v22261
%v22418 = vsel /*vm=*/%vm22266, /*on_true_vy=*/%v22271, /*on_false_vx=*/%v22414
%v22422 = vmul.f32 1.4140625, %v22418
%v22425 = vpack.c.bf16 %v120417, %v22422
%119861 = vst [vmem:[%s280 + $0x314] sm:$0xf] /*vst_source=*/%v22425
%v22429 = vadd.s32 %v19199, %v3816
%v22439 = vadd.s32 %v22429, %v415
%vm22443 = vcmp.lt.u32.totalorder %v22439, %v22429
%vm22448 = vcmp.lt.u32.totalorder %v22429, %v3816
%v22453 = vadd.s32 %v19182, %v3803
%v22457 = vadd.s32 1, %v22453
%v22461 = vsel /*vm=*/%vm22448, /*on_true_vy=*/%v22457, /*on_false_vx=*/%v22453
%v22465 = vadd.s32 1, %v22461
%v22469 = vsel /*vm=*/%vm22443, /*on_true_vy=*/%v22465, /*on_false_vx=*/%v22461
%v22474 = vadd.s32 %v22469, %v10
%v22478 = vadd.s32 %v22439, %v9
%v22482 = vadd.s32 %v22478, %v22474
%v22484 = vshll.u32 %v22478, 13
%v22485 = vshrl.u32 %v22478, 19
%v22486 = vor.u32 %v22485, %v22484
%v22487 = vxor.u32 %v22486, %v22482
%v22490 = vadd.s32 %v22487, %v22482
%v22492 = vshll.u32 %v22487, 15
%v22493 = vshrl.u32 %v22487, 17
%v22494 = vor.u32 %v22493, %v22492
%v22495 = vxor.u32 %v22494, %v22490
%v22498 = vadd.s32 %v22495, %v22490
%v22500 = vshll.u32 %v22495, 26
%v22501 = vshrl.u32 %v22495, 6
%v22502 = vor.u32 %v22501, %v22500
%v22503 = vxor.u32 %v22502, %v22498
%v22506 = vadd.s32 %v22503, %v22498
%v22510 = vadd.s32 %v22506, %v9
%v22512 = vshll.u32 %v22503, 6
%v22513 = vshrl.u32 %v22503, 26
%v22514 = vor.u32 %v22513, %v22512
%v22515 = vxor.u32 %v22514, %v22506
%v22518 = vadd.s32 %v22515, %v8
%v22522 = vadd.s32 1, %v22518
%v22526 = vadd.s32 %v22522, %v22510
%v22528 = vshll.u32 %v22522, 17
%v22529 = vshrl.u32 %v22522, 15
%v22530 = vor.u32 %v22529, %v22528
%v22531 = vxor.u32 %v22530, %v22526
%v22534 = vadd.s32 %v22531, %v22526
%v22536 = vshll.u32 %v22531, 29
%v22537 = vshrl.u32 %v22531, 3
%v22538 = vor.u32 %v22537, %v22536
%v22539 = vxor.u32 %v22538, %v22534
%v22542 = vadd.s32 %v22539, %v22534
%v22544 = vshll.u32 %v22539, 16
%v22545 = vshrl.u32 %v22539, 16
%v22546 = vor.u32 %v22545, %v22544
%v22547 = vxor.u32 %v22546, %v22542
%v22550 = vadd.s32 %v22547, %v22542
%v22554 = vadd.s32 %v22550, %v8
%v22556 = vshll.u32 %v22547, 24
%v22557 = vshrl.u32 %v22547, 8
%v22558 = vor.u32 %v22557, %v22556
%v22559 = vxor.u32 %v22558, %v22550
%v22562 = vadd.s32 %v22559, %v10
%v22566 = vadd.s32 2, %v22562
%v22570 = vadd.s32 %v22566, %v22554
%v22572 = vshll.u32 %v22566, 13
%v22573 = vshrl.u32 %v22566, 19
%v22574 = vor.u32 %v22573, %v22572
%v22575 = vxor.u32 %v22574, %v22570
%v22578 = vadd.s32 %v22575, %v22570
%v22580 = vshll.u32 %v22575, 15
%v22581 = vshrl.u32 %v22575, 17
%v22582 = vor.u32 %v22581, %v22580
%v22583 = vxor.u32 %v22582, %v22578
%v22586 = vadd.s32 %v22583, %v22578
%v22588 = vshll.u32 %v22583, 26
%v22589 = vshrl.u32 %v22583, 6
%v22590 = vor.u32 %v22589, %v22588
%v22591 = vxor.u32 %v22590, %v22586
%v22594 = vadd.s32 %v22591, %v22586
%v22598 = vadd.s32 %v22594, %v10
%v22600 = vshll.u32 %v22591, 6
%v22601 = vshrl.u32 %v22591, 26
%v22602 = vor.u32 %v22601, %v22600
%v22603 = vxor.u32 %v22602, %v22594
%v22606 = vadd.s32 %v22603, %v9
%v22610 = vadd.s32 3, %v22606
%v22614 = vadd.s32 %v22610, %v22598
%v22616 = vshll.u32 %v22610, 17
%v22617 = vshrl.u32 %v22610, 15
%v22618 = vor.u32 %v22617, %v22616
%v22619 = vxor.u32 %v22618, %v22614
%v22622 = vadd.s32 %v22619, %v22614
%v22624 = vshll.u32 %v22619, 29
%v22625 = vshrl.u32 %v22619, 3
%v22626 = vor.u32 %v22625, %v22624
%v22627 = vxor.u32 %v22626, %v22622
%v22630 = vadd.s32 %v22627, %v22622
%v22632 = vshll.u32 %v22627, 16
%v22633 = vshrl.u32 %v22627, 16
%v22634 = vor.u32 %v22633, %v22632
%v22635 = vxor.u32 %v22634, %v22630
%v22638 = vadd.s32 %v22635, %v22630
%v22642 = vadd.s32 %v22638, %v9
%v22644 = vshll.u32 %v22635, 24
%v22645 = vshrl.u32 %v22635, 8
%v22646 = vor.u32 %v22645, %v22644
%v22647 = vxor.u32 %v22646, %v22638
%v22650 = vadd.s32 %v22647, %v8
%v22654 = vadd.s32 4, %v22650
%v22658 = vadd.s32 %v22654, %v22642
%v22660 = vshll.u32 %v22654, 13
%v22661 = vshrl.u32 %v22654, 19
%v22662 = vor.u32 %v22661, %v22660
%v22663 = vxor.u32 %v22662, %v22658
%v22666 = vadd.s32 %v22663, %v22658
%v22668 = vshll.u32 %v22663, 15
%v22669 = vshrl.u32 %v22663, 17
%v22670 = vor.u32 %v22669, %v22668
%v22671 = vxor.u32 %v22670, %v22666
%v22674 = vadd.s32 %v22671, %v22666
%v22676 = vshll.u32 %v22671, 26
%v22677 = vshrl.u32 %v22671, 6
%v22678 = vor.u32 %v22677, %v22676
%v22679 = vxor.u32 %v22678, %v22674
%v22682 = vadd.s32 %v22679, %v22674
%v22686 = vadd.s32 %v22682, %v8
%v22688 = vshll.u32 %v22679, 6
%v22689 = vshrl.u32 %v22679, 26
%v22690 = vor.u32 %v22689, %v22688
%v22691 = vxor.u32 %v22690, %v22682
%v22694 = vadd.s32 %v22691, %v10
%v22698 = vadd.s32 5, %v22694
%v22700 = vxor.u32 %v22698, %v22686
%v22701 = vand.u32.u8 255, %v22700
%v22702 = vand.u32 65535, %v22701
%v22703 = vshrl.u32 %v22702, 1
%v22704 = vor.u32 16256, %v22703
%v22705 = vand.u32.u16 65535, %v22704
%v119862 = vadd.low.f32.bf16 -1.0, %v22705
%v22714 = vmul.f32 2.0, %v119862
%v22718 = vadd.f32 -0.99609375, %v22714
%v22722 = vmax.f32 %v22718, -0.99609375
%v22724 = vand.u32 2147483647, %v22722
%vm22727 = vcmp.eq.f32.partialorder %v22724, 1.0
%v22732 = vmul.f32 inf, %v22722
%v22734 = vxor.u32 2147483648, %v22722
%v22737 = vmul.f32 %v22734, %v22722
%v22739 = vadd.f32 1.0, %v22737
%v22740 = vlog2.pop %v22739
%v22741 = vmul.f32 0.6931472, %v22740
%v22742 = vmul.f32 -0.5, %v22737
%v22743 = vadd.f32 1.0, %v22742
%v22744 = vmul.f32 %v22743, %v22737
%v22745 = vand.u32 2147483647, %v22737
%vm22746 = vcmp.lt.f32.partialorder %v22745, 0.0004427343
%v22747 = vsel /*vm=*/%vm22746, /*on_true_vy=*/%v22744, /*on_false_vx=*/%v22741
%v22748 = vxor.u32 2147483648, %v22747
%vm22751 = vcmp.lt.f32.partialorder %v22748, 5.0
%v22756 = vsel /*vm=*/%vm22751, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v22760 = vsel /*vm=*/%vm22751, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v22764 = vsel /*vm=*/%vm22751, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v22768 = vsel /*vm=*/%vm22751, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v22772 = vsel /*vm=*/%vm22751, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v22776 = vsel /*vm=*/%vm22751, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v22780 = vsel /*vm=*/%vm22751, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v22784 = vsel /*vm=*/%vm22751, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v22788 = vsel /*vm=*/%vm22751, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v22792 = vadd.f32 -2.5, %v22748
%v22794 = vrsqrt.pop %v22748
%v22795 = vmul.f32 %v22794, %v22748
%vm22796 = vcmp.eq.f32.partialorder %v22748, inf
%v22797 = vsel /*vm=*/%vm22796, /*on_true_vy=*/%v22748, /*on_false_vx=*/%v22795
%vm22798 = vcmp.eq.f32.partialorder %v22748, 0.0
%v22799 = vand.u32 2147483648, %v22748
%v22800 = vsel /*vm=*/%vm22798, /*on_true_vy=*/%v22799, /*on_false_vx=*/%v22797
%v22803 = vadd.f32 -3.0, %v22800
%v22807 = vsel /*vm=*/%vm22751, /*on_true_vy=*/%v22792, /*on_false_vx=*/%v22803
%v22811 = vmul.f32 %v22807, %v22788
%v22815 = vadd.f32 %v22811, %v22784
%v22819 = vmul.f32 %v22815, %v22807
%v22823 = vadd.f32 %v22819, %v22780
%v22827 = vmul.f32 %v22823, %v22807
%v22831 = vadd.f32 %v22827, %v22776
%v22835 = vmul.f32 %v22831, %v22807
%v22839 = vadd.f32 %v22835, %v22772
%v22843 = vmul.f32 %v22839, %v22807
%v22847 = vadd.f32 %v22843, %v22768
%v22851 = vmul.f32 %v22847, %v22807
%v22855 = vadd.f32 %v22851, %v22764
%v22859 = vmul.f32 %v22855, %v22807
%v22863 = vadd.f32 %v22859, %v22760
%v22867 = vmul.f32 %v22863, %v22807
%v22871 = vadd.f32 %v22867, %v22756
%v22875 = vmul.f32 %v22871, %v22722
%v22879 = vsel /*vm=*/%vm22727, /*on_true_vy=*/%v22732, /*on_false_vx=*/%v22875
%v22883 = vmul.f32 1.4140625, %v22879
%v22886 = vpack.c.bf16 %v120417, %v22883
%119863 = vst [vmem:[%s280 + $0x394] sm:$0xf] /*vst_source=*/%v22886
%v22924 = vadd.s32 %v22921, %v408
%v22934 = vadd.s32 %v22924, %v415
%vm22938 = vcmp.lt.u32.totalorder %v22934, %v22924
%vm22943 = vcmp.lt.u32.totalorder %v22924, %v408
%v22948 = vadd.s32 %v22904, %v380
%v22952 = vadd.s32 1, %v22948
%v22956 = vsel /*vm=*/%vm22943, /*on_true_vy=*/%v22952, /*on_false_vx=*/%v22948
%v22960 = vadd.s32 1, %v22956
%v22964 = vsel /*vm=*/%vm22938, /*on_true_vy=*/%v22960, /*on_false_vx=*/%v22956
%v22969 = vadd.s32 %v22964, %v10
%v22973 = vadd.s32 %v22934, %v9
%v22977 = vadd.s32 %v22973, %v22969
%v22979 = vshll.u32 %v22973, 13
%v22980 = vshrl.u32 %v22973, 19
%v22981 = vor.u32 %v22980, %v22979
%v22982 = vxor.u32 %v22981, %v22977
%v22985 = vadd.s32 %v22982, %v22977
%v22987 = vshll.u32 %v22982, 15
%v22988 = vshrl.u32 %v22982, 17
%v22989 = vor.u32 %v22988, %v22987
%v22990 = vxor.u32 %v22989, %v22985
%v22993 = vadd.s32 %v22990, %v22985
%v22995 = vshll.u32 %v22990, 26
%v22996 = vshrl.u32 %v22990, 6
%v22997 = vor.u32 %v22996, %v22995
%v22998 = vxor.u32 %v22997, %v22993
%v23001 = vadd.s32 %v22998, %v22993
%v23005 = vadd.s32 %v23001, %v9
%v23007 = vshll.u32 %v22998, 6
%v23008 = vshrl.u32 %v22998, 26
%v23009 = vor.u32 %v23008, %v23007
%v23010 = vxor.u32 %v23009, %v23001
%v23013 = vadd.s32 %v23010, %v8
%v23017 = vadd.s32 1, %v23013
%v23021 = vadd.s32 %v23017, %v23005
%v23023 = vshll.u32 %v23017, 17
%v23024 = vshrl.u32 %v23017, 15
%v23025 = vor.u32 %v23024, %v23023
%v23026 = vxor.u32 %v23025, %v23021
%v23029 = vadd.s32 %v23026, %v23021
%v23031 = vshll.u32 %v23026, 29
%v23032 = vshrl.u32 %v23026, 3
%v23033 = vor.u32 %v23032, %v23031
%v23034 = vxor.u32 %v23033, %v23029
%v23037 = vadd.s32 %v23034, %v23029
%v23039 = vshll.u32 %v23034, 16
%v23040 = vshrl.u32 %v23034, 16
%v23041 = vor.u32 %v23040, %v23039
%v23042 = vxor.u32 %v23041, %v23037
%v23045 = vadd.s32 %v23042, %v23037
%v23049 = vadd.s32 %v23045, %v8
%v23051 = vshll.u32 %v23042, 24
%v23052 = vshrl.u32 %v23042, 8
%v23053 = vor.u32 %v23052, %v23051
%v23054 = vxor.u32 %v23053, %v23045
%v23057 = vadd.s32 %v23054, %v10
%v23061 = vadd.s32 2, %v23057
%v23065 = vadd.s32 %v23061, %v23049
%v23067 = vshll.u32 %v23061, 13
%v23068 = vshrl.u32 %v23061, 19
%v23069 = vor.u32 %v23068, %v23067
%v23070 = vxor.u32 %v23069, %v23065
%v23073 = vadd.s32 %v23070, %v23065
%v23075 = vshll.u32 %v23070, 15
%v23076 = vshrl.u32 %v23070, 17
%v23077 = vor.u32 %v23076, %v23075
%v23078 = vxor.u32 %v23077, %v23073
%v23081 = vadd.s32 %v23078, %v23073
%v23083 = vshll.u32 %v23078, 26
%v23084 = vshrl.u32 %v23078, 6
%v23085 = vor.u32 %v23084, %v23083
%v23086 = vxor.u32 %v23085, %v23081
%v23089 = vadd.s32 %v23086, %v23081
%v23093 = vadd.s32 %v23089, %v10
%v23095 = vshll.u32 %v23086, 6
%v23096 = vshrl.u32 %v23086, 26
%v23097 = vor.u32 %v23096, %v23095
%v23098 = vxor.u32 %v23097, %v23089
%v23101 = vadd.s32 %v23098, %v9
%v23105 = vadd.s32 3, %v23101
%v23109 = vadd.s32 %v23105, %v23093
%v23111 = vshll.u32 %v23105, 17
%v23112 = vshrl.u32 %v23105, 15
%v23113 = vor.u32 %v23112, %v23111
%v23114 = vxor.u32 %v23113, %v23109
%v23117 = vadd.s32 %v23114, %v23109
%v23119 = vshll.u32 %v23114, 29
%v23120 = vshrl.u32 %v23114, 3
%v23121 = vor.u32 %v23120, %v23119
%v23122 = vxor.u32 %v23121, %v23117
%v23125 = vadd.s32 %v23122, %v23117
%v23127 = vshll.u32 %v23122, 16
%v23128 = vshrl.u32 %v23122, 16
%v23129 = vor.u32 %v23128, %v23127
%v23130 = vxor.u32 %v23129, %v23125
%v23133 = vadd.s32 %v23130, %v23125
%v23137 = vadd.s32 %v23133, %v9
%v23139 = vshll.u32 %v23130, 24
%v23140 = vshrl.u32 %v23130, 8
%v23141 = vor.u32 %v23140, %v23139
%v23142 = vxor.u32 %v23141, %v23133
%v23145 = vadd.s32 %v23142, %v8
%v23149 = vadd.s32 4, %v23145
%v23153 = vadd.s32 %v23149, %v23137
%v23155 = vshll.u32 %v23149, 13
%v23156 = vshrl.u32 %v23149, 19
%v23157 = vor.u32 %v23156, %v23155
%v23158 = vxor.u32 %v23157, %v23153
%v23161 = vadd.s32 %v23158, %v23153
%v23163 = vshll.u32 %v23158, 15
%v23164 = vshrl.u32 %v23158, 17
%v23165 = vor.u32 %v23164, %v23163
%v23166 = vxor.u32 %v23165, %v23161
%v23169 = vadd.s32 %v23166, %v23161
%v23171 = vshll.u32 %v23166, 26
%v23172 = vshrl.u32 %v23166, 6
%v23173 = vor.u32 %v23172, %v23171
%v23174 = vxor.u32 %v23173, %v23169
%v23177 = vadd.s32 %v23174, %v23169
%v23181 = vadd.s32 %v23177, %v8
%v23183 = vshll.u32 %v23174, 6
%v23184 = vshrl.u32 %v23174, 26
%v23185 = vor.u32 %v23184, %v23183
%v23186 = vxor.u32 %v23185, %v23177
%v23189 = vadd.s32 %v23186, %v10
%v23193 = vadd.s32 5, %v23189
%v23195 = vxor.u32 %v23193, %v23181
%v23196 = vand.u32.u8 255, %v23195
%v23197 = vand.u32 65535, %v23196
%v23198 = vshrl.u32 %v23197, 1
%v23199 = vor.u32 16256, %v23198
%v23200 = vand.u32.u16 65535, %v23199
%v119868 = vadd.low.f32.bf16 -1.0, %v23200
%v23209 = vmul.f32 2.0, %v119868
%v23213 = vadd.f32 -0.99609375, %v23209
%v23217 = vmax.f32 %v23213, -0.99609375
%v23219 = vand.u32 2147483647, %v23217
%vm23222 = vcmp.eq.f32.partialorder %v23219, 1.0
%v23227 = vmul.f32 inf, %v23217
%v23229 = vxor.u32 2147483648, %v23217
%v23232 = vmul.f32 %v23229, %v23217
%v23234 = vadd.f32 1.0, %v23232
%v23235 = vlog2.pop %v23234
%v23236 = vmul.f32 0.6931472, %v23235
%v23237 = vmul.f32 -0.5, %v23232
%v23238 = vadd.f32 1.0, %v23237
%v23239 = vmul.f32 %v23238, %v23232
%v23240 = vand.u32 2147483647, %v23232
%vm23241 = vcmp.lt.f32.partialorder %v23240, 0.0004427343
%v23242 = vsel /*vm=*/%vm23241, /*on_true_vy=*/%v23239, /*on_false_vx=*/%v23236
%v23243 = vxor.u32 2147483648, %v23242
%vm23246 = vcmp.lt.f32.partialorder %v23243, 5.0
%v23251 = vsel /*vm=*/%vm23246, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v23255 = vsel /*vm=*/%vm23246, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v23259 = vsel /*vm=*/%vm23246, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v23263 = vsel /*vm=*/%vm23246, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v23267 = vsel /*vm=*/%vm23246, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v23271 = vsel /*vm=*/%vm23246, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v23275 = vsel /*vm=*/%vm23246, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v23279 = vsel /*vm=*/%vm23246, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v23283 = vsel /*vm=*/%vm23246, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v23287 = vadd.f32 -2.5, %v23243
%v23289 = vrsqrt.pop %v23243
%v23290 = vmul.f32 %v23289, %v23243
%vm23291 = vcmp.eq.f32.partialorder %v23243, inf
%v23292 = vsel /*vm=*/%vm23291, /*on_true_vy=*/%v23243, /*on_false_vx=*/%v23290
%vm23293 = vcmp.eq.f32.partialorder %v23243, 0.0
%v23294 = vand.u32 2147483648, %v23243
%v23295 = vsel /*vm=*/%vm23293, /*on_true_vy=*/%v23294, /*on_false_vx=*/%v23292
%v23298 = vadd.f32 -3.0, %v23295
%v23302 = vsel /*vm=*/%vm23246, /*on_true_vy=*/%v23287, /*on_false_vx=*/%v23298
%v23306 = vmul.f32 %v23302, %v23283
%v23310 = vadd.f32 %v23306, %v23279
%v23314 = vmul.f32 %v23310, %v23302
%v23318 = vadd.f32 %v23314, %v23275
%v23322 = vmul.f32 %v23318, %v23302
%v23326 = vadd.f32 %v23322, %v23271
%v23330 = vmul.f32 %v23326, %v23302
%v23334 = vadd.f32 %v23330, %v23267
%v23338 = vmul.f32 %v23334, %v23302
%v23342 = vadd.f32 %v23338, %v23263
%v23346 = vmul.f32 %v23342, %v23302
%v23350 = vadd.f32 %v23346, %v23259
%v23354 = vmul.f32 %v23350, %v23302
%v23358 = vadd.f32 %v23354, %v23255
%v23362 = vmul.f32 %v23358, %v23302
%v23366 = vadd.f32 %v23362, %v23251
%v23370 = vmul.f32 %v23366, %v23217
%v23374 = vsel /*vm=*/%vm23222, /*on_true_vy=*/%v23227, /*on_false_vx=*/%v23370
%v23378 = vmul.f32 1.4140625, %v23374
%v23381 = vpack.c.bf16 %v120417, %v23378
%119869 = vst [vmem:[%s280 + $0x18] sm:$0xf] /*vst_source=*/%v23381
%v23385 = vadd.s32 %v22921, %v894
%v23395 = vadd.s32 %v23385, %v415
%vm23399 = vcmp.lt.u32.totalorder %v23395, %v23385
%vm23404 = vcmp.lt.u32.totalorder %v23385, %v894
%v23409 = vadd.s32 %v22904, %v881
%v23413 = vadd.s32 1, %v23409
%v23417 = vsel /*vm=*/%vm23404, /*on_true_vy=*/%v23413, /*on_false_vx=*/%v23409
%v23421 = vadd.s32 1, %v23417
%v23425 = vsel /*vm=*/%vm23399, /*on_true_vy=*/%v23421, /*on_false_vx=*/%v23417
%v23430 = vadd.s32 %v23425, %v10
%v23434 = vadd.s32 %v23395, %v9
%v23438 = vadd.s32 %v23434, %v23430
%v23440 = vshll.u32 %v23434, 13
%v23441 = vshrl.u32 %v23434, 19
%v23442 = vor.u32 %v23441, %v23440
%v23443 = vxor.u32 %v23442, %v23438
%v23446 = vadd.s32 %v23443, %v23438
%v23448 = vshll.u32 %v23443, 15
%v23449 = vshrl.u32 %v23443, 17
%v23450 = vor.u32 %v23449, %v23448
%v23451 = vxor.u32 %v23450, %v23446
%v23454 = vadd.s32 %v23451, %v23446
%v23456 = vshll.u32 %v23451, 26
%v23457 = vshrl.u32 %v23451, 6
%v23458 = vor.u32 %v23457, %v23456
%v23459 = vxor.u32 %v23458, %v23454
%v23462 = vadd.s32 %v23459, %v23454
%v23466 = vadd.s32 %v23462, %v9
%v23468 = vshll.u32 %v23459, 6
%v23469 = vshrl.u32 %v23459, 26
%v23470 = vor.u32 %v23469, %v23468
%v23471 = vxor.u32 %v23470, %v23462
%v23474 = vadd.s32 %v23471, %v8
%v23478 = vadd.s32 1, %v23474
%v23482 = vadd.s32 %v23478, %v23466
%v23484 = vshll.u32 %v23478, 17
%v23485 = vshrl.u32 %v23478, 15
%v23486 = vor.u32 %v23485, %v23484
%v23487 = vxor.u32 %v23486, %v23482
%v23490 = vadd.s32 %v23487, %v23482
%v23492 = vshll.u32 %v23487, 29
%v23493 = vshrl.u32 %v23487, 3
%v23494 = vor.u32 %v23493, %v23492
%v23495 = vxor.u32 %v23494, %v23490
%v23498 = vadd.s32 %v23495, %v23490
%v23500 = vshll.u32 %v23495, 16
%v23501 = vshrl.u32 %v23495, 16
%v23502 = vor.u32 %v23501, %v23500
%v23503 = vxor.u32 %v23502, %v23498
%v23506 = vadd.s32 %v23503, %v23498
%v23510 = vadd.s32 %v23506, %v8
%v23512 = vshll.u32 %v23503, 24
%v23513 = vshrl.u32 %v23503, 8
%v23514 = vor.u32 %v23513, %v23512
%v23515 = vxor.u32 %v23514, %v23506
%v23518 = vadd.s32 %v23515, %v10
%v23522 = vadd.s32 2, %v23518
%v23526 = vadd.s32 %v23522, %v23510
%v23528 = vshll.u32 %v23522, 13
%v23529 = vshrl.u32 %v23522, 19
%v23530 = vor.u32 %v23529, %v23528
%v23531 = vxor.u32 %v23530, %v23526
%v23534 = vadd.s32 %v23531, %v23526
%v23536 = vshll.u32 %v23531, 15
%v23537 = vshrl.u32 %v23531, 17
%v23538 = vor.u32 %v23537, %v23536
%v23539 = vxor.u32 %v23538, %v23534
%v23542 = vadd.s32 %v23539, %v23534
%v23544 = vshll.u32 %v23539, 26
%v23545 = vshrl.u32 %v23539, 6
%v23546 = vor.u32 %v23545, %v23544
%v23547 = vxor.u32 %v23546, %v23542
%v23550 = vadd.s32 %v23547, %v23542
%v23554 = vadd.s32 %v23550, %v10
%v23556 = vshll.u32 %v23547, 6
%v23557 = vshrl.u32 %v23547, 26
%v23558 = vor.u32 %v23557, %v23556
%v23559 = vxor.u32 %v23558, %v23550
%v23562 = vadd.s32 %v23559, %v9
%v23566 = vadd.s32 3, %v23562
%v23570 = vadd.s32 %v23566, %v23554
%v23572 = vshll.u32 %v23566, 17
%v23573 = vshrl.u32 %v23566, 15
%v23574 = vor.u32 %v23573, %v23572
%v23575 = vxor.u32 %v23574, %v23570
%v23578 = vadd.s32 %v23575, %v23570
%v23580 = vshll.u32 %v23575, 29
%v23581 = vshrl.u32 %v23575, 3
%v23582 = vor.u32 %v23581, %v23580
%v23583 = vxor.u32 %v23582, %v23578
%v23586 = vadd.s32 %v23583, %v23578
%v23588 = vshll.u32 %v23583, 16
%v23589 = vshrl.u32 %v23583, 16
%v23590 = vor.u32 %v23589, %v23588
%v23591 = vxor.u32 %v23590, %v23586
%v23594 = vadd.s32 %v23591, %v23586
%v23598 = vadd.s32 %v23594, %v9
%v23600 = vshll.u32 %v23591, 24
%v23601 = vshrl.u32 %v23591, 8
%v23602 = vor.u32 %v23601, %v23600
%v23603 = vxor.u32 %v23602, %v23594
%v23606 = vadd.s32 %v23603, %v8
%v23610 = vadd.s32 4, %v23606
%v23614 = vadd.s32 %v23610, %v23598
%v23616 = vshll.u32 %v23610, 13
%v23617 = vshrl.u32 %v23610, 19
%v23618 = vor.u32 %v23617, %v23616
%v23619 = vxor.u32 %v23618, %v23614
%v23622 = vadd.s32 %v23619, %v23614
%v23624 = vshll.u32 %v23619, 15
%v23625 = vshrl.u32 %v23619, 17
%v23626 = vor.u32 %v23625, %v23624
%v23627 = vxor.u32 %v23626, %v23622
%v23630 = vadd.s32 %v23627, %v23622
%v23632 = vshll.u32 %v23627, 26
%v23633 = vshrl.u32 %v23627, 6
%v23634 = vor.u32 %v23633, %v23632
%v23635 = vxor.u32 %v23634, %v23630
%v23638 = vadd.s32 %v23635, %v23630
%v23642 = vadd.s32 %v23638, %v8
%v23644 = vshll.u32 %v23635, 6
%v23645 = vshrl.u32 %v23635, 26
%v23646 = vor.u32 %v23645, %v23644
%v23647 = vxor.u32 %v23646, %v23638
%v23650 = vadd.s32 %v23647, %v10
%v23654 = vadd.s32 5, %v23650
%v23656 = vxor.u32 %v23654, %v23642
%v23657 = vand.u32.u8 255, %v23656
%v23658 = vand.u32 65535, %v23657
%v23659 = vshrl.u32 %v23658, 1
%v23660 = vor.u32 16256, %v23659
%v23661 = vand.u32.u16 65535, %v23660
%v119870 = vadd.low.f32.bf16 -1.0, %v23661
%v23670 = vmul.f32 2.0, %v119870
%v23674 = vadd.f32 -0.99609375, %v23670
%v23678 = vmax.f32 %v23674, -0.99609375
%v23680 = vand.u32 2147483647, %v23678
%vm23683 = vcmp.eq.f32.partialorder %v23680, 1.0
%v23688 = vmul.f32 inf, %v23678
%v23690 = vxor.u32 2147483648, %v23678
%v23693 = vmul.f32 %v23690, %v23678
%v23695 = vadd.f32 1.0, %v23693
%v23696 = vlog2.pop %v23695
%v23697 = vmul.f32 0.6931472, %v23696
%v23698 = vmul.f32 -0.5, %v23693
%v23699 = vadd.f32 1.0, %v23698
%v23700 = vmul.f32 %v23699, %v23693
%v23701 = vand.u32 2147483647, %v23693
%vm23702 = vcmp.lt.f32.partialorder %v23701, 0.0004427343
%v23703 = vsel /*vm=*/%vm23702, /*on_true_vy=*/%v23700, /*on_false_vx=*/%v23697
%v23704 = vxor.u32 2147483648, %v23703
%vm23707 = vcmp.lt.f32.partialorder %v23704, 5.0
%v23712 = vsel /*vm=*/%vm23707, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v23716 = vsel /*vm=*/%vm23707, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v23720 = vsel /*vm=*/%vm23707, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v23724 = vsel /*vm=*/%vm23707, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v23728 = vsel /*vm=*/%vm23707, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v23732 = vsel /*vm=*/%vm23707, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v23736 = vsel /*vm=*/%vm23707, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v23740 = vsel /*vm=*/%vm23707, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v23744 = vsel /*vm=*/%vm23707, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v23748 = vadd.f32 -2.5, %v23704
%v23750 = vrsqrt.pop %v23704
%v23751 = vmul.f32 %v23750, %v23704
%vm23752 = vcmp.eq.f32.partialorder %v23704, inf
%v23753 = vsel /*vm=*/%vm23752, /*on_true_vy=*/%v23704, /*on_false_vx=*/%v23751
%vm23754 = vcmp.eq.f32.partialorder %v23704, 0.0
%v23755 = vand.u32 2147483648, %v23704
%v23756 = vsel /*vm=*/%vm23754, /*on_true_vy=*/%v23755, /*on_false_vx=*/%v23753
%v23759 = vadd.f32 -3.0, %v23756
%v23763 = vsel /*vm=*/%vm23707, /*on_true_vy=*/%v23748, /*on_false_vx=*/%v23759
%v23767 = vmul.f32 %v23763, %v23744
%v23771 = vadd.f32 %v23767, %v23740
%v23775 = vmul.f32 %v23771, %v23763
%v23779 = vadd.f32 %v23775, %v23736
%v23783 = vmul.f32 %v23779, %v23763
%v23787 = vadd.f32 %v23783, %v23732
%v23791 = vmul.f32 %v23787, %v23763
%v23795 = vadd.f32 %v23791, %v23728
%v23799 = vmul.f32 %v23795, %v23763
%v23803 = vadd.f32 %v23799, %v23724
%v23807 = vmul.f32 %v23803, %v23763
%v23811 = vadd.f32 %v23807, %v23720
%v23815 = vmul.f32 %v23811, %v23763
%v23819 = vadd.f32 %v23815, %v23716
%v23823 = vmul.f32 %v23819, %v23763
%v23827 = vadd.f32 %v23823, %v23712
%v23831 = vmul.f32 %v23827, %v23678
%v23835 = vsel /*vm=*/%vm23683, /*on_true_vy=*/%v23688, /*on_false_vx=*/%v23831
%v23839 = vmul.f32 1.4140625, %v23835
%v23842 = vpack.c.bf16 %v120417, %v23839
%119871 = vst [vmem:[%s280 + $0x98] sm:$0xf] /*vst_source=*/%v23842
%v23846 = vadd.s32 %v22921, %v1381
%v23856 = vadd.s32 %v23846, %v415
%vm23860 = vcmp.lt.u32.totalorder %v23856, %v23846
%vm23865 = vcmp.lt.u32.totalorder %v23846, %v1381
%v23870 = vadd.s32 %v22904, %v1368
%v23874 = vadd.s32 1, %v23870
%v23878 = vsel /*vm=*/%vm23865, /*on_true_vy=*/%v23874, /*on_false_vx=*/%v23870
%v23882 = vadd.s32 1, %v23878
%v23886 = vsel /*vm=*/%vm23860, /*on_true_vy=*/%v23882, /*on_false_vx=*/%v23878
%v23891 = vadd.s32 %v23886, %v10
%v23895 = vadd.s32 %v23856, %v9
%v23899 = vadd.s32 %v23895, %v23891
%v23901 = vshll.u32 %v23895, 13
%v23902 = vshrl.u32 %v23895, 19
%v23903 = vor.u32 %v23902, %v23901
%v23904 = vxor.u32 %v23903, %v23899
%v23907 = vadd.s32 %v23904, %v23899
%v23909 = vshll.u32 %v23904, 15
%v23910 = vshrl.u32 %v23904, 17
%v23911 = vor.u32 %v23910, %v23909
%v23912 = vxor.u32 %v23911, %v23907
%v23915 = vadd.s32 %v23912, %v23907
%v23917 = vshll.u32 %v23912, 26
%v23918 = vshrl.u32 %v23912, 6
%v23919 = vor.u32 %v23918, %v23917
%v23920 = vxor.u32 %v23919, %v23915
%v23923 = vadd.s32 %v23920, %v23915
%v23927 = vadd.s32 %v23923, %v9
%v23929 = vshll.u32 %v23920, 6
%v23930 = vshrl.u32 %v23920, 26
%v23931 = vor.u32 %v23930, %v23929
%v23932 = vxor.u32 %v23931, %v23923
%v23935 = vadd.s32 %v23932, %v8
%v23939 = vadd.s32 1, %v23935
%v23943 = vadd.s32 %v23939, %v23927
%v23945 = vshll.u32 %v23939, 17
%v23946 = vshrl.u32 %v23939, 15
%v23947 = vor.u32 %v23946, %v23945
%v23948 = vxor.u32 %v23947, %v23943
%v23951 = vadd.s32 %v23948, %v23943
%v23953 = vshll.u32 %v23948, 29
%v23954 = vshrl.u32 %v23948, 3
%v23955 = vor.u32 %v23954, %v23953
%v23956 = vxor.u32 %v23955, %v23951
%v23959 = vadd.s32 %v23956, %v23951
%v23961 = vshll.u32 %v23956, 16
%v23962 = vshrl.u32 %v23956, 16
%v23963 = vor.u32 %v23962, %v23961
%v23964 = vxor.u32 %v23963, %v23959
%v23967 = vadd.s32 %v23964, %v23959
%v23971 = vadd.s32 %v23967, %v8
%v23973 = vshll.u32 %v23964, 24
%v23974 = vshrl.u32 %v23964, 8
%v23975 = vor.u32 %v23974, %v23973
%v23976 = vxor.u32 %v23975, %v23967
%v23979 = vadd.s32 %v23976, %v10
%v23983 = vadd.s32 2, %v23979
%v23987 = vadd.s32 %v23983, %v23971
%v23989 = vshll.u32 %v23983, 13
%v23990 = vshrl.u32 %v23983, 19
%v23991 = vor.u32 %v23990, %v23989
%v23992 = vxor.u32 %v23991, %v23987
%v23995 = vadd.s32 %v23992, %v23987
%v23997 = vshll.u32 %v23992, 15
%v23998 = vshrl.u32 %v23992, 17
%v23999 = vor.u32 %v23998, %v23997
%v24000 = vxor.u32 %v23999, %v23995
%v24003 = vadd.s32 %v24000, %v23995
%v24005 = vshll.u32 %v24000, 26
%v24006 = vshrl.u32 %v24000, 6
%v24007 = vor.u32 %v24006, %v24005
%v24008 = vxor.u32 %v24007, %v24003
%v24011 = vadd.s32 %v24008, %v24003
%v24015 = vadd.s32 %v24011, %v10
%v24017 = vshll.u32 %v24008, 6
%v24018 = vshrl.u32 %v24008, 26
%v24019 = vor.u32 %v24018, %v24017
%v24020 = vxor.u32 %v24019, %v24011
%v24023 = vadd.s32 %v24020, %v9
%v24027 = vadd.s32 3, %v24023
%v24031 = vadd.s32 %v24027, %v24015
%v24033 = vshll.u32 %v24027, 17
%v24034 = vshrl.u32 %v24027, 15
%v24035 = vor.u32 %v24034, %v24033
%v24036 = vxor.u32 %v24035, %v24031
%v24039 = vadd.s32 %v24036, %v24031
%v24041 = vshll.u32 %v24036, 29
%v24042 = vshrl.u32 %v24036, 3
%v24043 = vor.u32 %v24042, %v24041
%v24044 = vxor.u32 %v24043, %v24039
%v24047 = vadd.s32 %v24044, %v24039
%v24049 = vshll.u32 %v24044, 16
%v24050 = vshrl.u32 %v24044, 16
%v24051 = vor.u32 %v24050, %v24049
%v24052 = vxor.u32 %v24051, %v24047
%v24055 = vadd.s32 %v24052, %v24047
%v24059 = vadd.s32 %v24055, %v9
%v24061 = vshll.u32 %v24052, 24
%v24062 = vshrl.u32 %v24052, 8
%v24063 = vor.u32 %v24062, %v24061
%v24064 = vxor.u32 %v24063, %v24055
%v24067 = vadd.s32 %v24064, %v8
%v24071 = vadd.s32 4, %v24067
%v24075 = vadd.s32 %v24071, %v24059
%v24077 = vshll.u32 %v24071, 13
%v24078 = vshrl.u32 %v24071, 19
%v24079 = vor.u32 %v24078, %v24077
%v24080 = vxor.u32 %v24079, %v24075
%v24083 = vadd.s32 %v24080, %v24075
%v24085 = vshll.u32 %v24080, 15
%v24086 = vshrl.u32 %v24080, 17
%v24087 = vor.u32 %v24086, %v24085
%v24088 = vxor.u32 %v24087, %v24083
%v24091 = vadd.s32 %v24088, %v24083
%v24093 = vshll.u32 %v24088, 26
%v24094 = vshrl.u32 %v24088, 6
%v24095 = vor.u32 %v24094, %v24093
%v24096 = vxor.u32 %v24095, %v24091
%v24099 = vadd.s32 %v24096, %v24091
%v24103 = vadd.s32 %v24099, %v8
%v24105 = vshll.u32 %v24096, 6
%v24106 = vshrl.u32 %v24096, 26
%v24107 = vor.u32 %v24106, %v24105
%v24108 = vxor.u32 %v24107, %v24099
%v24111 = vadd.s32 %v24108, %v10
%v24115 = vadd.s32 5, %v24111
%v24117 = vxor.u32 %v24115, %v24103
%v24118 = vand.u32.u8 255, %v24117
%v24119 = vand.u32 65535, %v24118
%v24120 = vshrl.u32 %v24119, 1
%v24121 = vor.u32 16256, %v24120
%v24122 = vand.u32.u16 65535, %v24121
%v119872 = vadd.low.f32.bf16 -1.0, %v24122
%v24131 = vmul.f32 2.0, %v119872
%v24135 = vadd.f32 -0.99609375, %v24131
%v24139 = vmax.f32 %v24135, -0.99609375
%v24141 = vand.u32 2147483647, %v24139
%vm24144 = vcmp.eq.f32.partialorder %v24141, 1.0
%v24149 = vmul.f32 inf, %v24139
%v24151 = vxor.u32 2147483648, %v24139
%v24154 = vmul.f32 %v24151, %v24139
%v24156 = vadd.f32 1.0, %v24154
%v24157 = vlog2.pop %v24156
%v24158 = vmul.f32 0.6931472, %v24157
%v24159 = vmul.f32 -0.5, %v24154
%v24160 = vadd.f32 1.0, %v24159
%v24161 = vmul.f32 %v24160, %v24154
%v24162 = vand.u32 2147483647, %v24154
%vm24163 = vcmp.lt.f32.partialorder %v24162, 0.0004427343
%v24164 = vsel /*vm=*/%vm24163, /*on_true_vy=*/%v24161, /*on_false_vx=*/%v24158
%v24165 = vxor.u32 2147483648, %v24164
%vm24168 = vcmp.lt.f32.partialorder %v24165, 5.0
%v24173 = vsel /*vm=*/%vm24168, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v24177 = vsel /*vm=*/%vm24168, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v24181 = vsel /*vm=*/%vm24168, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v24185 = vsel /*vm=*/%vm24168, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v24189 = vsel /*vm=*/%vm24168, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v24193 = vsel /*vm=*/%vm24168, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v24197 = vsel /*vm=*/%vm24168, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v24201 = vsel /*vm=*/%vm24168, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v24205 = vsel /*vm=*/%vm24168, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v24209 = vadd.f32 -2.5, %v24165
%v24211 = vrsqrt.pop %v24165
%v24212 = vmul.f32 %v24211, %v24165
%vm24213 = vcmp.eq.f32.partialorder %v24165, inf
%v24214 = vsel /*vm=*/%vm24213, /*on_true_vy=*/%v24165, /*on_false_vx=*/%v24212
%vm24215 = vcmp.eq.f32.partialorder %v24165, 0.0
%v24216 = vand.u32 2147483648, %v24165
%v24217 = vsel /*vm=*/%vm24215, /*on_true_vy=*/%v24216, /*on_false_vx=*/%v24214
%v24220 = vadd.f32 -3.0, %v24217
%v24224 = vsel /*vm=*/%vm24168, /*on_true_vy=*/%v24209, /*on_false_vx=*/%v24220
%v24228 = vmul.f32 %v24224, %v24205
%v24232 = vadd.f32 %v24228, %v24201
%v24236 = vmul.f32 %v24232, %v24224
%v24240 = vadd.f32 %v24236, %v24197
%v24244 = vmul.f32 %v24240, %v24224
%v24248 = vadd.f32 %v24244, %v24193
%v24252 = vmul.f32 %v24248, %v24224
%v24256 = vadd.f32 %v24252, %v24189
%v24260 = vmul.f32 %v24256, %v24224
%v24264 = vadd.f32 %v24260, %v24185
%v24268 = vmul.f32 %v24264, %v24224
%v24272 = vadd.f32 %v24268, %v24181
%v24276 = vmul.f32 %v24272, %v24224
%v24280 = vadd.f32 %v24276, %v24177
%v24284 = vmul.f32 %v24280, %v24224
%v24288 = vadd.f32 %v24284, %v24173
%v24292 = vmul.f32 %v24288, %v24139
%v24296 = vsel /*vm=*/%vm24144, /*on_true_vy=*/%v24149, /*on_false_vx=*/%v24292
%v24300 = vmul.f32 1.4140625, %v24296
%v24303 = vpack.c.bf16 %v120417, %v24300
%119873 = vst [vmem:[%s280 + $0x118] sm:$0xf] /*vst_source=*/%v24303
%v24307 = vadd.s32 %v22921, %v1868
%v24317 = vadd.s32 %v24307, %v415
%vm24321 = vcmp.lt.u32.totalorder %v24317, %v24307
%vm24326 = vcmp.lt.u32.totalorder %v24307, %v1868
%v24331 = vadd.s32 %v22904, %v1855
%v24335 = vadd.s32 1, %v24331
%v24339 = vsel /*vm=*/%vm24326, /*on_true_vy=*/%v24335, /*on_false_vx=*/%v24331
%v24343 = vadd.s32 1, %v24339
%v24347 = vsel /*vm=*/%vm24321, /*on_true_vy=*/%v24343, /*on_false_vx=*/%v24339
%v24352 = vadd.s32 %v24347, %v10
%v24356 = vadd.s32 %v24317, %v9
%v24360 = vadd.s32 %v24356, %v24352
%v24362 = vshll.u32 %v24356, 13
%v24363 = vshrl.u32 %v24356, 19
%v24364 = vor.u32 %v24363, %v24362
%v24365 = vxor.u32 %v24364, %v24360
%v24368 = vadd.s32 %v24365, %v24360
%v24370 = vshll.u32 %v24365, 15
%v24371 = vshrl.u32 %v24365, 17
%v24372 = vor.u32 %v24371, %v24370
%v24373 = vxor.u32 %v24372, %v24368
%v24376 = vadd.s32 %v24373, %v24368
%v24378 = vshll.u32 %v24373, 26
%v24379 = vshrl.u32 %v24373, 6
%v24380 = vor.u32 %v24379, %v24378
%v24381 = vxor.u32 %v24380, %v24376
%v24384 = vadd.s32 %v24381, %v24376
%v24388 = vadd.s32 %v24384, %v9
%v24390 = vshll.u32 %v24381, 6
%v24391 = vshrl.u32 %v24381, 26
%v24392 = vor.u32 %v24391, %v24390
%v24393 = vxor.u32 %v24392, %v24384
%v24396 = vadd.s32 %v24393, %v8
%v24400 = vadd.s32 1, %v24396
%v24404 = vadd.s32 %v24400, %v24388
%v24406 = vshll.u32 %v24400, 17
%v24407 = vshrl.u32 %v24400, 15
%v24408 = vor.u32 %v24407, %v24406
%v24409 = vxor.u32 %v24408, %v24404
%v24412 = vadd.s32 %v24409, %v24404
%v24414 = vshll.u32 %v24409, 29
%v24415 = vshrl.u32 %v24409, 3
%v24416 = vor.u32 %v24415, %v24414
%v24417 = vxor.u32 %v24416, %v24412
%v24420 = vadd.s32 %v24417, %v24412
%v24422 = vshll.u32 %v24417, 16
%v24423 = vshrl.u32 %v24417, 16
%v24424 = vor.u32 %v24423, %v24422
%v24425 = vxor.u32 %v24424, %v24420
%v24428 = vadd.s32 %v24425, %v24420
%v24432 = vadd.s32 %v24428, %v8
%v24434 = vshll.u32 %v24425, 24
%v24435 = vshrl.u32 %v24425, 8
%v24436 = vor.u32 %v24435, %v24434
%v24437 = vxor.u32 %v24436, %v24428
%v24440 = vadd.s32 %v24437, %v10
%v24444 = vadd.s32 2, %v24440
%v24448 = vadd.s32 %v24444, %v24432
%v24450 = vshll.u32 %v24444, 13
%v24451 = vshrl.u32 %v24444, 19
%v24452 = vor.u32 %v24451, %v24450
%v24453 = vxor.u32 %v24452, %v24448
%v24456 = vadd.s32 %v24453, %v24448
%v24458 = vshll.u32 %v24453, 15
%v24459 = vshrl.u32 %v24453, 17
%v24460 = vor.u32 %v24459, %v24458
%v24461 = vxor.u32 %v24460, %v24456
%v24464 = vadd.s32 %v24461, %v24456
%v24466 = vshll.u32 %v24461, 26
%v24467 = vshrl.u32 %v24461, 6
%v24468 = vor.u32 %v24467, %v24466
%v24469 = vxor.u32 %v24468, %v24464
%v24472 = vadd.s32 %v24469, %v24464
%v24476 = vadd.s32 %v24472, %v10
%v24478 = vshll.u32 %v24469, 6
%v24479 = vshrl.u32 %v24469, 26
%v24480 = vor.u32 %v24479, %v24478
%v24481 = vxor.u32 %v24480, %v24472
%v24484 = vadd.s32 %v24481, %v9
%v24488 = vadd.s32 3, %v24484
%v24492 = vadd.s32 %v24488, %v24476
%v24494 = vshll.u32 %v24488, 17
%v24495 = vshrl.u32 %v24488, 15
%v24496 = vor.u32 %v24495, %v24494
%v24497 = vxor.u32 %v24496, %v24492
%v24500 = vadd.s32 %v24497, %v24492
%v24502 = vshll.u32 %v24497, 29
%v24503 = vshrl.u32 %v24497, 3
%v24504 = vor.u32 %v24503, %v24502
%v24505 = vxor.u32 %v24504, %v24500
%v24508 = vadd.s32 %v24505, %v24500
%v24510 = vshll.u32 %v24505, 16
%v24511 = vshrl.u32 %v24505, 16
%v24512 = vor.u32 %v24511, %v24510
%v24513 = vxor.u32 %v24512, %v24508
%v24516 = vadd.s32 %v24513, %v24508
%v24520 = vadd.s32 %v24516, %v9
%v24522 = vshll.u32 %v24513, 24
%v24523 = vshrl.u32 %v24513, 8
%v24524 = vor.u32 %v24523, %v24522
%v24525 = vxor.u32 %v24524, %v24516
%v24528 = vadd.s32 %v24525, %v8
%v24532 = vadd.s32 4, %v24528
%v24536 = vadd.s32 %v24532, %v24520
%v24538 = vshll.u32 %v24532, 13
%v24539 = vshrl.u32 %v24532, 19
%v24540 = vor.u32 %v24539, %v24538
%v24541 = vxor.u32 %v24540, %v24536
%v24544 = vadd.s32 %v24541, %v24536
%v24546 = vshll.u32 %v24541, 15
%v24547 = vshrl.u32 %v24541, 17
%v24548 = vor.u32 %v24547, %v24546
%v24549 = vxor.u32 %v24548, %v24544
%v24552 = vadd.s32 %v24549, %v24544
%v24554 = vshll.u32 %v24549, 26
%v24555 = vshrl.u32 %v24549, 6
%v24556 = vor.u32 %v24555, %v24554
%v24557 = vxor.u32 %v24556, %v24552
%v24560 = vadd.s32 %v24557, %v24552
%v24564 = vadd.s32 %v24560, %v8
%v24566 = vshll.u32 %v24557, 6
%v24567 = vshrl.u32 %v24557, 26
%v24568 = vor.u32 %v24567, %v24566
%v24569 = vxor.u32 %v24568, %v24560
%v24572 = vadd.s32 %v24569, %v10
%v24576 = vadd.s32 5, %v24572
%v24578 = vxor.u32 %v24576, %v24564
%v24579 = vand.u32.u8 255, %v24578
%v24580 = vand.u32 65535, %v24579
%v24581 = vshrl.u32 %v24580, 1
%v24582 = vor.u32 16256, %v24581
%v24583 = vand.u32.u16 65535, %v24582
%v119874 = vadd.low.f32.bf16 -1.0, %v24583
%v24592 = vmul.f32 2.0, %v119874
%v24596 = vadd.f32 -0.99609375, %v24592
%v24600 = vmax.f32 %v24596, -0.99609375
%v24602 = vand.u32 2147483647, %v24600
%vm24605 = vcmp.eq.f32.partialorder %v24602, 1.0
%v24610 = vmul.f32 inf, %v24600
%v24612 = vxor.u32 2147483648, %v24600
%v24615 = vmul.f32 %v24612, %v24600
%v24617 = vadd.f32 1.0, %v24615
%v24618 = vlog2.pop %v24617
%v24619 = vmul.f32 0.6931472, %v24618
%v24620 = vmul.f32 -0.5, %v24615
%v24621 = vadd.f32 1.0, %v24620
%v24622 = vmul.f32 %v24621, %v24615
%v24623 = vand.u32 2147483647, %v24615
%vm24624 = vcmp.lt.f32.partialorder %v24623, 0.0004427343
%v24625 = vsel /*vm=*/%vm24624, /*on_true_vy=*/%v24622, /*on_false_vx=*/%v24619
%v24626 = vxor.u32 2147483648, %v24625
%vm24629 = vcmp.lt.f32.partialorder %v24626, 5.0
%v24634 = vsel /*vm=*/%vm24629, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v24638 = vsel /*vm=*/%vm24629, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v24642 = vsel /*vm=*/%vm24629, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v24646 = vsel /*vm=*/%vm24629, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v24650 = vsel /*vm=*/%vm24629, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v24654 = vsel /*vm=*/%vm24629, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v24658 = vsel /*vm=*/%vm24629, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v24662 = vsel /*vm=*/%vm24629, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v24666 = vsel /*vm=*/%vm24629, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v24670 = vadd.f32 -2.5, %v24626
%v24672 = vrsqrt.pop %v24626
%v24673 = vmul.f32 %v24672, %v24626
%vm24674 = vcmp.eq.f32.partialorder %v24626, inf
%v24675 = vsel /*vm=*/%vm24674, /*on_true_vy=*/%v24626, /*on_false_vx=*/%v24673
%vm24676 = vcmp.eq.f32.partialorder %v24626, 0.0
%v24677 = vand.u32 2147483648, %v24626
%v24678 = vsel /*vm=*/%vm24676, /*on_true_vy=*/%v24677, /*on_false_vx=*/%v24675
%v24681 = vadd.f32 -3.0, %v24678
%v24685 = vsel /*vm=*/%vm24629, /*on_true_vy=*/%v24670, /*on_false_vx=*/%v24681
%v24689 = vmul.f32 %v24685, %v24666
%v24693 = vadd.f32 %v24689, %v24662
%v24697 = vmul.f32 %v24693, %v24685
%v24701 = vadd.f32 %v24697, %v24658
%v24705 = vmul.f32 %v24701, %v24685
%v24709 = vadd.f32 %v24705, %v24654
%v24713 = vmul.f32 %v24709, %v24685
%v24717 = vadd.f32 %v24713, %v24650
%v24721 = vmul.f32 %v24717, %v24685
%v24725 = vadd.f32 %v24721, %v24646
%v24729 = vmul.f32 %v24725, %v24685
%v24733 = vadd.f32 %v24729, %v24642
%v24737 = vmul.f32 %v24733, %v24685
%v24741 = vadd.f32 %v24737, %v24638
%v24745 = vmul.f32 %v24741, %v24685
%v24749 = vadd.f32 %v24745, %v24634
%v24753 = vmul.f32 %v24749, %v24600
%v24757 = vsel /*vm=*/%vm24605, /*on_true_vy=*/%v24610, /*on_false_vx=*/%v24753
%v24761 = vmul.f32 1.4140625, %v24757
%v24764 = vpack.c.bf16 %v120417, %v24761
%119875 = vst [vmem:[%s280 + $0x198] sm:$0xf] /*vst_source=*/%v24764
%v24768 = vadd.s32 %v22921, %v2355
%v24778 = vadd.s32 %v24768, %v415
%vm24782 = vcmp.lt.u32.totalorder %v24778, %v24768
%vm24787 = vcmp.lt.u32.totalorder %v24768, %v2355
%v24792 = vadd.s32 %v22904, %v2342
%v24796 = vadd.s32 1, %v24792
%v24800 = vsel /*vm=*/%vm24787, /*on_true_vy=*/%v24796, /*on_false_vx=*/%v24792
%v24804 = vadd.s32 1, %v24800
%v24808 = vsel /*vm=*/%vm24782, /*on_true_vy=*/%v24804, /*on_false_vx=*/%v24800
%v24813 = vadd.s32 %v24808, %v10
%v24817 = vadd.s32 %v24778, %v9
%v24821 = vadd.s32 %v24817, %v24813
%v24823 = vshll.u32 %v24817, 13
%v24824 = vshrl.u32 %v24817, 19
%v24825 = vor.u32 %v24824, %v24823
%v24826 = vxor.u32 %v24825, %v24821
%v24829 = vadd.s32 %v24826, %v24821
%v24831 = vshll.u32 %v24826, 15
%v24832 = vshrl.u32 %v24826, 17
%v24833 = vor.u32 %v24832, %v24831
%v24834 = vxor.u32 %v24833, %v24829
%v24837 = vadd.s32 %v24834, %v24829
%v24839 = vshll.u32 %v24834, 26
%v24840 = vshrl.u32 %v24834, 6
%v24841 = vor.u32 %v24840, %v24839
%v24842 = vxor.u32 %v24841, %v24837
%v24845 = vadd.s32 %v24842, %v24837
%v24849 = vadd.s32 %v24845, %v9
%v24851 = vshll.u32 %v24842, 6
%v24852 = vshrl.u32 %v24842, 26
%v24853 = vor.u32 %v24852, %v24851
%v24854 = vxor.u32 %v24853, %v24845
%v24857 = vadd.s32 %v24854, %v8
%v24861 = vadd.s32 1, %v24857
%v24865 = vadd.s32 %v24861, %v24849
%v24867 = vshll.u32 %v24861, 17
%v24868 = vshrl.u32 %v24861, 15
%v24869 = vor.u32 %v24868, %v24867
%v24870 = vxor.u32 %v24869, %v24865
%v24873 = vadd.s32 %v24870, %v24865
%v24875 = vshll.u32 %v24870, 29
%v24876 = vshrl.u32 %v24870, 3
%v24877 = vor.u32 %v24876, %v24875
%v24878 = vxor.u32 %v24877, %v24873
%v24881 = vadd.s32 %v24878, %v24873
%v24883 = vshll.u32 %v24878, 16
%v24884 = vshrl.u32 %v24878, 16
%v24885 = vor.u32 %v24884, %v24883
%v24886 = vxor.u32 %v24885, %v24881
%v24889 = vadd.s32 %v24886, %v24881
%v24893 = vadd.s32 %v24889, %v8
%v24895 = vshll.u32 %v24886, 24
%v24896 = vshrl.u32 %v24886, 8
%v24897 = vor.u32 %v24896, %v24895
%v24898 = vxor.u32 %v24897, %v24889
%v24901 = vadd.s32 %v24898, %v10
%v24905 = vadd.s32 2, %v24901
%v24909 = vadd.s32 %v24905, %v24893
%v24911 = vshll.u32 %v24905, 13
%v24912 = vshrl.u32 %v24905, 19
%v24913 = vor.u32 %v24912, %v24911
%v24914 = vxor.u32 %v24913, %v24909
%v24917 = vadd.s32 %v24914, %v24909
%v24919 = vshll.u32 %v24914, 15
%v24920 = vshrl.u32 %v24914, 17
%v24921 = vor.u32 %v24920, %v24919
%v24922 = vxor.u32 %v24921, %v24917
%v24925 = vadd.s32 %v24922, %v24917
%v24927 = vshll.u32 %v24922, 26
%v24928 = vshrl.u32 %v24922, 6
%v24929 = vor.u32 %v24928, %v24927
%v24930 = vxor.u32 %v24929, %v24925
%v24933 = vadd.s32 %v24930, %v24925
%v24937 = vadd.s32 %v24933, %v10
%v24939 = vshll.u32 %v24930, 6
%v24940 = vshrl.u32 %v24930, 26
%v24941 = vor.u32 %v24940, %v24939
%v24942 = vxor.u32 %v24941, %v24933
%v24945 = vadd.s32 %v24942, %v9
%v24949 = vadd.s32 3, %v24945
%v24953 = vadd.s32 %v24949, %v24937
%v24955 = vshll.u32 %v24949, 17
%v24956 = vshrl.u32 %v24949, 15
%v24957 = vor.u32 %v24956, %v24955
%v24958 = vxor.u32 %v24957, %v24953
%v24961 = vadd.s32 %v24958, %v24953
%v24963 = vshll.u32 %v24958, 29
%v24964 = vshrl.u32 %v24958, 3
%v24965 = vor.u32 %v24964, %v24963
%v24966 = vxor.u32 %v24965, %v24961
%v24969 = vadd.s32 %v24966, %v24961
%v24971 = vshll.u32 %v24966, 16
%v24972 = vshrl.u32 %v24966, 16
%v24973 = vor.u32 %v24972, %v24971
%v24974 = vxor.u32 %v24973, %v24969
%v24977 = vadd.s32 %v24974, %v24969
%v24981 = vadd.s32 %v24977, %v9
%v24983 = vshll.u32 %v24974, 24
%v24984 = vshrl.u32 %v24974, 8
%v24985 = vor.u32 %v24984, %v24983
%v24986 = vxor.u32 %v24985, %v24977
%v24989 = vadd.s32 %v24986, %v8
%v24993 = vadd.s32 4, %v24989
%v24997 = vadd.s32 %v24993, %v24981
%v24999 = vshll.u32 %v24993, 13
%v25000 = vshrl.u32 %v24993, 19
%v25001 = vor.u32 %v25000, %v24999
%v25002 = vxor.u32 %v25001, %v24997
%v25005 = vadd.s32 %v25002, %v24997
%v25007 = vshll.u32 %v25002, 15
%v25008 = vshrl.u32 %v25002, 17
%v25009 = vor.u32 %v25008, %v25007
%v25010 = vxor.u32 %v25009, %v25005
%v25013 = vadd.s32 %v25010, %v25005
%v25015 = vshll.u32 %v25010, 26
%v25016 = vshrl.u32 %v25010, 6
%v25017 = vor.u32 %v25016, %v25015
%v25018 = vxor.u32 %v25017, %v25013
%v25021 = vadd.s32 %v25018, %v25013
%v25025 = vadd.s32 %v25021, %v8
%v25027 = vshll.u32 %v25018, 6
%v25028 = vshrl.u32 %v25018, 26
%v25029 = vor.u32 %v25028, %v25027
%v25030 = vxor.u32 %v25029, %v25021
%v25033 = vadd.s32 %v25030, %v10
%v25037 = vadd.s32 5, %v25033
%v25039 = vxor.u32 %v25037, %v25025
%v25040 = vand.u32.u8 255, %v25039
%v25041 = vand.u32 65535, %v25040
%v25042 = vshrl.u32 %v25041, 1
%v25043 = vor.u32 16256, %v25042
%v25044 = vand.u32.u16 65535, %v25043
%v119876 = vadd.low.f32.bf16 -1.0, %v25044
%v25053 = vmul.f32 2.0, %v119876
%v25057 = vadd.f32 -0.99609375, %v25053
%v25061 = vmax.f32 %v25057, -0.99609375
%v25063 = vand.u32 2147483647, %v25061
%vm25066 = vcmp.eq.f32.partialorder %v25063, 1.0
%v25071 = vmul.f32 inf, %v25061
%v25073 = vxor.u32 2147483648, %v25061
%v25076 = vmul.f32 %v25073, %v25061
%v25078 = vadd.f32 1.0, %v25076
%v25079 = vlog2.pop %v25078
%v25080 = vmul.f32 0.6931472, %v25079
%v25081 = vmul.f32 -0.5, %v25076
%v25082 = vadd.f32 1.0, %v25081
%v25083 = vmul.f32 %v25082, %v25076
%v25084 = vand.u32 2147483647, %v25076
%vm25085 = vcmp.lt.f32.partialorder %v25084, 0.0004427343
%v25086 = vsel /*vm=*/%vm25085, /*on_true_vy=*/%v25083, /*on_false_vx=*/%v25080
%v25087 = vxor.u32 2147483648, %v25086
%vm25090 = vcmp.lt.f32.partialorder %v25087, 5.0
%v25095 = vsel /*vm=*/%vm25090, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v25099 = vsel /*vm=*/%vm25090, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v25103 = vsel /*vm=*/%vm25090, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v25107 = vsel /*vm=*/%vm25090, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v25111 = vsel /*vm=*/%vm25090, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v25115 = vsel /*vm=*/%vm25090, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v25119 = vsel /*vm=*/%vm25090, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v25123 = vsel /*vm=*/%vm25090, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v25127 = vsel /*vm=*/%vm25090, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v25131 = vadd.f32 -2.5, %v25087
%v25133 = vrsqrt.pop %v25087
%v25134 = vmul.f32 %v25133, %v25087
%vm25135 = vcmp.eq.f32.partialorder %v25087, inf
%v25136 = vsel /*vm=*/%vm25135, /*on_true_vy=*/%v25087, /*on_false_vx=*/%v25134
%vm25137 = vcmp.eq.f32.partialorder %v25087, 0.0
%v25138 = vand.u32 2147483648, %v25087
%v25139 = vsel /*vm=*/%vm25137, /*on_true_vy=*/%v25138, /*on_false_vx=*/%v25136
%v25142 = vadd.f32 -3.0, %v25139
%v25146 = vsel /*vm=*/%vm25090, /*on_true_vy=*/%v25131, /*on_false_vx=*/%v25142
%v25150 = vmul.f32 %v25146, %v25127
%v25154 = vadd.f32 %v25150, %v25123
%v25158 = vmul.f32 %v25154, %v25146
%v25162 = vadd.f32 %v25158, %v25119
%v25166 = vmul.f32 %v25162, %v25146
%v25170 = vadd.f32 %v25166, %v25115
%v25174 = vmul.f32 %v25170, %v25146
%v25178 = vadd.f32 %v25174, %v25111
%v25182 = vmul.f32 %v25178, %v25146
%v25186 = vadd.f32 %v25182, %v25107
%v25190 = vmul.f32 %v25186, %v25146
%v25194 = vadd.f32 %v25190, %v25103
%v25198 = vmul.f32 %v25194, %v25146
%v25202 = vadd.f32 %v25198, %v25099
%v25206 = vmul.f32 %v25202, %v25146
%v25210 = vadd.f32 %v25206, %v25095
%v25214 = vmul.f32 %v25210, %v25061
%v25218 = vsel /*vm=*/%vm25066, /*on_true_vy=*/%v25071, /*on_false_vx=*/%v25214
%v25222 = vmul.f32 1.4140625, %v25218
%v25225 = vpack.c.bf16 %v120417, %v25222
%119877 = vst [vmem:[%s280 + $0x218] sm:$0xf] /*vst_source=*/%v25225
%v25229 = vadd.s32 %v22921, %v2842
%v25239 = vadd.s32 %v25229, %v415
%vm25243 = vcmp.lt.u32.totalorder %v25239, %v25229
%vm25248 = vcmp.lt.u32.totalorder %v25229, %v2842
%v25253 = vadd.s32 %v22904, %v2829
%v25257 = vadd.s32 1, %v25253
%v25261 = vsel /*vm=*/%vm25248, /*on_true_vy=*/%v25257, /*on_false_vx=*/%v25253
%v25265 = vadd.s32 1, %v25261
%v25269 = vsel /*vm=*/%vm25243, /*on_true_vy=*/%v25265, /*on_false_vx=*/%v25261
%v25274 = vadd.s32 %v25269, %v10
%v25278 = vadd.s32 %v25239, %v9
%v25282 = vadd.s32 %v25278, %v25274
%v25284 = vshll.u32 %v25278, 13
%v25285 = vshrl.u32 %v25278, 19
%v25286 = vor.u32 %v25285, %v25284
%v25287 = vxor.u32 %v25286, %v25282
%v25290 = vadd.s32 %v25287, %v25282
%v25292 = vshll.u32 %v25287, 15
%v25293 = vshrl.u32 %v25287, 17
%v25294 = vor.u32 %v25293, %v25292
%v25295 = vxor.u32 %v25294, %v25290
%v25298 = vadd.s32 %v25295, %v25290
%v25300 = vshll.u32 %v25295, 26
%v25301 = vshrl.u32 %v25295, 6
%v25302 = vor.u32 %v25301, %v25300
%v25303 = vxor.u32 %v25302, %v25298
%v25306 = vadd.s32 %v25303, %v25298
%v25310 = vadd.s32 %v25306, %v9
%v25312 = vshll.u32 %v25303, 6
%v25313 = vshrl.u32 %v25303, 26
%v25314 = vor.u32 %v25313, %v25312
%v25315 = vxor.u32 %v25314, %v25306
%v25318 = vadd.s32 %v25315, %v8
%v25322 = vadd.s32 1, %v25318
%v25326 = vadd.s32 %v25322, %v25310
%v25328 = vshll.u32 %v25322, 17
%v25329 = vshrl.u32 %v25322, 15
%v25330 = vor.u32 %v25329, %v25328
%v25331 = vxor.u32 %v25330, %v25326
%v25334 = vadd.s32 %v25331, %v25326
%v25336 = vshll.u32 %v25331, 29
%v25337 = vshrl.u32 %v25331, 3
%v25338 = vor.u32 %v25337, %v25336
%v25339 = vxor.u32 %v25338, %v25334
%v25342 = vadd.s32 %v25339, %v25334
%v25344 = vshll.u32 %v25339, 16
%v25345 = vshrl.u32 %v25339, 16
%v25346 = vor.u32 %v25345, %v25344
%v25347 = vxor.u32 %v25346, %v25342
%v25350 = vadd.s32 %v25347, %v25342
%v25354 = vadd.s32 %v25350, %v8
%v25356 = vshll.u32 %v25347, 24
%v25357 = vshrl.u32 %v25347, 8
%v25358 = vor.u32 %v25357, %v25356
%v25359 = vxor.u32 %v25358, %v25350
%v25362 = vadd.s32 %v25359, %v10
%v25366 = vadd.s32 2, %v25362
%v25370 = vadd.s32 %v25366, %v25354
%v25372 = vshll.u32 %v25366, 13
%v25373 = vshrl.u32 %v25366, 19
%v25374 = vor.u32 %v25373, %v25372
%v25375 = vxor.u32 %v25374, %v25370
%v25378 = vadd.s32 %v25375, %v25370
%v25380 = vshll.u32 %v25375, 15
%v25381 = vshrl.u32 %v25375, 17
%v25382 = vor.u32 %v25381, %v25380
%v25383 = vxor.u32 %v25382, %v25378
%v25386 = vadd.s32 %v25383, %v25378
%v25388 = vshll.u32 %v25383, 26
%v25389 = vshrl.u32 %v25383, 6
%v25390 = vor.u32 %v25389, %v25388
%v25391 = vxor.u32 %v25390, %v25386
%v25394 = vadd.s32 %v25391, %v25386
%v25398 = vadd.s32 %v25394, %v10
%v25400 = vshll.u32 %v25391, 6
%v25401 = vshrl.u32 %v25391, 26
%v25402 = vor.u32 %v25401, %v25400
%v25403 = vxor.u32 %v25402, %v25394
%v25406 = vadd.s32 %v25403, %v9
%v25410 = vadd.s32 3, %v25406
%v25414 = vadd.s32 %v25410, %v25398
%v25416 = vshll.u32 %v25410, 17
%v25417 = vshrl.u32 %v25410, 15
%v25418 = vor.u32 %v25417, %v25416
%v25419 = vxor.u32 %v25418, %v25414
%v25422 = vadd.s32 %v25419, %v25414
%v25424 = vshll.u32 %v25419, 29
%v25425 = vshrl.u32 %v25419, 3
%v25426 = vor.u32 %v25425, %v25424
%v25427 = vxor.u32 %v25426, %v25422
%v25430 = vadd.s32 %v25427, %v25422
%v25432 = vshll.u32 %v25427, 16
%v25433 = vshrl.u32 %v25427, 16
%v25434 = vor.u32 %v25433, %v25432
%v25435 = vxor.u32 %v25434, %v25430
%v25438 = vadd.s32 %v25435, %v25430
%v25442 = vadd.s32 %v25438, %v9
%v25444 = vshll.u32 %v25435, 24
%v25445 = vshrl.u32 %v25435, 8
%v25446 = vor.u32 %v25445, %v25444
%v25447 = vxor.u32 %v25446, %v25438
%v25450 = vadd.s32 %v25447, %v8
%v25454 = vadd.s32 4, %v25450
%v25458 = vadd.s32 %v25454, %v25442
%v25460 = vshll.u32 %v25454, 13
%v25461 = vshrl.u32 %v25454, 19
%v25462 = vor.u32 %v25461, %v25460
%v25463 = vxor.u32 %v25462, %v25458
%v25466 = vadd.s32 %v25463, %v25458
%v25468 = vshll.u32 %v25463, 15
%v25469 = vshrl.u32 %v25463, 17
%v25470 = vor.u32 %v25469, %v25468
%v25471 = vxor.u32 %v25470, %v25466
%v25474 = vadd.s32 %v25471, %v25466
%v25476 = vshll.u32 %v25471, 26
%v25477 = vshrl.u32 %v25471, 6
%v25478 = vor.u32 %v25477, %v25476
%v25479 = vxor.u32 %v25478, %v25474
%v25482 = vadd.s32 %v25479, %v25474
%v25486 = vadd.s32 %v25482, %v8
%v25488 = vshll.u32 %v25479, 6
%v25489 = vshrl.u32 %v25479, 26
%v25490 = vor.u32 %v25489, %v25488
%v25491 = vxor.u32 %v25490, %v25482
%v25494 = vadd.s32 %v25491, %v10
%v25498 = vadd.s32 5, %v25494
%v25500 = vxor.u32 %v25498, %v25486
%v25501 = vand.u32.u8 255, %v25500
%v25502 = vand.u32 65535, %v25501
%v25503 = vshrl.u32 %v25502, 1
%v25504 = vor.u32 16256, %v25503
%v25505 = vand.u32.u16 65535, %v25504
%v119878 = vadd.low.f32.bf16 -1.0, %v25505
%v25514 = vmul.f32 2.0, %v119878
%v25518 = vadd.f32 -0.99609375, %v25514
%v25522 = vmax.f32 %v25518, -0.99609375
%v25524 = vand.u32 2147483647, %v25522
%vm25527 = vcmp.eq.f32.partialorder %v25524, 1.0
%v25532 = vmul.f32 inf, %v25522
%v25534 = vxor.u32 2147483648, %v25522
%v25537 = vmul.f32 %v25534, %v25522
%v25539 = vadd.f32 1.0, %v25537
%v25540 = vlog2.pop %v25539
%v25541 = vmul.f32 0.6931472, %v25540
%v25542 = vmul.f32 -0.5, %v25537
%v25543 = vadd.f32 1.0, %v25542
%v25544 = vmul.f32 %v25543, %v25537
%v25545 = vand.u32 2147483647, %v25537
%vm25546 = vcmp.lt.f32.partialorder %v25545, 0.0004427343
%v25547 = vsel /*vm=*/%vm25546, /*on_true_vy=*/%v25544, /*on_false_vx=*/%v25541
%v25548 = vxor.u32 2147483648, %v25547
%vm25551 = vcmp.lt.f32.partialorder %v25548, 5.0
%v25556 = vsel /*vm=*/%vm25551, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v25560 = vsel /*vm=*/%vm25551, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v25564 = vsel /*vm=*/%vm25551, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v25568 = vsel /*vm=*/%vm25551, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v25572 = vsel /*vm=*/%vm25551, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v25576 = vsel /*vm=*/%vm25551, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v25580 = vsel /*vm=*/%vm25551, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v25584 = vsel /*vm=*/%vm25551, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v25588 = vsel /*vm=*/%vm25551, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v25592 = vadd.f32 -2.5, %v25548
%v25594 = vrsqrt.pop %v25548
%v25595 = vmul.f32 %v25594, %v25548
%vm25596 = vcmp.eq.f32.partialorder %v25548, inf
%v25597 = vsel /*vm=*/%vm25596, /*on_true_vy=*/%v25548, /*on_false_vx=*/%v25595
%vm25598 = vcmp.eq.f32.partialorder %v25548, 0.0
%v25599 = vand.u32 2147483648, %v25548
%v25600 = vsel /*vm=*/%vm25598, /*on_true_vy=*/%v25599, /*on_false_vx=*/%v25597
%v25603 = vadd.f32 -3.0, %v25600
%v25607 = vsel /*vm=*/%vm25551, /*on_true_vy=*/%v25592, /*on_false_vx=*/%v25603
%v25611 = vmul.f32 %v25607, %v25588
%v25615 = vadd.f32 %v25611, %v25584
%v25619 = vmul.f32 %v25615, %v25607
%v25623 = vadd.f32 %v25619, %v25580
%v25627 = vmul.f32 %v25623, %v25607
%v25631 = vadd.f32 %v25627, %v25576
%v25635 = vmul.f32 %v25631, %v25607
%v25639 = vadd.f32 %v25635, %v25572
%v25643 = vmul.f32 %v25639, %v25607
%v25647 = vadd.f32 %v25643, %v25568
%v25651 = vmul.f32 %v25647, %v25607
%v25655 = vadd.f32 %v25651, %v25564
%v25659 = vmul.f32 %v25655, %v25607
%v25663 = vadd.f32 %v25659, %v25560
%v25667 = vmul.f32 %v25663, %v25607
%v25671 = vadd.f32 %v25667, %v25556
%v25675 = vmul.f32 %v25671, %v25522
%v25679 = vsel /*vm=*/%vm25527, /*on_true_vy=*/%v25532, /*on_false_vx=*/%v25675
%v25683 = vmul.f32 1.4140625, %v25679
%v25686 = vpack.c.bf16 %v120417, %v25683
%119879 = vst [vmem:[%s280 + $0x298] sm:$0xf] /*vst_source=*/%v25686
%v25690 = vadd.s32 %v22921, %v3329
%v25700 = vadd.s32 %v25690, %v415
%vm25704 = vcmp.lt.u32.totalorder %v25700, %v25690
%vm25709 = vcmp.lt.u32.totalorder %v25690, %v3329
%v25714 = vadd.s32 %v22904, %v3316
%v25718 = vadd.s32 1, %v25714
%v25722 = vsel /*vm=*/%vm25709, /*on_true_vy=*/%v25718, /*on_false_vx=*/%v25714
%v25726 = vadd.s32 1, %v25722
%v25730 = vsel /*vm=*/%vm25704, /*on_true_vy=*/%v25726, /*on_false_vx=*/%v25722
%v25735 = vadd.s32 %v25730, %v10
%v25739 = vadd.s32 %v25700, %v9
%v25743 = vadd.s32 %v25739, %v25735
%v25745 = vshll.u32 %v25739, 13
%v25746 = vshrl.u32 %v25739, 19
%v25747 = vor.u32 %v25746, %v25745
%v25748 = vxor.u32 %v25747, %v25743
%v25751 = vadd.s32 %v25748, %v25743
%v25753 = vshll.u32 %v25748, 15
%v25754 = vshrl.u32 %v25748, 17
%v25755 = vor.u32 %v25754, %v25753
%v25756 = vxor.u32 %v25755, %v25751
%v25759 = vadd.s32 %v25756, %v25751
%v25761 = vshll.u32 %v25756, 26
%v25762 = vshrl.u32 %v25756, 6
%v25763 = vor.u32 %v25762, %v25761
%v25764 = vxor.u32 %v25763, %v25759
%v25767 = vadd.s32 %v25764, %v25759
%v25771 = vadd.s32 %v25767, %v9
%v25773 = vshll.u32 %v25764, 6
%v25774 = vshrl.u32 %v25764, 26
%v25775 = vor.u32 %v25774, %v25773
%v25776 = vxor.u32 %v25775, %v25767
%v25779 = vadd.s32 %v25776, %v8
%v25783 = vadd.s32 1, %v25779
%v25787 = vadd.s32 %v25783, %v25771
%v25789 = vshll.u32 %v25783, 17
%v25790 = vshrl.u32 %v25783, 15
%v25791 = vor.u32 %v25790, %v25789
%v25792 = vxor.u32 %v25791, %v25787
%v25795 = vadd.s32 %v25792, %v25787
%v25797 = vshll.u32 %v25792, 29
%v25798 = vshrl.u32 %v25792, 3
%v25799 = vor.u32 %v25798, %v25797
%v25800 = vxor.u32 %v25799, %v25795
%v25803 = vadd.s32 %v25800, %v25795
%v25805 = vshll.u32 %v25800, 16
%v25806 = vshrl.u32 %v25800, 16
%v25807 = vor.u32 %v25806, %v25805
%v25808 = vxor.u32 %v25807, %v25803
%v25811 = vadd.s32 %v25808, %v25803
%v25815 = vadd.s32 %v25811, %v8
%v25817 = vshll.u32 %v25808, 24
%v25818 = vshrl.u32 %v25808, 8
%v25819 = vor.u32 %v25818, %v25817
%v25820 = vxor.u32 %v25819, %v25811
%v25823 = vadd.s32 %v25820, %v10
%v25827 = vadd.s32 2, %v25823
%v25831 = vadd.s32 %v25827, %v25815
%v25833 = vshll.u32 %v25827, 13
%v25834 = vshrl.u32 %v25827, 19
%v25835 = vor.u32 %v25834, %v25833
%v25836 = vxor.u32 %v25835, %v25831
%v25839 = vadd.s32 %v25836, %v25831
%v25841 = vshll.u32 %v25836, 15
%v25842 = vshrl.u32 %v25836, 17
%v25843 = vor.u32 %v25842, %v25841
%v25844 = vxor.u32 %v25843, %v25839
%v25847 = vadd.s32 %v25844, %v25839
%v25849 = vshll.u32 %v25844, 26
%v25850 = vshrl.u32 %v25844, 6
%v25851 = vor.u32 %v25850, %v25849
%v25852 = vxor.u32 %v25851, %v25847
%v25855 = vadd.s32 %v25852, %v25847
%v25859 = vadd.s32 %v25855, %v10
%v25861 = vshll.u32 %v25852, 6
%v25862 = vshrl.u32 %v25852, 26
%v25863 = vor.u32 %v25862, %v25861
%v25864 = vxor.u32 %v25863, %v25855
%v25867 = vadd.s32 %v25864, %v9
%v25871 = vadd.s32 3, %v25867
%v25875 = vadd.s32 %v25871, %v25859
%v25877 = vshll.u32 %v25871, 17
%v25878 = vshrl.u32 %v25871, 15
%v25879 = vor.u32 %v25878, %v25877
%v25880 = vxor.u32 %v25879, %v25875
%v25883 = vadd.s32 %v25880, %v25875
%v25885 = vshll.u32 %v25880, 29
%v25886 = vshrl.u32 %v25880, 3
%v25887 = vor.u32 %v25886, %v25885
%v25888 = vxor.u32 %v25887, %v25883
%v25891 = vadd.s32 %v25888, %v25883
%v25893 = vshll.u32 %v25888, 16
%v25894 = vshrl.u32 %v25888, 16
%v25895 = vor.u32 %v25894, %v25893
%v25896 = vxor.u32 %v25895, %v25891
%v25899 = vadd.s32 %v25896, %v25891
%v25903 = vadd.s32 %v25899, %v9
%v25905 = vshll.u32 %v25896, 24
%v25906 = vshrl.u32 %v25896, 8
%v25907 = vor.u32 %v25906, %v25905
%v25908 = vxor.u32 %v25907, %v25899
%v25911 = vadd.s32 %v25908, %v8
%v25915 = vadd.s32 4, %v25911
%v25919 = vadd.s32 %v25915, %v25903
%v25921 = vshll.u32 %v25915, 13
%v25922 = vshrl.u32 %v25915, 19
%v25923 = vor.u32 %v25922, %v25921
%v25924 = vxor.u32 %v25923, %v25919
%v25927 = vadd.s32 %v25924, %v25919
%v25929 = vshll.u32 %v25924, 15
%v25930 = vshrl.u32 %v25924, 17
%v25931 = vor.u32 %v25930, %v25929
%v25932 = vxor.u32 %v25931, %v25927
%v25935 = vadd.s32 %v25932, %v25927
%v25937 = vshll.u32 %v25932, 26
%v25938 = vshrl.u32 %v25932, 6
%v25939 = vor.u32 %v25938, %v25937
%v25940 = vxor.u32 %v25939, %v25935
%v25943 = vadd.s32 %v25940, %v25935
%v25947 = vadd.s32 %v25943, %v8
%v25949 = vshll.u32 %v25940, 6
%v25950 = vshrl.u32 %v25940, 26
%v25951 = vor.u32 %v25950, %v25949
%v25952 = vxor.u32 %v25951, %v25943
%v25955 = vadd.s32 %v25952, %v10
%v25959 = vadd.s32 5, %v25955
%v25961 = vxor.u32 %v25959, %v25947
%v25962 = vand.u32.u8 255, %v25961
%v25963 = vand.u32 65535, %v25962
%v25964 = vshrl.u32 %v25963, 1
%v25965 = vor.u32 16256, %v25964
%v25966 = vand.u32.u16 65535, %v25965
%v119880 = vadd.low.f32.bf16 -1.0, %v25966
%v25975 = vmul.f32 2.0, %v119880
%v25979 = vadd.f32 -0.99609375, %v25975
%v25983 = vmax.f32 %v25979, -0.99609375
%v25985 = vand.u32 2147483647, %v25983
%vm25988 = vcmp.eq.f32.partialorder %v25985, 1.0
%v25993 = vmul.f32 inf, %v25983
%v25995 = vxor.u32 2147483648, %v25983
%v25998 = vmul.f32 %v25995, %v25983
%v26000 = vadd.f32 1.0, %v25998
%v26001 = vlog2.pop %v26000
%v26002 = vmul.f32 0.6931472, %v26001
%v26003 = vmul.f32 -0.5, %v25998
%v26004 = vadd.f32 1.0, %v26003
%v26005 = vmul.f32 %v26004, %v25998
%v26006 = vand.u32 2147483647, %v25998
%vm26007 = vcmp.lt.f32.partialorder %v26006, 0.0004427343
%v26008 = vsel /*vm=*/%vm26007, /*on_true_vy=*/%v26005, /*on_false_vx=*/%v26002
%v26009 = vxor.u32 2147483648, %v26008
%vm26012 = vcmp.lt.f32.partialorder %v26009, 5.0
%v26017 = vsel /*vm=*/%vm26012, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v26021 = vsel /*vm=*/%vm26012, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v26025 = vsel /*vm=*/%vm26012, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v26029 = vsel /*vm=*/%vm26012, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v26033 = vsel /*vm=*/%vm26012, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v26037 = vsel /*vm=*/%vm26012, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v26041 = vsel /*vm=*/%vm26012, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v26045 = vsel /*vm=*/%vm26012, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v26049 = vsel /*vm=*/%vm26012, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v26053 = vadd.f32 -2.5, %v26009
%v26055 = vrsqrt.pop %v26009
%v26056 = vmul.f32 %v26055, %v26009
%vm26057 = vcmp.eq.f32.partialorder %v26009, inf
%v26058 = vsel /*vm=*/%vm26057, /*on_true_vy=*/%v26009, /*on_false_vx=*/%v26056
%vm26059 = vcmp.eq.f32.partialorder %v26009, 0.0
%v26060 = vand.u32 2147483648, %v26009
%v26061 = vsel /*vm=*/%vm26059, /*on_true_vy=*/%v26060, /*on_false_vx=*/%v26058
%v26064 = vadd.f32 -3.0, %v26061
%v26068 = vsel /*vm=*/%vm26012, /*on_true_vy=*/%v26053, /*on_false_vx=*/%v26064
%v26072 = vmul.f32 %v26068, %v26049
%v26076 = vadd.f32 %v26072, %v26045
%v26080 = vmul.f32 %v26076, %v26068
%v26084 = vadd.f32 %v26080, %v26041
%v26088 = vmul.f32 %v26084, %v26068
%v26092 = vadd.f32 %v26088, %v26037
%v26096 = vmul.f32 %v26092, %v26068
%v26100 = vadd.f32 %v26096, %v26033
%v26104 = vmul.f32 %v26100, %v26068
%v26108 = vadd.f32 %v26104, %v26029
%v26112 = vmul.f32 %v26108, %v26068
%v26116 = vadd.f32 %v26112, %v26025
%v26120 = vmul.f32 %v26116, %v26068
%v26124 = vadd.f32 %v26120, %v26021
%v26128 = vmul.f32 %v26124, %v26068
%v26132 = vadd.f32 %v26128, %v26017
%v26136 = vmul.f32 %v26132, %v25983
%v26140 = vsel /*vm=*/%vm25988, /*on_true_vy=*/%v25993, /*on_false_vx=*/%v26136
%v26144 = vmul.f32 1.4140625, %v26140
%v26147 = vpack.c.bf16 %v120417, %v26144
%119881 = vst [vmem:[%s280 + $0x318] sm:$0xf] /*vst_source=*/%v26147
%v26151 = vadd.s32 %v22921, %v3816
%v26161 = vadd.s32 %v26151, %v415
%vm26165 = vcmp.lt.u32.totalorder %v26161, %v26151
%vm26170 = vcmp.lt.u32.totalorder %v26151, %v3816
%v26175 = vadd.s32 %v22904, %v3803
%v26179 = vadd.s32 1, %v26175
%v26183 = vsel /*vm=*/%vm26170, /*on_true_vy=*/%v26179, /*on_false_vx=*/%v26175
%v26187 = vadd.s32 1, %v26183
%v26191 = vsel /*vm=*/%vm26165, /*on_true_vy=*/%v26187, /*on_false_vx=*/%v26183
%v26196 = vadd.s32 %v26191, %v10
%v26200 = vadd.s32 %v26161, %v9
%v26204 = vadd.s32 %v26200, %v26196
%v26206 = vshll.u32 %v26200, 13
%v26207 = vshrl.u32 %v26200, 19
%v26208 = vor.u32 %v26207, %v26206
%v26209 = vxor.u32 %v26208, %v26204
%v26212 = vadd.s32 %v26209, %v26204
%v26214 = vshll.u32 %v26209, 15
%v26215 = vshrl.u32 %v26209, 17
%v26216 = vor.u32 %v26215, %v26214
%v26217 = vxor.u32 %v26216, %v26212
%v26220 = vadd.s32 %v26217, %v26212
%v26222 = vshll.u32 %v26217, 26
%v26223 = vshrl.u32 %v26217, 6
%v26224 = vor.u32 %v26223, %v26222
%v26225 = vxor.u32 %v26224, %v26220
%v26228 = vadd.s32 %v26225, %v26220
%v26232 = vadd.s32 %v26228, %v9
%v26234 = vshll.u32 %v26225, 6
%v26235 = vshrl.u32 %v26225, 26
%v26236 = vor.u32 %v26235, %v26234
%v26237 = vxor.u32 %v26236, %v26228
%v26240 = vadd.s32 %v26237, %v8
%v26244 = vadd.s32 1, %v26240
%v26248 = vadd.s32 %v26244, %v26232
%v26250 = vshll.u32 %v26244, 17
%v26251 = vshrl.u32 %v26244, 15
%v26252 = vor.u32 %v26251, %v26250
%v26253 = vxor.u32 %v26252, %v26248
%v26256 = vadd.s32 %v26253, %v26248
%v26258 = vshll.u32 %v26253, 29
%v26259 = vshrl.u32 %v26253, 3
%v26260 = vor.u32 %v26259, %v26258
%v26261 = vxor.u32 %v26260, %v26256
%v26264 = vadd.s32 %v26261, %v26256
%v26266 = vshll.u32 %v26261, 16
%v26267 = vshrl.u32 %v26261, 16
%v26268 = vor.u32 %v26267, %v26266
%v26269 = vxor.u32 %v26268, %v26264
%v26272 = vadd.s32 %v26269, %v26264
%v26276 = vadd.s32 %v26272, %v8
%v26278 = vshll.u32 %v26269, 24
%v26279 = vshrl.u32 %v26269, 8
%v26280 = vor.u32 %v26279, %v26278
%v26281 = vxor.u32 %v26280, %v26272
%v26284 = vadd.s32 %v26281, %v10
%v26288 = vadd.s32 2, %v26284
%v26292 = vadd.s32 %v26288, %v26276
%v26294 = vshll.u32 %v26288, 13
%v26295 = vshrl.u32 %v26288, 19
%v26296 = vor.u32 %v26295, %v26294
%v26297 = vxor.u32 %v26296, %v26292
%v26300 = vadd.s32 %v26297, %v26292
%v26302 = vshll.u32 %v26297, 15
%v26303 = vshrl.u32 %v26297, 17
%v26304 = vor.u32 %v26303, %v26302
%v26305 = vxor.u32 %v26304, %v26300
%v26308 = vadd.s32 %v26305, %v26300
%v26310 = vshll.u32 %v26305, 26
%v26311 = vshrl.u32 %v26305, 6
%v26312 = vor.u32 %v26311, %v26310
%v26313 = vxor.u32 %v26312, %v26308
%v26316 = vadd.s32 %v26313, %v26308
%v26320 = vadd.s32 %v26316, %v10
%v26322 = vshll.u32 %v26313, 6
%v26323 = vshrl.u32 %v26313, 26
%v26324 = vor.u32 %v26323, %v26322
%v26325 = vxor.u32 %v26324, %v26316
%v26328 = vadd.s32 %v26325, %v9
%v26332 = vadd.s32 3, %v26328
%v26336 = vadd.s32 %v26332, %v26320
%v26338 = vshll.u32 %v26332, 17
%v26339 = vshrl.u32 %v26332, 15
%v26340 = vor.u32 %v26339, %v26338
%v26341 = vxor.u32 %v26340, %v26336
%v26344 = vadd.s32 %v26341, %v26336
%v26346 = vshll.u32 %v26341, 29
%v26347 = vshrl.u32 %v26341, 3
%v26348 = vor.u32 %v26347, %v26346
%v26349 = vxor.u32 %v26348, %v26344
%v26352 = vadd.s32 %v26349, %v26344
%v26354 = vshll.u32 %v26349, 16
%v26355 = vshrl.u32 %v26349, 16
%v26356 = vor.u32 %v26355, %v26354
%v26357 = vxor.u32 %v26356, %v26352
%v26360 = vadd.s32 %v26357, %v26352
%v26364 = vadd.s32 %v26360, %v9
%v26366 = vshll.u32 %v26357, 24
%v26367 = vshrl.u32 %v26357, 8
%v26368 = vor.u32 %v26367, %v26366
%v26369 = vxor.u32 %v26368, %v26360
%v26372 = vadd.s32 %v26369, %v8
%v26376 = vadd.s32 4, %v26372
%v26380 = vadd.s32 %v26376, %v26364
%v26382 = vshll.u32 %v26376, 13
%v26383 = vshrl.u32 %v26376, 19
%v26384 = vor.u32 %v26383, %v26382
%v26385 = vxor.u32 %v26384, %v26380
%v26388 = vadd.s32 %v26385, %v26380
%v26390 = vshll.u32 %v26385, 15
%v26391 = vshrl.u32 %v26385, 17
%v26392 = vor.u32 %v26391, %v26390
%v26393 = vxor.u32 %v26392, %v26388
%v26396 = vadd.s32 %v26393, %v26388
%v26398 = vshll.u32 %v26393, 26
%v26399 = vshrl.u32 %v26393, 6
%v26400 = vor.u32 %v26399, %v26398
%v26401 = vxor.u32 %v26400, %v26396
%v26404 = vadd.s32 %v26401, %v26396
%v26408 = vadd.s32 %v26404, %v8
%v26410 = vshll.u32 %v26401, 6
%v26411 = vshrl.u32 %v26401, 26
%v26412 = vor.u32 %v26411, %v26410
%v26413 = vxor.u32 %v26412, %v26404
%v26416 = vadd.s32 %v26413, %v10
%v26420 = vadd.s32 5, %v26416
%v26422 = vxor.u32 %v26420, %v26408
%v26423 = vand.u32.u8 255, %v26422
%v26424 = vand.u32 65535, %v26423
%v26425 = vshrl.u32 %v26424, 1
%v26426 = vor.u32 16256, %v26425
%v26427 = vand.u32.u16 65535, %v26426
%v119882 = vadd.low.f32.bf16 -1.0, %v26427
%v26436 = vmul.f32 2.0, %v119882
%v26440 = vadd.f32 -0.99609375, %v26436
%v26444 = vmax.f32 %v26440, -0.99609375
%v26446 = vand.u32 2147483647, %v26444
%vm26449 = vcmp.eq.f32.partialorder %v26446, 1.0
%v26454 = vmul.f32 inf, %v26444
%v26456 = vxor.u32 2147483648, %v26444
%v26459 = vmul.f32 %v26456, %v26444
%v26461 = vadd.f32 1.0, %v26459
%v26462 = vlog2.pop %v26461
%v26463 = vmul.f32 0.6931472, %v26462
%v26464 = vmul.f32 -0.5, %v26459
%v26465 = vadd.f32 1.0, %v26464
%v26466 = vmul.f32 %v26465, %v26459
%v26467 = vand.u32 2147483647, %v26459
%vm26468 = vcmp.lt.f32.partialorder %v26467, 0.0004427343
%v26469 = vsel /*vm=*/%vm26468, /*on_true_vy=*/%v26466, /*on_false_vx=*/%v26463
%v26470 = vxor.u32 2147483648, %v26469
%vm26473 = vcmp.lt.f32.partialorder %v26470, 5.0
%v26478 = vsel /*vm=*/%vm26473, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v26482 = vsel /*vm=*/%vm26473, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v26486 = vsel /*vm=*/%vm26473, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v26490 = vsel /*vm=*/%vm26473, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v26494 = vsel /*vm=*/%vm26473, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v26498 = vsel /*vm=*/%vm26473, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v26502 = vsel /*vm=*/%vm26473, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v26506 = vsel /*vm=*/%vm26473, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v26510 = vsel /*vm=*/%vm26473, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v26514 = vadd.f32 -2.5, %v26470
%v26516 = vrsqrt.pop %v26470
%v26517 = vmul.f32 %v26516, %v26470
%vm26518 = vcmp.eq.f32.partialorder %v26470, inf
%v26519 = vsel /*vm=*/%vm26518, /*on_true_vy=*/%v26470, /*on_false_vx=*/%v26517
%vm26520 = vcmp.eq.f32.partialorder %v26470, 0.0
%v26521 = vand.u32 2147483648, %v26470
%v26522 = vsel /*vm=*/%vm26520, /*on_true_vy=*/%v26521, /*on_false_vx=*/%v26519
%v26525 = vadd.f32 -3.0, %v26522
%v26529 = vsel /*vm=*/%vm26473, /*on_true_vy=*/%v26514, /*on_false_vx=*/%v26525
%v26533 = vmul.f32 %v26529, %v26510
%v26537 = vadd.f32 %v26533, %v26506
%v26541 = vmul.f32 %v26537, %v26529
%v26545 = vadd.f32 %v26541, %v26502
%v26549 = vmul.f32 %v26545, %v26529
%v26553 = vadd.f32 %v26549, %v26498
%v26557 = vmul.f32 %v26553, %v26529
%v26561 = vadd.f32 %v26557, %v26494
%v26565 = vmul.f32 %v26561, %v26529
%v26569 = vadd.f32 %v26565, %v26490
%v26573 = vmul.f32 %v26569, %v26529
%v26577 = vadd.f32 %v26573, %v26486
%v26581 = vmul.f32 %v26577, %v26529
%v26585 = vadd.f32 %v26581, %v26482
%v26589 = vmul.f32 %v26585, %v26529
%v26593 = vadd.f32 %v26589, %v26478
%v26597 = vmul.f32 %v26593, %v26444
%v26601 = vsel /*vm=*/%vm26449, /*on_true_vy=*/%v26454, /*on_false_vx=*/%v26597
%v26605 = vmul.f32 1.4140625, %v26601
%v26608 = vpack.c.bf16 %v120417, %v26605
%119883 = vst [vmem:[%s280 + $0x398] sm:$0xf] /*vst_source=*/%v26608
%v26646 = vadd.s32 %v26643, %v408
%v26656 = vadd.s32 %v26646, %v415
%vm26660 = vcmp.lt.u32.totalorder %v26656, %v26646
%vm26665 = vcmp.lt.u32.totalorder %v26646, %v408
%v26670 = vadd.s32 %v26626, %v380
%v26674 = vadd.s32 1, %v26670
%v26678 = vsel /*vm=*/%vm26665, /*on_true_vy=*/%v26674, /*on_false_vx=*/%v26670
%v26682 = vadd.s32 1, %v26678
%v26686 = vsel /*vm=*/%vm26660, /*on_true_vy=*/%v26682, /*on_false_vx=*/%v26678
%v26691 = vadd.s32 %v26686, %v10
%v26695 = vadd.s32 %v26656, %v9
%v26699 = vadd.s32 %v26695, %v26691
%v26701 = vshll.u32 %v26695, 13
%v26702 = vshrl.u32 %v26695, 19
%v26703 = vor.u32 %v26702, %v26701
%v26704 = vxor.u32 %v26703, %v26699
%v26707 = vadd.s32 %v26704, %v26699
%v26709 = vshll.u32 %v26704, 15
%v26710 = vshrl.u32 %v26704, 17
%v26711 = vor.u32 %v26710, %v26709
%v26712 = vxor.u32 %v26711, %v26707
%v26715 = vadd.s32 %v26712, %v26707
%v26717 = vshll.u32 %v26712, 26
%v26718 = vshrl.u32 %v26712, 6
%v26719 = vor.u32 %v26718, %v26717
%v26720 = vxor.u32 %v26719, %v26715
%v26723 = vadd.s32 %v26720, %v26715
%v26727 = vadd.s32 %v26723, %v9
%v26729 = vshll.u32 %v26720, 6
%v26730 = vshrl.u32 %v26720, 26
%v26731 = vor.u32 %v26730, %v26729
%v26732 = vxor.u32 %v26731, %v26723
%v26735 = vadd.s32 %v26732, %v8
%v26739 = vadd.s32 1, %v26735
%v26743 = vadd.s32 %v26739, %v26727
%v26745 = vshll.u32 %v26739, 17
%v26746 = vshrl.u32 %v26739, 15
%v26747 = vor.u32 %v26746, %v26745
%v26748 = vxor.u32 %v26747, %v26743
%v26751 = vadd.s32 %v26748, %v26743
%v26753 = vshll.u32 %v26748, 29
%v26754 = vshrl.u32 %v26748, 3
%v26755 = vor.u32 %v26754, %v26753
%v26756 = vxor.u32 %v26755, %v26751
%v26759 = vadd.s32 %v26756, %v26751
%v26761 = vshll.u32 %v26756, 16
%v26762 = vshrl.u32 %v26756, 16
%v26763 = vor.u32 %v26762, %v26761
%v26764 = vxor.u32 %v26763, %v26759
%v26767 = vadd.s32 %v26764, %v26759
%v26771 = vadd.s32 %v26767, %v8
%v26773 = vshll.u32 %v26764, 24
%v26774 = vshrl.u32 %v26764, 8
%v26775 = vor.u32 %v26774, %v26773
%v26776 = vxor.u32 %v26775, %v26767
%v26779 = vadd.s32 %v26776, %v10
%v26783 = vadd.s32 2, %v26779
%v26787 = vadd.s32 %v26783, %v26771
%v26789 = vshll.u32 %v26783, 13
%v26790 = vshrl.u32 %v26783, 19
%v26791 = vor.u32 %v26790, %v26789
%v26792 = vxor.u32 %v26791, %v26787
%v26795 = vadd.s32 %v26792, %v26787
%v26797 = vshll.u32 %v26792, 15
%v26798 = vshrl.u32 %v26792, 17
%v26799 = vor.u32 %v26798, %v26797
%v26800 = vxor.u32 %v26799, %v26795
%v26803 = vadd.s32 %v26800, %v26795
%v26805 = vshll.u32 %v26800, 26
%v26806 = vshrl.u32 %v26800, 6
%v26807 = vor.u32 %v26806, %v26805
%v26808 = vxor.u32 %v26807, %v26803
%v26811 = vadd.s32 %v26808, %v26803
%v26815 = vadd.s32 %v26811, %v10
%v26817 = vshll.u32 %v26808, 6
%v26818 = vshrl.u32 %v26808, 26
%v26819 = vor.u32 %v26818, %v26817
%v26820 = vxor.u32 %v26819, %v26811
%v26823 = vadd.s32 %v26820, %v9
%v26827 = vadd.s32 3, %v26823
%v26831 = vadd.s32 %v26827, %v26815
%v26833 = vshll.u32 %v26827, 17
%v26834 = vshrl.u32 %v26827, 15
%v26835 = vor.u32 %v26834, %v26833
%v26836 = vxor.u32 %v26835, %v26831
%v26839 = vadd.s32 %v26836, %v26831
%v26841 = vshll.u32 %v26836, 29
%v26842 = vshrl.u32 %v26836, 3
%v26843 = vor.u32 %v26842, %v26841
%v26844 = vxor.u32 %v26843, %v26839
%v26847 = vadd.s32 %v26844, %v26839
%v26849 = vshll.u32 %v26844, 16
%v26850 = vshrl.u32 %v26844, 16
%v26851 = vor.u32 %v26850, %v26849
%v26852 = vxor.u32 %v26851, %v26847
%v26855 = vadd.s32 %v26852, %v26847
%v26859 = vadd.s32 %v26855, %v9
%v26861 = vshll.u32 %v26852, 24
%v26862 = vshrl.u32 %v26852, 8
%v26863 = vor.u32 %v26862, %v26861
%v26864 = vxor.u32 %v26863, %v26855
%v26867 = vadd.s32 %v26864, %v8
%v26871 = vadd.s32 4, %v26867
%v26875 = vadd.s32 %v26871, %v26859
%v26877 = vshll.u32 %v26871, 13
%v26878 = vshrl.u32 %v26871, 19
%v26879 = vor.u32 %v26878, %v26877
%v26880 = vxor.u32 %v26879, %v26875
%v26883 = vadd.s32 %v26880, %v26875
%v26885 = vshll.u32 %v26880, 15
%v26886 = vshrl.u32 %v26880, 17
%v26887 = vor.u32 %v26886, %v26885
%v26888 = vxor.u32 %v26887, %v26883
%v26891 = vadd.s32 %v26888, %v26883
%v26893 = vshll.u32 %v26888, 26
%v26894 = vshrl.u32 %v26888, 6
%v26895 = vor.u32 %v26894, %v26893
%v26896 = vxor.u32 %v26895, %v26891
%v26899 = vadd.s32 %v26896, %v26891
%v26903 = vadd.s32 %v26899, %v8
%v26905 = vshll.u32 %v26896, 6
%v26906 = vshrl.u32 %v26896, 26
%v26907 = vor.u32 %v26906, %v26905
%v26908 = vxor.u32 %v26907, %v26899
%v26911 = vadd.s32 %v26908, %v10
%v26915 = vadd.s32 5, %v26911
%v26917 = vxor.u32 %v26915, %v26903
%v26918 = vand.u32.u8 255, %v26917
%v26919 = vand.u32 65535, %v26918
%v26920 = vshrl.u32 %v26919, 1
%v26921 = vor.u32 16256, %v26920
%v26922 = vand.u32.u16 65535, %v26921
%v119888 = vadd.low.f32.bf16 -1.0, %v26922
%v26931 = vmul.f32 2.0, %v119888
%v26935 = vadd.f32 -0.99609375, %v26931
%v26939 = vmax.f32 %v26935, -0.99609375
%v26941 = vand.u32 2147483647, %v26939
%vm26944 = vcmp.eq.f32.partialorder %v26941, 1.0
%v26949 = vmul.f32 inf, %v26939
%v26951 = vxor.u32 2147483648, %v26939
%v26954 = vmul.f32 %v26951, %v26939
%v26956 = vadd.f32 1.0, %v26954
%v26957 = vlog2.pop %v26956
%v26958 = vmul.f32 0.6931472, %v26957
%v26959 = vmul.f32 -0.5, %v26954
%v26960 = vadd.f32 1.0, %v26959
%v26961 = vmul.f32 %v26960, %v26954
%v26962 = vand.u32 2147483647, %v26954
%vm26963 = vcmp.lt.f32.partialorder %v26962, 0.0004427343
%v26964 = vsel /*vm=*/%vm26963, /*on_true_vy=*/%v26961, /*on_false_vx=*/%v26958
%v26965 = vxor.u32 2147483648, %v26964
%vm26968 = vcmp.lt.f32.partialorder %v26965, 5.0
%v26973 = vsel /*vm=*/%vm26968, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v26977 = vsel /*vm=*/%vm26968, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v26981 = vsel /*vm=*/%vm26968, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v26985 = vsel /*vm=*/%vm26968, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v26989 = vsel /*vm=*/%vm26968, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v26993 = vsel /*vm=*/%vm26968, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v26997 = vsel /*vm=*/%vm26968, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v27001 = vsel /*vm=*/%vm26968, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v27005 = vsel /*vm=*/%vm26968, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v27009 = vadd.f32 -2.5, %v26965
%v27011 = vrsqrt.pop %v26965
%v27012 = vmul.f32 %v27011, %v26965
%vm27013 = vcmp.eq.f32.partialorder %v26965, inf
%v27014 = vsel /*vm=*/%vm27013, /*on_true_vy=*/%v26965, /*on_false_vx=*/%v27012
%vm27015 = vcmp.eq.f32.partialorder %v26965, 0.0
%v27016 = vand.u32 2147483648, %v26965
%v27017 = vsel /*vm=*/%vm27015, /*on_true_vy=*/%v27016, /*on_false_vx=*/%v27014
%v27020 = vadd.f32 -3.0, %v27017
%v27024 = vsel /*vm=*/%vm26968, /*on_true_vy=*/%v27009, /*on_false_vx=*/%v27020
%v27028 = vmul.f32 %v27024, %v27005
%v27032 = vadd.f32 %v27028, %v27001
%v27036 = vmul.f32 %v27032, %v27024
%v27040 = vadd.f32 %v27036, %v26997
%v27044 = vmul.f32 %v27040, %v27024
%v27048 = vadd.f32 %v27044, %v26993
%v27052 = vmul.f32 %v27048, %v27024
%v27056 = vadd.f32 %v27052, %v26989
%v27060 = vmul.f32 %v27056, %v27024
%v27064 = vadd.f32 %v27060, %v26985
%v27068 = vmul.f32 %v27064, %v27024
%v27072 = vadd.f32 %v27068, %v26981
%v27076 = vmul.f32 %v27072, %v27024
%v27080 = vadd.f32 %v27076, %v26977
%v27084 = vmul.f32 %v27080, %v27024
%v27088 = vadd.f32 %v27084, %v26973
%v27092 = vmul.f32 %v27088, %v26939
%v27096 = vsel /*vm=*/%vm26944, /*on_true_vy=*/%v26949, /*on_false_vx=*/%v27092
%v27100 = vmul.f32 1.4140625, %v27096
%v27103 = vpack.c.bf16 %v120417, %v27100
%119889 = vst [vmem:[%s280 + $0x1c] sm:$0xf] /*vst_source=*/%v27103
%v27107 = vadd.s32 %v26643, %v894
%v27117 = vadd.s32 %v27107, %v415
%vm27121 = vcmp.lt.u32.totalorder %v27117, %v27107
%vm27126 = vcmp.lt.u32.totalorder %v27107, %v894
%v27131 = vadd.s32 %v26626, %v881
%v27135 = vadd.s32 1, %v27131
%v27139 = vsel /*vm=*/%vm27126, /*on_true_vy=*/%v27135, /*on_false_vx=*/%v27131
%v27143 = vadd.s32 1, %v27139
%v27147 = vsel /*vm=*/%vm27121, /*on_true_vy=*/%v27143, /*on_false_vx=*/%v27139
%v27152 = vadd.s32 %v27147, %v10
%v27156 = vadd.s32 %v27117, %v9
%v27160 = vadd.s32 %v27156, %v27152
%v27162 = vshll.u32 %v27156, 13
%v27163 = vshrl.u32 %v27156, 19
%v27164 = vor.u32 %v27163, %v27162
%v27165 = vxor.u32 %v27164, %v27160
%v27168 = vadd.s32 %v27165, %v27160
%v27170 = vshll.u32 %v27165, 15
%v27171 = vshrl.u32 %v27165, 17
%v27172 = vor.u32 %v27171, %v27170
%v27173 = vxor.u32 %v27172, %v27168
%v27176 = vadd.s32 %v27173, %v27168
%v27178 = vshll.u32 %v27173, 26
%v27179 = vshrl.u32 %v27173, 6
%v27180 = vor.u32 %v27179, %v27178
%v27181 = vxor.u32 %v27180, %v27176
%v27184 = vadd.s32 %v27181, %v27176
%v27188 = vadd.s32 %v27184, %v9
%v27190 = vshll.u32 %v27181, 6
%v27191 = vshrl.u32 %v27181, 26
%v27192 = vor.u32 %v27191, %v27190
%v27193 = vxor.u32 %v27192, %v27184
%v27196 = vadd.s32 %v27193, %v8
%v27200 = vadd.s32 1, %v27196
%v27204 = vadd.s32 %v27200, %v27188
%v27206 = vshll.u32 %v27200, 17
%v27207 = vshrl.u32 %v27200, 15
%v27208 = vor.u32 %v27207, %v27206
%v27209 = vxor.u32 %v27208, %v27204
%v27212 = vadd.s32 %v27209, %v27204
%v27214 = vshll.u32 %v27209, 29
%v27215 = vshrl.u32 %v27209, 3
%v27216 = vor.u32 %v27215, %v27214
%v27217 = vxor.u32 %v27216, %v27212
%v27220 = vadd.s32 %v27217, %v27212
%v27222 = vshll.u32 %v27217, 16
%v27223 = vshrl.u32 %v27217, 16
%v27224 = vor.u32 %v27223, %v27222
%v27225 = vxor.u32 %v27224, %v27220
%v27228 = vadd.s32 %v27225, %v27220
%v27232 = vadd.s32 %v27228, %v8
%v27234 = vshll.u32 %v27225, 24
%v27235 = vshrl.u32 %v27225, 8
%v27236 = vor.u32 %v27235, %v27234
%v27237 = vxor.u32 %v27236, %v27228
%v27240 = vadd.s32 %v27237, %v10
%v27244 = vadd.s32 2, %v27240
%v27248 = vadd.s32 %v27244, %v27232
%v27250 = vshll.u32 %v27244, 13
%v27251 = vshrl.u32 %v27244, 19
%v27252 = vor.u32 %v27251, %v27250
%v27253 = vxor.u32 %v27252, %v27248
%v27256 = vadd.s32 %v27253, %v27248
%v27258 = vshll.u32 %v27253, 15
%v27259 = vshrl.u32 %v27253, 17
%v27260 = vor.u32 %v27259, %v27258
%v27261 = vxor.u32 %v27260, %v27256
%v27264 = vadd.s32 %v27261, %v27256
%v27266 = vshll.u32 %v27261, 26
%v27267 = vshrl.u32 %v27261, 6
%v27268 = vor.u32 %v27267, %v27266
%v27269 = vxor.u32 %v27268, %v27264
%v27272 = vadd.s32 %v27269, %v27264
%v27276 = vadd.s32 %v27272, %v10
%v27278 = vshll.u32 %v27269, 6
%v27279 = vshrl.u32 %v27269, 26
%v27280 = vor.u32 %v27279, %v27278
%v27281 = vxor.u32 %v27280, %v27272
%v27284 = vadd.s32 %v27281, %v9
%v27288 = vadd.s32 3, %v27284
%v27292 = vadd.s32 %v27288, %v27276
%v27294 = vshll.u32 %v27288, 17
%v27295 = vshrl.u32 %v27288, 15
%v27296 = vor.u32 %v27295, %v27294
%v27297 = vxor.u32 %v27296, %v27292
%v27300 = vadd.s32 %v27297, %v27292
%v27302 = vshll.u32 %v27297, 29
%v27303 = vshrl.u32 %v27297, 3
%v27304 = vor.u32 %v27303, %v27302
%v27305 = vxor.u32 %v27304, %v27300
%v27308 = vadd.s32 %v27305, %v27300
%v27310 = vshll.u32 %v27305, 16
%v27311 = vshrl.u32 %v27305, 16
%v27312 = vor.u32 %v27311, %v27310
%v27313 = vxor.u32 %v27312, %v27308
%v27316 = vadd.s32 %v27313, %v27308
%v27320 = vadd.s32 %v27316, %v9
%v27322 = vshll.u32 %v27313, 24
%v27323 = vshrl.u32 %v27313, 8
%v27324 = vor.u32 %v27323, %v27322
%v27325 = vxor.u32 %v27324, %v27316
%v27328 = vadd.s32 %v27325, %v8
%v27332 = vadd.s32 4, %v27328
%v27336 = vadd.s32 %v27332, %v27320
%v27338 = vshll.u32 %v27332, 13
%v27339 = vshrl.u32 %v27332, 19
%v27340 = vor.u32 %v27339, %v27338
%v27341 = vxor.u32 %v27340, %v27336
%v27344 = vadd.s32 %v27341, %v27336
%v27346 = vshll.u32 %v27341, 15
%v27347 = vshrl.u32 %v27341, 17
%v27348 = vor.u32 %v27347, %v27346
%v27349 = vxor.u32 %v27348, %v27344
%v27352 = vadd.s32 %v27349, %v27344
%v27354 = vshll.u32 %v27349, 26
%v27355 = vshrl.u32 %v27349, 6
%v27356 = vor.u32 %v27355, %v27354
%v27357 = vxor.u32 %v27356, %v27352
%v27360 = vadd.s32 %v27357, %v27352
%v27364 = vadd.s32 %v27360, %v8
%v27366 = vshll.u32 %v27357, 6
%v27367 = vshrl.u32 %v27357, 26
%v27368 = vor.u32 %v27367, %v27366
%v27369 = vxor.u32 %v27368, %v27360
%v27372 = vadd.s32 %v27369, %v10
%v27376 = vadd.s32 5, %v27372
%v27378 = vxor.u32 %v27376, %v27364
%v27379 = vand.u32.u8 255, %v27378
%v27380 = vand.u32 65535, %v27379
%v27381 = vshrl.u32 %v27380, 1
%v27382 = vor.u32 16256, %v27381
%v27383 = vand.u32.u16 65535, %v27382
%v119890 = vadd.low.f32.bf16 -1.0, %v27383
%v27392 = vmul.f32 2.0, %v119890
%v27396 = vadd.f32 -0.99609375, %v27392
%v27400 = vmax.f32 %v27396, -0.99609375
%v27402 = vand.u32 2147483647, %v27400
%vm27405 = vcmp.eq.f32.partialorder %v27402, 1.0
%v27410 = vmul.f32 inf, %v27400
%v27412 = vxor.u32 2147483648, %v27400
%v27415 = vmul.f32 %v27412, %v27400
%v27417 = vadd.f32 1.0, %v27415
%v27418 = vlog2.pop %v27417
%v27419 = vmul.f32 0.6931472, %v27418
%v27420 = vmul.f32 -0.5, %v27415
%v27421 = vadd.f32 1.0, %v27420
%v27422 = vmul.f32 %v27421, %v27415
%v27423 = vand.u32 2147483647, %v27415
%vm27424 = vcmp.lt.f32.partialorder %v27423, 0.0004427343
%v27425 = vsel /*vm=*/%vm27424, /*on_true_vy=*/%v27422, /*on_false_vx=*/%v27419
%v27426 = vxor.u32 2147483648, %v27425
%vm27429 = vcmp.lt.f32.partialorder %v27426, 5.0
%v27434 = vsel /*vm=*/%vm27429, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v27438 = vsel /*vm=*/%vm27429, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v27442 = vsel /*vm=*/%vm27429, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v27446 = vsel /*vm=*/%vm27429, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v27450 = vsel /*vm=*/%vm27429, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v27454 = vsel /*vm=*/%vm27429, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v27458 = vsel /*vm=*/%vm27429, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v27462 = vsel /*vm=*/%vm27429, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v27466 = vsel /*vm=*/%vm27429, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v27470 = vadd.f32 -2.5, %v27426
%v27472 = vrsqrt.pop %v27426
%v27473 = vmul.f32 %v27472, %v27426
%vm27474 = vcmp.eq.f32.partialorder %v27426, inf
%v27475 = vsel /*vm=*/%vm27474, /*on_true_vy=*/%v27426, /*on_false_vx=*/%v27473
%vm27476 = vcmp.eq.f32.partialorder %v27426, 0.0
%v27477 = vand.u32 2147483648, %v27426
%v27478 = vsel /*vm=*/%vm27476, /*on_true_vy=*/%v27477, /*on_false_vx=*/%v27475
%v27481 = vadd.f32 -3.0, %v27478
%v27485 = vsel /*vm=*/%vm27429, /*on_true_vy=*/%v27470, /*on_false_vx=*/%v27481
%v27489 = vmul.f32 %v27485, %v27466
%v27493 = vadd.f32 %v27489, %v27462
%v27497 = vmul.f32 %v27493, %v27485
%v27501 = vadd.f32 %v27497, %v27458
%v27505 = vmul.f32 %v27501, %v27485
%v27509 = vadd.f32 %v27505, %v27454
%v27513 = vmul.f32 %v27509, %v27485
%v27517 = vadd.f32 %v27513, %v27450
%v27521 = vmul.f32 %v27517, %v27485
%v27525 = vadd.f32 %v27521, %v27446
%v27529 = vmul.f32 %v27525, %v27485
%v27533 = vadd.f32 %v27529, %v27442
%v27537 = vmul.f32 %v27533, %v27485
%v27541 = vadd.f32 %v27537, %v27438
%v27545 = vmul.f32 %v27541, %v27485
%v27549 = vadd.f32 %v27545, %v27434
%v27553 = vmul.f32 %v27549, %v27400
%v27557 = vsel /*vm=*/%vm27405, /*on_true_vy=*/%v27410, /*on_false_vx=*/%v27553
%v27561 = vmul.f32 1.4140625, %v27557
%v27564 = vpack.c.bf16 %v120417, %v27561
%119891 = vst [vmem:[%s280 + $0x9c] sm:$0xf] /*vst_source=*/%v27564
%v27568 = vadd.s32 %v26643, %v1381
%v27578 = vadd.s32 %v27568, %v415
%vm27582 = vcmp.lt.u32.totalorder %v27578, %v27568
%vm27587 = vcmp.lt.u32.totalorder %v27568, %v1381
%v27592 = vadd.s32 %v26626, %v1368
%v27596 = vadd.s32 1, %v27592
%v27600 = vsel /*vm=*/%vm27587, /*on_true_vy=*/%v27596, /*on_false_vx=*/%v27592
%v27604 = vadd.s32 1, %v27600
%v27608 = vsel /*vm=*/%vm27582, /*on_true_vy=*/%v27604, /*on_false_vx=*/%v27600
%v27613 = vadd.s32 %v27608, %v10
%v27617 = vadd.s32 %v27578, %v9
%v27621 = vadd.s32 %v27617, %v27613
%v27623 = vshll.u32 %v27617, 13
%v27624 = vshrl.u32 %v27617, 19
%v27625 = vor.u32 %v27624, %v27623
%v27626 = vxor.u32 %v27625, %v27621
%v27629 = vadd.s32 %v27626, %v27621
%v27631 = vshll.u32 %v27626, 15
%v27632 = vshrl.u32 %v27626, 17
%v27633 = vor.u32 %v27632, %v27631
%v27634 = vxor.u32 %v27633, %v27629
%v27637 = vadd.s32 %v27634, %v27629
%v27639 = vshll.u32 %v27634, 26
%v27640 = vshrl.u32 %v27634, 6
%v27641 = vor.u32 %v27640, %v27639
%v27642 = vxor.u32 %v27641, %v27637
%v27645 = vadd.s32 %v27642, %v27637
%v27649 = vadd.s32 %v27645, %v9
%v27651 = vshll.u32 %v27642, 6
%v27652 = vshrl.u32 %v27642, 26
%v27653 = vor.u32 %v27652, %v27651
%v27654 = vxor.u32 %v27653, %v27645
%v27657 = vadd.s32 %v27654, %v8
%v27661 = vadd.s32 1, %v27657
%v27665 = vadd.s32 %v27661, %v27649
%v27667 = vshll.u32 %v27661, 17
%v27668 = vshrl.u32 %v27661, 15
%v27669 = vor.u32 %v27668, %v27667
%v27670 = vxor.u32 %v27669, %v27665
%v27673 = vadd.s32 %v27670, %v27665
%v27675 = vshll.u32 %v27670, 29
%v27676 = vshrl.u32 %v27670, 3
%v27677 = vor.u32 %v27676, %v27675
%v27678 = vxor.u32 %v27677, %v27673
%v27681 = vadd.s32 %v27678, %v27673
%v27683 = vshll.u32 %v27678, 16
%v27684 = vshrl.u32 %v27678, 16
%v27685 = vor.u32 %v27684, %v27683
%v27686 = vxor.u32 %v27685, %v27681
%v27689 = vadd.s32 %v27686, %v27681
%v27693 = vadd.s32 %v27689, %v8
%v27695 = vshll.u32 %v27686, 24
%v27696 = vshrl.u32 %v27686, 8
%v27697 = vor.u32 %v27696, %v27695
%v27698 = vxor.u32 %v27697, %v27689
%v27701 = vadd.s32 %v27698, %v10
%v27705 = vadd.s32 2, %v27701
%v27709 = vadd.s32 %v27705, %v27693
%v27711 = vshll.u32 %v27705, 13
%v27712 = vshrl.u32 %v27705, 19
%v27713 = vor.u32 %v27712, %v27711
%v27714 = vxor.u32 %v27713, %v27709
%v27717 = vadd.s32 %v27714, %v27709
%v27719 = vshll.u32 %v27714, 15
%v27720 = vshrl.u32 %v27714, 17
%v27721 = vor.u32 %v27720, %v27719
%v27722 = vxor.u32 %v27721, %v27717
%v27725 = vadd.s32 %v27722, %v27717
%v27727 = vshll.u32 %v27722, 26
%v27728 = vshrl.u32 %v27722, 6
%v27729 = vor.u32 %v27728, %v27727
%v27730 = vxor.u32 %v27729, %v27725
%v27733 = vadd.s32 %v27730, %v27725
%v27737 = vadd.s32 %v27733, %v10
%v27739 = vshll.u32 %v27730, 6
%v27740 = vshrl.u32 %v27730, 26
%v27741 = vor.u32 %v27740, %v27739
%v27742 = vxor.u32 %v27741, %v27733
%v27745 = vadd.s32 %v27742, %v9
%v27749 = vadd.s32 3, %v27745
%v27753 = vadd.s32 %v27749, %v27737
%v27755 = vshll.u32 %v27749, 17
%v27756 = vshrl.u32 %v27749, 15
%v27757 = vor.u32 %v27756, %v27755
%v27758 = vxor.u32 %v27757, %v27753
%v27761 = vadd.s32 %v27758, %v27753
%v27763 = vshll.u32 %v27758, 29
%v27764 = vshrl.u32 %v27758, 3
%v27765 = vor.u32 %v27764, %v27763
%v27766 = vxor.u32 %v27765, %v27761
%v27769 = vadd.s32 %v27766, %v27761
%v27771 = vshll.u32 %v27766, 16
%v27772 = vshrl.u32 %v27766, 16
%v27773 = vor.u32 %v27772, %v27771
%v27774 = vxor.u32 %v27773, %v27769
%v27777 = vadd.s32 %v27774, %v27769
%v27781 = vadd.s32 %v27777, %v9
%v27783 = vshll.u32 %v27774, 24
%v27784 = vshrl.u32 %v27774, 8
%v27785 = vor.u32 %v27784, %v27783
%v27786 = vxor.u32 %v27785, %v27777
%v27789 = vadd.s32 %v27786, %v8
%v27793 = vadd.s32 4, %v27789
%v27797 = vadd.s32 %v27793, %v27781
%v27799 = vshll.u32 %v27793, 13
%v27800 = vshrl.u32 %v27793, 19
%v27801 = vor.u32 %v27800, %v27799
%v27802 = vxor.u32 %v27801, %v27797
%v27805 = vadd.s32 %v27802, %v27797
%v27807 = vshll.u32 %v27802, 15
%v27808 = vshrl.u32 %v27802, 17
%v27809 = vor.u32 %v27808, %v27807
%v27810 = vxor.u32 %v27809, %v27805
%v27813 = vadd.s32 %v27810, %v27805
%v27815 = vshll.u32 %v27810, 26
%v27816 = vshrl.u32 %v27810, 6
%v27817 = vor.u32 %v27816, %v27815
%v27818 = vxor.u32 %v27817, %v27813
%v27821 = vadd.s32 %v27818, %v27813
%v27825 = vadd.s32 %v27821, %v8
%v27827 = vshll.u32 %v27818, 6
%v27828 = vshrl.u32 %v27818, 26
%v27829 = vor.u32 %v27828, %v27827
%v27830 = vxor.u32 %v27829, %v27821
%v27833 = vadd.s32 %v27830, %v10
%v27837 = vadd.s32 5, %v27833
%v27839 = vxor.u32 %v27837, %v27825
%v27840 = vand.u32.u8 255, %v27839
%v27841 = vand.u32 65535, %v27840
%v27842 = vshrl.u32 %v27841, 1
%v27843 = vor.u32 16256, %v27842
%v27844 = vand.u32.u16 65535, %v27843
%v119892 = vadd.low.f32.bf16 -1.0, %v27844
%v27853 = vmul.f32 2.0, %v119892
%v27857 = vadd.f32 -0.99609375, %v27853
%v27861 = vmax.f32 %v27857, -0.99609375
%v27863 = vand.u32 2147483647, %v27861
%vm27866 = vcmp.eq.f32.partialorder %v27863, 1.0
%v27871 = vmul.f32 inf, %v27861
%v27873 = vxor.u32 2147483648, %v27861
%v27876 = vmul.f32 %v27873, %v27861
%v27878 = vadd.f32 1.0, %v27876
%v27879 = vlog2.pop %v27878
%v27880 = vmul.f32 0.6931472, %v27879
%v27881 = vmul.f32 -0.5, %v27876
%v27882 = vadd.f32 1.0, %v27881
%v27883 = vmul.f32 %v27882, %v27876
%v27884 = vand.u32 2147483647, %v27876
%vm27885 = vcmp.lt.f32.partialorder %v27884, 0.0004427343
%v27886 = vsel /*vm=*/%vm27885, /*on_true_vy=*/%v27883, /*on_false_vx=*/%v27880
%v27887 = vxor.u32 2147483648, %v27886
%vm27890 = vcmp.lt.f32.partialorder %v27887, 5.0
%v27895 = vsel /*vm=*/%vm27890, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v27899 = vsel /*vm=*/%vm27890, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v27903 = vsel /*vm=*/%vm27890, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v27907 = vsel /*vm=*/%vm27890, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v27911 = vsel /*vm=*/%vm27890, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v27915 = vsel /*vm=*/%vm27890, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v27919 = vsel /*vm=*/%vm27890, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v27923 = vsel /*vm=*/%vm27890, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v27927 = vsel /*vm=*/%vm27890, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v27931 = vadd.f32 -2.5, %v27887
%v27933 = vrsqrt.pop %v27887
%v27934 = vmul.f32 %v27933, %v27887
%vm27935 = vcmp.eq.f32.partialorder %v27887, inf
%v27936 = vsel /*vm=*/%vm27935, /*on_true_vy=*/%v27887, /*on_false_vx=*/%v27934
%vm27937 = vcmp.eq.f32.partialorder %v27887, 0.0
%v27938 = vand.u32 2147483648, %v27887
%v27939 = vsel /*vm=*/%vm27937, /*on_true_vy=*/%v27938, /*on_false_vx=*/%v27936
%v27942 = vadd.f32 -3.0, %v27939
%v27946 = vsel /*vm=*/%vm27890, /*on_true_vy=*/%v27931, /*on_false_vx=*/%v27942
%v27950 = vmul.f32 %v27946, %v27927
%v27954 = vadd.f32 %v27950, %v27923
%v27958 = vmul.f32 %v27954, %v27946
%v27962 = vadd.f32 %v27958, %v27919
%v27966 = vmul.f32 %v27962, %v27946
%v27970 = vadd.f32 %v27966, %v27915
%v27974 = vmul.f32 %v27970, %v27946
%v27978 = vadd.f32 %v27974, %v27911
%v27982 = vmul.f32 %v27978, %v27946
%v27986 = vadd.f32 %v27982, %v27907
%v27990 = vmul.f32 %v27986, %v27946
%v27994 = vadd.f32 %v27990, %v27903
%v27998 = vmul.f32 %v27994, %v27946
%v28002 = vadd.f32 %v27998, %v27899
%v28006 = vmul.f32 %v28002, %v27946
%v28010 = vadd.f32 %v28006, %v27895
%v28014 = vmul.f32 %v28010, %v27861
%v28018 = vsel /*vm=*/%vm27866, /*on_true_vy=*/%v27871, /*on_false_vx=*/%v28014
%v28022 = vmul.f32 1.4140625, %v28018
%v28025 = vpack.c.bf16 %v120417, %v28022
%119893 = vst [vmem:[%s280 + $0x11c] sm:$0xf] /*vst_source=*/%v28025
%v28029 = vadd.s32 %v26643, %v1868
%v28039 = vadd.s32 %v28029, %v415
%vm28043 = vcmp.lt.u32.totalorder %v28039, %v28029
%vm28048 = vcmp.lt.u32.totalorder %v28029, %v1868
%v28053 = vadd.s32 %v26626, %v1855
%v28057 = vadd.s32 1, %v28053
%v28061 = vsel /*vm=*/%vm28048, /*on_true_vy=*/%v28057, /*on_false_vx=*/%v28053
%v28065 = vadd.s32 1, %v28061
%v28069 = vsel /*vm=*/%vm28043, /*on_true_vy=*/%v28065, /*on_false_vx=*/%v28061
%v28074 = vadd.s32 %v28069, %v10
%v28078 = vadd.s32 %v28039, %v9
%v28082 = vadd.s32 %v28078, %v28074
%v28084 = vshll.u32 %v28078, 13
%v28085 = vshrl.u32 %v28078, 19
%v28086 = vor.u32 %v28085, %v28084
%v28087 = vxor.u32 %v28086, %v28082
%v28090 = vadd.s32 %v28087, %v28082
%v28092 = vshll.u32 %v28087, 15
%v28093 = vshrl.u32 %v28087, 17
%v28094 = vor.u32 %v28093, %v28092
%v28095 = vxor.u32 %v28094, %v28090
%v28098 = vadd.s32 %v28095, %v28090
%v28100 = vshll.u32 %v28095, 26
%v28101 = vshrl.u32 %v28095, 6
%v28102 = vor.u32 %v28101, %v28100
%v28103 = vxor.u32 %v28102, %v28098
%v28106 = vadd.s32 %v28103, %v28098
%v28110 = vadd.s32 %v28106, %v9
%v28112 = vshll.u32 %v28103, 6
%v28113 = vshrl.u32 %v28103, 26
%v28114 = vor.u32 %v28113, %v28112
%v28115 = vxor.u32 %v28114, %v28106
%v28118 = vadd.s32 %v28115, %v8
%v28122 = vadd.s32 1, %v28118
%v28126 = vadd.s32 %v28122, %v28110
%v28128 = vshll.u32 %v28122, 17
%v28129 = vshrl.u32 %v28122, 15
%v28130 = vor.u32 %v28129, %v28128
%v28131 = vxor.u32 %v28130, %v28126
%v28134 = vadd.s32 %v28131, %v28126
%v28136 = vshll.u32 %v28131, 29
%v28137 = vshrl.u32 %v28131, 3
%v28138 = vor.u32 %v28137, %v28136
%v28139 = vxor.u32 %v28138, %v28134
%v28142 = vadd.s32 %v28139, %v28134
%v28144 = vshll.u32 %v28139, 16
%v28145 = vshrl.u32 %v28139, 16
%v28146 = vor.u32 %v28145, %v28144
%v28147 = vxor.u32 %v28146, %v28142
%v28150 = vadd.s32 %v28147, %v28142
%v28154 = vadd.s32 %v28150, %v8
%v28156 = vshll.u32 %v28147, 24
%v28157 = vshrl.u32 %v28147, 8
%v28158 = vor.u32 %v28157, %v28156
%v28159 = vxor.u32 %v28158, %v28150
%v28162 = vadd.s32 %v28159, %v10
%v28166 = vadd.s32 2, %v28162
%v28170 = vadd.s32 %v28166, %v28154
%v28172 = vshll.u32 %v28166, 13
%v28173 = vshrl.u32 %v28166, 19
%v28174 = vor.u32 %v28173, %v28172
%v28175 = vxor.u32 %v28174, %v28170
%v28178 = vadd.s32 %v28175, %v28170
%v28180 = vshll.u32 %v28175, 15
%v28181 = vshrl.u32 %v28175, 17
%v28182 = vor.u32 %v28181, %v28180
%v28183 = vxor.u32 %v28182, %v28178
%v28186 = vadd.s32 %v28183, %v28178
%v28188 = vshll.u32 %v28183, 26
%v28189 = vshrl.u32 %v28183, 6
%v28190 = vor.u32 %v28189, %v28188
%v28191 = vxor.u32 %v28190, %v28186
%v28194 = vadd.s32 %v28191, %v28186
%v28198 = vadd.s32 %v28194, %v10
%v28200 = vshll.u32 %v28191, 6
%v28201 = vshrl.u32 %v28191, 26
%v28202 = vor.u32 %v28201, %v28200
%v28203 = vxor.u32 %v28202, %v28194
%v28206 = vadd.s32 %v28203, %v9
%v28210 = vadd.s32 3, %v28206
%v28214 = vadd.s32 %v28210, %v28198
%v28216 = vshll.u32 %v28210, 17
%v28217 = vshrl.u32 %v28210, 15
%v28218 = vor.u32 %v28217, %v28216
%v28219 = vxor.u32 %v28218, %v28214
%v28222 = vadd.s32 %v28219, %v28214
%v28224 = vshll.u32 %v28219, 29
%v28225 = vshrl.u32 %v28219, 3
%v28226 = vor.u32 %v28225, %v28224
%v28227 = vxor.u32 %v28226, %v28222
%v28230 = vadd.s32 %v28227, %v28222
%v28232 = vshll.u32 %v28227, 16
%v28233 = vshrl.u32 %v28227, 16
%v28234 = vor.u32 %v28233, %v28232
%v28235 = vxor.u32 %v28234, %v28230
%v28238 = vadd.s32 %v28235, %v28230
%v28242 = vadd.s32 %v28238, %v9
%v28244 = vshll.u32 %v28235, 24
%v28245 = vshrl.u32 %v28235, 8
%v28246 = vor.u32 %v28245, %v28244
%v28247 = vxor.u32 %v28246, %v28238
%v28250 = vadd.s32 %v28247, %v8
%v28254 = vadd.s32 4, %v28250
%v28258 = vadd.s32 %v28254, %v28242
%v28260 = vshll.u32 %v28254, 13
%v28261 = vshrl.u32 %v28254, 19
%v28262 = vor.u32 %v28261, %v28260
%v28263 = vxor.u32 %v28262, %v28258
%v28266 = vadd.s32 %v28263, %v28258
%v28268 = vshll.u32 %v28263, 15
%v28269 = vshrl.u32 %v28263, 17
%v28270 = vor.u32 %v28269, %v28268
%v28271 = vxor.u32 %v28270, %v28266
%v28274 = vadd.s32 %v28271, %v28266
%v28276 = vshll.u32 %v28271, 26
%v28277 = vshrl.u32 %v28271, 6
%v28278 = vor.u32 %v28277, %v28276
%v28279 = vxor.u32 %v28278, %v28274
%v28282 = vadd.s32 %v28279, %v28274
%v28286 = vadd.s32 %v28282, %v8
%v28288 = vshll.u32 %v28279, 6
%v28289 = vshrl.u32 %v28279, 26
%v28290 = vor.u32 %v28289, %v28288
%v28291 = vxor.u32 %v28290, %v28282
%v28294 = vadd.s32 %v28291, %v10
%v28298 = vadd.s32 5, %v28294
%v28300 = vxor.u32 %v28298, %v28286
%v28301 = vand.u32.u8 255, %v28300
%v28302 = vand.u32 65535, %v28301
%v28303 = vshrl.u32 %v28302, 1
%v28304 = vor.u32 16256, %v28303
%v28305 = vand.u32.u16 65535, %v28304
%v119894 = vadd.low.f32.bf16 -1.0, %v28305
%v28314 = vmul.f32 2.0, %v119894
%v28318 = vadd.f32 -0.99609375, %v28314
%v28322 = vmax.f32 %v28318, -0.99609375
%v28324 = vand.u32 2147483647, %v28322
%vm28327 = vcmp.eq.f32.partialorder %v28324, 1.0
%v28332 = vmul.f32 inf, %v28322
%v28334 = vxor.u32 2147483648, %v28322
%v28337 = vmul.f32 %v28334, %v28322
%v28339 = vadd.f32 1.0, %v28337
%v28340 = vlog2.pop %v28339
%v28341 = vmul.f32 0.6931472, %v28340
%v28342 = vmul.f32 -0.5, %v28337
%v28343 = vadd.f32 1.0, %v28342
%v28344 = vmul.f32 %v28343, %v28337
%v28345 = vand.u32 2147483647, %v28337
%vm28346 = vcmp.lt.f32.partialorder %v28345, 0.0004427343
%v28347 = vsel /*vm=*/%vm28346, /*on_true_vy=*/%v28344, /*on_false_vx=*/%v28341
%v28348 = vxor.u32 2147483648, %v28347
%vm28351 = vcmp.lt.f32.partialorder %v28348, 5.0
%v28356 = vsel /*vm=*/%vm28351, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v28360 = vsel /*vm=*/%vm28351, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v28364 = vsel /*vm=*/%vm28351, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v28368 = vsel /*vm=*/%vm28351, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v28372 = vsel /*vm=*/%vm28351, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v28376 = vsel /*vm=*/%vm28351, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v28380 = vsel /*vm=*/%vm28351, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v28384 = vsel /*vm=*/%vm28351, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v28388 = vsel /*vm=*/%vm28351, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v28392 = vadd.f32 -2.5, %v28348
%v28394 = vrsqrt.pop %v28348
%v28395 = vmul.f32 %v28394, %v28348
%vm28396 = vcmp.eq.f32.partialorder %v28348, inf
%v28397 = vsel /*vm=*/%vm28396, /*on_true_vy=*/%v28348, /*on_false_vx=*/%v28395
%vm28398 = vcmp.eq.f32.partialorder %v28348, 0.0
%v28399 = vand.u32 2147483648, %v28348
%v28400 = vsel /*vm=*/%vm28398, /*on_true_vy=*/%v28399, /*on_false_vx=*/%v28397
%v28403 = vadd.f32 -3.0, %v28400
%v28407 = vsel /*vm=*/%vm28351, /*on_true_vy=*/%v28392, /*on_false_vx=*/%v28403
%v28411 = vmul.f32 %v28407, %v28388
%v28415 = vadd.f32 %v28411, %v28384
%v28419 = vmul.f32 %v28415, %v28407
%v28423 = vadd.f32 %v28419, %v28380
%v28427 = vmul.f32 %v28423, %v28407
%v28431 = vadd.f32 %v28427, %v28376
%v28435 = vmul.f32 %v28431, %v28407
%v28439 = vadd.f32 %v28435, %v28372
%v28443 = vmul.f32 %v28439, %v28407
%v28447 = vadd.f32 %v28443, %v28368
%v28451 = vmul.f32 %v28447, %v28407
%v28455 = vadd.f32 %v28451, %v28364
%v28459 = vmul.f32 %v28455, %v28407
%v28463 = vadd.f32 %v28459, %v28360
%v28467 = vmul.f32 %v28463, %v28407
%v28471 = vadd.f32 %v28467, %v28356
%v28475 = vmul.f32 %v28471, %v28322
%v28479 = vsel /*vm=*/%vm28327, /*on_true_vy=*/%v28332, /*on_false_vx=*/%v28475
%v28483 = vmul.f32 1.4140625, %v28479
%v28486 = vpack.c.bf16 %v120417, %v28483
%119895 = vst [vmem:[%s280 + $0x19c] sm:$0xf] /*vst_source=*/%v28486
%v28490 = vadd.s32 %v26643, %v2355
%v28500 = vadd.s32 %v28490, %v415
%vm28504 = vcmp.lt.u32.totalorder %v28500, %v28490
%vm28509 = vcmp.lt.u32.totalorder %v28490, %v2355
%v28514 = vadd.s32 %v26626, %v2342
%v28518 = vadd.s32 1, %v28514
%v28522 = vsel /*vm=*/%vm28509, /*on_true_vy=*/%v28518, /*on_false_vx=*/%v28514
%v28526 = vadd.s32 1, %v28522
%v28530 = vsel /*vm=*/%vm28504, /*on_true_vy=*/%v28526, /*on_false_vx=*/%v28522
%v28535 = vadd.s32 %v28530, %v10
%v28539 = vadd.s32 %v28500, %v9
%v28543 = vadd.s32 %v28539, %v28535
%v28545 = vshll.u32 %v28539, 13
%v28546 = vshrl.u32 %v28539, 19
%v28547 = vor.u32 %v28546, %v28545
%v28548 = vxor.u32 %v28547, %v28543
%v28551 = vadd.s32 %v28548, %v28543
%v28553 = vshll.u32 %v28548, 15
%v28554 = vshrl.u32 %v28548, 17
%v28555 = vor.u32 %v28554, %v28553
%v28556 = vxor.u32 %v28555, %v28551
%v28559 = vadd.s32 %v28556, %v28551
%v28561 = vshll.u32 %v28556, 26
%v28562 = vshrl.u32 %v28556, 6
%v28563 = vor.u32 %v28562, %v28561
%v28564 = vxor.u32 %v28563, %v28559
%v28567 = vadd.s32 %v28564, %v28559
%v28571 = vadd.s32 %v28567, %v9
%v28573 = vshll.u32 %v28564, 6
%v28574 = vshrl.u32 %v28564, 26
%v28575 = vor.u32 %v28574, %v28573
%v28576 = vxor.u32 %v28575, %v28567
%v28579 = vadd.s32 %v28576, %v8
%v28583 = vadd.s32 1, %v28579
%v28587 = vadd.s32 %v28583, %v28571
%v28589 = vshll.u32 %v28583, 17
%v28590 = vshrl.u32 %v28583, 15
%v28591 = vor.u32 %v28590, %v28589
%v28592 = vxor.u32 %v28591, %v28587
%v28595 = vadd.s32 %v28592, %v28587
%v28597 = vshll.u32 %v28592, 29
%v28598 = vshrl.u32 %v28592, 3
%v28599 = vor.u32 %v28598, %v28597
%v28600 = vxor.u32 %v28599, %v28595
%v28603 = vadd.s32 %v28600, %v28595
%v28605 = vshll.u32 %v28600, 16
%v28606 = vshrl.u32 %v28600, 16
%v28607 = vor.u32 %v28606, %v28605
%v28608 = vxor.u32 %v28607, %v28603
%v28611 = vadd.s32 %v28608, %v28603
%v28615 = vadd.s32 %v28611, %v8
%v28617 = vshll.u32 %v28608, 24
%v28618 = vshrl.u32 %v28608, 8
%v28619 = vor.u32 %v28618, %v28617
%v28620 = vxor.u32 %v28619, %v28611
%v28623 = vadd.s32 %v28620, %v10
%v28627 = vadd.s32 2, %v28623
%v28631 = vadd.s32 %v28627, %v28615
%v28633 = vshll.u32 %v28627, 13
%v28634 = vshrl.u32 %v28627, 19
%v28635 = vor.u32 %v28634, %v28633
%v28636 = vxor.u32 %v28635, %v28631
%v28639 = vadd.s32 %v28636, %v28631
%v28641 = vshll.u32 %v28636, 15
%v28642 = vshrl.u32 %v28636, 17
%v28643 = vor.u32 %v28642, %v28641
%v28644 = vxor.u32 %v28643, %v28639
%v28647 = vadd.s32 %v28644, %v28639
%v28649 = vshll.u32 %v28644, 26
%v28650 = vshrl.u32 %v28644, 6
%v28651 = vor.u32 %v28650, %v28649
%v28652 = vxor.u32 %v28651, %v28647
%v28655 = vadd.s32 %v28652, %v28647
%v28659 = vadd.s32 %v28655, %v10
%v28661 = vshll.u32 %v28652, 6
%v28662 = vshrl.u32 %v28652, 26
%v28663 = vor.u32 %v28662, %v28661
%v28664 = vxor.u32 %v28663, %v28655
%v28667 = vadd.s32 %v28664, %v9
%v28671 = vadd.s32 3, %v28667
%v28675 = vadd.s32 %v28671, %v28659
%v28677 = vshll.u32 %v28671, 17
%v28678 = vshrl.u32 %v28671, 15
%v28679 = vor.u32 %v28678, %v28677
%v28680 = vxor.u32 %v28679, %v28675
%v28683 = vadd.s32 %v28680, %v28675
%v28685 = vshll.u32 %v28680, 29
%v28686 = vshrl.u32 %v28680, 3
%v28687 = vor.u32 %v28686, %v28685
%v28688 = vxor.u32 %v28687, %v28683
%v28691 = vadd.s32 %v28688, %v28683
%v28693 = vshll.u32 %v28688, 16
%v28694 = vshrl.u32 %v28688, 16
%v28695 = vor.u32 %v28694, %v28693
%v28696 = vxor.u32 %v28695, %v28691
%v28699 = vadd.s32 %v28696, %v28691
%v28703 = vadd.s32 %v28699, %v9
%v28705 = vshll.u32 %v28696, 24
%v28706 = vshrl.u32 %v28696, 8
%v28707 = vor.u32 %v28706, %v28705
%v28708 = vxor.u32 %v28707, %v28699
%v28711 = vadd.s32 %v28708, %v8
%v28715 = vadd.s32 4, %v28711
%v28719 = vadd.s32 %v28715, %v28703
%v28721 = vshll.u32 %v28715, 13
%v28722 = vshrl.u32 %v28715, 19
%v28723 = vor.u32 %v28722, %v28721
%v28724 = vxor.u32 %v28723, %v28719
%v28727 = vadd.s32 %v28724, %v28719
%v28729 = vshll.u32 %v28724, 15
%v28730 = vshrl.u32 %v28724, 17
%v28731 = vor.u32 %v28730, %v28729
%v28732 = vxor.u32 %v28731, %v28727
%v28735 = vadd.s32 %v28732, %v28727
%v28737 = vshll.u32 %v28732, 26
%v28738 = vshrl.u32 %v28732, 6
%v28739 = vor.u32 %v28738, %v28737
%v28740 = vxor.u32 %v28739, %v28735
%v28743 = vadd.s32 %v28740, %v28735
%v28747 = vadd.s32 %v28743, %v8
%v28749 = vshll.u32 %v28740, 6
%v28750 = vshrl.u32 %v28740, 26
%v28751 = vor.u32 %v28750, %v28749
%v28752 = vxor.u32 %v28751, %v28743
%v28755 = vadd.s32 %v28752, %v10
%v28759 = vadd.s32 5, %v28755
%v28761 = vxor.u32 %v28759, %v28747
%v28762 = vand.u32.u8 255, %v28761
%v28763 = vand.u32 65535, %v28762
%v28764 = vshrl.u32 %v28763, 1
%v28765 = vor.u32 16256, %v28764
%v28766 = vand.u32.u16 65535, %v28765
%v119896 = vadd.low.f32.bf16 -1.0, %v28766
%v28775 = vmul.f32 2.0, %v119896
%v28779 = vadd.f32 -0.99609375, %v28775
%v28783 = vmax.f32 %v28779, -0.99609375
%v28785 = vand.u32 2147483647, %v28783
%vm28788 = vcmp.eq.f32.partialorder %v28785, 1.0
%v28793 = vmul.f32 inf, %v28783
%v28795 = vxor.u32 2147483648, %v28783
%v28798 = vmul.f32 %v28795, %v28783
%v28800 = vadd.f32 1.0, %v28798
%v28801 = vlog2.pop %v28800
%v28802 = vmul.f32 0.6931472, %v28801
%v28803 = vmul.f32 -0.5, %v28798
%v28804 = vadd.f32 1.0, %v28803
%v28805 = vmul.f32 %v28804, %v28798
%v28806 = vand.u32 2147483647, %v28798
%vm28807 = vcmp.lt.f32.partialorder %v28806, 0.0004427343
%v28808 = vsel /*vm=*/%vm28807, /*on_true_vy=*/%v28805, /*on_false_vx=*/%v28802
%v28809 = vxor.u32 2147483648, %v28808
%vm28812 = vcmp.lt.f32.partialorder %v28809, 5.0
%v28817 = vsel /*vm=*/%vm28812, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v28821 = vsel /*vm=*/%vm28812, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v28825 = vsel /*vm=*/%vm28812, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v28829 = vsel /*vm=*/%vm28812, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v28833 = vsel /*vm=*/%vm28812, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v28837 = vsel /*vm=*/%vm28812, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v28841 = vsel /*vm=*/%vm28812, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v28845 = vsel /*vm=*/%vm28812, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v28849 = vsel /*vm=*/%vm28812, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v28853 = vadd.f32 -2.5, %v28809
%v28855 = vrsqrt.pop %v28809
%v28856 = vmul.f32 %v28855, %v28809
%vm28857 = vcmp.eq.f32.partialorder %v28809, inf
%v28858 = vsel /*vm=*/%vm28857, /*on_true_vy=*/%v28809, /*on_false_vx=*/%v28856
%vm28859 = vcmp.eq.f32.partialorder %v28809, 0.0
%v28860 = vand.u32 2147483648, %v28809
%v28861 = vsel /*vm=*/%vm28859, /*on_true_vy=*/%v28860, /*on_false_vx=*/%v28858
%v28864 = vadd.f32 -3.0, %v28861
%v28868 = vsel /*vm=*/%vm28812, /*on_true_vy=*/%v28853, /*on_false_vx=*/%v28864
%v28872 = vmul.f32 %v28868, %v28849
%v28876 = vadd.f32 %v28872, %v28845
%v28880 = vmul.f32 %v28876, %v28868
%v28884 = vadd.f32 %v28880, %v28841
%v28888 = vmul.f32 %v28884, %v28868
%v28892 = vadd.f32 %v28888, %v28837
%v28896 = vmul.f32 %v28892, %v28868
%v28900 = vadd.f32 %v28896, %v28833
%v28904 = vmul.f32 %v28900, %v28868
%v28908 = vadd.f32 %v28904, %v28829
%v28912 = vmul.f32 %v28908, %v28868
%v28916 = vadd.f32 %v28912, %v28825
%v28920 = vmul.f32 %v28916, %v28868
%v28924 = vadd.f32 %v28920, %v28821
%v28928 = vmul.f32 %v28924, %v28868
%v28932 = vadd.f32 %v28928, %v28817
%v28936 = vmul.f32 %v28932, %v28783
%v28940 = vsel /*vm=*/%vm28788, /*on_true_vy=*/%v28793, /*on_false_vx=*/%v28936
%v28944 = vmul.f32 1.4140625, %v28940
%v28947 = vpack.c.bf16 %v120417, %v28944
%119897 = vst [vmem:[%s280 + $0x21c] sm:$0xf] /*vst_source=*/%v28947
%v28951 = vadd.s32 %v26643, %v2842
%v28961 = vadd.s32 %v28951, %v415
%vm28965 = vcmp.lt.u32.totalorder %v28961, %v28951
%vm28970 = vcmp.lt.u32.totalorder %v28951, %v2842
%v28975 = vadd.s32 %v26626, %v2829
%v28979 = vadd.s32 1, %v28975
%v28983 = vsel /*vm=*/%vm28970, /*on_true_vy=*/%v28979, /*on_false_vx=*/%v28975
%v28987 = vadd.s32 1, %v28983
%v28991 = vsel /*vm=*/%vm28965, /*on_true_vy=*/%v28987, /*on_false_vx=*/%v28983
%v28996 = vadd.s32 %v28991, %v10
%v29000 = vadd.s32 %v28961, %v9
%v29004 = vadd.s32 %v29000, %v28996
%v29006 = vshll.u32 %v29000, 13
%v29007 = vshrl.u32 %v29000, 19
%v29008 = vor.u32 %v29007, %v29006
%v29009 = vxor.u32 %v29008, %v29004
%v29012 = vadd.s32 %v29009, %v29004
%v29014 = vshll.u32 %v29009, 15
%v29015 = vshrl.u32 %v29009, 17
%v29016 = vor.u32 %v29015, %v29014
%v29017 = vxor.u32 %v29016, %v29012
%v29020 = vadd.s32 %v29017, %v29012
%v29022 = vshll.u32 %v29017, 26
%v29023 = vshrl.u32 %v29017, 6
%v29024 = vor.u32 %v29023, %v29022
%v29025 = vxor.u32 %v29024, %v29020
%v29028 = vadd.s32 %v29025, %v29020
%v29032 = vadd.s32 %v29028, %v9
%v29034 = vshll.u32 %v29025, 6
%v29035 = vshrl.u32 %v29025, 26
%v29036 = vor.u32 %v29035, %v29034
%v29037 = vxor.u32 %v29036, %v29028
%v29040 = vadd.s32 %v29037, %v8
%v29044 = vadd.s32 1, %v29040
%v29048 = vadd.s32 %v29044, %v29032
%v29050 = vshll.u32 %v29044, 17
%v29051 = vshrl.u32 %v29044, 15
%v29052 = vor.u32 %v29051, %v29050
%v29053 = vxor.u32 %v29052, %v29048
%v29056 = vadd.s32 %v29053, %v29048
%v29058 = vshll.u32 %v29053, 29
%v29059 = vshrl.u32 %v29053, 3
%v29060 = vor.u32 %v29059, %v29058
%v29061 = vxor.u32 %v29060, %v29056
%v29064 = vadd.s32 %v29061, %v29056
%v29066 = vshll.u32 %v29061, 16
%v29067 = vshrl.u32 %v29061, 16
%v29068 = vor.u32 %v29067, %v29066
%v29069 = vxor.u32 %v29068, %v29064
%v29072 = vadd.s32 %v29069, %v29064
%v29076 = vadd.s32 %v29072, %v8
%v29078 = vshll.u32 %v29069, 24
%v29079 = vshrl.u32 %v29069, 8
%v29080 = vor.u32 %v29079, %v29078
%v29081 = vxor.u32 %v29080, %v29072
%v29084 = vadd.s32 %v29081, %v10
%v29088 = vadd.s32 2, %v29084
%v29092 = vadd.s32 %v29088, %v29076
%v29094 = vshll.u32 %v29088, 13
%v29095 = vshrl.u32 %v29088, 19
%v29096 = vor.u32 %v29095, %v29094
%v29097 = vxor.u32 %v29096, %v29092
%v29100 = vadd.s32 %v29097, %v29092
%v29102 = vshll.u32 %v29097, 15
%v29103 = vshrl.u32 %v29097, 17
%v29104 = vor.u32 %v29103, %v29102
%v29105 = vxor.u32 %v29104, %v29100
%v29108 = vadd.s32 %v29105, %v29100
%v29110 = vshll.u32 %v29105, 26
%v29111 = vshrl.u32 %v29105, 6
%v29112 = vor.u32 %v29111, %v29110
%v29113 = vxor.u32 %v29112, %v29108
%v29116 = vadd.s32 %v29113, %v29108
%v29120 = vadd.s32 %v29116, %v10
%v29122 = vshll.u32 %v29113, 6
%v29123 = vshrl.u32 %v29113, 26
%v29124 = vor.u32 %v29123, %v29122
%v29125 = vxor.u32 %v29124, %v29116
%v29128 = vadd.s32 %v29125, %v9
%v29132 = vadd.s32 3, %v29128
%v29136 = vadd.s32 %v29132, %v29120
%v29138 = vshll.u32 %v29132, 17
%v29139 = vshrl.u32 %v29132, 15
%v29140 = vor.u32 %v29139, %v29138
%v29141 = vxor.u32 %v29140, %v29136
%v29144 = vadd.s32 %v29141, %v29136
%v29146 = vshll.u32 %v29141, 29
%v29147 = vshrl.u32 %v29141, 3
%v29148 = vor.u32 %v29147, %v29146
%v29149 = vxor.u32 %v29148, %v29144
%v29152 = vadd.s32 %v29149, %v29144
%v29154 = vshll.u32 %v29149, 16
%v29155 = vshrl.u32 %v29149, 16
%v29156 = vor.u32 %v29155, %v29154
%v29157 = vxor.u32 %v29156, %v29152
%v29160 = vadd.s32 %v29157, %v29152
%v29164 = vadd.s32 %v29160, %v9
%v29166 = vshll.u32 %v29157, 24
%v29167 = vshrl.u32 %v29157, 8
%v29168 = vor.u32 %v29167, %v29166
%v29169 = vxor.u32 %v29168, %v29160
%v29172 = vadd.s32 %v29169, %v8
%v29176 = vadd.s32 4, %v29172
%v29180 = vadd.s32 %v29176, %v29164
%v29182 = vshll.u32 %v29176, 13
%v29183 = vshrl.u32 %v29176, 19
%v29184 = vor.u32 %v29183, %v29182
%v29185 = vxor.u32 %v29184, %v29180
%v29188 = vadd.s32 %v29185, %v29180
%v29190 = vshll.u32 %v29185, 15
%v29191 = vshrl.u32 %v29185, 17
%v29192 = vor.u32 %v29191, %v29190
%v29193 = vxor.u32 %v29192, %v29188
%v29196 = vadd.s32 %v29193, %v29188
%v29198 = vshll.u32 %v29193, 26
%v29199 = vshrl.u32 %v29193, 6
%v29200 = vor.u32 %v29199, %v29198
%v29201 = vxor.u32 %v29200, %v29196
%v29204 = vadd.s32 %v29201, %v29196
%v29208 = vadd.s32 %v29204, %v8
%v29210 = vshll.u32 %v29201, 6
%v29211 = vshrl.u32 %v29201, 26
%v29212 = vor.u32 %v29211, %v29210
%v29213 = vxor.u32 %v29212, %v29204
%v29216 = vadd.s32 %v29213, %v10
%v29220 = vadd.s32 5, %v29216
%v29222 = vxor.u32 %v29220, %v29208
%v29223 = vand.u32.u8 255, %v29222
%v29224 = vand.u32 65535, %v29223
%v29225 = vshrl.u32 %v29224, 1
%v29226 = vor.u32 16256, %v29225
%v29227 = vand.u32.u16 65535, %v29226
%v119898 = vadd.low.f32.bf16 -1.0, %v29227
%v29236 = vmul.f32 2.0, %v119898
%v29240 = vadd.f32 -0.99609375, %v29236
%v29244 = vmax.f32 %v29240, -0.99609375
%v29246 = vand.u32 2147483647, %v29244
%vm29249 = vcmp.eq.f32.partialorder %v29246, 1.0
%v29254 = vmul.f32 inf, %v29244
%v29256 = vxor.u32 2147483648, %v29244
%v29259 = vmul.f32 %v29256, %v29244
%v29261 = vadd.f32 1.0, %v29259
%v29262 = vlog2.pop %v29261
%v29263 = vmul.f32 0.6931472, %v29262
%v29264 = vmul.f32 -0.5, %v29259
%v29265 = vadd.f32 1.0, %v29264
%v29266 = vmul.f32 %v29265, %v29259
%v29267 = vand.u32 2147483647, %v29259
%vm29268 = vcmp.lt.f32.partialorder %v29267, 0.0004427343
%v29269 = vsel /*vm=*/%vm29268, /*on_true_vy=*/%v29266, /*on_false_vx=*/%v29263
%v29270 = vxor.u32 2147483648, %v29269
%vm29273 = vcmp.lt.f32.partialorder %v29270, 5.0
%v29278 = vsel /*vm=*/%vm29273, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v29282 = vsel /*vm=*/%vm29273, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v29286 = vsel /*vm=*/%vm29273, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v29290 = vsel /*vm=*/%vm29273, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v29294 = vsel /*vm=*/%vm29273, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v29298 = vsel /*vm=*/%vm29273, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v29302 = vsel /*vm=*/%vm29273, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v29306 = vsel /*vm=*/%vm29273, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v29310 = vsel /*vm=*/%vm29273, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v29314 = vadd.f32 -2.5, %v29270
%v29316 = vrsqrt.pop %v29270
%v29317 = vmul.f32 %v29316, %v29270
%vm29318 = vcmp.eq.f32.partialorder %v29270, inf
%v29319 = vsel /*vm=*/%vm29318, /*on_true_vy=*/%v29270, /*on_false_vx=*/%v29317
%vm29320 = vcmp.eq.f32.partialorder %v29270, 0.0
%v29321 = vand.u32 2147483648, %v29270
%v29322 = vsel /*vm=*/%vm29320, /*on_true_vy=*/%v29321, /*on_false_vx=*/%v29319
%v29325 = vadd.f32 -3.0, %v29322
%v29329 = vsel /*vm=*/%vm29273, /*on_true_vy=*/%v29314, /*on_false_vx=*/%v29325
%v29333 = vmul.f32 %v29329, %v29310
%v29337 = vadd.f32 %v29333, %v29306
%v29341 = vmul.f32 %v29337, %v29329
%v29345 = vadd.f32 %v29341, %v29302
%v29349 = vmul.f32 %v29345, %v29329
%v29353 = vadd.f32 %v29349, %v29298
%v29357 = vmul.f32 %v29353, %v29329
%v29361 = vadd.f32 %v29357, %v29294
%v29365 = vmul.f32 %v29361, %v29329
%v29369 = vadd.f32 %v29365, %v29290
%v29373 = vmul.f32 %v29369, %v29329
%v29377 = vadd.f32 %v29373, %v29286
%v29381 = vmul.f32 %v29377, %v29329
%v29385 = vadd.f32 %v29381, %v29282
%v29389 = vmul.f32 %v29385, %v29329
%v29393 = vadd.f32 %v29389, %v29278
%v29397 = vmul.f32 %v29393, %v29244
%v29401 = vsel /*vm=*/%vm29249, /*on_true_vy=*/%v29254, /*on_false_vx=*/%v29397
%v29405 = vmul.f32 1.4140625, %v29401
%v29408 = vpack.c.bf16 %v120417, %v29405
%119899 = vst [vmem:[%s280 + $0x29c] sm:$0xf] /*vst_source=*/%v29408
%v29412 = vadd.s32 %v26643, %v3329
%v29422 = vadd.s32 %v29412, %v415
%vm29426 = vcmp.lt.u32.totalorder %v29422, %v29412
%vm29431 = vcmp.lt.u32.totalorder %v29412, %v3329
%v29436 = vadd.s32 %v26626, %v3316
%v29440 = vadd.s32 1, %v29436
%v29444 = vsel /*vm=*/%vm29431, /*on_true_vy=*/%v29440, /*on_false_vx=*/%v29436
%v29448 = vadd.s32 1, %v29444
%v29452 = vsel /*vm=*/%vm29426, /*on_true_vy=*/%v29448, /*on_false_vx=*/%v29444
%v29457 = vadd.s32 %v29452, %v10
%v29461 = vadd.s32 %v29422, %v9
%v29465 = vadd.s32 %v29461, %v29457
%v29467 = vshll.u32 %v29461, 13
%v29468 = vshrl.u32 %v29461, 19
%v29469 = vor.u32 %v29468, %v29467
%v29470 = vxor.u32 %v29469, %v29465
%v29473 = vadd.s32 %v29470, %v29465
%v29475 = vshll.u32 %v29470, 15
%v29476 = vshrl.u32 %v29470, 17
%v29477 = vor.u32 %v29476, %v29475
%v29478 = vxor.u32 %v29477, %v29473
%v29481 = vadd.s32 %v29478, %v29473
%v29483 = vshll.u32 %v29478, 26
%v29484 = vshrl.u32 %v29478, 6
%v29485 = vor.u32 %v29484, %v29483
%v29486 = vxor.u32 %v29485, %v29481
%v29489 = vadd.s32 %v29486, %v29481
%v29493 = vadd.s32 %v29489, %v9
%v29495 = vshll.u32 %v29486, 6
%v29496 = vshrl.u32 %v29486, 26
%v29497 = vor.u32 %v29496, %v29495
%v29498 = vxor.u32 %v29497, %v29489
%v29501 = vadd.s32 %v29498, %v8
%v29505 = vadd.s32 1, %v29501
%v29509 = vadd.s32 %v29505, %v29493
%v29511 = vshll.u32 %v29505, 17
%v29512 = vshrl.u32 %v29505, 15
%v29513 = vor.u32 %v29512, %v29511
%v29514 = vxor.u32 %v29513, %v29509
%v29517 = vadd.s32 %v29514, %v29509
%v29519 = vshll.u32 %v29514, 29
%v29520 = vshrl.u32 %v29514, 3
%v29521 = vor.u32 %v29520, %v29519
%v29522 = vxor.u32 %v29521, %v29517
%v29525 = vadd.s32 %v29522, %v29517
%v29527 = vshll.u32 %v29522, 16
%v29528 = vshrl.u32 %v29522, 16
%v29529 = vor.u32 %v29528, %v29527
%v29530 = vxor.u32 %v29529, %v29525
%v29533 = vadd.s32 %v29530, %v29525
%v29537 = vadd.s32 %v29533, %v8
%v29539 = vshll.u32 %v29530, 24
%v29540 = vshrl.u32 %v29530, 8
%v29541 = vor.u32 %v29540, %v29539
%v29542 = vxor.u32 %v29541, %v29533
%v29545 = vadd.s32 %v29542, %v10
%v29549 = vadd.s32 2, %v29545
%v29553 = vadd.s32 %v29549, %v29537
%v29555 = vshll.u32 %v29549, 13
%v29556 = vshrl.u32 %v29549, 19
%v29557 = vor.u32 %v29556, %v29555
%v29558 = vxor.u32 %v29557, %v29553
%v29561 = vadd.s32 %v29558, %v29553
%v29563 = vshll.u32 %v29558, 15
%v29564 = vshrl.u32 %v29558, 17
%v29565 = vor.u32 %v29564, %v29563
%v29566 = vxor.u32 %v29565, %v29561
%v29569 = vadd.s32 %v29566, %v29561
%v29571 = vshll.u32 %v29566, 26
%v29572 = vshrl.u32 %v29566, 6
%v29573 = vor.u32 %v29572, %v29571
%v29574 = vxor.u32 %v29573, %v29569
%v29577 = vadd.s32 %v29574, %v29569
%v29581 = vadd.s32 %v29577, %v10
%v29583 = vshll.u32 %v29574, 6
%v29584 = vshrl.u32 %v29574, 26
%v29585 = vor.u32 %v29584, %v29583
%v29586 = vxor.u32 %v29585, %v29577
%v29589 = vadd.s32 %v29586, %v9
%v29593 = vadd.s32 3, %v29589
%v29597 = vadd.s32 %v29593, %v29581
%v29599 = vshll.u32 %v29593, 17
%v29600 = vshrl.u32 %v29593, 15
%v29601 = vor.u32 %v29600, %v29599
%v29602 = vxor.u32 %v29601, %v29597
%v29605 = vadd.s32 %v29602, %v29597
%v29607 = vshll.u32 %v29602, 29
%v29608 = vshrl.u32 %v29602, 3
%v29609 = vor.u32 %v29608, %v29607
%v29610 = vxor.u32 %v29609, %v29605
%v29613 = vadd.s32 %v29610, %v29605
%v29615 = vshll.u32 %v29610, 16
%v29616 = vshrl.u32 %v29610, 16
%v29617 = vor.u32 %v29616, %v29615
%v29618 = vxor.u32 %v29617, %v29613
%v29621 = vadd.s32 %v29618, %v29613
%v29625 = vadd.s32 %v29621, %v9
%v29627 = vshll.u32 %v29618, 24
%v29628 = vshrl.u32 %v29618, 8
%v29629 = vor.u32 %v29628, %v29627
%v29630 = vxor.u32 %v29629, %v29621
%v29633 = vadd.s32 %v29630, %v8
%v29637 = vadd.s32 4, %v29633
%v29641 = vadd.s32 %v29637, %v29625
%v29643 = vshll.u32 %v29637, 13
%v29644 = vshrl.u32 %v29637, 19
%v29645 = vor.u32 %v29644, %v29643
%v29646 = vxor.u32 %v29645, %v29641
%v29649 = vadd.s32 %v29646, %v29641
%v29651 = vshll.u32 %v29646, 15
%v29652 = vshrl.u32 %v29646, 17
%v29653 = vor.u32 %v29652, %v29651
%v29654 = vxor.u32 %v29653, %v29649
%v29657 = vadd.s32 %v29654, %v29649
%v29659 = vshll.u32 %v29654, 26
%v29660 = vshrl.u32 %v29654, 6
%v29661 = vor.u32 %v29660, %v29659
%v29662 = vxor.u32 %v29661, %v29657
%v29665 = vadd.s32 %v29662, %v29657
%v29669 = vadd.s32 %v29665, %v8
%v29671 = vshll.u32 %v29662, 6
%v29672 = vshrl.u32 %v29662, 26
%v29673 = vor.u32 %v29672, %v29671
%v29674 = vxor.u32 %v29673, %v29665
%v29677 = vadd.s32 %v29674, %v10
%v29681 = vadd.s32 5, %v29677
%v29683 = vxor.u32 %v29681, %v29669
%v29684 = vand.u32.u8 255, %v29683
%v29685 = vand.u32 65535, %v29684
%v29686 = vshrl.u32 %v29685, 1
%v29687 = vor.u32 16256, %v29686
%v29688 = vand.u32.u16 65535, %v29687
%v119900 = vadd.low.f32.bf16 -1.0, %v29688
%v29697 = vmul.f32 2.0, %v119900
%v29701 = vadd.f32 -0.99609375, %v29697
%v29705 = vmax.f32 %v29701, -0.99609375
%v29707 = vand.u32 2147483647, %v29705
%vm29710 = vcmp.eq.f32.partialorder %v29707, 1.0
%v29715 = vmul.f32 inf, %v29705
%v29717 = vxor.u32 2147483648, %v29705
%v29720 = vmul.f32 %v29717, %v29705
%v29722 = vadd.f32 1.0, %v29720
%v29723 = vlog2.pop %v29722
%v29724 = vmul.f32 0.6931472, %v29723
%v29725 = vmul.f32 -0.5, %v29720
%v29726 = vadd.f32 1.0, %v29725
%v29727 = vmul.f32 %v29726, %v29720
%v29728 = vand.u32 2147483647, %v29720
%vm29729 = vcmp.lt.f32.partialorder %v29728, 0.0004427343
%v29730 = vsel /*vm=*/%vm29729, /*on_true_vy=*/%v29727, /*on_false_vx=*/%v29724
%v29731 = vxor.u32 2147483648, %v29730
%vm29734 = vcmp.lt.f32.partialorder %v29731, 5.0
%v29739 = vsel /*vm=*/%vm29734, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v29743 = vsel /*vm=*/%vm29734, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v29747 = vsel /*vm=*/%vm29734, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v29751 = vsel /*vm=*/%vm29734, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v29755 = vsel /*vm=*/%vm29734, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v29759 = vsel /*vm=*/%vm29734, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v29763 = vsel /*vm=*/%vm29734, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v29767 = vsel /*vm=*/%vm29734, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v29771 = vsel /*vm=*/%vm29734, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v29775 = vadd.f32 -2.5, %v29731
%v29777 = vrsqrt.pop %v29731
%v29778 = vmul.f32 %v29777, %v29731
%vm29779 = vcmp.eq.f32.partialorder %v29731, inf
%v29780 = vsel /*vm=*/%vm29779, /*on_true_vy=*/%v29731, /*on_false_vx=*/%v29778
%vm29781 = vcmp.eq.f32.partialorder %v29731, 0.0
%v29782 = vand.u32 2147483648, %v29731
%v29783 = vsel /*vm=*/%vm29781, /*on_true_vy=*/%v29782, /*on_false_vx=*/%v29780
%v29786 = vadd.f32 -3.0, %v29783
%v29790 = vsel /*vm=*/%vm29734, /*on_true_vy=*/%v29775, /*on_false_vx=*/%v29786
%v29794 = vmul.f32 %v29790, %v29771
%v29798 = vadd.f32 %v29794, %v29767
%v29802 = vmul.f32 %v29798, %v29790
%v29806 = vadd.f32 %v29802, %v29763
%v29810 = vmul.f32 %v29806, %v29790
%v29814 = vadd.f32 %v29810, %v29759
%v29818 = vmul.f32 %v29814, %v29790
%v29822 = vadd.f32 %v29818, %v29755
%v29826 = vmul.f32 %v29822, %v29790
%v29830 = vadd.f32 %v29826, %v29751
%v29834 = vmul.f32 %v29830, %v29790
%v29838 = vadd.f32 %v29834, %v29747
%v29842 = vmul.f32 %v29838, %v29790
%v29846 = vadd.f32 %v29842, %v29743
%v29850 = vmul.f32 %v29846, %v29790
%v29854 = vadd.f32 %v29850, %v29739
%v29858 = vmul.f32 %v29854, %v29705
%v29862 = vsel /*vm=*/%vm29710, /*on_true_vy=*/%v29715, /*on_false_vx=*/%v29858
%v29866 = vmul.f32 1.4140625, %v29862
%v29869 = vpack.c.bf16 %v120417, %v29866
%119901 = vst [vmem:[%s280 + $0x31c] sm:$0xf] /*vst_source=*/%v29869
%v29873 = vadd.s32 %v26643, %v3816
%v29883 = vadd.s32 %v29873, %v415
%vm29887 = vcmp.lt.u32.totalorder %v29883, %v29873
%vm29892 = vcmp.lt.u32.totalorder %v29873, %v3816
%v29897 = vadd.s32 %v26626, %v3803
%v29901 = vadd.s32 1, %v29897
%v29905 = vsel /*vm=*/%vm29892, /*on_true_vy=*/%v29901, /*on_false_vx=*/%v29897
%v29909 = vadd.s32 1, %v29905
%v29913 = vsel /*vm=*/%vm29887, /*on_true_vy=*/%v29909, /*on_false_vx=*/%v29905
%v29918 = vadd.s32 %v29913, %v10
%v29922 = vadd.s32 %v29883, %v9
%v29926 = vadd.s32 %v29922, %v29918
%v29928 = vshll.u32 %v29922, 13
%v29929 = vshrl.u32 %v29922, 19
%v29930 = vor.u32 %v29929, %v29928
%v29931 = vxor.u32 %v29930, %v29926
%v29934 = vadd.s32 %v29931, %v29926
%v29936 = vshll.u32 %v29931, 15
%v29937 = vshrl.u32 %v29931, 17
%v29938 = vor.u32 %v29937, %v29936
%v29939 = vxor.u32 %v29938, %v29934
%v29942 = vadd.s32 %v29939, %v29934
%v29944 = vshll.u32 %v29939, 26
%v29945 = vshrl.u32 %v29939, 6
%v29946 = vor.u32 %v29945, %v29944
%v29947 = vxor.u32 %v29946, %v29942
%v29950 = vadd.s32 %v29947, %v29942
%v29954 = vadd.s32 %v29950, %v9
%v29956 = vshll.u32 %v29947, 6
%v29957 = vshrl.u32 %v29947, 26
%v29958 = vor.u32 %v29957, %v29956
%v29959 = vxor.u32 %v29958, %v29950
%v29962 = vadd.s32 %v29959, %v8
%v29966 = vadd.s32 1, %v29962
%v29970 = vadd.s32 %v29966, %v29954
%v29972 = vshll.u32 %v29966, 17
%v29973 = vshrl.u32 %v29966, 15
%v29974 = vor.u32 %v29973, %v29972
%v29975 = vxor.u32 %v29974, %v29970
%v29978 = vadd.s32 %v29975, %v29970
%v29980 = vshll.u32 %v29975, 29
%v29981 = vshrl.u32 %v29975, 3
%v29982 = vor.u32 %v29981, %v29980
%v29983 = vxor.u32 %v29982, %v29978
%v29986 = vadd.s32 %v29983, %v29978
%v29988 = vshll.u32 %v29983, 16
%v29989 = vshrl.u32 %v29983, 16
%v29990 = vor.u32 %v29989, %v29988
%v29991 = vxor.u32 %v29990, %v29986
%v29994 = vadd.s32 %v29991, %v29986
%v29998 = vadd.s32 %v29994, %v8
%v30000 = vshll.u32 %v29991, 24
%v30001 = vshrl.u32 %v29991, 8
%v30002 = vor.u32 %v30001, %v30000
%v30003 = vxor.u32 %v30002, %v29994
%v30006 = vadd.s32 %v30003, %v10
%v30010 = vadd.s32 2, %v30006
%v30014 = vadd.s32 %v30010, %v29998
%v30016 = vshll.u32 %v30010, 13
%v30017 = vshrl.u32 %v30010, 19
%v30018 = vor.u32 %v30017, %v30016
%v30019 = vxor.u32 %v30018, %v30014
%v30022 = vadd.s32 %v30019, %v30014
%v30024 = vshll.u32 %v30019, 15
%v30025 = vshrl.u32 %v30019, 17
%v30026 = vor.u32 %v30025, %v30024
%v30027 = vxor.u32 %v30026, %v30022
%v30030 = vadd.s32 %v30027, %v30022
%v30032 = vshll.u32 %v30027, 26
%v30033 = vshrl.u32 %v30027, 6
%v30034 = vor.u32 %v30033, %v30032
%v30035 = vxor.u32 %v30034, %v30030
%v30038 = vadd.s32 %v30035, %v30030
%v30042 = vadd.s32 %v30038, %v10
%v30044 = vshll.u32 %v30035, 6
%v30045 = vshrl.u32 %v30035, 26
%v30046 = vor.u32 %v30045, %v30044
%v30047 = vxor.u32 %v30046, %v30038
%v30050 = vadd.s32 %v30047, %v9
%v30054 = vadd.s32 3, %v30050
%v30058 = vadd.s32 %v30054, %v30042
%v30060 = vshll.u32 %v30054, 17
%v30061 = vshrl.u32 %v30054, 15
%v30062 = vor.u32 %v30061, %v30060
%v30063 = vxor.u32 %v30062, %v30058
%v30066 = vadd.s32 %v30063, %v30058
%v30068 = vshll.u32 %v30063, 29
%v30069 = vshrl.u32 %v30063, 3
%v30070 = vor.u32 %v30069, %v30068
%v30071 = vxor.u32 %v30070, %v30066
%v30074 = vadd.s32 %v30071, %v30066
%v30076 = vshll.u32 %v30071, 16
%v30077 = vshrl.u32 %v30071, 16
%v30078 = vor.u32 %v30077, %v30076
%v30079 = vxor.u32 %v30078, %v30074
%v30082 = vadd.s32 %v30079, %v30074
%v30086 = vadd.s32 %v30082, %v9
%v30088 = vshll.u32 %v30079, 24
%v30089 = vshrl.u32 %v30079, 8
%v30090 = vor.u32 %v30089, %v30088
%v30091 = vxor.u32 %v30090, %v30082
%v30094 = vadd.s32 %v30091, %v8
%v30098 = vadd.s32 4, %v30094
%v30102 = vadd.s32 %v30098, %v30086
%v30104 = vshll.u32 %v30098, 13
%v30105 = vshrl.u32 %v30098, 19
%v30106 = vor.u32 %v30105, %v30104
%v30107 = vxor.u32 %v30106, %v30102
%v30110 = vadd.s32 %v30107, %v30102
%v30112 = vshll.u32 %v30107, 15
%v30113 = vshrl.u32 %v30107, 17
%v30114 = vor.u32 %v30113, %v30112
%v30115 = vxor.u32 %v30114, %v30110
%v30118 = vadd.s32 %v30115, %v30110
%v30120 = vshll.u32 %v30115, 26
%v30121 = vshrl.u32 %v30115, 6
%v30122 = vor.u32 %v30121, %v30120
%v30123 = vxor.u32 %v30122, %v30118
%v30126 = vadd.s32 %v30123, %v30118
%v30130 = vadd.s32 %v30126, %v8
%v30132 = vshll.u32 %v30123, 6
%v30133 = vshrl.u32 %v30123, 26
%v30134 = vor.u32 %v30133, %v30132
%v30135 = vxor.u32 %v30134, %v30126
%v30138 = vadd.s32 %v30135, %v10
%v30142 = vadd.s32 5, %v30138
%v30144 = vxor.u32 %v30142, %v30130
%v30145 = vand.u32.u8 255, %v30144
%v30146 = vand.u32 65535, %v30145
%v30147 = vshrl.u32 %v30146, 1
%v30148 = vor.u32 16256, %v30147
%v30149 = vand.u32.u16 65535, %v30148
%v119902 = vadd.low.f32.bf16 -1.0, %v30149
%v30158 = vmul.f32 2.0, %v119902
%v30162 = vadd.f32 -0.99609375, %v30158
%v30166 = vmax.f32 %v30162, -0.99609375
%v30168 = vand.u32 2147483647, %v30166
%vm30171 = vcmp.eq.f32.partialorder %v30168, 1.0
%v30176 = vmul.f32 inf, %v30166
%v30178 = vxor.u32 2147483648, %v30166
%v30181 = vmul.f32 %v30178, %v30166
%v30183 = vadd.f32 1.0, %v30181
%v30184 = vlog2.pop %v30183
%v30185 = vmul.f32 0.6931472, %v30184
%v30186 = vmul.f32 -0.5, %v30181
%v30187 = vadd.f32 1.0, %v30186
%v30188 = vmul.f32 %v30187, %v30181
%v30189 = vand.u32 2147483647, %v30181
%vm30190 = vcmp.lt.f32.partialorder %v30189, 0.0004427343
%v30191 = vsel /*vm=*/%vm30190, /*on_true_vy=*/%v30188, /*on_false_vx=*/%v30185
%v30192 = vxor.u32 2147483648, %v30191
%vm30195 = vcmp.lt.f32.partialorder %v30192, 5.0
%v30200 = vsel /*vm=*/%vm30195, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v30204 = vsel /*vm=*/%vm30195, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v30208 = vsel /*vm=*/%vm30195, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v30212 = vsel /*vm=*/%vm30195, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v30216 = vsel /*vm=*/%vm30195, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v30220 = vsel /*vm=*/%vm30195, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v30224 = vsel /*vm=*/%vm30195, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v30228 = vsel /*vm=*/%vm30195, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v30232 = vsel /*vm=*/%vm30195, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v30236 = vadd.f32 -2.5, %v30192
%v30238 = vrsqrt.pop %v30192
%v30239 = vmul.f32 %v30238, %v30192
%vm30240 = vcmp.eq.f32.partialorder %v30192, inf
%v30241 = vsel /*vm=*/%vm30240, /*on_true_vy=*/%v30192, /*on_false_vx=*/%v30239
%vm30242 = vcmp.eq.f32.partialorder %v30192, 0.0
%v30243 = vand.u32 2147483648, %v30192
%v30244 = vsel /*vm=*/%vm30242, /*on_true_vy=*/%v30243, /*on_false_vx=*/%v30241
%v30247 = vadd.f32 -3.0, %v30244
%v30251 = vsel /*vm=*/%vm30195, /*on_true_vy=*/%v30236, /*on_false_vx=*/%v30247
%v30255 = vmul.f32 %v30251, %v30232
%v30259 = vadd.f32 %v30255, %v30228
%v30263 = vmul.f32 %v30259, %v30251
%v30267 = vadd.f32 %v30263, %v30224
%v30271 = vmul.f32 %v30267, %v30251
%v30275 = vadd.f32 %v30271, %v30220
%v30279 = vmul.f32 %v30275, %v30251
%v30283 = vadd.f32 %v30279, %v30216
%v30287 = vmul.f32 %v30283, %v30251
%v30291 = vadd.f32 %v30287, %v30212
%v30295 = vmul.f32 %v30291, %v30251
%v30299 = vadd.f32 %v30295, %v30208
%v30303 = vmul.f32 %v30299, %v30251
%v30307 = vadd.f32 %v30303, %v30204
%v30311 = vmul.f32 %v30307, %v30251
%v30315 = vadd.f32 %v30311, %v30200
%v30319 = vmul.f32 %v30315, %v30166
%v30323 = vsel /*vm=*/%vm30171, /*on_true_vy=*/%v30176, /*on_false_vx=*/%v30319
%v30327 = vmul.f32 1.4140625, %v30323
%v30330 = vpack.c.bf16 %v120417, %v30327
%119903 = vst [vmem:[%s280 + $0x39c] sm:$0xf] /*vst_source=*/%v30330
%v30368 = vadd.s32 %v30365, %v408
%v30378 = vadd.s32 %v30368, %v415
%vm30382 = vcmp.lt.u32.totalorder %v30378, %v30368
%vm30387 = vcmp.lt.u32.totalorder %v30368, %v408
%v30392 = vadd.s32 %v30348, %v380
%v30396 = vadd.s32 1, %v30392
%v30400 = vsel /*vm=*/%vm30387, /*on_true_vy=*/%v30396, /*on_false_vx=*/%v30392
%v30404 = vadd.s32 1, %v30400
%v30408 = vsel /*vm=*/%vm30382, /*on_true_vy=*/%v30404, /*on_false_vx=*/%v30400
%v30413 = vadd.s32 %v30408, %v10
%v30417 = vadd.s32 %v30378, %v9
%v30421 = vadd.s32 %v30417, %v30413
%v30423 = vshll.u32 %v30417, 13
%v30424 = vshrl.u32 %v30417, 19
%v30425 = vor.u32 %v30424, %v30423
%v30426 = vxor.u32 %v30425, %v30421
%v30429 = vadd.s32 %v30426, %v30421
%v30431 = vshll.u32 %v30426, 15
%v30432 = vshrl.u32 %v30426, 17
%v30433 = vor.u32 %v30432, %v30431
%v30434 = vxor.u32 %v30433, %v30429
%v30437 = vadd.s32 %v30434, %v30429
%v30439 = vshll.u32 %v30434, 26
%v30440 = vshrl.u32 %v30434, 6
%v30441 = vor.u32 %v30440, %v30439
%v30442 = vxor.u32 %v30441, %v30437
%v30445 = vadd.s32 %v30442, %v30437
%v30449 = vadd.s32 %v30445, %v9
%v30451 = vshll.u32 %v30442, 6
%v30452 = vshrl.u32 %v30442, 26
%v30453 = vor.u32 %v30452, %v30451
%v30454 = vxor.u32 %v30453, %v30445
%v30457 = vadd.s32 %v30454, %v8
%v30461 = vadd.s32 1, %v30457
%v30465 = vadd.s32 %v30461, %v30449
%v30467 = vshll.u32 %v30461, 17
%v30468 = vshrl.u32 %v30461, 15
%v30469 = vor.u32 %v30468, %v30467
%v30470 = vxor.u32 %v30469, %v30465
%v30473 = vadd.s32 %v30470, %v30465
%v30475 = vshll.u32 %v30470, 29
%v30476 = vshrl.u32 %v30470, 3
%v30477 = vor.u32 %v30476, %v30475
%v30478 = vxor.u32 %v30477, %v30473
%v30481 = vadd.s32 %v30478, %v30473
%v30483 = vshll.u32 %v30478, 16
%v30484 = vshrl.u32 %v30478, 16
%v30485 = vor.u32 %v30484, %v30483
%v30486 = vxor.u32 %v30485, %v30481
%v30489 = vadd.s32 %v30486, %v30481
%v30493 = vadd.s32 %v30489, %v8
%v30495 = vshll.u32 %v30486, 24
%v30496 = vshrl.u32 %v30486, 8
%v30497 = vor.u32 %v30496, %v30495
%v30498 = vxor.u32 %v30497, %v30489
%v30501 = vadd.s32 %v30498, %v10
%v30505 = vadd.s32 2, %v30501
%v30509 = vadd.s32 %v30505, %v30493
%v30511 = vshll.u32 %v30505, 13
%v30512 = vshrl.u32 %v30505, 19
%v30513 = vor.u32 %v30512, %v30511
%v30514 = vxor.u32 %v30513, %v30509
%v30517 = vadd.s32 %v30514, %v30509
%v30519 = vshll.u32 %v30514, 15
%v30520 = vshrl.u32 %v30514, 17
%v30521 = vor.u32 %v30520, %v30519
%v30522 = vxor.u32 %v30521, %v30517
%v30525 = vadd.s32 %v30522, %v30517
%v30527 = vshll.u32 %v30522, 26
%v30528 = vshrl.u32 %v30522, 6
%v30529 = vor.u32 %v30528, %v30527
%v30530 = vxor.u32 %v30529, %v30525
%v30533 = vadd.s32 %v30530, %v30525
%v30537 = vadd.s32 %v30533, %v10
%v30539 = vshll.u32 %v30530, 6
%v30540 = vshrl.u32 %v30530, 26
%v30541 = vor.u32 %v30540, %v30539
%v30542 = vxor.u32 %v30541, %v30533
%v30545 = vadd.s32 %v30542, %v9
%v30549 = vadd.s32 3, %v30545
%v30553 = vadd.s32 %v30549, %v30537
%v30555 = vshll.u32 %v30549, 17
%v30556 = vshrl.u32 %v30549, 15
%v30557 = vor.u32 %v30556, %v30555
%v30558 = vxor.u32 %v30557, %v30553
%v30561 = vadd.s32 %v30558, %v30553
%v30563 = vshll.u32 %v30558, 29
%v30564 = vshrl.u32 %v30558, 3
%v30565 = vor.u32 %v30564, %v30563
%v30566 = vxor.u32 %v30565, %v30561
%v30569 = vadd.s32 %v30566, %v30561
%v30571 = vshll.u32 %v30566, 16
%v30572 = vshrl.u32 %v30566, 16
%v30573 = vor.u32 %v30572, %v30571
%v30574 = vxor.u32 %v30573, %v30569
%v30577 = vadd.s32 %v30574, %v30569
%v30581 = vadd.s32 %v30577, %v9
%v30583 = vshll.u32 %v30574, 24
%v30584 = vshrl.u32 %v30574, 8
%v30585 = vor.u32 %v30584, %v30583
%v30586 = vxor.u32 %v30585, %v30577
%v30589 = vadd.s32 %v30586, %v8
%v30593 = vadd.s32 4, %v30589
%v30597 = vadd.s32 %v30593, %v30581
%v30599 = vshll.u32 %v30593, 13
%v30600 = vshrl.u32 %v30593, 19
%v30601 = vor.u32 %v30600, %v30599
%v30602 = vxor.u32 %v30601, %v30597
%v30605 = vadd.s32 %v30602, %v30597
%v30607 = vshll.u32 %v30602, 15
%v30608 = vshrl.u32 %v30602, 17
%v30609 = vor.u32 %v30608, %v30607
%v30610 = vxor.u32 %v30609, %v30605
%v30613 = vadd.s32 %v30610, %v30605
%v30615 = vshll.u32 %v30610, 26
%v30616 = vshrl.u32 %v30610, 6
%v30617 = vor.u32 %v30616, %v30615
%v30618 = vxor.u32 %v30617, %v30613
%v30621 = vadd.s32 %v30618, %v30613
%v30625 = vadd.s32 %v30621, %v8
%v30627 = vshll.u32 %v30618, 6
%v30628 = vshrl.u32 %v30618, 26
%v30629 = vor.u32 %v30628, %v30627
%v30630 = vxor.u32 %v30629, %v30621
%v30633 = vadd.s32 %v30630, %v10
%v30637 = vadd.s32 5, %v30633
%v30639 = vxor.u32 %v30637, %v30625
%v30640 = vand.u32.u8 255, %v30639
%v30641 = vand.u32 65535, %v30640
%v30642 = vshrl.u32 %v30641, 1
%v30643 = vor.u32 16256, %v30642
%v30644 = vand.u32.u16 65535, %v30643
%v119908 = vadd.low.f32.bf16 -1.0, %v30644
%v30653 = vmul.f32 2.0, %v119908
%v30657 = vadd.f32 -0.99609375, %v30653
%v30661 = vmax.f32 %v30657, -0.99609375
%v30663 = vand.u32 2147483647, %v30661
%vm30666 = vcmp.eq.f32.partialorder %v30663, 1.0
%v30671 = vmul.f32 inf, %v30661
%v30673 = vxor.u32 2147483648, %v30661
%v30676 = vmul.f32 %v30673, %v30661
%v30678 = vadd.f32 1.0, %v30676
%v30679 = vlog2.pop %v30678
%v30680 = vmul.f32 0.6931472, %v30679
%v30681 = vmul.f32 -0.5, %v30676
%v30682 = vadd.f32 1.0, %v30681
%v30683 = vmul.f32 %v30682, %v30676
%v30684 = vand.u32 2147483647, %v30676
%vm30685 = vcmp.lt.f32.partialorder %v30684, 0.0004427343
%v30686 = vsel /*vm=*/%vm30685, /*on_true_vy=*/%v30683, /*on_false_vx=*/%v30680
%v30687 = vxor.u32 2147483648, %v30686
%vm30690 = vcmp.lt.f32.partialorder %v30687, 5.0
%v30695 = vsel /*vm=*/%vm30690, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v30699 = vsel /*vm=*/%vm30690, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v30703 = vsel /*vm=*/%vm30690, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v30707 = vsel /*vm=*/%vm30690, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v30711 = vsel /*vm=*/%vm30690, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v30715 = vsel /*vm=*/%vm30690, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v30719 = vsel /*vm=*/%vm30690, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v30723 = vsel /*vm=*/%vm30690, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v30727 = vsel /*vm=*/%vm30690, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v30731 = vadd.f32 -2.5, %v30687
%v30733 = vrsqrt.pop %v30687
%v30734 = vmul.f32 %v30733, %v30687
%vm30735 = vcmp.eq.f32.partialorder %v30687, inf
%v30736 = vsel /*vm=*/%vm30735, /*on_true_vy=*/%v30687, /*on_false_vx=*/%v30734
%vm30737 = vcmp.eq.f32.partialorder %v30687, 0.0
%v30738 = vand.u32 2147483648, %v30687
%v30739 = vsel /*vm=*/%vm30737, /*on_true_vy=*/%v30738, /*on_false_vx=*/%v30736
%v30742 = vadd.f32 -3.0, %v30739
%v30746 = vsel /*vm=*/%vm30690, /*on_true_vy=*/%v30731, /*on_false_vx=*/%v30742
%v30750 = vmul.f32 %v30746, %v30727
%v30754 = vadd.f32 %v30750, %v30723
%v30758 = vmul.f32 %v30754, %v30746
%v30762 = vadd.f32 %v30758, %v30719
%v30766 = vmul.f32 %v30762, %v30746
%v30770 = vadd.f32 %v30766, %v30715
%v30774 = vmul.f32 %v30770, %v30746
%v30778 = vadd.f32 %v30774, %v30711
%v30782 = vmul.f32 %v30778, %v30746
%v30786 = vadd.f32 %v30782, %v30707
%v30790 = vmul.f32 %v30786, %v30746
%v30794 = vadd.f32 %v30790, %v30703
%v30798 = vmul.f32 %v30794, %v30746
%v30802 = vadd.f32 %v30798, %v30699
%v30806 = vmul.f32 %v30802, %v30746
%v30810 = vadd.f32 %v30806, %v30695
%v30814 = vmul.f32 %v30810, %v30661
%v30818 = vsel /*vm=*/%vm30666, /*on_true_vy=*/%v30671, /*on_false_vx=*/%v30814
%v30822 = vmul.f32 1.4140625, %v30818
%v30825 = vpack.c.bf16 %v120417, %v30822
%119909 = vst [vmem:[%s280 + $0x20] sm:$0xf] /*vst_source=*/%v30825
%v30829 = vadd.s32 %v30365, %v894
%v30839 = vadd.s32 %v30829, %v415
%vm30843 = vcmp.lt.u32.totalorder %v30839, %v30829
%vm30848 = vcmp.lt.u32.totalorder %v30829, %v894
%v30853 = vadd.s32 %v30348, %v881
%v30857 = vadd.s32 1, %v30853
%v30861 = vsel /*vm=*/%vm30848, /*on_true_vy=*/%v30857, /*on_false_vx=*/%v30853
%v30865 = vadd.s32 1, %v30861
%v30869 = vsel /*vm=*/%vm30843, /*on_true_vy=*/%v30865, /*on_false_vx=*/%v30861
%v30874 = vadd.s32 %v30869, %v10
%v30878 = vadd.s32 %v30839, %v9
%v30882 = vadd.s32 %v30878, %v30874
%v30884 = vshll.u32 %v30878, 13
%v30885 = vshrl.u32 %v30878, 19
%v30886 = vor.u32 %v30885, %v30884
%v30887 = vxor.u32 %v30886, %v30882
%v30890 = vadd.s32 %v30887, %v30882
%v30892 = vshll.u32 %v30887, 15
%v30893 = vshrl.u32 %v30887, 17
%v30894 = vor.u32 %v30893, %v30892
%v30895 = vxor.u32 %v30894, %v30890
%v30898 = vadd.s32 %v30895, %v30890
%v30900 = vshll.u32 %v30895, 26
%v30901 = vshrl.u32 %v30895, 6
%v30902 = vor.u32 %v30901, %v30900
%v30903 = vxor.u32 %v30902, %v30898
%v30906 = vadd.s32 %v30903, %v30898
%v30910 = vadd.s32 %v30906, %v9
%v30912 = vshll.u32 %v30903, 6
%v30913 = vshrl.u32 %v30903, 26
%v30914 = vor.u32 %v30913, %v30912
%v30915 = vxor.u32 %v30914, %v30906
%v30918 = vadd.s32 %v30915, %v8
%v30922 = vadd.s32 1, %v30918
%v30926 = vadd.s32 %v30922, %v30910
%v30928 = vshll.u32 %v30922, 17
%v30929 = vshrl.u32 %v30922, 15
%v30930 = vor.u32 %v30929, %v30928
%v30931 = vxor.u32 %v30930, %v30926
%v30934 = vadd.s32 %v30931, %v30926
%v30936 = vshll.u32 %v30931, 29
%v30937 = vshrl.u32 %v30931, 3
%v30938 = vor.u32 %v30937, %v30936
%v30939 = vxor.u32 %v30938, %v30934
%v30942 = vadd.s32 %v30939, %v30934
%v30944 = vshll.u32 %v30939, 16
%v30945 = vshrl.u32 %v30939, 16
%v30946 = vor.u32 %v30945, %v30944
%v30947 = vxor.u32 %v30946, %v30942
%v30950 = vadd.s32 %v30947, %v30942
%v30954 = vadd.s32 %v30950, %v8
%v30956 = vshll.u32 %v30947, 24
%v30957 = vshrl.u32 %v30947, 8
%v30958 = vor.u32 %v30957, %v30956
%v30959 = vxor.u32 %v30958, %v30950
%v30962 = vadd.s32 %v30959, %v10
%v30966 = vadd.s32 2, %v30962
%v30970 = vadd.s32 %v30966, %v30954
%v30972 = vshll.u32 %v30966, 13
%v30973 = vshrl.u32 %v30966, 19
%v30974 = vor.u32 %v30973, %v30972
%v30975 = vxor.u32 %v30974, %v30970
%v30978 = vadd.s32 %v30975, %v30970
%v30980 = vshll.u32 %v30975, 15
%v30981 = vshrl.u32 %v30975, 17
%v30982 = vor.u32 %v30981, %v30980
%v30983 = vxor.u32 %v30982, %v30978
%v30986 = vadd.s32 %v30983, %v30978
%v30988 = vshll.u32 %v30983, 26
%v30989 = vshrl.u32 %v30983, 6
%v30990 = vor.u32 %v30989, %v30988
%v30991 = vxor.u32 %v30990, %v30986
%v30994 = vadd.s32 %v30991, %v30986
%v30998 = vadd.s32 %v30994, %v10
%v31000 = vshll.u32 %v30991, 6
%v31001 = vshrl.u32 %v30991, 26
%v31002 = vor.u32 %v31001, %v31000
%v31003 = vxor.u32 %v31002, %v30994
%v31006 = vadd.s32 %v31003, %v9
%v31010 = vadd.s32 3, %v31006
%v31014 = vadd.s32 %v31010, %v30998
%v31016 = vshll.u32 %v31010, 17
%v31017 = vshrl.u32 %v31010, 15
%v31018 = vor.u32 %v31017, %v31016
%v31019 = vxor.u32 %v31018, %v31014
%v31022 = vadd.s32 %v31019, %v31014
%v31024 = vshll.u32 %v31019, 29
%v31025 = vshrl.u32 %v31019, 3
%v31026 = vor.u32 %v31025, %v31024
%v31027 = vxor.u32 %v31026, %v31022
%v31030 = vadd.s32 %v31027, %v31022
%v31032 = vshll.u32 %v31027, 16
%v31033 = vshrl.u32 %v31027, 16
%v31034 = vor.u32 %v31033, %v31032
%v31035 = vxor.u32 %v31034, %v31030
%v31038 = vadd.s32 %v31035, %v31030
%v31042 = vadd.s32 %v31038, %v9
%v31044 = vshll.u32 %v31035, 24
%v31045 = vshrl.u32 %v31035, 8
%v31046 = vor.u32 %v31045, %v31044
%v31047 = vxor.u32 %v31046, %v31038
%v31050 = vadd.s32 %v31047, %v8
%v31054 = vadd.s32 4, %v31050
%v31058 = vadd.s32 %v31054, %v31042
%v31060 = vshll.u32 %v31054, 13
%v31061 = vshrl.u32 %v31054, 19
%v31062 = vor.u32 %v31061, %v31060
%v31063 = vxor.u32 %v31062, %v31058
%v31066 = vadd.s32 %v31063, %v31058
%v31068 = vshll.u32 %v31063, 15
%v31069 = vshrl.u32 %v31063, 17
%v31070 = vor.u32 %v31069, %v31068
%v31071 = vxor.u32 %v31070, %v31066
%v31074 = vadd.s32 %v31071, %v31066
%v31076 = vshll.u32 %v31071, 26
%v31077 = vshrl.u32 %v31071, 6
%v31078 = vor.u32 %v31077, %v31076
%v31079 = vxor.u32 %v31078, %v31074
%v31082 = vadd.s32 %v31079, %v31074
%v31086 = vadd.s32 %v31082, %v8
%v31088 = vshll.u32 %v31079, 6
%v31089 = vshrl.u32 %v31079, 26
%v31090 = vor.u32 %v31089, %v31088
%v31091 = vxor.u32 %v31090, %v31082
%v31094 = vadd.s32 %v31091, %v10
%v31098 = vadd.s32 5, %v31094
%v31100 = vxor.u32 %v31098, %v31086
%v31101 = vand.u32.u8 255, %v31100
%v31102 = vand.u32 65535, %v31101
%v31103 = vshrl.u32 %v31102, 1
%v31104 = vor.u32 16256, %v31103
%v31105 = vand.u32.u16 65535, %v31104
%v119910 = vadd.low.f32.bf16 -1.0, %v31105
%v31114 = vmul.f32 2.0, %v119910
%v31118 = vadd.f32 -0.99609375, %v31114
%v31122 = vmax.f32 %v31118, -0.99609375
%v31124 = vand.u32 2147483647, %v31122
%vm31127 = vcmp.eq.f32.partialorder %v31124, 1.0
%v31132 = vmul.f32 inf, %v31122
%v31134 = vxor.u32 2147483648, %v31122
%v31137 = vmul.f32 %v31134, %v31122
%v31139 = vadd.f32 1.0, %v31137
%v31140 = vlog2.pop %v31139
%v31141 = vmul.f32 0.6931472, %v31140
%v31142 = vmul.f32 -0.5, %v31137
%v31143 = vadd.f32 1.0, %v31142
%v31144 = vmul.f32 %v31143, %v31137
%v31145 = vand.u32 2147483647, %v31137
%vm31146 = vcmp.lt.f32.partialorder %v31145, 0.0004427343
%v31147 = vsel /*vm=*/%vm31146, /*on_true_vy=*/%v31144, /*on_false_vx=*/%v31141
%v31148 = vxor.u32 2147483648, %v31147
%vm31151 = vcmp.lt.f32.partialorder %v31148, 5.0
%v31156 = vsel /*vm=*/%vm31151, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v31160 = vsel /*vm=*/%vm31151, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v31164 = vsel /*vm=*/%vm31151, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v31168 = vsel /*vm=*/%vm31151, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v31172 = vsel /*vm=*/%vm31151, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v31176 = vsel /*vm=*/%vm31151, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v31180 = vsel /*vm=*/%vm31151, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v31184 = vsel /*vm=*/%vm31151, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v31188 = vsel /*vm=*/%vm31151, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v31192 = vadd.f32 -2.5, %v31148
%v31194 = vrsqrt.pop %v31148
%v31195 = vmul.f32 %v31194, %v31148
%vm31196 = vcmp.eq.f32.partialorder %v31148, inf
%v31197 = vsel /*vm=*/%vm31196, /*on_true_vy=*/%v31148, /*on_false_vx=*/%v31195
%vm31198 = vcmp.eq.f32.partialorder %v31148, 0.0
%v31199 = vand.u32 2147483648, %v31148
%v31200 = vsel /*vm=*/%vm31198, /*on_true_vy=*/%v31199, /*on_false_vx=*/%v31197
%v31203 = vadd.f32 -3.0, %v31200
%v31207 = vsel /*vm=*/%vm31151, /*on_true_vy=*/%v31192, /*on_false_vx=*/%v31203
%v31211 = vmul.f32 %v31207, %v31188
%v31215 = vadd.f32 %v31211, %v31184
%v31219 = vmul.f32 %v31215, %v31207
%v31223 = vadd.f32 %v31219, %v31180
%v31227 = vmul.f32 %v31223, %v31207
%v31231 = vadd.f32 %v31227, %v31176
%v31235 = vmul.f32 %v31231, %v31207
%v31239 = vadd.f32 %v31235, %v31172
%v31243 = vmul.f32 %v31239, %v31207
%v31247 = vadd.f32 %v31243, %v31168
%v31251 = vmul.f32 %v31247, %v31207
%v31255 = vadd.f32 %v31251, %v31164
%v31259 = vmul.f32 %v31255, %v31207
%v31263 = vadd.f32 %v31259, %v31160
%v31267 = vmul.f32 %v31263, %v31207
%v31271 = vadd.f32 %v31267, %v31156
%v31275 = vmul.f32 %v31271, %v31122
%v31279 = vsel /*vm=*/%vm31127, /*on_true_vy=*/%v31132, /*on_false_vx=*/%v31275
%v31283 = vmul.f32 1.4140625, %v31279
%v31286 = vpack.c.bf16 %v120417, %v31283
%119911 = vst [vmem:[%s280 + $0xa0] sm:$0xf] /*vst_source=*/%v31286
%v31290 = vadd.s32 %v30365, %v1381
%v31300 = vadd.s32 %v31290, %v415
%vm31304 = vcmp.lt.u32.totalorder %v31300, %v31290
%vm31309 = vcmp.lt.u32.totalorder %v31290, %v1381
%v31314 = vadd.s32 %v30348, %v1368
%v31318 = vadd.s32 1, %v31314
%v31322 = vsel /*vm=*/%vm31309, /*on_true_vy=*/%v31318, /*on_false_vx=*/%v31314
%v31326 = vadd.s32 1, %v31322
%v31330 = vsel /*vm=*/%vm31304, /*on_true_vy=*/%v31326, /*on_false_vx=*/%v31322
%v31335 = vadd.s32 %v31330, %v10
%v31339 = vadd.s32 %v31300, %v9
%v31343 = vadd.s32 %v31339, %v31335
%v31345 = vshll.u32 %v31339, 13
%v31346 = vshrl.u32 %v31339, 19
%v31347 = vor.u32 %v31346, %v31345
%v31348 = vxor.u32 %v31347, %v31343
%v31351 = vadd.s32 %v31348, %v31343
%v31353 = vshll.u32 %v31348, 15
%v31354 = vshrl.u32 %v31348, 17
%v31355 = vor.u32 %v31354, %v31353
%v31356 = vxor.u32 %v31355, %v31351
%v31359 = vadd.s32 %v31356, %v31351
%v31361 = vshll.u32 %v31356, 26
%v31362 = vshrl.u32 %v31356, 6
%v31363 = vor.u32 %v31362, %v31361
%v31364 = vxor.u32 %v31363, %v31359
%v31367 = vadd.s32 %v31364, %v31359
%v31371 = vadd.s32 %v31367, %v9
%v31373 = vshll.u32 %v31364, 6
%v31374 = vshrl.u32 %v31364, 26
%v31375 = vor.u32 %v31374, %v31373
%v31376 = vxor.u32 %v31375, %v31367
%v31379 = vadd.s32 %v31376, %v8
%v31383 = vadd.s32 1, %v31379
%v31387 = vadd.s32 %v31383, %v31371
%v31389 = vshll.u32 %v31383, 17
%v31390 = vshrl.u32 %v31383, 15
%v31391 = vor.u32 %v31390, %v31389
%v31392 = vxor.u32 %v31391, %v31387
%v31395 = vadd.s32 %v31392, %v31387
%v31397 = vshll.u32 %v31392, 29
%v31398 = vshrl.u32 %v31392, 3
%v31399 = vor.u32 %v31398, %v31397
%v31400 = vxor.u32 %v31399, %v31395
%v31403 = vadd.s32 %v31400, %v31395
%v31405 = vshll.u32 %v31400, 16
%v31406 = vshrl.u32 %v31400, 16
%v31407 = vor.u32 %v31406, %v31405
%v31408 = vxor.u32 %v31407, %v31403
%v31411 = vadd.s32 %v31408, %v31403
%v31415 = vadd.s32 %v31411, %v8
%v31417 = vshll.u32 %v31408, 24
%v31418 = vshrl.u32 %v31408, 8
%v31419 = vor.u32 %v31418, %v31417
%v31420 = vxor.u32 %v31419, %v31411
%v31423 = vadd.s32 %v31420, %v10
%v31427 = vadd.s32 2, %v31423
%v31431 = vadd.s32 %v31427, %v31415
%v31433 = vshll.u32 %v31427, 13
%v31434 = vshrl.u32 %v31427, 19
%v31435 = vor.u32 %v31434, %v31433
%v31436 = vxor.u32 %v31435, %v31431
%v31439 = vadd.s32 %v31436, %v31431
%v31441 = vshll.u32 %v31436, 15
%v31442 = vshrl.u32 %v31436, 17
%v31443 = vor.u32 %v31442, %v31441
%v31444 = vxor.u32 %v31443, %v31439
%v31447 = vadd.s32 %v31444, %v31439
%v31449 = vshll.u32 %v31444, 26
%v31450 = vshrl.u32 %v31444, 6
%v31451 = vor.u32 %v31450, %v31449
%v31452 = vxor.u32 %v31451, %v31447
%v31455 = vadd.s32 %v31452, %v31447
%v31459 = vadd.s32 %v31455, %v10
%v31461 = vshll.u32 %v31452, 6
%v31462 = vshrl.u32 %v31452, 26
%v31463 = vor.u32 %v31462, %v31461
%v31464 = vxor.u32 %v31463, %v31455
%v31467 = vadd.s32 %v31464, %v9
%v31471 = vadd.s32 3, %v31467
%v31475 = vadd.s32 %v31471, %v31459
%v31477 = vshll.u32 %v31471, 17
%v31478 = vshrl.u32 %v31471, 15
%v31479 = vor.u32 %v31478, %v31477
%v31480 = vxor.u32 %v31479, %v31475
%v31483 = vadd.s32 %v31480, %v31475
%v31485 = vshll.u32 %v31480, 29
%v31486 = vshrl.u32 %v31480, 3
%v31487 = vor.u32 %v31486, %v31485
%v31488 = vxor.u32 %v31487, %v31483
%v31491 = vadd.s32 %v31488, %v31483
%v31493 = vshll.u32 %v31488, 16
%v31494 = vshrl.u32 %v31488, 16
%v31495 = vor.u32 %v31494, %v31493
%v31496 = vxor.u32 %v31495, %v31491
%v31499 = vadd.s32 %v31496, %v31491
%v31503 = vadd.s32 %v31499, %v9
%v31505 = vshll.u32 %v31496, 24
%v31506 = vshrl.u32 %v31496, 8
%v31507 = vor.u32 %v31506, %v31505
%v31508 = vxor.u32 %v31507, %v31499
%v31511 = vadd.s32 %v31508, %v8
%v31515 = vadd.s32 4, %v31511
%v31519 = vadd.s32 %v31515, %v31503
%v31521 = vshll.u32 %v31515, 13
%v31522 = vshrl.u32 %v31515, 19
%v31523 = vor.u32 %v31522, %v31521
%v31524 = vxor.u32 %v31523, %v31519
%v31527 = vadd.s32 %v31524, %v31519
%v31529 = vshll.u32 %v31524, 15
%v31530 = vshrl.u32 %v31524, 17
%v31531 = vor.u32 %v31530, %v31529
%v31532 = vxor.u32 %v31531, %v31527
%v31535 = vadd.s32 %v31532, %v31527
%v31537 = vshll.u32 %v31532, 26
%v31538 = vshrl.u32 %v31532, 6
%v31539 = vor.u32 %v31538, %v31537
%v31540 = vxor.u32 %v31539, %v31535
%v31543 = vadd.s32 %v31540, %v31535
%v31547 = vadd.s32 %v31543, %v8
%v31549 = vshll.u32 %v31540, 6
%v31550 = vshrl.u32 %v31540, 26
%v31551 = vor.u32 %v31550, %v31549
%v31552 = vxor.u32 %v31551, %v31543
%v31555 = vadd.s32 %v31552, %v10
%v31559 = vadd.s32 5, %v31555
%v31561 = vxor.u32 %v31559, %v31547
%v31562 = vand.u32.u8 255, %v31561
%v31563 = vand.u32 65535, %v31562
%v31564 = vshrl.u32 %v31563, 1
%v31565 = vor.u32 16256, %v31564
%v31566 = vand.u32.u16 65535, %v31565
%v119912 = vadd.low.f32.bf16 -1.0, %v31566
%v31575 = vmul.f32 2.0, %v119912
%v31579 = vadd.f32 -0.99609375, %v31575
%v31583 = vmax.f32 %v31579, -0.99609375
%v31585 = vand.u32 2147483647, %v31583
%vm31588 = vcmp.eq.f32.partialorder %v31585, 1.0
%v31593 = vmul.f32 inf, %v31583
%v31595 = vxor.u32 2147483648, %v31583
%v31598 = vmul.f32 %v31595, %v31583
%v31600 = vadd.f32 1.0, %v31598
%v31601 = vlog2.pop %v31600
%v31602 = vmul.f32 0.6931472, %v31601
%v31603 = vmul.f32 -0.5, %v31598
%v31604 = vadd.f32 1.0, %v31603
%v31605 = vmul.f32 %v31604, %v31598
%v31606 = vand.u32 2147483647, %v31598
%vm31607 = vcmp.lt.f32.partialorder %v31606, 0.0004427343
%v31608 = vsel /*vm=*/%vm31607, /*on_true_vy=*/%v31605, /*on_false_vx=*/%v31602
%v31609 = vxor.u32 2147483648, %v31608
%vm31612 = vcmp.lt.f32.partialorder %v31609, 5.0
%v31617 = vsel /*vm=*/%vm31612, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v31621 = vsel /*vm=*/%vm31612, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v31625 = vsel /*vm=*/%vm31612, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v31629 = vsel /*vm=*/%vm31612, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v31633 = vsel /*vm=*/%vm31612, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v31637 = vsel /*vm=*/%vm31612, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v31641 = vsel /*vm=*/%vm31612, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v31645 = vsel /*vm=*/%vm31612, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v31649 = vsel /*vm=*/%vm31612, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v31653 = vadd.f32 -2.5, %v31609
%v31655 = vrsqrt.pop %v31609
%v31656 = vmul.f32 %v31655, %v31609
%vm31657 = vcmp.eq.f32.partialorder %v31609, inf
%v31658 = vsel /*vm=*/%vm31657, /*on_true_vy=*/%v31609, /*on_false_vx=*/%v31656
%vm31659 = vcmp.eq.f32.partialorder %v31609, 0.0
%v31660 = vand.u32 2147483648, %v31609
%v31661 = vsel /*vm=*/%vm31659, /*on_true_vy=*/%v31660, /*on_false_vx=*/%v31658
%v31664 = vadd.f32 -3.0, %v31661
%v31668 = vsel /*vm=*/%vm31612, /*on_true_vy=*/%v31653, /*on_false_vx=*/%v31664
%v31672 = vmul.f32 %v31668, %v31649
%v31676 = vadd.f32 %v31672, %v31645
%v31680 = vmul.f32 %v31676, %v31668
%v31684 = vadd.f32 %v31680, %v31641
%v31688 = vmul.f32 %v31684, %v31668
%v31692 = vadd.f32 %v31688, %v31637
%v31696 = vmul.f32 %v31692, %v31668
%v31700 = vadd.f32 %v31696, %v31633
%v31704 = vmul.f32 %v31700, %v31668
%v31708 = vadd.f32 %v31704, %v31629
%v31712 = vmul.f32 %v31708, %v31668
%v31716 = vadd.f32 %v31712, %v31625
%v31720 = vmul.f32 %v31716, %v31668
%v31724 = vadd.f32 %v31720, %v31621
%v31728 = vmul.f32 %v31724, %v31668
%v31732 = vadd.f32 %v31728, %v31617
%v31736 = vmul.f32 %v31732, %v31583
%v31740 = vsel /*vm=*/%vm31588, /*on_true_vy=*/%v31593, /*on_false_vx=*/%v31736
%v31744 = vmul.f32 1.4140625, %v31740
%v31747 = vpack.c.bf16 %v120417, %v31744
%119913 = vst [vmem:[%s280 + $0x120] sm:$0xf] /*vst_source=*/%v31747
%v31751 = vadd.s32 %v30365, %v1868
%v31761 = vadd.s32 %v31751, %v415
%vm31765 = vcmp.lt.u32.totalorder %v31761, %v31751
%vm31770 = vcmp.lt.u32.totalorder %v31751, %v1868
%v31775 = vadd.s32 %v30348, %v1855
%v31779 = vadd.s32 1, %v31775
%v31783 = vsel /*vm=*/%vm31770, /*on_true_vy=*/%v31779, /*on_false_vx=*/%v31775
%v31787 = vadd.s32 1, %v31783
%v31791 = vsel /*vm=*/%vm31765, /*on_true_vy=*/%v31787, /*on_false_vx=*/%v31783
%v31796 = vadd.s32 %v31791, %v10
%v31800 = vadd.s32 %v31761, %v9
%v31804 = vadd.s32 %v31800, %v31796
%v31806 = vshll.u32 %v31800, 13
%v31807 = vshrl.u32 %v31800, 19
%v31808 = vor.u32 %v31807, %v31806
%v31809 = vxor.u32 %v31808, %v31804
%v31812 = vadd.s32 %v31809, %v31804
%v31814 = vshll.u32 %v31809, 15
%v31815 = vshrl.u32 %v31809, 17
%v31816 = vor.u32 %v31815, %v31814
%v31817 = vxor.u32 %v31816, %v31812
%v31820 = vadd.s32 %v31817, %v31812
%v31822 = vshll.u32 %v31817, 26
%v31823 = vshrl.u32 %v31817, 6
%v31824 = vor.u32 %v31823, %v31822
%v31825 = vxor.u32 %v31824, %v31820
%v31828 = vadd.s32 %v31825, %v31820
%v31832 = vadd.s32 %v31828, %v9
%v31834 = vshll.u32 %v31825, 6
%v31835 = vshrl.u32 %v31825, 26
%v31836 = vor.u32 %v31835, %v31834
%v31837 = vxor.u32 %v31836, %v31828
%v31840 = vadd.s32 %v31837, %v8
%v31844 = vadd.s32 1, %v31840
%v31848 = vadd.s32 %v31844, %v31832
%v31850 = vshll.u32 %v31844, 17
%v31851 = vshrl.u32 %v31844, 15
%v31852 = vor.u32 %v31851, %v31850
%v31853 = vxor.u32 %v31852, %v31848
%v31856 = vadd.s32 %v31853, %v31848
%v31858 = vshll.u32 %v31853, 29
%v31859 = vshrl.u32 %v31853, 3
%v31860 = vor.u32 %v31859, %v31858
%v31861 = vxor.u32 %v31860, %v31856
%v31864 = vadd.s32 %v31861, %v31856
%v31866 = vshll.u32 %v31861, 16
%v31867 = vshrl.u32 %v31861, 16
%v31868 = vor.u32 %v31867, %v31866
%v31869 = vxor.u32 %v31868, %v31864
%v31872 = vadd.s32 %v31869, %v31864
%v31876 = vadd.s32 %v31872, %v8
%v31878 = vshll.u32 %v31869, 24
%v31879 = vshrl.u32 %v31869, 8
%v31880 = vor.u32 %v31879, %v31878
%v31881 = vxor.u32 %v31880, %v31872
%v31884 = vadd.s32 %v31881, %v10
%v31888 = vadd.s32 2, %v31884
%v31892 = vadd.s32 %v31888, %v31876
%v31894 = vshll.u32 %v31888, 13
%v31895 = vshrl.u32 %v31888, 19
%v31896 = vor.u32 %v31895, %v31894
%v31897 = vxor.u32 %v31896, %v31892
%v31900 = vadd.s32 %v31897, %v31892
%v31902 = vshll.u32 %v31897, 15
%v31903 = vshrl.u32 %v31897, 17
%v31904 = vor.u32 %v31903, %v31902
%v31905 = vxor.u32 %v31904, %v31900
%v31908 = vadd.s32 %v31905, %v31900
%v31910 = vshll.u32 %v31905, 26
%v31911 = vshrl.u32 %v31905, 6
%v31912 = vor.u32 %v31911, %v31910
%v31913 = vxor.u32 %v31912, %v31908
%v31916 = vadd.s32 %v31913, %v31908
%v31920 = vadd.s32 %v31916, %v10
%v31922 = vshll.u32 %v31913, 6
%v31923 = vshrl.u32 %v31913, 26
%v31924 = vor.u32 %v31923, %v31922
%v31925 = vxor.u32 %v31924, %v31916
%v31928 = vadd.s32 %v31925, %v9
%v31932 = vadd.s32 3, %v31928
%v31936 = vadd.s32 %v31932, %v31920
%v31938 = vshll.u32 %v31932, 17
%v31939 = vshrl.u32 %v31932, 15
%v31940 = vor.u32 %v31939, %v31938
%v31941 = vxor.u32 %v31940, %v31936
%v31944 = vadd.s32 %v31941, %v31936
%v31946 = vshll.u32 %v31941, 29
%v31947 = vshrl.u32 %v31941, 3
%v31948 = vor.u32 %v31947, %v31946
%v31949 = vxor.u32 %v31948, %v31944
%v31952 = vadd.s32 %v31949, %v31944
%v31954 = vshll.u32 %v31949, 16
%v31955 = vshrl.u32 %v31949, 16
%v31956 = vor.u32 %v31955, %v31954
%v31957 = vxor.u32 %v31956, %v31952
%v31960 = vadd.s32 %v31957, %v31952
%v31964 = vadd.s32 %v31960, %v9
%v31966 = vshll.u32 %v31957, 24
%v31967 = vshrl.u32 %v31957, 8
%v31968 = vor.u32 %v31967, %v31966
%v31969 = vxor.u32 %v31968, %v31960
%v31972 = vadd.s32 %v31969, %v8
%v31976 = vadd.s32 4, %v31972
%v31980 = vadd.s32 %v31976, %v31964
%v31982 = vshll.u32 %v31976, 13
%v31983 = vshrl.u32 %v31976, 19
%v31984 = vor.u32 %v31983, %v31982
%v31985 = vxor.u32 %v31984, %v31980
%v31988 = vadd.s32 %v31985, %v31980
%v31990 = vshll.u32 %v31985, 15
%v31991 = vshrl.u32 %v31985, 17
%v31992 = vor.u32 %v31991, %v31990
%v31993 = vxor.u32 %v31992, %v31988
%v31996 = vadd.s32 %v31993, %v31988
%v31998 = vshll.u32 %v31993, 26
%v31999 = vshrl.u32 %v31993, 6
%v32000 = vor.u32 %v31999, %v31998
%v32001 = vxor.u32 %v32000, %v31996
%v32004 = vadd.s32 %v32001, %v31996
%v32008 = vadd.s32 %v32004, %v8
%v32010 = vshll.u32 %v32001, 6
%v32011 = vshrl.u32 %v32001, 26
%v32012 = vor.u32 %v32011, %v32010
%v32013 = vxor.u32 %v32012, %v32004
%v32016 = vadd.s32 %v32013, %v10
%v32020 = vadd.s32 5, %v32016
%v32022 = vxor.u32 %v32020, %v32008
%v32023 = vand.u32.u8 255, %v32022
%v32024 = vand.u32 65535, %v32023
%v32025 = vshrl.u32 %v32024, 1
%v32026 = vor.u32 16256, %v32025
%v32027 = vand.u32.u16 65535, %v32026
%v119914 = vadd.low.f32.bf16 -1.0, %v32027
%v32036 = vmul.f32 2.0, %v119914
%v32040 = vadd.f32 -0.99609375, %v32036
%v32044 = vmax.f32 %v32040, -0.99609375
%v32046 = vand.u32 2147483647, %v32044
%vm32049 = vcmp.eq.f32.partialorder %v32046, 1.0
%v32054 = vmul.f32 inf, %v32044
%v32056 = vxor.u32 2147483648, %v32044
%v32059 = vmul.f32 %v32056, %v32044
%v32061 = vadd.f32 1.0, %v32059
%v32062 = vlog2.pop %v32061
%v32063 = vmul.f32 0.6931472, %v32062
%v32064 = vmul.f32 -0.5, %v32059
%v32065 = vadd.f32 1.0, %v32064
%v32066 = vmul.f32 %v32065, %v32059
%v32067 = vand.u32 2147483647, %v32059
%vm32068 = vcmp.lt.f32.partialorder %v32067, 0.0004427343
%v32069 = vsel /*vm=*/%vm32068, /*on_true_vy=*/%v32066, /*on_false_vx=*/%v32063
%v32070 = vxor.u32 2147483648, %v32069
%vm32073 = vcmp.lt.f32.partialorder %v32070, 5.0
%v32078 = vsel /*vm=*/%vm32073, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v32082 = vsel /*vm=*/%vm32073, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v32086 = vsel /*vm=*/%vm32073, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v32090 = vsel /*vm=*/%vm32073, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v32094 = vsel /*vm=*/%vm32073, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v32098 = vsel /*vm=*/%vm32073, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v32102 = vsel /*vm=*/%vm32073, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v32106 = vsel /*vm=*/%vm32073, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v32110 = vsel /*vm=*/%vm32073, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v32114 = vadd.f32 -2.5, %v32070
%v32116 = vrsqrt.pop %v32070
%v32117 = vmul.f32 %v32116, %v32070
%vm32118 = vcmp.eq.f32.partialorder %v32070, inf
%v32119 = vsel /*vm=*/%vm32118, /*on_true_vy=*/%v32070, /*on_false_vx=*/%v32117
%vm32120 = vcmp.eq.f32.partialorder %v32070, 0.0
%v32121 = vand.u32 2147483648, %v32070
%v32122 = vsel /*vm=*/%vm32120, /*on_true_vy=*/%v32121, /*on_false_vx=*/%v32119
%v32125 = vadd.f32 -3.0, %v32122
%v32129 = vsel /*vm=*/%vm32073, /*on_true_vy=*/%v32114, /*on_false_vx=*/%v32125
%v32133 = vmul.f32 %v32129, %v32110
%v32137 = vadd.f32 %v32133, %v32106
%v32141 = vmul.f32 %v32137, %v32129
%v32145 = vadd.f32 %v32141, %v32102
%v32149 = vmul.f32 %v32145, %v32129
%v32153 = vadd.f32 %v32149, %v32098
%v32157 = vmul.f32 %v32153, %v32129
%v32161 = vadd.f32 %v32157, %v32094
%v32165 = vmul.f32 %v32161, %v32129
%v32169 = vadd.f32 %v32165, %v32090
%v32173 = vmul.f32 %v32169, %v32129
%v32177 = vadd.f32 %v32173, %v32086
%v32181 = vmul.f32 %v32177, %v32129
%v32185 = vadd.f32 %v32181, %v32082
%v32189 = vmul.f32 %v32185, %v32129
%v32193 = vadd.f32 %v32189, %v32078
%v32197 = vmul.f32 %v32193, %v32044
%v32201 = vsel /*vm=*/%vm32049, /*on_true_vy=*/%v32054, /*on_false_vx=*/%v32197
%v32205 = vmul.f32 1.4140625, %v32201
%v32208 = vpack.c.bf16 %v120417, %v32205
%119915 = vst [vmem:[%s280 + $0x1a0] sm:$0xf] /*vst_source=*/%v32208
%v32212 = vadd.s32 %v30365, %v2355
%v32222 = vadd.s32 %v32212, %v415
%vm32226 = vcmp.lt.u32.totalorder %v32222, %v32212
%vm32231 = vcmp.lt.u32.totalorder %v32212, %v2355
%v32236 = vadd.s32 %v30348, %v2342
%v32240 = vadd.s32 1, %v32236
%v32244 = vsel /*vm=*/%vm32231, /*on_true_vy=*/%v32240, /*on_false_vx=*/%v32236
%v32248 = vadd.s32 1, %v32244
%v32252 = vsel /*vm=*/%vm32226, /*on_true_vy=*/%v32248, /*on_false_vx=*/%v32244
%v32257 = vadd.s32 %v32252, %v10
%v32261 = vadd.s32 %v32222, %v9
%v32265 = vadd.s32 %v32261, %v32257
%v32267 = vshll.u32 %v32261, 13
%v32268 = vshrl.u32 %v32261, 19
%v32269 = vor.u32 %v32268, %v32267
%v32270 = vxor.u32 %v32269, %v32265
%v32273 = vadd.s32 %v32270, %v32265
%v32275 = vshll.u32 %v32270, 15
%v32276 = vshrl.u32 %v32270, 17
%v32277 = vor.u32 %v32276, %v32275
%v32278 = vxor.u32 %v32277, %v32273
%v32281 = vadd.s32 %v32278, %v32273
%v32283 = vshll.u32 %v32278, 26
%v32284 = vshrl.u32 %v32278, 6
%v32285 = vor.u32 %v32284, %v32283
%v32286 = vxor.u32 %v32285, %v32281
%v32289 = vadd.s32 %v32286, %v32281
%v32293 = vadd.s32 %v32289, %v9
%v32295 = vshll.u32 %v32286, 6
%v32296 = vshrl.u32 %v32286, 26
%v32297 = vor.u32 %v32296, %v32295
%v32298 = vxor.u32 %v32297, %v32289
%v32301 = vadd.s32 %v32298, %v8
%v32305 = vadd.s32 1, %v32301
%v32309 = vadd.s32 %v32305, %v32293
%v32311 = vshll.u32 %v32305, 17
%v32312 = vshrl.u32 %v32305, 15
%v32313 = vor.u32 %v32312, %v32311
%v32314 = vxor.u32 %v32313, %v32309
%v32317 = vadd.s32 %v32314, %v32309
%v32319 = vshll.u32 %v32314, 29
%v32320 = vshrl.u32 %v32314, 3
%v32321 = vor.u32 %v32320, %v32319
%v32322 = vxor.u32 %v32321, %v32317
%v32325 = vadd.s32 %v32322, %v32317
%v32327 = vshll.u32 %v32322, 16
%v32328 = vshrl.u32 %v32322, 16
%v32329 = vor.u32 %v32328, %v32327
%v32330 = vxor.u32 %v32329, %v32325
%v32333 = vadd.s32 %v32330, %v32325
%v32337 = vadd.s32 %v32333, %v8
%v32339 = vshll.u32 %v32330, 24
%v32340 = vshrl.u32 %v32330, 8
%v32341 = vor.u32 %v32340, %v32339
%v32342 = vxor.u32 %v32341, %v32333
%v32345 = vadd.s32 %v32342, %v10
%v32349 = vadd.s32 2, %v32345
%v32353 = vadd.s32 %v32349, %v32337
%v32355 = vshll.u32 %v32349, 13
%v32356 = vshrl.u32 %v32349, 19
%v32357 = vor.u32 %v32356, %v32355
%v32358 = vxor.u32 %v32357, %v32353
%v32361 = vadd.s32 %v32358, %v32353
%v32363 = vshll.u32 %v32358, 15
%v32364 = vshrl.u32 %v32358, 17
%v32365 = vor.u32 %v32364, %v32363
%v32366 = vxor.u32 %v32365, %v32361
%v32369 = vadd.s32 %v32366, %v32361
%v32371 = vshll.u32 %v32366, 26
%v32372 = vshrl.u32 %v32366, 6
%v32373 = vor.u32 %v32372, %v32371
%v32374 = vxor.u32 %v32373, %v32369
%v32377 = vadd.s32 %v32374, %v32369
%v32381 = vadd.s32 %v32377, %v10
%v32383 = vshll.u32 %v32374, 6
%v32384 = vshrl.u32 %v32374, 26
%v32385 = vor.u32 %v32384, %v32383
%v32386 = vxor.u32 %v32385, %v32377
%v32389 = vadd.s32 %v32386, %v9
%v32393 = vadd.s32 3, %v32389
%v32397 = vadd.s32 %v32393, %v32381
%v32399 = vshll.u32 %v32393, 17
%v32400 = vshrl.u32 %v32393, 15
%v32401 = vor.u32 %v32400, %v32399
%v32402 = vxor.u32 %v32401, %v32397
%v32405 = vadd.s32 %v32402, %v32397
%v32407 = vshll.u32 %v32402, 29
%v32408 = vshrl.u32 %v32402, 3
%v32409 = vor.u32 %v32408, %v32407
%v32410 = vxor.u32 %v32409, %v32405
%v32413 = vadd.s32 %v32410, %v32405
%v32415 = vshll.u32 %v32410, 16
%v32416 = vshrl.u32 %v32410, 16
%v32417 = vor.u32 %v32416, %v32415
%v32418 = vxor.u32 %v32417, %v32413
%v32421 = vadd.s32 %v32418, %v32413
%v32425 = vadd.s32 %v32421, %v9
%v32427 = vshll.u32 %v32418, 24
%v32428 = vshrl.u32 %v32418, 8
%v32429 = vor.u32 %v32428, %v32427
%v32430 = vxor.u32 %v32429, %v32421
%v32433 = vadd.s32 %v32430, %v8
%v32437 = vadd.s32 4, %v32433
%v32441 = vadd.s32 %v32437, %v32425
%v32443 = vshll.u32 %v32437, 13
%v32444 = vshrl.u32 %v32437, 19
%v32445 = vor.u32 %v32444, %v32443
%v32446 = vxor.u32 %v32445, %v32441
%v32449 = vadd.s32 %v32446, %v32441
%v32451 = vshll.u32 %v32446, 15
%v32452 = vshrl.u32 %v32446, 17
%v32453 = vor.u32 %v32452, %v32451
%v32454 = vxor.u32 %v32453, %v32449
%v32457 = vadd.s32 %v32454, %v32449
%v32459 = vshll.u32 %v32454, 26
%v32460 = vshrl.u32 %v32454, 6
%v32461 = vor.u32 %v32460, %v32459
%v32462 = vxor.u32 %v32461, %v32457
%v32465 = vadd.s32 %v32462, %v32457
%v32469 = vadd.s32 %v32465, %v8
%v32471 = vshll.u32 %v32462, 6
%v32472 = vshrl.u32 %v32462, 26
%v32473 = vor.u32 %v32472, %v32471
%v32474 = vxor.u32 %v32473, %v32465
%v32477 = vadd.s32 %v32474, %v10
%v32481 = vadd.s32 5, %v32477
%v32483 = vxor.u32 %v32481, %v32469
%v32484 = vand.u32.u8 255, %v32483
%v32485 = vand.u32 65535, %v32484
%v32486 = vshrl.u32 %v32485, 1
%v32487 = vor.u32 16256, %v32486
%v32488 = vand.u32.u16 65535, %v32487
%v119916 = vadd.low.f32.bf16 -1.0, %v32488
%v32497 = vmul.f32 2.0, %v119916
%v32501 = vadd.f32 -0.99609375, %v32497
%v32505 = vmax.f32 %v32501, -0.99609375
%v32507 = vand.u32 2147483647, %v32505
%vm32510 = vcmp.eq.f32.partialorder %v32507, 1.0
%v32515 = vmul.f32 inf, %v32505
%v32517 = vxor.u32 2147483648, %v32505
%v32520 = vmul.f32 %v32517, %v32505
%v32522 = vadd.f32 1.0, %v32520
%v32523 = vlog2.pop %v32522
%v32524 = vmul.f32 0.6931472, %v32523
%v32525 = vmul.f32 -0.5, %v32520
%v32526 = vadd.f32 1.0, %v32525
%v32527 = vmul.f32 %v32526, %v32520
%v32528 = vand.u32 2147483647, %v32520
%vm32529 = vcmp.lt.f32.partialorder %v32528, 0.0004427343
%v32530 = vsel /*vm=*/%vm32529, /*on_true_vy=*/%v32527, /*on_false_vx=*/%v32524
%v32531 = vxor.u32 2147483648, %v32530
%vm32534 = vcmp.lt.f32.partialorder %v32531, 5.0
%v32539 = vsel /*vm=*/%vm32534, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v32543 = vsel /*vm=*/%vm32534, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v32547 = vsel /*vm=*/%vm32534, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v32551 = vsel /*vm=*/%vm32534, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v32555 = vsel /*vm=*/%vm32534, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v32559 = vsel /*vm=*/%vm32534, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v32563 = vsel /*vm=*/%vm32534, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v32567 = vsel /*vm=*/%vm32534, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v32571 = vsel /*vm=*/%vm32534, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v32575 = vadd.f32 -2.5, %v32531
%v32577 = vrsqrt.pop %v32531
%v32578 = vmul.f32 %v32577, %v32531
%vm32579 = vcmp.eq.f32.partialorder %v32531, inf
%v32580 = vsel /*vm=*/%vm32579, /*on_true_vy=*/%v32531, /*on_false_vx=*/%v32578
%vm32581 = vcmp.eq.f32.partialorder %v32531, 0.0
%v32582 = vand.u32 2147483648, %v32531
%v32583 = vsel /*vm=*/%vm32581, /*on_true_vy=*/%v32582, /*on_false_vx=*/%v32580
%v32586 = vadd.f32 -3.0, %v32583
%v32590 = vsel /*vm=*/%vm32534, /*on_true_vy=*/%v32575, /*on_false_vx=*/%v32586
%v32594 = vmul.f32 %v32590, %v32571
%v32598 = vadd.f32 %v32594, %v32567
%v32602 = vmul.f32 %v32598, %v32590
%v32606 = vadd.f32 %v32602, %v32563
%v32610 = vmul.f32 %v32606, %v32590
%v32614 = vadd.f32 %v32610, %v32559
%v32618 = vmul.f32 %v32614, %v32590
%v32622 = vadd.f32 %v32618, %v32555
%v32626 = vmul.f32 %v32622, %v32590
%v32630 = vadd.f32 %v32626, %v32551
%v32634 = vmul.f32 %v32630, %v32590
%v32638 = vadd.f32 %v32634, %v32547
%v32642 = vmul.f32 %v32638, %v32590
%v32646 = vadd.f32 %v32642, %v32543
%v32650 = vmul.f32 %v32646, %v32590
%v32654 = vadd.f32 %v32650, %v32539
%v32658 = vmul.f32 %v32654, %v32505
%v32662 = vsel /*vm=*/%vm32510, /*on_true_vy=*/%v32515, /*on_false_vx=*/%v32658
%v32666 = vmul.f32 1.4140625, %v32662
%v32669 = vpack.c.bf16 %v120417, %v32666
%119917 = vst [vmem:[%s280 + $0x220] sm:$0xf] /*vst_source=*/%v32669
%v32673 = vadd.s32 %v30365, %v2842
%v32683 = vadd.s32 %v32673, %v415
%vm32687 = vcmp.lt.u32.totalorder %v32683, %v32673
%vm32692 = vcmp.lt.u32.totalorder %v32673, %v2842
%v32697 = vadd.s32 %v30348, %v2829
%v32701 = vadd.s32 1, %v32697
%v32705 = vsel /*vm=*/%vm32692, /*on_true_vy=*/%v32701, /*on_false_vx=*/%v32697
%v32709 = vadd.s32 1, %v32705
%v32713 = vsel /*vm=*/%vm32687, /*on_true_vy=*/%v32709, /*on_false_vx=*/%v32705
%v32718 = vadd.s32 %v32713, %v10
%v32722 = vadd.s32 %v32683, %v9
%v32726 = vadd.s32 %v32722, %v32718
%v32728 = vshll.u32 %v32722, 13
%v32729 = vshrl.u32 %v32722, 19
%v32730 = vor.u32 %v32729, %v32728
%v32731 = vxor.u32 %v32730, %v32726
%v32734 = vadd.s32 %v32731, %v32726
%v32736 = vshll.u32 %v32731, 15
%v32737 = vshrl.u32 %v32731, 17
%v32738 = vor.u32 %v32737, %v32736
%v32739 = vxor.u32 %v32738, %v32734
%v32742 = vadd.s32 %v32739, %v32734
%v32744 = vshll.u32 %v32739, 26
%v32745 = vshrl.u32 %v32739, 6
%v32746 = vor.u32 %v32745, %v32744
%v32747 = vxor.u32 %v32746, %v32742
%v32750 = vadd.s32 %v32747, %v32742
%v32754 = vadd.s32 %v32750, %v9
%v32756 = vshll.u32 %v32747, 6
%v32757 = vshrl.u32 %v32747, 26
%v32758 = vor.u32 %v32757, %v32756
%v32759 = vxor.u32 %v32758, %v32750
%v32762 = vadd.s32 %v32759, %v8
%v32766 = vadd.s32 1, %v32762
%v32770 = vadd.s32 %v32766, %v32754
%v32772 = vshll.u32 %v32766, 17
%v32773 = vshrl.u32 %v32766, 15
%v32774 = vor.u32 %v32773, %v32772
%v32775 = vxor.u32 %v32774, %v32770
%v32778 = vadd.s32 %v32775, %v32770
%v32780 = vshll.u32 %v32775, 29
%v32781 = vshrl.u32 %v32775, 3
%v32782 = vor.u32 %v32781, %v32780
%v32783 = vxor.u32 %v32782, %v32778
%v32786 = vadd.s32 %v32783, %v32778
%v32788 = vshll.u32 %v32783, 16
%v32789 = vshrl.u32 %v32783, 16
%v32790 = vor.u32 %v32789, %v32788
%v32791 = vxor.u32 %v32790, %v32786
%v32794 = vadd.s32 %v32791, %v32786
%v32798 = vadd.s32 %v32794, %v8
%v32800 = vshll.u32 %v32791, 24
%v32801 = vshrl.u32 %v32791, 8
%v32802 = vor.u32 %v32801, %v32800
%v32803 = vxor.u32 %v32802, %v32794
%v32806 = vadd.s32 %v32803, %v10
%v32810 = vadd.s32 2, %v32806
%v32814 = vadd.s32 %v32810, %v32798
%v32816 = vshll.u32 %v32810, 13
%v32817 = vshrl.u32 %v32810, 19
%v32818 = vor.u32 %v32817, %v32816
%v32819 = vxor.u32 %v32818, %v32814
%v32822 = vadd.s32 %v32819, %v32814
%v32824 = vshll.u32 %v32819, 15
%v32825 = vshrl.u32 %v32819, 17
%v32826 = vor.u32 %v32825, %v32824
%v32827 = vxor.u32 %v32826, %v32822
%v32830 = vadd.s32 %v32827, %v32822
%v32832 = vshll.u32 %v32827, 26
%v32833 = vshrl.u32 %v32827, 6
%v32834 = vor.u32 %v32833, %v32832
%v32835 = vxor.u32 %v32834, %v32830
%v32838 = vadd.s32 %v32835, %v32830
%v32842 = vadd.s32 %v32838, %v10
%v32844 = vshll.u32 %v32835, 6
%v32845 = vshrl.u32 %v32835, 26
%v32846 = vor.u32 %v32845, %v32844
%v32847 = vxor.u32 %v32846, %v32838
%v32850 = vadd.s32 %v32847, %v9
%v32854 = vadd.s32 3, %v32850
%v32858 = vadd.s32 %v32854, %v32842
%v32860 = vshll.u32 %v32854, 17
%v32861 = vshrl.u32 %v32854, 15
%v32862 = vor.u32 %v32861, %v32860
%v32863 = vxor.u32 %v32862, %v32858
%v32866 = vadd.s32 %v32863, %v32858
%v32868 = vshll.u32 %v32863, 29
%v32869 = vshrl.u32 %v32863, 3
%v32870 = vor.u32 %v32869, %v32868
%v32871 = vxor.u32 %v32870, %v32866
%v32874 = vadd.s32 %v32871, %v32866
%v32876 = vshll.u32 %v32871, 16
%v32877 = vshrl.u32 %v32871, 16
%v32878 = vor.u32 %v32877, %v32876
%v32879 = vxor.u32 %v32878, %v32874
%v32882 = vadd.s32 %v32879, %v32874
%v32886 = vadd.s32 %v32882, %v9
%v32888 = vshll.u32 %v32879, 24
%v32889 = vshrl.u32 %v32879, 8
%v32890 = vor.u32 %v32889, %v32888
%v32891 = vxor.u32 %v32890, %v32882
%v32894 = vadd.s32 %v32891, %v8
%v32898 = vadd.s32 4, %v32894
%v32902 = vadd.s32 %v32898, %v32886
%v32904 = vshll.u32 %v32898, 13
%v32905 = vshrl.u32 %v32898, 19
%v32906 = vor.u32 %v32905, %v32904
%v32907 = vxor.u32 %v32906, %v32902
%v32910 = vadd.s32 %v32907, %v32902
%v32912 = vshll.u32 %v32907, 15
%v32913 = vshrl.u32 %v32907, 17
%v32914 = vor.u32 %v32913, %v32912
%v32915 = vxor.u32 %v32914, %v32910
%v32918 = vadd.s32 %v32915, %v32910
%v32920 = vshll.u32 %v32915, 26
%v32921 = vshrl.u32 %v32915, 6
%v32922 = vor.u32 %v32921, %v32920
%v32923 = vxor.u32 %v32922, %v32918
%v32926 = vadd.s32 %v32923, %v32918
%v32930 = vadd.s32 %v32926, %v8
%v32932 = vshll.u32 %v32923, 6
%v32933 = vshrl.u32 %v32923, 26
%v32934 = vor.u32 %v32933, %v32932
%v32935 = vxor.u32 %v32934, %v32926
%v32938 = vadd.s32 %v32935, %v10
%v32942 = vadd.s32 5, %v32938
%v32944 = vxor.u32 %v32942, %v32930
%v32945 = vand.u32.u8 255, %v32944
%v32946 = vand.u32 65535, %v32945
%v32947 = vshrl.u32 %v32946, 1
%v32948 = vor.u32 16256, %v32947
%v32949 = vand.u32.u16 65535, %v32948
%v119918 = vadd.low.f32.bf16 -1.0, %v32949
%v32958 = vmul.f32 2.0, %v119918
%v32962 = vadd.f32 -0.99609375, %v32958
%v32966 = vmax.f32 %v32962, -0.99609375
%v32968 = vand.u32 2147483647, %v32966
%vm32971 = vcmp.eq.f32.partialorder %v32968, 1.0
%v32976 = vmul.f32 inf, %v32966
%v32978 = vxor.u32 2147483648, %v32966
%v32981 = vmul.f32 %v32978, %v32966
%v32983 = vadd.f32 1.0, %v32981
%v32984 = vlog2.pop %v32983
%v32985 = vmul.f32 0.6931472, %v32984
%v32986 = vmul.f32 -0.5, %v32981
%v32987 = vadd.f32 1.0, %v32986
%v32988 = vmul.f32 %v32987, %v32981
%v32989 = vand.u32 2147483647, %v32981
%vm32990 = vcmp.lt.f32.partialorder %v32989, 0.0004427343
%v32991 = vsel /*vm=*/%vm32990, /*on_true_vy=*/%v32988, /*on_false_vx=*/%v32985
%v32992 = vxor.u32 2147483648, %v32991
%vm32995 = vcmp.lt.f32.partialorder %v32992, 5.0
%v33000 = vsel /*vm=*/%vm32995, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v33004 = vsel /*vm=*/%vm32995, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v33008 = vsel /*vm=*/%vm32995, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v33012 = vsel /*vm=*/%vm32995, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v33016 = vsel /*vm=*/%vm32995, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v33020 = vsel /*vm=*/%vm32995, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v33024 = vsel /*vm=*/%vm32995, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v33028 = vsel /*vm=*/%vm32995, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v33032 = vsel /*vm=*/%vm32995, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v33036 = vadd.f32 -2.5, %v32992
%v33038 = vrsqrt.pop %v32992
%v33039 = vmul.f32 %v33038, %v32992
%vm33040 = vcmp.eq.f32.partialorder %v32992, inf
%v33041 = vsel /*vm=*/%vm33040, /*on_true_vy=*/%v32992, /*on_false_vx=*/%v33039
%vm33042 = vcmp.eq.f32.partialorder %v32992, 0.0
%v33043 = vand.u32 2147483648, %v32992
%v33044 = vsel /*vm=*/%vm33042, /*on_true_vy=*/%v33043, /*on_false_vx=*/%v33041
%v33047 = vadd.f32 -3.0, %v33044
%v33051 = vsel /*vm=*/%vm32995, /*on_true_vy=*/%v33036, /*on_false_vx=*/%v33047
%v33055 = vmul.f32 %v33051, %v33032
%v33059 = vadd.f32 %v33055, %v33028
%v33063 = vmul.f32 %v33059, %v33051
%v33067 = vadd.f32 %v33063, %v33024
%v33071 = vmul.f32 %v33067, %v33051
%v33075 = vadd.f32 %v33071, %v33020
%v33079 = vmul.f32 %v33075, %v33051
%v33083 = vadd.f32 %v33079, %v33016
%v33087 = vmul.f32 %v33083, %v33051
%v33091 = vadd.f32 %v33087, %v33012
%v33095 = vmul.f32 %v33091, %v33051
%v33099 = vadd.f32 %v33095, %v33008
%v33103 = vmul.f32 %v33099, %v33051
%v33107 = vadd.f32 %v33103, %v33004
%v33111 = vmul.f32 %v33107, %v33051
%v33115 = vadd.f32 %v33111, %v33000
%v33119 = vmul.f32 %v33115, %v32966
%v33123 = vsel /*vm=*/%vm32971, /*on_true_vy=*/%v32976, /*on_false_vx=*/%v33119
%v33127 = vmul.f32 1.4140625, %v33123
%v33130 = vpack.c.bf16 %v120417, %v33127
%119919 = vst [vmem:[%s280 + $0x2a0] sm:$0xf] /*vst_source=*/%v33130
%v33134 = vadd.s32 %v30365, %v3329
%v33144 = vadd.s32 %v33134, %v415
%vm33148 = vcmp.lt.u32.totalorder %v33144, %v33134
%vm33153 = vcmp.lt.u32.totalorder %v33134, %v3329
%v33158 = vadd.s32 %v30348, %v3316
%v33162 = vadd.s32 1, %v33158
%v33166 = vsel /*vm=*/%vm33153, /*on_true_vy=*/%v33162, /*on_false_vx=*/%v33158
%v33170 = vadd.s32 1, %v33166
%v33174 = vsel /*vm=*/%vm33148, /*on_true_vy=*/%v33170, /*on_false_vx=*/%v33166
%v33179 = vadd.s32 %v33174, %v10
%v33183 = vadd.s32 %v33144, %v9
%v33187 = vadd.s32 %v33183, %v33179
%v33189 = vshll.u32 %v33183, 13
%v33190 = vshrl.u32 %v33183, 19
%v33191 = vor.u32 %v33190, %v33189
%v33192 = vxor.u32 %v33191, %v33187
%v33195 = vadd.s32 %v33192, %v33187
%v33197 = vshll.u32 %v33192, 15
%v33198 = vshrl.u32 %v33192, 17
%v33199 = vor.u32 %v33198, %v33197
%v33200 = vxor.u32 %v33199, %v33195
%v33203 = vadd.s32 %v33200, %v33195
%v33205 = vshll.u32 %v33200, 26
%v33206 = vshrl.u32 %v33200, 6
%v33207 = vor.u32 %v33206, %v33205
%v33208 = vxor.u32 %v33207, %v33203
%v33211 = vadd.s32 %v33208, %v33203
%v33215 = vadd.s32 %v33211, %v9
%v33217 = vshll.u32 %v33208, 6
%v33218 = vshrl.u32 %v33208, 26
%v33219 = vor.u32 %v33218, %v33217
%v33220 = vxor.u32 %v33219, %v33211
%v33223 = vadd.s32 %v33220, %v8
%v33227 = vadd.s32 1, %v33223
%v33231 = vadd.s32 %v33227, %v33215
%v33233 = vshll.u32 %v33227, 17
%v33234 = vshrl.u32 %v33227, 15
%v33235 = vor.u32 %v33234, %v33233
%v33236 = vxor.u32 %v33235, %v33231
%v33239 = vadd.s32 %v33236, %v33231
%v33241 = vshll.u32 %v33236, 29
%v33242 = vshrl.u32 %v33236, 3
%v33243 = vor.u32 %v33242, %v33241
%v33244 = vxor.u32 %v33243, %v33239
%v33247 = vadd.s32 %v33244, %v33239
%v33249 = vshll.u32 %v33244, 16
%v33250 = vshrl.u32 %v33244, 16
%v33251 = vor.u32 %v33250, %v33249
%v33252 = vxor.u32 %v33251, %v33247
%v33255 = vadd.s32 %v33252, %v33247
%v33259 = vadd.s32 %v33255, %v8
%v33261 = vshll.u32 %v33252, 24
%v33262 = vshrl.u32 %v33252, 8
%v33263 = vor.u32 %v33262, %v33261
%v33264 = vxor.u32 %v33263, %v33255
%v33267 = vadd.s32 %v33264, %v10
%v33271 = vadd.s32 2, %v33267
%v33275 = vadd.s32 %v33271, %v33259
%v33277 = vshll.u32 %v33271, 13
%v33278 = vshrl.u32 %v33271, 19
%v33279 = vor.u32 %v33278, %v33277
%v33280 = vxor.u32 %v33279, %v33275
%v33283 = vadd.s32 %v33280, %v33275
%v33285 = vshll.u32 %v33280, 15
%v33286 = vshrl.u32 %v33280, 17
%v33287 = vor.u32 %v33286, %v33285
%v33288 = vxor.u32 %v33287, %v33283
%v33291 = vadd.s32 %v33288, %v33283
%v33293 = vshll.u32 %v33288, 26
%v33294 = vshrl.u32 %v33288, 6
%v33295 = vor.u32 %v33294, %v33293
%v33296 = vxor.u32 %v33295, %v33291
%v33299 = vadd.s32 %v33296, %v33291
%v33303 = vadd.s32 %v33299, %v10
%v33305 = vshll.u32 %v33296, 6
%v33306 = vshrl.u32 %v33296, 26
%v33307 = vor.u32 %v33306, %v33305
%v33308 = vxor.u32 %v33307, %v33299
%v33311 = vadd.s32 %v33308, %v9
%v33315 = vadd.s32 3, %v33311
%v33319 = vadd.s32 %v33315, %v33303
%v33321 = vshll.u32 %v33315, 17
%v33322 = vshrl.u32 %v33315, 15
%v33323 = vor.u32 %v33322, %v33321
%v33324 = vxor.u32 %v33323, %v33319
%v33327 = vadd.s32 %v33324, %v33319
%v33329 = vshll.u32 %v33324, 29
%v33330 = vshrl.u32 %v33324, 3
%v33331 = vor.u32 %v33330, %v33329
%v33332 = vxor.u32 %v33331, %v33327
%v33335 = vadd.s32 %v33332, %v33327
%v33337 = vshll.u32 %v33332, 16
%v33338 = vshrl.u32 %v33332, 16
%v33339 = vor.u32 %v33338, %v33337
%v33340 = vxor.u32 %v33339, %v33335
%v33343 = vadd.s32 %v33340, %v33335
%v33347 = vadd.s32 %v33343, %v9
%v33349 = vshll.u32 %v33340, 24
%v33350 = vshrl.u32 %v33340, 8
%v33351 = vor.u32 %v33350, %v33349
%v33352 = vxor.u32 %v33351, %v33343
%v33355 = vadd.s32 %v33352, %v8
%v33359 = vadd.s32 4, %v33355
%v33363 = vadd.s32 %v33359, %v33347
%v33365 = vshll.u32 %v33359, 13
%v33366 = vshrl.u32 %v33359, 19
%v33367 = vor.u32 %v33366, %v33365
%v33368 = vxor.u32 %v33367, %v33363
%v33371 = vadd.s32 %v33368, %v33363
%v33373 = vshll.u32 %v33368, 15
%v33374 = vshrl.u32 %v33368, 17
%v33375 = vor.u32 %v33374, %v33373
%v33376 = vxor.u32 %v33375, %v33371
%v33379 = vadd.s32 %v33376, %v33371
%v33381 = vshll.u32 %v33376, 26
%v33382 = vshrl.u32 %v33376, 6
%v33383 = vor.u32 %v33382, %v33381
%v33384 = vxor.u32 %v33383, %v33379
%v33387 = vadd.s32 %v33384, %v33379
%v33391 = vadd.s32 %v33387, %v8
%v33393 = vshll.u32 %v33384, 6
%v33394 = vshrl.u32 %v33384, 26
%v33395 = vor.u32 %v33394, %v33393
%v33396 = vxor.u32 %v33395, %v33387
%v33399 = vadd.s32 %v33396, %v10
%v33403 = vadd.s32 5, %v33399
%v33405 = vxor.u32 %v33403, %v33391
%v33406 = vand.u32.u8 255, %v33405
%v33407 = vand.u32 65535, %v33406
%v33408 = vshrl.u32 %v33407, 1
%v33409 = vor.u32 16256, %v33408
%v33410 = vand.u32.u16 65535, %v33409
%v119920 = vadd.low.f32.bf16 -1.0, %v33410
%v33419 = vmul.f32 2.0, %v119920
%v33423 = vadd.f32 -0.99609375, %v33419
%v33427 = vmax.f32 %v33423, -0.99609375
%v33429 = vand.u32 2147483647, %v33427
%vm33432 = vcmp.eq.f32.partialorder %v33429, 1.0
%v33437 = vmul.f32 inf, %v33427
%v33439 = vxor.u32 2147483648, %v33427
%v33442 = vmul.f32 %v33439, %v33427
%v33444 = vadd.f32 1.0, %v33442
%v33445 = vlog2.pop %v33444
%v33446 = vmul.f32 0.6931472, %v33445
%v33447 = vmul.f32 -0.5, %v33442
%v33448 = vadd.f32 1.0, %v33447
%v33449 = vmul.f32 %v33448, %v33442
%v33450 = vand.u32 2147483647, %v33442
%vm33451 = vcmp.lt.f32.partialorder %v33450, 0.0004427343
%v33452 = vsel /*vm=*/%vm33451, /*on_true_vy=*/%v33449, /*on_false_vx=*/%v33446
%v33453 = vxor.u32 2147483648, %v33452
%vm33456 = vcmp.lt.f32.partialorder %v33453, 5.0
%v33461 = vsel /*vm=*/%vm33456, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v33465 = vsel /*vm=*/%vm33456, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v33469 = vsel /*vm=*/%vm33456, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v33473 = vsel /*vm=*/%vm33456, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v33477 = vsel /*vm=*/%vm33456, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v33481 = vsel /*vm=*/%vm33456, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v33485 = vsel /*vm=*/%vm33456, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v33489 = vsel /*vm=*/%vm33456, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v33493 = vsel /*vm=*/%vm33456, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v33497 = vadd.f32 -2.5, %v33453
%v33499 = vrsqrt.pop %v33453
%v33500 = vmul.f32 %v33499, %v33453
%vm33501 = vcmp.eq.f32.partialorder %v33453, inf
%v33502 = vsel /*vm=*/%vm33501, /*on_true_vy=*/%v33453, /*on_false_vx=*/%v33500
%vm33503 = vcmp.eq.f32.partialorder %v33453, 0.0
%v33504 = vand.u32 2147483648, %v33453
%v33505 = vsel /*vm=*/%vm33503, /*on_true_vy=*/%v33504, /*on_false_vx=*/%v33502
%v33508 = vadd.f32 -3.0, %v33505
%v33512 = vsel /*vm=*/%vm33456, /*on_true_vy=*/%v33497, /*on_false_vx=*/%v33508
%v33516 = vmul.f32 %v33512, %v33493
%v33520 = vadd.f32 %v33516, %v33489
%v33524 = vmul.f32 %v33520, %v33512
%v33528 = vadd.f32 %v33524, %v33485
%v33532 = vmul.f32 %v33528, %v33512
%v33536 = vadd.f32 %v33532, %v33481
%v33540 = vmul.f32 %v33536, %v33512
%v33544 = vadd.f32 %v33540, %v33477
%v33548 = vmul.f32 %v33544, %v33512
%v33552 = vadd.f32 %v33548, %v33473
%v33556 = vmul.f32 %v33552, %v33512
%v33560 = vadd.f32 %v33556, %v33469
%v33564 = vmul.f32 %v33560, %v33512
%v33568 = vadd.f32 %v33564, %v33465
%v33572 = vmul.f32 %v33568, %v33512
%v33576 = vadd.f32 %v33572, %v33461
%v33580 = vmul.f32 %v33576, %v33427
%v33584 = vsel /*vm=*/%vm33432, /*on_true_vy=*/%v33437, /*on_false_vx=*/%v33580
%v33588 = vmul.f32 1.4140625, %v33584
%v33591 = vpack.c.bf16 %v120417, %v33588
%119921 = vst [vmem:[%s280 + $0x320] sm:$0xf] /*vst_source=*/%v33591
%v33595 = vadd.s32 %v30365, %v3816
%v33605 = vadd.s32 %v33595, %v415
%vm33609 = vcmp.lt.u32.totalorder %v33605, %v33595
%vm33614 = vcmp.lt.u32.totalorder %v33595, %v3816
%v33619 = vadd.s32 %v30348, %v3803
%v33623 = vadd.s32 1, %v33619
%v33627 = vsel /*vm=*/%vm33614, /*on_true_vy=*/%v33623, /*on_false_vx=*/%v33619
%v33631 = vadd.s32 1, %v33627
%v33635 = vsel /*vm=*/%vm33609, /*on_true_vy=*/%v33631, /*on_false_vx=*/%v33627
%v33640 = vadd.s32 %v33635, %v10
%v33644 = vadd.s32 %v33605, %v9
%v33648 = vadd.s32 %v33644, %v33640
%v33650 = vshll.u32 %v33644, 13
%v33651 = vshrl.u32 %v33644, 19
%v33652 = vor.u32 %v33651, %v33650
%v33653 = vxor.u32 %v33652, %v33648
%v33656 = vadd.s32 %v33653, %v33648
%v33658 = vshll.u32 %v33653, 15
%v33659 = vshrl.u32 %v33653, 17
%v33660 = vor.u32 %v33659, %v33658
%v33661 = vxor.u32 %v33660, %v33656
%v33664 = vadd.s32 %v33661, %v33656
%v33666 = vshll.u32 %v33661, 26
%v33667 = vshrl.u32 %v33661, 6
%v33668 = vor.u32 %v33667, %v33666
%v33669 = vxor.u32 %v33668, %v33664
%v33672 = vadd.s32 %v33669, %v33664
%v33676 = vadd.s32 %v33672, %v9
%v33678 = vshll.u32 %v33669, 6
%v33679 = vshrl.u32 %v33669, 26
%v33680 = vor.u32 %v33679, %v33678
%v33681 = vxor.u32 %v33680, %v33672
%v33684 = vadd.s32 %v33681, %v8
%v33688 = vadd.s32 1, %v33684
%v33692 = vadd.s32 %v33688, %v33676
%v33694 = vshll.u32 %v33688, 17
%v33695 = vshrl.u32 %v33688, 15
%v33696 = vor.u32 %v33695, %v33694
%v33697 = vxor.u32 %v33696, %v33692
%v33700 = vadd.s32 %v33697, %v33692
%v33702 = vshll.u32 %v33697, 29
%v33703 = vshrl.u32 %v33697, 3
%v33704 = vor.u32 %v33703, %v33702
%v33705 = vxor.u32 %v33704, %v33700
%v33708 = vadd.s32 %v33705, %v33700
%v33710 = vshll.u32 %v33705, 16
%v33711 = vshrl.u32 %v33705, 16
%v33712 = vor.u32 %v33711, %v33710
%v33713 = vxor.u32 %v33712, %v33708
%v33716 = vadd.s32 %v33713, %v33708
%v33720 = vadd.s32 %v33716, %v8
%v33722 = vshll.u32 %v33713, 24
%v33723 = vshrl.u32 %v33713, 8
%v33724 = vor.u32 %v33723, %v33722
%v33725 = vxor.u32 %v33724, %v33716
%v33728 = vadd.s32 %v33725, %v10
%v33732 = vadd.s32 2, %v33728
%v33736 = vadd.s32 %v33732, %v33720
%v33738 = vshll.u32 %v33732, 13
%v33739 = vshrl.u32 %v33732, 19
%v33740 = vor.u32 %v33739, %v33738
%v33741 = vxor.u32 %v33740, %v33736
%v33744 = vadd.s32 %v33741, %v33736
%v33746 = vshll.u32 %v33741, 15
%v33747 = vshrl.u32 %v33741, 17
%v33748 = vor.u32 %v33747, %v33746
%v33749 = vxor.u32 %v33748, %v33744
%v33752 = vadd.s32 %v33749, %v33744
%v33754 = vshll.u32 %v33749, 26
%v33755 = vshrl.u32 %v33749, 6
%v33756 = vor.u32 %v33755, %v33754
%v33757 = vxor.u32 %v33756, %v33752
%v33760 = vadd.s32 %v33757, %v33752
%v33764 = vadd.s32 %v33760, %v10
%v33766 = vshll.u32 %v33757, 6
%v33767 = vshrl.u32 %v33757, 26
%v33768 = vor.u32 %v33767, %v33766
%v33769 = vxor.u32 %v33768, %v33760
%v33772 = vadd.s32 %v33769, %v9
%v33776 = vadd.s32 3, %v33772
%v33780 = vadd.s32 %v33776, %v33764
%v33782 = vshll.u32 %v33776, 17
%v33783 = vshrl.u32 %v33776, 15
%v33784 = vor.u32 %v33783, %v33782
%v33785 = vxor.u32 %v33784, %v33780
%v33788 = vadd.s32 %v33785, %v33780
%v33790 = vshll.u32 %v33785, 29
%v33791 = vshrl.u32 %v33785, 3
%v33792 = vor.u32 %v33791, %v33790
%v33793 = vxor.u32 %v33792, %v33788
%v33796 = vadd.s32 %v33793, %v33788
%v33798 = vshll.u32 %v33793, 16
%v33799 = vshrl.u32 %v33793, 16
%v33800 = vor.u32 %v33799, %v33798
%v33801 = vxor.u32 %v33800, %v33796
%v33804 = vadd.s32 %v33801, %v33796
%v33808 = vadd.s32 %v33804, %v9
%v33810 = vshll.u32 %v33801, 24
%v33811 = vshrl.u32 %v33801, 8
%v33812 = vor.u32 %v33811, %v33810
%v33813 = vxor.u32 %v33812, %v33804
%v33816 = vadd.s32 %v33813, %v8
%v33820 = vadd.s32 4, %v33816
%v33824 = vadd.s32 %v33820, %v33808
%v33826 = vshll.u32 %v33820, 13
%v33827 = vshrl.u32 %v33820, 19
%v33828 = vor.u32 %v33827, %v33826
%v33829 = vxor.u32 %v33828, %v33824
%v33832 = vadd.s32 %v33829, %v33824
%v33834 = vshll.u32 %v33829, 15
%v33835 = vshrl.u32 %v33829, 17
%v33836 = vor.u32 %v33835, %v33834
%v33837 = vxor.u32 %v33836, %v33832
%v33840 = vadd.s32 %v33837, %v33832
%v33842 = vshll.u32 %v33837, 26
%v33843 = vshrl.u32 %v33837, 6
%v33844 = vor.u32 %v33843, %v33842
%v33845 = vxor.u32 %v33844, %v33840
%v33848 = vadd.s32 %v33845, %v33840
%v33852 = vadd.s32 %v33848, %v8
%v33854 = vshll.u32 %v33845, 6
%v33855 = vshrl.u32 %v33845, 26
%v33856 = vor.u32 %v33855, %v33854
%v33857 = vxor.u32 %v33856, %v33848
%v33860 = vadd.s32 %v33857, %v10
%v33864 = vadd.s32 5, %v33860
%v33866 = vxor.u32 %v33864, %v33852
%v33867 = vand.u32.u8 255, %v33866
%v33868 = vand.u32 65535, %v33867
%v33869 = vshrl.u32 %v33868, 1
%v33870 = vor.u32 16256, %v33869
%v33871 = vand.u32.u16 65535, %v33870
%v119922 = vadd.low.f32.bf16 -1.0, %v33871
%v33880 = vmul.f32 2.0, %v119922
%v33884 = vadd.f32 -0.99609375, %v33880
%v33888 = vmax.f32 %v33884, -0.99609375
%v33890 = vand.u32 2147483647, %v33888
%vm33893 = vcmp.eq.f32.partialorder %v33890, 1.0
%v33898 = vmul.f32 inf, %v33888
%v33900 = vxor.u32 2147483648, %v33888
%v33903 = vmul.f32 %v33900, %v33888
%v33905 = vadd.f32 1.0, %v33903
%v33906 = vlog2.pop %v33905
%v33907 = vmul.f32 0.6931472, %v33906
%v33908 = vmul.f32 -0.5, %v33903
%v33909 = vadd.f32 1.0, %v33908
%v33910 = vmul.f32 %v33909, %v33903
%v33911 = vand.u32 2147483647, %v33903
%vm33912 = vcmp.lt.f32.partialorder %v33911, 0.0004427343
%v33913 = vsel /*vm=*/%vm33912, /*on_true_vy=*/%v33910, /*on_false_vx=*/%v33907
%v33914 = vxor.u32 2147483648, %v33913
%vm33917 = vcmp.lt.f32.partialorder %v33914, 5.0
%v33922 = vsel /*vm=*/%vm33917, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v33926 = vsel /*vm=*/%vm33917, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v33930 = vsel /*vm=*/%vm33917, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v33934 = vsel /*vm=*/%vm33917, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v33938 = vsel /*vm=*/%vm33917, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v33942 = vsel /*vm=*/%vm33917, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v33946 = vsel /*vm=*/%vm33917, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v33950 = vsel /*vm=*/%vm33917, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v33954 = vsel /*vm=*/%vm33917, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v33958 = vadd.f32 -2.5, %v33914
%v33960 = vrsqrt.pop %v33914
%v33961 = vmul.f32 %v33960, %v33914
%vm33962 = vcmp.eq.f32.partialorder %v33914, inf
%v33963 = vsel /*vm=*/%vm33962, /*on_true_vy=*/%v33914, /*on_false_vx=*/%v33961
%vm33964 = vcmp.eq.f32.partialorder %v33914, 0.0
%v33965 = vand.u32 2147483648, %v33914
%v33966 = vsel /*vm=*/%vm33964, /*on_true_vy=*/%v33965, /*on_false_vx=*/%v33963
%v33969 = vadd.f32 -3.0, %v33966
%v33973 = vsel /*vm=*/%vm33917, /*on_true_vy=*/%v33958, /*on_false_vx=*/%v33969
%v33977 = vmul.f32 %v33973, %v33954
%v33981 = vadd.f32 %v33977, %v33950
%v33985 = vmul.f32 %v33981, %v33973
%v33989 = vadd.f32 %v33985, %v33946
%v33993 = vmul.f32 %v33989, %v33973
%v33997 = vadd.f32 %v33993, %v33942
%v34001 = vmul.f32 %v33997, %v33973
%v34005 = vadd.f32 %v34001, %v33938
%v34009 = vmul.f32 %v34005, %v33973
%v34013 = vadd.f32 %v34009, %v33934
%v34017 = vmul.f32 %v34013, %v33973
%v34021 = vadd.f32 %v34017, %v33930
%v34025 = vmul.f32 %v34021, %v33973
%v34029 = vadd.f32 %v34025, %v33926
%v34033 = vmul.f32 %v34029, %v33973
%v34037 = vadd.f32 %v34033, %v33922
%v34041 = vmul.f32 %v34037, %v33888
%v34045 = vsel /*vm=*/%vm33893, /*on_true_vy=*/%v33898, /*on_false_vx=*/%v34041
%v34049 = vmul.f32 1.4140625, %v34045
%v34052 = vpack.c.bf16 %v120417, %v34049
%119923 = vst [vmem:[%s280 + $0x3a0] sm:$0xf] /*vst_source=*/%v34052
%v34090 = vadd.s32 %v34087, %v408
%v34100 = vadd.s32 %v34090, %v415
%vm34104 = vcmp.lt.u32.totalorder %v34100, %v34090
%vm34109 = vcmp.lt.u32.totalorder %v34090, %v408
%v34114 = vadd.s32 %v34070, %v380
%v34118 = vadd.s32 1, %v34114
%v34122 = vsel /*vm=*/%vm34109, /*on_true_vy=*/%v34118, /*on_false_vx=*/%v34114
%v34126 = vadd.s32 1, %v34122
%v34130 = vsel /*vm=*/%vm34104, /*on_true_vy=*/%v34126, /*on_false_vx=*/%v34122
%v34135 = vadd.s32 %v34130, %v10
%v34139 = vadd.s32 %v34100, %v9
%v34143 = vadd.s32 %v34139, %v34135
%v34145 = vshll.u32 %v34139, 13
%v34146 = vshrl.u32 %v34139, 19
%v34147 = vor.u32 %v34146, %v34145
%v34148 = vxor.u32 %v34147, %v34143
%v34151 = vadd.s32 %v34148, %v34143
%v34153 = vshll.u32 %v34148, 15
%v34154 = vshrl.u32 %v34148, 17
%v34155 = vor.u32 %v34154, %v34153
%v34156 = vxor.u32 %v34155, %v34151
%v34159 = vadd.s32 %v34156, %v34151
%v34161 = vshll.u32 %v34156, 26
%v34162 = vshrl.u32 %v34156, 6
%v34163 = vor.u32 %v34162, %v34161
%v34164 = vxor.u32 %v34163, %v34159
%v34167 = vadd.s32 %v34164, %v34159
%v34171 = vadd.s32 %v34167, %v9
%v34173 = vshll.u32 %v34164, 6
%v34174 = vshrl.u32 %v34164, 26
%v34175 = vor.u32 %v34174, %v34173
%v34176 = vxor.u32 %v34175, %v34167
%v34179 = vadd.s32 %v34176, %v8
%v34183 = vadd.s32 1, %v34179
%v34187 = vadd.s32 %v34183, %v34171
%v34189 = vshll.u32 %v34183, 17
%v34190 = vshrl.u32 %v34183, 15
%v34191 = vor.u32 %v34190, %v34189
%v34192 = vxor.u32 %v34191, %v34187
%v34195 = vadd.s32 %v34192, %v34187
%v34197 = vshll.u32 %v34192, 29
%v34198 = vshrl.u32 %v34192, 3
%v34199 = vor.u32 %v34198, %v34197
%v34200 = vxor.u32 %v34199, %v34195
%v34203 = vadd.s32 %v34200, %v34195
%v34205 = vshll.u32 %v34200, 16
%v34206 = vshrl.u32 %v34200, 16
%v34207 = vor.u32 %v34206, %v34205
%v34208 = vxor.u32 %v34207, %v34203
%v34211 = vadd.s32 %v34208, %v34203
%v34215 = vadd.s32 %v34211, %v8
%v34217 = vshll.u32 %v34208, 24
%v34218 = vshrl.u32 %v34208, 8
%v34219 = vor.u32 %v34218, %v34217
%v34220 = vxor.u32 %v34219, %v34211
%v34223 = vadd.s32 %v34220, %v10
%v34227 = vadd.s32 2, %v34223
%v34231 = vadd.s32 %v34227, %v34215
%v34233 = vshll.u32 %v34227, 13
%v34234 = vshrl.u32 %v34227, 19
%v34235 = vor.u32 %v34234, %v34233
%v34236 = vxor.u32 %v34235, %v34231
%v34239 = vadd.s32 %v34236, %v34231
%v34241 = vshll.u32 %v34236, 15
%v34242 = vshrl.u32 %v34236, 17
%v34243 = vor.u32 %v34242, %v34241
%v34244 = vxor.u32 %v34243, %v34239
%v34247 = vadd.s32 %v34244, %v34239
%v34249 = vshll.u32 %v34244, 26
%v34250 = vshrl.u32 %v34244, 6
%v34251 = vor.u32 %v34250, %v34249
%v34252 = vxor.u32 %v34251, %v34247
%v34255 = vadd.s32 %v34252, %v34247
%v34259 = vadd.s32 %v34255, %v10
%v34261 = vshll.u32 %v34252, 6
%v34262 = vshrl.u32 %v34252, 26
%v34263 = vor.u32 %v34262, %v34261
%v34264 = vxor.u32 %v34263, %v34255
%v34267 = vadd.s32 %v34264, %v9
%v34271 = vadd.s32 3, %v34267
%v34275 = vadd.s32 %v34271, %v34259
%v34277 = vshll.u32 %v34271, 17
%v34278 = vshrl.u32 %v34271, 15
%v34279 = vor.u32 %v34278, %v34277
%v34280 = vxor.u32 %v34279, %v34275
%v34283 = vadd.s32 %v34280, %v34275
%v34285 = vshll.u32 %v34280, 29
%v34286 = vshrl.u32 %v34280, 3
%v34287 = vor.u32 %v34286, %v34285
%v34288 = vxor.u32 %v34287, %v34283
%v34291 = vadd.s32 %v34288, %v34283
%v34293 = vshll.u32 %v34288, 16
%v34294 = vshrl.u32 %v34288, 16
%v34295 = vor.u32 %v34294, %v34293
%v34296 = vxor.u32 %v34295, %v34291
%v34299 = vadd.s32 %v34296, %v34291
%v34303 = vadd.s32 %v34299, %v9
%v34305 = vshll.u32 %v34296, 24
%v34306 = vshrl.u32 %v34296, 8
%v34307 = vor.u32 %v34306, %v34305
%v34308 = vxor.u32 %v34307, %v34299
%v34311 = vadd.s32 %v34308, %v8
%v34315 = vadd.s32 4, %v34311
%v34319 = vadd.s32 %v34315, %v34303
%v34321 = vshll.u32 %v34315, 13
%v34322 = vshrl.u32 %v34315, 19
%v34323 = vor.u32 %v34322, %v34321
%v34324 = vxor.u32 %v34323, %v34319
%v34327 = vadd.s32 %v34324, %v34319
%v34329 = vshll.u32 %v34324, 15
%v34330 = vshrl.u32 %v34324, 17
%v34331 = vor.u32 %v34330, %v34329
%v34332 = vxor.u32 %v34331, %v34327
%v34335 = vadd.s32 %v34332, %v34327
%v34337 = vshll.u32 %v34332, 26
%v34338 = vshrl.u32 %v34332, 6
%v34339 = vor.u32 %v34338, %v34337
%v34340 = vxor.u32 %v34339, %v34335
%v34343 = vadd.s32 %v34340, %v34335
%v34347 = vadd.s32 %v34343, %v8
%v34349 = vshll.u32 %v34340, 6
%v34350 = vshrl.u32 %v34340, 26
%v34351 = vor.u32 %v34350, %v34349
%v34352 = vxor.u32 %v34351, %v34343
%v34355 = vadd.s32 %v34352, %v10
%v34359 = vadd.s32 5, %v34355
%v34361 = vxor.u32 %v34359, %v34347
%v34362 = vand.u32.u8 255, %v34361
%v34363 = vand.u32 65535, %v34362
%v34364 = vshrl.u32 %v34363, 1
%v34365 = vor.u32 16256, %v34364
%v34366 = vand.u32.u16 65535, %v34365
%v119928 = vadd.low.f32.bf16 -1.0, %v34366
%v34375 = vmul.f32 2.0, %v119928
%v34379 = vadd.f32 -0.99609375, %v34375
%v34383 = vmax.f32 %v34379, -0.99609375
%v34385 = vand.u32 2147483647, %v34383
%vm34388 = vcmp.eq.f32.partialorder %v34385, 1.0
%v34393 = vmul.f32 inf, %v34383
%v34395 = vxor.u32 2147483648, %v34383
%v34398 = vmul.f32 %v34395, %v34383
%v34400 = vadd.f32 1.0, %v34398
%v34401 = vlog2.pop %v34400
%v34402 = vmul.f32 0.6931472, %v34401
%v34403 = vmul.f32 -0.5, %v34398
%v34404 = vadd.f32 1.0, %v34403
%v34405 = vmul.f32 %v34404, %v34398
%v34406 = vand.u32 2147483647, %v34398
%vm34407 = vcmp.lt.f32.partialorder %v34406, 0.0004427343
%v34408 = vsel /*vm=*/%vm34407, /*on_true_vy=*/%v34405, /*on_false_vx=*/%v34402
%v34409 = vxor.u32 2147483648, %v34408
%vm34412 = vcmp.lt.f32.partialorder %v34409, 5.0
%v34417 = vsel /*vm=*/%vm34412, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v34421 = vsel /*vm=*/%vm34412, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v34425 = vsel /*vm=*/%vm34412, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v34429 = vsel /*vm=*/%vm34412, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v34433 = vsel /*vm=*/%vm34412, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v34437 = vsel /*vm=*/%vm34412, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v34441 = vsel /*vm=*/%vm34412, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v34445 = vsel /*vm=*/%vm34412, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v34449 = vsel /*vm=*/%vm34412, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v34453 = vadd.f32 -2.5, %v34409
%v34455 = vrsqrt.pop %v34409
%v34456 = vmul.f32 %v34455, %v34409
%vm34457 = vcmp.eq.f32.partialorder %v34409, inf
%v34458 = vsel /*vm=*/%vm34457, /*on_true_vy=*/%v34409, /*on_false_vx=*/%v34456
%vm34459 = vcmp.eq.f32.partialorder %v34409, 0.0
%v34460 = vand.u32 2147483648, %v34409
%v34461 = vsel /*vm=*/%vm34459, /*on_true_vy=*/%v34460, /*on_false_vx=*/%v34458
%v34464 = vadd.f32 -3.0, %v34461
%v34468 = vsel /*vm=*/%vm34412, /*on_true_vy=*/%v34453, /*on_false_vx=*/%v34464
%v34472 = vmul.f32 %v34468, %v34449
%v34476 = vadd.f32 %v34472, %v34445
%v34480 = vmul.f32 %v34476, %v34468
%v34484 = vadd.f32 %v34480, %v34441
%v34488 = vmul.f32 %v34484, %v34468
%v34492 = vadd.f32 %v34488, %v34437
%v34496 = vmul.f32 %v34492, %v34468
%v34500 = vadd.f32 %v34496, %v34433
%v34504 = vmul.f32 %v34500, %v34468
%v34508 = vadd.f32 %v34504, %v34429
%v34512 = vmul.f32 %v34508, %v34468
%v34516 = vadd.f32 %v34512, %v34425
%v34520 = vmul.f32 %v34516, %v34468
%v34524 = vadd.f32 %v34520, %v34421
%v34528 = vmul.f32 %v34524, %v34468
%v34532 = vadd.f32 %v34528, %v34417
%v34536 = vmul.f32 %v34532, %v34383
%v34540 = vsel /*vm=*/%vm34388, /*on_true_vy=*/%v34393, /*on_false_vx=*/%v34536
%v34544 = vmul.f32 1.4140625, %v34540
%v34547 = vpack.c.bf16 %v120417, %v34544
%119929 = vst [vmem:[%s280 + $0x24] sm:$0xf] /*vst_source=*/%v34547
%v34551 = vadd.s32 %v34087, %v894
%v34561 = vadd.s32 %v34551, %v415
%vm34565 = vcmp.lt.u32.totalorder %v34561, %v34551
%vm34570 = vcmp.lt.u32.totalorder %v34551, %v894
%v34575 = vadd.s32 %v34070, %v881
%v34579 = vadd.s32 1, %v34575
%v34583 = vsel /*vm=*/%vm34570, /*on_true_vy=*/%v34579, /*on_false_vx=*/%v34575
%v34587 = vadd.s32 1, %v34583
%v34591 = vsel /*vm=*/%vm34565, /*on_true_vy=*/%v34587, /*on_false_vx=*/%v34583
%v34596 = vadd.s32 %v34591, %v10
%v34600 = vadd.s32 %v34561, %v9
%v34604 = vadd.s32 %v34600, %v34596
%v34606 = vshll.u32 %v34600, 13
%v34607 = vshrl.u32 %v34600, 19
%v34608 = vor.u32 %v34607, %v34606
%v34609 = vxor.u32 %v34608, %v34604
%v34612 = vadd.s32 %v34609, %v34604
%v34614 = vshll.u32 %v34609, 15
%v34615 = vshrl.u32 %v34609, 17
%v34616 = vor.u32 %v34615, %v34614
%v34617 = vxor.u32 %v34616, %v34612
%v34620 = vadd.s32 %v34617, %v34612
%v34622 = vshll.u32 %v34617, 26
%v34623 = vshrl.u32 %v34617, 6
%v34624 = vor.u32 %v34623, %v34622
%v34625 = vxor.u32 %v34624, %v34620
%v34628 = vadd.s32 %v34625, %v34620
%v34632 = vadd.s32 %v34628, %v9
%v34634 = vshll.u32 %v34625, 6
%v34635 = vshrl.u32 %v34625, 26
%v34636 = vor.u32 %v34635, %v34634
%v34637 = vxor.u32 %v34636, %v34628
%v34640 = vadd.s32 %v34637, %v8
%v34644 = vadd.s32 1, %v34640
%v34648 = vadd.s32 %v34644, %v34632
%v34650 = vshll.u32 %v34644, 17
%v34651 = vshrl.u32 %v34644, 15
%v34652 = vor.u32 %v34651, %v34650
%v34653 = vxor.u32 %v34652, %v34648
%v34656 = vadd.s32 %v34653, %v34648
%v34658 = vshll.u32 %v34653, 29
%v34659 = vshrl.u32 %v34653, 3
%v34660 = vor.u32 %v34659, %v34658
%v34661 = vxor.u32 %v34660, %v34656
%v34664 = vadd.s32 %v34661, %v34656
%v34666 = vshll.u32 %v34661, 16
%v34667 = vshrl.u32 %v34661, 16
%v34668 = vor.u32 %v34667, %v34666
%v34669 = vxor.u32 %v34668, %v34664
%v34672 = vadd.s32 %v34669, %v34664
%v34676 = vadd.s32 %v34672, %v8
%v34678 = vshll.u32 %v34669, 24
%v34679 = vshrl.u32 %v34669, 8
%v34680 = vor.u32 %v34679, %v34678
%v34681 = vxor.u32 %v34680, %v34672
%v34684 = vadd.s32 %v34681, %v10
%v34688 = vadd.s32 2, %v34684
%v34692 = vadd.s32 %v34688, %v34676
%v34694 = vshll.u32 %v34688, 13
%v34695 = vshrl.u32 %v34688, 19
%v34696 = vor.u32 %v34695, %v34694
%v34697 = vxor.u32 %v34696, %v34692
%v34700 = vadd.s32 %v34697, %v34692
%v34702 = vshll.u32 %v34697, 15
%v34703 = vshrl.u32 %v34697, 17
%v34704 = vor.u32 %v34703, %v34702
%v34705 = vxor.u32 %v34704, %v34700
%v34708 = vadd.s32 %v34705, %v34700
%v34710 = vshll.u32 %v34705, 26
%v34711 = vshrl.u32 %v34705, 6
%v34712 = vor.u32 %v34711, %v34710
%v34713 = vxor.u32 %v34712, %v34708
%v34716 = vadd.s32 %v34713, %v34708
%v34720 = vadd.s32 %v34716, %v10
%v34722 = vshll.u32 %v34713, 6
%v34723 = vshrl.u32 %v34713, 26
%v34724 = vor.u32 %v34723, %v34722
%v34725 = vxor.u32 %v34724, %v34716
%v34728 = vadd.s32 %v34725, %v9
%v34732 = vadd.s32 3, %v34728
%v34736 = vadd.s32 %v34732, %v34720
%v34738 = vshll.u32 %v34732, 17
%v34739 = vshrl.u32 %v34732, 15
%v34740 = vor.u32 %v34739, %v34738
%v34741 = vxor.u32 %v34740, %v34736
%v34744 = vadd.s32 %v34741, %v34736
%v34746 = vshll.u32 %v34741, 29
%v34747 = vshrl.u32 %v34741, 3
%v34748 = vor.u32 %v34747, %v34746
%v34749 = vxor.u32 %v34748, %v34744
%v34752 = vadd.s32 %v34749, %v34744
%v34754 = vshll.u32 %v34749, 16
%v34755 = vshrl.u32 %v34749, 16
%v34756 = vor.u32 %v34755, %v34754
%v34757 = vxor.u32 %v34756, %v34752
%v34760 = vadd.s32 %v34757, %v34752
%v34764 = vadd.s32 %v34760, %v9
%v34766 = vshll.u32 %v34757, 24
%v34767 = vshrl.u32 %v34757, 8
%v34768 = vor.u32 %v34767, %v34766
%v34769 = vxor.u32 %v34768, %v34760
%v34772 = vadd.s32 %v34769, %v8
%v34776 = vadd.s32 4, %v34772
%v34780 = vadd.s32 %v34776, %v34764
%v34782 = vshll.u32 %v34776, 13
%v34783 = vshrl.u32 %v34776, 19
%v34784 = vor.u32 %v34783, %v34782
%v34785 = vxor.u32 %v34784, %v34780
%v34788 = vadd.s32 %v34785, %v34780
%v34790 = vshll.u32 %v34785, 15
%v34791 = vshrl.u32 %v34785, 17
%v34792 = vor.u32 %v34791, %v34790
%v34793 = vxor.u32 %v34792, %v34788
%v34796 = vadd.s32 %v34793, %v34788
%v34798 = vshll.u32 %v34793, 26
%v34799 = vshrl.u32 %v34793, 6
%v34800 = vor.u32 %v34799, %v34798
%v34801 = vxor.u32 %v34800, %v34796
%v34804 = vadd.s32 %v34801, %v34796
%v34808 = vadd.s32 %v34804, %v8
%v34810 = vshll.u32 %v34801, 6
%v34811 = vshrl.u32 %v34801, 26
%v34812 = vor.u32 %v34811, %v34810
%v34813 = vxor.u32 %v34812, %v34804
%v34816 = vadd.s32 %v34813, %v10
%v34820 = vadd.s32 5, %v34816
%v34822 = vxor.u32 %v34820, %v34808
%v34823 = vand.u32.u8 255, %v34822
%v34824 = vand.u32 65535, %v34823
%v34825 = vshrl.u32 %v34824, 1
%v34826 = vor.u32 16256, %v34825
%v34827 = vand.u32.u16 65535, %v34826
%v119930 = vadd.low.f32.bf16 -1.0, %v34827
%v34836 = vmul.f32 2.0, %v119930
%v34840 = vadd.f32 -0.99609375, %v34836
%v34844 = vmax.f32 %v34840, -0.99609375
%v34846 = vand.u32 2147483647, %v34844
%vm34849 = vcmp.eq.f32.partialorder %v34846, 1.0
%v34854 = vmul.f32 inf, %v34844
%v34856 = vxor.u32 2147483648, %v34844
%v34859 = vmul.f32 %v34856, %v34844
%v34861 = vadd.f32 1.0, %v34859
%v34862 = vlog2.pop %v34861
%v34863 = vmul.f32 0.6931472, %v34862
%v34864 = vmul.f32 -0.5, %v34859
%v34865 = vadd.f32 1.0, %v34864
%v34866 = vmul.f32 %v34865, %v34859
%v34867 = vand.u32 2147483647, %v34859
%vm34868 = vcmp.lt.f32.partialorder %v34867, 0.0004427343
%v34869 = vsel /*vm=*/%vm34868, /*on_true_vy=*/%v34866, /*on_false_vx=*/%v34863
%v34870 = vxor.u32 2147483648, %v34869
%vm34873 = vcmp.lt.f32.partialorder %v34870, 5.0
%v34878 = vsel /*vm=*/%vm34873, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v34882 = vsel /*vm=*/%vm34873, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v34886 = vsel /*vm=*/%vm34873, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v34890 = vsel /*vm=*/%vm34873, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v34894 = vsel /*vm=*/%vm34873, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v34898 = vsel /*vm=*/%vm34873, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v34902 = vsel /*vm=*/%vm34873, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v34906 = vsel /*vm=*/%vm34873, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v34910 = vsel /*vm=*/%vm34873, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v34914 = vadd.f32 -2.5, %v34870
%v34916 = vrsqrt.pop %v34870
%v34917 = vmul.f32 %v34916, %v34870
%vm34918 = vcmp.eq.f32.partialorder %v34870, inf
%v34919 = vsel /*vm=*/%vm34918, /*on_true_vy=*/%v34870, /*on_false_vx=*/%v34917
%vm34920 = vcmp.eq.f32.partialorder %v34870, 0.0
%v34921 = vand.u32 2147483648, %v34870
%v34922 = vsel /*vm=*/%vm34920, /*on_true_vy=*/%v34921, /*on_false_vx=*/%v34919
%v34925 = vadd.f32 -3.0, %v34922
%v34929 = vsel /*vm=*/%vm34873, /*on_true_vy=*/%v34914, /*on_false_vx=*/%v34925
%v34933 = vmul.f32 %v34929, %v34910
%v34937 = vadd.f32 %v34933, %v34906
%v34941 = vmul.f32 %v34937, %v34929
%v34945 = vadd.f32 %v34941, %v34902
%v34949 = vmul.f32 %v34945, %v34929
%v34953 = vadd.f32 %v34949, %v34898
%v34957 = vmul.f32 %v34953, %v34929
%v34961 = vadd.f32 %v34957, %v34894
%v34965 = vmul.f32 %v34961, %v34929
%v34969 = vadd.f32 %v34965, %v34890
%v34973 = vmul.f32 %v34969, %v34929
%v34977 = vadd.f32 %v34973, %v34886
%v34981 = vmul.f32 %v34977, %v34929
%v34985 = vadd.f32 %v34981, %v34882
%v34989 = vmul.f32 %v34985, %v34929
%v34993 = vadd.f32 %v34989, %v34878
%v34997 = vmul.f32 %v34993, %v34844
%v35001 = vsel /*vm=*/%vm34849, /*on_true_vy=*/%v34854, /*on_false_vx=*/%v34997
%v35005 = vmul.f32 1.4140625, %v35001
%v35008 = vpack.c.bf16 %v120417, %v35005
%119931 = vst [vmem:[%s280 + $0xa4] sm:$0xf] /*vst_source=*/%v35008
%v35012 = vadd.s32 %v34087, %v1381
%v35022 = vadd.s32 %v35012, %v415
%vm35026 = vcmp.lt.u32.totalorder %v35022, %v35012
%vm35031 = vcmp.lt.u32.totalorder %v35012, %v1381
%v35036 = vadd.s32 %v34070, %v1368
%v35040 = vadd.s32 1, %v35036
%v35044 = vsel /*vm=*/%vm35031, /*on_true_vy=*/%v35040, /*on_false_vx=*/%v35036
%v35048 = vadd.s32 1, %v35044
%v35052 = vsel /*vm=*/%vm35026, /*on_true_vy=*/%v35048, /*on_false_vx=*/%v35044
%v35057 = vadd.s32 %v35052, %v10
%v35061 = vadd.s32 %v35022, %v9
%v35065 = vadd.s32 %v35061, %v35057
%v35067 = vshll.u32 %v35061, 13
%v35068 = vshrl.u32 %v35061, 19
%v35069 = vor.u32 %v35068, %v35067
%v35070 = vxor.u32 %v35069, %v35065
%v35073 = vadd.s32 %v35070, %v35065
%v35075 = vshll.u32 %v35070, 15
%v35076 = vshrl.u32 %v35070, 17
%v35077 = vor.u32 %v35076, %v35075
%v35078 = vxor.u32 %v35077, %v35073
%v35081 = vadd.s32 %v35078, %v35073
%v35083 = vshll.u32 %v35078, 26
%v35084 = vshrl.u32 %v35078, 6
%v35085 = vor.u32 %v35084, %v35083
%v35086 = vxor.u32 %v35085, %v35081
%v35089 = vadd.s32 %v35086, %v35081
%v35093 = vadd.s32 %v35089, %v9
%v35095 = vshll.u32 %v35086, 6
%v35096 = vshrl.u32 %v35086, 26
%v35097 = vor.u32 %v35096, %v35095
%v35098 = vxor.u32 %v35097, %v35089
%v35101 = vadd.s32 %v35098, %v8
%v35105 = vadd.s32 1, %v35101
%v35109 = vadd.s32 %v35105, %v35093
%v35111 = vshll.u32 %v35105, 17
%v35112 = vshrl.u32 %v35105, 15
%v35113 = vor.u32 %v35112, %v35111
%v35114 = vxor.u32 %v35113, %v35109
%v35117 = vadd.s32 %v35114, %v35109
%v35119 = vshll.u32 %v35114, 29
%v35120 = vshrl.u32 %v35114, 3
%v35121 = vor.u32 %v35120, %v35119
%v35122 = vxor.u32 %v35121, %v35117
%v35125 = vadd.s32 %v35122, %v35117
%v35127 = vshll.u32 %v35122, 16
%v35128 = vshrl.u32 %v35122, 16
%v35129 = vor.u32 %v35128, %v35127
%v35130 = vxor.u32 %v35129, %v35125
%v35133 = vadd.s32 %v35130, %v35125
%v35137 = vadd.s32 %v35133, %v8
%v35139 = vshll.u32 %v35130, 24
%v35140 = vshrl.u32 %v35130, 8
%v35141 = vor.u32 %v35140, %v35139
%v35142 = vxor.u32 %v35141, %v35133
%v35145 = vadd.s32 %v35142, %v10
%v35149 = vadd.s32 2, %v35145
%v35153 = vadd.s32 %v35149, %v35137
%v35155 = vshll.u32 %v35149, 13
%v35156 = vshrl.u32 %v35149, 19
%v35157 = vor.u32 %v35156, %v35155
%v35158 = vxor.u32 %v35157, %v35153
%v35161 = vadd.s32 %v35158, %v35153
%v35163 = vshll.u32 %v35158, 15
%v35164 = vshrl.u32 %v35158, 17
%v35165 = vor.u32 %v35164, %v35163
%v35166 = vxor.u32 %v35165, %v35161
%v35169 = vadd.s32 %v35166, %v35161
%v35171 = vshll.u32 %v35166, 26
%v35172 = vshrl.u32 %v35166, 6
%v35173 = vor.u32 %v35172, %v35171
%v35174 = vxor.u32 %v35173, %v35169
%v35177 = vadd.s32 %v35174, %v35169
%v35181 = vadd.s32 %v35177, %v10
%v35183 = vshll.u32 %v35174, 6
%v35184 = vshrl.u32 %v35174, 26
%v35185 = vor.u32 %v35184, %v35183
%v35186 = vxor.u32 %v35185, %v35177
%v35189 = vadd.s32 %v35186, %v9
%v35193 = vadd.s32 3, %v35189
%v35197 = vadd.s32 %v35193, %v35181
%v35199 = vshll.u32 %v35193, 17
%v35200 = vshrl.u32 %v35193, 15
%v35201 = vor.u32 %v35200, %v35199
%v35202 = vxor.u32 %v35201, %v35197
%v35205 = vadd.s32 %v35202, %v35197
%v35207 = vshll.u32 %v35202, 29
%v35208 = vshrl.u32 %v35202, 3
%v35209 = vor.u32 %v35208, %v35207
%v35210 = vxor.u32 %v35209, %v35205
%v35213 = vadd.s32 %v35210, %v35205
%v35215 = vshll.u32 %v35210, 16
%v35216 = vshrl.u32 %v35210, 16
%v35217 = vor.u32 %v35216, %v35215
%v35218 = vxor.u32 %v35217, %v35213
%v35221 = vadd.s32 %v35218, %v35213
%v35225 = vadd.s32 %v35221, %v9
%v35227 = vshll.u32 %v35218, 24
%v35228 = vshrl.u32 %v35218, 8
%v35229 = vor.u32 %v35228, %v35227
%v35230 = vxor.u32 %v35229, %v35221
%v35233 = vadd.s32 %v35230, %v8
%v35237 = vadd.s32 4, %v35233
%v35241 = vadd.s32 %v35237, %v35225
%v35243 = vshll.u32 %v35237, 13
%v35244 = vshrl.u32 %v35237, 19
%v35245 = vor.u32 %v35244, %v35243
%v35246 = vxor.u32 %v35245, %v35241
%v35249 = vadd.s32 %v35246, %v35241
%v35251 = vshll.u32 %v35246, 15
%v35252 = vshrl.u32 %v35246, 17
%v35253 = vor.u32 %v35252, %v35251
%v35254 = vxor.u32 %v35253, %v35249
%v35257 = vadd.s32 %v35254, %v35249
%v35259 = vshll.u32 %v35254, 26
%v35260 = vshrl.u32 %v35254, 6
%v35261 = vor.u32 %v35260, %v35259
%v35262 = vxor.u32 %v35261, %v35257
%v35265 = vadd.s32 %v35262, %v35257
%v35269 = vadd.s32 %v35265, %v8
%v35271 = vshll.u32 %v35262, 6
%v35272 = vshrl.u32 %v35262, 26
%v35273 = vor.u32 %v35272, %v35271
%v35274 = vxor.u32 %v35273, %v35265
%v35277 = vadd.s32 %v35274, %v10
%v35281 = vadd.s32 5, %v35277
%v35283 = vxor.u32 %v35281, %v35269
%v35284 = vand.u32.u8 255, %v35283
%v35285 = vand.u32 65535, %v35284
%v35286 = vshrl.u32 %v35285, 1
%v35287 = vor.u32 16256, %v35286
%v35288 = vand.u32.u16 65535, %v35287
%v119932 = vadd.low.f32.bf16 -1.0, %v35288
%v35297 = vmul.f32 2.0, %v119932
%v35301 = vadd.f32 -0.99609375, %v35297
%v35305 = vmax.f32 %v35301, -0.99609375
%v35307 = vand.u32 2147483647, %v35305
%vm35310 = vcmp.eq.f32.partialorder %v35307, 1.0
%v35315 = vmul.f32 inf, %v35305
%v35317 = vxor.u32 2147483648, %v35305
%v35320 = vmul.f32 %v35317, %v35305
%v35322 = vadd.f32 1.0, %v35320
%v35323 = vlog2.pop %v35322
%v35324 = vmul.f32 0.6931472, %v35323
%v35325 = vmul.f32 -0.5, %v35320
%v35326 = vadd.f32 1.0, %v35325
%v35327 = vmul.f32 %v35326, %v35320
%v35328 = vand.u32 2147483647, %v35320
%vm35329 = vcmp.lt.f32.partialorder %v35328, 0.0004427343
%v35330 = vsel /*vm=*/%vm35329, /*on_true_vy=*/%v35327, /*on_false_vx=*/%v35324
%v35331 = vxor.u32 2147483648, %v35330
%vm35334 = vcmp.lt.f32.partialorder %v35331, 5.0
%v35339 = vsel /*vm=*/%vm35334, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v35343 = vsel /*vm=*/%vm35334, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v35347 = vsel /*vm=*/%vm35334, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v35351 = vsel /*vm=*/%vm35334, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v35355 = vsel /*vm=*/%vm35334, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v35359 = vsel /*vm=*/%vm35334, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v35363 = vsel /*vm=*/%vm35334, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v35367 = vsel /*vm=*/%vm35334, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v35371 = vsel /*vm=*/%vm35334, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v35375 = vadd.f32 -2.5, %v35331
%v35377 = vrsqrt.pop %v35331
%v35378 = vmul.f32 %v35377, %v35331
%vm35379 = vcmp.eq.f32.partialorder %v35331, inf
%v35380 = vsel /*vm=*/%vm35379, /*on_true_vy=*/%v35331, /*on_false_vx=*/%v35378
%vm35381 = vcmp.eq.f32.partialorder %v35331, 0.0
%v35382 = vand.u32 2147483648, %v35331
%v35383 = vsel /*vm=*/%vm35381, /*on_true_vy=*/%v35382, /*on_false_vx=*/%v35380
%v35386 = vadd.f32 -3.0, %v35383
%v35390 = vsel /*vm=*/%vm35334, /*on_true_vy=*/%v35375, /*on_false_vx=*/%v35386
%v35394 = vmul.f32 %v35390, %v35371
%v35398 = vadd.f32 %v35394, %v35367
%v35402 = vmul.f32 %v35398, %v35390
%v35406 = vadd.f32 %v35402, %v35363
%v35410 = vmul.f32 %v35406, %v35390
%v35414 = vadd.f32 %v35410, %v35359
%v35418 = vmul.f32 %v35414, %v35390
%v35422 = vadd.f32 %v35418, %v35355
%v35426 = vmul.f32 %v35422, %v35390
%v35430 = vadd.f32 %v35426, %v35351
%v35434 = vmul.f32 %v35430, %v35390
%v35438 = vadd.f32 %v35434, %v35347
%v35442 = vmul.f32 %v35438, %v35390
%v35446 = vadd.f32 %v35442, %v35343
%v35450 = vmul.f32 %v35446, %v35390
%v35454 = vadd.f32 %v35450, %v35339
%v35458 = vmul.f32 %v35454, %v35305
%v35462 = vsel /*vm=*/%vm35310, /*on_true_vy=*/%v35315, /*on_false_vx=*/%v35458
%v35466 = vmul.f32 1.4140625, %v35462
%v35469 = vpack.c.bf16 %v120417, %v35466
%119933 = vst [vmem:[%s280 + $0x124] sm:$0xf] /*vst_source=*/%v35469
%v35473 = vadd.s32 %v34087, %v1868
%v35483 = vadd.s32 %v35473, %v415
%vm35487 = vcmp.lt.u32.totalorder %v35483, %v35473
%vm35492 = vcmp.lt.u32.totalorder %v35473, %v1868
%v35497 = vadd.s32 %v34070, %v1855
%v35501 = vadd.s32 1, %v35497
%v35505 = vsel /*vm=*/%vm35492, /*on_true_vy=*/%v35501, /*on_false_vx=*/%v35497
%v35509 = vadd.s32 1, %v35505
%v35513 = vsel /*vm=*/%vm35487, /*on_true_vy=*/%v35509, /*on_false_vx=*/%v35505
%v35518 = vadd.s32 %v35513, %v10
%v35522 = vadd.s32 %v35483, %v9
%v35526 = vadd.s32 %v35522, %v35518
%v35528 = vshll.u32 %v35522, 13
%v35529 = vshrl.u32 %v35522, 19
%v35530 = vor.u32 %v35529, %v35528
%v35531 = vxor.u32 %v35530, %v35526
%v35534 = vadd.s32 %v35531, %v35526
%v35536 = vshll.u32 %v35531, 15
%v35537 = vshrl.u32 %v35531, 17
%v35538 = vor.u32 %v35537, %v35536
%v35539 = vxor.u32 %v35538, %v35534
%v35542 = vadd.s32 %v35539, %v35534
%v35544 = vshll.u32 %v35539, 26
%v35545 = vshrl.u32 %v35539, 6
%v35546 = vor.u32 %v35545, %v35544
%v35547 = vxor.u32 %v35546, %v35542
%v35550 = vadd.s32 %v35547, %v35542
%v35554 = vadd.s32 %v35550, %v9
%v35556 = vshll.u32 %v35547, 6
%v35557 = vshrl.u32 %v35547, 26
%v35558 = vor.u32 %v35557, %v35556
%v35559 = vxor.u32 %v35558, %v35550
%v35562 = vadd.s32 %v35559, %v8
%v35566 = vadd.s32 1, %v35562
%v35570 = vadd.s32 %v35566, %v35554
%v35572 = vshll.u32 %v35566, 17
%v35573 = vshrl.u32 %v35566, 15
%v35574 = vor.u32 %v35573, %v35572
%v35575 = vxor.u32 %v35574, %v35570
%v35578 = vadd.s32 %v35575, %v35570
%v35580 = vshll.u32 %v35575, 29
%v35581 = vshrl.u32 %v35575, 3
%v35582 = vor.u32 %v35581, %v35580
%v35583 = vxor.u32 %v35582, %v35578
%v35586 = vadd.s32 %v35583, %v35578
%v35588 = vshll.u32 %v35583, 16
%v35589 = vshrl.u32 %v35583, 16
%v35590 = vor.u32 %v35589, %v35588
%v35591 = vxor.u32 %v35590, %v35586
%v35594 = vadd.s32 %v35591, %v35586
%v35598 = vadd.s32 %v35594, %v8
%v35600 = vshll.u32 %v35591, 24
%v35601 = vshrl.u32 %v35591, 8
%v35602 = vor.u32 %v35601, %v35600
%v35603 = vxor.u32 %v35602, %v35594
%v35606 = vadd.s32 %v35603, %v10
%v35610 = vadd.s32 2, %v35606
%v35614 = vadd.s32 %v35610, %v35598
%v35616 = vshll.u32 %v35610, 13
%v35617 = vshrl.u32 %v35610, 19
%v35618 = vor.u32 %v35617, %v35616
%v35619 = vxor.u32 %v35618, %v35614
%v35622 = vadd.s32 %v35619, %v35614
%v35624 = vshll.u32 %v35619, 15
%v35625 = vshrl.u32 %v35619, 17
%v35626 = vor.u32 %v35625, %v35624
%v35627 = vxor.u32 %v35626, %v35622
%v35630 = vadd.s32 %v35627, %v35622
%v35632 = vshll.u32 %v35627, 26
%v35633 = vshrl.u32 %v35627, 6
%v35634 = vor.u32 %v35633, %v35632
%v35635 = vxor.u32 %v35634, %v35630
%v35638 = vadd.s32 %v35635, %v35630
%v35642 = vadd.s32 %v35638, %v10
%v35644 = vshll.u32 %v35635, 6
%v35645 = vshrl.u32 %v35635, 26
%v35646 = vor.u32 %v35645, %v35644
%v35647 = vxor.u32 %v35646, %v35638
%v35650 = vadd.s32 %v35647, %v9
%v35654 = vadd.s32 3, %v35650
%v35658 = vadd.s32 %v35654, %v35642
%v35660 = vshll.u32 %v35654, 17
%v35661 = vshrl.u32 %v35654, 15
%v35662 = vor.u32 %v35661, %v35660
%v35663 = vxor.u32 %v35662, %v35658
%v35666 = vadd.s32 %v35663, %v35658
%v35668 = vshll.u32 %v35663, 29
%v35669 = vshrl.u32 %v35663, 3
%v35670 = vor.u32 %v35669, %v35668
%v35671 = vxor.u32 %v35670, %v35666
%v35674 = vadd.s32 %v35671, %v35666
%v35676 = vshll.u32 %v35671, 16
%v35677 = vshrl.u32 %v35671, 16
%v35678 = vor.u32 %v35677, %v35676
%v35679 = vxor.u32 %v35678, %v35674
%v35682 = vadd.s32 %v35679, %v35674
%v35686 = vadd.s32 %v35682, %v9
%v35688 = vshll.u32 %v35679, 24
%v35689 = vshrl.u32 %v35679, 8
%v35690 = vor.u32 %v35689, %v35688
%v35691 = vxor.u32 %v35690, %v35682
%v35694 = vadd.s32 %v35691, %v8
%v35698 = vadd.s32 4, %v35694
%v35702 = vadd.s32 %v35698, %v35686
%v35704 = vshll.u32 %v35698, 13
%v35705 = vshrl.u32 %v35698, 19
%v35706 = vor.u32 %v35705, %v35704
%v35707 = vxor.u32 %v35706, %v35702
%v35710 = vadd.s32 %v35707, %v35702
%v35712 = vshll.u32 %v35707, 15
%v35713 = vshrl.u32 %v35707, 17
%v35714 = vor.u32 %v35713, %v35712
%v35715 = vxor.u32 %v35714, %v35710
%v35718 = vadd.s32 %v35715, %v35710
%v35720 = vshll.u32 %v35715, 26
%v35721 = vshrl.u32 %v35715, 6
%v35722 = vor.u32 %v35721, %v35720
%v35723 = vxor.u32 %v35722, %v35718
%v35726 = vadd.s32 %v35723, %v35718
%v35730 = vadd.s32 %v35726, %v8
%v35732 = vshll.u32 %v35723, 6
%v35733 = vshrl.u32 %v35723, 26
%v35734 = vor.u32 %v35733, %v35732
%v35735 = vxor.u32 %v35734, %v35726
%v35738 = vadd.s32 %v35735, %v10
%v35742 = vadd.s32 5, %v35738
%v35744 = vxor.u32 %v35742, %v35730
%v35745 = vand.u32.u8 255, %v35744
%v35746 = vand.u32 65535, %v35745
%v35747 = vshrl.u32 %v35746, 1
%v35748 = vor.u32 16256, %v35747
%v35749 = vand.u32.u16 65535, %v35748
%v119934 = vadd.low.f32.bf16 -1.0, %v35749
%v35758 = vmul.f32 2.0, %v119934
%v35762 = vadd.f32 -0.99609375, %v35758
%v35766 = vmax.f32 %v35762, -0.99609375
%v35768 = vand.u32 2147483647, %v35766
%vm35771 = vcmp.eq.f32.partialorder %v35768, 1.0
%v35776 = vmul.f32 inf, %v35766
%v35778 = vxor.u32 2147483648, %v35766
%v35781 = vmul.f32 %v35778, %v35766
%v35783 = vadd.f32 1.0, %v35781
%v35784 = vlog2.pop %v35783
%v35785 = vmul.f32 0.6931472, %v35784
%v35786 = vmul.f32 -0.5, %v35781
%v35787 = vadd.f32 1.0, %v35786
%v35788 = vmul.f32 %v35787, %v35781
%v35789 = vand.u32 2147483647, %v35781
%vm35790 = vcmp.lt.f32.partialorder %v35789, 0.0004427343
%v35791 = vsel /*vm=*/%vm35790, /*on_true_vy=*/%v35788, /*on_false_vx=*/%v35785
%v35792 = vxor.u32 2147483648, %v35791
%vm35795 = vcmp.lt.f32.partialorder %v35792, 5.0
%v35800 = vsel /*vm=*/%vm35795, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v35804 = vsel /*vm=*/%vm35795, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v35808 = vsel /*vm=*/%vm35795, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v35812 = vsel /*vm=*/%vm35795, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v35816 = vsel /*vm=*/%vm35795, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v35820 = vsel /*vm=*/%vm35795, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v35824 = vsel /*vm=*/%vm35795, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v35828 = vsel /*vm=*/%vm35795, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v35832 = vsel /*vm=*/%vm35795, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v35836 = vadd.f32 -2.5, %v35792
%v35838 = vrsqrt.pop %v35792
%v35839 = vmul.f32 %v35838, %v35792
%vm35840 = vcmp.eq.f32.partialorder %v35792, inf
%v35841 = vsel /*vm=*/%vm35840, /*on_true_vy=*/%v35792, /*on_false_vx=*/%v35839
%vm35842 = vcmp.eq.f32.partialorder %v35792, 0.0
%v35843 = vand.u32 2147483648, %v35792
%v35844 = vsel /*vm=*/%vm35842, /*on_true_vy=*/%v35843, /*on_false_vx=*/%v35841
%v35847 = vadd.f32 -3.0, %v35844
%v35851 = vsel /*vm=*/%vm35795, /*on_true_vy=*/%v35836, /*on_false_vx=*/%v35847
%v35855 = vmul.f32 %v35851, %v35832
%v35859 = vadd.f32 %v35855, %v35828
%v35863 = vmul.f32 %v35859, %v35851
%v35867 = vadd.f32 %v35863, %v35824
%v35871 = vmul.f32 %v35867, %v35851
%v35875 = vadd.f32 %v35871, %v35820
%v35879 = vmul.f32 %v35875, %v35851
%v35883 = vadd.f32 %v35879, %v35816
%v35887 = vmul.f32 %v35883, %v35851
%v35891 = vadd.f32 %v35887, %v35812
%v35895 = vmul.f32 %v35891, %v35851
%v35899 = vadd.f32 %v35895, %v35808
%v35903 = vmul.f32 %v35899, %v35851
%v35907 = vadd.f32 %v35903, %v35804
%v35911 = vmul.f32 %v35907, %v35851
%v35915 = vadd.f32 %v35911, %v35800
%v35919 = vmul.f32 %v35915, %v35766
%v35923 = vsel /*vm=*/%vm35771, /*on_true_vy=*/%v35776, /*on_false_vx=*/%v35919
%v35927 = vmul.f32 1.4140625, %v35923
%v35930 = vpack.c.bf16 %v120417, %v35927
%119935 = vst [vmem:[%s280 + $0x1a4] sm:$0xf] /*vst_source=*/%v35930
%v35934 = vadd.s32 %v34087, %v2355
%v35944 = vadd.s32 %v35934, %v415
%vm35948 = vcmp.lt.u32.totalorder %v35944, %v35934
%vm35953 = vcmp.lt.u32.totalorder %v35934, %v2355
%v35958 = vadd.s32 %v34070, %v2342
%v35962 = vadd.s32 1, %v35958
%v35966 = vsel /*vm=*/%vm35953, /*on_true_vy=*/%v35962, /*on_false_vx=*/%v35958
%v35970 = vadd.s32 1, %v35966
%v35974 = vsel /*vm=*/%vm35948, /*on_true_vy=*/%v35970, /*on_false_vx=*/%v35966
%v35979 = vadd.s32 %v35974, %v10
%v35983 = vadd.s32 %v35944, %v9
%v35987 = vadd.s32 %v35983, %v35979
%v35989 = vshll.u32 %v35983, 13
%v35990 = vshrl.u32 %v35983, 19
%v35991 = vor.u32 %v35990, %v35989
%v35992 = vxor.u32 %v35991, %v35987
%v35995 = vadd.s32 %v35992, %v35987
%v35997 = vshll.u32 %v35992, 15
%v35998 = vshrl.u32 %v35992, 17
%v35999 = vor.u32 %v35998, %v35997
%v36000 = vxor.u32 %v35999, %v35995
%v36003 = vadd.s32 %v36000, %v35995
%v36005 = vshll.u32 %v36000, 26
%v36006 = vshrl.u32 %v36000, 6
%v36007 = vor.u32 %v36006, %v36005
%v36008 = vxor.u32 %v36007, %v36003
%v36011 = vadd.s32 %v36008, %v36003
%v36015 = vadd.s32 %v36011, %v9
%v36017 = vshll.u32 %v36008, 6
%v36018 = vshrl.u32 %v36008, 26
%v36019 = vor.u32 %v36018, %v36017
%v36020 = vxor.u32 %v36019, %v36011
%v36023 = vadd.s32 %v36020, %v8
%v36027 = vadd.s32 1, %v36023
%v36031 = vadd.s32 %v36027, %v36015
%v36033 = vshll.u32 %v36027, 17
%v36034 = vshrl.u32 %v36027, 15
%v36035 = vor.u32 %v36034, %v36033
%v36036 = vxor.u32 %v36035, %v36031
%v36039 = vadd.s32 %v36036, %v36031
%v36041 = vshll.u32 %v36036, 29
%v36042 = vshrl.u32 %v36036, 3
%v36043 = vor.u32 %v36042, %v36041
%v36044 = vxor.u32 %v36043, %v36039
%v36047 = vadd.s32 %v36044, %v36039
%v36049 = vshll.u32 %v36044, 16
%v36050 = vshrl.u32 %v36044, 16
%v36051 = vor.u32 %v36050, %v36049
%v36052 = vxor.u32 %v36051, %v36047
%v36055 = vadd.s32 %v36052, %v36047
%v36059 = vadd.s32 %v36055, %v8
%v36061 = vshll.u32 %v36052, 24
%v36062 = vshrl.u32 %v36052, 8
%v36063 = vor.u32 %v36062, %v36061
%v36064 = vxor.u32 %v36063, %v36055
%v36067 = vadd.s32 %v36064, %v10
%v36071 = vadd.s32 2, %v36067
%v36075 = vadd.s32 %v36071, %v36059
%v36077 = vshll.u32 %v36071, 13
%v36078 = vshrl.u32 %v36071, 19
%v36079 = vor.u32 %v36078, %v36077
%v36080 = vxor.u32 %v36079, %v36075
%v36083 = vadd.s32 %v36080, %v36075
%v36085 = vshll.u32 %v36080, 15
%v36086 = vshrl.u32 %v36080, 17
%v36087 = vor.u32 %v36086, %v36085
%v36088 = vxor.u32 %v36087, %v36083
%v36091 = vadd.s32 %v36088, %v36083
%v36093 = vshll.u32 %v36088, 26
%v36094 = vshrl.u32 %v36088, 6
%v36095 = vor.u32 %v36094, %v36093
%v36096 = vxor.u32 %v36095, %v36091
%v36099 = vadd.s32 %v36096, %v36091
%v36103 = vadd.s32 %v36099, %v10
%v36105 = vshll.u32 %v36096, 6
%v36106 = vshrl.u32 %v36096, 26
%v36107 = vor.u32 %v36106, %v36105
%v36108 = vxor.u32 %v36107, %v36099
%v36111 = vadd.s32 %v36108, %v9
%v36115 = vadd.s32 3, %v36111
%v36119 = vadd.s32 %v36115, %v36103
%v36121 = vshll.u32 %v36115, 17
%v36122 = vshrl.u32 %v36115, 15
%v36123 = vor.u32 %v36122, %v36121
%v36124 = vxor.u32 %v36123, %v36119
%v36127 = vadd.s32 %v36124, %v36119
%v36129 = vshll.u32 %v36124, 29
%v36130 = vshrl.u32 %v36124, 3
%v36131 = vor.u32 %v36130, %v36129
%v36132 = vxor.u32 %v36131, %v36127
%v36135 = vadd.s32 %v36132, %v36127
%v36137 = vshll.u32 %v36132, 16
%v36138 = vshrl.u32 %v36132, 16
%v36139 = vor.u32 %v36138, %v36137
%v36140 = vxor.u32 %v36139, %v36135
%v36143 = vadd.s32 %v36140, %v36135
%v36147 = vadd.s32 %v36143, %v9
%v36149 = vshll.u32 %v36140, 24
%v36150 = vshrl.u32 %v36140, 8
%v36151 = vor.u32 %v36150, %v36149
%v36152 = vxor.u32 %v36151, %v36143
%v36155 = vadd.s32 %v36152, %v8
%v36159 = vadd.s32 4, %v36155
%v36163 = vadd.s32 %v36159, %v36147
%v36165 = vshll.u32 %v36159, 13
%v36166 = vshrl.u32 %v36159, 19
%v36167 = vor.u32 %v36166, %v36165
%v36168 = vxor.u32 %v36167, %v36163
%v36171 = vadd.s32 %v36168, %v36163
%v36173 = vshll.u32 %v36168, 15
%v36174 = vshrl.u32 %v36168, 17
%v36175 = vor.u32 %v36174, %v36173
%v36176 = vxor.u32 %v36175, %v36171
%v36179 = vadd.s32 %v36176, %v36171
%v36181 = vshll.u32 %v36176, 26
%v36182 = vshrl.u32 %v36176, 6
%v36183 = vor.u32 %v36182, %v36181
%v36184 = vxor.u32 %v36183, %v36179
%v36187 = vadd.s32 %v36184, %v36179
%v36191 = vadd.s32 %v36187, %v8
%v36193 = vshll.u32 %v36184, 6
%v36194 = vshrl.u32 %v36184, 26
%v36195 = vor.u32 %v36194, %v36193
%v36196 = vxor.u32 %v36195, %v36187
%v36199 = vadd.s32 %v36196, %v10
%v36203 = vadd.s32 5, %v36199
%v36205 = vxor.u32 %v36203, %v36191
%v36206 = vand.u32.u8 255, %v36205
%v36207 = vand.u32 65535, %v36206
%v36208 = vshrl.u32 %v36207, 1
%v36209 = vor.u32 16256, %v36208
%v36210 = vand.u32.u16 65535, %v36209
%v119936 = vadd.low.f32.bf16 -1.0, %v36210
%v36219 = vmul.f32 2.0, %v119936
%v36223 = vadd.f32 -0.99609375, %v36219
%v36227 = vmax.f32 %v36223, -0.99609375
%v36229 = vand.u32 2147483647, %v36227
%vm36232 = vcmp.eq.f32.partialorder %v36229, 1.0
%v36237 = vmul.f32 inf, %v36227
%v36239 = vxor.u32 2147483648, %v36227
%v36242 = vmul.f32 %v36239, %v36227
%v36244 = vadd.f32 1.0, %v36242
%v36245 = vlog2.pop %v36244
%v36246 = vmul.f32 0.6931472, %v36245
%v36247 = vmul.f32 -0.5, %v36242
%v36248 = vadd.f32 1.0, %v36247
%v36249 = vmul.f32 %v36248, %v36242
%v36250 = vand.u32 2147483647, %v36242
%vm36251 = vcmp.lt.f32.partialorder %v36250, 0.0004427343
%v36252 = vsel /*vm=*/%vm36251, /*on_true_vy=*/%v36249, /*on_false_vx=*/%v36246
%v36253 = vxor.u32 2147483648, %v36252
%vm36256 = vcmp.lt.f32.partialorder %v36253, 5.0
%v36261 = vsel /*vm=*/%vm36256, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v36265 = vsel /*vm=*/%vm36256, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v36269 = vsel /*vm=*/%vm36256, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v36273 = vsel /*vm=*/%vm36256, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v36277 = vsel /*vm=*/%vm36256, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v36281 = vsel /*vm=*/%vm36256, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v36285 = vsel /*vm=*/%vm36256, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v36289 = vsel /*vm=*/%vm36256, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v36293 = vsel /*vm=*/%vm36256, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v36297 = vadd.f32 -2.5, %v36253
%v36299 = vrsqrt.pop %v36253
%v36300 = vmul.f32 %v36299, %v36253
%vm36301 = vcmp.eq.f32.partialorder %v36253, inf
%v36302 = vsel /*vm=*/%vm36301, /*on_true_vy=*/%v36253, /*on_false_vx=*/%v36300
%vm36303 = vcmp.eq.f32.partialorder %v36253, 0.0
%v36304 = vand.u32 2147483648, %v36253
%v36305 = vsel /*vm=*/%vm36303, /*on_true_vy=*/%v36304, /*on_false_vx=*/%v36302
%v36308 = vadd.f32 -3.0, %v36305
%v36312 = vsel /*vm=*/%vm36256, /*on_true_vy=*/%v36297, /*on_false_vx=*/%v36308
%v36316 = vmul.f32 %v36312, %v36293
%v36320 = vadd.f32 %v36316, %v36289
%v36324 = vmul.f32 %v36320, %v36312
%v36328 = vadd.f32 %v36324, %v36285
%v36332 = vmul.f32 %v36328, %v36312
%v36336 = vadd.f32 %v36332, %v36281
%v36340 = vmul.f32 %v36336, %v36312
%v36344 = vadd.f32 %v36340, %v36277
%v36348 = vmul.f32 %v36344, %v36312
%v36352 = vadd.f32 %v36348, %v36273
%v36356 = vmul.f32 %v36352, %v36312
%v36360 = vadd.f32 %v36356, %v36269
%v36364 = vmul.f32 %v36360, %v36312
%v36368 = vadd.f32 %v36364, %v36265
%v36372 = vmul.f32 %v36368, %v36312
%v36376 = vadd.f32 %v36372, %v36261
%v36380 = vmul.f32 %v36376, %v36227
%v36384 = vsel /*vm=*/%vm36232, /*on_true_vy=*/%v36237, /*on_false_vx=*/%v36380
%v36388 = vmul.f32 1.4140625, %v36384
%v36391 = vpack.c.bf16 %v120417, %v36388
%119937 = vst [vmem:[%s280 + $0x224] sm:$0xf] /*vst_source=*/%v36391
%v36395 = vadd.s32 %v34087, %v2842
%v36405 = vadd.s32 %v36395, %v415
%vm36409 = vcmp.lt.u32.totalorder %v36405, %v36395
%vm36414 = vcmp.lt.u32.totalorder %v36395, %v2842
%v36419 = vadd.s32 %v34070, %v2829
%v36423 = vadd.s32 1, %v36419
%v36427 = vsel /*vm=*/%vm36414, /*on_true_vy=*/%v36423, /*on_false_vx=*/%v36419
%v36431 = vadd.s32 1, %v36427
%v36435 = vsel /*vm=*/%vm36409, /*on_true_vy=*/%v36431, /*on_false_vx=*/%v36427
%v36440 = vadd.s32 %v36435, %v10
%v36444 = vadd.s32 %v36405, %v9
%v36448 = vadd.s32 %v36444, %v36440
%v36450 = vshll.u32 %v36444, 13
%v36451 = vshrl.u32 %v36444, 19
%v36452 = vor.u32 %v36451, %v36450
%v36453 = vxor.u32 %v36452, %v36448
%v36456 = vadd.s32 %v36453, %v36448
%v36458 = vshll.u32 %v36453, 15
%v36459 = vshrl.u32 %v36453, 17
%v36460 = vor.u32 %v36459, %v36458
%v36461 = vxor.u32 %v36460, %v36456
%v36464 = vadd.s32 %v36461, %v36456
%v36466 = vshll.u32 %v36461, 26
%v36467 = vshrl.u32 %v36461, 6
%v36468 = vor.u32 %v36467, %v36466
%v36469 = vxor.u32 %v36468, %v36464
%v36472 = vadd.s32 %v36469, %v36464
%v36476 = vadd.s32 %v36472, %v9
%v36478 = vshll.u32 %v36469, 6
%v36479 = vshrl.u32 %v36469, 26
%v36480 = vor.u32 %v36479, %v36478
%v36481 = vxor.u32 %v36480, %v36472
%v36484 = vadd.s32 %v36481, %v8
%v36488 = vadd.s32 1, %v36484
%v36492 = vadd.s32 %v36488, %v36476
%v36494 = vshll.u32 %v36488, 17
%v36495 = vshrl.u32 %v36488, 15
%v36496 = vor.u32 %v36495, %v36494
%v36497 = vxor.u32 %v36496, %v36492
%v36500 = vadd.s32 %v36497, %v36492
%v36502 = vshll.u32 %v36497, 29
%v36503 = vshrl.u32 %v36497, 3
%v36504 = vor.u32 %v36503, %v36502
%v36505 = vxor.u32 %v36504, %v36500
%v36508 = vadd.s32 %v36505, %v36500
%v36510 = vshll.u32 %v36505, 16
%v36511 = vshrl.u32 %v36505, 16
%v36512 = vor.u32 %v36511, %v36510
%v36513 = vxor.u32 %v36512, %v36508
%v36516 = vadd.s32 %v36513, %v36508
%v36520 = vadd.s32 %v36516, %v8
%v36522 = vshll.u32 %v36513, 24
%v36523 = vshrl.u32 %v36513, 8
%v36524 = vor.u32 %v36523, %v36522
%v36525 = vxor.u32 %v36524, %v36516
%v36528 = vadd.s32 %v36525, %v10
%v36532 = vadd.s32 2, %v36528
%v36536 = vadd.s32 %v36532, %v36520
%v36538 = vshll.u32 %v36532, 13
%v36539 = vshrl.u32 %v36532, 19
%v36540 = vor.u32 %v36539, %v36538
%v36541 = vxor.u32 %v36540, %v36536
%v36544 = vadd.s32 %v36541, %v36536
%v36546 = vshll.u32 %v36541, 15
%v36547 = vshrl.u32 %v36541, 17
%v36548 = vor.u32 %v36547, %v36546
%v36549 = vxor.u32 %v36548, %v36544
%v36552 = vadd.s32 %v36549, %v36544
%v36554 = vshll.u32 %v36549, 26
%v36555 = vshrl.u32 %v36549, 6
%v36556 = vor.u32 %v36555, %v36554
%v36557 = vxor.u32 %v36556, %v36552
%v36560 = vadd.s32 %v36557, %v36552
%v36564 = vadd.s32 %v36560, %v10
%v36566 = vshll.u32 %v36557, 6
%v36567 = vshrl.u32 %v36557, 26
%v36568 = vor.u32 %v36567, %v36566
%v36569 = vxor.u32 %v36568, %v36560
%v36572 = vadd.s32 %v36569, %v9
%v36576 = vadd.s32 3, %v36572
%v36580 = vadd.s32 %v36576, %v36564
%v36582 = vshll.u32 %v36576, 17
%v36583 = vshrl.u32 %v36576, 15
%v36584 = vor.u32 %v36583, %v36582
%v36585 = vxor.u32 %v36584, %v36580
%v36588 = vadd.s32 %v36585, %v36580
%v36590 = vshll.u32 %v36585, 29
%v36591 = vshrl.u32 %v36585, 3
%v36592 = vor.u32 %v36591, %v36590
%v36593 = vxor.u32 %v36592, %v36588
%v36596 = vadd.s32 %v36593, %v36588
%v36598 = vshll.u32 %v36593, 16
%v36599 = vshrl.u32 %v36593, 16
%v36600 = vor.u32 %v36599, %v36598
%v36601 = vxor.u32 %v36600, %v36596
%v36604 = vadd.s32 %v36601, %v36596
%v36608 = vadd.s32 %v36604, %v9
%v36610 = vshll.u32 %v36601, 24
%v36611 = vshrl.u32 %v36601, 8
%v36612 = vor.u32 %v36611, %v36610
%v36613 = vxor.u32 %v36612, %v36604
%v36616 = vadd.s32 %v36613, %v8
%v36620 = vadd.s32 4, %v36616
%v36624 = vadd.s32 %v36620, %v36608
%v36626 = vshll.u32 %v36620, 13
%v36627 = vshrl.u32 %v36620, 19
%v36628 = vor.u32 %v36627, %v36626
%v36629 = vxor.u32 %v36628, %v36624
%v36632 = vadd.s32 %v36629, %v36624
%v36634 = vshll.u32 %v36629, 15
%v36635 = vshrl.u32 %v36629, 17
%v36636 = vor.u32 %v36635, %v36634
%v36637 = vxor.u32 %v36636, %v36632
%v36640 = vadd.s32 %v36637, %v36632
%v36642 = vshll.u32 %v36637, 26
%v36643 = vshrl.u32 %v36637, 6
%v36644 = vor.u32 %v36643, %v36642
%v36645 = vxor.u32 %v36644, %v36640
%v36648 = vadd.s32 %v36645, %v36640
%v36652 = vadd.s32 %v36648, %v8
%v36654 = vshll.u32 %v36645, 6
%v36655 = vshrl.u32 %v36645, 26
%v36656 = vor.u32 %v36655, %v36654
%v36657 = vxor.u32 %v36656, %v36648
%v36660 = vadd.s32 %v36657, %v10
%v36664 = vadd.s32 5, %v36660
%v36666 = vxor.u32 %v36664, %v36652
%v36667 = vand.u32.u8 255, %v36666
%v36668 = vand.u32 65535, %v36667
%v36669 = vshrl.u32 %v36668, 1
%v36670 = vor.u32 16256, %v36669
%v36671 = vand.u32.u16 65535, %v36670
%v119938 = vadd.low.f32.bf16 -1.0, %v36671
%v36680 = vmul.f32 2.0, %v119938
%v36684 = vadd.f32 -0.99609375, %v36680
%v36688 = vmax.f32 %v36684, -0.99609375
%v36690 = vand.u32 2147483647, %v36688
%vm36693 = vcmp.eq.f32.partialorder %v36690, 1.0
%v36698 = vmul.f32 inf, %v36688
%v36700 = vxor.u32 2147483648, %v36688
%v36703 = vmul.f32 %v36700, %v36688
%v36705 = vadd.f32 1.0, %v36703
%v36706 = vlog2.pop %v36705
%v36707 = vmul.f32 0.6931472, %v36706
%v36708 = vmul.f32 -0.5, %v36703
%v36709 = vadd.f32 1.0, %v36708
%v36710 = vmul.f32 %v36709, %v36703
%v36711 = vand.u32 2147483647, %v36703
%vm36712 = vcmp.lt.f32.partialorder %v36711, 0.0004427343
%v36713 = vsel /*vm=*/%vm36712, /*on_true_vy=*/%v36710, /*on_false_vx=*/%v36707
%v36714 = vxor.u32 2147483648, %v36713
%vm36717 = vcmp.lt.f32.partialorder %v36714, 5.0
%v36722 = vsel /*vm=*/%vm36717, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v36726 = vsel /*vm=*/%vm36717, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v36730 = vsel /*vm=*/%vm36717, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v36734 = vsel /*vm=*/%vm36717, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v36738 = vsel /*vm=*/%vm36717, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v36742 = vsel /*vm=*/%vm36717, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v36746 = vsel /*vm=*/%vm36717, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v36750 = vsel /*vm=*/%vm36717, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v36754 = vsel /*vm=*/%vm36717, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v36758 = vadd.f32 -2.5, %v36714
%v36760 = vrsqrt.pop %v36714
%v36761 = vmul.f32 %v36760, %v36714
%vm36762 = vcmp.eq.f32.partialorder %v36714, inf
%v36763 = vsel /*vm=*/%vm36762, /*on_true_vy=*/%v36714, /*on_false_vx=*/%v36761
%vm36764 = vcmp.eq.f32.partialorder %v36714, 0.0
%v36765 = vand.u32 2147483648, %v36714
%v36766 = vsel /*vm=*/%vm36764, /*on_true_vy=*/%v36765, /*on_false_vx=*/%v36763
%v36769 = vadd.f32 -3.0, %v36766
%v36773 = vsel /*vm=*/%vm36717, /*on_true_vy=*/%v36758, /*on_false_vx=*/%v36769
%v36777 = vmul.f32 %v36773, %v36754
%v36781 = vadd.f32 %v36777, %v36750
%v36785 = vmul.f32 %v36781, %v36773
%v36789 = vadd.f32 %v36785, %v36746
%v36793 = vmul.f32 %v36789, %v36773
%v36797 = vadd.f32 %v36793, %v36742
%v36801 = vmul.f32 %v36797, %v36773
%v36805 = vadd.f32 %v36801, %v36738
%v36809 = vmul.f32 %v36805, %v36773
%v36813 = vadd.f32 %v36809, %v36734
%v36817 = vmul.f32 %v36813, %v36773
%v36821 = vadd.f32 %v36817, %v36730
%v36825 = vmul.f32 %v36821, %v36773
%v36829 = vadd.f32 %v36825, %v36726
%v36833 = vmul.f32 %v36829, %v36773
%v36837 = vadd.f32 %v36833, %v36722
%v36841 = vmul.f32 %v36837, %v36688
%v36845 = vsel /*vm=*/%vm36693, /*on_true_vy=*/%v36698, /*on_false_vx=*/%v36841
%v36849 = vmul.f32 1.4140625, %v36845
%v36852 = vpack.c.bf16 %v120417, %v36849
%119939 = vst [vmem:[%s280 + $0x2a4] sm:$0xf] /*vst_source=*/%v36852
%v36856 = vadd.s32 %v34087, %v3329
%v36866 = vadd.s32 %v36856, %v415
%vm36870 = vcmp.lt.u32.totalorder %v36866, %v36856
%vm36875 = vcmp.lt.u32.totalorder %v36856, %v3329
%v36880 = vadd.s32 %v34070, %v3316
%v36884 = vadd.s32 1, %v36880
%v36888 = vsel /*vm=*/%vm36875, /*on_true_vy=*/%v36884, /*on_false_vx=*/%v36880
%v36892 = vadd.s32 1, %v36888
%v36896 = vsel /*vm=*/%vm36870, /*on_true_vy=*/%v36892, /*on_false_vx=*/%v36888
%v36901 = vadd.s32 %v36896, %v10
%v36905 = vadd.s32 %v36866, %v9
%v36909 = vadd.s32 %v36905, %v36901
%v36911 = vshll.u32 %v36905, 13
%v36912 = vshrl.u32 %v36905, 19
%v36913 = vor.u32 %v36912, %v36911
%v36914 = vxor.u32 %v36913, %v36909
%v36917 = vadd.s32 %v36914, %v36909
%v36919 = vshll.u32 %v36914, 15
%v36920 = vshrl.u32 %v36914, 17
%v36921 = vor.u32 %v36920, %v36919
%v36922 = vxor.u32 %v36921, %v36917
%v36925 = vadd.s32 %v36922, %v36917
%v36927 = vshll.u32 %v36922, 26
%v36928 = vshrl.u32 %v36922, 6
%v36929 = vor.u32 %v36928, %v36927
%v36930 = vxor.u32 %v36929, %v36925
%v36933 = vadd.s32 %v36930, %v36925
%v36937 = vadd.s32 %v36933, %v9
%v36939 = vshll.u32 %v36930, 6
%v36940 = vshrl.u32 %v36930, 26
%v36941 = vor.u32 %v36940, %v36939
%v36942 = vxor.u32 %v36941, %v36933
%v36945 = vadd.s32 %v36942, %v8
%v36949 = vadd.s32 1, %v36945
%v36953 = vadd.s32 %v36949, %v36937
%v36955 = vshll.u32 %v36949, 17
%v36956 = vshrl.u32 %v36949, 15
%v36957 = vor.u32 %v36956, %v36955
%v36958 = vxor.u32 %v36957, %v36953
%v36961 = vadd.s32 %v36958, %v36953
%v36963 = vshll.u32 %v36958, 29
%v36964 = vshrl.u32 %v36958, 3
%v36965 = vor.u32 %v36964, %v36963
%v36966 = vxor.u32 %v36965, %v36961
%v36969 = vadd.s32 %v36966, %v36961
%v36971 = vshll.u32 %v36966, 16
%v36972 = vshrl.u32 %v36966, 16
%v36973 = vor.u32 %v36972, %v36971
%v36974 = vxor.u32 %v36973, %v36969
%v36977 = vadd.s32 %v36974, %v36969
%v36981 = vadd.s32 %v36977, %v8
%v36983 = vshll.u32 %v36974, 24
%v36984 = vshrl.u32 %v36974, 8
%v36985 = vor.u32 %v36984, %v36983
%v36986 = vxor.u32 %v36985, %v36977
%v36989 = vadd.s32 %v36986, %v10
%v36993 = vadd.s32 2, %v36989
%v36997 = vadd.s32 %v36993, %v36981
%v36999 = vshll.u32 %v36993, 13
%v37000 = vshrl.u32 %v36993, 19
%v37001 = vor.u32 %v37000, %v36999
%v37002 = vxor.u32 %v37001, %v36997
%v37005 = vadd.s32 %v37002, %v36997
%v37007 = vshll.u32 %v37002, 15
%v37008 = vshrl.u32 %v37002, 17
%v37009 = vor.u32 %v37008, %v37007
%v37010 = vxor.u32 %v37009, %v37005
%v37013 = vadd.s32 %v37010, %v37005
%v37015 = vshll.u32 %v37010, 26
%v37016 = vshrl.u32 %v37010, 6
%v37017 = vor.u32 %v37016, %v37015
%v37018 = vxor.u32 %v37017, %v37013
%v37021 = vadd.s32 %v37018, %v37013
%v37025 = vadd.s32 %v37021, %v10
%v37027 = vshll.u32 %v37018, 6
%v37028 = vshrl.u32 %v37018, 26
%v37029 = vor.u32 %v37028, %v37027
%v37030 = vxor.u32 %v37029, %v37021
%v37033 = vadd.s32 %v37030, %v9
%v37037 = vadd.s32 3, %v37033
%v37041 = vadd.s32 %v37037, %v37025
%v37043 = vshll.u32 %v37037, 17
%v37044 = vshrl.u32 %v37037, 15
%v37045 = vor.u32 %v37044, %v37043
%v37046 = vxor.u32 %v37045, %v37041
%v37049 = vadd.s32 %v37046, %v37041
%v37051 = vshll.u32 %v37046, 29
%v37052 = vshrl.u32 %v37046, 3
%v37053 = vor.u32 %v37052, %v37051
%v37054 = vxor.u32 %v37053, %v37049
%v37057 = vadd.s32 %v37054, %v37049
%v37059 = vshll.u32 %v37054, 16
%v37060 = vshrl.u32 %v37054, 16
%v37061 = vor.u32 %v37060, %v37059
%v37062 = vxor.u32 %v37061, %v37057
%v37065 = vadd.s32 %v37062, %v37057
%v37069 = vadd.s32 %v37065, %v9
%v37071 = vshll.u32 %v37062, 24
%v37072 = vshrl.u32 %v37062, 8
%v37073 = vor.u32 %v37072, %v37071
%v37074 = vxor.u32 %v37073, %v37065
%v37077 = vadd.s32 %v37074, %v8
%v37081 = vadd.s32 4, %v37077
%v37085 = vadd.s32 %v37081, %v37069
%v37087 = vshll.u32 %v37081, 13
%v37088 = vshrl.u32 %v37081, 19
%v37089 = vor.u32 %v37088, %v37087
%v37090 = vxor.u32 %v37089, %v37085
%v37093 = vadd.s32 %v37090, %v37085
%v37095 = vshll.u32 %v37090, 15
%v37096 = vshrl.u32 %v37090, 17
%v37097 = vor.u32 %v37096, %v37095
%v37098 = vxor.u32 %v37097, %v37093
%v37101 = vadd.s32 %v37098, %v37093
%v37103 = vshll.u32 %v37098, 26
%v37104 = vshrl.u32 %v37098, 6
%v37105 = vor.u32 %v37104, %v37103
%v37106 = vxor.u32 %v37105, %v37101
%v37109 = vadd.s32 %v37106, %v37101
%v37113 = vadd.s32 %v37109, %v8
%v37115 = vshll.u32 %v37106, 6
%v37116 = vshrl.u32 %v37106, 26
%v37117 = vor.u32 %v37116, %v37115
%v37118 = vxor.u32 %v37117, %v37109
%v37121 = vadd.s32 %v37118, %v10
%v37125 = vadd.s32 5, %v37121
%v37127 = vxor.u32 %v37125, %v37113
%v37128 = vand.u32.u8 255, %v37127
%v37129 = vand.u32 65535, %v37128
%v37130 = vshrl.u32 %v37129, 1
%v37131 = vor.u32 16256, %v37130
%v37132 = vand.u32.u16 65535, %v37131
%v119940 = vadd.low.f32.bf16 -1.0, %v37132
%v37141 = vmul.f32 2.0, %v119940
%v37145 = vadd.f32 -0.99609375, %v37141
%v37149 = vmax.f32 %v37145, -0.99609375
%v37151 = vand.u32 2147483647, %v37149
%vm37154 = vcmp.eq.f32.partialorder %v37151, 1.0
%v37159 = vmul.f32 inf, %v37149
%v37161 = vxor.u32 2147483648, %v37149
%v37164 = vmul.f32 %v37161, %v37149
%v37166 = vadd.f32 1.0, %v37164
%v37167 = vlog2.pop %v37166
%v37168 = vmul.f32 0.6931472, %v37167
%v37169 = vmul.f32 -0.5, %v37164
%v37170 = vadd.f32 1.0, %v37169
%v37171 = vmul.f32 %v37170, %v37164
%v37172 = vand.u32 2147483647, %v37164
%vm37173 = vcmp.lt.f32.partialorder %v37172, 0.0004427343
%v37174 = vsel /*vm=*/%vm37173, /*on_true_vy=*/%v37171, /*on_false_vx=*/%v37168
%v37175 = vxor.u32 2147483648, %v37174
%vm37178 = vcmp.lt.f32.partialorder %v37175, 5.0
%v37183 = vsel /*vm=*/%vm37178, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v37187 = vsel /*vm=*/%vm37178, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v37191 = vsel /*vm=*/%vm37178, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v37195 = vsel /*vm=*/%vm37178, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v37199 = vsel /*vm=*/%vm37178, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v37203 = vsel /*vm=*/%vm37178, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v37207 = vsel /*vm=*/%vm37178, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v37211 = vsel /*vm=*/%vm37178, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v37215 = vsel /*vm=*/%vm37178, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v37219 = vadd.f32 -2.5, %v37175
%v37221 = vrsqrt.pop %v37175
%v37222 = vmul.f32 %v37221, %v37175
%vm37223 = vcmp.eq.f32.partialorder %v37175, inf
%v37224 = vsel /*vm=*/%vm37223, /*on_true_vy=*/%v37175, /*on_false_vx=*/%v37222
%vm37225 = vcmp.eq.f32.partialorder %v37175, 0.0
%v37226 = vand.u32 2147483648, %v37175
%v37227 = vsel /*vm=*/%vm37225, /*on_true_vy=*/%v37226, /*on_false_vx=*/%v37224
%v37230 = vadd.f32 -3.0, %v37227
%v37234 = vsel /*vm=*/%vm37178, /*on_true_vy=*/%v37219, /*on_false_vx=*/%v37230
%v37238 = vmul.f32 %v37234, %v37215
%v37242 = vadd.f32 %v37238, %v37211
%v37246 = vmul.f32 %v37242, %v37234
%v37250 = vadd.f32 %v37246, %v37207
%v37254 = vmul.f32 %v37250, %v37234
%v37258 = vadd.f32 %v37254, %v37203
%v37262 = vmul.f32 %v37258, %v37234
%v37266 = vadd.f32 %v37262, %v37199
%v37270 = vmul.f32 %v37266, %v37234
%v37274 = vadd.f32 %v37270, %v37195
%v37278 = vmul.f32 %v37274, %v37234
%v37282 = vadd.f32 %v37278, %v37191
%v37286 = vmul.f32 %v37282, %v37234
%v37290 = vadd.f32 %v37286, %v37187
%v37294 = vmul.f32 %v37290, %v37234
%v37298 = vadd.f32 %v37294, %v37183
%v37302 = vmul.f32 %v37298, %v37149
%v37306 = vsel /*vm=*/%vm37154, /*on_true_vy=*/%v37159, /*on_false_vx=*/%v37302
%v37310 = vmul.f32 1.4140625, %v37306
%v37313 = vpack.c.bf16 %v120417, %v37310
%119941 = vst [vmem:[%s280 + $0x324] sm:$0xf] /*vst_source=*/%v37313
%v37317 = vadd.s32 %v34087, %v3816
%v37327 = vadd.s32 %v37317, %v415
%vm37331 = vcmp.lt.u32.totalorder %v37327, %v37317
%vm37336 = vcmp.lt.u32.totalorder %v37317, %v3816
%v37341 = vadd.s32 %v34070, %v3803
%v37345 = vadd.s32 1, %v37341
%v37349 = vsel /*vm=*/%vm37336, /*on_true_vy=*/%v37345, /*on_false_vx=*/%v37341
%v37353 = vadd.s32 1, %v37349
%v37357 = vsel /*vm=*/%vm37331, /*on_true_vy=*/%v37353, /*on_false_vx=*/%v37349
%v37362 = vadd.s32 %v37357, %v10
%v37366 = vadd.s32 %v37327, %v9
%v37370 = vadd.s32 %v37366, %v37362
%v37372 = vshll.u32 %v37366, 13
%v37373 = vshrl.u32 %v37366, 19
%v37374 = vor.u32 %v37373, %v37372
%v37375 = vxor.u32 %v37374, %v37370
%v37378 = vadd.s32 %v37375, %v37370
%v37380 = vshll.u32 %v37375, 15
%v37381 = vshrl.u32 %v37375, 17
%v37382 = vor.u32 %v37381, %v37380
%v37383 = vxor.u32 %v37382, %v37378
%v37386 = vadd.s32 %v37383, %v37378
%v37388 = vshll.u32 %v37383, 26
%v37389 = vshrl.u32 %v37383, 6
%v37390 = vor.u32 %v37389, %v37388
%v37391 = vxor.u32 %v37390, %v37386
%v37394 = vadd.s32 %v37391, %v37386
%v37398 = vadd.s32 %v37394, %v9
%v37400 = vshll.u32 %v37391, 6
%v37401 = vshrl.u32 %v37391, 26
%v37402 = vor.u32 %v37401, %v37400
%v37403 = vxor.u32 %v37402, %v37394
%v37406 = vadd.s32 %v37403, %v8
%v37410 = vadd.s32 1, %v37406
%v37414 = vadd.s32 %v37410, %v37398
%v37416 = vshll.u32 %v37410, 17
%v37417 = vshrl.u32 %v37410, 15
%v37418 = vor.u32 %v37417, %v37416
%v37419 = vxor.u32 %v37418, %v37414
%v37422 = vadd.s32 %v37419, %v37414
%v37424 = vshll.u32 %v37419, 29
%v37425 = vshrl.u32 %v37419, 3
%v37426 = vor.u32 %v37425, %v37424
%v37427 = vxor.u32 %v37426, %v37422
%v37430 = vadd.s32 %v37427, %v37422
%v37432 = vshll.u32 %v37427, 16
%v37433 = vshrl.u32 %v37427, 16
%v37434 = vor.u32 %v37433, %v37432
%v37435 = vxor.u32 %v37434, %v37430
%v37438 = vadd.s32 %v37435, %v37430
%v37442 = vadd.s32 %v37438, %v8
%v37444 = vshll.u32 %v37435, 24
%v37445 = vshrl.u32 %v37435, 8
%v37446 = vor.u32 %v37445, %v37444
%v37447 = vxor.u32 %v37446, %v37438
%v37450 = vadd.s32 %v37447, %v10
%v37454 = vadd.s32 2, %v37450
%v37458 = vadd.s32 %v37454, %v37442
%v37460 = vshll.u32 %v37454, 13
%v37461 = vshrl.u32 %v37454, 19
%v37462 = vor.u32 %v37461, %v37460
%v37463 = vxor.u32 %v37462, %v37458
%v37466 = vadd.s32 %v37463, %v37458
%v37468 = vshll.u32 %v37463, 15
%v37469 = vshrl.u32 %v37463, 17
%v37470 = vor.u32 %v37469, %v37468
%v37471 = vxor.u32 %v37470, %v37466
%v37474 = vadd.s32 %v37471, %v37466
%v37476 = vshll.u32 %v37471, 26
%v37477 = vshrl.u32 %v37471, 6
%v37478 = vor.u32 %v37477, %v37476
%v37479 = vxor.u32 %v37478, %v37474
%v37482 = vadd.s32 %v37479, %v37474
%v37486 = vadd.s32 %v37482, %v10
%v37488 = vshll.u32 %v37479, 6
%v37489 = vshrl.u32 %v37479, 26
%v37490 = vor.u32 %v37489, %v37488
%v37491 = vxor.u32 %v37490, %v37482
%v37494 = vadd.s32 %v37491, %v9
%v37498 = vadd.s32 3, %v37494
%v37502 = vadd.s32 %v37498, %v37486
%v37504 = vshll.u32 %v37498, 17
%v37505 = vshrl.u32 %v37498, 15
%v37506 = vor.u32 %v37505, %v37504
%v37507 = vxor.u32 %v37506, %v37502
%v37510 = vadd.s32 %v37507, %v37502
%v37512 = vshll.u32 %v37507, 29
%v37513 = vshrl.u32 %v37507, 3
%v37514 = vor.u32 %v37513, %v37512
%v37515 = vxor.u32 %v37514, %v37510
%v37518 = vadd.s32 %v37515, %v37510
%v37520 = vshll.u32 %v37515, 16
%v37521 = vshrl.u32 %v37515, 16
%v37522 = vor.u32 %v37521, %v37520
%v37523 = vxor.u32 %v37522, %v37518
%v37526 = vadd.s32 %v37523, %v37518
%v37530 = vadd.s32 %v37526, %v9
%v37532 = vshll.u32 %v37523, 24
%v37533 = vshrl.u32 %v37523, 8
%v37534 = vor.u32 %v37533, %v37532
%v37535 = vxor.u32 %v37534, %v37526
%v37538 = vadd.s32 %v37535, %v8
%v37542 = vadd.s32 4, %v37538
%v37546 = vadd.s32 %v37542, %v37530
%v37548 = vshll.u32 %v37542, 13
%v37549 = vshrl.u32 %v37542, 19
%v37550 = vor.u32 %v37549, %v37548
%v37551 = vxor.u32 %v37550, %v37546
%v37554 = vadd.s32 %v37551, %v37546
%v37556 = vshll.u32 %v37551, 15
%v37557 = vshrl.u32 %v37551, 17
%v37558 = vor.u32 %v37557, %v37556
%v37559 = vxor.u32 %v37558, %v37554
%v37562 = vadd.s32 %v37559, %v37554
%v37564 = vshll.u32 %v37559, 26
%v37565 = vshrl.u32 %v37559, 6
%v37566 = vor.u32 %v37565, %v37564
%v37567 = vxor.u32 %v37566, %v37562
%v37570 = vadd.s32 %v37567, %v37562
%v37574 = vadd.s32 %v37570, %v8
%v37576 = vshll.u32 %v37567, 6
%v37577 = vshrl.u32 %v37567, 26
%v37578 = vor.u32 %v37577, %v37576
%v37579 = vxor.u32 %v37578, %v37570
%v37582 = vadd.s32 %v37579, %v10
%v37586 = vadd.s32 5, %v37582
%v37588 = vxor.u32 %v37586, %v37574
%v37589 = vand.u32.u8 255, %v37588
%v37590 = vand.u32 65535, %v37589
%v37591 = vshrl.u32 %v37590, 1
%v37592 = vor.u32 16256, %v37591
%v37593 = vand.u32.u16 65535, %v37592
%v119942 = vadd.low.f32.bf16 -1.0, %v37593
%v37602 = vmul.f32 2.0, %v119942
%v37606 = vadd.f32 -0.99609375, %v37602
%v37610 = vmax.f32 %v37606, -0.99609375
%v37612 = vand.u32 2147483647, %v37610
%vm37615 = vcmp.eq.f32.partialorder %v37612, 1.0
%v37620 = vmul.f32 inf, %v37610
%v37622 = vxor.u32 2147483648, %v37610
%v37625 = vmul.f32 %v37622, %v37610
%v37627 = vadd.f32 1.0, %v37625
%v37628 = vlog2.pop %v37627
%v37629 = vmul.f32 0.6931472, %v37628
%v37630 = vmul.f32 -0.5, %v37625
%v37631 = vadd.f32 1.0, %v37630
%v37632 = vmul.f32 %v37631, %v37625
%v37633 = vand.u32 2147483647, %v37625
%vm37634 = vcmp.lt.f32.partialorder %v37633, 0.0004427343
%v37635 = vsel /*vm=*/%vm37634, /*on_true_vy=*/%v37632, /*on_false_vx=*/%v37629
%v37636 = vxor.u32 2147483648, %v37635
%vm37639 = vcmp.lt.f32.partialorder %v37636, 5.0
%v37644 = vsel /*vm=*/%vm37639, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v37648 = vsel /*vm=*/%vm37639, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v37652 = vsel /*vm=*/%vm37639, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v37656 = vsel /*vm=*/%vm37639, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v37660 = vsel /*vm=*/%vm37639, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v37664 = vsel /*vm=*/%vm37639, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v37668 = vsel /*vm=*/%vm37639, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v37672 = vsel /*vm=*/%vm37639, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v37676 = vsel /*vm=*/%vm37639, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v37680 = vadd.f32 -2.5, %v37636
%v37682 = vrsqrt.pop %v37636
%v37683 = vmul.f32 %v37682, %v37636
%vm37684 = vcmp.eq.f32.partialorder %v37636, inf
%v37685 = vsel /*vm=*/%vm37684, /*on_true_vy=*/%v37636, /*on_false_vx=*/%v37683
%vm37686 = vcmp.eq.f32.partialorder %v37636, 0.0
%v37687 = vand.u32 2147483648, %v37636
%v37688 = vsel /*vm=*/%vm37686, /*on_true_vy=*/%v37687, /*on_false_vx=*/%v37685
%v37691 = vadd.f32 -3.0, %v37688
%v37695 = vsel /*vm=*/%vm37639, /*on_true_vy=*/%v37680, /*on_false_vx=*/%v37691
%v37699 = vmul.f32 %v37695, %v37676
%v37703 = vadd.f32 %v37699, %v37672
%v37707 = vmul.f32 %v37703, %v37695
%v37711 = vadd.f32 %v37707, %v37668
%v37715 = vmul.f32 %v37711, %v37695
%v37719 = vadd.f32 %v37715, %v37664
%v37723 = vmul.f32 %v37719, %v37695
%v37727 = vadd.f32 %v37723, %v37660
%v37731 = vmul.f32 %v37727, %v37695
%v37735 = vadd.f32 %v37731, %v37656
%v37739 = vmul.f32 %v37735, %v37695
%v37743 = vadd.f32 %v37739, %v37652
%v37747 = vmul.f32 %v37743, %v37695
%v37751 = vadd.f32 %v37747, %v37648
%v37755 = vmul.f32 %v37751, %v37695
%v37759 = vadd.f32 %v37755, %v37644
%v37763 = vmul.f32 %v37759, %v37610
%v37767 = vsel /*vm=*/%vm37615, /*on_true_vy=*/%v37620, /*on_false_vx=*/%v37763
%v37771 = vmul.f32 1.4140625, %v37767
%v37774 = vpack.c.bf16 %v120417, %v37771
%119943 = vst [vmem:[%s280 + $0x3a4] sm:$0xf] /*vst_source=*/%v37774
%v37812 = vadd.s32 %v37809, %v408
%v37822 = vadd.s32 %v37812, %v415
%vm37826 = vcmp.lt.u32.totalorder %v37822, %v37812
%vm37831 = vcmp.lt.u32.totalorder %v37812, %v408
%v37836 = vadd.s32 %v37792, %v380
%v37840 = vadd.s32 1, %v37836
%v37844 = vsel /*vm=*/%vm37831, /*on_true_vy=*/%v37840, /*on_false_vx=*/%v37836
%v37848 = vadd.s32 1, %v37844
%v37852 = vsel /*vm=*/%vm37826, /*on_true_vy=*/%v37848, /*on_false_vx=*/%v37844
%v37857 = vadd.s32 %v37852, %v10
%v37861 = vadd.s32 %v37822, %v9
%v37865 = vadd.s32 %v37861, %v37857
%v37867 = vshll.u32 %v37861, 13
%v37868 = vshrl.u32 %v37861, 19
%v37869 = vor.u32 %v37868, %v37867
%v37870 = vxor.u32 %v37869, %v37865
%v37873 = vadd.s32 %v37870, %v37865
%v37875 = vshll.u32 %v37870, 15
%v37876 = vshrl.u32 %v37870, 17
%v37877 = vor.u32 %v37876, %v37875
%v37878 = vxor.u32 %v37877, %v37873
%v37881 = vadd.s32 %v37878, %v37873
%v37883 = vshll.u32 %v37878, 26
%v37884 = vshrl.u32 %v37878, 6
%v37885 = vor.u32 %v37884, %v37883
%v37886 = vxor.u32 %v37885, %v37881
%v37889 = vadd.s32 %v37886, %v37881
%v37893 = vadd.s32 %v37889, %v9
%v37895 = vshll.u32 %v37886, 6
%v37896 = vshrl.u32 %v37886, 26
%v37897 = vor.u32 %v37896, %v37895
%v37898 = vxor.u32 %v37897, %v37889
%v37901 = vadd.s32 %v37898, %v8
%v37905 = vadd.s32 1, %v37901
%v37909 = vadd.s32 %v37905, %v37893
%v37911 = vshll.u32 %v37905, 17
%v37912 = vshrl.u32 %v37905, 15
%v37913 = vor.u32 %v37912, %v37911
%v37914 = vxor.u32 %v37913, %v37909
%v37917 = vadd.s32 %v37914, %v37909
%v37919 = vshll.u32 %v37914, 29
%v37920 = vshrl.u32 %v37914, 3
%v37921 = vor.u32 %v37920, %v37919
%v37922 = vxor.u32 %v37921, %v37917
%v37925 = vadd.s32 %v37922, %v37917
%v37927 = vshll.u32 %v37922, 16
%v37928 = vshrl.u32 %v37922, 16
%v37929 = vor.u32 %v37928, %v37927
%v37930 = vxor.u32 %v37929, %v37925
%v37933 = vadd.s32 %v37930, %v37925
%v37937 = vadd.s32 %v37933, %v8
%v37939 = vshll.u32 %v37930, 24
%v37940 = vshrl.u32 %v37930, 8
%v37941 = vor.u32 %v37940, %v37939
%v37942 = vxor.u32 %v37941, %v37933
%v37945 = vadd.s32 %v37942, %v10
%v37949 = vadd.s32 2, %v37945
%v37953 = vadd.s32 %v37949, %v37937
%v37955 = vshll.u32 %v37949, 13
%v37956 = vshrl.u32 %v37949, 19
%v37957 = vor.u32 %v37956, %v37955
%v37958 = vxor.u32 %v37957, %v37953
%v37961 = vadd.s32 %v37958, %v37953
%v37963 = vshll.u32 %v37958, 15
%v37964 = vshrl.u32 %v37958, 17
%v37965 = vor.u32 %v37964, %v37963
%v37966 = vxor.u32 %v37965, %v37961
%v37969 = vadd.s32 %v37966, %v37961
%v37971 = vshll.u32 %v37966, 26
%v37972 = vshrl.u32 %v37966, 6
%v37973 = vor.u32 %v37972, %v37971
%v37974 = vxor.u32 %v37973, %v37969
%v37977 = vadd.s32 %v37974, %v37969
%v37981 = vadd.s32 %v37977, %v10
%v37983 = vshll.u32 %v37974, 6
%v37984 = vshrl.u32 %v37974, 26
%v37985 = vor.u32 %v37984, %v37983
%v37986 = vxor.u32 %v37985, %v37977
%v37989 = vadd.s32 %v37986, %v9
%v37993 = vadd.s32 3, %v37989
%v37997 = vadd.s32 %v37993, %v37981
%v37999 = vshll.u32 %v37993, 17
%v38000 = vshrl.u32 %v37993, 15
%v38001 = vor.u32 %v38000, %v37999
%v38002 = vxor.u32 %v38001, %v37997
%v38005 = vadd.s32 %v38002, %v37997
%v38007 = vshll.u32 %v38002, 29
%v38008 = vshrl.u32 %v38002, 3
%v38009 = vor.u32 %v38008, %v38007
%v38010 = vxor.u32 %v38009, %v38005
%v38013 = vadd.s32 %v38010, %v38005
%v38015 = vshll.u32 %v38010, 16
%v38016 = vshrl.u32 %v38010, 16
%v38017 = vor.u32 %v38016, %v38015
%v38018 = vxor.u32 %v38017, %v38013
%v38021 = vadd.s32 %v38018, %v38013
%v38025 = vadd.s32 %v38021, %v9
%v38027 = vshll.u32 %v38018, 24
%v38028 = vshrl.u32 %v38018, 8
%v38029 = vor.u32 %v38028, %v38027
%v38030 = vxor.u32 %v38029, %v38021
%v38033 = vadd.s32 %v38030, %v8
%v38037 = vadd.s32 4, %v38033
%v38041 = vadd.s32 %v38037, %v38025
%v38043 = vshll.u32 %v38037, 13
%v38044 = vshrl.u32 %v38037, 19
%v38045 = vor.u32 %v38044, %v38043
%v38046 = vxor.u32 %v38045, %v38041
%v38049 = vadd.s32 %v38046, %v38041
%v38051 = vshll.u32 %v38046, 15
%v38052 = vshrl.u32 %v38046, 17
%v38053 = vor.u32 %v38052, %v38051
%v38054 = vxor.u32 %v38053, %v38049
%v38057 = vadd.s32 %v38054, %v38049
%v38059 = vshll.u32 %v38054, 26
%v38060 = vshrl.u32 %v38054, 6
%v38061 = vor.u32 %v38060, %v38059
%v38062 = vxor.u32 %v38061, %v38057
%v38065 = vadd.s32 %v38062, %v38057
%v38069 = vadd.s32 %v38065, %v8
%v38071 = vshll.u32 %v38062, 6
%v38072 = vshrl.u32 %v38062, 26
%v38073 = vor.u32 %v38072, %v38071
%v38074 = vxor.u32 %v38073, %v38065
%v38077 = vadd.s32 %v38074, %v10
%v38081 = vadd.s32 5, %v38077
%v38083 = vxor.u32 %v38081, %v38069
%v38084 = vand.u32.u8 255, %v38083
%v38085 = vand.u32 65535, %v38084
%v38086 = vshrl.u32 %v38085, 1
%v38087 = vor.u32 16256, %v38086
%v38088 = vand.u32.u16 65535, %v38087
%v119948 = vadd.low.f32.bf16 -1.0, %v38088
%v38097 = vmul.f32 2.0, %v119948
%v38101 = vadd.f32 -0.99609375, %v38097
%v38105 = vmax.f32 %v38101, -0.99609375
%v38107 = vand.u32 2147483647, %v38105
%vm38110 = vcmp.eq.f32.partialorder %v38107, 1.0
%v38115 = vmul.f32 inf, %v38105
%v38117 = vxor.u32 2147483648, %v38105
%v38120 = vmul.f32 %v38117, %v38105
%v38122 = vadd.f32 1.0, %v38120
%v38123 = vlog2.pop %v38122
%v38124 = vmul.f32 0.6931472, %v38123
%v38125 = vmul.f32 -0.5, %v38120
%v38126 = vadd.f32 1.0, %v38125
%v38127 = vmul.f32 %v38126, %v38120
%v38128 = vand.u32 2147483647, %v38120
%vm38129 = vcmp.lt.f32.partialorder %v38128, 0.0004427343
%v38130 = vsel /*vm=*/%vm38129, /*on_true_vy=*/%v38127, /*on_false_vx=*/%v38124
%v38131 = vxor.u32 2147483648, %v38130
%vm38134 = vcmp.lt.f32.partialorder %v38131, 5.0
%v38139 = vsel /*vm=*/%vm38134, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v38143 = vsel /*vm=*/%vm38134, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v38147 = vsel /*vm=*/%vm38134, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v38151 = vsel /*vm=*/%vm38134, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v38155 = vsel /*vm=*/%vm38134, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v38159 = vsel /*vm=*/%vm38134, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v38163 = vsel /*vm=*/%vm38134, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v38167 = vsel /*vm=*/%vm38134, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v38171 = vsel /*vm=*/%vm38134, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v38175 = vadd.f32 -2.5, %v38131
%v38177 = vrsqrt.pop %v38131
%v38178 = vmul.f32 %v38177, %v38131
%vm38179 = vcmp.eq.f32.partialorder %v38131, inf
%v38180 = vsel /*vm=*/%vm38179, /*on_true_vy=*/%v38131, /*on_false_vx=*/%v38178
%vm38181 = vcmp.eq.f32.partialorder %v38131, 0.0
%v38182 = vand.u32 2147483648, %v38131
%v38183 = vsel /*vm=*/%vm38181, /*on_true_vy=*/%v38182, /*on_false_vx=*/%v38180
%v38186 = vadd.f32 -3.0, %v38183
%v38190 = vsel /*vm=*/%vm38134, /*on_true_vy=*/%v38175, /*on_false_vx=*/%v38186
%v38194 = vmul.f32 %v38190, %v38171
%v38198 = vadd.f32 %v38194, %v38167
%v38202 = vmul.f32 %v38198, %v38190
%v38206 = vadd.f32 %v38202, %v38163
%v38210 = vmul.f32 %v38206, %v38190
%v38214 = vadd.f32 %v38210, %v38159
%v38218 = vmul.f32 %v38214, %v38190
%v38222 = vadd.f32 %v38218, %v38155
%v38226 = vmul.f32 %v38222, %v38190
%v38230 = vadd.f32 %v38226, %v38151
%v38234 = vmul.f32 %v38230, %v38190
%v38238 = vadd.f32 %v38234, %v38147
%v38242 = vmul.f32 %v38238, %v38190
%v38246 = vadd.f32 %v38242, %v38143
%v38250 = vmul.f32 %v38246, %v38190
%v38254 = vadd.f32 %v38250, %v38139
%v38258 = vmul.f32 %v38254, %v38105
%v38262 = vsel /*vm=*/%vm38110, /*on_true_vy=*/%v38115, /*on_false_vx=*/%v38258
%v38266 = vmul.f32 1.4140625, %v38262
%v38269 = vpack.c.bf16 %v120417, %v38266
%119949 = vst [vmem:[%s280 + $0x28] sm:$0xf] /*vst_source=*/%v38269
%v38273 = vadd.s32 %v37809, %v894
%v38283 = vadd.s32 %v38273, %v415
%vm38287 = vcmp.lt.u32.totalorder %v38283, %v38273
%vm38292 = vcmp.lt.u32.totalorder %v38273, %v894
%v38297 = vadd.s32 %v37792, %v881
%v38301 = vadd.s32 1, %v38297
%v38305 = vsel /*vm=*/%vm38292, /*on_true_vy=*/%v38301, /*on_false_vx=*/%v38297
%v38309 = vadd.s32 1, %v38305
%v38313 = vsel /*vm=*/%vm38287, /*on_true_vy=*/%v38309, /*on_false_vx=*/%v38305
%v38318 = vadd.s32 %v38313, %v10
%v38322 = vadd.s32 %v38283, %v9
%v38326 = vadd.s32 %v38322, %v38318
%v38328 = vshll.u32 %v38322, 13
%v38329 = vshrl.u32 %v38322, 19
%v38330 = vor.u32 %v38329, %v38328
%v38331 = vxor.u32 %v38330, %v38326
%v38334 = vadd.s32 %v38331, %v38326
%v38336 = vshll.u32 %v38331, 15
%v38337 = vshrl.u32 %v38331, 17
%v38338 = vor.u32 %v38337, %v38336
%v38339 = vxor.u32 %v38338, %v38334
%v38342 = vadd.s32 %v38339, %v38334
%v38344 = vshll.u32 %v38339, 26
%v38345 = vshrl.u32 %v38339, 6
%v38346 = vor.u32 %v38345, %v38344
%v38347 = vxor.u32 %v38346, %v38342
%v38350 = vadd.s32 %v38347, %v38342
%v38354 = vadd.s32 %v38350, %v9
%v38356 = vshll.u32 %v38347, 6
%v38357 = vshrl.u32 %v38347, 26
%v38358 = vor.u32 %v38357, %v38356
%v38359 = vxor.u32 %v38358, %v38350
%v38362 = vadd.s32 %v38359, %v8
%v38366 = vadd.s32 1, %v38362
%v38370 = vadd.s32 %v38366, %v38354
%v38372 = vshll.u32 %v38366, 17
%v38373 = vshrl.u32 %v38366, 15
%v38374 = vor.u32 %v38373, %v38372
%v38375 = vxor.u32 %v38374, %v38370
%v38378 = vadd.s32 %v38375, %v38370
%v38380 = vshll.u32 %v38375, 29
%v38381 = vshrl.u32 %v38375, 3
%v38382 = vor.u32 %v38381, %v38380
%v38383 = vxor.u32 %v38382, %v38378
%v38386 = vadd.s32 %v38383, %v38378
%v38388 = vshll.u32 %v38383, 16
%v38389 = vshrl.u32 %v38383, 16
%v38390 = vor.u32 %v38389, %v38388
%v38391 = vxor.u32 %v38390, %v38386
%v38394 = vadd.s32 %v38391, %v38386
%v38398 = vadd.s32 %v38394, %v8
%v38400 = vshll.u32 %v38391, 24
%v38401 = vshrl.u32 %v38391, 8
%v38402 = vor.u32 %v38401, %v38400
%v38403 = vxor.u32 %v38402, %v38394
%v38406 = vadd.s32 %v38403, %v10
%v38410 = vadd.s32 2, %v38406
%v38414 = vadd.s32 %v38410, %v38398
%v38416 = vshll.u32 %v38410, 13
%v38417 = vshrl.u32 %v38410, 19
%v38418 = vor.u32 %v38417, %v38416
%v38419 = vxor.u32 %v38418, %v38414
%v38422 = vadd.s32 %v38419, %v38414
%v38424 = vshll.u32 %v38419, 15
%v38425 = vshrl.u32 %v38419, 17
%v38426 = vor.u32 %v38425, %v38424
%v38427 = vxor.u32 %v38426, %v38422
%v38430 = vadd.s32 %v38427, %v38422
%v38432 = vshll.u32 %v38427, 26
%v38433 = vshrl.u32 %v38427, 6
%v38434 = vor.u32 %v38433, %v38432
%v38435 = vxor.u32 %v38434, %v38430
%v38438 = vadd.s32 %v38435, %v38430
%v38442 = vadd.s32 %v38438, %v10
%v38444 = vshll.u32 %v38435, 6
%v38445 = vshrl.u32 %v38435, 26
%v38446 = vor.u32 %v38445, %v38444
%v38447 = vxor.u32 %v38446, %v38438
%v38450 = vadd.s32 %v38447, %v9
%v38454 = vadd.s32 3, %v38450
%v38458 = vadd.s32 %v38454, %v38442
%v38460 = vshll.u32 %v38454, 17
%v38461 = vshrl.u32 %v38454, 15
%v38462 = vor.u32 %v38461, %v38460
%v38463 = vxor.u32 %v38462, %v38458
%v38466 = vadd.s32 %v38463, %v38458
%v38468 = vshll.u32 %v38463, 29
%v38469 = vshrl.u32 %v38463, 3
%v38470 = vor.u32 %v38469, %v38468
%v38471 = vxor.u32 %v38470, %v38466
%v38474 = vadd.s32 %v38471, %v38466
%v38476 = vshll.u32 %v38471, 16
%v38477 = vshrl.u32 %v38471, 16
%v38478 = vor.u32 %v38477, %v38476
%v38479 = vxor.u32 %v38478, %v38474
%v38482 = vadd.s32 %v38479, %v38474
%v38486 = vadd.s32 %v38482, %v9
%v38488 = vshll.u32 %v38479, 24
%v38489 = vshrl.u32 %v38479, 8
%v38490 = vor.u32 %v38489, %v38488
%v38491 = vxor.u32 %v38490, %v38482
%v38494 = vadd.s32 %v38491, %v8
%v38498 = vadd.s32 4, %v38494
%v38502 = vadd.s32 %v38498, %v38486
%v38504 = vshll.u32 %v38498, 13
%v38505 = vshrl.u32 %v38498, 19
%v38506 = vor.u32 %v38505, %v38504
%v38507 = vxor.u32 %v38506, %v38502
%v38510 = vadd.s32 %v38507, %v38502
%v38512 = vshll.u32 %v38507, 15
%v38513 = vshrl.u32 %v38507, 17
%v38514 = vor.u32 %v38513, %v38512
%v38515 = vxor.u32 %v38514, %v38510
%v38518 = vadd.s32 %v38515, %v38510
%v38520 = vshll.u32 %v38515, 26
%v38521 = vshrl.u32 %v38515, 6
%v38522 = vor.u32 %v38521, %v38520
%v38523 = vxor.u32 %v38522, %v38518
%v38526 = vadd.s32 %v38523, %v38518
%v38530 = vadd.s32 %v38526, %v8
%v38532 = vshll.u32 %v38523, 6
%v38533 = vshrl.u32 %v38523, 26
%v38534 = vor.u32 %v38533, %v38532
%v38535 = vxor.u32 %v38534, %v38526
%v38538 = vadd.s32 %v38535, %v10
%v38542 = vadd.s32 5, %v38538
%v38544 = vxor.u32 %v38542, %v38530
%v38545 = vand.u32.u8 255, %v38544
%v38546 = vand.u32 65535, %v38545
%v38547 = vshrl.u32 %v38546, 1
%v38548 = vor.u32 16256, %v38547
%v38549 = vand.u32.u16 65535, %v38548
%v119950 = vadd.low.f32.bf16 -1.0, %v38549
%v38558 = vmul.f32 2.0, %v119950
%v38562 = vadd.f32 -0.99609375, %v38558
%v38566 = vmax.f32 %v38562, -0.99609375
%v38568 = vand.u32 2147483647, %v38566
%vm38571 = vcmp.eq.f32.partialorder %v38568, 1.0
%v38576 = vmul.f32 inf, %v38566
%v38578 = vxor.u32 2147483648, %v38566
%v38581 = vmul.f32 %v38578, %v38566
%v38583 = vadd.f32 1.0, %v38581
%v38584 = vlog2.pop %v38583
%v38585 = vmul.f32 0.6931472, %v38584
%v38586 = vmul.f32 -0.5, %v38581
%v38587 = vadd.f32 1.0, %v38586
%v38588 = vmul.f32 %v38587, %v38581
%v38589 = vand.u32 2147483647, %v38581
%vm38590 = vcmp.lt.f32.partialorder %v38589, 0.0004427343
%v38591 = vsel /*vm=*/%vm38590, /*on_true_vy=*/%v38588, /*on_false_vx=*/%v38585
%v38592 = vxor.u32 2147483648, %v38591
%vm38595 = vcmp.lt.f32.partialorder %v38592, 5.0
%v38600 = vsel /*vm=*/%vm38595, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v38604 = vsel /*vm=*/%vm38595, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v38608 = vsel /*vm=*/%vm38595, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v38612 = vsel /*vm=*/%vm38595, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v38616 = vsel /*vm=*/%vm38595, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v38620 = vsel /*vm=*/%vm38595, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v38624 = vsel /*vm=*/%vm38595, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v38628 = vsel /*vm=*/%vm38595, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v38632 = vsel /*vm=*/%vm38595, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v38636 = vadd.f32 -2.5, %v38592
%v38638 = vrsqrt.pop %v38592
%v38639 = vmul.f32 %v38638, %v38592
%vm38640 = vcmp.eq.f32.partialorder %v38592, inf
%v38641 = vsel /*vm=*/%vm38640, /*on_true_vy=*/%v38592, /*on_false_vx=*/%v38639
%vm38642 = vcmp.eq.f32.partialorder %v38592, 0.0
%v38643 = vand.u32 2147483648, %v38592
%v38644 = vsel /*vm=*/%vm38642, /*on_true_vy=*/%v38643, /*on_false_vx=*/%v38641
%v38647 = vadd.f32 -3.0, %v38644
%v38651 = vsel /*vm=*/%vm38595, /*on_true_vy=*/%v38636, /*on_false_vx=*/%v38647
%v38655 = vmul.f32 %v38651, %v38632
%v38659 = vadd.f32 %v38655, %v38628
%v38663 = vmul.f32 %v38659, %v38651
%v38667 = vadd.f32 %v38663, %v38624
%v38671 = vmul.f32 %v38667, %v38651
%v38675 = vadd.f32 %v38671, %v38620
%v38679 = vmul.f32 %v38675, %v38651
%v38683 = vadd.f32 %v38679, %v38616
%v38687 = vmul.f32 %v38683, %v38651
%v38691 = vadd.f32 %v38687, %v38612
%v38695 = vmul.f32 %v38691, %v38651
%v38699 = vadd.f32 %v38695, %v38608
%v38703 = vmul.f32 %v38699, %v38651
%v38707 = vadd.f32 %v38703, %v38604
%v38711 = vmul.f32 %v38707, %v38651
%v38715 = vadd.f32 %v38711, %v38600
%v38719 = vmul.f32 %v38715, %v38566
%v38723 = vsel /*vm=*/%vm38571, /*on_true_vy=*/%v38576, /*on_false_vx=*/%v38719
%v38727 = vmul.f32 1.4140625, %v38723
%v38730 = vpack.c.bf16 %v120417, %v38727
%119951 = vst [vmem:[%s280 + $0xa8] sm:$0xf] /*vst_source=*/%v38730
%v38734 = vadd.s32 %v37809, %v1381
%v38744 = vadd.s32 %v38734, %v415
%vm38748 = vcmp.lt.u32.totalorder %v38744, %v38734
%vm38753 = vcmp.lt.u32.totalorder %v38734, %v1381
%v38758 = vadd.s32 %v37792, %v1368
%v38762 = vadd.s32 1, %v38758
%v38766 = vsel /*vm=*/%vm38753, /*on_true_vy=*/%v38762, /*on_false_vx=*/%v38758
%v38770 = vadd.s32 1, %v38766
%v38774 = vsel /*vm=*/%vm38748, /*on_true_vy=*/%v38770, /*on_false_vx=*/%v38766
%v38779 = vadd.s32 %v38774, %v10
%v38783 = vadd.s32 %v38744, %v9
%v38787 = vadd.s32 %v38783, %v38779
%v38789 = vshll.u32 %v38783, 13
%v38790 = vshrl.u32 %v38783, 19
%v38791 = vor.u32 %v38790, %v38789
%v38792 = vxor.u32 %v38791, %v38787
%v38795 = vadd.s32 %v38792, %v38787
%v38797 = vshll.u32 %v38792, 15
%v38798 = vshrl.u32 %v38792, 17
%v38799 = vor.u32 %v38798, %v38797
%v38800 = vxor.u32 %v38799, %v38795
%v38803 = vadd.s32 %v38800, %v38795
%v38805 = vshll.u32 %v38800, 26
%v38806 = vshrl.u32 %v38800, 6
%v38807 = vor.u32 %v38806, %v38805
%v38808 = vxor.u32 %v38807, %v38803
%v38811 = vadd.s32 %v38808, %v38803
%v38815 = vadd.s32 %v38811, %v9
%v38817 = vshll.u32 %v38808, 6
%v38818 = vshrl.u32 %v38808, 26
%v38819 = vor.u32 %v38818, %v38817
%v38820 = vxor.u32 %v38819, %v38811
%v38823 = vadd.s32 %v38820, %v8
%v38827 = vadd.s32 1, %v38823
%v38831 = vadd.s32 %v38827, %v38815
%v38833 = vshll.u32 %v38827, 17
%v38834 = vshrl.u32 %v38827, 15
%v38835 = vor.u32 %v38834, %v38833
%v38836 = vxor.u32 %v38835, %v38831
%v38839 = vadd.s32 %v38836, %v38831
%v38841 = vshll.u32 %v38836, 29
%v38842 = vshrl.u32 %v38836, 3
%v38843 = vor.u32 %v38842, %v38841
%v38844 = vxor.u32 %v38843, %v38839
%v38847 = vadd.s32 %v38844, %v38839
%v38849 = vshll.u32 %v38844, 16
%v38850 = vshrl.u32 %v38844, 16
%v38851 = vor.u32 %v38850, %v38849
%v38852 = vxor.u32 %v38851, %v38847
%v38855 = vadd.s32 %v38852, %v38847
%v38859 = vadd.s32 %v38855, %v8
%v38861 = vshll.u32 %v38852, 24
%v38862 = vshrl.u32 %v38852, 8
%v38863 = vor.u32 %v38862, %v38861
%v38864 = vxor.u32 %v38863, %v38855
%v38867 = vadd.s32 %v38864, %v10
%v38871 = vadd.s32 2, %v38867
%v38875 = vadd.s32 %v38871, %v38859
%v38877 = vshll.u32 %v38871, 13
%v38878 = vshrl.u32 %v38871, 19
%v38879 = vor.u32 %v38878, %v38877
%v38880 = vxor.u32 %v38879, %v38875
%v38883 = vadd.s32 %v38880, %v38875
%v38885 = vshll.u32 %v38880, 15
%v38886 = vshrl.u32 %v38880, 17
%v38887 = vor.u32 %v38886, %v38885
%v38888 = vxor.u32 %v38887, %v38883
%v38891 = vadd.s32 %v38888, %v38883
%v38893 = vshll.u32 %v38888, 26
%v38894 = vshrl.u32 %v38888, 6
%v38895 = vor.u32 %v38894, %v38893
%v38896 = vxor.u32 %v38895, %v38891
%v38899 = vadd.s32 %v38896, %v38891
%v38903 = vadd.s32 %v38899, %v10
%v38905 = vshll.u32 %v38896, 6
%v38906 = vshrl.u32 %v38896, 26
%v38907 = vor.u32 %v38906, %v38905
%v38908 = vxor.u32 %v38907, %v38899
%v38911 = vadd.s32 %v38908, %v9
%v38915 = vadd.s32 3, %v38911
%v38919 = vadd.s32 %v38915, %v38903
%v38921 = vshll.u32 %v38915, 17
%v38922 = vshrl.u32 %v38915, 15
%v38923 = vor.u32 %v38922, %v38921
%v38924 = vxor.u32 %v38923, %v38919
%v38927 = vadd.s32 %v38924, %v38919
%v38929 = vshll.u32 %v38924, 29
%v38930 = vshrl.u32 %v38924, 3
%v38931 = vor.u32 %v38930, %v38929
%v38932 = vxor.u32 %v38931, %v38927
%v38935 = vadd.s32 %v38932, %v38927
%v38937 = vshll.u32 %v38932, 16
%v38938 = vshrl.u32 %v38932, 16
%v38939 = vor.u32 %v38938, %v38937
%v38940 = vxor.u32 %v38939, %v38935
%v38943 = vadd.s32 %v38940, %v38935
%v38947 = vadd.s32 %v38943, %v9
%v38949 = vshll.u32 %v38940, 24
%v38950 = vshrl.u32 %v38940, 8
%v38951 = vor.u32 %v38950, %v38949
%v38952 = vxor.u32 %v38951, %v38943
%v38955 = vadd.s32 %v38952, %v8
%v38959 = vadd.s32 4, %v38955
%v38963 = vadd.s32 %v38959, %v38947
%v38965 = vshll.u32 %v38959, 13
%v38966 = vshrl.u32 %v38959, 19
%v38967 = vor.u32 %v38966, %v38965
%v38968 = vxor.u32 %v38967, %v38963
%v38971 = vadd.s32 %v38968, %v38963
%v38973 = vshll.u32 %v38968, 15
%v38974 = vshrl.u32 %v38968, 17
%v38975 = vor.u32 %v38974, %v38973
%v38976 = vxor.u32 %v38975, %v38971
%v38979 = vadd.s32 %v38976, %v38971
%v38981 = vshll.u32 %v38976, 26
%v38982 = vshrl.u32 %v38976, 6
%v38983 = vor.u32 %v38982, %v38981
%v38984 = vxor.u32 %v38983, %v38979
%v38987 = vadd.s32 %v38984, %v38979
%v38991 = vadd.s32 %v38987, %v8
%v38993 = vshll.u32 %v38984, 6
%v38994 = vshrl.u32 %v38984, 26
%v38995 = vor.u32 %v38994, %v38993
%v38996 = vxor.u32 %v38995, %v38987
%v38999 = vadd.s32 %v38996, %v10
%v39003 = vadd.s32 5, %v38999
%v39005 = vxor.u32 %v39003, %v38991
%v39006 = vand.u32.u8 255, %v39005
%v39007 = vand.u32 65535, %v39006
%v39008 = vshrl.u32 %v39007, 1
%v39009 = vor.u32 16256, %v39008
%v39010 = vand.u32.u16 65535, %v39009
%v119952 = vadd.low.f32.bf16 -1.0, %v39010
%v39019 = vmul.f32 2.0, %v119952
%v39023 = vadd.f32 -0.99609375, %v39019
%v39027 = vmax.f32 %v39023, -0.99609375
%v39029 = vand.u32 2147483647, %v39027
%vm39032 = vcmp.eq.f32.partialorder %v39029, 1.0
%v39037 = vmul.f32 inf, %v39027
%v39039 = vxor.u32 2147483648, %v39027
%v39042 = vmul.f32 %v39039, %v39027
%v39044 = vadd.f32 1.0, %v39042
%v39045 = vlog2.pop %v39044
%v39046 = vmul.f32 0.6931472, %v39045
%v39047 = vmul.f32 -0.5, %v39042
%v39048 = vadd.f32 1.0, %v39047
%v39049 = vmul.f32 %v39048, %v39042
%v39050 = vand.u32 2147483647, %v39042
%vm39051 = vcmp.lt.f32.partialorder %v39050, 0.0004427343
%v39052 = vsel /*vm=*/%vm39051, /*on_true_vy=*/%v39049, /*on_false_vx=*/%v39046
%v39053 = vxor.u32 2147483648, %v39052
%vm39056 = vcmp.lt.f32.partialorder %v39053, 5.0
%v39061 = vsel /*vm=*/%vm39056, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v39065 = vsel /*vm=*/%vm39056, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v39069 = vsel /*vm=*/%vm39056, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v39073 = vsel /*vm=*/%vm39056, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v39077 = vsel /*vm=*/%vm39056, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v39081 = vsel /*vm=*/%vm39056, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v39085 = vsel /*vm=*/%vm39056, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v39089 = vsel /*vm=*/%vm39056, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v39093 = vsel /*vm=*/%vm39056, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v39097 = vadd.f32 -2.5, %v39053
%v39099 = vrsqrt.pop %v39053
%v39100 = vmul.f32 %v39099, %v39053
%vm39101 = vcmp.eq.f32.partialorder %v39053, inf
%v39102 = vsel /*vm=*/%vm39101, /*on_true_vy=*/%v39053, /*on_false_vx=*/%v39100
%vm39103 = vcmp.eq.f32.partialorder %v39053, 0.0
%v39104 = vand.u32 2147483648, %v39053
%v39105 = vsel /*vm=*/%vm39103, /*on_true_vy=*/%v39104, /*on_false_vx=*/%v39102
%v39108 = vadd.f32 -3.0, %v39105
%v39112 = vsel /*vm=*/%vm39056, /*on_true_vy=*/%v39097, /*on_false_vx=*/%v39108
%v39116 = vmul.f32 %v39112, %v39093
%v39120 = vadd.f32 %v39116, %v39089
%v39124 = vmul.f32 %v39120, %v39112
%v39128 = vadd.f32 %v39124, %v39085
%v39132 = vmul.f32 %v39128, %v39112
%v39136 = vadd.f32 %v39132, %v39081
%v39140 = vmul.f32 %v39136, %v39112
%v39144 = vadd.f32 %v39140, %v39077
%v39148 = vmul.f32 %v39144, %v39112
%v39152 = vadd.f32 %v39148, %v39073
%v39156 = vmul.f32 %v39152, %v39112
%v39160 = vadd.f32 %v39156, %v39069
%v39164 = vmul.f32 %v39160, %v39112
%v39168 = vadd.f32 %v39164, %v39065
%v39172 = vmul.f32 %v39168, %v39112
%v39176 = vadd.f32 %v39172, %v39061
%v39180 = vmul.f32 %v39176, %v39027
%v39184 = vsel /*vm=*/%vm39032, /*on_true_vy=*/%v39037, /*on_false_vx=*/%v39180
%v39188 = vmul.f32 1.4140625, %v39184
%v39191 = vpack.c.bf16 %v120417, %v39188
%119953 = vst [vmem:[%s280 + $0x128] sm:$0xf] /*vst_source=*/%v39191
%v39195 = vadd.s32 %v37809, %v1868
%v39205 = vadd.s32 %v39195, %v415
%vm39209 = vcmp.lt.u32.totalorder %v39205, %v39195
%vm39214 = vcmp.lt.u32.totalorder %v39195, %v1868
%v39219 = vadd.s32 %v37792, %v1855
%v39223 = vadd.s32 1, %v39219
%v39227 = vsel /*vm=*/%vm39214, /*on_true_vy=*/%v39223, /*on_false_vx=*/%v39219
%v39231 = vadd.s32 1, %v39227
%v39235 = vsel /*vm=*/%vm39209, /*on_true_vy=*/%v39231, /*on_false_vx=*/%v39227
%v39240 = vadd.s32 %v39235, %v10
%v39244 = vadd.s32 %v39205, %v9
%v39248 = vadd.s32 %v39244, %v39240
%v39250 = vshll.u32 %v39244, 13
%v39251 = vshrl.u32 %v39244, 19
%v39252 = vor.u32 %v39251, %v39250
%v39253 = vxor.u32 %v39252, %v39248
%v39256 = vadd.s32 %v39253, %v39248
%v39258 = vshll.u32 %v39253, 15
%v39259 = vshrl.u32 %v39253, 17
%v39260 = vor.u32 %v39259, %v39258
%v39261 = vxor.u32 %v39260, %v39256
%v39264 = vadd.s32 %v39261, %v39256
%v39266 = vshll.u32 %v39261, 26
%v39267 = vshrl.u32 %v39261, 6
%v39268 = vor.u32 %v39267, %v39266
%v39269 = vxor.u32 %v39268, %v39264
%v39272 = vadd.s32 %v39269, %v39264
%v39276 = vadd.s32 %v39272, %v9
%v39278 = vshll.u32 %v39269, 6
%v39279 = vshrl.u32 %v39269, 26
%v39280 = vor.u32 %v39279, %v39278
%v39281 = vxor.u32 %v39280, %v39272
%v39284 = vadd.s32 %v39281, %v8
%v39288 = vadd.s32 1, %v39284
%v39292 = vadd.s32 %v39288, %v39276
%v39294 = vshll.u32 %v39288, 17
%v39295 = vshrl.u32 %v39288, 15
%v39296 = vor.u32 %v39295, %v39294
%v39297 = vxor.u32 %v39296, %v39292
%v39300 = vadd.s32 %v39297, %v39292
%v39302 = vshll.u32 %v39297, 29
%v39303 = vshrl.u32 %v39297, 3
%v39304 = vor.u32 %v39303, %v39302
%v39305 = vxor.u32 %v39304, %v39300
%v39308 = vadd.s32 %v39305, %v39300
%v39310 = vshll.u32 %v39305, 16
%v39311 = vshrl.u32 %v39305, 16
%v39312 = vor.u32 %v39311, %v39310
%v39313 = vxor.u32 %v39312, %v39308
%v39316 = vadd.s32 %v39313, %v39308
%v39320 = vadd.s32 %v39316, %v8
%v39322 = vshll.u32 %v39313, 24
%v39323 = vshrl.u32 %v39313, 8
%v39324 = vor.u32 %v39323, %v39322
%v39325 = vxor.u32 %v39324, %v39316
%v39328 = vadd.s32 %v39325, %v10
%v39332 = vadd.s32 2, %v39328
%v39336 = vadd.s32 %v39332, %v39320
%v39338 = vshll.u32 %v39332, 13
%v39339 = vshrl.u32 %v39332, 19
%v39340 = vor.u32 %v39339, %v39338
%v39341 = vxor.u32 %v39340, %v39336
%v39344 = vadd.s32 %v39341, %v39336
%v39346 = vshll.u32 %v39341, 15
%v39347 = vshrl.u32 %v39341, 17
%v39348 = vor.u32 %v39347, %v39346
%v39349 = vxor.u32 %v39348, %v39344
%v39352 = vadd.s32 %v39349, %v39344
%v39354 = vshll.u32 %v39349, 26
%v39355 = vshrl.u32 %v39349, 6
%v39356 = vor.u32 %v39355, %v39354
%v39357 = vxor.u32 %v39356, %v39352
%v39360 = vadd.s32 %v39357, %v39352
%v39364 = vadd.s32 %v39360, %v10
%v39366 = vshll.u32 %v39357, 6
%v39367 = vshrl.u32 %v39357, 26
%v39368 = vor.u32 %v39367, %v39366
%v39369 = vxor.u32 %v39368, %v39360
%v39372 = vadd.s32 %v39369, %v9
%v39376 = vadd.s32 3, %v39372
%v39380 = vadd.s32 %v39376, %v39364
%v39382 = vshll.u32 %v39376, 17
%v39383 = vshrl.u32 %v39376, 15
%v39384 = vor.u32 %v39383, %v39382
%v39385 = vxor.u32 %v39384, %v39380
%v39388 = vadd.s32 %v39385, %v39380
%v39390 = vshll.u32 %v39385, 29
%v39391 = vshrl.u32 %v39385, 3
%v39392 = vor.u32 %v39391, %v39390
%v39393 = vxor.u32 %v39392, %v39388
%v39396 = vadd.s32 %v39393, %v39388
%v39398 = vshll.u32 %v39393, 16
%v39399 = vshrl.u32 %v39393, 16
%v39400 = vor.u32 %v39399, %v39398
%v39401 = vxor.u32 %v39400, %v39396
%v39404 = vadd.s32 %v39401, %v39396
%v39408 = vadd.s32 %v39404, %v9
%v39410 = vshll.u32 %v39401, 24
%v39411 = vshrl.u32 %v39401, 8
%v39412 = vor.u32 %v39411, %v39410
%v39413 = vxor.u32 %v39412, %v39404
%v39416 = vadd.s32 %v39413, %v8
%v39420 = vadd.s32 4, %v39416
%v39424 = vadd.s32 %v39420, %v39408
%v39426 = vshll.u32 %v39420, 13
%v39427 = vshrl.u32 %v39420, 19
%v39428 = vor.u32 %v39427, %v39426
%v39429 = vxor.u32 %v39428, %v39424
%v39432 = vadd.s32 %v39429, %v39424
%v39434 = vshll.u32 %v39429, 15
%v39435 = vshrl.u32 %v39429, 17
%v39436 = vor.u32 %v39435, %v39434
%v39437 = vxor.u32 %v39436, %v39432
%v39440 = vadd.s32 %v39437, %v39432
%v39442 = vshll.u32 %v39437, 26
%v39443 = vshrl.u32 %v39437, 6
%v39444 = vor.u32 %v39443, %v39442
%v39445 = vxor.u32 %v39444, %v39440
%v39448 = vadd.s32 %v39445, %v39440
%v39452 = vadd.s32 %v39448, %v8
%v39454 = vshll.u32 %v39445, 6
%v39455 = vshrl.u32 %v39445, 26
%v39456 = vor.u32 %v39455, %v39454
%v39457 = vxor.u32 %v39456, %v39448
%v39460 = vadd.s32 %v39457, %v10
%v39464 = vadd.s32 5, %v39460
%v39466 = vxor.u32 %v39464, %v39452
%v39467 = vand.u32.u8 255, %v39466
%v39468 = vand.u32 65535, %v39467
%v39469 = vshrl.u32 %v39468, 1
%v39470 = vor.u32 16256, %v39469
%v39471 = vand.u32.u16 65535, %v39470
%v119954 = vadd.low.f32.bf16 -1.0, %v39471
%v39480 = vmul.f32 2.0, %v119954
%v39484 = vadd.f32 -0.99609375, %v39480
%v39488 = vmax.f32 %v39484, -0.99609375
%v39490 = vand.u32 2147483647, %v39488
%vm39493 = vcmp.eq.f32.partialorder %v39490, 1.0
%v39498 = vmul.f32 inf, %v39488
%v39500 = vxor.u32 2147483648, %v39488
%v39503 = vmul.f32 %v39500, %v39488
%v39505 = vadd.f32 1.0, %v39503
%v39506 = vlog2.pop %v39505
%v39507 = vmul.f32 0.6931472, %v39506
%v39508 = vmul.f32 -0.5, %v39503
%v39509 = vadd.f32 1.0, %v39508
%v39510 = vmul.f32 %v39509, %v39503
%v39511 = vand.u32 2147483647, %v39503
%vm39512 = vcmp.lt.f32.partialorder %v39511, 0.0004427343
%v39513 = vsel /*vm=*/%vm39512, /*on_true_vy=*/%v39510, /*on_false_vx=*/%v39507
%v39514 = vxor.u32 2147483648, %v39513
%vm39517 = vcmp.lt.f32.partialorder %v39514, 5.0
%v39522 = vsel /*vm=*/%vm39517, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v39526 = vsel /*vm=*/%vm39517, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v39530 = vsel /*vm=*/%vm39517, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v39534 = vsel /*vm=*/%vm39517, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v39538 = vsel /*vm=*/%vm39517, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v39542 = vsel /*vm=*/%vm39517, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v39546 = vsel /*vm=*/%vm39517, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v39550 = vsel /*vm=*/%vm39517, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v39554 = vsel /*vm=*/%vm39517, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v39558 = vadd.f32 -2.5, %v39514
%v39560 = vrsqrt.pop %v39514
%v39561 = vmul.f32 %v39560, %v39514
%vm39562 = vcmp.eq.f32.partialorder %v39514, inf
%v39563 = vsel /*vm=*/%vm39562, /*on_true_vy=*/%v39514, /*on_false_vx=*/%v39561
%vm39564 = vcmp.eq.f32.partialorder %v39514, 0.0
%v39565 = vand.u32 2147483648, %v39514
%v39566 = vsel /*vm=*/%vm39564, /*on_true_vy=*/%v39565, /*on_false_vx=*/%v39563
%v39569 = vadd.f32 -3.0, %v39566
%v39573 = vsel /*vm=*/%vm39517, /*on_true_vy=*/%v39558, /*on_false_vx=*/%v39569
%v39577 = vmul.f32 %v39573, %v39554
%v39581 = vadd.f32 %v39577, %v39550
%v39585 = vmul.f32 %v39581, %v39573
%v39589 = vadd.f32 %v39585, %v39546
%v39593 = vmul.f32 %v39589, %v39573
%v39597 = vadd.f32 %v39593, %v39542
%v39601 = vmul.f32 %v39597, %v39573
%v39605 = vadd.f32 %v39601, %v39538
%v39609 = vmul.f32 %v39605, %v39573
%v39613 = vadd.f32 %v39609, %v39534
%v39617 = vmul.f32 %v39613, %v39573
%v39621 = vadd.f32 %v39617, %v39530
%v39625 = vmul.f32 %v39621, %v39573
%v39629 = vadd.f32 %v39625, %v39526
%v39633 = vmul.f32 %v39629, %v39573
%v39637 = vadd.f32 %v39633, %v39522
%v39641 = vmul.f32 %v39637, %v39488
%v39645 = vsel /*vm=*/%vm39493, /*on_true_vy=*/%v39498, /*on_false_vx=*/%v39641
%v39649 = vmul.f32 1.4140625, %v39645
%v39652 = vpack.c.bf16 %v120417, %v39649
%119955 = vst [vmem:[%s280 + $0x1a8] sm:$0xf] /*vst_source=*/%v39652
%v39656 = vadd.s32 %v37809, %v2355
%v39666 = vadd.s32 %v39656, %v415
%vm39670 = vcmp.lt.u32.totalorder %v39666, %v39656
%vm39675 = vcmp.lt.u32.totalorder %v39656, %v2355
%v39680 = vadd.s32 %v37792, %v2342
%v39684 = vadd.s32 1, %v39680
%v39688 = vsel /*vm=*/%vm39675, /*on_true_vy=*/%v39684, /*on_false_vx=*/%v39680
%v39692 = vadd.s32 1, %v39688
%v39696 = vsel /*vm=*/%vm39670, /*on_true_vy=*/%v39692, /*on_false_vx=*/%v39688
%v39701 = vadd.s32 %v39696, %v10
%v39705 = vadd.s32 %v39666, %v9
%v39709 = vadd.s32 %v39705, %v39701
%v39711 = vshll.u32 %v39705, 13
%v39712 = vshrl.u32 %v39705, 19
%v39713 = vor.u32 %v39712, %v39711
%v39714 = vxor.u32 %v39713, %v39709
%v39717 = vadd.s32 %v39714, %v39709
%v39719 = vshll.u32 %v39714, 15
%v39720 = vshrl.u32 %v39714, 17
%v39721 = vor.u32 %v39720, %v39719
%v39722 = vxor.u32 %v39721, %v39717
%v39725 = vadd.s32 %v39722, %v39717
%v39727 = vshll.u32 %v39722, 26
%v39728 = vshrl.u32 %v39722, 6
%v39729 = vor.u32 %v39728, %v39727
%v39730 = vxor.u32 %v39729, %v39725
%v39733 = vadd.s32 %v39730, %v39725
%v39737 = vadd.s32 %v39733, %v9
%v39739 = vshll.u32 %v39730, 6
%v39740 = vshrl.u32 %v39730, 26
%v39741 = vor.u32 %v39740, %v39739
%v39742 = vxor.u32 %v39741, %v39733
%v39745 = vadd.s32 %v39742, %v8
%v39749 = vadd.s32 1, %v39745
%v39753 = vadd.s32 %v39749, %v39737
%v39755 = vshll.u32 %v39749, 17
%v39756 = vshrl.u32 %v39749, 15
%v39757 = vor.u32 %v39756, %v39755
%v39758 = vxor.u32 %v39757, %v39753
%v39761 = vadd.s32 %v39758, %v39753
%v39763 = vshll.u32 %v39758, 29
%v39764 = vshrl.u32 %v39758, 3
%v39765 = vor.u32 %v39764, %v39763
%v39766 = vxor.u32 %v39765, %v39761
%v39769 = vadd.s32 %v39766, %v39761
%v39771 = vshll.u32 %v39766, 16
%v39772 = vshrl.u32 %v39766, 16
%v39773 = vor.u32 %v39772, %v39771
%v39774 = vxor.u32 %v39773, %v39769
%v39777 = vadd.s32 %v39774, %v39769
%v39781 = vadd.s32 %v39777, %v8
%v39783 = vshll.u32 %v39774, 24
%v39784 = vshrl.u32 %v39774, 8
%v39785 = vor.u32 %v39784, %v39783
%v39786 = vxor.u32 %v39785, %v39777
%v39789 = vadd.s32 %v39786, %v10
%v39793 = vadd.s32 2, %v39789
%v39797 = vadd.s32 %v39793, %v39781
%v39799 = vshll.u32 %v39793, 13
%v39800 = vshrl.u32 %v39793, 19
%v39801 = vor.u32 %v39800, %v39799
%v39802 = vxor.u32 %v39801, %v39797
%v39805 = vadd.s32 %v39802, %v39797
%v39807 = vshll.u32 %v39802, 15
%v39808 = vshrl.u32 %v39802, 17
%v39809 = vor.u32 %v39808, %v39807
%v39810 = vxor.u32 %v39809, %v39805
%v39813 = vadd.s32 %v39810, %v39805
%v39815 = vshll.u32 %v39810, 26
%v39816 = vshrl.u32 %v39810, 6
%v39817 = vor.u32 %v39816, %v39815
%v39818 = vxor.u32 %v39817, %v39813
%v39821 = vadd.s32 %v39818, %v39813
%v39825 = vadd.s32 %v39821, %v10
%v39827 = vshll.u32 %v39818, 6
%v39828 = vshrl.u32 %v39818, 26
%v39829 = vor.u32 %v39828, %v39827
%v39830 = vxor.u32 %v39829, %v39821
%v39833 = vadd.s32 %v39830, %v9
%v39837 = vadd.s32 3, %v39833
%v39841 = vadd.s32 %v39837, %v39825
%v39843 = vshll.u32 %v39837, 17
%v39844 = vshrl.u32 %v39837, 15
%v39845 = vor.u32 %v39844, %v39843
%v39846 = vxor.u32 %v39845, %v39841
%v39849 = vadd.s32 %v39846, %v39841
%v39851 = vshll.u32 %v39846, 29
%v39852 = vshrl.u32 %v39846, 3
%v39853 = vor.u32 %v39852, %v39851
%v39854 = vxor.u32 %v39853, %v39849
%v39857 = vadd.s32 %v39854, %v39849
%v39859 = vshll.u32 %v39854, 16
%v39860 = vshrl.u32 %v39854, 16
%v39861 = vor.u32 %v39860, %v39859
%v39862 = vxor.u32 %v39861, %v39857
%v39865 = vadd.s32 %v39862, %v39857
%v39869 = vadd.s32 %v39865, %v9
%v39871 = vshll.u32 %v39862, 24
%v39872 = vshrl.u32 %v39862, 8
%v39873 = vor.u32 %v39872, %v39871
%v39874 = vxor.u32 %v39873, %v39865
%v39877 = vadd.s32 %v39874, %v8
%v39881 = vadd.s32 4, %v39877
%v39885 = vadd.s32 %v39881, %v39869
%v39887 = vshll.u32 %v39881, 13
%v39888 = vshrl.u32 %v39881, 19
%v39889 = vor.u32 %v39888, %v39887
%v39890 = vxor.u32 %v39889, %v39885
%v39893 = vadd.s32 %v39890, %v39885
%v39895 = vshll.u32 %v39890, 15
%v39896 = vshrl.u32 %v39890, 17
%v39897 = vor.u32 %v39896, %v39895
%v39898 = vxor.u32 %v39897, %v39893
%v39901 = vadd.s32 %v39898, %v39893
%v39903 = vshll.u32 %v39898, 26
%v39904 = vshrl.u32 %v39898, 6
%v39905 = vor.u32 %v39904, %v39903
%v39906 = vxor.u32 %v39905, %v39901
%v39909 = vadd.s32 %v39906, %v39901
%v39913 = vadd.s32 %v39909, %v8
%v39915 = vshll.u32 %v39906, 6
%v39916 = vshrl.u32 %v39906, 26
%v39917 = vor.u32 %v39916, %v39915
%v39918 = vxor.u32 %v39917, %v39909
%v39921 = vadd.s32 %v39918, %v10
%v39925 = vadd.s32 5, %v39921
%v39927 = vxor.u32 %v39925, %v39913
%v39928 = vand.u32.u8 255, %v39927
%v39929 = vand.u32 65535, %v39928
%v39930 = vshrl.u32 %v39929, 1
%v39931 = vor.u32 16256, %v39930
%v39932 = vand.u32.u16 65535, %v39931
%v119956 = vadd.low.f32.bf16 -1.0, %v39932
%v39941 = vmul.f32 2.0, %v119956
%v39945 = vadd.f32 -0.99609375, %v39941
%v39949 = vmax.f32 %v39945, -0.99609375
%v39951 = vand.u32 2147483647, %v39949
%vm39954 = vcmp.eq.f32.partialorder %v39951, 1.0
%v39959 = vmul.f32 inf, %v39949
%v39961 = vxor.u32 2147483648, %v39949
%v39964 = vmul.f32 %v39961, %v39949
%v39966 = vadd.f32 1.0, %v39964
%v39967 = vlog2.pop %v39966
%v39968 = vmul.f32 0.6931472, %v39967
%v39969 = vmul.f32 -0.5, %v39964
%v39970 = vadd.f32 1.0, %v39969
%v39971 = vmul.f32 %v39970, %v39964
%v39972 = vand.u32 2147483647, %v39964
%vm39973 = vcmp.lt.f32.partialorder %v39972, 0.0004427343
%v39974 = vsel /*vm=*/%vm39973, /*on_true_vy=*/%v39971, /*on_false_vx=*/%v39968
%v39975 = vxor.u32 2147483648, %v39974
%vm39978 = vcmp.lt.f32.partialorder %v39975, 5.0
%v39983 = vsel /*vm=*/%vm39978, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v39987 = vsel /*vm=*/%vm39978, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v39991 = vsel /*vm=*/%vm39978, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v39995 = vsel /*vm=*/%vm39978, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v39999 = vsel /*vm=*/%vm39978, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v40003 = vsel /*vm=*/%vm39978, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v40007 = vsel /*vm=*/%vm39978, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v40011 = vsel /*vm=*/%vm39978, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v40015 = vsel /*vm=*/%vm39978, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v40019 = vadd.f32 -2.5, %v39975
%v40021 = vrsqrt.pop %v39975
%v40022 = vmul.f32 %v40021, %v39975
%vm40023 = vcmp.eq.f32.partialorder %v39975, inf
%v40024 = vsel /*vm=*/%vm40023, /*on_true_vy=*/%v39975, /*on_false_vx=*/%v40022
%vm40025 = vcmp.eq.f32.partialorder %v39975, 0.0
%v40026 = vand.u32 2147483648, %v39975
%v40027 = vsel /*vm=*/%vm40025, /*on_true_vy=*/%v40026, /*on_false_vx=*/%v40024
%v40030 = vadd.f32 -3.0, %v40027
%v40034 = vsel /*vm=*/%vm39978, /*on_true_vy=*/%v40019, /*on_false_vx=*/%v40030
%v40038 = vmul.f32 %v40034, %v40015
%v40042 = vadd.f32 %v40038, %v40011
%v40046 = vmul.f32 %v40042, %v40034
%v40050 = vadd.f32 %v40046, %v40007
%v40054 = vmul.f32 %v40050, %v40034
%v40058 = vadd.f32 %v40054, %v40003
%v40062 = vmul.f32 %v40058, %v40034
%v40066 = vadd.f32 %v40062, %v39999
%v40070 = vmul.f32 %v40066, %v40034
%v40074 = vadd.f32 %v40070, %v39995
%v40078 = vmul.f32 %v40074, %v40034
%v40082 = vadd.f32 %v40078, %v39991
%v40086 = vmul.f32 %v40082, %v40034
%v40090 = vadd.f32 %v40086, %v39987
%v40094 = vmul.f32 %v40090, %v40034
%v40098 = vadd.f32 %v40094, %v39983
%v40102 = vmul.f32 %v40098, %v39949
%v40106 = vsel /*vm=*/%vm39954, /*on_true_vy=*/%v39959, /*on_false_vx=*/%v40102
%v40110 = vmul.f32 1.4140625, %v40106
%v40113 = vpack.c.bf16 %v120417, %v40110
%119957 = vst [vmem:[%s280 + $0x228] sm:$0xf] /*vst_source=*/%v40113
%v40117 = vadd.s32 %v37809, %v2842
%v40127 = vadd.s32 %v40117, %v415
%vm40131 = vcmp.lt.u32.totalorder %v40127, %v40117
%vm40136 = vcmp.lt.u32.totalorder %v40117, %v2842
%v40141 = vadd.s32 %v37792, %v2829
%v40145 = vadd.s32 1, %v40141
%v40149 = vsel /*vm=*/%vm40136, /*on_true_vy=*/%v40145, /*on_false_vx=*/%v40141
%v40153 = vadd.s32 1, %v40149
%v40157 = vsel /*vm=*/%vm40131, /*on_true_vy=*/%v40153, /*on_false_vx=*/%v40149
%v40162 = vadd.s32 %v40157, %v10
%v40166 = vadd.s32 %v40127, %v9
%v40170 = vadd.s32 %v40166, %v40162
%v40172 = vshll.u32 %v40166, 13
%v40173 = vshrl.u32 %v40166, 19
%v40174 = vor.u32 %v40173, %v40172
%v40175 = vxor.u32 %v40174, %v40170
%v40178 = vadd.s32 %v40175, %v40170
%v40180 = vshll.u32 %v40175, 15
%v40181 = vshrl.u32 %v40175, 17
%v40182 = vor.u32 %v40181, %v40180
%v40183 = vxor.u32 %v40182, %v40178
%v40186 = vadd.s32 %v40183, %v40178
%v40188 = vshll.u32 %v40183, 26
%v40189 = vshrl.u32 %v40183, 6
%v40190 = vor.u32 %v40189, %v40188
%v40191 = vxor.u32 %v40190, %v40186
%v40194 = vadd.s32 %v40191, %v40186
%v40198 = vadd.s32 %v40194, %v9
%v40200 = vshll.u32 %v40191, 6
%v40201 = vshrl.u32 %v40191, 26
%v40202 = vor.u32 %v40201, %v40200
%v40203 = vxor.u32 %v40202, %v40194
%v40206 = vadd.s32 %v40203, %v8
%v40210 = vadd.s32 1, %v40206
%v40214 = vadd.s32 %v40210, %v40198
%v40216 = vshll.u32 %v40210, 17
%v40217 = vshrl.u32 %v40210, 15
%v40218 = vor.u32 %v40217, %v40216
%v40219 = vxor.u32 %v40218, %v40214
%v40222 = vadd.s32 %v40219, %v40214
%v40224 = vshll.u32 %v40219, 29
%v40225 = vshrl.u32 %v40219, 3
%v40226 = vor.u32 %v40225, %v40224
%v40227 = vxor.u32 %v40226, %v40222
%v40230 = vadd.s32 %v40227, %v40222
%v40232 = vshll.u32 %v40227, 16
%v40233 = vshrl.u32 %v40227, 16
%v40234 = vor.u32 %v40233, %v40232
%v40235 = vxor.u32 %v40234, %v40230
%v40238 = vadd.s32 %v40235, %v40230
%v40242 = vadd.s32 %v40238, %v8
%v40244 = vshll.u32 %v40235, 24
%v40245 = vshrl.u32 %v40235, 8
%v40246 = vor.u32 %v40245, %v40244
%v40247 = vxor.u32 %v40246, %v40238
%v40250 = vadd.s32 %v40247, %v10
%v40254 = vadd.s32 2, %v40250
%v40258 = vadd.s32 %v40254, %v40242
%v40260 = vshll.u32 %v40254, 13
%v40261 = vshrl.u32 %v40254, 19
%v40262 = vor.u32 %v40261, %v40260
%v40263 = vxor.u32 %v40262, %v40258
%v40266 = vadd.s32 %v40263, %v40258
%v40268 = vshll.u32 %v40263, 15
%v40269 = vshrl.u32 %v40263, 17
%v40270 = vor.u32 %v40269, %v40268
%v40271 = vxor.u32 %v40270, %v40266
%v40274 = vadd.s32 %v40271, %v40266
%v40276 = vshll.u32 %v40271, 26
%v40277 = vshrl.u32 %v40271, 6
%v40278 = vor.u32 %v40277, %v40276
%v40279 = vxor.u32 %v40278, %v40274
%v40282 = vadd.s32 %v40279, %v40274
%v40286 = vadd.s32 %v40282, %v10
%v40288 = vshll.u32 %v40279, 6
%v40289 = vshrl.u32 %v40279, 26
%v40290 = vor.u32 %v40289, %v40288
%v40291 = vxor.u32 %v40290, %v40282
%v40294 = vadd.s32 %v40291, %v9
%v40298 = vadd.s32 3, %v40294
%v40302 = vadd.s32 %v40298, %v40286
%v40304 = vshll.u32 %v40298, 17
%v40305 = vshrl.u32 %v40298, 15
%v40306 = vor.u32 %v40305, %v40304
%v40307 = vxor.u32 %v40306, %v40302
%v40310 = vadd.s32 %v40307, %v40302
%v40312 = vshll.u32 %v40307, 29
%v40313 = vshrl.u32 %v40307, 3
%v40314 = vor.u32 %v40313, %v40312
%v40315 = vxor.u32 %v40314, %v40310
%v40318 = vadd.s32 %v40315, %v40310
%v40320 = vshll.u32 %v40315, 16
%v40321 = vshrl.u32 %v40315, 16
%v40322 = vor.u32 %v40321, %v40320
%v40323 = vxor.u32 %v40322, %v40318
%v40326 = vadd.s32 %v40323, %v40318
%v40330 = vadd.s32 %v40326, %v9
%v40332 = vshll.u32 %v40323, 24
%v40333 = vshrl.u32 %v40323, 8
%v40334 = vor.u32 %v40333, %v40332
%v40335 = vxor.u32 %v40334, %v40326
%v40338 = vadd.s32 %v40335, %v8
%v40342 = vadd.s32 4, %v40338
%v40346 = vadd.s32 %v40342, %v40330
%v40348 = vshll.u32 %v40342, 13
%v40349 = vshrl.u32 %v40342, 19
%v40350 = vor.u32 %v40349, %v40348
%v40351 = vxor.u32 %v40350, %v40346
%v40354 = vadd.s32 %v40351, %v40346
%v40356 = vshll.u32 %v40351, 15
%v40357 = vshrl.u32 %v40351, 17
%v40358 = vor.u32 %v40357, %v40356
%v40359 = vxor.u32 %v40358, %v40354
%v40362 = vadd.s32 %v40359, %v40354
%v40364 = vshll.u32 %v40359, 26
%v40365 = vshrl.u32 %v40359, 6
%v40366 = vor.u32 %v40365, %v40364
%v40367 = vxor.u32 %v40366, %v40362
%v40370 = vadd.s32 %v40367, %v40362
%v40374 = vadd.s32 %v40370, %v8
%v40376 = vshll.u32 %v40367, 6
%v40377 = vshrl.u32 %v40367, 26
%v40378 = vor.u32 %v40377, %v40376
%v40379 = vxor.u32 %v40378, %v40370
%v40382 = vadd.s32 %v40379, %v10
%v40386 = vadd.s32 5, %v40382
%v40388 = vxor.u32 %v40386, %v40374
%v40389 = vand.u32.u8 255, %v40388
%v40390 = vand.u32 65535, %v40389
%v40391 = vshrl.u32 %v40390, 1
%v40392 = vor.u32 16256, %v40391
%v40393 = vand.u32.u16 65535, %v40392
%v119958 = vadd.low.f32.bf16 -1.0, %v40393
%v40402 = vmul.f32 2.0, %v119958
%v40406 = vadd.f32 -0.99609375, %v40402
%v40410 = vmax.f32 %v40406, -0.99609375
%v40412 = vand.u32 2147483647, %v40410
%vm40415 = vcmp.eq.f32.partialorder %v40412, 1.0
%v40420 = vmul.f32 inf, %v40410
%v40422 = vxor.u32 2147483648, %v40410
%v40425 = vmul.f32 %v40422, %v40410
%v40427 = vadd.f32 1.0, %v40425
%v40428 = vlog2.pop %v40427
%v40429 = vmul.f32 0.6931472, %v40428
%v40430 = vmul.f32 -0.5, %v40425
%v40431 = vadd.f32 1.0, %v40430
%v40432 = vmul.f32 %v40431, %v40425
%v40433 = vand.u32 2147483647, %v40425
%vm40434 = vcmp.lt.f32.partialorder %v40433, 0.0004427343
%v40435 = vsel /*vm=*/%vm40434, /*on_true_vy=*/%v40432, /*on_false_vx=*/%v40429
%v40436 = vxor.u32 2147483648, %v40435
%vm40439 = vcmp.lt.f32.partialorder %v40436, 5.0
%v40444 = vsel /*vm=*/%vm40439, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v40448 = vsel /*vm=*/%vm40439, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v40452 = vsel /*vm=*/%vm40439, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v40456 = vsel /*vm=*/%vm40439, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v40460 = vsel /*vm=*/%vm40439, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v40464 = vsel /*vm=*/%vm40439, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v40468 = vsel /*vm=*/%vm40439, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v40472 = vsel /*vm=*/%vm40439, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v40476 = vsel /*vm=*/%vm40439, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v40480 = vadd.f32 -2.5, %v40436
%v40482 = vrsqrt.pop %v40436
%v40483 = vmul.f32 %v40482, %v40436
%vm40484 = vcmp.eq.f32.partialorder %v40436, inf
%v40485 = vsel /*vm=*/%vm40484, /*on_true_vy=*/%v40436, /*on_false_vx=*/%v40483
%vm40486 = vcmp.eq.f32.partialorder %v40436, 0.0
%v40487 = vand.u32 2147483648, %v40436
%v40488 = vsel /*vm=*/%vm40486, /*on_true_vy=*/%v40487, /*on_false_vx=*/%v40485
%v40491 = vadd.f32 -3.0, %v40488
%v40495 = vsel /*vm=*/%vm40439, /*on_true_vy=*/%v40480, /*on_false_vx=*/%v40491
%v40499 = vmul.f32 %v40495, %v40476
%v40503 = vadd.f32 %v40499, %v40472
%v40507 = vmul.f32 %v40503, %v40495
%v40511 = vadd.f32 %v40507, %v40468
%v40515 = vmul.f32 %v40511, %v40495
%v40519 = vadd.f32 %v40515, %v40464
%v40523 = vmul.f32 %v40519, %v40495
%v40527 = vadd.f32 %v40523, %v40460
%v40531 = vmul.f32 %v40527, %v40495
%v40535 = vadd.f32 %v40531, %v40456
%v40539 = vmul.f32 %v40535, %v40495
%v40543 = vadd.f32 %v40539, %v40452
%v40547 = vmul.f32 %v40543, %v40495
%v40551 = vadd.f32 %v40547, %v40448
%v40555 = vmul.f32 %v40551, %v40495
%v40559 = vadd.f32 %v40555, %v40444
%v40563 = vmul.f32 %v40559, %v40410
%v40567 = vsel /*vm=*/%vm40415, /*on_true_vy=*/%v40420, /*on_false_vx=*/%v40563
%v40571 = vmul.f32 1.4140625, %v40567
%v40574 = vpack.c.bf16 %v120417, %v40571
%119959 = vst [vmem:[%s280 + $0x2a8] sm:$0xf] /*vst_source=*/%v40574
%v40578 = vadd.s32 %v37809, %v3329
%v40588 = vadd.s32 %v40578, %v415
%vm40592 = vcmp.lt.u32.totalorder %v40588, %v40578
%vm40597 = vcmp.lt.u32.totalorder %v40578, %v3329
%v40602 = vadd.s32 %v37792, %v3316
%v40606 = vadd.s32 1, %v40602
%v40610 = vsel /*vm=*/%vm40597, /*on_true_vy=*/%v40606, /*on_false_vx=*/%v40602
%v40614 = vadd.s32 1, %v40610
%v40618 = vsel /*vm=*/%vm40592, /*on_true_vy=*/%v40614, /*on_false_vx=*/%v40610
%v40623 = vadd.s32 %v40618, %v10
%v40627 = vadd.s32 %v40588, %v9
%v40631 = vadd.s32 %v40627, %v40623
%v40633 = vshll.u32 %v40627, 13
%v40634 = vshrl.u32 %v40627, 19
%v40635 = vor.u32 %v40634, %v40633
%v40636 = vxor.u32 %v40635, %v40631
%v40639 = vadd.s32 %v40636, %v40631
%v40641 = vshll.u32 %v40636, 15
%v40642 = vshrl.u32 %v40636, 17
%v40643 = vor.u32 %v40642, %v40641
%v40644 = vxor.u32 %v40643, %v40639
%v40647 = vadd.s32 %v40644, %v40639
%v40649 = vshll.u32 %v40644, 26
%v40650 = vshrl.u32 %v40644, 6
%v40651 = vor.u32 %v40650, %v40649
%v40652 = vxor.u32 %v40651, %v40647
%v40655 = vadd.s32 %v40652, %v40647
%v40659 = vadd.s32 %v40655, %v9
%v40661 = vshll.u32 %v40652, 6
%v40662 = vshrl.u32 %v40652, 26
%v40663 = vor.u32 %v40662, %v40661
%v40664 = vxor.u32 %v40663, %v40655
%v40667 = vadd.s32 %v40664, %v8
%v40671 = vadd.s32 1, %v40667
%v40675 = vadd.s32 %v40671, %v40659
%v40677 = vshll.u32 %v40671, 17
%v40678 = vshrl.u32 %v40671, 15
%v40679 = vor.u32 %v40678, %v40677
%v40680 = vxor.u32 %v40679, %v40675
%v40683 = vadd.s32 %v40680, %v40675
%v40685 = vshll.u32 %v40680, 29
%v40686 = vshrl.u32 %v40680, 3
%v40687 = vor.u32 %v40686, %v40685
%v40688 = vxor.u32 %v40687, %v40683
%v40691 = vadd.s32 %v40688, %v40683
%v40693 = vshll.u32 %v40688, 16
%v40694 = vshrl.u32 %v40688, 16
%v40695 = vor.u32 %v40694, %v40693
%v40696 = vxor.u32 %v40695, %v40691
%v40699 = vadd.s32 %v40696, %v40691
%v40703 = vadd.s32 %v40699, %v8
%v40705 = vshll.u32 %v40696, 24
%v40706 = vshrl.u32 %v40696, 8
%v40707 = vor.u32 %v40706, %v40705
%v40708 = vxor.u32 %v40707, %v40699
%v40711 = vadd.s32 %v40708, %v10
%v40715 = vadd.s32 2, %v40711
%v40719 = vadd.s32 %v40715, %v40703
%v40721 = vshll.u32 %v40715, 13
%v40722 = vshrl.u32 %v40715, 19
%v40723 = vor.u32 %v40722, %v40721
%v40724 = vxor.u32 %v40723, %v40719
%v40727 = vadd.s32 %v40724, %v40719
%v40729 = vshll.u32 %v40724, 15
%v40730 = vshrl.u32 %v40724, 17
%v40731 = vor.u32 %v40730, %v40729
%v40732 = vxor.u32 %v40731, %v40727
%v40735 = vadd.s32 %v40732, %v40727
%v40737 = vshll.u32 %v40732, 26
%v40738 = vshrl.u32 %v40732, 6
%v40739 = vor.u32 %v40738, %v40737
%v40740 = vxor.u32 %v40739, %v40735
%v40743 = vadd.s32 %v40740, %v40735
%v40747 = vadd.s32 %v40743, %v10
%v40749 = vshll.u32 %v40740, 6
%v40750 = vshrl.u32 %v40740, 26
%v40751 = vor.u32 %v40750, %v40749
%v40752 = vxor.u32 %v40751, %v40743
%v40755 = vadd.s32 %v40752, %v9
%v40759 = vadd.s32 3, %v40755
%v40763 = vadd.s32 %v40759, %v40747
%v40765 = vshll.u32 %v40759, 17
%v40766 = vshrl.u32 %v40759, 15
%v40767 = vor.u32 %v40766, %v40765
%v40768 = vxor.u32 %v40767, %v40763
%v40771 = vadd.s32 %v40768, %v40763
%v40773 = vshll.u32 %v40768, 29
%v40774 = vshrl.u32 %v40768, 3
%v40775 = vor.u32 %v40774, %v40773
%v40776 = vxor.u32 %v40775, %v40771
%v40779 = vadd.s32 %v40776, %v40771
%v40781 = vshll.u32 %v40776, 16
%v40782 = vshrl.u32 %v40776, 16
%v40783 = vor.u32 %v40782, %v40781
%v40784 = vxor.u32 %v40783, %v40779
%v40787 = vadd.s32 %v40784, %v40779
%v40791 = vadd.s32 %v40787, %v9
%v40793 = vshll.u32 %v40784, 24
%v40794 = vshrl.u32 %v40784, 8
%v40795 = vor.u32 %v40794, %v40793
%v40796 = vxor.u32 %v40795, %v40787
%v40799 = vadd.s32 %v40796, %v8
%v40803 = vadd.s32 4, %v40799
%v40807 = vadd.s32 %v40803, %v40791
%v40809 = vshll.u32 %v40803, 13
%v40810 = vshrl.u32 %v40803, 19
%v40811 = vor.u32 %v40810, %v40809
%v40812 = vxor.u32 %v40811, %v40807
%v40815 = vadd.s32 %v40812, %v40807
%v40817 = vshll.u32 %v40812, 15
%v40818 = vshrl.u32 %v40812, 17
%v40819 = vor.u32 %v40818, %v40817
%v40820 = vxor.u32 %v40819, %v40815
%v40823 = vadd.s32 %v40820, %v40815
%v40825 = vshll.u32 %v40820, 26
%v40826 = vshrl.u32 %v40820, 6
%v40827 = vor.u32 %v40826, %v40825
%v40828 = vxor.u32 %v40827, %v40823
%v40831 = vadd.s32 %v40828, %v40823
%v40835 = vadd.s32 %v40831, %v8
%v40837 = vshll.u32 %v40828, 6
%v40838 = vshrl.u32 %v40828, 26
%v40839 = vor.u32 %v40838, %v40837
%v40840 = vxor.u32 %v40839, %v40831
%v40843 = vadd.s32 %v40840, %v10
%v40847 = vadd.s32 5, %v40843
%v40849 = vxor.u32 %v40847, %v40835
%v40850 = vand.u32.u8 255, %v40849
%v40851 = vand.u32 65535, %v40850
%v40852 = vshrl.u32 %v40851, 1
%v40853 = vor.u32 16256, %v40852
%v40854 = vand.u32.u16 65535, %v40853
%v119960 = vadd.low.f32.bf16 -1.0, %v40854
%v40863 = vmul.f32 2.0, %v119960
%v40867 = vadd.f32 -0.99609375, %v40863
%v40871 = vmax.f32 %v40867, -0.99609375
%v40873 = vand.u32 2147483647, %v40871
%vm40876 = vcmp.eq.f32.partialorder %v40873, 1.0
%v40881 = vmul.f32 inf, %v40871
%v40883 = vxor.u32 2147483648, %v40871
%v40886 = vmul.f32 %v40883, %v40871
%v40888 = vadd.f32 1.0, %v40886
%v40889 = vlog2.pop %v40888
%v40890 = vmul.f32 0.6931472, %v40889
%v40891 = vmul.f32 -0.5, %v40886
%v40892 = vadd.f32 1.0, %v40891
%v40893 = vmul.f32 %v40892, %v40886
%v40894 = vand.u32 2147483647, %v40886
%vm40895 = vcmp.lt.f32.partialorder %v40894, 0.0004427343
%v40896 = vsel /*vm=*/%vm40895, /*on_true_vy=*/%v40893, /*on_false_vx=*/%v40890
%v40897 = vxor.u32 2147483648, %v40896
%vm40900 = vcmp.lt.f32.partialorder %v40897, 5.0
%v40905 = vsel /*vm=*/%vm40900, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v40909 = vsel /*vm=*/%vm40900, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v40913 = vsel /*vm=*/%vm40900, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v40917 = vsel /*vm=*/%vm40900, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v40921 = vsel /*vm=*/%vm40900, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v40925 = vsel /*vm=*/%vm40900, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v40929 = vsel /*vm=*/%vm40900, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v40933 = vsel /*vm=*/%vm40900, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v40937 = vsel /*vm=*/%vm40900, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v40941 = vadd.f32 -2.5, %v40897
%v40943 = vrsqrt.pop %v40897
%v40944 = vmul.f32 %v40943, %v40897
%vm40945 = vcmp.eq.f32.partialorder %v40897, inf
%v40946 = vsel /*vm=*/%vm40945, /*on_true_vy=*/%v40897, /*on_false_vx=*/%v40944
%vm40947 = vcmp.eq.f32.partialorder %v40897, 0.0
%v40948 = vand.u32 2147483648, %v40897
%v40949 = vsel /*vm=*/%vm40947, /*on_true_vy=*/%v40948, /*on_false_vx=*/%v40946
%v40952 = vadd.f32 -3.0, %v40949
%v40956 = vsel /*vm=*/%vm40900, /*on_true_vy=*/%v40941, /*on_false_vx=*/%v40952
%v40960 = vmul.f32 %v40956, %v40937
%v40964 = vadd.f32 %v40960, %v40933
%v40968 = vmul.f32 %v40964, %v40956
%v40972 = vadd.f32 %v40968, %v40929
%v40976 = vmul.f32 %v40972, %v40956
%v40980 = vadd.f32 %v40976, %v40925
%v40984 = vmul.f32 %v40980, %v40956
%v40988 = vadd.f32 %v40984, %v40921
%v40992 = vmul.f32 %v40988, %v40956
%v40996 = vadd.f32 %v40992, %v40917
%v41000 = vmul.f32 %v40996, %v40956
%v41004 = vadd.f32 %v41000, %v40913
%v41008 = vmul.f32 %v41004, %v40956
%v41012 = vadd.f32 %v41008, %v40909
%v41016 = vmul.f32 %v41012, %v40956
%v41020 = vadd.f32 %v41016, %v40905
%v41024 = vmul.f32 %v41020, %v40871
%v41028 = vsel /*vm=*/%vm40876, /*on_true_vy=*/%v40881, /*on_false_vx=*/%v41024
%v41032 = vmul.f32 1.4140625, %v41028
%v41035 = vpack.c.bf16 %v120417, %v41032
%119961 = vst [vmem:[%s280 + $0x328] sm:$0xf] /*vst_source=*/%v41035
%v41039 = vadd.s32 %v37809, %v3816
%v41049 = vadd.s32 %v41039, %v415
%vm41053 = vcmp.lt.u32.totalorder %v41049, %v41039
%vm41058 = vcmp.lt.u32.totalorder %v41039, %v3816
%v41063 = vadd.s32 %v37792, %v3803
%v41067 = vadd.s32 1, %v41063
%v41071 = vsel /*vm=*/%vm41058, /*on_true_vy=*/%v41067, /*on_false_vx=*/%v41063
%v41075 = vadd.s32 1, %v41071
%v41079 = vsel /*vm=*/%vm41053, /*on_true_vy=*/%v41075, /*on_false_vx=*/%v41071
%v41084 = vadd.s32 %v41079, %v10
%v41088 = vadd.s32 %v41049, %v9
%v41092 = vadd.s32 %v41088, %v41084
%v41094 = vshll.u32 %v41088, 13
%v41095 = vshrl.u32 %v41088, 19
%v41096 = vor.u32 %v41095, %v41094
%v41097 = vxor.u32 %v41096, %v41092
%v41100 = vadd.s32 %v41097, %v41092
%v41102 = vshll.u32 %v41097, 15
%v41103 = vshrl.u32 %v41097, 17
%v41104 = vor.u32 %v41103, %v41102
%v41105 = vxor.u32 %v41104, %v41100
%v41108 = vadd.s32 %v41105, %v41100
%v41110 = vshll.u32 %v41105, 26
%v41111 = vshrl.u32 %v41105, 6
%v41112 = vor.u32 %v41111, %v41110
%v41113 = vxor.u32 %v41112, %v41108
%v41116 = vadd.s32 %v41113, %v41108
%v41120 = vadd.s32 %v41116, %v9
%v41122 = vshll.u32 %v41113, 6
%v41123 = vshrl.u32 %v41113, 26
%v41124 = vor.u32 %v41123, %v41122
%v41125 = vxor.u32 %v41124, %v41116
%v41128 = vadd.s32 %v41125, %v8
%v41132 = vadd.s32 1, %v41128
%v41136 = vadd.s32 %v41132, %v41120
%v41138 = vshll.u32 %v41132, 17
%v41139 = vshrl.u32 %v41132, 15
%v41140 = vor.u32 %v41139, %v41138
%v41141 = vxor.u32 %v41140, %v41136
%v41144 = vadd.s32 %v41141, %v41136
%v41146 = vshll.u32 %v41141, 29
%v41147 = vshrl.u32 %v41141, 3
%v41148 = vor.u32 %v41147, %v41146
%v41149 = vxor.u32 %v41148, %v41144
%v41152 = vadd.s32 %v41149, %v41144
%v41154 = vshll.u32 %v41149, 16
%v41155 = vshrl.u32 %v41149, 16
%v41156 = vor.u32 %v41155, %v41154
%v41157 = vxor.u32 %v41156, %v41152
%v41160 = vadd.s32 %v41157, %v41152
%v41164 = vadd.s32 %v41160, %v8
%v41166 = vshll.u32 %v41157, 24
%v41167 = vshrl.u32 %v41157, 8
%v41168 = vor.u32 %v41167, %v41166
%v41169 = vxor.u32 %v41168, %v41160
%v41172 = vadd.s32 %v41169, %v10
%v41176 = vadd.s32 2, %v41172
%v41180 = vadd.s32 %v41176, %v41164
%v41182 = vshll.u32 %v41176, 13
%v41183 = vshrl.u32 %v41176, 19
%v41184 = vor.u32 %v41183, %v41182
%v41185 = vxor.u32 %v41184, %v41180
%v41188 = vadd.s32 %v41185, %v41180
%v41190 = vshll.u32 %v41185, 15
%v41191 = vshrl.u32 %v41185, 17
%v41192 = vor.u32 %v41191, %v41190
%v41193 = vxor.u32 %v41192, %v41188
%v41196 = vadd.s32 %v41193, %v41188
%v41198 = vshll.u32 %v41193, 26
%v41199 = vshrl.u32 %v41193, 6
%v41200 = vor.u32 %v41199, %v41198
%v41201 = vxor.u32 %v41200, %v41196
%v41204 = vadd.s32 %v41201, %v41196
%v41208 = vadd.s32 %v41204, %v10
%v41210 = vshll.u32 %v41201, 6
%v41211 = vshrl.u32 %v41201, 26
%v41212 = vor.u32 %v41211, %v41210
%v41213 = vxor.u32 %v41212, %v41204
%v41216 = vadd.s32 %v41213, %v9
%v41220 = vadd.s32 3, %v41216
%v41224 = vadd.s32 %v41220, %v41208
%v41226 = vshll.u32 %v41220, 17
%v41227 = vshrl.u32 %v41220, 15
%v41228 = vor.u32 %v41227, %v41226
%v41229 = vxor.u32 %v41228, %v41224
%v41232 = vadd.s32 %v41229, %v41224
%v41234 = vshll.u32 %v41229, 29
%v41235 = vshrl.u32 %v41229, 3
%v41236 = vor.u32 %v41235, %v41234
%v41237 = vxor.u32 %v41236, %v41232
%v41240 = vadd.s32 %v41237, %v41232
%v41242 = vshll.u32 %v41237, 16
%v41243 = vshrl.u32 %v41237, 16
%v41244 = vor.u32 %v41243, %v41242
%v41245 = vxor.u32 %v41244, %v41240
%v41248 = vadd.s32 %v41245, %v41240
%v41252 = vadd.s32 %v41248, %v9
%v41254 = vshll.u32 %v41245, 24
%v41255 = vshrl.u32 %v41245, 8
%v41256 = vor.u32 %v41255, %v41254
%v41257 = vxor.u32 %v41256, %v41248
%v41260 = vadd.s32 %v41257, %v8
%v41264 = vadd.s32 4, %v41260
%v41268 = vadd.s32 %v41264, %v41252
%v41270 = vshll.u32 %v41264, 13
%v41271 = vshrl.u32 %v41264, 19
%v41272 = vor.u32 %v41271, %v41270
%v41273 = vxor.u32 %v41272, %v41268
%v41276 = vadd.s32 %v41273, %v41268
%v41278 = vshll.u32 %v41273, 15
%v41279 = vshrl.u32 %v41273, 17
%v41280 = vor.u32 %v41279, %v41278
%v41281 = vxor.u32 %v41280, %v41276
%v41284 = vadd.s32 %v41281, %v41276
%v41286 = vshll.u32 %v41281, 26
%v41287 = vshrl.u32 %v41281, 6
%v41288 = vor.u32 %v41287, %v41286
%v41289 = vxor.u32 %v41288, %v41284
%v41292 = vadd.s32 %v41289, %v41284
%v41296 = vadd.s32 %v41292, %v8
%v41298 = vshll.u32 %v41289, 6
%v41299 = vshrl.u32 %v41289, 26
%v41300 = vor.u32 %v41299, %v41298
%v41301 = vxor.u32 %v41300, %v41292
%v41304 = vadd.s32 %v41301, %v10
%v41308 = vadd.s32 5, %v41304
%v41310 = vxor.u32 %v41308, %v41296
%v41311 = vand.u32.u8 255, %v41310
%v41312 = vand.u32 65535, %v41311
%v41313 = vshrl.u32 %v41312, 1
%v41314 = vor.u32 16256, %v41313
%v41315 = vand.u32.u16 65535, %v41314
%v119962 = vadd.low.f32.bf16 -1.0, %v41315
%v41324 = vmul.f32 2.0, %v119962
%v41328 = vadd.f32 -0.99609375, %v41324
%v41332 = vmax.f32 %v41328, -0.99609375
%v41334 = vand.u32 2147483647, %v41332
%vm41337 = vcmp.eq.f32.partialorder %v41334, 1.0
%v41342 = vmul.f32 inf, %v41332
%v41344 = vxor.u32 2147483648, %v41332
%v41347 = vmul.f32 %v41344, %v41332
%v41349 = vadd.f32 1.0, %v41347
%v41350 = vlog2.pop %v41349
%v41351 = vmul.f32 0.6931472, %v41350
%v41352 = vmul.f32 -0.5, %v41347
%v41353 = vadd.f32 1.0, %v41352
%v41354 = vmul.f32 %v41353, %v41347
%v41355 = vand.u32 2147483647, %v41347
%vm41356 = vcmp.lt.f32.partialorder %v41355, 0.0004427343
%v41357 = vsel /*vm=*/%vm41356, /*on_true_vy=*/%v41354, /*on_false_vx=*/%v41351
%v41358 = vxor.u32 2147483648, %v41357
%vm41361 = vcmp.lt.f32.partialorder %v41358, 5.0
%v41366 = vsel /*vm=*/%vm41361, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v41370 = vsel /*vm=*/%vm41361, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v41374 = vsel /*vm=*/%vm41361, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v41378 = vsel /*vm=*/%vm41361, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v41382 = vsel /*vm=*/%vm41361, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v41386 = vsel /*vm=*/%vm41361, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v41390 = vsel /*vm=*/%vm41361, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v41394 = vsel /*vm=*/%vm41361, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v41398 = vsel /*vm=*/%vm41361, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v41402 = vadd.f32 -2.5, %v41358
%v41404 = vrsqrt.pop %v41358
%v41405 = vmul.f32 %v41404, %v41358
%vm41406 = vcmp.eq.f32.partialorder %v41358, inf
%v41407 = vsel /*vm=*/%vm41406, /*on_true_vy=*/%v41358, /*on_false_vx=*/%v41405
%vm41408 = vcmp.eq.f32.partialorder %v41358, 0.0
%v41409 = vand.u32 2147483648, %v41358
%v41410 = vsel /*vm=*/%vm41408, /*on_true_vy=*/%v41409, /*on_false_vx=*/%v41407
%v41413 = vadd.f32 -3.0, %v41410
%v41417 = vsel /*vm=*/%vm41361, /*on_true_vy=*/%v41402, /*on_false_vx=*/%v41413
%v41421 = vmul.f32 %v41417, %v41398
%v41425 = vadd.f32 %v41421, %v41394
%v41429 = vmul.f32 %v41425, %v41417
%v41433 = vadd.f32 %v41429, %v41390
%v41437 = vmul.f32 %v41433, %v41417
%v41441 = vadd.f32 %v41437, %v41386
%v41445 = vmul.f32 %v41441, %v41417
%v41449 = vadd.f32 %v41445, %v41382
%v41453 = vmul.f32 %v41449, %v41417
%v41457 = vadd.f32 %v41453, %v41378
%v41461 = vmul.f32 %v41457, %v41417
%v41465 = vadd.f32 %v41461, %v41374
%v41469 = vmul.f32 %v41465, %v41417
%v41473 = vadd.f32 %v41469, %v41370
%v41477 = vmul.f32 %v41473, %v41417
%v41481 = vadd.f32 %v41477, %v41366
%v41485 = vmul.f32 %v41481, %v41332
%v41489 = vsel /*vm=*/%vm41337, /*on_true_vy=*/%v41342, /*on_false_vx=*/%v41485
%v41493 = vmul.f32 1.4140625, %v41489
%v41496 = vpack.c.bf16 %v120417, %v41493
%119963 = vst [vmem:[%s280 + $0x3a8] sm:$0xf] /*vst_source=*/%v41496
%v41534 = vadd.s32 %v41531, %v408
%v41544 = vadd.s32 %v41534, %v415
%vm41548 = vcmp.lt.u32.totalorder %v41544, %v41534
%vm41553 = vcmp.lt.u32.totalorder %v41534, %v408
%v41558 = vadd.s32 %v41514, %v380
%v41562 = vadd.s32 1, %v41558
%v41566 = vsel /*vm=*/%vm41553, /*on_true_vy=*/%v41562, /*on_false_vx=*/%v41558
%v41570 = vadd.s32 1, %v41566
%v41574 = vsel /*vm=*/%vm41548, /*on_true_vy=*/%v41570, /*on_false_vx=*/%v41566
%v41579 = vadd.s32 %v41574, %v10
%v41583 = vadd.s32 %v41544, %v9
%v41587 = vadd.s32 %v41583, %v41579
%v41589 = vshll.u32 %v41583, 13
%v41590 = vshrl.u32 %v41583, 19
%v41591 = vor.u32 %v41590, %v41589
%v41592 = vxor.u32 %v41591, %v41587
%v41595 = vadd.s32 %v41592, %v41587
%v41597 = vshll.u32 %v41592, 15
%v41598 = vshrl.u32 %v41592, 17
%v41599 = vor.u32 %v41598, %v41597
%v41600 = vxor.u32 %v41599, %v41595
%v41603 = vadd.s32 %v41600, %v41595
%v41605 = vshll.u32 %v41600, 26
%v41606 = vshrl.u32 %v41600, 6
%v41607 = vor.u32 %v41606, %v41605
%v41608 = vxor.u32 %v41607, %v41603
%v41611 = vadd.s32 %v41608, %v41603
%v41615 = vadd.s32 %v41611, %v9
%v41617 = vshll.u32 %v41608, 6
%v41618 = vshrl.u32 %v41608, 26
%v41619 = vor.u32 %v41618, %v41617
%v41620 = vxor.u32 %v41619, %v41611
%v41623 = vadd.s32 %v41620, %v8
%v41627 = vadd.s32 1, %v41623
%v41631 = vadd.s32 %v41627, %v41615
%v41633 = vshll.u32 %v41627, 17
%v41634 = vshrl.u32 %v41627, 15
%v41635 = vor.u32 %v41634, %v41633
%v41636 = vxor.u32 %v41635, %v41631
%v41639 = vadd.s32 %v41636, %v41631
%v41641 = vshll.u32 %v41636, 29
%v41642 = vshrl.u32 %v41636, 3
%v41643 = vor.u32 %v41642, %v41641
%v41644 = vxor.u32 %v41643, %v41639
%v41647 = vadd.s32 %v41644, %v41639
%v41649 = vshll.u32 %v41644, 16
%v41650 = vshrl.u32 %v41644, 16
%v41651 = vor.u32 %v41650, %v41649
%v41652 = vxor.u32 %v41651, %v41647
%v41655 = vadd.s32 %v41652, %v41647
%v41659 = vadd.s32 %v41655, %v8
%v41661 = vshll.u32 %v41652, 24
%v41662 = vshrl.u32 %v41652, 8
%v41663 = vor.u32 %v41662, %v41661
%v41664 = vxor.u32 %v41663, %v41655
%v41667 = vadd.s32 %v41664, %v10
%v41671 = vadd.s32 2, %v41667
%v41675 = vadd.s32 %v41671, %v41659
%v41677 = vshll.u32 %v41671, 13
%v41678 = vshrl.u32 %v41671, 19
%v41679 = vor.u32 %v41678, %v41677
%v41680 = vxor.u32 %v41679, %v41675
%v41683 = vadd.s32 %v41680, %v41675
%v41685 = vshll.u32 %v41680, 15
%v41686 = vshrl.u32 %v41680, 17
%v41687 = vor.u32 %v41686, %v41685
%v41688 = vxor.u32 %v41687, %v41683
%v41691 = vadd.s32 %v41688, %v41683
%v41693 = vshll.u32 %v41688, 26
%v41694 = vshrl.u32 %v41688, 6
%v41695 = vor.u32 %v41694, %v41693
%v41696 = vxor.u32 %v41695, %v41691
%v41699 = vadd.s32 %v41696, %v41691
%v41703 = vadd.s32 %v41699, %v10
%v41705 = vshll.u32 %v41696, 6
%v41706 = vshrl.u32 %v41696, 26
%v41707 = vor.u32 %v41706, %v41705
%v41708 = vxor.u32 %v41707, %v41699
%v41711 = vadd.s32 %v41708, %v9
%v41715 = vadd.s32 3, %v41711
%v41719 = vadd.s32 %v41715, %v41703
%v41721 = vshll.u32 %v41715, 17
%v41722 = vshrl.u32 %v41715, 15
%v41723 = vor.u32 %v41722, %v41721
%v41724 = vxor.u32 %v41723, %v41719
%v41727 = vadd.s32 %v41724, %v41719
%v41729 = vshll.u32 %v41724, 29
%v41730 = vshrl.u32 %v41724, 3
%v41731 = vor.u32 %v41730, %v41729
%v41732 = vxor.u32 %v41731, %v41727
%v41735 = vadd.s32 %v41732, %v41727
%v41737 = vshll.u32 %v41732, 16
%v41738 = vshrl.u32 %v41732, 16
%v41739 = vor.u32 %v41738, %v41737
%v41740 = vxor.u32 %v41739, %v41735
%v41743 = vadd.s32 %v41740, %v41735
%v41747 = vadd.s32 %v41743, %v9
%v41749 = vshll.u32 %v41740, 24
%v41750 = vshrl.u32 %v41740, 8
%v41751 = vor.u32 %v41750, %v41749
%v41752 = vxor.u32 %v41751, %v41743
%v41755 = vadd.s32 %v41752, %v8
%v41759 = vadd.s32 4, %v41755
%v41763 = vadd.s32 %v41759, %v41747
%v41765 = vshll.u32 %v41759, 13
%v41766 = vshrl.u32 %v41759, 19
%v41767 = vor.u32 %v41766, %v41765
%v41768 = vxor.u32 %v41767, %v41763
%v41771 = vadd.s32 %v41768, %v41763
%v41773 = vshll.u32 %v41768, 15
%v41774 = vshrl.u32 %v41768, 17
%v41775 = vor.u32 %v41774, %v41773
%v41776 = vxor.u32 %v41775, %v41771
%v41779 = vadd.s32 %v41776, %v41771
%v41781 = vshll.u32 %v41776, 26
%v41782 = vshrl.u32 %v41776, 6
%v41783 = vor.u32 %v41782, %v41781
%v41784 = vxor.u32 %v41783, %v41779
%v41787 = vadd.s32 %v41784, %v41779
%v41791 = vadd.s32 %v41787, %v8
%v41793 = vshll.u32 %v41784, 6
%v41794 = vshrl.u32 %v41784, 26
%v41795 = vor.u32 %v41794, %v41793
%v41796 = vxor.u32 %v41795, %v41787
%v41799 = vadd.s32 %v41796, %v10
%v41803 = vadd.s32 5, %v41799
%v41805 = vxor.u32 %v41803, %v41791
%v41806 = vand.u32.u8 255, %v41805
%v41807 = vand.u32 65535, %v41806
%v41808 = vshrl.u32 %v41807, 1
%v41809 = vor.u32 16256, %v41808
%v41810 = vand.u32.u16 65535, %v41809
%v119968 = vadd.low.f32.bf16 -1.0, %v41810
%v41819 = vmul.f32 2.0, %v119968
%v41823 = vadd.f32 -0.99609375, %v41819
%v41827 = vmax.f32 %v41823, -0.99609375
%v41829 = vand.u32 2147483647, %v41827
%vm41832 = vcmp.eq.f32.partialorder %v41829, 1.0
%v41837 = vmul.f32 inf, %v41827
%v41839 = vxor.u32 2147483648, %v41827
%v41842 = vmul.f32 %v41839, %v41827
%v41844 = vadd.f32 1.0, %v41842
%v41845 = vlog2.pop %v41844
%v41846 = vmul.f32 0.6931472, %v41845
%v41847 = vmul.f32 -0.5, %v41842
%v41848 = vadd.f32 1.0, %v41847
%v41849 = vmul.f32 %v41848, %v41842
%v41850 = vand.u32 2147483647, %v41842
%vm41851 = vcmp.lt.f32.partialorder %v41850, 0.0004427343
%v41852 = vsel /*vm=*/%vm41851, /*on_true_vy=*/%v41849, /*on_false_vx=*/%v41846
%v41853 = vxor.u32 2147483648, %v41852
%vm41856 = vcmp.lt.f32.partialorder %v41853, 5.0
%v41861 = vsel /*vm=*/%vm41856, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v41865 = vsel /*vm=*/%vm41856, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v41869 = vsel /*vm=*/%vm41856, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v41873 = vsel /*vm=*/%vm41856, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v41877 = vsel /*vm=*/%vm41856, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v41881 = vsel /*vm=*/%vm41856, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v41885 = vsel /*vm=*/%vm41856, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v41889 = vsel /*vm=*/%vm41856, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v41893 = vsel /*vm=*/%vm41856, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v41897 = vadd.f32 -2.5, %v41853
%v41899 = vrsqrt.pop %v41853
%v41900 = vmul.f32 %v41899, %v41853
%vm41901 = vcmp.eq.f32.partialorder %v41853, inf
%v41902 = vsel /*vm=*/%vm41901, /*on_true_vy=*/%v41853, /*on_false_vx=*/%v41900
%vm41903 = vcmp.eq.f32.partialorder %v41853, 0.0
%v41904 = vand.u32 2147483648, %v41853
%v41905 = vsel /*vm=*/%vm41903, /*on_true_vy=*/%v41904, /*on_false_vx=*/%v41902
%v41908 = vadd.f32 -3.0, %v41905
%v41912 = vsel /*vm=*/%vm41856, /*on_true_vy=*/%v41897, /*on_false_vx=*/%v41908
%v41916 = vmul.f32 %v41912, %v41893
%v41920 = vadd.f32 %v41916, %v41889
%v41924 = vmul.f32 %v41920, %v41912
%v41928 = vadd.f32 %v41924, %v41885
%v41932 = vmul.f32 %v41928, %v41912
%v41936 = vadd.f32 %v41932, %v41881
%v41940 = vmul.f32 %v41936, %v41912
%v41944 = vadd.f32 %v41940, %v41877
%v41948 = vmul.f32 %v41944, %v41912
%v41952 = vadd.f32 %v41948, %v41873
%v41956 = vmul.f32 %v41952, %v41912
%v41960 = vadd.f32 %v41956, %v41869
%v41964 = vmul.f32 %v41960, %v41912
%v41968 = vadd.f32 %v41964, %v41865
%v41972 = vmul.f32 %v41968, %v41912
%v41976 = vadd.f32 %v41972, %v41861
%v41980 = vmul.f32 %v41976, %v41827
%v41984 = vsel /*vm=*/%vm41832, /*on_true_vy=*/%v41837, /*on_false_vx=*/%v41980
%v41988 = vmul.f32 1.4140625, %v41984
%v41991 = vpack.c.bf16 %v120417, %v41988
%119969 = vst [vmem:[%s280 + $0x2c] sm:$0xf] /*vst_source=*/%v41991
%v41995 = vadd.s32 %v41531, %v894
%v42005 = vadd.s32 %v41995, %v415
%vm42009 = vcmp.lt.u32.totalorder %v42005, %v41995
%vm42014 = vcmp.lt.u32.totalorder %v41995, %v894
%v42019 = vadd.s32 %v41514, %v881
%v42023 = vadd.s32 1, %v42019
%v42027 = vsel /*vm=*/%vm42014, /*on_true_vy=*/%v42023, /*on_false_vx=*/%v42019
%v42031 = vadd.s32 1, %v42027
%v42035 = vsel /*vm=*/%vm42009, /*on_true_vy=*/%v42031, /*on_false_vx=*/%v42027
%v42040 = vadd.s32 %v42035, %v10
%v42044 = vadd.s32 %v42005, %v9
%v42048 = vadd.s32 %v42044, %v42040
%v42050 = vshll.u32 %v42044, 13
%v42051 = vshrl.u32 %v42044, 19
%v42052 = vor.u32 %v42051, %v42050
%v42053 = vxor.u32 %v42052, %v42048
%v42056 = vadd.s32 %v42053, %v42048
%v42058 = vshll.u32 %v42053, 15
%v42059 = vshrl.u32 %v42053, 17
%v42060 = vor.u32 %v42059, %v42058
%v42061 = vxor.u32 %v42060, %v42056
%v42064 = vadd.s32 %v42061, %v42056
%v42066 = vshll.u32 %v42061, 26
%v42067 = vshrl.u32 %v42061, 6
%v42068 = vor.u32 %v42067, %v42066
%v42069 = vxor.u32 %v42068, %v42064
%v42072 = vadd.s32 %v42069, %v42064
%v42076 = vadd.s32 %v42072, %v9
%v42078 = vshll.u32 %v42069, 6
%v42079 = vshrl.u32 %v42069, 26
%v42080 = vor.u32 %v42079, %v42078
%v42081 = vxor.u32 %v42080, %v42072
%v42084 = vadd.s32 %v42081, %v8
%v42088 = vadd.s32 1, %v42084
%v42092 = vadd.s32 %v42088, %v42076
%v42094 = vshll.u32 %v42088, 17
%v42095 = vshrl.u32 %v42088, 15
%v42096 = vor.u32 %v42095, %v42094
%v42097 = vxor.u32 %v42096, %v42092
%v42100 = vadd.s32 %v42097, %v42092
%v42102 = vshll.u32 %v42097, 29
%v42103 = vshrl.u32 %v42097, 3
%v42104 = vor.u32 %v42103, %v42102
%v42105 = vxor.u32 %v42104, %v42100
%v42108 = vadd.s32 %v42105, %v42100
%v42110 = vshll.u32 %v42105, 16
%v42111 = vshrl.u32 %v42105, 16
%v42112 = vor.u32 %v42111, %v42110
%v42113 = vxor.u32 %v42112, %v42108
%v42116 = vadd.s32 %v42113, %v42108
%v42120 = vadd.s32 %v42116, %v8
%v42122 = vshll.u32 %v42113, 24
%v42123 = vshrl.u32 %v42113, 8
%v42124 = vor.u32 %v42123, %v42122
%v42125 = vxor.u32 %v42124, %v42116
%v42128 = vadd.s32 %v42125, %v10
%v42132 = vadd.s32 2, %v42128
%v42136 = vadd.s32 %v42132, %v42120
%v42138 = vshll.u32 %v42132, 13
%v42139 = vshrl.u32 %v42132, 19
%v42140 = vor.u32 %v42139, %v42138
%v42141 = vxor.u32 %v42140, %v42136
%v42144 = vadd.s32 %v42141, %v42136
%v42146 = vshll.u32 %v42141, 15
%v42147 = vshrl.u32 %v42141, 17
%v42148 = vor.u32 %v42147, %v42146
%v42149 = vxor.u32 %v42148, %v42144
%v42152 = vadd.s32 %v42149, %v42144
%v42154 = vshll.u32 %v42149, 26
%v42155 = vshrl.u32 %v42149, 6
%v42156 = vor.u32 %v42155, %v42154
%v42157 = vxor.u32 %v42156, %v42152
%v42160 = vadd.s32 %v42157, %v42152
%v42164 = vadd.s32 %v42160, %v10
%v42166 = vshll.u32 %v42157, 6
%v42167 = vshrl.u32 %v42157, 26
%v42168 = vor.u32 %v42167, %v42166
%v42169 = vxor.u32 %v42168, %v42160
%v42172 = vadd.s32 %v42169, %v9
%v42176 = vadd.s32 3, %v42172
%v42180 = vadd.s32 %v42176, %v42164
%v42182 = vshll.u32 %v42176, 17
%v42183 = vshrl.u32 %v42176, 15
%v42184 = vor.u32 %v42183, %v42182
%v42185 = vxor.u32 %v42184, %v42180
%v42188 = vadd.s32 %v42185, %v42180
%v42190 = vshll.u32 %v42185, 29
%v42191 = vshrl.u32 %v42185, 3
%v42192 = vor.u32 %v42191, %v42190
%v42193 = vxor.u32 %v42192, %v42188
%v42196 = vadd.s32 %v42193, %v42188
%v42198 = vshll.u32 %v42193, 16
%v42199 = vshrl.u32 %v42193, 16
%v42200 = vor.u32 %v42199, %v42198
%v42201 = vxor.u32 %v42200, %v42196
%v42204 = vadd.s32 %v42201, %v42196
%v42208 = vadd.s32 %v42204, %v9
%v42210 = vshll.u32 %v42201, 24
%v42211 = vshrl.u32 %v42201, 8
%v42212 = vor.u32 %v42211, %v42210
%v42213 = vxor.u32 %v42212, %v42204
%v42216 = vadd.s32 %v42213, %v8
%v42220 = vadd.s32 4, %v42216
%v42224 = vadd.s32 %v42220, %v42208
%v42226 = vshll.u32 %v42220, 13
%v42227 = vshrl.u32 %v42220, 19
%v42228 = vor.u32 %v42227, %v42226
%v42229 = vxor.u32 %v42228, %v42224
%v42232 = vadd.s32 %v42229, %v42224
%v42234 = vshll.u32 %v42229, 15
%v42235 = vshrl.u32 %v42229, 17
%v42236 = vor.u32 %v42235, %v42234
%v42237 = vxor.u32 %v42236, %v42232
%v42240 = vadd.s32 %v42237, %v42232
%v42242 = vshll.u32 %v42237, 26
%v42243 = vshrl.u32 %v42237, 6
%v42244 = vor.u32 %v42243, %v42242
%v42245 = vxor.u32 %v42244, %v42240
%v42248 = vadd.s32 %v42245, %v42240
%v42252 = vadd.s32 %v42248, %v8
%v42254 = vshll.u32 %v42245, 6
%v42255 = vshrl.u32 %v42245, 26
%v42256 = vor.u32 %v42255, %v42254
%v42257 = vxor.u32 %v42256, %v42248
%v42260 = vadd.s32 %v42257, %v10
%v42264 = vadd.s32 5, %v42260
%v42266 = vxor.u32 %v42264, %v42252
%v42267 = vand.u32.u8 255, %v42266
%v42268 = vand.u32 65535, %v42267
%v42269 = vshrl.u32 %v42268, 1
%v42270 = vor.u32 16256, %v42269
%v42271 = vand.u32.u16 65535, %v42270
%v119970 = vadd.low.f32.bf16 -1.0, %v42271
%v42280 = vmul.f32 2.0, %v119970
%v42284 = vadd.f32 -0.99609375, %v42280
%v42288 = vmax.f32 %v42284, -0.99609375
%v42290 = vand.u32 2147483647, %v42288
%vm42293 = vcmp.eq.f32.partialorder %v42290, 1.0
%v42298 = vmul.f32 inf, %v42288
%v42300 = vxor.u32 2147483648, %v42288
%v42303 = vmul.f32 %v42300, %v42288
%v42305 = vadd.f32 1.0, %v42303
%v42306 = vlog2.pop %v42305
%v42307 = vmul.f32 0.6931472, %v42306
%v42308 = vmul.f32 -0.5, %v42303
%v42309 = vadd.f32 1.0, %v42308
%v42310 = vmul.f32 %v42309, %v42303
%v42311 = vand.u32 2147483647, %v42303
%vm42312 = vcmp.lt.f32.partialorder %v42311, 0.0004427343
%v42313 = vsel /*vm=*/%vm42312, /*on_true_vy=*/%v42310, /*on_false_vx=*/%v42307
%v42314 = vxor.u32 2147483648, %v42313
%vm42317 = vcmp.lt.f32.partialorder %v42314, 5.0
%v42322 = vsel /*vm=*/%vm42317, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v42326 = vsel /*vm=*/%vm42317, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v42330 = vsel /*vm=*/%vm42317, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v42334 = vsel /*vm=*/%vm42317, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v42338 = vsel /*vm=*/%vm42317, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v42342 = vsel /*vm=*/%vm42317, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v42346 = vsel /*vm=*/%vm42317, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v42350 = vsel /*vm=*/%vm42317, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v42354 = vsel /*vm=*/%vm42317, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v42358 = vadd.f32 -2.5, %v42314
%v42360 = vrsqrt.pop %v42314
%v42361 = vmul.f32 %v42360, %v42314
%vm42362 = vcmp.eq.f32.partialorder %v42314, inf
%v42363 = vsel /*vm=*/%vm42362, /*on_true_vy=*/%v42314, /*on_false_vx=*/%v42361
%vm42364 = vcmp.eq.f32.partialorder %v42314, 0.0
%v42365 = vand.u32 2147483648, %v42314
%v42366 = vsel /*vm=*/%vm42364, /*on_true_vy=*/%v42365, /*on_false_vx=*/%v42363
%v42369 = vadd.f32 -3.0, %v42366
%v42373 = vsel /*vm=*/%vm42317, /*on_true_vy=*/%v42358, /*on_false_vx=*/%v42369
%v42377 = vmul.f32 %v42373, %v42354
%v42381 = vadd.f32 %v42377, %v42350
%v42385 = vmul.f32 %v42381, %v42373
%v42389 = vadd.f32 %v42385, %v42346
%v42393 = vmul.f32 %v42389, %v42373
%v42397 = vadd.f32 %v42393, %v42342
%v42401 = vmul.f32 %v42397, %v42373
%v42405 = vadd.f32 %v42401, %v42338
%v42409 = vmul.f32 %v42405, %v42373
%v42413 = vadd.f32 %v42409, %v42334
%v42417 = vmul.f32 %v42413, %v42373
%v42421 = vadd.f32 %v42417, %v42330
%v42425 = vmul.f32 %v42421, %v42373
%v42429 = vadd.f32 %v42425, %v42326
%v42433 = vmul.f32 %v42429, %v42373
%v42437 = vadd.f32 %v42433, %v42322
%v42441 = vmul.f32 %v42437, %v42288
%v42445 = vsel /*vm=*/%vm42293, /*on_true_vy=*/%v42298, /*on_false_vx=*/%v42441
%v42449 = vmul.f32 1.4140625, %v42445
%v42452 = vpack.c.bf16 %v120417, %v42449
%119971 = vst [vmem:[%s280 + $0xac] sm:$0xf] /*vst_source=*/%v42452
%v42456 = vadd.s32 %v41531, %v1381
%v42466 = vadd.s32 %v42456, %v415
%vm42470 = vcmp.lt.u32.totalorder %v42466, %v42456
%vm42475 = vcmp.lt.u32.totalorder %v42456, %v1381
%v42480 = vadd.s32 %v41514, %v1368
%v42484 = vadd.s32 1, %v42480
%v42488 = vsel /*vm=*/%vm42475, /*on_true_vy=*/%v42484, /*on_false_vx=*/%v42480
%v42492 = vadd.s32 1, %v42488
%v42496 = vsel /*vm=*/%vm42470, /*on_true_vy=*/%v42492, /*on_false_vx=*/%v42488
%v42501 = vadd.s32 %v42496, %v10
%v42505 = vadd.s32 %v42466, %v9
%v42509 = vadd.s32 %v42505, %v42501
%v42511 = vshll.u32 %v42505, 13
%v42512 = vshrl.u32 %v42505, 19
%v42513 = vor.u32 %v42512, %v42511
%v42514 = vxor.u32 %v42513, %v42509
%v42517 = vadd.s32 %v42514, %v42509
%v42519 = vshll.u32 %v42514, 15
%v42520 = vshrl.u32 %v42514, 17
%v42521 = vor.u32 %v42520, %v42519
%v42522 = vxor.u32 %v42521, %v42517
%v42525 = vadd.s32 %v42522, %v42517
%v42527 = vshll.u32 %v42522, 26
%v42528 = vshrl.u32 %v42522, 6
%v42529 = vor.u32 %v42528, %v42527
%v42530 = vxor.u32 %v42529, %v42525
%v42533 = vadd.s32 %v42530, %v42525
%v42537 = vadd.s32 %v42533, %v9
%v42539 = vshll.u32 %v42530, 6
%v42540 = vshrl.u32 %v42530, 26
%v42541 = vor.u32 %v42540, %v42539
%v42542 = vxor.u32 %v42541, %v42533
%v42545 = vadd.s32 %v42542, %v8
%v42549 = vadd.s32 1, %v42545
%v42553 = vadd.s32 %v42549, %v42537
%v42555 = vshll.u32 %v42549, 17
%v42556 = vshrl.u32 %v42549, 15
%v42557 = vor.u32 %v42556, %v42555
%v42558 = vxor.u32 %v42557, %v42553
%v42561 = vadd.s32 %v42558, %v42553
%v42563 = vshll.u32 %v42558, 29
%v42564 = vshrl.u32 %v42558, 3
%v42565 = vor.u32 %v42564, %v42563
%v42566 = vxor.u32 %v42565, %v42561
%v42569 = vadd.s32 %v42566, %v42561
%v42571 = vshll.u32 %v42566, 16
%v42572 = vshrl.u32 %v42566, 16
%v42573 = vor.u32 %v42572, %v42571
%v42574 = vxor.u32 %v42573, %v42569
%v42577 = vadd.s32 %v42574, %v42569
%v42581 = vadd.s32 %v42577, %v8
%v42583 = vshll.u32 %v42574, 24
%v42584 = vshrl.u32 %v42574, 8
%v42585 = vor.u32 %v42584, %v42583
%v42586 = vxor.u32 %v42585, %v42577
%v42589 = vadd.s32 %v42586, %v10
%v42593 = vadd.s32 2, %v42589
%v42597 = vadd.s32 %v42593, %v42581
%v42599 = vshll.u32 %v42593, 13
%v42600 = vshrl.u32 %v42593, 19
%v42601 = vor.u32 %v42600, %v42599
%v42602 = vxor.u32 %v42601, %v42597
%v42605 = vadd.s32 %v42602, %v42597
%v42607 = vshll.u32 %v42602, 15
%v42608 = vshrl.u32 %v42602, 17
%v42609 = vor.u32 %v42608, %v42607
%v42610 = vxor.u32 %v42609, %v42605
%v42613 = vadd.s32 %v42610, %v42605
%v42615 = vshll.u32 %v42610, 26
%v42616 = vshrl.u32 %v42610, 6
%v42617 = vor.u32 %v42616, %v42615
%v42618 = vxor.u32 %v42617, %v42613
%v42621 = vadd.s32 %v42618, %v42613
%v42625 = vadd.s32 %v42621, %v10
%v42627 = vshll.u32 %v42618, 6
%v42628 = vshrl.u32 %v42618, 26
%v42629 = vor.u32 %v42628, %v42627
%v42630 = vxor.u32 %v42629, %v42621
%v42633 = vadd.s32 %v42630, %v9
%v42637 = vadd.s32 3, %v42633
%v42641 = vadd.s32 %v42637, %v42625
%v42643 = vshll.u32 %v42637, 17
%v42644 = vshrl.u32 %v42637, 15
%v42645 = vor.u32 %v42644, %v42643
%v42646 = vxor.u32 %v42645, %v42641
%v42649 = vadd.s32 %v42646, %v42641
%v42651 = vshll.u32 %v42646, 29
%v42652 = vshrl.u32 %v42646, 3
%v42653 = vor.u32 %v42652, %v42651
%v42654 = vxor.u32 %v42653, %v42649
%v42657 = vadd.s32 %v42654, %v42649
%v42659 = vshll.u32 %v42654, 16
%v42660 = vshrl.u32 %v42654, 16
%v42661 = vor.u32 %v42660, %v42659
%v42662 = vxor.u32 %v42661, %v42657
%v42665 = vadd.s32 %v42662, %v42657
%v42669 = vadd.s32 %v42665, %v9
%v42671 = vshll.u32 %v42662, 24
%v42672 = vshrl.u32 %v42662, 8
%v42673 = vor.u32 %v42672, %v42671
%v42674 = vxor.u32 %v42673, %v42665
%v42677 = vadd.s32 %v42674, %v8
%v42681 = vadd.s32 4, %v42677
%v42685 = vadd.s32 %v42681, %v42669
%v42687 = vshll.u32 %v42681, 13
%v42688 = vshrl.u32 %v42681, 19
%v42689 = vor.u32 %v42688, %v42687
%v42690 = vxor.u32 %v42689, %v42685
%v42693 = vadd.s32 %v42690, %v42685
%v42695 = vshll.u32 %v42690, 15
%v42696 = vshrl.u32 %v42690, 17
%v42697 = vor.u32 %v42696, %v42695
%v42698 = vxor.u32 %v42697, %v42693
%v42701 = vadd.s32 %v42698, %v42693
%v42703 = vshll.u32 %v42698, 26
%v42704 = vshrl.u32 %v42698, 6
%v42705 = vor.u32 %v42704, %v42703
%v42706 = vxor.u32 %v42705, %v42701
%v42709 = vadd.s32 %v42706, %v42701
%v42713 = vadd.s32 %v42709, %v8
%v42715 = vshll.u32 %v42706, 6
%v42716 = vshrl.u32 %v42706, 26
%v42717 = vor.u32 %v42716, %v42715
%v42718 = vxor.u32 %v42717, %v42709
%v42721 = vadd.s32 %v42718, %v10
%v42725 = vadd.s32 5, %v42721
%v42727 = vxor.u32 %v42725, %v42713
%v42728 = vand.u32.u8 255, %v42727
%v42729 = vand.u32 65535, %v42728
%v42730 = vshrl.u32 %v42729, 1
%v42731 = vor.u32 16256, %v42730
%v42732 = vand.u32.u16 65535, %v42731
%v119972 = vadd.low.f32.bf16 -1.0, %v42732
%v42741 = vmul.f32 2.0, %v119972
%v42745 = vadd.f32 -0.99609375, %v42741
%v42749 = vmax.f32 %v42745, -0.99609375
%v42751 = vand.u32 2147483647, %v42749
%vm42754 = vcmp.eq.f32.partialorder %v42751, 1.0
%v42759 = vmul.f32 inf, %v42749
%v42761 = vxor.u32 2147483648, %v42749
%v42764 = vmul.f32 %v42761, %v42749
%v42766 = vadd.f32 1.0, %v42764
%v42767 = vlog2.pop %v42766
%v42768 = vmul.f32 0.6931472, %v42767
%v42769 = vmul.f32 -0.5, %v42764
%v42770 = vadd.f32 1.0, %v42769
%v42771 = vmul.f32 %v42770, %v42764
%v42772 = vand.u32 2147483647, %v42764
%vm42773 = vcmp.lt.f32.partialorder %v42772, 0.0004427343
%v42774 = vsel /*vm=*/%vm42773, /*on_true_vy=*/%v42771, /*on_false_vx=*/%v42768
%v42775 = vxor.u32 2147483648, %v42774
%vm42778 = vcmp.lt.f32.partialorder %v42775, 5.0
%v42783 = vsel /*vm=*/%vm42778, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v42787 = vsel /*vm=*/%vm42778, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v42791 = vsel /*vm=*/%vm42778, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v42795 = vsel /*vm=*/%vm42778, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v42799 = vsel /*vm=*/%vm42778, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v42803 = vsel /*vm=*/%vm42778, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v42807 = vsel /*vm=*/%vm42778, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v42811 = vsel /*vm=*/%vm42778, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v42815 = vsel /*vm=*/%vm42778, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v42819 = vadd.f32 -2.5, %v42775
%v42821 = vrsqrt.pop %v42775
%v42822 = vmul.f32 %v42821, %v42775
%vm42823 = vcmp.eq.f32.partialorder %v42775, inf
%v42824 = vsel /*vm=*/%vm42823, /*on_true_vy=*/%v42775, /*on_false_vx=*/%v42822
%vm42825 = vcmp.eq.f32.partialorder %v42775, 0.0
%v42826 = vand.u32 2147483648, %v42775
%v42827 = vsel /*vm=*/%vm42825, /*on_true_vy=*/%v42826, /*on_false_vx=*/%v42824
%v42830 = vadd.f32 -3.0, %v42827
%v42834 = vsel /*vm=*/%vm42778, /*on_true_vy=*/%v42819, /*on_false_vx=*/%v42830
%v42838 = vmul.f32 %v42834, %v42815
%v42842 = vadd.f32 %v42838, %v42811
%v42846 = vmul.f32 %v42842, %v42834
%v42850 = vadd.f32 %v42846, %v42807
%v42854 = vmul.f32 %v42850, %v42834
%v42858 = vadd.f32 %v42854, %v42803
%v42862 = vmul.f32 %v42858, %v42834
%v42866 = vadd.f32 %v42862, %v42799
%v42870 = vmul.f32 %v42866, %v42834
%v42874 = vadd.f32 %v42870, %v42795
%v42878 = vmul.f32 %v42874, %v42834
%v42882 = vadd.f32 %v42878, %v42791
%v42886 = vmul.f32 %v42882, %v42834
%v42890 = vadd.f32 %v42886, %v42787
%v42894 = vmul.f32 %v42890, %v42834
%v42898 = vadd.f32 %v42894, %v42783
%v42902 = vmul.f32 %v42898, %v42749
%v42906 = vsel /*vm=*/%vm42754, /*on_true_vy=*/%v42759, /*on_false_vx=*/%v42902
%v42910 = vmul.f32 1.4140625, %v42906
%v42913 = vpack.c.bf16 %v120417, %v42910
%119973 = vst [vmem:[%s280 + $0x12c] sm:$0xf] /*vst_source=*/%v42913
%v42917 = vadd.s32 %v41531, %v1868
%v42927 = vadd.s32 %v42917, %v415
%vm42931 = vcmp.lt.u32.totalorder %v42927, %v42917
%vm42936 = vcmp.lt.u32.totalorder %v42917, %v1868
%v42941 = vadd.s32 %v41514, %v1855
%v42945 = vadd.s32 1, %v42941
%v42949 = vsel /*vm=*/%vm42936, /*on_true_vy=*/%v42945, /*on_false_vx=*/%v42941
%v42953 = vadd.s32 1, %v42949
%v42957 = vsel /*vm=*/%vm42931, /*on_true_vy=*/%v42953, /*on_false_vx=*/%v42949
%v42962 = vadd.s32 %v42957, %v10
%v42966 = vadd.s32 %v42927, %v9
%v42970 = vadd.s32 %v42966, %v42962
%v42972 = vshll.u32 %v42966, 13
%v42973 = vshrl.u32 %v42966, 19
%v42974 = vor.u32 %v42973, %v42972
%v42975 = vxor.u32 %v42974, %v42970
%v42978 = vadd.s32 %v42975, %v42970
%v42980 = vshll.u32 %v42975, 15
%v42981 = vshrl.u32 %v42975, 17
%v42982 = vor.u32 %v42981, %v42980
%v42983 = vxor.u32 %v42982, %v42978
%v42986 = vadd.s32 %v42983, %v42978
%v42988 = vshll.u32 %v42983, 26
%v42989 = vshrl.u32 %v42983, 6
%v42990 = vor.u32 %v42989, %v42988
%v42991 = vxor.u32 %v42990, %v42986
%v42994 = vadd.s32 %v42991, %v42986
%v42998 = vadd.s32 %v42994, %v9
%v43000 = vshll.u32 %v42991, 6
%v43001 = vshrl.u32 %v42991, 26
%v43002 = vor.u32 %v43001, %v43000
%v43003 = vxor.u32 %v43002, %v42994
%v43006 = vadd.s32 %v43003, %v8
%v43010 = vadd.s32 1, %v43006
%v43014 = vadd.s32 %v43010, %v42998
%v43016 = vshll.u32 %v43010, 17
%v43017 = vshrl.u32 %v43010, 15
%v43018 = vor.u32 %v43017, %v43016
%v43019 = vxor.u32 %v43018, %v43014
%v43022 = vadd.s32 %v43019, %v43014
%v43024 = vshll.u32 %v43019, 29
%v43025 = vshrl.u32 %v43019, 3
%v43026 = vor.u32 %v43025, %v43024
%v43027 = vxor.u32 %v43026, %v43022
%v43030 = vadd.s32 %v43027, %v43022
%v43032 = vshll.u32 %v43027, 16
%v43033 = vshrl.u32 %v43027, 16
%v43034 = vor.u32 %v43033, %v43032
%v43035 = vxor.u32 %v43034, %v43030
%v43038 = vadd.s32 %v43035, %v43030
%v43042 = vadd.s32 %v43038, %v8
%v43044 = vshll.u32 %v43035, 24
%v43045 = vshrl.u32 %v43035, 8
%v43046 = vor.u32 %v43045, %v43044
%v43047 = vxor.u32 %v43046, %v43038
%v43050 = vadd.s32 %v43047, %v10
%v43054 = vadd.s32 2, %v43050
%v43058 = vadd.s32 %v43054, %v43042
%v43060 = vshll.u32 %v43054, 13
%v43061 = vshrl.u32 %v43054, 19
%v43062 = vor.u32 %v43061, %v43060
%v43063 = vxor.u32 %v43062, %v43058
%v43066 = vadd.s32 %v43063, %v43058
%v43068 = vshll.u32 %v43063, 15
%v43069 = vshrl.u32 %v43063, 17
%v43070 = vor.u32 %v43069, %v43068
%v43071 = vxor.u32 %v43070, %v43066
%v43074 = vadd.s32 %v43071, %v43066
%v43076 = vshll.u32 %v43071, 26
%v43077 = vshrl.u32 %v43071, 6
%v43078 = vor.u32 %v43077, %v43076
%v43079 = vxor.u32 %v43078, %v43074
%v43082 = vadd.s32 %v43079, %v43074
%v43086 = vadd.s32 %v43082, %v10
%v43088 = vshll.u32 %v43079, 6
%v43089 = vshrl.u32 %v43079, 26
%v43090 = vor.u32 %v43089, %v43088
%v43091 = vxor.u32 %v43090, %v43082
%v43094 = vadd.s32 %v43091, %v9
%v43098 = vadd.s32 3, %v43094
%v43102 = vadd.s32 %v43098, %v43086
%v43104 = vshll.u32 %v43098, 17
%v43105 = vshrl.u32 %v43098, 15
%v43106 = vor.u32 %v43105, %v43104
%v43107 = vxor.u32 %v43106, %v43102
%v43110 = vadd.s32 %v43107, %v43102
%v43112 = vshll.u32 %v43107, 29
%v43113 = vshrl.u32 %v43107, 3
%v43114 = vor.u32 %v43113, %v43112
%v43115 = vxor.u32 %v43114, %v43110
%v43118 = vadd.s32 %v43115, %v43110
%v43120 = vshll.u32 %v43115, 16
%v43121 = vshrl.u32 %v43115, 16
%v43122 = vor.u32 %v43121, %v43120
%v43123 = vxor.u32 %v43122, %v43118
%v43126 = vadd.s32 %v43123, %v43118
%v43130 = vadd.s32 %v43126, %v9
%v43132 = vshll.u32 %v43123, 24
%v43133 = vshrl.u32 %v43123, 8
%v43134 = vor.u32 %v43133, %v43132
%v43135 = vxor.u32 %v43134, %v43126
%v43138 = vadd.s32 %v43135, %v8
%v43142 = vadd.s32 4, %v43138
%v43146 = vadd.s32 %v43142, %v43130
%v43148 = vshll.u32 %v43142, 13
%v43149 = vshrl.u32 %v43142, 19
%v43150 = vor.u32 %v43149, %v43148
%v43151 = vxor.u32 %v43150, %v43146
%v43154 = vadd.s32 %v43151, %v43146
%v43156 = vshll.u32 %v43151, 15
%v43157 = vshrl.u32 %v43151, 17
%v43158 = vor.u32 %v43157, %v43156
%v43159 = vxor.u32 %v43158, %v43154
%v43162 = vadd.s32 %v43159, %v43154
%v43164 = vshll.u32 %v43159, 26
%v43165 = vshrl.u32 %v43159, 6
%v43166 = vor.u32 %v43165, %v43164
%v43167 = vxor.u32 %v43166, %v43162
%v43170 = vadd.s32 %v43167, %v43162
%v43174 = vadd.s32 %v43170, %v8
%v43176 = vshll.u32 %v43167, 6
%v43177 = vshrl.u32 %v43167, 26
%v43178 = vor.u32 %v43177, %v43176
%v43179 = vxor.u32 %v43178, %v43170
%v43182 = vadd.s32 %v43179, %v10
%v43186 = vadd.s32 5, %v43182
%v43188 = vxor.u32 %v43186, %v43174
%v43189 = vand.u32.u8 255, %v43188
%v43190 = vand.u32 65535, %v43189
%v43191 = vshrl.u32 %v43190, 1
%v43192 = vor.u32 16256, %v43191
%v43193 = vand.u32.u16 65535, %v43192
%v119974 = vadd.low.f32.bf16 -1.0, %v43193
%v43202 = vmul.f32 2.0, %v119974
%v43206 = vadd.f32 -0.99609375, %v43202
%v43210 = vmax.f32 %v43206, -0.99609375
%v43212 = vand.u32 2147483647, %v43210
%vm43215 = vcmp.eq.f32.partialorder %v43212, 1.0
%v43220 = vmul.f32 inf, %v43210
%v43222 = vxor.u32 2147483648, %v43210
%v43225 = vmul.f32 %v43222, %v43210
%v43227 = vadd.f32 1.0, %v43225
%v43228 = vlog2.pop %v43227
%v43229 = vmul.f32 0.6931472, %v43228
%v43230 = vmul.f32 -0.5, %v43225
%v43231 = vadd.f32 1.0, %v43230
%v43232 = vmul.f32 %v43231, %v43225
%v43233 = vand.u32 2147483647, %v43225
%vm43234 = vcmp.lt.f32.partialorder %v43233, 0.0004427343
%v43235 = vsel /*vm=*/%vm43234, /*on_true_vy=*/%v43232, /*on_false_vx=*/%v43229
%v43236 = vxor.u32 2147483648, %v43235
%vm43239 = vcmp.lt.f32.partialorder %v43236, 5.0
%v43244 = vsel /*vm=*/%vm43239, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v43248 = vsel /*vm=*/%vm43239, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v43252 = vsel /*vm=*/%vm43239, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v43256 = vsel /*vm=*/%vm43239, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v43260 = vsel /*vm=*/%vm43239, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v43264 = vsel /*vm=*/%vm43239, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v43268 = vsel /*vm=*/%vm43239, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v43272 = vsel /*vm=*/%vm43239, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v43276 = vsel /*vm=*/%vm43239, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v43280 = vadd.f32 -2.5, %v43236
%v43282 = vrsqrt.pop %v43236
%v43283 = vmul.f32 %v43282, %v43236
%vm43284 = vcmp.eq.f32.partialorder %v43236, inf
%v43285 = vsel /*vm=*/%vm43284, /*on_true_vy=*/%v43236, /*on_false_vx=*/%v43283
%vm43286 = vcmp.eq.f32.partialorder %v43236, 0.0
%v43287 = vand.u32 2147483648, %v43236
%v43288 = vsel /*vm=*/%vm43286, /*on_true_vy=*/%v43287, /*on_false_vx=*/%v43285
%v43291 = vadd.f32 -3.0, %v43288
%v43295 = vsel /*vm=*/%vm43239, /*on_true_vy=*/%v43280, /*on_false_vx=*/%v43291
%v43299 = vmul.f32 %v43295, %v43276
%v43303 = vadd.f32 %v43299, %v43272
%v43307 = vmul.f32 %v43303, %v43295
%v43311 = vadd.f32 %v43307, %v43268
%v43315 = vmul.f32 %v43311, %v43295
%v43319 = vadd.f32 %v43315, %v43264
%v43323 = vmul.f32 %v43319, %v43295
%v43327 = vadd.f32 %v43323, %v43260
%v43331 = vmul.f32 %v43327, %v43295
%v43335 = vadd.f32 %v43331, %v43256
%v43339 = vmul.f32 %v43335, %v43295
%v43343 = vadd.f32 %v43339, %v43252
%v43347 = vmul.f32 %v43343, %v43295
%v43351 = vadd.f32 %v43347, %v43248
%v43355 = vmul.f32 %v43351, %v43295
%v43359 = vadd.f32 %v43355, %v43244
%v43363 = vmul.f32 %v43359, %v43210
%v43367 = vsel /*vm=*/%vm43215, /*on_true_vy=*/%v43220, /*on_false_vx=*/%v43363
%v43371 = vmul.f32 1.4140625, %v43367
%v43374 = vpack.c.bf16 %v120417, %v43371
%119975 = vst [vmem:[%s280 + $0x1ac] sm:$0xf] /*vst_source=*/%v43374
%v43378 = vadd.s32 %v41531, %v2355
%v43388 = vadd.s32 %v43378, %v415
%vm43392 = vcmp.lt.u32.totalorder %v43388, %v43378
%vm43397 = vcmp.lt.u32.totalorder %v43378, %v2355
%v43402 = vadd.s32 %v41514, %v2342
%v43406 = vadd.s32 1, %v43402
%v43410 = vsel /*vm=*/%vm43397, /*on_true_vy=*/%v43406, /*on_false_vx=*/%v43402
%v43414 = vadd.s32 1, %v43410
%v43418 = vsel /*vm=*/%vm43392, /*on_true_vy=*/%v43414, /*on_false_vx=*/%v43410
%v43423 = vadd.s32 %v43418, %v10
%v43427 = vadd.s32 %v43388, %v9
%v43431 = vadd.s32 %v43427, %v43423
%v43433 = vshll.u32 %v43427, 13
%v43434 = vshrl.u32 %v43427, 19
%v43435 = vor.u32 %v43434, %v43433
%v43436 = vxor.u32 %v43435, %v43431
%v43439 = vadd.s32 %v43436, %v43431
%v43441 = vshll.u32 %v43436, 15
%v43442 = vshrl.u32 %v43436, 17
%v43443 = vor.u32 %v43442, %v43441
%v43444 = vxor.u32 %v43443, %v43439
%v43447 = vadd.s32 %v43444, %v43439
%v43449 = vshll.u32 %v43444, 26
%v43450 = vshrl.u32 %v43444, 6
%v43451 = vor.u32 %v43450, %v43449
%v43452 = vxor.u32 %v43451, %v43447
%v43455 = vadd.s32 %v43452, %v43447
%v43459 = vadd.s32 %v43455, %v9
%v43461 = vshll.u32 %v43452, 6
%v43462 = vshrl.u32 %v43452, 26
%v43463 = vor.u32 %v43462, %v43461
%v43464 = vxor.u32 %v43463, %v43455
%v43467 = vadd.s32 %v43464, %v8
%v43471 = vadd.s32 1, %v43467
%v43475 = vadd.s32 %v43471, %v43459
%v43477 = vshll.u32 %v43471, 17
%v43478 = vshrl.u32 %v43471, 15
%v43479 = vor.u32 %v43478, %v43477
%v43480 = vxor.u32 %v43479, %v43475
%v43483 = vadd.s32 %v43480, %v43475
%v43485 = vshll.u32 %v43480, 29
%v43486 = vshrl.u32 %v43480, 3
%v43487 = vor.u32 %v43486, %v43485
%v43488 = vxor.u32 %v43487, %v43483
%v43491 = vadd.s32 %v43488, %v43483
%v43493 = vshll.u32 %v43488, 16
%v43494 = vshrl.u32 %v43488, 16
%v43495 = vor.u32 %v43494, %v43493
%v43496 = vxor.u32 %v43495, %v43491
%v43499 = vadd.s32 %v43496, %v43491
%v43503 = vadd.s32 %v43499, %v8
%v43505 = vshll.u32 %v43496, 24
%v43506 = vshrl.u32 %v43496, 8
%v43507 = vor.u32 %v43506, %v43505
%v43508 = vxor.u32 %v43507, %v43499
%v43511 = vadd.s32 %v43508, %v10
%v43515 = vadd.s32 2, %v43511
%v43519 = vadd.s32 %v43515, %v43503
%v43521 = vshll.u32 %v43515, 13
%v43522 = vshrl.u32 %v43515, 19
%v43523 = vor.u32 %v43522, %v43521
%v43524 = vxor.u32 %v43523, %v43519
%v43527 = vadd.s32 %v43524, %v43519
%v43529 = vshll.u32 %v43524, 15
%v43530 = vshrl.u32 %v43524, 17
%v43531 = vor.u32 %v43530, %v43529
%v43532 = vxor.u32 %v43531, %v43527
%v43535 = vadd.s32 %v43532, %v43527
%v43537 = vshll.u32 %v43532, 26
%v43538 = vshrl.u32 %v43532, 6
%v43539 = vor.u32 %v43538, %v43537
%v43540 = vxor.u32 %v43539, %v43535
%v43543 = vadd.s32 %v43540, %v43535
%v43547 = vadd.s32 %v43543, %v10
%v43549 = vshll.u32 %v43540, 6
%v43550 = vshrl.u32 %v43540, 26
%v43551 = vor.u32 %v43550, %v43549
%v43552 = vxor.u32 %v43551, %v43543
%v43555 = vadd.s32 %v43552, %v9
%v43559 = vadd.s32 3, %v43555
%v43563 = vadd.s32 %v43559, %v43547
%v43565 = vshll.u32 %v43559, 17
%v43566 = vshrl.u32 %v43559, 15
%v43567 = vor.u32 %v43566, %v43565
%v43568 = vxor.u32 %v43567, %v43563
%v43571 = vadd.s32 %v43568, %v43563
%v43573 = vshll.u32 %v43568, 29
%v43574 = vshrl.u32 %v43568, 3
%v43575 = vor.u32 %v43574, %v43573
%v43576 = vxor.u32 %v43575, %v43571
%v43579 = vadd.s32 %v43576, %v43571
%v43581 = vshll.u32 %v43576, 16
%v43582 = vshrl.u32 %v43576, 16
%v43583 = vor.u32 %v43582, %v43581
%v43584 = vxor.u32 %v43583, %v43579
%v43587 = vadd.s32 %v43584, %v43579
%v43591 = vadd.s32 %v43587, %v9
%v43593 = vshll.u32 %v43584, 24
%v43594 = vshrl.u32 %v43584, 8
%v43595 = vor.u32 %v43594, %v43593
%v43596 = vxor.u32 %v43595, %v43587
%v43599 = vadd.s32 %v43596, %v8
%v43603 = vadd.s32 4, %v43599
%v43607 = vadd.s32 %v43603, %v43591
%v43609 = vshll.u32 %v43603, 13
%v43610 = vshrl.u32 %v43603, 19
%v43611 = vor.u32 %v43610, %v43609
%v43612 = vxor.u32 %v43611, %v43607
%v43615 = vadd.s32 %v43612, %v43607
%v43617 = vshll.u32 %v43612, 15
%v43618 = vshrl.u32 %v43612, 17
%v43619 = vor.u32 %v43618, %v43617
%v43620 = vxor.u32 %v43619, %v43615
%v43623 = vadd.s32 %v43620, %v43615
%v43625 = vshll.u32 %v43620, 26
%v43626 = vshrl.u32 %v43620, 6
%v43627 = vor.u32 %v43626, %v43625
%v43628 = vxor.u32 %v43627, %v43623
%v43631 = vadd.s32 %v43628, %v43623
%v43635 = vadd.s32 %v43631, %v8
%v43637 = vshll.u32 %v43628, 6
%v43638 = vshrl.u32 %v43628, 26
%v43639 = vor.u32 %v43638, %v43637
%v43640 = vxor.u32 %v43639, %v43631
%v43643 = vadd.s32 %v43640, %v10
%v43647 = vadd.s32 5, %v43643
%v43649 = vxor.u32 %v43647, %v43635
%v43650 = vand.u32.u8 255, %v43649
%v43651 = vand.u32 65535, %v43650
%v43652 = vshrl.u32 %v43651, 1
%v43653 = vor.u32 16256, %v43652
%v43654 = vand.u32.u16 65535, %v43653
%v119976 = vadd.low.f32.bf16 -1.0, %v43654
%v43663 = vmul.f32 2.0, %v119976
%v43667 = vadd.f32 -0.99609375, %v43663
%v43671 = vmax.f32 %v43667, -0.99609375
%v43673 = vand.u32 2147483647, %v43671
%vm43676 = vcmp.eq.f32.partialorder %v43673, 1.0
%v43681 = vmul.f32 inf, %v43671
%v43683 = vxor.u32 2147483648, %v43671
%v43686 = vmul.f32 %v43683, %v43671
%v43688 = vadd.f32 1.0, %v43686
%v43689 = vlog2.pop %v43688
%v43690 = vmul.f32 0.6931472, %v43689
%v43691 = vmul.f32 -0.5, %v43686
%v43692 = vadd.f32 1.0, %v43691
%v43693 = vmul.f32 %v43692, %v43686
%v43694 = vand.u32 2147483647, %v43686
%vm43695 = vcmp.lt.f32.partialorder %v43694, 0.0004427343
%v43696 = vsel /*vm=*/%vm43695, /*on_true_vy=*/%v43693, /*on_false_vx=*/%v43690
%v43697 = vxor.u32 2147483648, %v43696
%vm43700 = vcmp.lt.f32.partialorder %v43697, 5.0
%v43705 = vsel /*vm=*/%vm43700, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v43709 = vsel /*vm=*/%vm43700, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v43713 = vsel /*vm=*/%vm43700, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v43717 = vsel /*vm=*/%vm43700, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v43721 = vsel /*vm=*/%vm43700, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v43725 = vsel /*vm=*/%vm43700, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v43729 = vsel /*vm=*/%vm43700, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v43733 = vsel /*vm=*/%vm43700, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v43737 = vsel /*vm=*/%vm43700, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v43741 = vadd.f32 -2.5, %v43697
%v43743 = vrsqrt.pop %v43697
%v43744 = vmul.f32 %v43743, %v43697
%vm43745 = vcmp.eq.f32.partialorder %v43697, inf
%v43746 = vsel /*vm=*/%vm43745, /*on_true_vy=*/%v43697, /*on_false_vx=*/%v43744
%vm43747 = vcmp.eq.f32.partialorder %v43697, 0.0
%v43748 = vand.u32 2147483648, %v43697
%v43749 = vsel /*vm=*/%vm43747, /*on_true_vy=*/%v43748, /*on_false_vx=*/%v43746
%v43752 = vadd.f32 -3.0, %v43749
%v43756 = vsel /*vm=*/%vm43700, /*on_true_vy=*/%v43741, /*on_false_vx=*/%v43752
%v43760 = vmul.f32 %v43756, %v43737
%v43764 = vadd.f32 %v43760, %v43733
%v43768 = vmul.f32 %v43764, %v43756
%v43772 = vadd.f32 %v43768, %v43729
%v43776 = vmul.f32 %v43772, %v43756
%v43780 = vadd.f32 %v43776, %v43725
%v43784 = vmul.f32 %v43780, %v43756
%v43788 = vadd.f32 %v43784, %v43721
%v43792 = vmul.f32 %v43788, %v43756
%v43796 = vadd.f32 %v43792, %v43717
%v43800 = vmul.f32 %v43796, %v43756
%v43804 = vadd.f32 %v43800, %v43713
%v43808 = vmul.f32 %v43804, %v43756
%v43812 = vadd.f32 %v43808, %v43709
%v43816 = vmul.f32 %v43812, %v43756
%v43820 = vadd.f32 %v43816, %v43705
%v43824 = vmul.f32 %v43820, %v43671
%v43828 = vsel /*vm=*/%vm43676, /*on_true_vy=*/%v43681, /*on_false_vx=*/%v43824
%v43832 = vmul.f32 1.4140625, %v43828
%v43835 = vpack.c.bf16 %v120417, %v43832
%119977 = vst [vmem:[%s280 + $0x22c] sm:$0xf] /*vst_source=*/%v43835
%v43839 = vadd.s32 %v41531, %v2842
%v43849 = vadd.s32 %v43839, %v415
%vm43853 = vcmp.lt.u32.totalorder %v43849, %v43839
%vm43858 = vcmp.lt.u32.totalorder %v43839, %v2842
%v43863 = vadd.s32 %v41514, %v2829
%v43867 = vadd.s32 1, %v43863
%v43871 = vsel /*vm=*/%vm43858, /*on_true_vy=*/%v43867, /*on_false_vx=*/%v43863
%v43875 = vadd.s32 1, %v43871
%v43879 = vsel /*vm=*/%vm43853, /*on_true_vy=*/%v43875, /*on_false_vx=*/%v43871
%v43884 = vadd.s32 %v43879, %v10
%v43888 = vadd.s32 %v43849, %v9
%v43892 = vadd.s32 %v43888, %v43884
%v43894 = vshll.u32 %v43888, 13
%v43895 = vshrl.u32 %v43888, 19
%v43896 = vor.u32 %v43895, %v43894
%v43897 = vxor.u32 %v43896, %v43892
%v43900 = vadd.s32 %v43897, %v43892
%v43902 = vshll.u32 %v43897, 15
%v43903 = vshrl.u32 %v43897, 17
%v43904 = vor.u32 %v43903, %v43902
%v43905 = vxor.u32 %v43904, %v43900
%v43908 = vadd.s32 %v43905, %v43900
%v43910 = vshll.u32 %v43905, 26
%v43911 = vshrl.u32 %v43905, 6
%v43912 = vor.u32 %v43911, %v43910
%v43913 = vxor.u32 %v43912, %v43908
%v43916 = vadd.s32 %v43913, %v43908
%v43920 = vadd.s32 %v43916, %v9
%v43922 = vshll.u32 %v43913, 6
%v43923 = vshrl.u32 %v43913, 26
%v43924 = vor.u32 %v43923, %v43922
%v43925 = vxor.u32 %v43924, %v43916
%v43928 = vadd.s32 %v43925, %v8
%v43932 = vadd.s32 1, %v43928
%v43936 = vadd.s32 %v43932, %v43920
%v43938 = vshll.u32 %v43932, 17
%v43939 = vshrl.u32 %v43932, 15
%v43940 = vor.u32 %v43939, %v43938
%v43941 = vxor.u32 %v43940, %v43936
%v43944 = vadd.s32 %v43941, %v43936
%v43946 = vshll.u32 %v43941, 29
%v43947 = vshrl.u32 %v43941, 3
%v43948 = vor.u32 %v43947, %v43946
%v43949 = vxor.u32 %v43948, %v43944
%v43952 = vadd.s32 %v43949, %v43944
%v43954 = vshll.u32 %v43949, 16
%v43955 = vshrl.u32 %v43949, 16
%v43956 = vor.u32 %v43955, %v43954
%v43957 = vxor.u32 %v43956, %v43952
%v43960 = vadd.s32 %v43957, %v43952
%v43964 = vadd.s32 %v43960, %v8
%v43966 = vshll.u32 %v43957, 24
%v43967 = vshrl.u32 %v43957, 8
%v43968 = vor.u32 %v43967, %v43966
%v43969 = vxor.u32 %v43968, %v43960
%v43972 = vadd.s32 %v43969, %v10
%v43976 = vadd.s32 2, %v43972
%v43980 = vadd.s32 %v43976, %v43964
%v43982 = vshll.u32 %v43976, 13
%v43983 = vshrl.u32 %v43976, 19
%v43984 = vor.u32 %v43983, %v43982
%v43985 = vxor.u32 %v43984, %v43980
%v43988 = vadd.s32 %v43985, %v43980
%v43990 = vshll.u32 %v43985, 15
%v43991 = vshrl.u32 %v43985, 17
%v43992 = vor.u32 %v43991, %v43990
%v43993 = vxor.u32 %v43992, %v43988
%v43996 = vadd.s32 %v43993, %v43988
%v43998 = vshll.u32 %v43993, 26
%v43999 = vshrl.u32 %v43993, 6
%v44000 = vor.u32 %v43999, %v43998
%v44001 = vxor.u32 %v44000, %v43996
%v44004 = vadd.s32 %v44001, %v43996
%v44008 = vadd.s32 %v44004, %v10
%v44010 = vshll.u32 %v44001, 6
%v44011 = vshrl.u32 %v44001, 26
%v44012 = vor.u32 %v44011, %v44010
%v44013 = vxor.u32 %v44012, %v44004
%v44016 = vadd.s32 %v44013, %v9
%v44020 = vadd.s32 3, %v44016
%v44024 = vadd.s32 %v44020, %v44008
%v44026 = vshll.u32 %v44020, 17
%v44027 = vshrl.u32 %v44020, 15
%v44028 = vor.u32 %v44027, %v44026
%v44029 = vxor.u32 %v44028, %v44024
%v44032 = vadd.s32 %v44029, %v44024
%v44034 = vshll.u32 %v44029, 29
%v44035 = vshrl.u32 %v44029, 3
%v44036 = vor.u32 %v44035, %v44034
%v44037 = vxor.u32 %v44036, %v44032
%v44040 = vadd.s32 %v44037, %v44032
%v44042 = vshll.u32 %v44037, 16
%v44043 = vshrl.u32 %v44037, 16
%v44044 = vor.u32 %v44043, %v44042
%v44045 = vxor.u32 %v44044, %v44040
%v44048 = vadd.s32 %v44045, %v44040
%v44052 = vadd.s32 %v44048, %v9
%v44054 = vshll.u32 %v44045, 24
%v44055 = vshrl.u32 %v44045, 8
%v44056 = vor.u32 %v44055, %v44054
%v44057 = vxor.u32 %v44056, %v44048
%v44060 = vadd.s32 %v44057, %v8
%v44064 = vadd.s32 4, %v44060
%v44068 = vadd.s32 %v44064, %v44052
%v44070 = vshll.u32 %v44064, 13
%v44071 = vshrl.u32 %v44064, 19
%v44072 = vor.u32 %v44071, %v44070
%v44073 = vxor.u32 %v44072, %v44068
%v44076 = vadd.s32 %v44073, %v44068
%v44078 = vshll.u32 %v44073, 15
%v44079 = vshrl.u32 %v44073, 17
%v44080 = vor.u32 %v44079, %v44078
%v44081 = vxor.u32 %v44080, %v44076
%v44084 = vadd.s32 %v44081, %v44076
%v44086 = vshll.u32 %v44081, 26
%v44087 = vshrl.u32 %v44081, 6
%v44088 = vor.u32 %v44087, %v44086
%v44089 = vxor.u32 %v44088, %v44084
%v44092 = vadd.s32 %v44089, %v44084
%v44096 = vadd.s32 %v44092, %v8
%v44098 = vshll.u32 %v44089, 6
%v44099 = vshrl.u32 %v44089, 26
%v44100 = vor.u32 %v44099, %v44098
%v44101 = vxor.u32 %v44100, %v44092
%v44104 = vadd.s32 %v44101, %v10
%v44108 = vadd.s32 5, %v44104
%v44110 = vxor.u32 %v44108, %v44096
%v44111 = vand.u32.u8 255, %v44110
%v44112 = vand.u32 65535, %v44111
%v44113 = vshrl.u32 %v44112, 1
%v44114 = vor.u32 16256, %v44113
%v44115 = vand.u32.u16 65535, %v44114
%v119978 = vadd.low.f32.bf16 -1.0, %v44115
%v44124 = vmul.f32 2.0, %v119978
%v44128 = vadd.f32 -0.99609375, %v44124
%v44132 = vmax.f32 %v44128, -0.99609375
%v44134 = vand.u32 2147483647, %v44132
%vm44137 = vcmp.eq.f32.partialorder %v44134, 1.0
%v44142 = vmul.f32 inf, %v44132
%v44144 = vxor.u32 2147483648, %v44132
%v44147 = vmul.f32 %v44144, %v44132
%v44149 = vadd.f32 1.0, %v44147
%v44150 = vlog2.pop %v44149
%v44151 = vmul.f32 0.6931472, %v44150
%v44152 = vmul.f32 -0.5, %v44147
%v44153 = vadd.f32 1.0, %v44152
%v44154 = vmul.f32 %v44153, %v44147
%v44155 = vand.u32 2147483647, %v44147
%vm44156 = vcmp.lt.f32.partialorder %v44155, 0.0004427343
%v44157 = vsel /*vm=*/%vm44156, /*on_true_vy=*/%v44154, /*on_false_vx=*/%v44151
%v44158 = vxor.u32 2147483648, %v44157
%vm44161 = vcmp.lt.f32.partialorder %v44158, 5.0
%v44166 = vsel /*vm=*/%vm44161, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v44170 = vsel /*vm=*/%vm44161, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v44174 = vsel /*vm=*/%vm44161, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v44178 = vsel /*vm=*/%vm44161, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v44182 = vsel /*vm=*/%vm44161, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v44186 = vsel /*vm=*/%vm44161, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v44190 = vsel /*vm=*/%vm44161, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v44194 = vsel /*vm=*/%vm44161, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v44198 = vsel /*vm=*/%vm44161, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v44202 = vadd.f32 -2.5, %v44158
%v44204 = vrsqrt.pop %v44158
%v44205 = vmul.f32 %v44204, %v44158
%vm44206 = vcmp.eq.f32.partialorder %v44158, inf
%v44207 = vsel /*vm=*/%vm44206, /*on_true_vy=*/%v44158, /*on_false_vx=*/%v44205
%vm44208 = vcmp.eq.f32.partialorder %v44158, 0.0
%v44209 = vand.u32 2147483648, %v44158
%v44210 = vsel /*vm=*/%vm44208, /*on_true_vy=*/%v44209, /*on_false_vx=*/%v44207
%v44213 = vadd.f32 -3.0, %v44210
%v44217 = vsel /*vm=*/%vm44161, /*on_true_vy=*/%v44202, /*on_false_vx=*/%v44213
%v44221 = vmul.f32 %v44217, %v44198
%v44225 = vadd.f32 %v44221, %v44194
%v44229 = vmul.f32 %v44225, %v44217
%v44233 = vadd.f32 %v44229, %v44190
%v44237 = vmul.f32 %v44233, %v44217
%v44241 = vadd.f32 %v44237, %v44186
%v44245 = vmul.f32 %v44241, %v44217
%v44249 = vadd.f32 %v44245, %v44182
%v44253 = vmul.f32 %v44249, %v44217
%v44257 = vadd.f32 %v44253, %v44178
%v44261 = vmul.f32 %v44257, %v44217
%v44265 = vadd.f32 %v44261, %v44174
%v44269 = vmul.f32 %v44265, %v44217
%v44273 = vadd.f32 %v44269, %v44170
%v44277 = vmul.f32 %v44273, %v44217
%v44281 = vadd.f32 %v44277, %v44166
%v44285 = vmul.f32 %v44281, %v44132
%v44289 = vsel /*vm=*/%vm44137, /*on_true_vy=*/%v44142, /*on_false_vx=*/%v44285
%v44293 = vmul.f32 1.4140625, %v44289
%v44296 = vpack.c.bf16 %v120417, %v44293
%119979 = vst [vmem:[%s280 + $0x2ac] sm:$0xf] /*vst_source=*/%v44296
%v44300 = vadd.s32 %v41531, %v3329
%v44310 = vadd.s32 %v44300, %v415
%vm44314 = vcmp.lt.u32.totalorder %v44310, %v44300
%vm44319 = vcmp.lt.u32.totalorder %v44300, %v3329
%v44324 = vadd.s32 %v41514, %v3316
%v44328 = vadd.s32 1, %v44324
%v44332 = vsel /*vm=*/%vm44319, /*on_true_vy=*/%v44328, /*on_false_vx=*/%v44324
%v44336 = vadd.s32 1, %v44332
%v44340 = vsel /*vm=*/%vm44314, /*on_true_vy=*/%v44336, /*on_false_vx=*/%v44332
%v44345 = vadd.s32 %v44340, %v10
%v44349 = vadd.s32 %v44310, %v9
%v44353 = vadd.s32 %v44349, %v44345
%v44355 = vshll.u32 %v44349, 13
%v44356 = vshrl.u32 %v44349, 19
%v44357 = vor.u32 %v44356, %v44355
%v44358 = vxor.u32 %v44357, %v44353
%v44361 = vadd.s32 %v44358, %v44353
%v44363 = vshll.u32 %v44358, 15
%v44364 = vshrl.u32 %v44358, 17
%v44365 = vor.u32 %v44364, %v44363
%v44366 = vxor.u32 %v44365, %v44361
%v44369 = vadd.s32 %v44366, %v44361
%v44371 = vshll.u32 %v44366, 26
%v44372 = vshrl.u32 %v44366, 6
%v44373 = vor.u32 %v44372, %v44371
%v44374 = vxor.u32 %v44373, %v44369
%v44377 = vadd.s32 %v44374, %v44369
%v44381 = vadd.s32 %v44377, %v9
%v44383 = vshll.u32 %v44374, 6
%v44384 = vshrl.u32 %v44374, 26
%v44385 = vor.u32 %v44384, %v44383
%v44386 = vxor.u32 %v44385, %v44377
%v44389 = vadd.s32 %v44386, %v8
%v44393 = vadd.s32 1, %v44389
%v44397 = vadd.s32 %v44393, %v44381
%v44399 = vshll.u32 %v44393, 17
%v44400 = vshrl.u32 %v44393, 15
%v44401 = vor.u32 %v44400, %v44399
%v44402 = vxor.u32 %v44401, %v44397
%v44405 = vadd.s32 %v44402, %v44397
%v44407 = vshll.u32 %v44402, 29
%v44408 = vshrl.u32 %v44402, 3
%v44409 = vor.u32 %v44408, %v44407
%v44410 = vxor.u32 %v44409, %v44405
%v44413 = vadd.s32 %v44410, %v44405
%v44415 = vshll.u32 %v44410, 16
%v44416 = vshrl.u32 %v44410, 16
%v44417 = vor.u32 %v44416, %v44415
%v44418 = vxor.u32 %v44417, %v44413
%v44421 = vadd.s32 %v44418, %v44413
%v44425 = vadd.s32 %v44421, %v8
%v44427 = vshll.u32 %v44418, 24
%v44428 = vshrl.u32 %v44418, 8
%v44429 = vor.u32 %v44428, %v44427
%v44430 = vxor.u32 %v44429, %v44421
%v44433 = vadd.s32 %v44430, %v10
%v44437 = vadd.s32 2, %v44433
%v44441 = vadd.s32 %v44437, %v44425
%v44443 = vshll.u32 %v44437, 13
%v44444 = vshrl.u32 %v44437, 19
%v44445 = vor.u32 %v44444, %v44443
%v44446 = vxor.u32 %v44445, %v44441
%v44449 = vadd.s32 %v44446, %v44441
%v44451 = vshll.u32 %v44446, 15
%v44452 = vshrl.u32 %v44446, 17
%v44453 = vor.u32 %v44452, %v44451
%v44454 = vxor.u32 %v44453, %v44449
%v44457 = vadd.s32 %v44454, %v44449
%v44459 = vshll.u32 %v44454, 26
%v44460 = vshrl.u32 %v44454, 6
%v44461 = vor.u32 %v44460, %v44459
%v44462 = vxor.u32 %v44461, %v44457
%v44465 = vadd.s32 %v44462, %v44457
%v44469 = vadd.s32 %v44465, %v10
%v44471 = vshll.u32 %v44462, 6
%v44472 = vshrl.u32 %v44462, 26
%v44473 = vor.u32 %v44472, %v44471
%v44474 = vxor.u32 %v44473, %v44465
%v44477 = vadd.s32 %v44474, %v9
%v44481 = vadd.s32 3, %v44477
%v44485 = vadd.s32 %v44481, %v44469
%v44487 = vshll.u32 %v44481, 17
%v44488 = vshrl.u32 %v44481, 15
%v44489 = vor.u32 %v44488, %v44487
%v44490 = vxor.u32 %v44489, %v44485
%v44493 = vadd.s32 %v44490, %v44485
%v44495 = vshll.u32 %v44490, 29
%v44496 = vshrl.u32 %v44490, 3
%v44497 = vor.u32 %v44496, %v44495
%v44498 = vxor.u32 %v44497, %v44493
%v44501 = vadd.s32 %v44498, %v44493
%v44503 = vshll.u32 %v44498, 16
%v44504 = vshrl.u32 %v44498, 16
%v44505 = vor.u32 %v44504, %v44503
%v44506 = vxor.u32 %v44505, %v44501
%v44509 = vadd.s32 %v44506, %v44501
%v44513 = vadd.s32 %v44509, %v9
%v44515 = vshll.u32 %v44506, 24
%v44516 = vshrl.u32 %v44506, 8
%v44517 = vor.u32 %v44516, %v44515
%v44518 = vxor.u32 %v44517, %v44509
%v44521 = vadd.s32 %v44518, %v8
%v44525 = vadd.s32 4, %v44521
%v44529 = vadd.s32 %v44525, %v44513
%v44531 = vshll.u32 %v44525, 13
%v44532 = vshrl.u32 %v44525, 19
%v44533 = vor.u32 %v44532, %v44531
%v44534 = vxor.u32 %v44533, %v44529
%v44537 = vadd.s32 %v44534, %v44529
%v44539 = vshll.u32 %v44534, 15
%v44540 = vshrl.u32 %v44534, 17
%v44541 = vor.u32 %v44540, %v44539
%v44542 = vxor.u32 %v44541, %v44537
%v44545 = vadd.s32 %v44542, %v44537
%v44547 = vshll.u32 %v44542, 26
%v44548 = vshrl.u32 %v44542, 6
%v44549 = vor.u32 %v44548, %v44547
%v44550 = vxor.u32 %v44549, %v44545
%v44553 = vadd.s32 %v44550, %v44545
%v44557 = vadd.s32 %v44553, %v8
%v44559 = vshll.u32 %v44550, 6
%v44560 = vshrl.u32 %v44550, 26
%v44561 = vor.u32 %v44560, %v44559
%v44562 = vxor.u32 %v44561, %v44553
%v44565 = vadd.s32 %v44562, %v10
%v44569 = vadd.s32 5, %v44565
%v44571 = vxor.u32 %v44569, %v44557
%v44572 = vand.u32.u8 255, %v44571
%v44573 = vand.u32 65535, %v44572
%v44574 = vshrl.u32 %v44573, 1
%v44575 = vor.u32 16256, %v44574
%v44576 = vand.u32.u16 65535, %v44575
%v119980 = vadd.low.f32.bf16 -1.0, %v44576
%v44585 = vmul.f32 2.0, %v119980
%v44589 = vadd.f32 -0.99609375, %v44585
%v44593 = vmax.f32 %v44589, -0.99609375
%v44595 = vand.u32 2147483647, %v44593
%vm44598 = vcmp.eq.f32.partialorder %v44595, 1.0
%v44603 = vmul.f32 inf, %v44593
%v44605 = vxor.u32 2147483648, %v44593
%v44608 = vmul.f32 %v44605, %v44593
%v44610 = vadd.f32 1.0, %v44608
%v44611 = vlog2.pop %v44610
%v44612 = vmul.f32 0.6931472, %v44611
%v44613 = vmul.f32 -0.5, %v44608
%v44614 = vadd.f32 1.0, %v44613
%v44615 = vmul.f32 %v44614, %v44608
%v44616 = vand.u32 2147483647, %v44608
%vm44617 = vcmp.lt.f32.partialorder %v44616, 0.0004427343
%v44618 = vsel /*vm=*/%vm44617, /*on_true_vy=*/%v44615, /*on_false_vx=*/%v44612
%v44619 = vxor.u32 2147483648, %v44618
%vm44622 = vcmp.lt.f32.partialorder %v44619, 5.0
%v44627 = vsel /*vm=*/%vm44622, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v44631 = vsel /*vm=*/%vm44622, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v44635 = vsel /*vm=*/%vm44622, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v44639 = vsel /*vm=*/%vm44622, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v44643 = vsel /*vm=*/%vm44622, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v44647 = vsel /*vm=*/%vm44622, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v44651 = vsel /*vm=*/%vm44622, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v44655 = vsel /*vm=*/%vm44622, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v44659 = vsel /*vm=*/%vm44622, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v44663 = vadd.f32 -2.5, %v44619
%v44665 = vrsqrt.pop %v44619
%v44666 = vmul.f32 %v44665, %v44619
%vm44667 = vcmp.eq.f32.partialorder %v44619, inf
%v44668 = vsel /*vm=*/%vm44667, /*on_true_vy=*/%v44619, /*on_false_vx=*/%v44666
%vm44669 = vcmp.eq.f32.partialorder %v44619, 0.0
%v44670 = vand.u32 2147483648, %v44619
%v44671 = vsel /*vm=*/%vm44669, /*on_true_vy=*/%v44670, /*on_false_vx=*/%v44668
%v44674 = vadd.f32 -3.0, %v44671
%v44678 = vsel /*vm=*/%vm44622, /*on_true_vy=*/%v44663, /*on_false_vx=*/%v44674
%v44682 = vmul.f32 %v44678, %v44659
%v44686 = vadd.f32 %v44682, %v44655
%v44690 = vmul.f32 %v44686, %v44678
%v44694 = vadd.f32 %v44690, %v44651
%v44698 = vmul.f32 %v44694, %v44678
%v44702 = vadd.f32 %v44698, %v44647
%v44706 = vmul.f32 %v44702, %v44678
%v44710 = vadd.f32 %v44706, %v44643
%v44714 = vmul.f32 %v44710, %v44678
%v44718 = vadd.f32 %v44714, %v44639
%v44722 = vmul.f32 %v44718, %v44678
%v44726 = vadd.f32 %v44722, %v44635
%v44730 = vmul.f32 %v44726, %v44678
%v44734 = vadd.f32 %v44730, %v44631
%v44738 = vmul.f32 %v44734, %v44678
%v44742 = vadd.f32 %v44738, %v44627
%v44746 = vmul.f32 %v44742, %v44593
%v44750 = vsel /*vm=*/%vm44598, /*on_true_vy=*/%v44603, /*on_false_vx=*/%v44746
%v44754 = vmul.f32 1.4140625, %v44750
%v44757 = vpack.c.bf16 %v120417, %v44754
%119981 = vst [vmem:[%s280 + $0x32c] sm:$0xf] /*vst_source=*/%v44757
%v44761 = vadd.s32 %v41531, %v3816
%v44771 = vadd.s32 %v44761, %v415
%vm44775 = vcmp.lt.u32.totalorder %v44771, %v44761
%vm44780 = vcmp.lt.u32.totalorder %v44761, %v3816
%v44785 = vadd.s32 %v41514, %v3803
%v44789 = vadd.s32 1, %v44785
%v44793 = vsel /*vm=*/%vm44780, /*on_true_vy=*/%v44789, /*on_false_vx=*/%v44785
%v44797 = vadd.s32 1, %v44793
%v44801 = vsel /*vm=*/%vm44775, /*on_true_vy=*/%v44797, /*on_false_vx=*/%v44793
%v44806 = vadd.s32 %v44801, %v10
%v44810 = vadd.s32 %v44771, %v9
%v44814 = vadd.s32 %v44810, %v44806
%v44816 = vshll.u32 %v44810, 13
%v44817 = vshrl.u32 %v44810, 19
%v44818 = vor.u32 %v44817, %v44816
%v44819 = vxor.u32 %v44818, %v44814
%v44822 = vadd.s32 %v44819, %v44814
%v44824 = vshll.u32 %v44819, 15
%v44825 = vshrl.u32 %v44819, 17
%v44826 = vor.u32 %v44825, %v44824
%v44827 = vxor.u32 %v44826, %v44822
%v44830 = vadd.s32 %v44827, %v44822
%v44832 = vshll.u32 %v44827, 26
%v44833 = vshrl.u32 %v44827, 6
%v44834 = vor.u32 %v44833, %v44832
%v44835 = vxor.u32 %v44834, %v44830
%v44838 = vadd.s32 %v44835, %v44830
%v44842 = vadd.s32 %v44838, %v9
%v44844 = vshll.u32 %v44835, 6
%v44845 = vshrl.u32 %v44835, 26
%v44846 = vor.u32 %v44845, %v44844
%v44847 = vxor.u32 %v44846, %v44838
%v44850 = vadd.s32 %v44847, %v8
%v44854 = vadd.s32 1, %v44850
%v44858 = vadd.s32 %v44854, %v44842
%v44860 = vshll.u32 %v44854, 17
%v44861 = vshrl.u32 %v44854, 15
%v44862 = vor.u32 %v44861, %v44860
%v44863 = vxor.u32 %v44862, %v44858
%v44866 = vadd.s32 %v44863, %v44858
%v44868 = vshll.u32 %v44863, 29
%v44869 = vshrl.u32 %v44863, 3
%v44870 = vor.u32 %v44869, %v44868
%v44871 = vxor.u32 %v44870, %v44866
%v44874 = vadd.s32 %v44871, %v44866
%v44876 = vshll.u32 %v44871, 16
%v44877 = vshrl.u32 %v44871, 16
%v44878 = vor.u32 %v44877, %v44876
%v44879 = vxor.u32 %v44878, %v44874
%v44882 = vadd.s32 %v44879, %v44874
%v44886 = vadd.s32 %v44882, %v8
%v44888 = vshll.u32 %v44879, 24
%v44889 = vshrl.u32 %v44879, 8
%v44890 = vor.u32 %v44889, %v44888
%v44891 = vxor.u32 %v44890, %v44882
%v44894 = vadd.s32 %v44891, %v10
%v44898 = vadd.s32 2, %v44894
%v44902 = vadd.s32 %v44898, %v44886
%v44904 = vshll.u32 %v44898, 13
%v44905 = vshrl.u32 %v44898, 19
%v44906 = vor.u32 %v44905, %v44904
%v44907 = vxor.u32 %v44906, %v44902
%v44910 = vadd.s32 %v44907, %v44902
%v44912 = vshll.u32 %v44907, 15
%v44913 = vshrl.u32 %v44907, 17
%v44914 = vor.u32 %v44913, %v44912
%v44915 = vxor.u32 %v44914, %v44910
%v44918 = vadd.s32 %v44915, %v44910
%v44920 = vshll.u32 %v44915, 26
%v44921 = vshrl.u32 %v44915, 6
%v44922 = vor.u32 %v44921, %v44920
%v44923 = vxor.u32 %v44922, %v44918
%v44926 = vadd.s32 %v44923, %v44918
%v44930 = vadd.s32 %v44926, %v10
%v44932 = vshll.u32 %v44923, 6
%v44933 = vshrl.u32 %v44923, 26
%v44934 = vor.u32 %v44933, %v44932
%v44935 = vxor.u32 %v44934, %v44926
%v44938 = vadd.s32 %v44935, %v9
%v44942 = vadd.s32 3, %v44938
%v44946 = vadd.s32 %v44942, %v44930
%v44948 = vshll.u32 %v44942, 17
%v44949 = vshrl.u32 %v44942, 15
%v44950 = vor.u32 %v44949, %v44948
%v44951 = vxor.u32 %v44950, %v44946
%v44954 = vadd.s32 %v44951, %v44946
%v44956 = vshll.u32 %v44951, 29
%v44957 = vshrl.u32 %v44951, 3
%v44958 = vor.u32 %v44957, %v44956
%v44959 = vxor.u32 %v44958, %v44954
%v44962 = vadd.s32 %v44959, %v44954
%v44964 = vshll.u32 %v44959, 16
%v44965 = vshrl.u32 %v44959, 16
%v44966 = vor.u32 %v44965, %v44964
%v44967 = vxor.u32 %v44966, %v44962
%v44970 = vadd.s32 %v44967, %v44962
%v44974 = vadd.s32 %v44970, %v9
%v44976 = vshll.u32 %v44967, 24
%v44977 = vshrl.u32 %v44967, 8
%v44978 = vor.u32 %v44977, %v44976
%v44979 = vxor.u32 %v44978, %v44970
%v44982 = vadd.s32 %v44979, %v8
%v44986 = vadd.s32 4, %v44982
%v44990 = vadd.s32 %v44986, %v44974
%v44992 = vshll.u32 %v44986, 13
%v44993 = vshrl.u32 %v44986, 19
%v44994 = vor.u32 %v44993, %v44992
%v44995 = vxor.u32 %v44994, %v44990
%v44998 = vadd.s32 %v44995, %v44990
%v45000 = vshll.u32 %v44995, 15
%v45001 = vshrl.u32 %v44995, 17
%v45002 = vor.u32 %v45001, %v45000
%v45003 = vxor.u32 %v45002, %v44998
%v45006 = vadd.s32 %v45003, %v44998
%v45008 = vshll.u32 %v45003, 26
%v45009 = vshrl.u32 %v45003, 6
%v45010 = vor.u32 %v45009, %v45008
%v45011 = vxor.u32 %v45010, %v45006
%v45014 = vadd.s32 %v45011, %v45006
%v45018 = vadd.s32 %v45014, %v8
%v45020 = vshll.u32 %v45011, 6
%v45021 = vshrl.u32 %v45011, 26
%v45022 = vor.u32 %v45021, %v45020
%v45023 = vxor.u32 %v45022, %v45014
%v45026 = vadd.s32 %v45023, %v10
%v45030 = vadd.s32 5, %v45026
%v45032 = vxor.u32 %v45030, %v45018
%v45033 = vand.u32.u8 255, %v45032
%v45034 = vand.u32 65535, %v45033
%v45035 = vshrl.u32 %v45034, 1
%v45036 = vor.u32 16256, %v45035
%v45037 = vand.u32.u16 65535, %v45036
%v119982 = vadd.low.f32.bf16 -1.0, %v45037
%v45046 = vmul.f32 2.0, %v119982
%v45050 = vadd.f32 -0.99609375, %v45046
%v45054 = vmax.f32 %v45050, -0.99609375
%v45056 = vand.u32 2147483647, %v45054
%vm45059 = vcmp.eq.f32.partialorder %v45056, 1.0
%v45064 = vmul.f32 inf, %v45054
%v45066 = vxor.u32 2147483648, %v45054
%v45069 = vmul.f32 %v45066, %v45054
%v45071 = vadd.f32 1.0, %v45069
%v45072 = vlog2.pop %v45071
%v45073 = vmul.f32 0.6931472, %v45072
%v45074 = vmul.f32 -0.5, %v45069
%v45075 = vadd.f32 1.0, %v45074
%v45076 = vmul.f32 %v45075, %v45069
%v45077 = vand.u32 2147483647, %v45069
%vm45078 = vcmp.lt.f32.partialorder %v45077, 0.0004427343
%v45079 = vsel /*vm=*/%vm45078, /*on_true_vy=*/%v45076, /*on_false_vx=*/%v45073
%v45080 = vxor.u32 2147483648, %v45079
%vm45083 = vcmp.lt.f32.partialorder %v45080, 5.0
%v45088 = vsel /*vm=*/%vm45083, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v45092 = vsel /*vm=*/%vm45083, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v45096 = vsel /*vm=*/%vm45083, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v45100 = vsel /*vm=*/%vm45083, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v45104 = vsel /*vm=*/%vm45083, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v45108 = vsel /*vm=*/%vm45083, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v45112 = vsel /*vm=*/%vm45083, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v45116 = vsel /*vm=*/%vm45083, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v45120 = vsel /*vm=*/%vm45083, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v45124 = vadd.f32 -2.5, %v45080
%v45126 = vrsqrt.pop %v45080
%v45127 = vmul.f32 %v45126, %v45080
%vm45128 = vcmp.eq.f32.partialorder %v45080, inf
%v45129 = vsel /*vm=*/%vm45128, /*on_true_vy=*/%v45080, /*on_false_vx=*/%v45127
%vm45130 = vcmp.eq.f32.partialorder %v45080, 0.0
%v45131 = vand.u32 2147483648, %v45080
%v45132 = vsel /*vm=*/%vm45130, /*on_true_vy=*/%v45131, /*on_false_vx=*/%v45129
%v45135 = vadd.f32 -3.0, %v45132
%v45139 = vsel /*vm=*/%vm45083, /*on_true_vy=*/%v45124, /*on_false_vx=*/%v45135
%v45143 = vmul.f32 %v45139, %v45120
%v45147 = vadd.f32 %v45143, %v45116
%v45151 = vmul.f32 %v45147, %v45139
%v45155 = vadd.f32 %v45151, %v45112
%v45159 = vmul.f32 %v45155, %v45139
%v45163 = vadd.f32 %v45159, %v45108
%v45167 = vmul.f32 %v45163, %v45139
%v45171 = vadd.f32 %v45167, %v45104
%v45175 = vmul.f32 %v45171, %v45139
%v45179 = vadd.f32 %v45175, %v45100
%v45183 = vmul.f32 %v45179, %v45139
%v45187 = vadd.f32 %v45183, %v45096
%v45191 = vmul.f32 %v45187, %v45139
%v45195 = vadd.f32 %v45191, %v45092
%v45199 = vmul.f32 %v45195, %v45139
%v45203 = vadd.f32 %v45199, %v45088
%v45207 = vmul.f32 %v45203, %v45054
%v45211 = vsel /*vm=*/%vm45059, /*on_true_vy=*/%v45064, /*on_false_vx=*/%v45207
%v45215 = vmul.f32 1.4140625, %v45211
%v45218 = vpack.c.bf16 %v120417, %v45215
%119983 = vst [vmem:[%s280 + $0x3ac] sm:$0xf] /*vst_source=*/%v45218
%v45256 = vadd.s32 %v45253, %v408
%v45266 = vadd.s32 %v45256, %v415
%vm45270 = vcmp.lt.u32.totalorder %v45266, %v45256
%vm45275 = vcmp.lt.u32.totalorder %v45256, %v408
%v45280 = vadd.s32 %v45236, %v380
%v45284 = vadd.s32 1, %v45280
%v45288 = vsel /*vm=*/%vm45275, /*on_true_vy=*/%v45284, /*on_false_vx=*/%v45280
%v45292 = vadd.s32 1, %v45288
%v45296 = vsel /*vm=*/%vm45270, /*on_true_vy=*/%v45292, /*on_false_vx=*/%v45288
%v45301 = vadd.s32 %v45296, %v10
%v45305 = vadd.s32 %v45266, %v9
%v45309 = vadd.s32 %v45305, %v45301
%v45311 = vshll.u32 %v45305, 13
%v45312 = vshrl.u32 %v45305, 19
%v45313 = vor.u32 %v45312, %v45311
%v45314 = vxor.u32 %v45313, %v45309
%v45317 = vadd.s32 %v45314, %v45309
%v45319 = vshll.u32 %v45314, 15
%v45320 = vshrl.u32 %v45314, 17
%v45321 = vor.u32 %v45320, %v45319
%v45322 = vxor.u32 %v45321, %v45317
%v45325 = vadd.s32 %v45322, %v45317
%v45327 = vshll.u32 %v45322, 26
%v45328 = vshrl.u32 %v45322, 6
%v45329 = vor.u32 %v45328, %v45327
%v45330 = vxor.u32 %v45329, %v45325
%v45333 = vadd.s32 %v45330, %v45325
%v45337 = vadd.s32 %v45333, %v9
%v45339 = vshll.u32 %v45330, 6
%v45340 = vshrl.u32 %v45330, 26
%v45341 = vor.u32 %v45340, %v45339
%v45342 = vxor.u32 %v45341, %v45333
%v45345 = vadd.s32 %v45342, %v8
%v45349 = vadd.s32 1, %v45345
%v45353 = vadd.s32 %v45349, %v45337
%v45355 = vshll.u32 %v45349, 17
%v45356 = vshrl.u32 %v45349, 15
%v45357 = vor.u32 %v45356, %v45355
%v45358 = vxor.u32 %v45357, %v45353
%v45361 = vadd.s32 %v45358, %v45353
%v45363 = vshll.u32 %v45358, 29
%v45364 = vshrl.u32 %v45358, 3
%v45365 = vor.u32 %v45364, %v45363
%v45366 = vxor.u32 %v45365, %v45361
%v45369 = vadd.s32 %v45366, %v45361
%v45371 = vshll.u32 %v45366, 16
%v45372 = vshrl.u32 %v45366, 16
%v45373 = vor.u32 %v45372, %v45371
%v45374 = vxor.u32 %v45373, %v45369
%v45377 = vadd.s32 %v45374, %v45369
%v45381 = vadd.s32 %v45377, %v8
%v45383 = vshll.u32 %v45374, 24
%v45384 = vshrl.u32 %v45374, 8
%v45385 = vor.u32 %v45384, %v45383
%v45386 = vxor.u32 %v45385, %v45377
%v45389 = vadd.s32 %v45386, %v10
%v45393 = vadd.s32 2, %v45389
%v45397 = vadd.s32 %v45393, %v45381
%v45399 = vshll.u32 %v45393, 13
%v45400 = vshrl.u32 %v45393, 19
%v45401 = vor.u32 %v45400, %v45399
%v45402 = vxor.u32 %v45401, %v45397
%v45405 = vadd.s32 %v45402, %v45397
%v45407 = vshll.u32 %v45402, 15
%v45408 = vshrl.u32 %v45402, 17
%v45409 = vor.u32 %v45408, %v45407
%v45410 = vxor.u32 %v45409, %v45405
%v45413 = vadd.s32 %v45410, %v45405
%v45415 = vshll.u32 %v45410, 26
%v45416 = vshrl.u32 %v45410, 6
%v45417 = vor.u32 %v45416, %v45415
%v45418 = vxor.u32 %v45417, %v45413
%v45421 = vadd.s32 %v45418, %v45413
%v45425 = vadd.s32 %v45421, %v10
%v45427 = vshll.u32 %v45418, 6
%v45428 = vshrl.u32 %v45418, 26
%v45429 = vor.u32 %v45428, %v45427
%v45430 = vxor.u32 %v45429, %v45421
%v45433 = vadd.s32 %v45430, %v9
%v45437 = vadd.s32 3, %v45433
%v45441 = vadd.s32 %v45437, %v45425
%v45443 = vshll.u32 %v45437, 17
%v45444 = vshrl.u32 %v45437, 15
%v45445 = vor.u32 %v45444, %v45443
%v45446 = vxor.u32 %v45445, %v45441
%v45449 = vadd.s32 %v45446, %v45441
%v45451 = vshll.u32 %v45446, 29
%v45452 = vshrl.u32 %v45446, 3
%v45453 = vor.u32 %v45452, %v45451
%v45454 = vxor.u32 %v45453, %v45449
%v45457 = vadd.s32 %v45454, %v45449
%v45459 = vshll.u32 %v45454, 16
%v45460 = vshrl.u32 %v45454, 16
%v45461 = vor.u32 %v45460, %v45459
%v45462 = vxor.u32 %v45461, %v45457
%v45465 = vadd.s32 %v45462, %v45457
%v45469 = vadd.s32 %v45465, %v9
%v45471 = vshll.u32 %v45462, 24
%v45472 = vshrl.u32 %v45462, 8
%v45473 = vor.u32 %v45472, %v45471
%v45474 = vxor.u32 %v45473, %v45465
%v45477 = vadd.s32 %v45474, %v8
%v45481 = vadd.s32 4, %v45477
%v45485 = vadd.s32 %v45481, %v45469
%v45487 = vshll.u32 %v45481, 13
%v45488 = vshrl.u32 %v45481, 19
%v45489 = vor.u32 %v45488, %v45487
%v45490 = vxor.u32 %v45489, %v45485
%v45493 = vadd.s32 %v45490, %v45485
%v45495 = vshll.u32 %v45490, 15
%v45496 = vshrl.u32 %v45490, 17
%v45497 = vor.u32 %v45496, %v45495
%v45498 = vxor.u32 %v45497, %v45493
%v45501 = vadd.s32 %v45498, %v45493
%v45503 = vshll.u32 %v45498, 26
%v45504 = vshrl.u32 %v45498, 6
%v45505 = vor.u32 %v45504, %v45503
%v45506 = vxor.u32 %v45505, %v45501
%v45509 = vadd.s32 %v45506, %v45501
%v45513 = vadd.s32 %v45509, %v8
%v45515 = vshll.u32 %v45506, 6
%v45516 = vshrl.u32 %v45506, 26
%v45517 = vor.u32 %v45516, %v45515
%v45518 = vxor.u32 %v45517, %v45509
%v45521 = vadd.s32 %v45518, %v10
%v45525 = vadd.s32 5, %v45521
%v45527 = vxor.u32 %v45525, %v45513
%v45528 = vand.u32.u8 255, %v45527
%v45529 = vand.u32 65535, %v45528
%v45530 = vshrl.u32 %v45529, 1
%v45531 = vor.u32 16256, %v45530
%v45532 = vand.u32.u16 65535, %v45531
%v119988 = vadd.low.f32.bf16 -1.0, %v45532
%v45541 = vmul.f32 2.0, %v119988
%v45545 = vadd.f32 -0.99609375, %v45541
%v45549 = vmax.f32 %v45545, -0.99609375
%v45551 = vand.u32 2147483647, %v45549
%vm45554 = vcmp.eq.f32.partialorder %v45551, 1.0
%v45559 = vmul.f32 inf, %v45549
%v45561 = vxor.u32 2147483648, %v45549
%v45564 = vmul.f32 %v45561, %v45549
%v45566 = vadd.f32 1.0, %v45564
%v45567 = vlog2.pop %v45566
%v45568 = vmul.f32 0.6931472, %v45567
%v45569 = vmul.f32 -0.5, %v45564
%v45570 = vadd.f32 1.0, %v45569
%v45571 = vmul.f32 %v45570, %v45564
%v45572 = vand.u32 2147483647, %v45564
%vm45573 = vcmp.lt.f32.partialorder %v45572, 0.0004427343
%v45574 = vsel /*vm=*/%vm45573, /*on_true_vy=*/%v45571, /*on_false_vx=*/%v45568
%v45575 = vxor.u32 2147483648, %v45574
%vm45578 = vcmp.lt.f32.partialorder %v45575, 5.0
%v45583 = vsel /*vm=*/%vm45578, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v45587 = vsel /*vm=*/%vm45578, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v45591 = vsel /*vm=*/%vm45578, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v45595 = vsel /*vm=*/%vm45578, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v45599 = vsel /*vm=*/%vm45578, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v45603 = vsel /*vm=*/%vm45578, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v45607 = vsel /*vm=*/%vm45578, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v45611 = vsel /*vm=*/%vm45578, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v45615 = vsel /*vm=*/%vm45578, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v45619 = vadd.f32 -2.5, %v45575
%v45621 = vrsqrt.pop %v45575
%v45622 = vmul.f32 %v45621, %v45575
%vm45623 = vcmp.eq.f32.partialorder %v45575, inf
%v45624 = vsel /*vm=*/%vm45623, /*on_true_vy=*/%v45575, /*on_false_vx=*/%v45622
%vm45625 = vcmp.eq.f32.partialorder %v45575, 0.0
%v45626 = vand.u32 2147483648, %v45575
%v45627 = vsel /*vm=*/%vm45625, /*on_true_vy=*/%v45626, /*on_false_vx=*/%v45624
%v45630 = vadd.f32 -3.0, %v45627
%v45634 = vsel /*vm=*/%vm45578, /*on_true_vy=*/%v45619, /*on_false_vx=*/%v45630
%v45638 = vmul.f32 %v45634, %v45615
%v45642 = vadd.f32 %v45638, %v45611
%v45646 = vmul.f32 %v45642, %v45634
%v45650 = vadd.f32 %v45646, %v45607
%v45654 = vmul.f32 %v45650, %v45634
%v45658 = vadd.f32 %v45654, %v45603
%v45662 = vmul.f32 %v45658, %v45634
%v45666 = vadd.f32 %v45662, %v45599
%v45670 = vmul.f32 %v45666, %v45634
%v45674 = vadd.f32 %v45670, %v45595
%v45678 = vmul.f32 %v45674, %v45634
%v45682 = vadd.f32 %v45678, %v45591
%v45686 = vmul.f32 %v45682, %v45634
%v45690 = vadd.f32 %v45686, %v45587
%v45694 = vmul.f32 %v45690, %v45634
%v45698 = vadd.f32 %v45694, %v45583
%v45702 = vmul.f32 %v45698, %v45549
%v45706 = vsel /*vm=*/%vm45554, /*on_true_vy=*/%v45559, /*on_false_vx=*/%v45702
%v45710 = vmul.f32 1.4140625, %v45706
%v45713 = vpack.c.bf16 %v120417, %v45710
%119989 = vst [vmem:[%s280 + $0x30] sm:$0xf] /*vst_source=*/%v45713
%v45717 = vadd.s32 %v45253, %v894
%v45727 = vadd.s32 %v45717, %v415
%vm45731 = vcmp.lt.u32.totalorder %v45727, %v45717
%vm45736 = vcmp.lt.u32.totalorder %v45717, %v894
%v45741 = vadd.s32 %v45236, %v881
%v45745 = vadd.s32 1, %v45741
%v45749 = vsel /*vm=*/%vm45736, /*on_true_vy=*/%v45745, /*on_false_vx=*/%v45741
%v45753 = vadd.s32 1, %v45749
%v45757 = vsel /*vm=*/%vm45731, /*on_true_vy=*/%v45753, /*on_false_vx=*/%v45749
%v45762 = vadd.s32 %v45757, %v10
%v45766 = vadd.s32 %v45727, %v9
%v45770 = vadd.s32 %v45766, %v45762
%v45772 = vshll.u32 %v45766, 13
%v45773 = vshrl.u32 %v45766, 19
%v45774 = vor.u32 %v45773, %v45772
%v45775 = vxor.u32 %v45774, %v45770
%v45778 = vadd.s32 %v45775, %v45770
%v45780 = vshll.u32 %v45775, 15
%v45781 = vshrl.u32 %v45775, 17
%v45782 = vor.u32 %v45781, %v45780
%v45783 = vxor.u32 %v45782, %v45778
%v45786 = vadd.s32 %v45783, %v45778
%v45788 = vshll.u32 %v45783, 26
%v45789 = vshrl.u32 %v45783, 6
%v45790 = vor.u32 %v45789, %v45788
%v45791 = vxor.u32 %v45790, %v45786
%v45794 = vadd.s32 %v45791, %v45786
%v45798 = vadd.s32 %v45794, %v9
%v45800 = vshll.u32 %v45791, 6
%v45801 = vshrl.u32 %v45791, 26
%v45802 = vor.u32 %v45801, %v45800
%v45803 = vxor.u32 %v45802, %v45794
%v45806 = vadd.s32 %v45803, %v8
%v45810 = vadd.s32 1, %v45806
%v45814 = vadd.s32 %v45810, %v45798
%v45816 = vshll.u32 %v45810, 17
%v45817 = vshrl.u32 %v45810, 15
%v45818 = vor.u32 %v45817, %v45816
%v45819 = vxor.u32 %v45818, %v45814
%v45822 = vadd.s32 %v45819, %v45814
%v45824 = vshll.u32 %v45819, 29
%v45825 = vshrl.u32 %v45819, 3
%v45826 = vor.u32 %v45825, %v45824
%v45827 = vxor.u32 %v45826, %v45822
%v45830 = vadd.s32 %v45827, %v45822
%v45832 = vshll.u32 %v45827, 16
%v45833 = vshrl.u32 %v45827, 16
%v45834 = vor.u32 %v45833, %v45832
%v45835 = vxor.u32 %v45834, %v45830
%v45838 = vadd.s32 %v45835, %v45830
%v45842 = vadd.s32 %v45838, %v8
%v45844 = vshll.u32 %v45835, 24
%v45845 = vshrl.u32 %v45835, 8
%v45846 = vor.u32 %v45845, %v45844
%v45847 = vxor.u32 %v45846, %v45838
%v45850 = vadd.s32 %v45847, %v10
%v45854 = vadd.s32 2, %v45850
%v45858 = vadd.s32 %v45854, %v45842
%v45860 = vshll.u32 %v45854, 13
%v45861 = vshrl.u32 %v45854, 19
%v45862 = vor.u32 %v45861, %v45860
%v45863 = vxor.u32 %v45862, %v45858
%v45866 = vadd.s32 %v45863, %v45858
%v45868 = vshll.u32 %v45863, 15
%v45869 = vshrl.u32 %v45863, 17
%v45870 = vor.u32 %v45869, %v45868
%v45871 = vxor.u32 %v45870, %v45866
%v45874 = vadd.s32 %v45871, %v45866
%v45876 = vshll.u32 %v45871, 26
%v45877 = vshrl.u32 %v45871, 6
%v45878 = vor.u32 %v45877, %v45876
%v45879 = vxor.u32 %v45878, %v45874
%v45882 = vadd.s32 %v45879, %v45874
%v45886 = vadd.s32 %v45882, %v10
%v45888 = vshll.u32 %v45879, 6
%v45889 = vshrl.u32 %v45879, 26
%v45890 = vor.u32 %v45889, %v45888
%v45891 = vxor.u32 %v45890, %v45882
%v45894 = vadd.s32 %v45891, %v9
%v45898 = vadd.s32 3, %v45894
%v45902 = vadd.s32 %v45898, %v45886
%v45904 = vshll.u32 %v45898, 17
%v45905 = vshrl.u32 %v45898, 15
%v45906 = vor.u32 %v45905, %v45904
%v45907 = vxor.u32 %v45906, %v45902
%v45910 = vadd.s32 %v45907, %v45902
%v45912 = vshll.u32 %v45907, 29
%v45913 = vshrl.u32 %v45907, 3
%v45914 = vor.u32 %v45913, %v45912
%v45915 = vxor.u32 %v45914, %v45910
%v45918 = vadd.s32 %v45915, %v45910
%v45920 = vshll.u32 %v45915, 16
%v45921 = vshrl.u32 %v45915, 16
%v45922 = vor.u32 %v45921, %v45920
%v45923 = vxor.u32 %v45922, %v45918
%v45926 = vadd.s32 %v45923, %v45918
%v45930 = vadd.s32 %v45926, %v9
%v45932 = vshll.u32 %v45923, 24
%v45933 = vshrl.u32 %v45923, 8
%v45934 = vor.u32 %v45933, %v45932
%v45935 = vxor.u32 %v45934, %v45926
%v45938 = vadd.s32 %v45935, %v8
%v45942 = vadd.s32 4, %v45938
%v45946 = vadd.s32 %v45942, %v45930
%v45948 = vshll.u32 %v45942, 13
%v45949 = vshrl.u32 %v45942, 19
%v45950 = vor.u32 %v45949, %v45948
%v45951 = vxor.u32 %v45950, %v45946
%v45954 = vadd.s32 %v45951, %v45946
%v45956 = vshll.u32 %v45951, 15
%v45957 = vshrl.u32 %v45951, 17
%v45958 = vor.u32 %v45957, %v45956
%v45959 = vxor.u32 %v45958, %v45954
%v45962 = vadd.s32 %v45959, %v45954
%v45964 = vshll.u32 %v45959, 26
%v45965 = vshrl.u32 %v45959, 6
%v45966 = vor.u32 %v45965, %v45964
%v45967 = vxor.u32 %v45966, %v45962
%v45970 = vadd.s32 %v45967, %v45962
%v45974 = vadd.s32 %v45970, %v8
%v45976 = vshll.u32 %v45967, 6
%v45977 = vshrl.u32 %v45967, 26
%v45978 = vor.u32 %v45977, %v45976
%v45979 = vxor.u32 %v45978, %v45970
%v45982 = vadd.s32 %v45979, %v10
%v45986 = vadd.s32 5, %v45982
%v45988 = vxor.u32 %v45986, %v45974
%v45989 = vand.u32.u8 255, %v45988
%v45990 = vand.u32 65535, %v45989
%v45991 = vshrl.u32 %v45990, 1
%v45992 = vor.u32 16256, %v45991
%v45993 = vand.u32.u16 65535, %v45992
%v119990 = vadd.low.f32.bf16 -1.0, %v45993
%v46002 = vmul.f32 2.0, %v119990
%v46006 = vadd.f32 -0.99609375, %v46002
%v46010 = vmax.f32 %v46006, -0.99609375
%v46012 = vand.u32 2147483647, %v46010
%vm46015 = vcmp.eq.f32.partialorder %v46012, 1.0
%v46020 = vmul.f32 inf, %v46010
%v46022 = vxor.u32 2147483648, %v46010
%v46025 = vmul.f32 %v46022, %v46010
%v46027 = vadd.f32 1.0, %v46025
%v46028 = vlog2.pop %v46027
%v46029 = vmul.f32 0.6931472, %v46028
%v46030 = vmul.f32 -0.5, %v46025
%v46031 = vadd.f32 1.0, %v46030
%v46032 = vmul.f32 %v46031, %v46025
%v46033 = vand.u32 2147483647, %v46025
%vm46034 = vcmp.lt.f32.partialorder %v46033, 0.0004427343
%v46035 = vsel /*vm=*/%vm46034, /*on_true_vy=*/%v46032, /*on_false_vx=*/%v46029
%v46036 = vxor.u32 2147483648, %v46035
%vm46039 = vcmp.lt.f32.partialorder %v46036, 5.0
%v46044 = vsel /*vm=*/%vm46039, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v46048 = vsel /*vm=*/%vm46039, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v46052 = vsel /*vm=*/%vm46039, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v46056 = vsel /*vm=*/%vm46039, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v46060 = vsel /*vm=*/%vm46039, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v46064 = vsel /*vm=*/%vm46039, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v46068 = vsel /*vm=*/%vm46039, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v46072 = vsel /*vm=*/%vm46039, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v46076 = vsel /*vm=*/%vm46039, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v46080 = vadd.f32 -2.5, %v46036
%v46082 = vrsqrt.pop %v46036
%v46083 = vmul.f32 %v46082, %v46036
%vm46084 = vcmp.eq.f32.partialorder %v46036, inf
%v46085 = vsel /*vm=*/%vm46084, /*on_true_vy=*/%v46036, /*on_false_vx=*/%v46083
%vm46086 = vcmp.eq.f32.partialorder %v46036, 0.0
%v46087 = vand.u32 2147483648, %v46036
%v46088 = vsel /*vm=*/%vm46086, /*on_true_vy=*/%v46087, /*on_false_vx=*/%v46085
%v46091 = vadd.f32 -3.0, %v46088
%v46095 = vsel /*vm=*/%vm46039, /*on_true_vy=*/%v46080, /*on_false_vx=*/%v46091
%v46099 = vmul.f32 %v46095, %v46076
%v46103 = vadd.f32 %v46099, %v46072
%v46107 = vmul.f32 %v46103, %v46095
%v46111 = vadd.f32 %v46107, %v46068
%v46115 = vmul.f32 %v46111, %v46095
%v46119 = vadd.f32 %v46115, %v46064
%v46123 = vmul.f32 %v46119, %v46095
%v46127 = vadd.f32 %v46123, %v46060
%v46131 = vmul.f32 %v46127, %v46095
%v46135 = vadd.f32 %v46131, %v46056
%v46139 = vmul.f32 %v46135, %v46095
%v46143 = vadd.f32 %v46139, %v46052
%v46147 = vmul.f32 %v46143, %v46095
%v46151 = vadd.f32 %v46147, %v46048
%v46155 = vmul.f32 %v46151, %v46095
%v46159 = vadd.f32 %v46155, %v46044
%v46163 = vmul.f32 %v46159, %v46010
%v46167 = vsel /*vm=*/%vm46015, /*on_true_vy=*/%v46020, /*on_false_vx=*/%v46163
%v46171 = vmul.f32 1.4140625, %v46167
%v46174 = vpack.c.bf16 %v120417, %v46171
%119991 = vst [vmem:[%s280 + $0xb0] sm:$0xf] /*vst_source=*/%v46174
%v46178 = vadd.s32 %v45253, %v1381
%v46188 = vadd.s32 %v46178, %v415
%vm46192 = vcmp.lt.u32.totalorder %v46188, %v46178
%vm46197 = vcmp.lt.u32.totalorder %v46178, %v1381
%v46202 = vadd.s32 %v45236, %v1368
%v46206 = vadd.s32 1, %v46202
%v46210 = vsel /*vm=*/%vm46197, /*on_true_vy=*/%v46206, /*on_false_vx=*/%v46202
%v46214 = vadd.s32 1, %v46210
%v46218 = vsel /*vm=*/%vm46192, /*on_true_vy=*/%v46214, /*on_false_vx=*/%v46210
%v46223 = vadd.s32 %v46218, %v10
%v46227 = vadd.s32 %v46188, %v9
%v46231 = vadd.s32 %v46227, %v46223
%v46233 = vshll.u32 %v46227, 13
%v46234 = vshrl.u32 %v46227, 19
%v46235 = vor.u32 %v46234, %v46233
%v46236 = vxor.u32 %v46235, %v46231
%v46239 = vadd.s32 %v46236, %v46231
%v46241 = vshll.u32 %v46236, 15
%v46242 = vshrl.u32 %v46236, 17
%v46243 = vor.u32 %v46242, %v46241
%v46244 = vxor.u32 %v46243, %v46239
%v46247 = vadd.s32 %v46244, %v46239
%v46249 = vshll.u32 %v46244, 26
%v46250 = vshrl.u32 %v46244, 6
%v46251 = vor.u32 %v46250, %v46249
%v46252 = vxor.u32 %v46251, %v46247
%v46255 = vadd.s32 %v46252, %v46247
%v46259 = vadd.s32 %v46255, %v9
%v46261 = vshll.u32 %v46252, 6
%v46262 = vshrl.u32 %v46252, 26
%v46263 = vor.u32 %v46262, %v46261
%v46264 = vxor.u32 %v46263, %v46255
%v46267 = vadd.s32 %v46264, %v8
%v46271 = vadd.s32 1, %v46267
%v46275 = vadd.s32 %v46271, %v46259
%v46277 = vshll.u32 %v46271, 17
%v46278 = vshrl.u32 %v46271, 15
%v46279 = vor.u32 %v46278, %v46277
%v46280 = vxor.u32 %v46279, %v46275
%v46283 = vadd.s32 %v46280, %v46275
%v46285 = vshll.u32 %v46280, 29
%v46286 = vshrl.u32 %v46280, 3
%v46287 = vor.u32 %v46286, %v46285
%v46288 = vxor.u32 %v46287, %v46283
%v46291 = vadd.s32 %v46288, %v46283
%v46293 = vshll.u32 %v46288, 16
%v46294 = vshrl.u32 %v46288, 16
%v46295 = vor.u32 %v46294, %v46293
%v46296 = vxor.u32 %v46295, %v46291
%v46299 = vadd.s32 %v46296, %v46291
%v46303 = vadd.s32 %v46299, %v8
%v46305 = vshll.u32 %v46296, 24
%v46306 = vshrl.u32 %v46296, 8
%v46307 = vor.u32 %v46306, %v46305
%v46308 = vxor.u32 %v46307, %v46299
%v46311 = vadd.s32 %v46308, %v10
%v46315 = vadd.s32 2, %v46311
%v46319 = vadd.s32 %v46315, %v46303
%v46321 = vshll.u32 %v46315, 13
%v46322 = vshrl.u32 %v46315, 19
%v46323 = vor.u32 %v46322, %v46321
%v46324 = vxor.u32 %v46323, %v46319
%v46327 = vadd.s32 %v46324, %v46319
%v46329 = vshll.u32 %v46324, 15
%v46330 = vshrl.u32 %v46324, 17
%v46331 = vor.u32 %v46330, %v46329
%v46332 = vxor.u32 %v46331, %v46327
%v46335 = vadd.s32 %v46332, %v46327
%v46337 = vshll.u32 %v46332, 26
%v46338 = vshrl.u32 %v46332, 6
%v46339 = vor.u32 %v46338, %v46337
%v46340 = vxor.u32 %v46339, %v46335
%v46343 = vadd.s32 %v46340, %v46335
%v46347 = vadd.s32 %v46343, %v10
%v46349 = vshll.u32 %v46340, 6
%v46350 = vshrl.u32 %v46340, 26
%v46351 = vor.u32 %v46350, %v46349
%v46352 = vxor.u32 %v46351, %v46343
%v46355 = vadd.s32 %v46352, %v9
%v46359 = vadd.s32 3, %v46355
%v46363 = vadd.s32 %v46359, %v46347
%v46365 = vshll.u32 %v46359, 17
%v46366 = vshrl.u32 %v46359, 15
%v46367 = vor.u32 %v46366, %v46365
%v46368 = vxor.u32 %v46367, %v46363
%v46371 = vadd.s32 %v46368, %v46363
%v46373 = vshll.u32 %v46368, 29
%v46374 = vshrl.u32 %v46368, 3
%v46375 = vor.u32 %v46374, %v46373
%v46376 = vxor.u32 %v46375, %v46371
%v46379 = vadd.s32 %v46376, %v46371
%v46381 = vshll.u32 %v46376, 16
%v46382 = vshrl.u32 %v46376, 16
%v46383 = vor.u32 %v46382, %v46381
%v46384 = vxor.u32 %v46383, %v46379
%v46387 = vadd.s32 %v46384, %v46379
%v46391 = vadd.s32 %v46387, %v9
%v46393 = vshll.u32 %v46384, 24
%v46394 = vshrl.u32 %v46384, 8
%v46395 = vor.u32 %v46394, %v46393
%v46396 = vxor.u32 %v46395, %v46387
%v46399 = vadd.s32 %v46396, %v8
%v46403 = vadd.s32 4, %v46399
%v46407 = vadd.s32 %v46403, %v46391
%v46409 = vshll.u32 %v46403, 13
%v46410 = vshrl.u32 %v46403, 19
%v46411 = vor.u32 %v46410, %v46409
%v46412 = vxor.u32 %v46411, %v46407
%v46415 = vadd.s32 %v46412, %v46407
%v46417 = vshll.u32 %v46412, 15
%v46418 = vshrl.u32 %v46412, 17
%v46419 = vor.u32 %v46418, %v46417
%v46420 = vxor.u32 %v46419, %v46415
%v46423 = vadd.s32 %v46420, %v46415
%v46425 = vshll.u32 %v46420, 26
%v46426 = vshrl.u32 %v46420, 6
%v46427 = vor.u32 %v46426, %v46425
%v46428 = vxor.u32 %v46427, %v46423
%v46431 = vadd.s32 %v46428, %v46423
%v46435 = vadd.s32 %v46431, %v8
%v46437 = vshll.u32 %v46428, 6
%v46438 = vshrl.u32 %v46428, 26
%v46439 = vor.u32 %v46438, %v46437
%v46440 = vxor.u32 %v46439, %v46431
%v46443 = vadd.s32 %v46440, %v10
%v46447 = vadd.s32 5, %v46443
%v46449 = vxor.u32 %v46447, %v46435
%v46450 = vand.u32.u8 255, %v46449
%v46451 = vand.u32 65535, %v46450
%v46452 = vshrl.u32 %v46451, 1
%v46453 = vor.u32 16256, %v46452
%v46454 = vand.u32.u16 65535, %v46453
%v119992 = vadd.low.f32.bf16 -1.0, %v46454
%v46463 = vmul.f32 2.0, %v119992
%v46467 = vadd.f32 -0.99609375, %v46463
%v46471 = vmax.f32 %v46467, -0.99609375
%v46473 = vand.u32 2147483647, %v46471
%vm46476 = vcmp.eq.f32.partialorder %v46473, 1.0
%v46481 = vmul.f32 inf, %v46471
%v46483 = vxor.u32 2147483648, %v46471
%v46486 = vmul.f32 %v46483, %v46471
%v46488 = vadd.f32 1.0, %v46486
%v46489 = vlog2.pop %v46488
%v46490 = vmul.f32 0.6931472, %v46489
%v46491 = vmul.f32 -0.5, %v46486
%v46492 = vadd.f32 1.0, %v46491
%v46493 = vmul.f32 %v46492, %v46486
%v46494 = vand.u32 2147483647, %v46486
%vm46495 = vcmp.lt.f32.partialorder %v46494, 0.0004427343
%v46496 = vsel /*vm=*/%vm46495, /*on_true_vy=*/%v46493, /*on_false_vx=*/%v46490
%v46497 = vxor.u32 2147483648, %v46496
%vm46500 = vcmp.lt.f32.partialorder %v46497, 5.0
%v46505 = vsel /*vm=*/%vm46500, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v46509 = vsel /*vm=*/%vm46500, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v46513 = vsel /*vm=*/%vm46500, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v46517 = vsel /*vm=*/%vm46500, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v46521 = vsel /*vm=*/%vm46500, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v46525 = vsel /*vm=*/%vm46500, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v46529 = vsel /*vm=*/%vm46500, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v46533 = vsel /*vm=*/%vm46500, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v46537 = vsel /*vm=*/%vm46500, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v46541 = vadd.f32 -2.5, %v46497
%v46543 = vrsqrt.pop %v46497
%v46544 = vmul.f32 %v46543, %v46497
%vm46545 = vcmp.eq.f32.partialorder %v46497, inf
%v46546 = vsel /*vm=*/%vm46545, /*on_true_vy=*/%v46497, /*on_false_vx=*/%v46544
%vm46547 = vcmp.eq.f32.partialorder %v46497, 0.0
%v46548 = vand.u32 2147483648, %v46497
%v46549 = vsel /*vm=*/%vm46547, /*on_true_vy=*/%v46548, /*on_false_vx=*/%v46546
%v46552 = vadd.f32 -3.0, %v46549
%v46556 = vsel /*vm=*/%vm46500, /*on_true_vy=*/%v46541, /*on_false_vx=*/%v46552
%v46560 = vmul.f32 %v46556, %v46537
%v46564 = vadd.f32 %v46560, %v46533
%v46568 = vmul.f32 %v46564, %v46556
%v46572 = vadd.f32 %v46568, %v46529
%v46576 = vmul.f32 %v46572, %v46556
%v46580 = vadd.f32 %v46576, %v46525
%v46584 = vmul.f32 %v46580, %v46556
%v46588 = vadd.f32 %v46584, %v46521
%v46592 = vmul.f32 %v46588, %v46556
%v46596 = vadd.f32 %v46592, %v46517
%v46600 = vmul.f32 %v46596, %v46556
%v46604 = vadd.f32 %v46600, %v46513
%v46608 = vmul.f32 %v46604, %v46556
%v46612 = vadd.f32 %v46608, %v46509
%v46616 = vmul.f32 %v46612, %v46556
%v46620 = vadd.f32 %v46616, %v46505
%v46624 = vmul.f32 %v46620, %v46471
%v46628 = vsel /*vm=*/%vm46476, /*on_true_vy=*/%v46481, /*on_false_vx=*/%v46624
%v46632 = vmul.f32 1.4140625, %v46628
%v46635 = vpack.c.bf16 %v120417, %v46632
%119993 = vst [vmem:[%s280 + $0x130] sm:$0xf] /*vst_source=*/%v46635
%v46639 = vadd.s32 %v45253, %v1868
%v46649 = vadd.s32 %v46639, %v415
%vm46653 = vcmp.lt.u32.totalorder %v46649, %v46639
%vm46658 = vcmp.lt.u32.totalorder %v46639, %v1868
%v46663 = vadd.s32 %v45236, %v1855
%v46667 = vadd.s32 1, %v46663
%v46671 = vsel /*vm=*/%vm46658, /*on_true_vy=*/%v46667, /*on_false_vx=*/%v46663
%v46675 = vadd.s32 1, %v46671
%v46679 = vsel /*vm=*/%vm46653, /*on_true_vy=*/%v46675, /*on_false_vx=*/%v46671
%v46684 = vadd.s32 %v46679, %v10
%v46688 = vadd.s32 %v46649, %v9
%v46692 = vadd.s32 %v46688, %v46684
%v46694 = vshll.u32 %v46688, 13
%v46695 = vshrl.u32 %v46688, 19
%v46696 = vor.u32 %v46695, %v46694
%v46697 = vxor.u32 %v46696, %v46692
%v46700 = vadd.s32 %v46697, %v46692
%v46702 = vshll.u32 %v46697, 15
%v46703 = vshrl.u32 %v46697, 17
%v46704 = vor.u32 %v46703, %v46702
%v46705 = vxor.u32 %v46704, %v46700
%v46708 = vadd.s32 %v46705, %v46700
%v46710 = vshll.u32 %v46705, 26
%v46711 = vshrl.u32 %v46705, 6
%v46712 = vor.u32 %v46711, %v46710
%v46713 = vxor.u32 %v46712, %v46708
%v46716 = vadd.s32 %v46713, %v46708
%v46720 = vadd.s32 %v46716, %v9
%v46722 = vshll.u32 %v46713, 6
%v46723 = vshrl.u32 %v46713, 26
%v46724 = vor.u32 %v46723, %v46722
%v46725 = vxor.u32 %v46724, %v46716
%v46728 = vadd.s32 %v46725, %v8
%v46732 = vadd.s32 1, %v46728
%v46736 = vadd.s32 %v46732, %v46720
%v46738 = vshll.u32 %v46732, 17
%v46739 = vshrl.u32 %v46732, 15
%v46740 = vor.u32 %v46739, %v46738
%v46741 = vxor.u32 %v46740, %v46736
%v46744 = vadd.s32 %v46741, %v46736
%v46746 = vshll.u32 %v46741, 29
%v46747 = vshrl.u32 %v46741, 3
%v46748 = vor.u32 %v46747, %v46746
%v46749 = vxor.u32 %v46748, %v46744
%v46752 = vadd.s32 %v46749, %v46744
%v46754 = vshll.u32 %v46749, 16
%v46755 = vshrl.u32 %v46749, 16
%v46756 = vor.u32 %v46755, %v46754
%v46757 = vxor.u32 %v46756, %v46752
%v46760 = vadd.s32 %v46757, %v46752
%v46764 = vadd.s32 %v46760, %v8
%v46766 = vshll.u32 %v46757, 24
%v46767 = vshrl.u32 %v46757, 8
%v46768 = vor.u32 %v46767, %v46766
%v46769 = vxor.u32 %v46768, %v46760
%v46772 = vadd.s32 %v46769, %v10
%v46776 = vadd.s32 2, %v46772
%v46780 = vadd.s32 %v46776, %v46764
%v46782 = vshll.u32 %v46776, 13
%v46783 = vshrl.u32 %v46776, 19
%v46784 = vor.u32 %v46783, %v46782
%v46785 = vxor.u32 %v46784, %v46780
%v46788 = vadd.s32 %v46785, %v46780
%v46790 = vshll.u32 %v46785, 15
%v46791 = vshrl.u32 %v46785, 17
%v46792 = vor.u32 %v46791, %v46790
%v46793 = vxor.u32 %v46792, %v46788
%v46796 = vadd.s32 %v46793, %v46788
%v46798 = vshll.u32 %v46793, 26
%v46799 = vshrl.u32 %v46793, 6
%v46800 = vor.u32 %v46799, %v46798
%v46801 = vxor.u32 %v46800, %v46796
%v46804 = vadd.s32 %v46801, %v46796
%v46808 = vadd.s32 %v46804, %v10
%v46810 = vshll.u32 %v46801, 6
%v46811 = vshrl.u32 %v46801, 26
%v46812 = vor.u32 %v46811, %v46810
%v46813 = vxor.u32 %v46812, %v46804
%v46816 = vadd.s32 %v46813, %v9
%v46820 = vadd.s32 3, %v46816
%v46824 = vadd.s32 %v46820, %v46808
%v46826 = vshll.u32 %v46820, 17
%v46827 = vshrl.u32 %v46820, 15
%v46828 = vor.u32 %v46827, %v46826
%v46829 = vxor.u32 %v46828, %v46824
%v46832 = vadd.s32 %v46829, %v46824
%v46834 = vshll.u32 %v46829, 29
%v46835 = vshrl.u32 %v46829, 3
%v46836 = vor.u32 %v46835, %v46834
%v46837 = vxor.u32 %v46836, %v46832
%v46840 = vadd.s32 %v46837, %v46832
%v46842 = vshll.u32 %v46837, 16
%v46843 = vshrl.u32 %v46837, 16
%v46844 = vor.u32 %v46843, %v46842
%v46845 = vxor.u32 %v46844, %v46840
%v46848 = vadd.s32 %v46845, %v46840
%v46852 = vadd.s32 %v46848, %v9
%v46854 = vshll.u32 %v46845, 24
%v46855 = vshrl.u32 %v46845, 8
%v46856 = vor.u32 %v46855, %v46854
%v46857 = vxor.u32 %v46856, %v46848
%v46860 = vadd.s32 %v46857, %v8
%v46864 = vadd.s32 4, %v46860
%v46868 = vadd.s32 %v46864, %v46852
%v46870 = vshll.u32 %v46864, 13
%v46871 = vshrl.u32 %v46864, 19
%v46872 = vor.u32 %v46871, %v46870
%v46873 = vxor.u32 %v46872, %v46868
%v46876 = vadd.s32 %v46873, %v46868
%v46878 = vshll.u32 %v46873, 15
%v46879 = vshrl.u32 %v46873, 17
%v46880 = vor.u32 %v46879, %v46878
%v46881 = vxor.u32 %v46880, %v46876
%v46884 = vadd.s32 %v46881, %v46876
%v46886 = vshll.u32 %v46881, 26
%v46887 = vshrl.u32 %v46881, 6
%v46888 = vor.u32 %v46887, %v46886
%v46889 = vxor.u32 %v46888, %v46884
%v46892 = vadd.s32 %v46889, %v46884
%v46896 = vadd.s32 %v46892, %v8
%v46898 = vshll.u32 %v46889, 6
%v46899 = vshrl.u32 %v46889, 26
%v46900 = vor.u32 %v46899, %v46898
%v46901 = vxor.u32 %v46900, %v46892
%v46904 = vadd.s32 %v46901, %v10
%v46908 = vadd.s32 5, %v46904
%v46910 = vxor.u32 %v46908, %v46896
%v46911 = vand.u32.u8 255, %v46910
%v46912 = vand.u32 65535, %v46911
%v46913 = vshrl.u32 %v46912, 1
%v46914 = vor.u32 16256, %v46913
%v46915 = vand.u32.u16 65535, %v46914
%v119994 = vadd.low.f32.bf16 -1.0, %v46915
%v46924 = vmul.f32 2.0, %v119994
%v46928 = vadd.f32 -0.99609375, %v46924
%v46932 = vmax.f32 %v46928, -0.99609375
%v46934 = vand.u32 2147483647, %v46932
%vm46937 = vcmp.eq.f32.partialorder %v46934, 1.0
%v46942 = vmul.f32 inf, %v46932
%v46944 = vxor.u32 2147483648, %v46932
%v46947 = vmul.f32 %v46944, %v46932
%v46949 = vadd.f32 1.0, %v46947
%v46950 = vlog2.pop %v46949
%v46951 = vmul.f32 0.6931472, %v46950
%v46952 = vmul.f32 -0.5, %v46947
%v46953 = vadd.f32 1.0, %v46952
%v46954 = vmul.f32 %v46953, %v46947
%v46955 = vand.u32 2147483647, %v46947
%vm46956 = vcmp.lt.f32.partialorder %v46955, 0.0004427343
%v46957 = vsel /*vm=*/%vm46956, /*on_true_vy=*/%v46954, /*on_false_vx=*/%v46951
%v46958 = vxor.u32 2147483648, %v46957
%vm46961 = vcmp.lt.f32.partialorder %v46958, 5.0
%v46966 = vsel /*vm=*/%vm46961, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v46970 = vsel /*vm=*/%vm46961, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v46974 = vsel /*vm=*/%vm46961, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v46978 = vsel /*vm=*/%vm46961, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v46982 = vsel /*vm=*/%vm46961, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v46986 = vsel /*vm=*/%vm46961, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v46990 = vsel /*vm=*/%vm46961, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v46994 = vsel /*vm=*/%vm46961, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v46998 = vsel /*vm=*/%vm46961, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v47002 = vadd.f32 -2.5, %v46958
%v47004 = vrsqrt.pop %v46958
%v47005 = vmul.f32 %v47004, %v46958
%vm47006 = vcmp.eq.f32.partialorder %v46958, inf
%v47007 = vsel /*vm=*/%vm47006, /*on_true_vy=*/%v46958, /*on_false_vx=*/%v47005
%vm47008 = vcmp.eq.f32.partialorder %v46958, 0.0
%v47009 = vand.u32 2147483648, %v46958
%v47010 = vsel /*vm=*/%vm47008, /*on_true_vy=*/%v47009, /*on_false_vx=*/%v47007
%v47013 = vadd.f32 -3.0, %v47010
%v47017 = vsel /*vm=*/%vm46961, /*on_true_vy=*/%v47002, /*on_false_vx=*/%v47013
%v47021 = vmul.f32 %v47017, %v46998
%v47025 = vadd.f32 %v47021, %v46994
%v47029 = vmul.f32 %v47025, %v47017
%v47033 = vadd.f32 %v47029, %v46990
%v47037 = vmul.f32 %v47033, %v47017
%v47041 = vadd.f32 %v47037, %v46986
%v47045 = vmul.f32 %v47041, %v47017
%v47049 = vadd.f32 %v47045, %v46982
%v47053 = vmul.f32 %v47049, %v47017
%v47057 = vadd.f32 %v47053, %v46978
%v47061 = vmul.f32 %v47057, %v47017
%v47065 = vadd.f32 %v47061, %v46974
%v47069 = vmul.f32 %v47065, %v47017
%v47073 = vadd.f32 %v47069, %v46970
%v47077 = vmul.f32 %v47073, %v47017
%v47081 = vadd.f32 %v47077, %v46966
%v47085 = vmul.f32 %v47081, %v46932
%v47089 = vsel /*vm=*/%vm46937, /*on_true_vy=*/%v46942, /*on_false_vx=*/%v47085
%v47093 = vmul.f32 1.4140625, %v47089
%v47096 = vpack.c.bf16 %v120417, %v47093
%119995 = vst [vmem:[%s280 + $0x1b0] sm:$0xf] /*vst_source=*/%v47096
%v47100 = vadd.s32 %v45253, %v2355
%v47110 = vadd.s32 %v47100, %v415
%vm47114 = vcmp.lt.u32.totalorder %v47110, %v47100
%vm47119 = vcmp.lt.u32.totalorder %v47100, %v2355
%v47124 = vadd.s32 %v45236, %v2342
%v47128 = vadd.s32 1, %v47124
%v47132 = vsel /*vm=*/%vm47119, /*on_true_vy=*/%v47128, /*on_false_vx=*/%v47124
%v47136 = vadd.s32 1, %v47132
%v47140 = vsel /*vm=*/%vm47114, /*on_true_vy=*/%v47136, /*on_false_vx=*/%v47132
%v47145 = vadd.s32 %v47140, %v10
%v47149 = vadd.s32 %v47110, %v9
%v47153 = vadd.s32 %v47149, %v47145
%v47155 = vshll.u32 %v47149, 13
%v47156 = vshrl.u32 %v47149, 19
%v47157 = vor.u32 %v47156, %v47155
%v47158 = vxor.u32 %v47157, %v47153
%v47161 = vadd.s32 %v47158, %v47153
%v47163 = vshll.u32 %v47158, 15
%v47164 = vshrl.u32 %v47158, 17
%v47165 = vor.u32 %v47164, %v47163
%v47166 = vxor.u32 %v47165, %v47161
%v47169 = vadd.s32 %v47166, %v47161
%v47171 = vshll.u32 %v47166, 26
%v47172 = vshrl.u32 %v47166, 6
%v47173 = vor.u32 %v47172, %v47171
%v47174 = vxor.u32 %v47173, %v47169
%v47177 = vadd.s32 %v47174, %v47169
%v47181 = vadd.s32 %v47177, %v9
%v47183 = vshll.u32 %v47174, 6
%v47184 = vshrl.u32 %v47174, 26
%v47185 = vor.u32 %v47184, %v47183
%v47186 = vxor.u32 %v47185, %v47177
%v47189 = vadd.s32 %v47186, %v8
%v47193 = vadd.s32 1, %v47189
%v47197 = vadd.s32 %v47193, %v47181
%v47199 = vshll.u32 %v47193, 17
%v47200 = vshrl.u32 %v47193, 15
%v47201 = vor.u32 %v47200, %v47199
%v47202 = vxor.u32 %v47201, %v47197
%v47205 = vadd.s32 %v47202, %v47197
%v47207 = vshll.u32 %v47202, 29
%v47208 = vshrl.u32 %v47202, 3
%v47209 = vor.u32 %v47208, %v47207
%v47210 = vxor.u32 %v47209, %v47205
%v47213 = vadd.s32 %v47210, %v47205
%v47215 = vshll.u32 %v47210, 16
%v47216 = vshrl.u32 %v47210, 16
%v47217 = vor.u32 %v47216, %v47215
%v47218 = vxor.u32 %v47217, %v47213
%v47221 = vadd.s32 %v47218, %v47213
%v47225 = vadd.s32 %v47221, %v8
%v47227 = vshll.u32 %v47218, 24
%v47228 = vshrl.u32 %v47218, 8
%v47229 = vor.u32 %v47228, %v47227
%v47230 = vxor.u32 %v47229, %v47221
%v47233 = vadd.s32 %v47230, %v10
%v47237 = vadd.s32 2, %v47233
%v47241 = vadd.s32 %v47237, %v47225
%v47243 = vshll.u32 %v47237, 13
%v47244 = vshrl.u32 %v47237, 19
%v47245 = vor.u32 %v47244, %v47243
%v47246 = vxor.u32 %v47245, %v47241
%v47249 = vadd.s32 %v47246, %v47241
%v47251 = vshll.u32 %v47246, 15
%v47252 = vshrl.u32 %v47246, 17
%v47253 = vor.u32 %v47252, %v47251
%v47254 = vxor.u32 %v47253, %v47249
%v47257 = vadd.s32 %v47254, %v47249
%v47259 = vshll.u32 %v47254, 26
%v47260 = vshrl.u32 %v47254, 6
%v47261 = vor.u32 %v47260, %v47259
%v47262 = vxor.u32 %v47261, %v47257
%v47265 = vadd.s32 %v47262, %v47257
%v47269 = vadd.s32 %v47265, %v10
%v47271 = vshll.u32 %v47262, 6
%v47272 = vshrl.u32 %v47262, 26
%v47273 = vor.u32 %v47272, %v47271
%v47274 = vxor.u32 %v47273, %v47265
%v47277 = vadd.s32 %v47274, %v9
%v47281 = vadd.s32 3, %v47277
%v47285 = vadd.s32 %v47281, %v47269
%v47287 = vshll.u32 %v47281, 17
%v47288 = vshrl.u32 %v47281, 15
%v47289 = vor.u32 %v47288, %v47287
%v47290 = vxor.u32 %v47289, %v47285
%v47293 = vadd.s32 %v47290, %v47285
%v47295 = vshll.u32 %v47290, 29
%v47296 = vshrl.u32 %v47290, 3
%v47297 = vor.u32 %v47296, %v47295
%v47298 = vxor.u32 %v47297, %v47293
%v47301 = vadd.s32 %v47298, %v47293
%v47303 = vshll.u32 %v47298, 16
%v47304 = vshrl.u32 %v47298, 16
%v47305 = vor.u32 %v47304, %v47303
%v47306 = vxor.u32 %v47305, %v47301
%v47309 = vadd.s32 %v47306, %v47301
%v47313 = vadd.s32 %v47309, %v9
%v47315 = vshll.u32 %v47306, 24
%v47316 = vshrl.u32 %v47306, 8
%v47317 = vor.u32 %v47316, %v47315
%v47318 = vxor.u32 %v47317, %v47309
%v47321 = vadd.s32 %v47318, %v8
%v47325 = vadd.s32 4, %v47321
%v47329 = vadd.s32 %v47325, %v47313
%v47331 = vshll.u32 %v47325, 13
%v47332 = vshrl.u32 %v47325, 19
%v47333 = vor.u32 %v47332, %v47331
%v47334 = vxor.u32 %v47333, %v47329
%v47337 = vadd.s32 %v47334, %v47329
%v47339 = vshll.u32 %v47334, 15
%v47340 = vshrl.u32 %v47334, 17
%v47341 = vor.u32 %v47340, %v47339
%v47342 = vxor.u32 %v47341, %v47337
%v47345 = vadd.s32 %v47342, %v47337
%v47347 = vshll.u32 %v47342, 26
%v47348 = vshrl.u32 %v47342, 6
%v47349 = vor.u32 %v47348, %v47347
%v47350 = vxor.u32 %v47349, %v47345
%v47353 = vadd.s32 %v47350, %v47345
%v47357 = vadd.s32 %v47353, %v8
%v47359 = vshll.u32 %v47350, 6
%v47360 = vshrl.u32 %v47350, 26
%v47361 = vor.u32 %v47360, %v47359
%v47362 = vxor.u32 %v47361, %v47353
%v47365 = vadd.s32 %v47362, %v10
%v47369 = vadd.s32 5, %v47365
%v47371 = vxor.u32 %v47369, %v47357
%v47372 = vand.u32.u8 255, %v47371
%v47373 = vand.u32 65535, %v47372
%v47374 = vshrl.u32 %v47373, 1
%v47375 = vor.u32 16256, %v47374
%v47376 = vand.u32.u16 65535, %v47375
%v119996 = vadd.low.f32.bf16 -1.0, %v47376
%v47385 = vmul.f32 2.0, %v119996
%v47389 = vadd.f32 -0.99609375, %v47385
%v47393 = vmax.f32 %v47389, -0.99609375
%v47395 = vand.u32 2147483647, %v47393
%vm47398 = vcmp.eq.f32.partialorder %v47395, 1.0
%v47403 = vmul.f32 inf, %v47393
%v47405 = vxor.u32 2147483648, %v47393
%v47408 = vmul.f32 %v47405, %v47393
%v47410 = vadd.f32 1.0, %v47408
%v47411 = vlog2.pop %v47410
%v47412 = vmul.f32 0.6931472, %v47411
%v47413 = vmul.f32 -0.5, %v47408
%v47414 = vadd.f32 1.0, %v47413
%v47415 = vmul.f32 %v47414, %v47408
%v47416 = vand.u32 2147483647, %v47408
%vm47417 = vcmp.lt.f32.partialorder %v47416, 0.0004427343
%v47418 = vsel /*vm=*/%vm47417, /*on_true_vy=*/%v47415, /*on_false_vx=*/%v47412
%v47419 = vxor.u32 2147483648, %v47418
%vm47422 = vcmp.lt.f32.partialorder %v47419, 5.0
%v47427 = vsel /*vm=*/%vm47422, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v47431 = vsel /*vm=*/%vm47422, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v47435 = vsel /*vm=*/%vm47422, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v47439 = vsel /*vm=*/%vm47422, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v47443 = vsel /*vm=*/%vm47422, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v47447 = vsel /*vm=*/%vm47422, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v47451 = vsel /*vm=*/%vm47422, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v47455 = vsel /*vm=*/%vm47422, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v47459 = vsel /*vm=*/%vm47422, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v47463 = vadd.f32 -2.5, %v47419
%v47465 = vrsqrt.pop %v47419
%v47466 = vmul.f32 %v47465, %v47419
%vm47467 = vcmp.eq.f32.partialorder %v47419, inf
%v47468 = vsel /*vm=*/%vm47467, /*on_true_vy=*/%v47419, /*on_false_vx=*/%v47466
%vm47469 = vcmp.eq.f32.partialorder %v47419, 0.0
%v47470 = vand.u32 2147483648, %v47419
%v47471 = vsel /*vm=*/%vm47469, /*on_true_vy=*/%v47470, /*on_false_vx=*/%v47468
%v47474 = vadd.f32 -3.0, %v47471
%v47478 = vsel /*vm=*/%vm47422, /*on_true_vy=*/%v47463, /*on_false_vx=*/%v47474
%v47482 = vmul.f32 %v47478, %v47459
%v47486 = vadd.f32 %v47482, %v47455
%v47490 = vmul.f32 %v47486, %v47478
%v47494 = vadd.f32 %v47490, %v47451
%v47498 = vmul.f32 %v47494, %v47478
%v47502 = vadd.f32 %v47498, %v47447
%v47506 = vmul.f32 %v47502, %v47478
%v47510 = vadd.f32 %v47506, %v47443
%v47514 = vmul.f32 %v47510, %v47478
%v47518 = vadd.f32 %v47514, %v47439
%v47522 = vmul.f32 %v47518, %v47478
%v47526 = vadd.f32 %v47522, %v47435
%v47530 = vmul.f32 %v47526, %v47478
%v47534 = vadd.f32 %v47530, %v47431
%v47538 = vmul.f32 %v47534, %v47478
%v47542 = vadd.f32 %v47538, %v47427
%v47546 = vmul.f32 %v47542, %v47393
%v47550 = vsel /*vm=*/%vm47398, /*on_true_vy=*/%v47403, /*on_false_vx=*/%v47546
%v47554 = vmul.f32 1.4140625, %v47550
%v47557 = vpack.c.bf16 %v120417, %v47554
%119997 = vst [vmem:[%s280 + $0x230] sm:$0xf] /*vst_source=*/%v47557
%v47561 = vadd.s32 %v45253, %v2842
%v47571 = vadd.s32 %v47561, %v415
%vm47575 = vcmp.lt.u32.totalorder %v47571, %v47561
%vm47580 = vcmp.lt.u32.totalorder %v47561, %v2842
%v47585 = vadd.s32 %v45236, %v2829
%v47589 = vadd.s32 1, %v47585
%v47593 = vsel /*vm=*/%vm47580, /*on_true_vy=*/%v47589, /*on_false_vx=*/%v47585
%v47597 = vadd.s32 1, %v47593
%v47601 = vsel /*vm=*/%vm47575, /*on_true_vy=*/%v47597, /*on_false_vx=*/%v47593
%v47606 = vadd.s32 %v47601, %v10
%v47610 = vadd.s32 %v47571, %v9
%v47614 = vadd.s32 %v47610, %v47606
%v47616 = vshll.u32 %v47610, 13
%v47617 = vshrl.u32 %v47610, 19
%v47618 = vor.u32 %v47617, %v47616
%v47619 = vxor.u32 %v47618, %v47614
%v47622 = vadd.s32 %v47619, %v47614
%v47624 = vshll.u32 %v47619, 15
%v47625 = vshrl.u32 %v47619, 17
%v47626 = vor.u32 %v47625, %v47624
%v47627 = vxor.u32 %v47626, %v47622
%v47630 = vadd.s32 %v47627, %v47622
%v47632 = vshll.u32 %v47627, 26
%v47633 = vshrl.u32 %v47627, 6
%v47634 = vor.u32 %v47633, %v47632
%v47635 = vxor.u32 %v47634, %v47630
%v47638 = vadd.s32 %v47635, %v47630
%v47642 = vadd.s32 %v47638, %v9
%v47644 = vshll.u32 %v47635, 6
%v47645 = vshrl.u32 %v47635, 26
%v47646 = vor.u32 %v47645, %v47644
%v47647 = vxor.u32 %v47646, %v47638
%v47650 = vadd.s32 %v47647, %v8
%v47654 = vadd.s32 1, %v47650
%v47658 = vadd.s32 %v47654, %v47642
%v47660 = vshll.u32 %v47654, 17
%v47661 = vshrl.u32 %v47654, 15
%v47662 = vor.u32 %v47661, %v47660
%v47663 = vxor.u32 %v47662, %v47658
%v47666 = vadd.s32 %v47663, %v47658
%v47668 = vshll.u32 %v47663, 29
%v47669 = vshrl.u32 %v47663, 3
%v47670 = vor.u32 %v47669, %v47668
%v47671 = vxor.u32 %v47670, %v47666
%v47674 = vadd.s32 %v47671, %v47666
%v47676 = vshll.u32 %v47671, 16
%v47677 = vshrl.u32 %v47671, 16
%v47678 = vor.u32 %v47677, %v47676
%v47679 = vxor.u32 %v47678, %v47674
%v47682 = vadd.s32 %v47679, %v47674
%v47686 = vadd.s32 %v47682, %v8
%v47688 = vshll.u32 %v47679, 24
%v47689 = vshrl.u32 %v47679, 8
%v47690 = vor.u32 %v47689, %v47688
%v47691 = vxor.u32 %v47690, %v47682
%v47694 = vadd.s32 %v47691, %v10
%v47698 = vadd.s32 2, %v47694
%v47702 = vadd.s32 %v47698, %v47686
%v47704 = vshll.u32 %v47698, 13
%v47705 = vshrl.u32 %v47698, 19
%v47706 = vor.u32 %v47705, %v47704
%v47707 = vxor.u32 %v47706, %v47702
%v47710 = vadd.s32 %v47707, %v47702
%v47712 = vshll.u32 %v47707, 15
%v47713 = vshrl.u32 %v47707, 17
%v47714 = vor.u32 %v47713, %v47712
%v47715 = vxor.u32 %v47714, %v47710
%v47718 = vadd.s32 %v47715, %v47710
%v47720 = vshll.u32 %v47715, 26
%v47721 = vshrl.u32 %v47715, 6
%v47722 = vor.u32 %v47721, %v47720
%v47723 = vxor.u32 %v47722, %v47718
%v47726 = vadd.s32 %v47723, %v47718
%v47730 = vadd.s32 %v47726, %v10
%v47732 = vshll.u32 %v47723, 6
%v47733 = vshrl.u32 %v47723, 26
%v47734 = vor.u32 %v47733, %v47732
%v47735 = vxor.u32 %v47734, %v47726
%v47738 = vadd.s32 %v47735, %v9
%v47742 = vadd.s32 3, %v47738
%v47746 = vadd.s32 %v47742, %v47730
%v47748 = vshll.u32 %v47742, 17
%v47749 = vshrl.u32 %v47742, 15
%v47750 = vor.u32 %v47749, %v47748
%v47751 = vxor.u32 %v47750, %v47746
%v47754 = vadd.s32 %v47751, %v47746
%v47756 = vshll.u32 %v47751, 29
%v47757 = vshrl.u32 %v47751, 3
%v47758 = vor.u32 %v47757, %v47756
%v47759 = vxor.u32 %v47758, %v47754
%v47762 = vadd.s32 %v47759, %v47754
%v47764 = vshll.u32 %v47759, 16
%v47765 = vshrl.u32 %v47759, 16
%v47766 = vor.u32 %v47765, %v47764
%v47767 = vxor.u32 %v47766, %v47762
%v47770 = vadd.s32 %v47767, %v47762
%v47774 = vadd.s32 %v47770, %v9
%v47776 = vshll.u32 %v47767, 24
%v47777 = vshrl.u32 %v47767, 8
%v47778 = vor.u32 %v47777, %v47776
%v47779 = vxor.u32 %v47778, %v47770
%v47782 = vadd.s32 %v47779, %v8
%v47786 = vadd.s32 4, %v47782
%v47790 = vadd.s32 %v47786, %v47774
%v47792 = vshll.u32 %v47786, 13
%v47793 = vshrl.u32 %v47786, 19
%v47794 = vor.u32 %v47793, %v47792
%v47795 = vxor.u32 %v47794, %v47790
%v47798 = vadd.s32 %v47795, %v47790
%v47800 = vshll.u32 %v47795, 15
%v47801 = vshrl.u32 %v47795, 17
%v47802 = vor.u32 %v47801, %v47800
%v47803 = vxor.u32 %v47802, %v47798
%v47806 = vadd.s32 %v47803, %v47798
%v47808 = vshll.u32 %v47803, 26
%v47809 = vshrl.u32 %v47803, 6
%v47810 = vor.u32 %v47809, %v47808
%v47811 = vxor.u32 %v47810, %v47806
%v47814 = vadd.s32 %v47811, %v47806
%v47818 = vadd.s32 %v47814, %v8
%v47820 = vshll.u32 %v47811, 6
%v47821 = vshrl.u32 %v47811, 26
%v47822 = vor.u32 %v47821, %v47820
%v47823 = vxor.u32 %v47822, %v47814
%v47826 = vadd.s32 %v47823, %v10
%v47830 = vadd.s32 5, %v47826
%v47832 = vxor.u32 %v47830, %v47818
%v47833 = vand.u32.u8 255, %v47832
%v47834 = vand.u32 65535, %v47833
%v47835 = vshrl.u32 %v47834, 1
%v47836 = vor.u32 16256, %v47835
%v47837 = vand.u32.u16 65535, %v47836
%v119998 = vadd.low.f32.bf16 -1.0, %v47837
%v47846 = vmul.f32 2.0, %v119998
%v47850 = vadd.f32 -0.99609375, %v47846
%v47854 = vmax.f32 %v47850, -0.99609375
%v47856 = vand.u32 2147483647, %v47854
%vm47859 = vcmp.eq.f32.partialorder %v47856, 1.0
%v47864 = vmul.f32 inf, %v47854
%v47866 = vxor.u32 2147483648, %v47854
%v47869 = vmul.f32 %v47866, %v47854
%v47871 = vadd.f32 1.0, %v47869
%v47872 = vlog2.pop %v47871
%v47873 = vmul.f32 0.6931472, %v47872
%v47874 = vmul.f32 -0.5, %v47869
%v47875 = vadd.f32 1.0, %v47874
%v47876 = vmul.f32 %v47875, %v47869
%v47877 = vand.u32 2147483647, %v47869
%vm47878 = vcmp.lt.f32.partialorder %v47877, 0.0004427343
%v47879 = vsel /*vm=*/%vm47878, /*on_true_vy=*/%v47876, /*on_false_vx=*/%v47873
%v47880 = vxor.u32 2147483648, %v47879
%vm47883 = vcmp.lt.f32.partialorder %v47880, 5.0
%v47888 = vsel /*vm=*/%vm47883, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v47892 = vsel /*vm=*/%vm47883, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v47896 = vsel /*vm=*/%vm47883, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v47900 = vsel /*vm=*/%vm47883, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v47904 = vsel /*vm=*/%vm47883, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v47908 = vsel /*vm=*/%vm47883, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v47912 = vsel /*vm=*/%vm47883, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v47916 = vsel /*vm=*/%vm47883, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v47920 = vsel /*vm=*/%vm47883, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v47924 = vadd.f32 -2.5, %v47880
%v47926 = vrsqrt.pop %v47880
%v47927 = vmul.f32 %v47926, %v47880
%vm47928 = vcmp.eq.f32.partialorder %v47880, inf
%v47929 = vsel /*vm=*/%vm47928, /*on_true_vy=*/%v47880, /*on_false_vx=*/%v47927
%vm47930 = vcmp.eq.f32.partialorder %v47880, 0.0
%v47931 = vand.u32 2147483648, %v47880
%v47932 = vsel /*vm=*/%vm47930, /*on_true_vy=*/%v47931, /*on_false_vx=*/%v47929
%v47935 = vadd.f32 -3.0, %v47932
%v47939 = vsel /*vm=*/%vm47883, /*on_true_vy=*/%v47924, /*on_false_vx=*/%v47935
%v47943 = vmul.f32 %v47939, %v47920
%v47947 = vadd.f32 %v47943, %v47916
%v47951 = vmul.f32 %v47947, %v47939
%v47955 = vadd.f32 %v47951, %v47912
%v47959 = vmul.f32 %v47955, %v47939
%v47963 = vadd.f32 %v47959, %v47908
%v47967 = vmul.f32 %v47963, %v47939
%v47971 = vadd.f32 %v47967, %v47904
%v47975 = vmul.f32 %v47971, %v47939
%v47979 = vadd.f32 %v47975, %v47900
%v47983 = vmul.f32 %v47979, %v47939
%v47987 = vadd.f32 %v47983, %v47896
%v47991 = vmul.f32 %v47987, %v47939
%v47995 = vadd.f32 %v47991, %v47892
%v47999 = vmul.f32 %v47995, %v47939
%v48003 = vadd.f32 %v47999, %v47888
%v48007 = vmul.f32 %v48003, %v47854
%v48011 = vsel /*vm=*/%vm47859, /*on_true_vy=*/%v47864, /*on_false_vx=*/%v48007
%v48015 = vmul.f32 1.4140625, %v48011
%v48018 = vpack.c.bf16 %v120417, %v48015
%119999 = vst [vmem:[%s280 + $0x2b0] sm:$0xf] /*vst_source=*/%v48018
%v48022 = vadd.s32 %v45253, %v3329
%v48032 = vadd.s32 %v48022, %v415
%vm48036 = vcmp.lt.u32.totalorder %v48032, %v48022
%vm48041 = vcmp.lt.u32.totalorder %v48022, %v3329
%v48046 = vadd.s32 %v45236, %v3316
%v48050 = vadd.s32 1, %v48046
%v48054 = vsel /*vm=*/%vm48041, /*on_true_vy=*/%v48050, /*on_false_vx=*/%v48046
%v48058 = vadd.s32 1, %v48054
%v48062 = vsel /*vm=*/%vm48036, /*on_true_vy=*/%v48058, /*on_false_vx=*/%v48054
%v48067 = vadd.s32 %v48062, %v10
%v48071 = vadd.s32 %v48032, %v9
%v48075 = vadd.s32 %v48071, %v48067
%v48077 = vshll.u32 %v48071, 13
%v48078 = vshrl.u32 %v48071, 19
%v48079 = vor.u32 %v48078, %v48077
%v48080 = vxor.u32 %v48079, %v48075
%v48083 = vadd.s32 %v48080, %v48075
%v48085 = vshll.u32 %v48080, 15
%v48086 = vshrl.u32 %v48080, 17
%v48087 = vor.u32 %v48086, %v48085
%v48088 = vxor.u32 %v48087, %v48083
%v48091 = vadd.s32 %v48088, %v48083
%v48093 = vshll.u32 %v48088, 26
%v48094 = vshrl.u32 %v48088, 6
%v48095 = vor.u32 %v48094, %v48093
%v48096 = vxor.u32 %v48095, %v48091
%v48099 = vadd.s32 %v48096, %v48091
%v48103 = vadd.s32 %v48099, %v9
%v48105 = vshll.u32 %v48096, 6
%v48106 = vshrl.u32 %v48096, 26
%v48107 = vor.u32 %v48106, %v48105
%v48108 = vxor.u32 %v48107, %v48099
%v48111 = vadd.s32 %v48108, %v8
%v48115 = vadd.s32 1, %v48111
%v48119 = vadd.s32 %v48115, %v48103
%v48121 = vshll.u32 %v48115, 17
%v48122 = vshrl.u32 %v48115, 15
%v48123 = vor.u32 %v48122, %v48121
%v48124 = vxor.u32 %v48123, %v48119
%v48127 = vadd.s32 %v48124, %v48119
%v48129 = vshll.u32 %v48124, 29
%v48130 = vshrl.u32 %v48124, 3
%v48131 = vor.u32 %v48130, %v48129
%v48132 = vxor.u32 %v48131, %v48127
%v48135 = vadd.s32 %v48132, %v48127
%v48137 = vshll.u32 %v48132, 16
%v48138 = vshrl.u32 %v48132, 16
%v48139 = vor.u32 %v48138, %v48137
%v48140 = vxor.u32 %v48139, %v48135
%v48143 = vadd.s32 %v48140, %v48135
%v48147 = vadd.s32 %v48143, %v8
%v48149 = vshll.u32 %v48140, 24
%v48150 = vshrl.u32 %v48140, 8
%v48151 = vor.u32 %v48150, %v48149
%v48152 = vxor.u32 %v48151, %v48143
%v48155 = vadd.s32 %v48152, %v10
%v48159 = vadd.s32 2, %v48155
%v48163 = vadd.s32 %v48159, %v48147
%v48165 = vshll.u32 %v48159, 13
%v48166 = vshrl.u32 %v48159, 19
%v48167 = vor.u32 %v48166, %v48165
%v48168 = vxor.u32 %v48167, %v48163
%v48171 = vadd.s32 %v48168, %v48163
%v48173 = vshll.u32 %v48168, 15
%v48174 = vshrl.u32 %v48168, 17
%v48175 = vor.u32 %v48174, %v48173
%v48176 = vxor.u32 %v48175, %v48171
%v48179 = vadd.s32 %v48176, %v48171
%v48181 = vshll.u32 %v48176, 26
%v48182 = vshrl.u32 %v48176, 6
%v48183 = vor.u32 %v48182, %v48181
%v48184 = vxor.u32 %v48183, %v48179
%v48187 = vadd.s32 %v48184, %v48179
%v48191 = vadd.s32 %v48187, %v10
%v48193 = vshll.u32 %v48184, 6
%v48194 = vshrl.u32 %v48184, 26
%v48195 = vor.u32 %v48194, %v48193
%v48196 = vxor.u32 %v48195, %v48187
%v48199 = vadd.s32 %v48196, %v9
%v48203 = vadd.s32 3, %v48199
%v48207 = vadd.s32 %v48203, %v48191
%v48209 = vshll.u32 %v48203, 17
%v48210 = vshrl.u32 %v48203, 15
%v48211 = vor.u32 %v48210, %v48209
%v48212 = vxor.u32 %v48211, %v48207
%v48215 = vadd.s32 %v48212, %v48207
%v48217 = vshll.u32 %v48212, 29
%v48218 = vshrl.u32 %v48212, 3
%v48219 = vor.u32 %v48218, %v48217
%v48220 = vxor.u32 %v48219, %v48215
%v48223 = vadd.s32 %v48220, %v48215
%v48225 = vshll.u32 %v48220, 16
%v48226 = vshrl.u32 %v48220, 16
%v48227 = vor.u32 %v48226, %v48225
%v48228 = vxor.u32 %v48227, %v48223
%v48231 = vadd.s32 %v48228, %v48223
%v48235 = vadd.s32 %v48231, %v9
%v48237 = vshll.u32 %v48228, 24
%v48238 = vshrl.u32 %v48228, 8
%v48239 = vor.u32 %v48238, %v48237
%v48240 = vxor.u32 %v48239, %v48231
%v48243 = vadd.s32 %v48240, %v8
%v48247 = vadd.s32 4, %v48243
%v48251 = vadd.s32 %v48247, %v48235
%v48253 = vshll.u32 %v48247, 13
%v48254 = vshrl.u32 %v48247, 19
%v48255 = vor.u32 %v48254, %v48253
%v48256 = vxor.u32 %v48255, %v48251
%v48259 = vadd.s32 %v48256, %v48251
%v48261 = vshll.u32 %v48256, 15
%v48262 = vshrl.u32 %v48256, 17
%v48263 = vor.u32 %v48262, %v48261
%v48264 = vxor.u32 %v48263, %v48259
%v48267 = vadd.s32 %v48264, %v48259
%v48269 = vshll.u32 %v48264, 26
%v48270 = vshrl.u32 %v48264, 6
%v48271 = vor.u32 %v48270, %v48269
%v48272 = vxor.u32 %v48271, %v48267
%v48275 = vadd.s32 %v48272, %v48267
%v48279 = vadd.s32 %v48275, %v8
%v48281 = vshll.u32 %v48272, 6
%v48282 = vshrl.u32 %v48272, 26
%v48283 = vor.u32 %v48282, %v48281
%v48284 = vxor.u32 %v48283, %v48275
%v48287 = vadd.s32 %v48284, %v10
%v48291 = vadd.s32 5, %v48287
%v48293 = vxor.u32 %v48291, %v48279
%v48294 = vand.u32.u8 255, %v48293
%v48295 = vand.u32 65535, %v48294
%v48296 = vshrl.u32 %v48295, 1
%v48297 = vor.u32 16256, %v48296
%v48298 = vand.u32.u16 65535, %v48297
%v120000 = vadd.low.f32.bf16 -1.0, %v48298
%v48307 = vmul.f32 2.0, %v120000
%v48311 = vadd.f32 -0.99609375, %v48307
%v48315 = vmax.f32 %v48311, -0.99609375
%v48317 = vand.u32 2147483647, %v48315
%vm48320 = vcmp.eq.f32.partialorder %v48317, 1.0
%v48325 = vmul.f32 inf, %v48315
%v48327 = vxor.u32 2147483648, %v48315
%v48330 = vmul.f32 %v48327, %v48315
%v48332 = vadd.f32 1.0, %v48330
%v48333 = vlog2.pop %v48332
%v48334 = vmul.f32 0.6931472, %v48333
%v48335 = vmul.f32 -0.5, %v48330
%v48336 = vadd.f32 1.0, %v48335
%v48337 = vmul.f32 %v48336, %v48330
%v48338 = vand.u32 2147483647, %v48330
%vm48339 = vcmp.lt.f32.partialorder %v48338, 0.0004427343
%v48340 = vsel /*vm=*/%vm48339, /*on_true_vy=*/%v48337, /*on_false_vx=*/%v48334
%v48341 = vxor.u32 2147483648, %v48340
%vm48344 = vcmp.lt.f32.partialorder %v48341, 5.0
%v48349 = vsel /*vm=*/%vm48344, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v48353 = vsel /*vm=*/%vm48344, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v48357 = vsel /*vm=*/%vm48344, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v48361 = vsel /*vm=*/%vm48344, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v48365 = vsel /*vm=*/%vm48344, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v48369 = vsel /*vm=*/%vm48344, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v48373 = vsel /*vm=*/%vm48344, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v48377 = vsel /*vm=*/%vm48344, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v48381 = vsel /*vm=*/%vm48344, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v48385 = vadd.f32 -2.5, %v48341
%v48387 = vrsqrt.pop %v48341
%v48388 = vmul.f32 %v48387, %v48341
%vm48389 = vcmp.eq.f32.partialorder %v48341, inf
%v48390 = vsel /*vm=*/%vm48389, /*on_true_vy=*/%v48341, /*on_false_vx=*/%v48388
%vm48391 = vcmp.eq.f32.partialorder %v48341, 0.0
%v48392 = vand.u32 2147483648, %v48341
%v48393 = vsel /*vm=*/%vm48391, /*on_true_vy=*/%v48392, /*on_false_vx=*/%v48390
%v48396 = vadd.f32 -3.0, %v48393
%v48400 = vsel /*vm=*/%vm48344, /*on_true_vy=*/%v48385, /*on_false_vx=*/%v48396
%v48404 = vmul.f32 %v48400, %v48381
%v48408 = vadd.f32 %v48404, %v48377
%v48412 = vmul.f32 %v48408, %v48400
%v48416 = vadd.f32 %v48412, %v48373
%v48420 = vmul.f32 %v48416, %v48400
%v48424 = vadd.f32 %v48420, %v48369
%v48428 = vmul.f32 %v48424, %v48400
%v48432 = vadd.f32 %v48428, %v48365
%v48436 = vmul.f32 %v48432, %v48400
%v48440 = vadd.f32 %v48436, %v48361
%v48444 = vmul.f32 %v48440, %v48400
%v48448 = vadd.f32 %v48444, %v48357
%v48452 = vmul.f32 %v48448, %v48400
%v48456 = vadd.f32 %v48452, %v48353
%v48460 = vmul.f32 %v48456, %v48400
%v48464 = vadd.f32 %v48460, %v48349
%v48468 = vmul.f32 %v48464, %v48315
%v48472 = vsel /*vm=*/%vm48320, /*on_true_vy=*/%v48325, /*on_false_vx=*/%v48468
%v48476 = vmul.f32 1.4140625, %v48472
%v48479 = vpack.c.bf16 %v120417, %v48476
%120001 = vst [vmem:[%s280 + $0x330] sm:$0xf] /*vst_source=*/%v48479
%v48483 = vadd.s32 %v45253, %v3816
%v48493 = vadd.s32 %v48483, %v415
%vm48497 = vcmp.lt.u32.totalorder %v48493, %v48483
%vm48502 = vcmp.lt.u32.totalorder %v48483, %v3816
%v48507 = vadd.s32 %v45236, %v3803
%v48511 = vadd.s32 1, %v48507
%v48515 = vsel /*vm=*/%vm48502, /*on_true_vy=*/%v48511, /*on_false_vx=*/%v48507
%v48519 = vadd.s32 1, %v48515
%v48523 = vsel /*vm=*/%vm48497, /*on_true_vy=*/%v48519, /*on_false_vx=*/%v48515
%v48528 = vadd.s32 %v48523, %v10
%v48532 = vadd.s32 %v48493, %v9
%v48536 = vadd.s32 %v48532, %v48528
%v48538 = vshll.u32 %v48532, 13
%v48539 = vshrl.u32 %v48532, 19
%v48540 = vor.u32 %v48539, %v48538
%v48541 = vxor.u32 %v48540, %v48536
%v48544 = vadd.s32 %v48541, %v48536
%v48546 = vshll.u32 %v48541, 15
%v48547 = vshrl.u32 %v48541, 17
%v48548 = vor.u32 %v48547, %v48546
%v48549 = vxor.u32 %v48548, %v48544
%v48552 = vadd.s32 %v48549, %v48544
%v48554 = vshll.u32 %v48549, 26
%v48555 = vshrl.u32 %v48549, 6
%v48556 = vor.u32 %v48555, %v48554
%v48557 = vxor.u32 %v48556, %v48552
%v48560 = vadd.s32 %v48557, %v48552
%v48564 = vadd.s32 %v48560, %v9
%v48566 = vshll.u32 %v48557, 6
%v48567 = vshrl.u32 %v48557, 26
%v48568 = vor.u32 %v48567, %v48566
%v48569 = vxor.u32 %v48568, %v48560
%v48572 = vadd.s32 %v48569, %v8
%v48576 = vadd.s32 1, %v48572
%v48580 = vadd.s32 %v48576, %v48564
%v48582 = vshll.u32 %v48576, 17
%v48583 = vshrl.u32 %v48576, 15
%v48584 = vor.u32 %v48583, %v48582
%v48585 = vxor.u32 %v48584, %v48580
%v48588 = vadd.s32 %v48585, %v48580
%v48590 = vshll.u32 %v48585, 29
%v48591 = vshrl.u32 %v48585, 3
%v48592 = vor.u32 %v48591, %v48590
%v48593 = vxor.u32 %v48592, %v48588
%v48596 = vadd.s32 %v48593, %v48588
%v48598 = vshll.u32 %v48593, 16
%v48599 = vshrl.u32 %v48593, 16
%v48600 = vor.u32 %v48599, %v48598
%v48601 = vxor.u32 %v48600, %v48596
%v48604 = vadd.s32 %v48601, %v48596
%v48608 = vadd.s32 %v48604, %v8
%v48610 = vshll.u32 %v48601, 24
%v48611 = vshrl.u32 %v48601, 8
%v48612 = vor.u32 %v48611, %v48610
%v48613 = vxor.u32 %v48612, %v48604
%v48616 = vadd.s32 %v48613, %v10
%v48620 = vadd.s32 2, %v48616
%v48624 = vadd.s32 %v48620, %v48608
%v48626 = vshll.u32 %v48620, 13
%v48627 = vshrl.u32 %v48620, 19
%v48628 = vor.u32 %v48627, %v48626
%v48629 = vxor.u32 %v48628, %v48624
%v48632 = vadd.s32 %v48629, %v48624
%v48634 = vshll.u32 %v48629, 15
%v48635 = vshrl.u32 %v48629, 17
%v48636 = vor.u32 %v48635, %v48634
%v48637 = vxor.u32 %v48636, %v48632
%v48640 = vadd.s32 %v48637, %v48632
%v48642 = vshll.u32 %v48637, 26
%v48643 = vshrl.u32 %v48637, 6
%v48644 = vor.u32 %v48643, %v48642
%v48645 = vxor.u32 %v48644, %v48640
%v48648 = vadd.s32 %v48645, %v48640
%v48652 = vadd.s32 %v48648, %v10
%v48654 = vshll.u32 %v48645, 6
%v48655 = vshrl.u32 %v48645, 26
%v48656 = vor.u32 %v48655, %v48654
%v48657 = vxor.u32 %v48656, %v48648
%v48660 = vadd.s32 %v48657, %v9
%v48664 = vadd.s32 3, %v48660
%v48668 = vadd.s32 %v48664, %v48652
%v48670 = vshll.u32 %v48664, 17
%v48671 = vshrl.u32 %v48664, 15
%v48672 = vor.u32 %v48671, %v48670
%v48673 = vxor.u32 %v48672, %v48668
%v48676 = vadd.s32 %v48673, %v48668
%v48678 = vshll.u32 %v48673, 29
%v48679 = vshrl.u32 %v48673, 3
%v48680 = vor.u32 %v48679, %v48678
%v48681 = vxor.u32 %v48680, %v48676
%v48684 = vadd.s32 %v48681, %v48676
%v48686 = vshll.u32 %v48681, 16
%v48687 = vshrl.u32 %v48681, 16
%v48688 = vor.u32 %v48687, %v48686
%v48689 = vxor.u32 %v48688, %v48684
%v48692 = vadd.s32 %v48689, %v48684
%v48696 = vadd.s32 %v48692, %v9
%v48698 = vshll.u32 %v48689, 24
%v48699 = vshrl.u32 %v48689, 8
%v48700 = vor.u32 %v48699, %v48698
%v48701 = vxor.u32 %v48700, %v48692
%v48704 = vadd.s32 %v48701, %v8
%v48708 = vadd.s32 4, %v48704
%v48712 = vadd.s32 %v48708, %v48696
%v48714 = vshll.u32 %v48708, 13
%v48715 = vshrl.u32 %v48708, 19
%v48716 = vor.u32 %v48715, %v48714
%v48717 = vxor.u32 %v48716, %v48712
%v48720 = vadd.s32 %v48717, %v48712
%v48722 = vshll.u32 %v48717, 15
%v48723 = vshrl.u32 %v48717, 17
%v48724 = vor.u32 %v48723, %v48722
%v48725 = vxor.u32 %v48724, %v48720
%v48728 = vadd.s32 %v48725, %v48720
%v48730 = vshll.u32 %v48725, 26
%v48731 = vshrl.u32 %v48725, 6
%v48732 = vor.u32 %v48731, %v48730
%v48733 = vxor.u32 %v48732, %v48728
%v48736 = vadd.s32 %v48733, %v48728
%v48740 = vadd.s32 %v48736, %v8
%v48742 = vshll.u32 %v48733, 6
%v48743 = vshrl.u32 %v48733, 26
%v48744 = vor.u32 %v48743, %v48742
%v48745 = vxor.u32 %v48744, %v48736
%v48748 = vadd.s32 %v48745, %v10
%v48752 = vadd.s32 5, %v48748
%v48754 = vxor.u32 %v48752, %v48740
%v48755 = vand.u32.u8 255, %v48754
%v48756 = vand.u32 65535, %v48755
%v48757 = vshrl.u32 %v48756, 1
%v48758 = vor.u32 16256, %v48757
%v48759 = vand.u32.u16 65535, %v48758
%v120002 = vadd.low.f32.bf16 -1.0, %v48759
%v48768 = vmul.f32 2.0, %v120002
%v48772 = vadd.f32 -0.99609375, %v48768
%v48776 = vmax.f32 %v48772, -0.99609375
%v48778 = vand.u32 2147483647, %v48776
%vm48781 = vcmp.eq.f32.partialorder %v48778, 1.0
%v48786 = vmul.f32 inf, %v48776
%v48788 = vxor.u32 2147483648, %v48776
%v48791 = vmul.f32 %v48788, %v48776
%v48793 = vadd.f32 1.0, %v48791
%v48794 = vlog2.pop %v48793
%v48795 = vmul.f32 0.6931472, %v48794
%v48796 = vmul.f32 -0.5, %v48791
%v48797 = vadd.f32 1.0, %v48796
%v48798 = vmul.f32 %v48797, %v48791
%v48799 = vand.u32 2147483647, %v48791
%vm48800 = vcmp.lt.f32.partialorder %v48799, 0.0004427343
%v48801 = vsel /*vm=*/%vm48800, /*on_true_vy=*/%v48798, /*on_false_vx=*/%v48795
%v48802 = vxor.u32 2147483648, %v48801
%vm48805 = vcmp.lt.f32.partialorder %v48802, 5.0
%v48810 = vsel /*vm=*/%vm48805, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v48814 = vsel /*vm=*/%vm48805, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v48818 = vsel /*vm=*/%vm48805, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v48822 = vsel /*vm=*/%vm48805, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v48826 = vsel /*vm=*/%vm48805, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v48830 = vsel /*vm=*/%vm48805, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v48834 = vsel /*vm=*/%vm48805, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v48838 = vsel /*vm=*/%vm48805, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v48842 = vsel /*vm=*/%vm48805, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v48846 = vadd.f32 -2.5, %v48802
%v48848 = vrsqrt.pop %v48802
%v48849 = vmul.f32 %v48848, %v48802
%vm48850 = vcmp.eq.f32.partialorder %v48802, inf
%v48851 = vsel /*vm=*/%vm48850, /*on_true_vy=*/%v48802, /*on_false_vx=*/%v48849
%vm48852 = vcmp.eq.f32.partialorder %v48802, 0.0
%v48853 = vand.u32 2147483648, %v48802
%v48854 = vsel /*vm=*/%vm48852, /*on_true_vy=*/%v48853, /*on_false_vx=*/%v48851
%v48857 = vadd.f32 -3.0, %v48854
%v48861 = vsel /*vm=*/%vm48805, /*on_true_vy=*/%v48846, /*on_false_vx=*/%v48857
%v48865 = vmul.f32 %v48861, %v48842
%v48869 = vadd.f32 %v48865, %v48838
%v48873 = vmul.f32 %v48869, %v48861
%v48877 = vadd.f32 %v48873, %v48834
%v48881 = vmul.f32 %v48877, %v48861
%v48885 = vadd.f32 %v48881, %v48830
%v48889 = vmul.f32 %v48885, %v48861
%v48893 = vadd.f32 %v48889, %v48826
%v48897 = vmul.f32 %v48893, %v48861
%v48901 = vadd.f32 %v48897, %v48822
%v48905 = vmul.f32 %v48901, %v48861
%v48909 = vadd.f32 %v48905, %v48818
%v48913 = vmul.f32 %v48909, %v48861
%v48917 = vadd.f32 %v48913, %v48814
%v48921 = vmul.f32 %v48917, %v48861
%v48925 = vadd.f32 %v48921, %v48810
%v48929 = vmul.f32 %v48925, %v48776
%v48933 = vsel /*vm=*/%vm48781, /*on_true_vy=*/%v48786, /*on_false_vx=*/%v48929
%v48937 = vmul.f32 1.4140625, %v48933
%v48940 = vpack.c.bf16 %v120417, %v48937
%120003 = vst [vmem:[%s280 + $0x3b0] sm:$0xf] /*vst_source=*/%v48940
%v48978 = vadd.s32 %v48975, %v408
%v48988 = vadd.s32 %v48978, %v415
%vm48992 = vcmp.lt.u32.totalorder %v48988, %v48978
%vm48997 = vcmp.lt.u32.totalorder %v48978, %v408
%v49002 = vadd.s32 %v48958, %v380
%v49006 = vadd.s32 1, %v49002
%v49010 = vsel /*vm=*/%vm48997, /*on_true_vy=*/%v49006, /*on_false_vx=*/%v49002
%v49014 = vadd.s32 1, %v49010
%v49018 = vsel /*vm=*/%vm48992, /*on_true_vy=*/%v49014, /*on_false_vx=*/%v49010
%v49023 = vadd.s32 %v49018, %v10
%v49027 = vadd.s32 %v48988, %v9
%v49031 = vadd.s32 %v49027, %v49023
%v49033 = vshll.u32 %v49027, 13
%v49034 = vshrl.u32 %v49027, 19
%v49035 = vor.u32 %v49034, %v49033
%v49036 = vxor.u32 %v49035, %v49031
%v49039 = vadd.s32 %v49036, %v49031
%v49041 = vshll.u32 %v49036, 15
%v49042 = vshrl.u32 %v49036, 17
%v49043 = vor.u32 %v49042, %v49041
%v49044 = vxor.u32 %v49043, %v49039
%v49047 = vadd.s32 %v49044, %v49039
%v49049 = vshll.u32 %v49044, 26
%v49050 = vshrl.u32 %v49044, 6
%v49051 = vor.u32 %v49050, %v49049
%v49052 = vxor.u32 %v49051, %v49047
%v49055 = vadd.s32 %v49052, %v49047
%v49059 = vadd.s32 %v49055, %v9
%v49061 = vshll.u32 %v49052, 6
%v49062 = vshrl.u32 %v49052, 26
%v49063 = vor.u32 %v49062, %v49061
%v49064 = vxor.u32 %v49063, %v49055
%v49067 = vadd.s32 %v49064, %v8
%v49071 = vadd.s32 1, %v49067
%v49075 = vadd.s32 %v49071, %v49059
%v49077 = vshll.u32 %v49071, 17
%v49078 = vshrl.u32 %v49071, 15
%v49079 = vor.u32 %v49078, %v49077
%v49080 = vxor.u32 %v49079, %v49075
%v49083 = vadd.s32 %v49080, %v49075
%v49085 = vshll.u32 %v49080, 29
%v49086 = vshrl.u32 %v49080, 3
%v49087 = vor.u32 %v49086, %v49085
%v49088 = vxor.u32 %v49087, %v49083
%v49091 = vadd.s32 %v49088, %v49083
%v49093 = vshll.u32 %v49088, 16
%v49094 = vshrl.u32 %v49088, 16
%v49095 = vor.u32 %v49094, %v49093
%v49096 = vxor.u32 %v49095, %v49091
%v49099 = vadd.s32 %v49096, %v49091
%v49103 = vadd.s32 %v49099, %v8
%v49105 = vshll.u32 %v49096, 24
%v49106 = vshrl.u32 %v49096, 8
%v49107 = vor.u32 %v49106, %v49105
%v49108 = vxor.u32 %v49107, %v49099
%v49111 = vadd.s32 %v49108, %v10
%v49115 = vadd.s32 2, %v49111
%v49119 = vadd.s32 %v49115, %v49103
%v49121 = vshll.u32 %v49115, 13
%v49122 = vshrl.u32 %v49115, 19
%v49123 = vor.u32 %v49122, %v49121
%v49124 = vxor.u32 %v49123, %v49119
%v49127 = vadd.s32 %v49124, %v49119
%v49129 = vshll.u32 %v49124, 15
%v49130 = vshrl.u32 %v49124, 17
%v49131 = vor.u32 %v49130, %v49129
%v49132 = vxor.u32 %v49131, %v49127
%v49135 = vadd.s32 %v49132, %v49127
%v49137 = vshll.u32 %v49132, 26
%v49138 = vshrl.u32 %v49132, 6
%v49139 = vor.u32 %v49138, %v49137
%v49140 = vxor.u32 %v49139, %v49135
%v49143 = vadd.s32 %v49140, %v49135
%v49147 = vadd.s32 %v49143, %v10
%v49149 = vshll.u32 %v49140, 6
%v49150 = vshrl.u32 %v49140, 26
%v49151 = vor.u32 %v49150, %v49149
%v49152 = vxor.u32 %v49151, %v49143
%v49155 = vadd.s32 %v49152, %v9
%v49159 = vadd.s32 3, %v49155
%v49163 = vadd.s32 %v49159, %v49147
%v49165 = vshll.u32 %v49159, 17
%v49166 = vshrl.u32 %v49159, 15
%v49167 = vor.u32 %v49166, %v49165
%v49168 = vxor.u32 %v49167, %v49163
%v49171 = vadd.s32 %v49168, %v49163
%v49173 = vshll.u32 %v49168, 29
%v49174 = vshrl.u32 %v49168, 3
%v49175 = vor.u32 %v49174, %v49173
%v49176 = vxor.u32 %v49175, %v49171
%v49179 = vadd.s32 %v49176, %v49171
%v49181 = vshll.u32 %v49176, 16
%v49182 = vshrl.u32 %v49176, 16
%v49183 = vor.u32 %v49182, %v49181
%v49184 = vxor.u32 %v49183, %v49179
%v49187 = vadd.s32 %v49184, %v49179
%v49191 = vadd.s32 %v49187, %v9
%v49193 = vshll.u32 %v49184, 24
%v49194 = vshrl.u32 %v49184, 8
%v49195 = vor.u32 %v49194, %v49193
%v49196 = vxor.u32 %v49195, %v49187
%v49199 = vadd.s32 %v49196, %v8
%v49203 = vadd.s32 4, %v49199
%v49207 = vadd.s32 %v49203, %v49191
%v49209 = vshll.u32 %v49203, 13
%v49210 = vshrl.u32 %v49203, 19
%v49211 = vor.u32 %v49210, %v49209
%v49212 = vxor.u32 %v49211, %v49207
%v49215 = vadd.s32 %v49212, %v49207
%v49217 = vshll.u32 %v49212, 15
%v49218 = vshrl.u32 %v49212, 17
%v49219 = vor.u32 %v49218, %v49217
%v49220 = vxor.u32 %v49219, %v49215
%v49223 = vadd.s32 %v49220, %v49215
%v49225 = vshll.u32 %v49220, 26
%v49226 = vshrl.u32 %v49220, 6
%v49227 = vor.u32 %v49226, %v49225
%v49228 = vxor.u32 %v49227, %v49223
%v49231 = vadd.s32 %v49228, %v49223
%v49235 = vadd.s32 %v49231, %v8
%v49237 = vshll.u32 %v49228, 6
%v49238 = vshrl.u32 %v49228, 26
%v49239 = vor.u32 %v49238, %v49237
%v49240 = vxor.u32 %v49239, %v49231
%v49243 = vadd.s32 %v49240, %v10
%v49247 = vadd.s32 5, %v49243
%v49249 = vxor.u32 %v49247, %v49235
%v49250 = vand.u32.u8 255, %v49249
%v49251 = vand.u32 65535, %v49250
%v49252 = vshrl.u32 %v49251, 1
%v49253 = vor.u32 16256, %v49252
%v49254 = vand.u32.u16 65535, %v49253
%v120008 = vadd.low.f32.bf16 -1.0, %v49254
%v49263 = vmul.f32 2.0, %v120008
%v49267 = vadd.f32 -0.99609375, %v49263
%v49271 = vmax.f32 %v49267, -0.99609375
%v49273 = vand.u32 2147483647, %v49271
%vm49276 = vcmp.eq.f32.partialorder %v49273, 1.0
%v49281 = vmul.f32 inf, %v49271
%v49283 = vxor.u32 2147483648, %v49271
%v49286 = vmul.f32 %v49283, %v49271
%v49288 = vadd.f32 1.0, %v49286
%v49289 = vlog2.pop %v49288
%v49290 = vmul.f32 0.6931472, %v49289
%v49291 = vmul.f32 -0.5, %v49286
%v49292 = vadd.f32 1.0, %v49291
%v49293 = vmul.f32 %v49292, %v49286
%v49294 = vand.u32 2147483647, %v49286
%vm49295 = vcmp.lt.f32.partialorder %v49294, 0.0004427343
%v49296 = vsel /*vm=*/%vm49295, /*on_true_vy=*/%v49293, /*on_false_vx=*/%v49290
%v49297 = vxor.u32 2147483648, %v49296
%vm49300 = vcmp.lt.f32.partialorder %v49297, 5.0
%v49305 = vsel /*vm=*/%vm49300, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v49309 = vsel /*vm=*/%vm49300, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v49313 = vsel /*vm=*/%vm49300, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v49317 = vsel /*vm=*/%vm49300, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v49321 = vsel /*vm=*/%vm49300, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v49325 = vsel /*vm=*/%vm49300, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v49329 = vsel /*vm=*/%vm49300, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v49333 = vsel /*vm=*/%vm49300, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v49337 = vsel /*vm=*/%vm49300, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v49341 = vadd.f32 -2.5, %v49297
%v49343 = vrsqrt.pop %v49297
%v49344 = vmul.f32 %v49343, %v49297
%vm49345 = vcmp.eq.f32.partialorder %v49297, inf
%v49346 = vsel /*vm=*/%vm49345, /*on_true_vy=*/%v49297, /*on_false_vx=*/%v49344
%vm49347 = vcmp.eq.f32.partialorder %v49297, 0.0
%v49348 = vand.u32 2147483648, %v49297
%v49349 = vsel /*vm=*/%vm49347, /*on_true_vy=*/%v49348, /*on_false_vx=*/%v49346
%v49352 = vadd.f32 -3.0, %v49349
%v49356 = vsel /*vm=*/%vm49300, /*on_true_vy=*/%v49341, /*on_false_vx=*/%v49352
%v49360 = vmul.f32 %v49356, %v49337
%v49364 = vadd.f32 %v49360, %v49333
%v49368 = vmul.f32 %v49364, %v49356
%v49372 = vadd.f32 %v49368, %v49329
%v49376 = vmul.f32 %v49372, %v49356
%v49380 = vadd.f32 %v49376, %v49325
%v49384 = vmul.f32 %v49380, %v49356
%v49388 = vadd.f32 %v49384, %v49321
%v49392 = vmul.f32 %v49388, %v49356
%v49396 = vadd.f32 %v49392, %v49317
%v49400 = vmul.f32 %v49396, %v49356
%v49404 = vadd.f32 %v49400, %v49313
%v49408 = vmul.f32 %v49404, %v49356
%v49412 = vadd.f32 %v49408, %v49309
%v49416 = vmul.f32 %v49412, %v49356
%v49420 = vadd.f32 %v49416, %v49305
%v49424 = vmul.f32 %v49420, %v49271
%v49428 = vsel /*vm=*/%vm49276, /*on_true_vy=*/%v49281, /*on_false_vx=*/%v49424
%v49432 = vmul.f32 1.4140625, %v49428
%v49435 = vpack.c.bf16 %v120417, %v49432
%120009 = vst [vmem:[%s280 + $0x34] sm:$0xf] /*vst_source=*/%v49435
%v49439 = vadd.s32 %v48975, %v894
%v49449 = vadd.s32 %v49439, %v415
%vm49453 = vcmp.lt.u32.totalorder %v49449, %v49439
%vm49458 = vcmp.lt.u32.totalorder %v49439, %v894
%v49463 = vadd.s32 %v48958, %v881
%v49467 = vadd.s32 1, %v49463
%v49471 = vsel /*vm=*/%vm49458, /*on_true_vy=*/%v49467, /*on_false_vx=*/%v49463
%v49475 = vadd.s32 1, %v49471
%v49479 = vsel /*vm=*/%vm49453, /*on_true_vy=*/%v49475, /*on_false_vx=*/%v49471
%v49484 = vadd.s32 %v49479, %v10
%v49488 = vadd.s32 %v49449, %v9
%v49492 = vadd.s32 %v49488, %v49484
%v49494 = vshll.u32 %v49488, 13
%v49495 = vshrl.u32 %v49488, 19
%v49496 = vor.u32 %v49495, %v49494
%v49497 = vxor.u32 %v49496, %v49492
%v49500 = vadd.s32 %v49497, %v49492
%v49502 = vshll.u32 %v49497, 15
%v49503 = vshrl.u32 %v49497, 17
%v49504 = vor.u32 %v49503, %v49502
%v49505 = vxor.u32 %v49504, %v49500
%v49508 = vadd.s32 %v49505, %v49500
%v49510 = vshll.u32 %v49505, 26
%v49511 = vshrl.u32 %v49505, 6
%v49512 = vor.u32 %v49511, %v49510
%v49513 = vxor.u32 %v49512, %v49508
%v49516 = vadd.s32 %v49513, %v49508
%v49520 = vadd.s32 %v49516, %v9
%v49522 = vshll.u32 %v49513, 6
%v49523 = vshrl.u32 %v49513, 26
%v49524 = vor.u32 %v49523, %v49522
%v49525 = vxor.u32 %v49524, %v49516
%v49528 = vadd.s32 %v49525, %v8
%v49532 = vadd.s32 1, %v49528
%v49536 = vadd.s32 %v49532, %v49520
%v49538 = vshll.u32 %v49532, 17
%v49539 = vshrl.u32 %v49532, 15
%v49540 = vor.u32 %v49539, %v49538
%v49541 = vxor.u32 %v49540, %v49536
%v49544 = vadd.s32 %v49541, %v49536
%v49546 = vshll.u32 %v49541, 29
%v49547 = vshrl.u32 %v49541, 3
%v49548 = vor.u32 %v49547, %v49546
%v49549 = vxor.u32 %v49548, %v49544
%v49552 = vadd.s32 %v49549, %v49544
%v49554 = vshll.u32 %v49549, 16
%v49555 = vshrl.u32 %v49549, 16
%v49556 = vor.u32 %v49555, %v49554
%v49557 = vxor.u32 %v49556, %v49552
%v49560 = vadd.s32 %v49557, %v49552
%v49564 = vadd.s32 %v49560, %v8
%v49566 = vshll.u32 %v49557, 24
%v49567 = vshrl.u32 %v49557, 8
%v49568 = vor.u32 %v49567, %v49566
%v49569 = vxor.u32 %v49568, %v49560
%v49572 = vadd.s32 %v49569, %v10
%v49576 = vadd.s32 2, %v49572
%v49580 = vadd.s32 %v49576, %v49564
%v49582 = vshll.u32 %v49576, 13
%v49583 = vshrl.u32 %v49576, 19
%v49584 = vor.u32 %v49583, %v49582
%v49585 = vxor.u32 %v49584, %v49580
%v49588 = vadd.s32 %v49585, %v49580
%v49590 = vshll.u32 %v49585, 15
%v49591 = vshrl.u32 %v49585, 17
%v49592 = vor.u32 %v49591, %v49590
%v49593 = vxor.u32 %v49592, %v49588
%v49596 = vadd.s32 %v49593, %v49588
%v49598 = vshll.u32 %v49593, 26
%v49599 = vshrl.u32 %v49593, 6
%v49600 = vor.u32 %v49599, %v49598
%v49601 = vxor.u32 %v49600, %v49596
%v49604 = vadd.s32 %v49601, %v49596
%v49608 = vadd.s32 %v49604, %v10
%v49610 = vshll.u32 %v49601, 6
%v49611 = vshrl.u32 %v49601, 26
%v49612 = vor.u32 %v49611, %v49610
%v49613 = vxor.u32 %v49612, %v49604
%v49616 = vadd.s32 %v49613, %v9
%v49620 = vadd.s32 3, %v49616
%v49624 = vadd.s32 %v49620, %v49608
%v49626 = vshll.u32 %v49620, 17
%v49627 = vshrl.u32 %v49620, 15
%v49628 = vor.u32 %v49627, %v49626
%v49629 = vxor.u32 %v49628, %v49624
%v49632 = vadd.s32 %v49629, %v49624
%v49634 = vshll.u32 %v49629, 29
%v49635 = vshrl.u32 %v49629, 3
%v49636 = vor.u32 %v49635, %v49634
%v49637 = vxor.u32 %v49636, %v49632
%v49640 = vadd.s32 %v49637, %v49632
%v49642 = vshll.u32 %v49637, 16
%v49643 = vshrl.u32 %v49637, 16
%v49644 = vor.u32 %v49643, %v49642
%v49645 = vxor.u32 %v49644, %v49640
%v49648 = vadd.s32 %v49645, %v49640
%v49652 = vadd.s32 %v49648, %v9
%v49654 = vshll.u32 %v49645, 24
%v49655 = vshrl.u32 %v49645, 8
%v49656 = vor.u32 %v49655, %v49654
%v49657 = vxor.u32 %v49656, %v49648
%v49660 = vadd.s32 %v49657, %v8
%v49664 = vadd.s32 4, %v49660
%v49668 = vadd.s32 %v49664, %v49652
%v49670 = vshll.u32 %v49664, 13
%v49671 = vshrl.u32 %v49664, 19
%v49672 = vor.u32 %v49671, %v49670
%v49673 = vxor.u32 %v49672, %v49668
%v49676 = vadd.s32 %v49673, %v49668
%v49678 = vshll.u32 %v49673, 15
%v49679 = vshrl.u32 %v49673, 17
%v49680 = vor.u32 %v49679, %v49678
%v49681 = vxor.u32 %v49680, %v49676
%v49684 = vadd.s32 %v49681, %v49676
%v49686 = vshll.u32 %v49681, 26
%v49687 = vshrl.u32 %v49681, 6
%v49688 = vor.u32 %v49687, %v49686
%v49689 = vxor.u32 %v49688, %v49684
%v49692 = vadd.s32 %v49689, %v49684
%v49696 = vadd.s32 %v49692, %v8
%v49698 = vshll.u32 %v49689, 6
%v49699 = vshrl.u32 %v49689, 26
%v49700 = vor.u32 %v49699, %v49698
%v49701 = vxor.u32 %v49700, %v49692
%v49704 = vadd.s32 %v49701, %v10
%v49708 = vadd.s32 5, %v49704
%v49710 = vxor.u32 %v49708, %v49696
%v49711 = vand.u32.u8 255, %v49710
%v49712 = vand.u32 65535, %v49711
%v49713 = vshrl.u32 %v49712, 1
%v49714 = vor.u32 16256, %v49713
%v49715 = vand.u32.u16 65535, %v49714
%v120010 = vadd.low.f32.bf16 -1.0, %v49715
%v49724 = vmul.f32 2.0, %v120010
%v49728 = vadd.f32 -0.99609375, %v49724
%v49732 = vmax.f32 %v49728, -0.99609375
%v49734 = vand.u32 2147483647, %v49732
%vm49737 = vcmp.eq.f32.partialorder %v49734, 1.0
%v49742 = vmul.f32 inf, %v49732
%v49744 = vxor.u32 2147483648, %v49732
%v49747 = vmul.f32 %v49744, %v49732
%v49749 = vadd.f32 1.0, %v49747
%v49750 = vlog2.pop %v49749
%v49751 = vmul.f32 0.6931472, %v49750
%v49752 = vmul.f32 -0.5, %v49747
%v49753 = vadd.f32 1.0, %v49752
%v49754 = vmul.f32 %v49753, %v49747
%v49755 = vand.u32 2147483647, %v49747
%vm49756 = vcmp.lt.f32.partialorder %v49755, 0.0004427343
%v49757 = vsel /*vm=*/%vm49756, /*on_true_vy=*/%v49754, /*on_false_vx=*/%v49751
%v49758 = vxor.u32 2147483648, %v49757
%vm49761 = vcmp.lt.f32.partialorder %v49758, 5.0
%v49766 = vsel /*vm=*/%vm49761, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v49770 = vsel /*vm=*/%vm49761, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v49774 = vsel /*vm=*/%vm49761, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v49778 = vsel /*vm=*/%vm49761, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v49782 = vsel /*vm=*/%vm49761, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v49786 = vsel /*vm=*/%vm49761, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v49790 = vsel /*vm=*/%vm49761, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v49794 = vsel /*vm=*/%vm49761, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v49798 = vsel /*vm=*/%vm49761, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v49802 = vadd.f32 -2.5, %v49758
%v49804 = vrsqrt.pop %v49758
%v49805 = vmul.f32 %v49804, %v49758
%vm49806 = vcmp.eq.f32.partialorder %v49758, inf
%v49807 = vsel /*vm=*/%vm49806, /*on_true_vy=*/%v49758, /*on_false_vx=*/%v49805
%vm49808 = vcmp.eq.f32.partialorder %v49758, 0.0
%v49809 = vand.u32 2147483648, %v49758
%v49810 = vsel /*vm=*/%vm49808, /*on_true_vy=*/%v49809, /*on_false_vx=*/%v49807
%v49813 = vadd.f32 -3.0, %v49810
%v49817 = vsel /*vm=*/%vm49761, /*on_true_vy=*/%v49802, /*on_false_vx=*/%v49813
%v49821 = vmul.f32 %v49817, %v49798
%v49825 = vadd.f32 %v49821, %v49794
%v49829 = vmul.f32 %v49825, %v49817
%v49833 = vadd.f32 %v49829, %v49790
%v49837 = vmul.f32 %v49833, %v49817
%v49841 = vadd.f32 %v49837, %v49786
%v49845 = vmul.f32 %v49841, %v49817
%v49849 = vadd.f32 %v49845, %v49782
%v49853 = vmul.f32 %v49849, %v49817
%v49857 = vadd.f32 %v49853, %v49778
%v49861 = vmul.f32 %v49857, %v49817
%v49865 = vadd.f32 %v49861, %v49774
%v49869 = vmul.f32 %v49865, %v49817
%v49873 = vadd.f32 %v49869, %v49770
%v49877 = vmul.f32 %v49873, %v49817
%v49881 = vadd.f32 %v49877, %v49766
%v49885 = vmul.f32 %v49881, %v49732
%v49889 = vsel /*vm=*/%vm49737, /*on_true_vy=*/%v49742, /*on_false_vx=*/%v49885
%v49893 = vmul.f32 1.4140625, %v49889
%v49896 = vpack.c.bf16 %v120417, %v49893
%120011 = vst [vmem:[%s280 + $0xb4] sm:$0xf] /*vst_source=*/%v49896
%v49900 = vadd.s32 %v48975, %v1381
%v49910 = vadd.s32 %v49900, %v415
%vm49914 = vcmp.lt.u32.totalorder %v49910, %v49900
%vm49919 = vcmp.lt.u32.totalorder %v49900, %v1381
%v49924 = vadd.s32 %v48958, %v1368
%v49928 = vadd.s32 1, %v49924
%v49932 = vsel /*vm=*/%vm49919, /*on_true_vy=*/%v49928, /*on_false_vx=*/%v49924
%v49936 = vadd.s32 1, %v49932
%v49940 = vsel /*vm=*/%vm49914, /*on_true_vy=*/%v49936, /*on_false_vx=*/%v49932
%v49945 = vadd.s32 %v49940, %v10
%v49949 = vadd.s32 %v49910, %v9
%v49953 = vadd.s32 %v49949, %v49945
%v49955 = vshll.u32 %v49949, 13
%v49956 = vshrl.u32 %v49949, 19
%v49957 = vor.u32 %v49956, %v49955
%v49958 = vxor.u32 %v49957, %v49953
%v49961 = vadd.s32 %v49958, %v49953
%v49963 = vshll.u32 %v49958, 15
%v49964 = vshrl.u32 %v49958, 17
%v49965 = vor.u32 %v49964, %v49963
%v49966 = vxor.u32 %v49965, %v49961
%v49969 = vadd.s32 %v49966, %v49961
%v49971 = vshll.u32 %v49966, 26
%v49972 = vshrl.u32 %v49966, 6
%v49973 = vor.u32 %v49972, %v49971
%v49974 = vxor.u32 %v49973, %v49969
%v49977 = vadd.s32 %v49974, %v49969
%v49981 = vadd.s32 %v49977, %v9
%v49983 = vshll.u32 %v49974, 6
%v49984 = vshrl.u32 %v49974, 26
%v49985 = vor.u32 %v49984, %v49983
%v49986 = vxor.u32 %v49985, %v49977
%v49989 = vadd.s32 %v49986, %v8
%v49993 = vadd.s32 1, %v49989
%v49997 = vadd.s32 %v49993, %v49981
%v49999 = vshll.u32 %v49993, 17
%v50000 = vshrl.u32 %v49993, 15
%v50001 = vor.u32 %v50000, %v49999
%v50002 = vxor.u32 %v50001, %v49997
%v50005 = vadd.s32 %v50002, %v49997
%v50007 = vshll.u32 %v50002, 29
%v50008 = vshrl.u32 %v50002, 3
%v50009 = vor.u32 %v50008, %v50007
%v50010 = vxor.u32 %v50009, %v50005
%v50013 = vadd.s32 %v50010, %v50005
%v50015 = vshll.u32 %v50010, 16
%v50016 = vshrl.u32 %v50010, 16
%v50017 = vor.u32 %v50016, %v50015
%v50018 = vxor.u32 %v50017, %v50013
%v50021 = vadd.s32 %v50018, %v50013
%v50025 = vadd.s32 %v50021, %v8
%v50027 = vshll.u32 %v50018, 24
%v50028 = vshrl.u32 %v50018, 8
%v50029 = vor.u32 %v50028, %v50027
%v50030 = vxor.u32 %v50029, %v50021
%v50033 = vadd.s32 %v50030, %v10
%v50037 = vadd.s32 2, %v50033
%v50041 = vadd.s32 %v50037, %v50025
%v50043 = vshll.u32 %v50037, 13
%v50044 = vshrl.u32 %v50037, 19
%v50045 = vor.u32 %v50044, %v50043
%v50046 = vxor.u32 %v50045, %v50041
%v50049 = vadd.s32 %v50046, %v50041
%v50051 = vshll.u32 %v50046, 15
%v50052 = vshrl.u32 %v50046, 17
%v50053 = vor.u32 %v50052, %v50051
%v50054 = vxor.u32 %v50053, %v50049
%v50057 = vadd.s32 %v50054, %v50049
%v50059 = vshll.u32 %v50054, 26
%v50060 = vshrl.u32 %v50054, 6
%v50061 = vor.u32 %v50060, %v50059
%v50062 = vxor.u32 %v50061, %v50057
%v50065 = vadd.s32 %v50062, %v50057
%v50069 = vadd.s32 %v50065, %v10
%v50071 = vshll.u32 %v50062, 6
%v50072 = vshrl.u32 %v50062, 26
%v50073 = vor.u32 %v50072, %v50071
%v50074 = vxor.u32 %v50073, %v50065
%v50077 = vadd.s32 %v50074, %v9
%v50081 = vadd.s32 3, %v50077
%v50085 = vadd.s32 %v50081, %v50069
%v50087 = vshll.u32 %v50081, 17
%v50088 = vshrl.u32 %v50081, 15
%v50089 = vor.u32 %v50088, %v50087
%v50090 = vxor.u32 %v50089, %v50085
%v50093 = vadd.s32 %v50090, %v50085
%v50095 = vshll.u32 %v50090, 29
%v50096 = vshrl.u32 %v50090, 3
%v50097 = vor.u32 %v50096, %v50095
%v50098 = vxor.u32 %v50097, %v50093
%v50101 = vadd.s32 %v50098, %v50093
%v50103 = vshll.u32 %v50098, 16
%v50104 = vshrl.u32 %v50098, 16
%v50105 = vor.u32 %v50104, %v50103
%v50106 = vxor.u32 %v50105, %v50101
%v50109 = vadd.s32 %v50106, %v50101
%v50113 = vadd.s32 %v50109, %v9
%v50115 = vshll.u32 %v50106, 24
%v50116 = vshrl.u32 %v50106, 8
%v50117 = vor.u32 %v50116, %v50115
%v50118 = vxor.u32 %v50117, %v50109
%v50121 = vadd.s32 %v50118, %v8
%v50125 = vadd.s32 4, %v50121
%v50129 = vadd.s32 %v50125, %v50113
%v50131 = vshll.u32 %v50125, 13
%v50132 = vshrl.u32 %v50125, 19
%v50133 = vor.u32 %v50132, %v50131
%v50134 = vxor.u32 %v50133, %v50129
%v50137 = vadd.s32 %v50134, %v50129
%v50139 = vshll.u32 %v50134, 15
%v50140 = vshrl.u32 %v50134, 17
%v50141 = vor.u32 %v50140, %v50139
%v50142 = vxor.u32 %v50141, %v50137
%v50145 = vadd.s32 %v50142, %v50137
%v50147 = vshll.u32 %v50142, 26
%v50148 = vshrl.u32 %v50142, 6
%v50149 = vor.u32 %v50148, %v50147
%v50150 = vxor.u32 %v50149, %v50145
%v50153 = vadd.s32 %v50150, %v50145
%v50157 = vadd.s32 %v50153, %v8
%v50159 = vshll.u32 %v50150, 6
%v50160 = vshrl.u32 %v50150, 26
%v50161 = vor.u32 %v50160, %v50159
%v50162 = vxor.u32 %v50161, %v50153
%v50165 = vadd.s32 %v50162, %v10
%v50169 = vadd.s32 5, %v50165
%v50171 = vxor.u32 %v50169, %v50157
%v50172 = vand.u32.u8 255, %v50171
%v50173 = vand.u32 65535, %v50172
%v50174 = vshrl.u32 %v50173, 1
%v50175 = vor.u32 16256, %v50174
%v50176 = vand.u32.u16 65535, %v50175
%v120012 = vadd.low.f32.bf16 -1.0, %v50176
%v50185 = vmul.f32 2.0, %v120012
%v50189 = vadd.f32 -0.99609375, %v50185
%v50193 = vmax.f32 %v50189, -0.99609375
%v50195 = vand.u32 2147483647, %v50193
%vm50198 = vcmp.eq.f32.partialorder %v50195, 1.0
%v50203 = vmul.f32 inf, %v50193
%v50205 = vxor.u32 2147483648, %v50193
%v50208 = vmul.f32 %v50205, %v50193
%v50210 = vadd.f32 1.0, %v50208
%v50211 = vlog2.pop %v50210
%v50212 = vmul.f32 0.6931472, %v50211
%v50213 = vmul.f32 -0.5, %v50208
%v50214 = vadd.f32 1.0, %v50213
%v50215 = vmul.f32 %v50214, %v50208
%v50216 = vand.u32 2147483647, %v50208
%vm50217 = vcmp.lt.f32.partialorder %v50216, 0.0004427343
%v50218 = vsel /*vm=*/%vm50217, /*on_true_vy=*/%v50215, /*on_false_vx=*/%v50212
%v50219 = vxor.u32 2147483648, %v50218
%vm50222 = vcmp.lt.f32.partialorder %v50219, 5.0
%v50227 = vsel /*vm=*/%vm50222, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v50231 = vsel /*vm=*/%vm50222, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v50235 = vsel /*vm=*/%vm50222, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v50239 = vsel /*vm=*/%vm50222, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v50243 = vsel /*vm=*/%vm50222, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v50247 = vsel /*vm=*/%vm50222, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v50251 = vsel /*vm=*/%vm50222, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v50255 = vsel /*vm=*/%vm50222, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v50259 = vsel /*vm=*/%vm50222, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v50263 = vadd.f32 -2.5, %v50219
%v50265 = vrsqrt.pop %v50219
%v50266 = vmul.f32 %v50265, %v50219
%vm50267 = vcmp.eq.f32.partialorder %v50219, inf
%v50268 = vsel /*vm=*/%vm50267, /*on_true_vy=*/%v50219, /*on_false_vx=*/%v50266
%vm50269 = vcmp.eq.f32.partialorder %v50219, 0.0
%v50270 = vand.u32 2147483648, %v50219
%v50271 = vsel /*vm=*/%vm50269, /*on_true_vy=*/%v50270, /*on_false_vx=*/%v50268
%v50274 = vadd.f32 -3.0, %v50271
%v50278 = vsel /*vm=*/%vm50222, /*on_true_vy=*/%v50263, /*on_false_vx=*/%v50274
%v50282 = vmul.f32 %v50278, %v50259
%v50286 = vadd.f32 %v50282, %v50255
%v50290 = vmul.f32 %v50286, %v50278
%v50294 = vadd.f32 %v50290, %v50251
%v50298 = vmul.f32 %v50294, %v50278
%v50302 = vadd.f32 %v50298, %v50247
%v50306 = vmul.f32 %v50302, %v50278
%v50310 = vadd.f32 %v50306, %v50243
%v50314 = vmul.f32 %v50310, %v50278
%v50318 = vadd.f32 %v50314, %v50239
%v50322 = vmul.f32 %v50318, %v50278
%v50326 = vadd.f32 %v50322, %v50235
%v50330 = vmul.f32 %v50326, %v50278
%v50334 = vadd.f32 %v50330, %v50231
%v50338 = vmul.f32 %v50334, %v50278
%v50342 = vadd.f32 %v50338, %v50227
%v50346 = vmul.f32 %v50342, %v50193
%v50350 = vsel /*vm=*/%vm50198, /*on_true_vy=*/%v50203, /*on_false_vx=*/%v50346
%v50354 = vmul.f32 1.4140625, %v50350
%v50357 = vpack.c.bf16 %v120417, %v50354
%120013 = vst [vmem:[%s280 + $0x134] sm:$0xf] /*vst_source=*/%v50357
%v50361 = vadd.s32 %v48975, %v1868
%v50371 = vadd.s32 %v50361, %v415
%vm50375 = vcmp.lt.u32.totalorder %v50371, %v50361
%vm50380 = vcmp.lt.u32.totalorder %v50361, %v1868
%v50385 = vadd.s32 %v48958, %v1855
%v50389 = vadd.s32 1, %v50385
%v50393 = vsel /*vm=*/%vm50380, /*on_true_vy=*/%v50389, /*on_false_vx=*/%v50385
%v50397 = vadd.s32 1, %v50393
%v50401 = vsel /*vm=*/%vm50375, /*on_true_vy=*/%v50397, /*on_false_vx=*/%v50393
%v50406 = vadd.s32 %v50401, %v10
%v50410 = vadd.s32 %v50371, %v9
%v50414 = vadd.s32 %v50410, %v50406
%v50416 = vshll.u32 %v50410, 13
%v50417 = vshrl.u32 %v50410, 19
%v50418 = vor.u32 %v50417, %v50416
%v50419 = vxor.u32 %v50418, %v50414
%v50422 = vadd.s32 %v50419, %v50414
%v50424 = vshll.u32 %v50419, 15
%v50425 = vshrl.u32 %v50419, 17
%v50426 = vor.u32 %v50425, %v50424
%v50427 = vxor.u32 %v50426, %v50422
%v50430 = vadd.s32 %v50427, %v50422
%v50432 = vshll.u32 %v50427, 26
%v50433 = vshrl.u32 %v50427, 6
%v50434 = vor.u32 %v50433, %v50432
%v50435 = vxor.u32 %v50434, %v50430
%v50438 = vadd.s32 %v50435, %v50430
%v50442 = vadd.s32 %v50438, %v9
%v50444 = vshll.u32 %v50435, 6
%v50445 = vshrl.u32 %v50435, 26
%v50446 = vor.u32 %v50445, %v50444
%v50447 = vxor.u32 %v50446, %v50438
%v50450 = vadd.s32 %v50447, %v8
%v50454 = vadd.s32 1, %v50450
%v50458 = vadd.s32 %v50454, %v50442
%v50460 = vshll.u32 %v50454, 17
%v50461 = vshrl.u32 %v50454, 15
%v50462 = vor.u32 %v50461, %v50460
%v50463 = vxor.u32 %v50462, %v50458
%v50466 = vadd.s32 %v50463, %v50458
%v50468 = vshll.u32 %v50463, 29
%v50469 = vshrl.u32 %v50463, 3
%v50470 = vor.u32 %v50469, %v50468
%v50471 = vxor.u32 %v50470, %v50466
%v50474 = vadd.s32 %v50471, %v50466
%v50476 = vshll.u32 %v50471, 16
%v50477 = vshrl.u32 %v50471, 16
%v50478 = vor.u32 %v50477, %v50476
%v50479 = vxor.u32 %v50478, %v50474
%v50482 = vadd.s32 %v50479, %v50474
%v50486 = vadd.s32 %v50482, %v8
%v50488 = vshll.u32 %v50479, 24
%v50489 = vshrl.u32 %v50479, 8
%v50490 = vor.u32 %v50489, %v50488
%v50491 = vxor.u32 %v50490, %v50482
%v50494 = vadd.s32 %v50491, %v10
%v50498 = vadd.s32 2, %v50494
%v50502 = vadd.s32 %v50498, %v50486
%v50504 = vshll.u32 %v50498, 13
%v50505 = vshrl.u32 %v50498, 19
%v50506 = vor.u32 %v50505, %v50504
%v50507 = vxor.u32 %v50506, %v50502
%v50510 = vadd.s32 %v50507, %v50502
%v50512 = vshll.u32 %v50507, 15
%v50513 = vshrl.u32 %v50507, 17
%v50514 = vor.u32 %v50513, %v50512
%v50515 = vxor.u32 %v50514, %v50510
%v50518 = vadd.s32 %v50515, %v50510
%v50520 = vshll.u32 %v50515, 26
%v50521 = vshrl.u32 %v50515, 6
%v50522 = vor.u32 %v50521, %v50520
%v50523 = vxor.u32 %v50522, %v50518
%v50526 = vadd.s32 %v50523, %v50518
%v50530 = vadd.s32 %v50526, %v10
%v50532 = vshll.u32 %v50523, 6
%v50533 = vshrl.u32 %v50523, 26
%v50534 = vor.u32 %v50533, %v50532
%v50535 = vxor.u32 %v50534, %v50526
%v50538 = vadd.s32 %v50535, %v9
%v50542 = vadd.s32 3, %v50538
%v50546 = vadd.s32 %v50542, %v50530
%v50548 = vshll.u32 %v50542, 17
%v50549 = vshrl.u32 %v50542, 15
%v50550 = vor.u32 %v50549, %v50548
%v50551 = vxor.u32 %v50550, %v50546
%v50554 = vadd.s32 %v50551, %v50546
%v50556 = vshll.u32 %v50551, 29
%v50557 = vshrl.u32 %v50551, 3
%v50558 = vor.u32 %v50557, %v50556
%v50559 = vxor.u32 %v50558, %v50554
%v50562 = vadd.s32 %v50559, %v50554
%v50564 = vshll.u32 %v50559, 16
%v50565 = vshrl.u32 %v50559, 16
%v50566 = vor.u32 %v50565, %v50564
%v50567 = vxor.u32 %v50566, %v50562
%v50570 = vadd.s32 %v50567, %v50562
%v50574 = vadd.s32 %v50570, %v9
%v50576 = vshll.u32 %v50567, 24
%v50577 = vshrl.u32 %v50567, 8
%v50578 = vor.u32 %v50577, %v50576
%v50579 = vxor.u32 %v50578, %v50570
%v50582 = vadd.s32 %v50579, %v8
%v50586 = vadd.s32 4, %v50582
%v50590 = vadd.s32 %v50586, %v50574
%v50592 = vshll.u32 %v50586, 13
%v50593 = vshrl.u32 %v50586, 19
%v50594 = vor.u32 %v50593, %v50592
%v50595 = vxor.u32 %v50594, %v50590
%v50598 = vadd.s32 %v50595, %v50590
%v50600 = vshll.u32 %v50595, 15
%v50601 = vshrl.u32 %v50595, 17
%v50602 = vor.u32 %v50601, %v50600
%v50603 = vxor.u32 %v50602, %v50598
%v50606 = vadd.s32 %v50603, %v50598
%v50608 = vshll.u32 %v50603, 26
%v50609 = vshrl.u32 %v50603, 6
%v50610 = vor.u32 %v50609, %v50608
%v50611 = vxor.u32 %v50610, %v50606
%v50614 = vadd.s32 %v50611, %v50606
%v50618 = vadd.s32 %v50614, %v8
%v50620 = vshll.u32 %v50611, 6
%v50621 = vshrl.u32 %v50611, 26
%v50622 = vor.u32 %v50621, %v50620
%v50623 = vxor.u32 %v50622, %v50614
%v50626 = vadd.s32 %v50623, %v10
%v50630 = vadd.s32 5, %v50626
%v50632 = vxor.u32 %v50630, %v50618
%v50633 = vand.u32.u8 255, %v50632
%v50634 = vand.u32 65535, %v50633
%v50635 = vshrl.u32 %v50634, 1
%v50636 = vor.u32 16256, %v50635
%v50637 = vand.u32.u16 65535, %v50636
%v120014 = vadd.low.f32.bf16 -1.0, %v50637
%v50646 = vmul.f32 2.0, %v120014
%v50650 = vadd.f32 -0.99609375, %v50646
%v50654 = vmax.f32 %v50650, -0.99609375
%v50656 = vand.u32 2147483647, %v50654
%vm50659 = vcmp.eq.f32.partialorder %v50656, 1.0
%v50664 = vmul.f32 inf, %v50654
%v50666 = vxor.u32 2147483648, %v50654
%v50669 = vmul.f32 %v50666, %v50654
%v50671 = vadd.f32 1.0, %v50669
%v50672 = vlog2.pop %v50671
%v50673 = vmul.f32 0.6931472, %v50672
%v50674 = vmul.f32 -0.5, %v50669
%v50675 = vadd.f32 1.0, %v50674
%v50676 = vmul.f32 %v50675, %v50669
%v50677 = vand.u32 2147483647, %v50669
%vm50678 = vcmp.lt.f32.partialorder %v50677, 0.0004427343
%v50679 = vsel /*vm=*/%vm50678, /*on_true_vy=*/%v50676, /*on_false_vx=*/%v50673
%v50680 = vxor.u32 2147483648, %v50679
%vm50683 = vcmp.lt.f32.partialorder %v50680, 5.0
%v50688 = vsel /*vm=*/%vm50683, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v50692 = vsel /*vm=*/%vm50683, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v50696 = vsel /*vm=*/%vm50683, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v50700 = vsel /*vm=*/%vm50683, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v50704 = vsel /*vm=*/%vm50683, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v50708 = vsel /*vm=*/%vm50683, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v50712 = vsel /*vm=*/%vm50683, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v50716 = vsel /*vm=*/%vm50683, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v50720 = vsel /*vm=*/%vm50683, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v50724 = vadd.f32 -2.5, %v50680
%v50726 = vrsqrt.pop %v50680
%v50727 = vmul.f32 %v50726, %v50680
%vm50728 = vcmp.eq.f32.partialorder %v50680, inf
%v50729 = vsel /*vm=*/%vm50728, /*on_true_vy=*/%v50680, /*on_false_vx=*/%v50727
%vm50730 = vcmp.eq.f32.partialorder %v50680, 0.0
%v50731 = vand.u32 2147483648, %v50680
%v50732 = vsel /*vm=*/%vm50730, /*on_true_vy=*/%v50731, /*on_false_vx=*/%v50729
%v50735 = vadd.f32 -3.0, %v50732
%v50739 = vsel /*vm=*/%vm50683, /*on_true_vy=*/%v50724, /*on_false_vx=*/%v50735
%v50743 = vmul.f32 %v50739, %v50720
%v50747 = vadd.f32 %v50743, %v50716
%v50751 = vmul.f32 %v50747, %v50739
%v50755 = vadd.f32 %v50751, %v50712
%v50759 = vmul.f32 %v50755, %v50739
%v50763 = vadd.f32 %v50759, %v50708
%v50767 = vmul.f32 %v50763, %v50739
%v50771 = vadd.f32 %v50767, %v50704
%v50775 = vmul.f32 %v50771, %v50739
%v50779 = vadd.f32 %v50775, %v50700
%v50783 = vmul.f32 %v50779, %v50739
%v50787 = vadd.f32 %v50783, %v50696
%v50791 = vmul.f32 %v50787, %v50739
%v50795 = vadd.f32 %v50791, %v50692
%v50799 = vmul.f32 %v50795, %v50739
%v50803 = vadd.f32 %v50799, %v50688
%v50807 = vmul.f32 %v50803, %v50654
%v50811 = vsel /*vm=*/%vm50659, /*on_true_vy=*/%v50664, /*on_false_vx=*/%v50807
%v50815 = vmul.f32 1.4140625, %v50811
%v50818 = vpack.c.bf16 %v120417, %v50815
%120015 = vst [vmem:[%s280 + $0x1b4] sm:$0xf] /*vst_source=*/%v50818
%v50822 = vadd.s32 %v48975, %v2355
%v50832 = vadd.s32 %v50822, %v415
%vm50836 = vcmp.lt.u32.totalorder %v50832, %v50822
%vm50841 = vcmp.lt.u32.totalorder %v50822, %v2355
%v50846 = vadd.s32 %v48958, %v2342
%v50850 = vadd.s32 1, %v50846
%v50854 = vsel /*vm=*/%vm50841, /*on_true_vy=*/%v50850, /*on_false_vx=*/%v50846
%v50858 = vadd.s32 1, %v50854
%v50862 = vsel /*vm=*/%vm50836, /*on_true_vy=*/%v50858, /*on_false_vx=*/%v50854
%v50867 = vadd.s32 %v50862, %v10
%v50871 = vadd.s32 %v50832, %v9
%v50875 = vadd.s32 %v50871, %v50867
%v50877 = vshll.u32 %v50871, 13
%v50878 = vshrl.u32 %v50871, 19
%v50879 = vor.u32 %v50878, %v50877
%v50880 = vxor.u32 %v50879, %v50875
%v50883 = vadd.s32 %v50880, %v50875
%v50885 = vshll.u32 %v50880, 15
%v50886 = vshrl.u32 %v50880, 17
%v50887 = vor.u32 %v50886, %v50885
%v50888 = vxor.u32 %v50887, %v50883
%v50891 = vadd.s32 %v50888, %v50883
%v50893 = vshll.u32 %v50888, 26
%v50894 = vshrl.u32 %v50888, 6
%v50895 = vor.u32 %v50894, %v50893
%v50896 = vxor.u32 %v50895, %v50891
%v50899 = vadd.s32 %v50896, %v50891
%v50903 = vadd.s32 %v50899, %v9
%v50905 = vshll.u32 %v50896, 6
%v50906 = vshrl.u32 %v50896, 26
%v50907 = vor.u32 %v50906, %v50905
%v50908 = vxor.u32 %v50907, %v50899
%v50911 = vadd.s32 %v50908, %v8
%v50915 = vadd.s32 1, %v50911
%v50919 = vadd.s32 %v50915, %v50903
%v50921 = vshll.u32 %v50915, 17
%v50922 = vshrl.u32 %v50915, 15
%v50923 = vor.u32 %v50922, %v50921
%v50924 = vxor.u32 %v50923, %v50919
%v50927 = vadd.s32 %v50924, %v50919
%v50929 = vshll.u32 %v50924, 29
%v50930 = vshrl.u32 %v50924, 3
%v50931 = vor.u32 %v50930, %v50929
%v50932 = vxor.u32 %v50931, %v50927
%v50935 = vadd.s32 %v50932, %v50927
%v50937 = vshll.u32 %v50932, 16
%v50938 = vshrl.u32 %v50932, 16
%v50939 = vor.u32 %v50938, %v50937
%v50940 = vxor.u32 %v50939, %v50935
%v50943 = vadd.s32 %v50940, %v50935
%v50947 = vadd.s32 %v50943, %v8
%v50949 = vshll.u32 %v50940, 24
%v50950 = vshrl.u32 %v50940, 8
%v50951 = vor.u32 %v50950, %v50949
%v50952 = vxor.u32 %v50951, %v50943
%v50955 = vadd.s32 %v50952, %v10
%v50959 = vadd.s32 2, %v50955
%v50963 = vadd.s32 %v50959, %v50947
%v50965 = vshll.u32 %v50959, 13
%v50966 = vshrl.u32 %v50959, 19
%v50967 = vor.u32 %v50966, %v50965
%v50968 = vxor.u32 %v50967, %v50963
%v50971 = vadd.s32 %v50968, %v50963
%v50973 = vshll.u32 %v50968, 15
%v50974 = vshrl.u32 %v50968, 17
%v50975 = vor.u32 %v50974, %v50973
%v50976 = vxor.u32 %v50975, %v50971
%v50979 = vadd.s32 %v50976, %v50971
%v50981 = vshll.u32 %v50976, 26
%v50982 = vshrl.u32 %v50976, 6
%v50983 = vor.u32 %v50982, %v50981
%v50984 = vxor.u32 %v50983, %v50979
%v50987 = vadd.s32 %v50984, %v50979
%v50991 = vadd.s32 %v50987, %v10
%v50993 = vshll.u32 %v50984, 6
%v50994 = vshrl.u32 %v50984, 26
%v50995 = vor.u32 %v50994, %v50993
%v50996 = vxor.u32 %v50995, %v50987
%v50999 = vadd.s32 %v50996, %v9
%v51003 = vadd.s32 3, %v50999
%v51007 = vadd.s32 %v51003, %v50991
%v51009 = vshll.u32 %v51003, 17
%v51010 = vshrl.u32 %v51003, 15
%v51011 = vor.u32 %v51010, %v51009
%v51012 = vxor.u32 %v51011, %v51007
%v51015 = vadd.s32 %v51012, %v51007
%v51017 = vshll.u32 %v51012, 29
%v51018 = vshrl.u32 %v51012, 3
%v51019 = vor.u32 %v51018, %v51017
%v51020 = vxor.u32 %v51019, %v51015
%v51023 = vadd.s32 %v51020, %v51015
%v51025 = vshll.u32 %v51020, 16
%v51026 = vshrl.u32 %v51020, 16
%v51027 = vor.u32 %v51026, %v51025
%v51028 = vxor.u32 %v51027, %v51023
%v51031 = vadd.s32 %v51028, %v51023
%v51035 = vadd.s32 %v51031, %v9
%v51037 = vshll.u32 %v51028, 24
%v51038 = vshrl.u32 %v51028, 8
%v51039 = vor.u32 %v51038, %v51037
%v51040 = vxor.u32 %v51039, %v51031
%v51043 = vadd.s32 %v51040, %v8
%v51047 = vadd.s32 4, %v51043
%v51051 = vadd.s32 %v51047, %v51035
%v51053 = vshll.u32 %v51047, 13
%v51054 = vshrl.u32 %v51047, 19
%v51055 = vor.u32 %v51054, %v51053
%v51056 = vxor.u32 %v51055, %v51051
%v51059 = vadd.s32 %v51056, %v51051
%v51061 = vshll.u32 %v51056, 15
%v51062 = vshrl.u32 %v51056, 17
%v51063 = vor.u32 %v51062, %v51061
%v51064 = vxor.u32 %v51063, %v51059
%v51067 = vadd.s32 %v51064, %v51059
%v51069 = vshll.u32 %v51064, 26
%v51070 = vshrl.u32 %v51064, 6
%v51071 = vor.u32 %v51070, %v51069
%v51072 = vxor.u32 %v51071, %v51067
%v51075 = vadd.s32 %v51072, %v51067
%v51079 = vadd.s32 %v51075, %v8
%v51081 = vshll.u32 %v51072, 6
%v51082 = vshrl.u32 %v51072, 26
%v51083 = vor.u32 %v51082, %v51081
%v51084 = vxor.u32 %v51083, %v51075
%v51087 = vadd.s32 %v51084, %v10
%v51091 = vadd.s32 5, %v51087
%v51093 = vxor.u32 %v51091, %v51079
%v51094 = vand.u32.u8 255, %v51093
%v51095 = vand.u32 65535, %v51094
%v51096 = vshrl.u32 %v51095, 1
%v51097 = vor.u32 16256, %v51096
%v51098 = vand.u32.u16 65535, %v51097
%v120016 = vadd.low.f32.bf16 -1.0, %v51098
%v51107 = vmul.f32 2.0, %v120016
%v51111 = vadd.f32 -0.99609375, %v51107
%v51115 = vmax.f32 %v51111, -0.99609375
%v51117 = vand.u32 2147483647, %v51115
%vm51120 = vcmp.eq.f32.partialorder %v51117, 1.0
%v51125 = vmul.f32 inf, %v51115
%v51127 = vxor.u32 2147483648, %v51115
%v51130 = vmul.f32 %v51127, %v51115
%v51132 = vadd.f32 1.0, %v51130
%v51133 = vlog2.pop %v51132
%v51134 = vmul.f32 0.6931472, %v51133
%v51135 = vmul.f32 -0.5, %v51130
%v51136 = vadd.f32 1.0, %v51135
%v51137 = vmul.f32 %v51136, %v51130
%v51138 = vand.u32 2147483647, %v51130
%vm51139 = vcmp.lt.f32.partialorder %v51138, 0.0004427343
%v51140 = vsel /*vm=*/%vm51139, /*on_true_vy=*/%v51137, /*on_false_vx=*/%v51134
%v51141 = vxor.u32 2147483648, %v51140
%vm51144 = vcmp.lt.f32.partialorder %v51141, 5.0
%v51149 = vsel /*vm=*/%vm51144, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v51153 = vsel /*vm=*/%vm51144, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v51157 = vsel /*vm=*/%vm51144, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v51161 = vsel /*vm=*/%vm51144, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v51165 = vsel /*vm=*/%vm51144, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v51169 = vsel /*vm=*/%vm51144, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v51173 = vsel /*vm=*/%vm51144, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v51177 = vsel /*vm=*/%vm51144, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v51181 = vsel /*vm=*/%vm51144, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v51185 = vadd.f32 -2.5, %v51141
%v51187 = vrsqrt.pop %v51141
%v51188 = vmul.f32 %v51187, %v51141
%vm51189 = vcmp.eq.f32.partialorder %v51141, inf
%v51190 = vsel /*vm=*/%vm51189, /*on_true_vy=*/%v51141, /*on_false_vx=*/%v51188
%vm51191 = vcmp.eq.f32.partialorder %v51141, 0.0
%v51192 = vand.u32 2147483648, %v51141
%v51193 = vsel /*vm=*/%vm51191, /*on_true_vy=*/%v51192, /*on_false_vx=*/%v51190
%v51196 = vadd.f32 -3.0, %v51193
%v51200 = vsel /*vm=*/%vm51144, /*on_true_vy=*/%v51185, /*on_false_vx=*/%v51196
%v51204 = vmul.f32 %v51200, %v51181
%v51208 = vadd.f32 %v51204, %v51177
%v51212 = vmul.f32 %v51208, %v51200
%v51216 = vadd.f32 %v51212, %v51173
%v51220 = vmul.f32 %v51216, %v51200
%v51224 = vadd.f32 %v51220, %v51169
%v51228 = vmul.f32 %v51224, %v51200
%v51232 = vadd.f32 %v51228, %v51165
%v51236 = vmul.f32 %v51232, %v51200
%v51240 = vadd.f32 %v51236, %v51161
%v51244 = vmul.f32 %v51240, %v51200
%v51248 = vadd.f32 %v51244, %v51157
%v51252 = vmul.f32 %v51248, %v51200
%v51256 = vadd.f32 %v51252, %v51153
%v51260 = vmul.f32 %v51256, %v51200
%v51264 = vadd.f32 %v51260, %v51149
%v51268 = vmul.f32 %v51264, %v51115
%v51272 = vsel /*vm=*/%vm51120, /*on_true_vy=*/%v51125, /*on_false_vx=*/%v51268
%v51276 = vmul.f32 1.4140625, %v51272
%v51279 = vpack.c.bf16 %v120417, %v51276
%120017 = vst [vmem:[%s280 + $0x234] sm:$0xf] /*vst_source=*/%v51279
%v51283 = vadd.s32 %v48975, %v2842
%v51293 = vadd.s32 %v51283, %v415
%vm51297 = vcmp.lt.u32.totalorder %v51293, %v51283
%vm51302 = vcmp.lt.u32.totalorder %v51283, %v2842
%v51307 = vadd.s32 %v48958, %v2829
%v51311 = vadd.s32 1, %v51307
%v51315 = vsel /*vm=*/%vm51302, /*on_true_vy=*/%v51311, /*on_false_vx=*/%v51307
%v51319 = vadd.s32 1, %v51315
%v51323 = vsel /*vm=*/%vm51297, /*on_true_vy=*/%v51319, /*on_false_vx=*/%v51315
%v51328 = vadd.s32 %v51323, %v10
%v51332 = vadd.s32 %v51293, %v9
%v51336 = vadd.s32 %v51332, %v51328
%v51338 = vshll.u32 %v51332, 13
%v51339 = vshrl.u32 %v51332, 19
%v51340 = vor.u32 %v51339, %v51338
%v51341 = vxor.u32 %v51340, %v51336
%v51344 = vadd.s32 %v51341, %v51336
%v51346 = vshll.u32 %v51341, 15
%v51347 = vshrl.u32 %v51341, 17
%v51348 = vor.u32 %v51347, %v51346
%v51349 = vxor.u32 %v51348, %v51344
%v51352 = vadd.s32 %v51349, %v51344
%v51354 = vshll.u32 %v51349, 26
%v51355 = vshrl.u32 %v51349, 6
%v51356 = vor.u32 %v51355, %v51354
%v51357 = vxor.u32 %v51356, %v51352
%v51360 = vadd.s32 %v51357, %v51352
%v51364 = vadd.s32 %v51360, %v9
%v51366 = vshll.u32 %v51357, 6
%v51367 = vshrl.u32 %v51357, 26
%v51368 = vor.u32 %v51367, %v51366
%v51369 = vxor.u32 %v51368, %v51360
%v51372 = vadd.s32 %v51369, %v8
%v51376 = vadd.s32 1, %v51372
%v51380 = vadd.s32 %v51376, %v51364
%v51382 = vshll.u32 %v51376, 17
%v51383 = vshrl.u32 %v51376, 15
%v51384 = vor.u32 %v51383, %v51382
%v51385 = vxor.u32 %v51384, %v51380
%v51388 = vadd.s32 %v51385, %v51380
%v51390 = vshll.u32 %v51385, 29
%v51391 = vshrl.u32 %v51385, 3
%v51392 = vor.u32 %v51391, %v51390
%v51393 = vxor.u32 %v51392, %v51388
%v51396 = vadd.s32 %v51393, %v51388
%v51398 = vshll.u32 %v51393, 16
%v51399 = vshrl.u32 %v51393, 16
%v51400 = vor.u32 %v51399, %v51398
%v51401 = vxor.u32 %v51400, %v51396
%v51404 = vadd.s32 %v51401, %v51396
%v51408 = vadd.s32 %v51404, %v8
%v51410 = vshll.u32 %v51401, 24
%v51411 = vshrl.u32 %v51401, 8
%v51412 = vor.u32 %v51411, %v51410
%v51413 = vxor.u32 %v51412, %v51404
%v51416 = vadd.s32 %v51413, %v10
%v51420 = vadd.s32 2, %v51416
%v51424 = vadd.s32 %v51420, %v51408
%v51426 = vshll.u32 %v51420, 13
%v51427 = vshrl.u32 %v51420, 19
%v51428 = vor.u32 %v51427, %v51426
%v51429 = vxor.u32 %v51428, %v51424
%v51432 = vadd.s32 %v51429, %v51424
%v51434 = vshll.u32 %v51429, 15
%v51435 = vshrl.u32 %v51429, 17
%v51436 = vor.u32 %v51435, %v51434
%v51437 = vxor.u32 %v51436, %v51432
%v51440 = vadd.s32 %v51437, %v51432
%v51442 = vshll.u32 %v51437, 26
%v51443 = vshrl.u32 %v51437, 6
%v51444 = vor.u32 %v51443, %v51442
%v51445 = vxor.u32 %v51444, %v51440
%v51448 = vadd.s32 %v51445, %v51440
%v51452 = vadd.s32 %v51448, %v10
%v51454 = vshll.u32 %v51445, 6
%v51455 = vshrl.u32 %v51445, 26
%v51456 = vor.u32 %v51455, %v51454
%v51457 = vxor.u32 %v51456, %v51448
%v51460 = vadd.s32 %v51457, %v9
%v51464 = vadd.s32 3, %v51460
%v51468 = vadd.s32 %v51464, %v51452
%v51470 = vshll.u32 %v51464, 17
%v51471 = vshrl.u32 %v51464, 15
%v51472 = vor.u32 %v51471, %v51470
%v51473 = vxor.u32 %v51472, %v51468
%v51476 = vadd.s32 %v51473, %v51468
%v51478 = vshll.u32 %v51473, 29
%v51479 = vshrl.u32 %v51473, 3
%v51480 = vor.u32 %v51479, %v51478
%v51481 = vxor.u32 %v51480, %v51476
%v51484 = vadd.s32 %v51481, %v51476
%v51486 = vshll.u32 %v51481, 16
%v51487 = vshrl.u32 %v51481, 16
%v51488 = vor.u32 %v51487, %v51486
%v51489 = vxor.u32 %v51488, %v51484
%v51492 = vadd.s32 %v51489, %v51484
%v51496 = vadd.s32 %v51492, %v9
%v51498 = vshll.u32 %v51489, 24
%v51499 = vshrl.u32 %v51489, 8
%v51500 = vor.u32 %v51499, %v51498
%v51501 = vxor.u32 %v51500, %v51492
%v51504 = vadd.s32 %v51501, %v8
%v51508 = vadd.s32 4, %v51504
%v51512 = vadd.s32 %v51508, %v51496
%v51514 = vshll.u32 %v51508, 13
%v51515 = vshrl.u32 %v51508, 19
%v51516 = vor.u32 %v51515, %v51514
%v51517 = vxor.u32 %v51516, %v51512
%v51520 = vadd.s32 %v51517, %v51512
%v51522 = vshll.u32 %v51517, 15
%v51523 = vshrl.u32 %v51517, 17
%v51524 = vor.u32 %v51523, %v51522
%v51525 = vxor.u32 %v51524, %v51520
%v51528 = vadd.s32 %v51525, %v51520
%v51530 = vshll.u32 %v51525, 26
%v51531 = vshrl.u32 %v51525, 6
%v51532 = vor.u32 %v51531, %v51530
%v51533 = vxor.u32 %v51532, %v51528
%v51536 = vadd.s32 %v51533, %v51528
%v51540 = vadd.s32 %v51536, %v8
%v51542 = vshll.u32 %v51533, 6
%v51543 = vshrl.u32 %v51533, 26
%v51544 = vor.u32 %v51543, %v51542
%v51545 = vxor.u32 %v51544, %v51536
%v51548 = vadd.s32 %v51545, %v10
%v51552 = vadd.s32 5, %v51548
%v51554 = vxor.u32 %v51552, %v51540
%v51555 = vand.u32.u8 255, %v51554
%v51556 = vand.u32 65535, %v51555
%v51557 = vshrl.u32 %v51556, 1
%v51558 = vor.u32 16256, %v51557
%v51559 = vand.u32.u16 65535, %v51558
%v120018 = vadd.low.f32.bf16 -1.0, %v51559
%v51568 = vmul.f32 2.0, %v120018
%v51572 = vadd.f32 -0.99609375, %v51568
%v51576 = vmax.f32 %v51572, -0.99609375
%v51578 = vand.u32 2147483647, %v51576
%vm51581 = vcmp.eq.f32.partialorder %v51578, 1.0
%v51586 = vmul.f32 inf, %v51576
%v51588 = vxor.u32 2147483648, %v51576
%v51591 = vmul.f32 %v51588, %v51576
%v51593 = vadd.f32 1.0, %v51591
%v51594 = vlog2.pop %v51593
%v51595 = vmul.f32 0.6931472, %v51594
%v51596 = vmul.f32 -0.5, %v51591
%v51597 = vadd.f32 1.0, %v51596
%v51598 = vmul.f32 %v51597, %v51591
%v51599 = vand.u32 2147483647, %v51591
%vm51600 = vcmp.lt.f32.partialorder %v51599, 0.0004427343
%v51601 = vsel /*vm=*/%vm51600, /*on_true_vy=*/%v51598, /*on_false_vx=*/%v51595
%v51602 = vxor.u32 2147483648, %v51601
%vm51605 = vcmp.lt.f32.partialorder %v51602, 5.0
%v51610 = vsel /*vm=*/%vm51605, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v51614 = vsel /*vm=*/%vm51605, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v51618 = vsel /*vm=*/%vm51605, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v51622 = vsel /*vm=*/%vm51605, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v51626 = vsel /*vm=*/%vm51605, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v51630 = vsel /*vm=*/%vm51605, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v51634 = vsel /*vm=*/%vm51605, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v51638 = vsel /*vm=*/%vm51605, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v51642 = vsel /*vm=*/%vm51605, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v51646 = vadd.f32 -2.5, %v51602
%v51648 = vrsqrt.pop %v51602
%v51649 = vmul.f32 %v51648, %v51602
%vm51650 = vcmp.eq.f32.partialorder %v51602, inf
%v51651 = vsel /*vm=*/%vm51650, /*on_true_vy=*/%v51602, /*on_false_vx=*/%v51649
%vm51652 = vcmp.eq.f32.partialorder %v51602, 0.0
%v51653 = vand.u32 2147483648, %v51602
%v51654 = vsel /*vm=*/%vm51652, /*on_true_vy=*/%v51653, /*on_false_vx=*/%v51651
%v51657 = vadd.f32 -3.0, %v51654
%v51661 = vsel /*vm=*/%vm51605, /*on_true_vy=*/%v51646, /*on_false_vx=*/%v51657
%v51665 = vmul.f32 %v51661, %v51642
%v51669 = vadd.f32 %v51665, %v51638
%v51673 = vmul.f32 %v51669, %v51661
%v51677 = vadd.f32 %v51673, %v51634
%v51681 = vmul.f32 %v51677, %v51661
%v51685 = vadd.f32 %v51681, %v51630
%v51689 = vmul.f32 %v51685, %v51661
%v51693 = vadd.f32 %v51689, %v51626
%v51697 = vmul.f32 %v51693, %v51661
%v51701 = vadd.f32 %v51697, %v51622
%v51705 = vmul.f32 %v51701, %v51661
%v51709 = vadd.f32 %v51705, %v51618
%v51713 = vmul.f32 %v51709, %v51661
%v51717 = vadd.f32 %v51713, %v51614
%v51721 = vmul.f32 %v51717, %v51661
%v51725 = vadd.f32 %v51721, %v51610
%v51729 = vmul.f32 %v51725, %v51576
%v51733 = vsel /*vm=*/%vm51581, /*on_true_vy=*/%v51586, /*on_false_vx=*/%v51729
%v51737 = vmul.f32 1.4140625, %v51733
%v51740 = vpack.c.bf16 %v120417, %v51737
%120019 = vst [vmem:[%s280 + $0x2b4] sm:$0xf] /*vst_source=*/%v51740
%v51744 = vadd.s32 %v48975, %v3329
%v51754 = vadd.s32 %v51744, %v415
%vm51758 = vcmp.lt.u32.totalorder %v51754, %v51744
%vm51763 = vcmp.lt.u32.totalorder %v51744, %v3329
%v51768 = vadd.s32 %v48958, %v3316
%v51772 = vadd.s32 1, %v51768
%v51776 = vsel /*vm=*/%vm51763, /*on_true_vy=*/%v51772, /*on_false_vx=*/%v51768
%v51780 = vadd.s32 1, %v51776
%v51784 = vsel /*vm=*/%vm51758, /*on_true_vy=*/%v51780, /*on_false_vx=*/%v51776
%v51789 = vadd.s32 %v51784, %v10
%v51793 = vadd.s32 %v51754, %v9
%v51797 = vadd.s32 %v51793, %v51789
%v51799 = vshll.u32 %v51793, 13
%v51800 = vshrl.u32 %v51793, 19
%v51801 = vor.u32 %v51800, %v51799
%v51802 = vxor.u32 %v51801, %v51797
%v51805 = vadd.s32 %v51802, %v51797
%v51807 = vshll.u32 %v51802, 15
%v51808 = vshrl.u32 %v51802, 17
%v51809 = vor.u32 %v51808, %v51807
%v51810 = vxor.u32 %v51809, %v51805
%v51813 = vadd.s32 %v51810, %v51805
%v51815 = vshll.u32 %v51810, 26
%v51816 = vshrl.u32 %v51810, 6
%v51817 = vor.u32 %v51816, %v51815
%v51818 = vxor.u32 %v51817, %v51813
%v51821 = vadd.s32 %v51818, %v51813
%v51825 = vadd.s32 %v51821, %v9
%v51827 = vshll.u32 %v51818, 6
%v51828 = vshrl.u32 %v51818, 26
%v51829 = vor.u32 %v51828, %v51827
%v51830 = vxor.u32 %v51829, %v51821
%v51833 = vadd.s32 %v51830, %v8
%v51837 = vadd.s32 1, %v51833
%v51841 = vadd.s32 %v51837, %v51825
%v51843 = vshll.u32 %v51837, 17
%v51844 = vshrl.u32 %v51837, 15
%v51845 = vor.u32 %v51844, %v51843
%v51846 = vxor.u32 %v51845, %v51841
%v51849 = vadd.s32 %v51846, %v51841
%v51851 = vshll.u32 %v51846, 29
%v51852 = vshrl.u32 %v51846, 3
%v51853 = vor.u32 %v51852, %v51851
%v51854 = vxor.u32 %v51853, %v51849
%v51857 = vadd.s32 %v51854, %v51849
%v51859 = vshll.u32 %v51854, 16
%v51860 = vshrl.u32 %v51854, 16
%v51861 = vor.u32 %v51860, %v51859
%v51862 = vxor.u32 %v51861, %v51857
%v51865 = vadd.s32 %v51862, %v51857
%v51869 = vadd.s32 %v51865, %v8
%v51871 = vshll.u32 %v51862, 24
%v51872 = vshrl.u32 %v51862, 8
%v51873 = vor.u32 %v51872, %v51871
%v51874 = vxor.u32 %v51873, %v51865
%v51877 = vadd.s32 %v51874, %v10
%v51881 = vadd.s32 2, %v51877
%v51885 = vadd.s32 %v51881, %v51869
%v51887 = vshll.u32 %v51881, 13
%v51888 = vshrl.u32 %v51881, 19
%v51889 = vor.u32 %v51888, %v51887
%v51890 = vxor.u32 %v51889, %v51885
%v51893 = vadd.s32 %v51890, %v51885
%v51895 = vshll.u32 %v51890, 15
%v51896 = vshrl.u32 %v51890, 17
%v51897 = vor.u32 %v51896, %v51895
%v51898 = vxor.u32 %v51897, %v51893
%v51901 = vadd.s32 %v51898, %v51893
%v51903 = vshll.u32 %v51898, 26
%v51904 = vshrl.u32 %v51898, 6
%v51905 = vor.u32 %v51904, %v51903
%v51906 = vxor.u32 %v51905, %v51901
%v51909 = vadd.s32 %v51906, %v51901
%v51913 = vadd.s32 %v51909, %v10
%v51915 = vshll.u32 %v51906, 6
%v51916 = vshrl.u32 %v51906, 26
%v51917 = vor.u32 %v51916, %v51915
%v51918 = vxor.u32 %v51917, %v51909
%v51921 = vadd.s32 %v51918, %v9
%v51925 = vadd.s32 3, %v51921
%v51929 = vadd.s32 %v51925, %v51913
%v51931 = vshll.u32 %v51925, 17
%v51932 = vshrl.u32 %v51925, 15
%v51933 = vor.u32 %v51932, %v51931
%v51934 = vxor.u32 %v51933, %v51929
%v51937 = vadd.s32 %v51934, %v51929
%v51939 = vshll.u32 %v51934, 29
%v51940 = vshrl.u32 %v51934, 3
%v51941 = vor.u32 %v51940, %v51939
%v51942 = vxor.u32 %v51941, %v51937
%v51945 = vadd.s32 %v51942, %v51937
%v51947 = vshll.u32 %v51942, 16
%v51948 = vshrl.u32 %v51942, 16
%v51949 = vor.u32 %v51948, %v51947
%v51950 = vxor.u32 %v51949, %v51945
%v51953 = vadd.s32 %v51950, %v51945
%v51957 = vadd.s32 %v51953, %v9
%v51959 = vshll.u32 %v51950, 24
%v51960 = vshrl.u32 %v51950, 8
%v51961 = vor.u32 %v51960, %v51959
%v51962 = vxor.u32 %v51961, %v51953
%v51965 = vadd.s32 %v51962, %v8
%v51969 = vadd.s32 4, %v51965
%v51973 = vadd.s32 %v51969, %v51957
%v51975 = vshll.u32 %v51969, 13
%v51976 = vshrl.u32 %v51969, 19
%v51977 = vor.u32 %v51976, %v51975
%v51978 = vxor.u32 %v51977, %v51973
%v51981 = vadd.s32 %v51978, %v51973
%v51983 = vshll.u32 %v51978, 15
%v51984 = vshrl.u32 %v51978, 17
%v51985 = vor.u32 %v51984, %v51983
%v51986 = vxor.u32 %v51985, %v51981
%v51989 = vadd.s32 %v51986, %v51981
%v51991 = vshll.u32 %v51986, 26
%v51992 = vshrl.u32 %v51986, 6
%v51993 = vor.u32 %v51992, %v51991
%v51994 = vxor.u32 %v51993, %v51989
%v51997 = vadd.s32 %v51994, %v51989
%v52001 = vadd.s32 %v51997, %v8
%v52003 = vshll.u32 %v51994, 6
%v52004 = vshrl.u32 %v51994, 26
%v52005 = vor.u32 %v52004, %v52003
%v52006 = vxor.u32 %v52005, %v51997
%v52009 = vadd.s32 %v52006, %v10
%v52013 = vadd.s32 5, %v52009
%v52015 = vxor.u32 %v52013, %v52001
%v52016 = vand.u32.u8 255, %v52015
%v52017 = vand.u32 65535, %v52016
%v52018 = vshrl.u32 %v52017, 1
%v52019 = vor.u32 16256, %v52018
%v52020 = vand.u32.u16 65535, %v52019
%v120020 = vadd.low.f32.bf16 -1.0, %v52020
%v52029 = vmul.f32 2.0, %v120020
%v52033 = vadd.f32 -0.99609375, %v52029
%v52037 = vmax.f32 %v52033, -0.99609375
%v52039 = vand.u32 2147483647, %v52037
%vm52042 = vcmp.eq.f32.partialorder %v52039, 1.0
%v52047 = vmul.f32 inf, %v52037
%v52049 = vxor.u32 2147483648, %v52037
%v52052 = vmul.f32 %v52049, %v52037
%v52054 = vadd.f32 1.0, %v52052
%v52055 = vlog2.pop %v52054
%v52056 = vmul.f32 0.6931472, %v52055
%v52057 = vmul.f32 -0.5, %v52052
%v52058 = vadd.f32 1.0, %v52057
%v52059 = vmul.f32 %v52058, %v52052
%v52060 = vand.u32 2147483647, %v52052
%vm52061 = vcmp.lt.f32.partialorder %v52060, 0.0004427343
%v52062 = vsel /*vm=*/%vm52061, /*on_true_vy=*/%v52059, /*on_false_vx=*/%v52056
%v52063 = vxor.u32 2147483648, %v52062
%vm52066 = vcmp.lt.f32.partialorder %v52063, 5.0
%v52071 = vsel /*vm=*/%vm52066, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v52075 = vsel /*vm=*/%vm52066, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v52079 = vsel /*vm=*/%vm52066, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v52083 = vsel /*vm=*/%vm52066, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v52087 = vsel /*vm=*/%vm52066, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v52091 = vsel /*vm=*/%vm52066, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v52095 = vsel /*vm=*/%vm52066, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v52099 = vsel /*vm=*/%vm52066, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v52103 = vsel /*vm=*/%vm52066, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v52107 = vadd.f32 -2.5, %v52063
%v52109 = vrsqrt.pop %v52063
%v52110 = vmul.f32 %v52109, %v52063
%vm52111 = vcmp.eq.f32.partialorder %v52063, inf
%v52112 = vsel /*vm=*/%vm52111, /*on_true_vy=*/%v52063, /*on_false_vx=*/%v52110
%vm52113 = vcmp.eq.f32.partialorder %v52063, 0.0
%v52114 = vand.u32 2147483648, %v52063
%v52115 = vsel /*vm=*/%vm52113, /*on_true_vy=*/%v52114, /*on_false_vx=*/%v52112
%v52118 = vadd.f32 -3.0, %v52115
%v52122 = vsel /*vm=*/%vm52066, /*on_true_vy=*/%v52107, /*on_false_vx=*/%v52118
%v52126 = vmul.f32 %v52122, %v52103
%v52130 = vadd.f32 %v52126, %v52099
%v52134 = vmul.f32 %v52130, %v52122
%v52138 = vadd.f32 %v52134, %v52095
%v52142 = vmul.f32 %v52138, %v52122
%v52146 = vadd.f32 %v52142, %v52091
%v52150 = vmul.f32 %v52146, %v52122
%v52154 = vadd.f32 %v52150, %v52087
%v52158 = vmul.f32 %v52154, %v52122
%v52162 = vadd.f32 %v52158, %v52083
%v52166 = vmul.f32 %v52162, %v52122
%v52170 = vadd.f32 %v52166, %v52079
%v52174 = vmul.f32 %v52170, %v52122
%v52178 = vadd.f32 %v52174, %v52075
%v52182 = vmul.f32 %v52178, %v52122
%v52186 = vadd.f32 %v52182, %v52071
%v52190 = vmul.f32 %v52186, %v52037
%v52194 = vsel /*vm=*/%vm52042, /*on_true_vy=*/%v52047, /*on_false_vx=*/%v52190
%v52198 = vmul.f32 1.4140625, %v52194
%v52201 = vpack.c.bf16 %v120417, %v52198
%120021 = vst [vmem:[%s280 + $0x334] sm:$0xf] /*vst_source=*/%v52201
%v52205 = vadd.s32 %v48975, %v3816
%v52215 = vadd.s32 %v52205, %v415
%vm52219 = vcmp.lt.u32.totalorder %v52215, %v52205
%vm52224 = vcmp.lt.u32.totalorder %v52205, %v3816
%v52229 = vadd.s32 %v48958, %v3803
%v52233 = vadd.s32 1, %v52229
%v52237 = vsel /*vm=*/%vm52224, /*on_true_vy=*/%v52233, /*on_false_vx=*/%v52229
%v52241 = vadd.s32 1, %v52237
%v52245 = vsel /*vm=*/%vm52219, /*on_true_vy=*/%v52241, /*on_false_vx=*/%v52237
%v52250 = vadd.s32 %v52245, %v10
%v52254 = vadd.s32 %v52215, %v9
%v52258 = vadd.s32 %v52254, %v52250
%v52260 = vshll.u32 %v52254, 13
%v52261 = vshrl.u32 %v52254, 19
%v52262 = vor.u32 %v52261, %v52260
%v52263 = vxor.u32 %v52262, %v52258
%v52266 = vadd.s32 %v52263, %v52258
%v52268 = vshll.u32 %v52263, 15
%v52269 = vshrl.u32 %v52263, 17
%v52270 = vor.u32 %v52269, %v52268
%v52271 = vxor.u32 %v52270, %v52266
%v52274 = vadd.s32 %v52271, %v52266
%v52276 = vshll.u32 %v52271, 26
%v52277 = vshrl.u32 %v52271, 6
%v52278 = vor.u32 %v52277, %v52276
%v52279 = vxor.u32 %v52278, %v52274
%v52282 = vadd.s32 %v52279, %v52274
%v52286 = vadd.s32 %v52282, %v9
%v52288 = vshll.u32 %v52279, 6
%v52289 = vshrl.u32 %v52279, 26
%v52290 = vor.u32 %v52289, %v52288
%v52291 = vxor.u32 %v52290, %v52282
%v52294 = vadd.s32 %v52291, %v8
%v52298 = vadd.s32 1, %v52294
%v52302 = vadd.s32 %v52298, %v52286
%v52304 = vshll.u32 %v52298, 17
%v52305 = vshrl.u32 %v52298, 15
%v52306 = vor.u32 %v52305, %v52304
%v52307 = vxor.u32 %v52306, %v52302
%v52310 = vadd.s32 %v52307, %v52302
%v52312 = vshll.u32 %v52307, 29
%v52313 = vshrl.u32 %v52307, 3
%v52314 = vor.u32 %v52313, %v52312
%v52315 = vxor.u32 %v52314, %v52310
%v52318 = vadd.s32 %v52315, %v52310
%v52320 = vshll.u32 %v52315, 16
%v52321 = vshrl.u32 %v52315, 16
%v52322 = vor.u32 %v52321, %v52320
%v52323 = vxor.u32 %v52322, %v52318
%v52326 = vadd.s32 %v52323, %v52318
%v52330 = vadd.s32 %v52326, %v8
%v52332 = vshll.u32 %v52323, 24
%v52333 = vshrl.u32 %v52323, 8
%v52334 = vor.u32 %v52333, %v52332
%v52335 = vxor.u32 %v52334, %v52326
%v52338 = vadd.s32 %v52335, %v10
%v52342 = vadd.s32 2, %v52338
%v52346 = vadd.s32 %v52342, %v52330
%v52348 = vshll.u32 %v52342, 13
%v52349 = vshrl.u32 %v52342, 19
%v52350 = vor.u32 %v52349, %v52348
%v52351 = vxor.u32 %v52350, %v52346
%v52354 = vadd.s32 %v52351, %v52346
%v52356 = vshll.u32 %v52351, 15
%v52357 = vshrl.u32 %v52351, 17
%v52358 = vor.u32 %v52357, %v52356
%v52359 = vxor.u32 %v52358, %v52354
%v52362 = vadd.s32 %v52359, %v52354
%v52364 = vshll.u32 %v52359, 26
%v52365 = vshrl.u32 %v52359, 6
%v52366 = vor.u32 %v52365, %v52364
%v52367 = vxor.u32 %v52366, %v52362
%v52370 = vadd.s32 %v52367, %v52362
%v52374 = vadd.s32 %v52370, %v10
%v52376 = vshll.u32 %v52367, 6
%v52377 = vshrl.u32 %v52367, 26
%v52378 = vor.u32 %v52377, %v52376
%v52379 = vxor.u32 %v52378, %v52370
%v52382 = vadd.s32 %v52379, %v9
%v52386 = vadd.s32 3, %v52382
%v52390 = vadd.s32 %v52386, %v52374
%v52392 = vshll.u32 %v52386, 17
%v52393 = vshrl.u32 %v52386, 15
%v52394 = vor.u32 %v52393, %v52392
%v52395 = vxor.u32 %v52394, %v52390
%v52398 = vadd.s32 %v52395, %v52390
%v52400 = vshll.u32 %v52395, 29
%v52401 = vshrl.u32 %v52395, 3
%v52402 = vor.u32 %v52401, %v52400
%v52403 = vxor.u32 %v52402, %v52398
%v52406 = vadd.s32 %v52403, %v52398
%v52408 = vshll.u32 %v52403, 16
%v52409 = vshrl.u32 %v52403, 16
%v52410 = vor.u32 %v52409, %v52408
%v52411 = vxor.u32 %v52410, %v52406
%v52414 = vadd.s32 %v52411, %v52406
%v52418 = vadd.s32 %v52414, %v9
%v52420 = vshll.u32 %v52411, 24
%v52421 = vshrl.u32 %v52411, 8
%v52422 = vor.u32 %v52421, %v52420
%v52423 = vxor.u32 %v52422, %v52414
%v52426 = vadd.s32 %v52423, %v8
%v52430 = vadd.s32 4, %v52426
%v52434 = vadd.s32 %v52430, %v52418
%v52436 = vshll.u32 %v52430, 13
%v52437 = vshrl.u32 %v52430, 19
%v52438 = vor.u32 %v52437, %v52436
%v52439 = vxor.u32 %v52438, %v52434
%v52442 = vadd.s32 %v52439, %v52434
%v52444 = vshll.u32 %v52439, 15
%v52445 = vshrl.u32 %v52439, 17
%v52446 = vor.u32 %v52445, %v52444
%v52447 = vxor.u32 %v52446, %v52442
%v52450 = vadd.s32 %v52447, %v52442
%v52452 = vshll.u32 %v52447, 26
%v52453 = vshrl.u32 %v52447, 6
%v52454 = vor.u32 %v52453, %v52452
%v52455 = vxor.u32 %v52454, %v52450
%v52458 = vadd.s32 %v52455, %v52450
%v52462 = vadd.s32 %v52458, %v8
%v52464 = vshll.u32 %v52455, 6
%v52465 = vshrl.u32 %v52455, 26
%v52466 = vor.u32 %v52465, %v52464
%v52467 = vxor.u32 %v52466, %v52458
%v52470 = vadd.s32 %v52467, %v10
%v52474 = vadd.s32 5, %v52470
%v52476 = vxor.u32 %v52474, %v52462
%v52477 = vand.u32.u8 255, %v52476
%v52478 = vand.u32 65535, %v52477
%v52479 = vshrl.u32 %v52478, 1
%v52480 = vor.u32 16256, %v52479
%v52481 = vand.u32.u16 65535, %v52480
%v120022 = vadd.low.f32.bf16 -1.0, %v52481
%v52490 = vmul.f32 2.0, %v120022
%v52494 = vadd.f32 -0.99609375, %v52490
%v52498 = vmax.f32 %v52494, -0.99609375
%v52500 = vand.u32 2147483647, %v52498
%vm52503 = vcmp.eq.f32.partialorder %v52500, 1.0
%v52508 = vmul.f32 inf, %v52498
%v52510 = vxor.u32 2147483648, %v52498
%v52513 = vmul.f32 %v52510, %v52498
%v52515 = vadd.f32 1.0, %v52513
%v52516 = vlog2.pop %v52515
%v52517 = vmul.f32 0.6931472, %v52516
%v52518 = vmul.f32 -0.5, %v52513
%v52519 = vadd.f32 1.0, %v52518
%v52520 = vmul.f32 %v52519, %v52513
%v52521 = vand.u32 2147483647, %v52513
%vm52522 = vcmp.lt.f32.partialorder %v52521, 0.0004427343
%v52523 = vsel /*vm=*/%vm52522, /*on_true_vy=*/%v52520, /*on_false_vx=*/%v52517
%v52524 = vxor.u32 2147483648, %v52523
%vm52527 = vcmp.lt.f32.partialorder %v52524, 5.0
%v52532 = vsel /*vm=*/%vm52527, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v52536 = vsel /*vm=*/%vm52527, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v52540 = vsel /*vm=*/%vm52527, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v52544 = vsel /*vm=*/%vm52527, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v52548 = vsel /*vm=*/%vm52527, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v52552 = vsel /*vm=*/%vm52527, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v52556 = vsel /*vm=*/%vm52527, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v52560 = vsel /*vm=*/%vm52527, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v52564 = vsel /*vm=*/%vm52527, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v52568 = vadd.f32 -2.5, %v52524
%v52570 = vrsqrt.pop %v52524
%v52571 = vmul.f32 %v52570, %v52524
%vm52572 = vcmp.eq.f32.partialorder %v52524, inf
%v52573 = vsel /*vm=*/%vm52572, /*on_true_vy=*/%v52524, /*on_false_vx=*/%v52571
%vm52574 = vcmp.eq.f32.partialorder %v52524, 0.0
%v52575 = vand.u32 2147483648, %v52524
%v52576 = vsel /*vm=*/%vm52574, /*on_true_vy=*/%v52575, /*on_false_vx=*/%v52573
%v52579 = vadd.f32 -3.0, %v52576
%v52583 = vsel /*vm=*/%vm52527, /*on_true_vy=*/%v52568, /*on_false_vx=*/%v52579
%v52587 = vmul.f32 %v52583, %v52564
%v52591 = vadd.f32 %v52587, %v52560
%v52595 = vmul.f32 %v52591, %v52583
%v52599 = vadd.f32 %v52595, %v52556
%v52603 = vmul.f32 %v52599, %v52583
%v52607 = vadd.f32 %v52603, %v52552
%v52611 = vmul.f32 %v52607, %v52583
%v52615 = vadd.f32 %v52611, %v52548
%v52619 = vmul.f32 %v52615, %v52583
%v52623 = vadd.f32 %v52619, %v52544
%v52627 = vmul.f32 %v52623, %v52583
%v52631 = vadd.f32 %v52627, %v52540
%v52635 = vmul.f32 %v52631, %v52583
%v52639 = vadd.f32 %v52635, %v52536
%v52643 = vmul.f32 %v52639, %v52583
%v52647 = vadd.f32 %v52643, %v52532
%v52651 = vmul.f32 %v52647, %v52498
%v52655 = vsel /*vm=*/%vm52503, /*on_true_vy=*/%v52508, /*on_false_vx=*/%v52651
%v52659 = vmul.f32 1.4140625, %v52655
%v52662 = vpack.c.bf16 %v120417, %v52659
%120023 = vst [vmem:[%s280 + $0x3b4] sm:$0xf] /*vst_source=*/%v52662
%v52700 = vadd.s32 %v52697, %v408
%v52710 = vadd.s32 %v52700, %v415
%vm52714 = vcmp.lt.u32.totalorder %v52710, %v52700
%vm52719 = vcmp.lt.u32.totalorder %v52700, %v408
%v52724 = vadd.s32 %v52680, %v380
%v52728 = vadd.s32 1, %v52724
%v52732 = vsel /*vm=*/%vm52719, /*on_true_vy=*/%v52728, /*on_false_vx=*/%v52724
%v52736 = vadd.s32 1, %v52732
%v52740 = vsel /*vm=*/%vm52714, /*on_true_vy=*/%v52736, /*on_false_vx=*/%v52732
%v52745 = vadd.s32 %v52740, %v10
%v52749 = vadd.s32 %v52710, %v9
%v52753 = vadd.s32 %v52749, %v52745
%v52755 = vshll.u32 %v52749, 13
%v52756 = vshrl.u32 %v52749, 19
%v52757 = vor.u32 %v52756, %v52755
%v52758 = vxor.u32 %v52757, %v52753
%v52761 = vadd.s32 %v52758, %v52753
%v52763 = vshll.u32 %v52758, 15
%v52764 = vshrl.u32 %v52758, 17
%v52765 = vor.u32 %v52764, %v52763
%v52766 = vxor.u32 %v52765, %v52761
%v52769 = vadd.s32 %v52766, %v52761
%v52771 = vshll.u32 %v52766, 26
%v52772 = vshrl.u32 %v52766, 6
%v52773 = vor.u32 %v52772, %v52771
%v52774 = vxor.u32 %v52773, %v52769
%v52777 = vadd.s32 %v52774, %v52769
%v52781 = vadd.s32 %v52777, %v9
%v52783 = vshll.u32 %v52774, 6
%v52784 = vshrl.u32 %v52774, 26
%v52785 = vor.u32 %v52784, %v52783
%v52786 = vxor.u32 %v52785, %v52777
%v52789 = vadd.s32 %v52786, %v8
%v52793 = vadd.s32 1, %v52789
%v52797 = vadd.s32 %v52793, %v52781
%v52799 = vshll.u32 %v52793, 17
%v52800 = vshrl.u32 %v52793, 15
%v52801 = vor.u32 %v52800, %v52799
%v52802 = vxor.u32 %v52801, %v52797
%v52805 = vadd.s32 %v52802, %v52797
%v52807 = vshll.u32 %v52802, 29
%v52808 = vshrl.u32 %v52802, 3
%v52809 = vor.u32 %v52808, %v52807
%v52810 = vxor.u32 %v52809, %v52805
%v52813 = vadd.s32 %v52810, %v52805
%v52815 = vshll.u32 %v52810, 16
%v52816 = vshrl.u32 %v52810, 16
%v52817 = vor.u32 %v52816, %v52815
%v52818 = vxor.u32 %v52817, %v52813
%v52821 = vadd.s32 %v52818, %v52813
%v52825 = vadd.s32 %v52821, %v8
%v52827 = vshll.u32 %v52818, 24
%v52828 = vshrl.u32 %v52818, 8
%v52829 = vor.u32 %v52828, %v52827
%v52830 = vxor.u32 %v52829, %v52821
%v52833 = vadd.s32 %v52830, %v10
%v52837 = vadd.s32 2, %v52833
%v52841 = vadd.s32 %v52837, %v52825
%v52843 = vshll.u32 %v52837, 13
%v52844 = vshrl.u32 %v52837, 19
%v52845 = vor.u32 %v52844, %v52843
%v52846 = vxor.u32 %v52845, %v52841
%v52849 = vadd.s32 %v52846, %v52841
%v52851 = vshll.u32 %v52846, 15
%v52852 = vshrl.u32 %v52846, 17
%v52853 = vor.u32 %v52852, %v52851
%v52854 = vxor.u32 %v52853, %v52849
%v52857 = vadd.s32 %v52854, %v52849
%v52859 = vshll.u32 %v52854, 26
%v52860 = vshrl.u32 %v52854, 6
%v52861 = vor.u32 %v52860, %v52859
%v52862 = vxor.u32 %v52861, %v52857
%v52865 = vadd.s32 %v52862, %v52857
%v52869 = vadd.s32 %v52865, %v10
%v52871 = vshll.u32 %v52862, 6
%v52872 = vshrl.u32 %v52862, 26
%v52873 = vor.u32 %v52872, %v52871
%v52874 = vxor.u32 %v52873, %v52865
%v52877 = vadd.s32 %v52874, %v9
%v52881 = vadd.s32 3, %v52877
%v52885 = vadd.s32 %v52881, %v52869
%v52887 = vshll.u32 %v52881, 17
%v52888 = vshrl.u32 %v52881, 15
%v52889 = vor.u32 %v52888, %v52887
%v52890 = vxor.u32 %v52889, %v52885
%v52893 = vadd.s32 %v52890, %v52885
%v52895 = vshll.u32 %v52890, 29
%v52896 = vshrl.u32 %v52890, 3
%v52897 = vor.u32 %v52896, %v52895
%v52898 = vxor.u32 %v52897, %v52893
%v52901 = vadd.s32 %v52898, %v52893
%v52903 = vshll.u32 %v52898, 16
%v52904 = vshrl.u32 %v52898, 16
%v52905 = vor.u32 %v52904, %v52903
%v52906 = vxor.u32 %v52905, %v52901
%v52909 = vadd.s32 %v52906, %v52901
%v52913 = vadd.s32 %v52909, %v9
%v52915 = vshll.u32 %v52906, 24
%v52916 = vshrl.u32 %v52906, 8
%v52917 = vor.u32 %v52916, %v52915
%v52918 = vxor.u32 %v52917, %v52909
%v52921 = vadd.s32 %v52918, %v8
%v52925 = vadd.s32 4, %v52921
%v52929 = vadd.s32 %v52925, %v52913
%v52931 = vshll.u32 %v52925, 13
%v52932 = vshrl.u32 %v52925, 19
%v52933 = vor.u32 %v52932, %v52931
%v52934 = vxor.u32 %v52933, %v52929
%v52937 = vadd.s32 %v52934, %v52929
%v52939 = vshll.u32 %v52934, 15
%v52940 = vshrl.u32 %v52934, 17
%v52941 = vor.u32 %v52940, %v52939
%v52942 = vxor.u32 %v52941, %v52937
%v52945 = vadd.s32 %v52942, %v52937
%v52947 = vshll.u32 %v52942, 26
%v52948 = vshrl.u32 %v52942, 6
%v52949 = vor.u32 %v52948, %v52947
%v52950 = vxor.u32 %v52949, %v52945
%v52953 = vadd.s32 %v52950, %v52945
%v52957 = vadd.s32 %v52953, %v8
%v52959 = vshll.u32 %v52950, 6
%v52960 = vshrl.u32 %v52950, 26
%v52961 = vor.u32 %v52960, %v52959
%v52962 = vxor.u32 %v52961, %v52953
%v52965 = vadd.s32 %v52962, %v10
%v52969 = vadd.s32 5, %v52965
%v52971 = vxor.u32 %v52969, %v52957
%v52972 = vand.u32.u8 255, %v52971
%v52973 = vand.u32 65535, %v52972
%v52974 = vshrl.u32 %v52973, 1
%v52975 = vor.u32 16256, %v52974
%v52976 = vand.u32.u16 65535, %v52975
%v120028 = vadd.low.f32.bf16 -1.0, %v52976
%v52985 = vmul.f32 2.0, %v120028
%v52989 = vadd.f32 -0.99609375, %v52985
%v52993 = vmax.f32 %v52989, -0.99609375
%v52995 = vand.u32 2147483647, %v52993
%vm52998 = vcmp.eq.f32.partialorder %v52995, 1.0
%v53003 = vmul.f32 inf, %v52993
%v53005 = vxor.u32 2147483648, %v52993
%v53008 = vmul.f32 %v53005, %v52993
%v53010 = vadd.f32 1.0, %v53008
%v53011 = vlog2.pop %v53010
%v53012 = vmul.f32 0.6931472, %v53011
%v53013 = vmul.f32 -0.5, %v53008
%v53014 = vadd.f32 1.0, %v53013
%v53015 = vmul.f32 %v53014, %v53008
%v53016 = vand.u32 2147483647, %v53008
%vm53017 = vcmp.lt.f32.partialorder %v53016, 0.0004427343
%v53018 = vsel /*vm=*/%vm53017, /*on_true_vy=*/%v53015, /*on_false_vx=*/%v53012
%v53019 = vxor.u32 2147483648, %v53018
%vm53022 = vcmp.lt.f32.partialorder %v53019, 5.0
%v53027 = vsel /*vm=*/%vm53022, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v53031 = vsel /*vm=*/%vm53022, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v53035 = vsel /*vm=*/%vm53022, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v53039 = vsel /*vm=*/%vm53022, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v53043 = vsel /*vm=*/%vm53022, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v53047 = vsel /*vm=*/%vm53022, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v53051 = vsel /*vm=*/%vm53022, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v53055 = vsel /*vm=*/%vm53022, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v53059 = vsel /*vm=*/%vm53022, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v53063 = vadd.f32 -2.5, %v53019
%v53065 = vrsqrt.pop %v53019
%v53066 = vmul.f32 %v53065, %v53019
%vm53067 = vcmp.eq.f32.partialorder %v53019, inf
%v53068 = vsel /*vm=*/%vm53067, /*on_true_vy=*/%v53019, /*on_false_vx=*/%v53066
%vm53069 = vcmp.eq.f32.partialorder %v53019, 0.0
%v53070 = vand.u32 2147483648, %v53019
%v53071 = vsel /*vm=*/%vm53069, /*on_true_vy=*/%v53070, /*on_false_vx=*/%v53068
%v53074 = vadd.f32 -3.0, %v53071
%v53078 = vsel /*vm=*/%vm53022, /*on_true_vy=*/%v53063, /*on_false_vx=*/%v53074
%v53082 = vmul.f32 %v53078, %v53059
%v53086 = vadd.f32 %v53082, %v53055
%v53090 = vmul.f32 %v53086, %v53078
%v53094 = vadd.f32 %v53090, %v53051
%v53098 = vmul.f32 %v53094, %v53078
%v53102 = vadd.f32 %v53098, %v53047
%v53106 = vmul.f32 %v53102, %v53078
%v53110 = vadd.f32 %v53106, %v53043
%v53114 = vmul.f32 %v53110, %v53078
%v53118 = vadd.f32 %v53114, %v53039
%v53122 = vmul.f32 %v53118, %v53078
%v53126 = vadd.f32 %v53122, %v53035
%v53130 = vmul.f32 %v53126, %v53078
%v53134 = vadd.f32 %v53130, %v53031
%v53138 = vmul.f32 %v53134, %v53078
%v53142 = vadd.f32 %v53138, %v53027
%v53146 = vmul.f32 %v53142, %v52993
%v53150 = vsel /*vm=*/%vm52998, /*on_true_vy=*/%v53003, /*on_false_vx=*/%v53146
%v53154 = vmul.f32 1.4140625, %v53150
%v53157 = vpack.c.bf16 %v120417, %v53154
%120029 = vst [vmem:[%s280 + $0x38] sm:$0xf] /*vst_source=*/%v53157
%v53161 = vadd.s32 %v52697, %v894
%v53171 = vadd.s32 %v53161, %v415
%vm53175 = vcmp.lt.u32.totalorder %v53171, %v53161
%vm53180 = vcmp.lt.u32.totalorder %v53161, %v894
%v53185 = vadd.s32 %v52680, %v881
%v53189 = vadd.s32 1, %v53185
%v53193 = vsel /*vm=*/%vm53180, /*on_true_vy=*/%v53189, /*on_false_vx=*/%v53185
%v53197 = vadd.s32 1, %v53193
%v53201 = vsel /*vm=*/%vm53175, /*on_true_vy=*/%v53197, /*on_false_vx=*/%v53193
%v53206 = vadd.s32 %v53201, %v10
%v53210 = vadd.s32 %v53171, %v9
%v53214 = vadd.s32 %v53210, %v53206
%v53216 = vshll.u32 %v53210, 13
%v53217 = vshrl.u32 %v53210, 19
%v53218 = vor.u32 %v53217, %v53216
%v53219 = vxor.u32 %v53218, %v53214
%v53222 = vadd.s32 %v53219, %v53214
%v53224 = vshll.u32 %v53219, 15
%v53225 = vshrl.u32 %v53219, 17
%v53226 = vor.u32 %v53225, %v53224
%v53227 = vxor.u32 %v53226, %v53222
%v53230 = vadd.s32 %v53227, %v53222
%v53232 = vshll.u32 %v53227, 26
%v53233 = vshrl.u32 %v53227, 6
%v53234 = vor.u32 %v53233, %v53232
%v53235 = vxor.u32 %v53234, %v53230
%v53238 = vadd.s32 %v53235, %v53230
%v53242 = vadd.s32 %v53238, %v9
%v53244 = vshll.u32 %v53235, 6
%v53245 = vshrl.u32 %v53235, 26
%v53246 = vor.u32 %v53245, %v53244
%v53247 = vxor.u32 %v53246, %v53238
%v53250 = vadd.s32 %v53247, %v8
%v53254 = vadd.s32 1, %v53250
%v53258 = vadd.s32 %v53254, %v53242
%v53260 = vshll.u32 %v53254, 17
%v53261 = vshrl.u32 %v53254, 15
%v53262 = vor.u32 %v53261, %v53260
%v53263 = vxor.u32 %v53262, %v53258
%v53266 = vadd.s32 %v53263, %v53258
%v53268 = vshll.u32 %v53263, 29
%v53269 = vshrl.u32 %v53263, 3
%v53270 = vor.u32 %v53269, %v53268
%v53271 = vxor.u32 %v53270, %v53266
%v53274 = vadd.s32 %v53271, %v53266
%v53276 = vshll.u32 %v53271, 16
%v53277 = vshrl.u32 %v53271, 16
%v53278 = vor.u32 %v53277, %v53276
%v53279 = vxor.u32 %v53278, %v53274
%v53282 = vadd.s32 %v53279, %v53274
%v53286 = vadd.s32 %v53282, %v8
%v53288 = vshll.u32 %v53279, 24
%v53289 = vshrl.u32 %v53279, 8
%v53290 = vor.u32 %v53289, %v53288
%v53291 = vxor.u32 %v53290, %v53282
%v53294 = vadd.s32 %v53291, %v10
%v53298 = vadd.s32 2, %v53294
%v53302 = vadd.s32 %v53298, %v53286
%v53304 = vshll.u32 %v53298, 13
%v53305 = vshrl.u32 %v53298, 19
%v53306 = vor.u32 %v53305, %v53304
%v53307 = vxor.u32 %v53306, %v53302
%v53310 = vadd.s32 %v53307, %v53302
%v53312 = vshll.u32 %v53307, 15
%v53313 = vshrl.u32 %v53307, 17
%v53314 = vor.u32 %v53313, %v53312
%v53315 = vxor.u32 %v53314, %v53310
%v53318 = vadd.s32 %v53315, %v53310
%v53320 = vshll.u32 %v53315, 26
%v53321 = vshrl.u32 %v53315, 6
%v53322 = vor.u32 %v53321, %v53320
%v53323 = vxor.u32 %v53322, %v53318
%v53326 = vadd.s32 %v53323, %v53318
%v53330 = vadd.s32 %v53326, %v10
%v53332 = vshll.u32 %v53323, 6
%v53333 = vshrl.u32 %v53323, 26
%v53334 = vor.u32 %v53333, %v53332
%v53335 = vxor.u32 %v53334, %v53326
%v53338 = vadd.s32 %v53335, %v9
%v53342 = vadd.s32 3, %v53338
%v53346 = vadd.s32 %v53342, %v53330
%v53348 = vshll.u32 %v53342, 17
%v53349 = vshrl.u32 %v53342, 15
%v53350 = vor.u32 %v53349, %v53348
%v53351 = vxor.u32 %v53350, %v53346
%v53354 = vadd.s32 %v53351, %v53346
%v53356 = vshll.u32 %v53351, 29
%v53357 = vshrl.u32 %v53351, 3
%v53358 = vor.u32 %v53357, %v53356
%v53359 = vxor.u32 %v53358, %v53354
%v53362 = vadd.s32 %v53359, %v53354
%v53364 = vshll.u32 %v53359, 16
%v53365 = vshrl.u32 %v53359, 16
%v53366 = vor.u32 %v53365, %v53364
%v53367 = vxor.u32 %v53366, %v53362
%v53370 = vadd.s32 %v53367, %v53362
%v53374 = vadd.s32 %v53370, %v9
%v53376 = vshll.u32 %v53367, 24
%v53377 = vshrl.u32 %v53367, 8
%v53378 = vor.u32 %v53377, %v53376
%v53379 = vxor.u32 %v53378, %v53370
%v53382 = vadd.s32 %v53379, %v8
%v53386 = vadd.s32 4, %v53382
%v53390 = vadd.s32 %v53386, %v53374
%v53392 = vshll.u32 %v53386, 13
%v53393 = vshrl.u32 %v53386, 19
%v53394 = vor.u32 %v53393, %v53392
%v53395 = vxor.u32 %v53394, %v53390
%v53398 = vadd.s32 %v53395, %v53390
%v53400 = vshll.u32 %v53395, 15
%v53401 = vshrl.u32 %v53395, 17
%v53402 = vor.u32 %v53401, %v53400
%v53403 = vxor.u32 %v53402, %v53398
%v53406 = vadd.s32 %v53403, %v53398
%v53408 = vshll.u32 %v53403, 26
%v53409 = vshrl.u32 %v53403, 6
%v53410 = vor.u32 %v53409, %v53408
%v53411 = vxor.u32 %v53410, %v53406
%v53414 = vadd.s32 %v53411, %v53406
%v53418 = vadd.s32 %v53414, %v8
%v53420 = vshll.u32 %v53411, 6
%v53421 = vshrl.u32 %v53411, 26
%v53422 = vor.u32 %v53421, %v53420
%v53423 = vxor.u32 %v53422, %v53414
%v53426 = vadd.s32 %v53423, %v10
%v53430 = vadd.s32 5, %v53426
%v53432 = vxor.u32 %v53430, %v53418
%v53433 = vand.u32.u8 255, %v53432
%v53434 = vand.u32 65535, %v53433
%v53435 = vshrl.u32 %v53434, 1
%v53436 = vor.u32 16256, %v53435
%v53437 = vand.u32.u16 65535, %v53436
%v120030 = vadd.low.f32.bf16 -1.0, %v53437
%v53446 = vmul.f32 2.0, %v120030
%v53450 = vadd.f32 -0.99609375, %v53446
%v53454 = vmax.f32 %v53450, -0.99609375
%v53456 = vand.u32 2147483647, %v53454
%vm53459 = vcmp.eq.f32.partialorder %v53456, 1.0
%v53464 = vmul.f32 inf, %v53454
%v53466 = vxor.u32 2147483648, %v53454
%v53469 = vmul.f32 %v53466, %v53454
%v53471 = vadd.f32 1.0, %v53469
%v53472 = vlog2.pop %v53471
%v53473 = vmul.f32 0.6931472, %v53472
%v53474 = vmul.f32 -0.5, %v53469
%v53475 = vadd.f32 1.0, %v53474
%v53476 = vmul.f32 %v53475, %v53469
%v53477 = vand.u32 2147483647, %v53469
%vm53478 = vcmp.lt.f32.partialorder %v53477, 0.0004427343
%v53479 = vsel /*vm=*/%vm53478, /*on_true_vy=*/%v53476, /*on_false_vx=*/%v53473
%v53480 = vxor.u32 2147483648, %v53479
%vm53483 = vcmp.lt.f32.partialorder %v53480, 5.0
%v53488 = vsel /*vm=*/%vm53483, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v53492 = vsel /*vm=*/%vm53483, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v53496 = vsel /*vm=*/%vm53483, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v53500 = vsel /*vm=*/%vm53483, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v53504 = vsel /*vm=*/%vm53483, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v53508 = vsel /*vm=*/%vm53483, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v53512 = vsel /*vm=*/%vm53483, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v53516 = vsel /*vm=*/%vm53483, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v53520 = vsel /*vm=*/%vm53483, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v53524 = vadd.f32 -2.5, %v53480
%v53526 = vrsqrt.pop %v53480
%v53527 = vmul.f32 %v53526, %v53480
%vm53528 = vcmp.eq.f32.partialorder %v53480, inf
%v53529 = vsel /*vm=*/%vm53528, /*on_true_vy=*/%v53480, /*on_false_vx=*/%v53527
%vm53530 = vcmp.eq.f32.partialorder %v53480, 0.0
%v53531 = vand.u32 2147483648, %v53480
%v53532 = vsel /*vm=*/%vm53530, /*on_true_vy=*/%v53531, /*on_false_vx=*/%v53529
%v53535 = vadd.f32 -3.0, %v53532
%v53539 = vsel /*vm=*/%vm53483, /*on_true_vy=*/%v53524, /*on_false_vx=*/%v53535
%v53543 = vmul.f32 %v53539, %v53520
%v53547 = vadd.f32 %v53543, %v53516
%v53551 = vmul.f32 %v53547, %v53539
%v53555 = vadd.f32 %v53551, %v53512
%v53559 = vmul.f32 %v53555, %v53539
%v53563 = vadd.f32 %v53559, %v53508
%v53567 = vmul.f32 %v53563, %v53539
%v53571 = vadd.f32 %v53567, %v53504
%v53575 = vmul.f32 %v53571, %v53539
%v53579 = vadd.f32 %v53575, %v53500
%v53583 = vmul.f32 %v53579, %v53539
%v53587 = vadd.f32 %v53583, %v53496
%v53591 = vmul.f32 %v53587, %v53539
%v53595 = vadd.f32 %v53591, %v53492
%v53599 = vmul.f32 %v53595, %v53539
%v53603 = vadd.f32 %v53599, %v53488
%v53607 = vmul.f32 %v53603, %v53454
%v53611 = vsel /*vm=*/%vm53459, /*on_true_vy=*/%v53464, /*on_false_vx=*/%v53607
%v53615 = vmul.f32 1.4140625, %v53611
%v53618 = vpack.c.bf16 %v120417, %v53615
%120031 = vst [vmem:[%s280 + $0xb8] sm:$0xf] /*vst_source=*/%v53618
%v53622 = vadd.s32 %v52697, %v1381
%v53632 = vadd.s32 %v53622, %v415
%vm53636 = vcmp.lt.u32.totalorder %v53632, %v53622
%vm53641 = vcmp.lt.u32.totalorder %v53622, %v1381
%v53646 = vadd.s32 %v52680, %v1368
%v53650 = vadd.s32 1, %v53646
%v53654 = vsel /*vm=*/%vm53641, /*on_true_vy=*/%v53650, /*on_false_vx=*/%v53646
%v53658 = vadd.s32 1, %v53654
%v53662 = vsel /*vm=*/%vm53636, /*on_true_vy=*/%v53658, /*on_false_vx=*/%v53654
%v53667 = vadd.s32 %v53662, %v10
%v53671 = vadd.s32 %v53632, %v9
%v53675 = vadd.s32 %v53671, %v53667
%v53677 = vshll.u32 %v53671, 13
%v53678 = vshrl.u32 %v53671, 19
%v53679 = vor.u32 %v53678, %v53677
%v53680 = vxor.u32 %v53679, %v53675
%v53683 = vadd.s32 %v53680, %v53675
%v53685 = vshll.u32 %v53680, 15
%v53686 = vshrl.u32 %v53680, 17
%v53687 = vor.u32 %v53686, %v53685
%v53688 = vxor.u32 %v53687, %v53683
%v53691 = vadd.s32 %v53688, %v53683
%v53693 = vshll.u32 %v53688, 26
%v53694 = vshrl.u32 %v53688, 6
%v53695 = vor.u32 %v53694, %v53693
%v53696 = vxor.u32 %v53695, %v53691
%v53699 = vadd.s32 %v53696, %v53691
%v53703 = vadd.s32 %v53699, %v9
%v53705 = vshll.u32 %v53696, 6
%v53706 = vshrl.u32 %v53696, 26
%v53707 = vor.u32 %v53706, %v53705
%v53708 = vxor.u32 %v53707, %v53699
%v53711 = vadd.s32 %v53708, %v8
%v53715 = vadd.s32 1, %v53711
%v53719 = vadd.s32 %v53715, %v53703
%v53721 = vshll.u32 %v53715, 17
%v53722 = vshrl.u32 %v53715, 15
%v53723 = vor.u32 %v53722, %v53721
%v53724 = vxor.u32 %v53723, %v53719
%v53727 = vadd.s32 %v53724, %v53719
%v53729 = vshll.u32 %v53724, 29
%v53730 = vshrl.u32 %v53724, 3
%v53731 = vor.u32 %v53730, %v53729
%v53732 = vxor.u32 %v53731, %v53727
%v53735 = vadd.s32 %v53732, %v53727
%v53737 = vshll.u32 %v53732, 16
%v53738 = vshrl.u32 %v53732, 16
%v53739 = vor.u32 %v53738, %v53737
%v53740 = vxor.u32 %v53739, %v53735
%v53743 = vadd.s32 %v53740, %v53735
%v53747 = vadd.s32 %v53743, %v8
%v53749 = vshll.u32 %v53740, 24
%v53750 = vshrl.u32 %v53740, 8
%v53751 = vor.u32 %v53750, %v53749
%v53752 = vxor.u32 %v53751, %v53743
%v53755 = vadd.s32 %v53752, %v10
%v53759 = vadd.s32 2, %v53755
%v53763 = vadd.s32 %v53759, %v53747
%v53765 = vshll.u32 %v53759, 13
%v53766 = vshrl.u32 %v53759, 19
%v53767 = vor.u32 %v53766, %v53765
%v53768 = vxor.u32 %v53767, %v53763
%v53771 = vadd.s32 %v53768, %v53763
%v53773 = vshll.u32 %v53768, 15
%v53774 = vshrl.u32 %v53768, 17
%v53775 = vor.u32 %v53774, %v53773
%v53776 = vxor.u32 %v53775, %v53771
%v53779 = vadd.s32 %v53776, %v53771
%v53781 = vshll.u32 %v53776, 26
%v53782 = vshrl.u32 %v53776, 6
%v53783 = vor.u32 %v53782, %v53781
%v53784 = vxor.u32 %v53783, %v53779
%v53787 = vadd.s32 %v53784, %v53779
%v53791 = vadd.s32 %v53787, %v10
%v53793 = vshll.u32 %v53784, 6
%v53794 = vshrl.u32 %v53784, 26
%v53795 = vor.u32 %v53794, %v53793
%v53796 = vxor.u32 %v53795, %v53787
%v53799 = vadd.s32 %v53796, %v9
%v53803 = vadd.s32 3, %v53799
%v53807 = vadd.s32 %v53803, %v53791
%v53809 = vshll.u32 %v53803, 17
%v53810 = vshrl.u32 %v53803, 15
%v53811 = vor.u32 %v53810, %v53809
%v53812 = vxor.u32 %v53811, %v53807
%v53815 = vadd.s32 %v53812, %v53807
%v53817 = vshll.u32 %v53812, 29
%v53818 = vshrl.u32 %v53812, 3
%v53819 = vor.u32 %v53818, %v53817
%v53820 = vxor.u32 %v53819, %v53815
%v53823 = vadd.s32 %v53820, %v53815
%v53825 = vshll.u32 %v53820, 16
%v53826 = vshrl.u32 %v53820, 16
%v53827 = vor.u32 %v53826, %v53825
%v53828 = vxor.u32 %v53827, %v53823
%v53831 = vadd.s32 %v53828, %v53823
%v53835 = vadd.s32 %v53831, %v9
%v53837 = vshll.u32 %v53828, 24
%v53838 = vshrl.u32 %v53828, 8
%v53839 = vor.u32 %v53838, %v53837
%v53840 = vxor.u32 %v53839, %v53831
%v53843 = vadd.s32 %v53840, %v8
%v53847 = vadd.s32 4, %v53843
%v53851 = vadd.s32 %v53847, %v53835
%v53853 = vshll.u32 %v53847, 13
%v53854 = vshrl.u32 %v53847, 19
%v53855 = vor.u32 %v53854, %v53853
%v53856 = vxor.u32 %v53855, %v53851
%v53859 = vadd.s32 %v53856, %v53851
%v53861 = vshll.u32 %v53856, 15
%v53862 = vshrl.u32 %v53856, 17
%v53863 = vor.u32 %v53862, %v53861
%v53864 = vxor.u32 %v53863, %v53859
%v53867 = vadd.s32 %v53864, %v53859
%v53869 = vshll.u32 %v53864, 26
%v53870 = vshrl.u32 %v53864, 6
%v53871 = vor.u32 %v53870, %v53869
%v53872 = vxor.u32 %v53871, %v53867
%v53875 = vadd.s32 %v53872, %v53867
%v53879 = vadd.s32 %v53875, %v8
%v53881 = vshll.u32 %v53872, 6
%v53882 = vshrl.u32 %v53872, 26
%v53883 = vor.u32 %v53882, %v53881
%v53884 = vxor.u32 %v53883, %v53875
%v53887 = vadd.s32 %v53884, %v10
%v53891 = vadd.s32 5, %v53887
%v53893 = vxor.u32 %v53891, %v53879
%v53894 = vand.u32.u8 255, %v53893
%v53895 = vand.u32 65535, %v53894
%v53896 = vshrl.u32 %v53895, 1
%v53897 = vor.u32 16256, %v53896
%v53898 = vand.u32.u16 65535, %v53897
%v120032 = vadd.low.f32.bf16 -1.0, %v53898
%v53907 = vmul.f32 2.0, %v120032
%v53911 = vadd.f32 -0.99609375, %v53907
%v53915 = vmax.f32 %v53911, -0.99609375
%v53917 = vand.u32 2147483647, %v53915
%vm53920 = vcmp.eq.f32.partialorder %v53917, 1.0
%v53925 = vmul.f32 inf, %v53915
%v53927 = vxor.u32 2147483648, %v53915
%v53930 = vmul.f32 %v53927, %v53915
%v53932 = vadd.f32 1.0, %v53930
%v53933 = vlog2.pop %v53932
%v53934 = vmul.f32 0.6931472, %v53933
%v53935 = vmul.f32 -0.5, %v53930
%v53936 = vadd.f32 1.0, %v53935
%v53937 = vmul.f32 %v53936, %v53930
%v53938 = vand.u32 2147483647, %v53930
%vm53939 = vcmp.lt.f32.partialorder %v53938, 0.0004427343
%v53940 = vsel /*vm=*/%vm53939, /*on_true_vy=*/%v53937, /*on_false_vx=*/%v53934
%v53941 = vxor.u32 2147483648, %v53940
%vm53944 = vcmp.lt.f32.partialorder %v53941, 5.0
%v53949 = vsel /*vm=*/%vm53944, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v53953 = vsel /*vm=*/%vm53944, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v53957 = vsel /*vm=*/%vm53944, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v53961 = vsel /*vm=*/%vm53944, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v53965 = vsel /*vm=*/%vm53944, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v53969 = vsel /*vm=*/%vm53944, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v53973 = vsel /*vm=*/%vm53944, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v53977 = vsel /*vm=*/%vm53944, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v53981 = vsel /*vm=*/%vm53944, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v53985 = vadd.f32 -2.5, %v53941
%v53987 = vrsqrt.pop %v53941
%v53988 = vmul.f32 %v53987, %v53941
%vm53989 = vcmp.eq.f32.partialorder %v53941, inf
%v53990 = vsel /*vm=*/%vm53989, /*on_true_vy=*/%v53941, /*on_false_vx=*/%v53988
%vm53991 = vcmp.eq.f32.partialorder %v53941, 0.0
%v53992 = vand.u32 2147483648, %v53941
%v53993 = vsel /*vm=*/%vm53991, /*on_true_vy=*/%v53992, /*on_false_vx=*/%v53990
%v53996 = vadd.f32 -3.0, %v53993
%v54000 = vsel /*vm=*/%vm53944, /*on_true_vy=*/%v53985, /*on_false_vx=*/%v53996
%v54004 = vmul.f32 %v54000, %v53981
%v54008 = vadd.f32 %v54004, %v53977
%v54012 = vmul.f32 %v54008, %v54000
%v54016 = vadd.f32 %v54012, %v53973
%v54020 = vmul.f32 %v54016, %v54000
%v54024 = vadd.f32 %v54020, %v53969
%v54028 = vmul.f32 %v54024, %v54000
%v54032 = vadd.f32 %v54028, %v53965
%v54036 = vmul.f32 %v54032, %v54000
%v54040 = vadd.f32 %v54036, %v53961
%v54044 = vmul.f32 %v54040, %v54000
%v54048 = vadd.f32 %v54044, %v53957
%v54052 = vmul.f32 %v54048, %v54000
%v54056 = vadd.f32 %v54052, %v53953
%v54060 = vmul.f32 %v54056, %v54000
%v54064 = vadd.f32 %v54060, %v53949
%v54068 = vmul.f32 %v54064, %v53915
%v54072 = vsel /*vm=*/%vm53920, /*on_true_vy=*/%v53925, /*on_false_vx=*/%v54068
%v54076 = vmul.f32 1.4140625, %v54072
%v54079 = vpack.c.bf16 %v120417, %v54076
%120033 = vst [vmem:[%s280 + $0x138] sm:$0xf] /*vst_source=*/%v54079
%v54083 = vadd.s32 %v52697, %v1868
%v54093 = vadd.s32 %v54083, %v415
%vm54097 = vcmp.lt.u32.totalorder %v54093, %v54083
%vm54102 = vcmp.lt.u32.totalorder %v54083, %v1868
%v54107 = vadd.s32 %v52680, %v1855
%v54111 = vadd.s32 1, %v54107
%v54115 = vsel /*vm=*/%vm54102, /*on_true_vy=*/%v54111, /*on_false_vx=*/%v54107
%v54119 = vadd.s32 1, %v54115
%v54123 = vsel /*vm=*/%vm54097, /*on_true_vy=*/%v54119, /*on_false_vx=*/%v54115
%v54128 = vadd.s32 %v54123, %v10
%v54132 = vadd.s32 %v54093, %v9
%v54136 = vadd.s32 %v54132, %v54128
%v54138 = vshll.u32 %v54132, 13
%v54139 = vshrl.u32 %v54132, 19
%v54140 = vor.u32 %v54139, %v54138
%v54141 = vxor.u32 %v54140, %v54136
%v54144 = vadd.s32 %v54141, %v54136
%v54146 = vshll.u32 %v54141, 15
%v54147 = vshrl.u32 %v54141, 17
%v54148 = vor.u32 %v54147, %v54146
%v54149 = vxor.u32 %v54148, %v54144
%v54152 = vadd.s32 %v54149, %v54144
%v54154 = vshll.u32 %v54149, 26
%v54155 = vshrl.u32 %v54149, 6
%v54156 = vor.u32 %v54155, %v54154
%v54157 = vxor.u32 %v54156, %v54152
%v54160 = vadd.s32 %v54157, %v54152
%v54164 = vadd.s32 %v54160, %v9
%v54166 = vshll.u32 %v54157, 6
%v54167 = vshrl.u32 %v54157, 26
%v54168 = vor.u32 %v54167, %v54166
%v54169 = vxor.u32 %v54168, %v54160
%v54172 = vadd.s32 %v54169, %v8
%v54176 = vadd.s32 1, %v54172
%v54180 = vadd.s32 %v54176, %v54164
%v54182 = vshll.u32 %v54176, 17
%v54183 = vshrl.u32 %v54176, 15
%v54184 = vor.u32 %v54183, %v54182
%v54185 = vxor.u32 %v54184, %v54180
%v54188 = vadd.s32 %v54185, %v54180
%v54190 = vshll.u32 %v54185, 29
%v54191 = vshrl.u32 %v54185, 3
%v54192 = vor.u32 %v54191, %v54190
%v54193 = vxor.u32 %v54192, %v54188
%v54196 = vadd.s32 %v54193, %v54188
%v54198 = vshll.u32 %v54193, 16
%v54199 = vshrl.u32 %v54193, 16
%v54200 = vor.u32 %v54199, %v54198
%v54201 = vxor.u32 %v54200, %v54196
%v54204 = vadd.s32 %v54201, %v54196
%v54208 = vadd.s32 %v54204, %v8
%v54210 = vshll.u32 %v54201, 24
%v54211 = vshrl.u32 %v54201, 8
%v54212 = vor.u32 %v54211, %v54210
%v54213 = vxor.u32 %v54212, %v54204
%v54216 = vadd.s32 %v54213, %v10
%v54220 = vadd.s32 2, %v54216
%v54224 = vadd.s32 %v54220, %v54208
%v54226 = vshll.u32 %v54220, 13
%v54227 = vshrl.u32 %v54220, 19
%v54228 = vor.u32 %v54227, %v54226
%v54229 = vxor.u32 %v54228, %v54224
%v54232 = vadd.s32 %v54229, %v54224
%v54234 = vshll.u32 %v54229, 15
%v54235 = vshrl.u32 %v54229, 17
%v54236 = vor.u32 %v54235, %v54234
%v54237 = vxor.u32 %v54236, %v54232
%v54240 = vadd.s32 %v54237, %v54232
%v54242 = vshll.u32 %v54237, 26
%v54243 = vshrl.u32 %v54237, 6
%v54244 = vor.u32 %v54243, %v54242
%v54245 = vxor.u32 %v54244, %v54240
%v54248 = vadd.s32 %v54245, %v54240
%v54252 = vadd.s32 %v54248, %v10
%v54254 = vshll.u32 %v54245, 6
%v54255 = vshrl.u32 %v54245, 26
%v54256 = vor.u32 %v54255, %v54254
%v54257 = vxor.u32 %v54256, %v54248
%v54260 = vadd.s32 %v54257, %v9
%v54264 = vadd.s32 3, %v54260
%v54268 = vadd.s32 %v54264, %v54252
%v54270 = vshll.u32 %v54264, 17
%v54271 = vshrl.u32 %v54264, 15
%v54272 = vor.u32 %v54271, %v54270
%v54273 = vxor.u32 %v54272, %v54268
%v54276 = vadd.s32 %v54273, %v54268
%v54278 = vshll.u32 %v54273, 29
%v54279 = vshrl.u32 %v54273, 3
%v54280 = vor.u32 %v54279, %v54278
%v54281 = vxor.u32 %v54280, %v54276
%v54284 = vadd.s32 %v54281, %v54276
%v54286 = vshll.u32 %v54281, 16
%v54287 = vshrl.u32 %v54281, 16
%v54288 = vor.u32 %v54287, %v54286
%v54289 = vxor.u32 %v54288, %v54284
%v54292 = vadd.s32 %v54289, %v54284
%v54296 = vadd.s32 %v54292, %v9
%v54298 = vshll.u32 %v54289, 24
%v54299 = vshrl.u32 %v54289, 8
%v54300 = vor.u32 %v54299, %v54298
%v54301 = vxor.u32 %v54300, %v54292
%v54304 = vadd.s32 %v54301, %v8
%v54308 = vadd.s32 4, %v54304
%v54312 = vadd.s32 %v54308, %v54296
%v54314 = vshll.u32 %v54308, 13
%v54315 = vshrl.u32 %v54308, 19
%v54316 = vor.u32 %v54315, %v54314
%v54317 = vxor.u32 %v54316, %v54312
%v54320 = vadd.s32 %v54317, %v54312
%v54322 = vshll.u32 %v54317, 15
%v54323 = vshrl.u32 %v54317, 17
%v54324 = vor.u32 %v54323, %v54322
%v54325 = vxor.u32 %v54324, %v54320
%v54328 = vadd.s32 %v54325, %v54320
%v54330 = vshll.u32 %v54325, 26
%v54331 = vshrl.u32 %v54325, 6
%v54332 = vor.u32 %v54331, %v54330
%v54333 = vxor.u32 %v54332, %v54328
%v54336 = vadd.s32 %v54333, %v54328
%v54340 = vadd.s32 %v54336, %v8
%v54342 = vshll.u32 %v54333, 6
%v54343 = vshrl.u32 %v54333, 26
%v54344 = vor.u32 %v54343, %v54342
%v54345 = vxor.u32 %v54344, %v54336
%v54348 = vadd.s32 %v54345, %v10
%v54352 = vadd.s32 5, %v54348
%v54354 = vxor.u32 %v54352, %v54340
%v54355 = vand.u32.u8 255, %v54354
%v54356 = vand.u32 65535, %v54355
%v54357 = vshrl.u32 %v54356, 1
%v54358 = vor.u32 16256, %v54357
%v54359 = vand.u32.u16 65535, %v54358
%v120034 = vadd.low.f32.bf16 -1.0, %v54359
%v54368 = vmul.f32 2.0, %v120034
%v54372 = vadd.f32 -0.99609375, %v54368
%v54376 = vmax.f32 %v54372, -0.99609375
%v54378 = vand.u32 2147483647, %v54376
%vm54381 = vcmp.eq.f32.partialorder %v54378, 1.0
%v54386 = vmul.f32 inf, %v54376
%v54388 = vxor.u32 2147483648, %v54376
%v54391 = vmul.f32 %v54388, %v54376
%v54393 = vadd.f32 1.0, %v54391
%v54394 = vlog2.pop %v54393
%v54395 = vmul.f32 0.6931472, %v54394
%v54396 = vmul.f32 -0.5, %v54391
%v54397 = vadd.f32 1.0, %v54396
%v54398 = vmul.f32 %v54397, %v54391
%v54399 = vand.u32 2147483647, %v54391
%vm54400 = vcmp.lt.f32.partialorder %v54399, 0.0004427343
%v54401 = vsel /*vm=*/%vm54400, /*on_true_vy=*/%v54398, /*on_false_vx=*/%v54395
%v54402 = vxor.u32 2147483648, %v54401
%vm54405 = vcmp.lt.f32.partialorder %v54402, 5.0
%v54410 = vsel /*vm=*/%vm54405, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v54414 = vsel /*vm=*/%vm54405, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v54418 = vsel /*vm=*/%vm54405, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v54422 = vsel /*vm=*/%vm54405, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v54426 = vsel /*vm=*/%vm54405, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v54430 = vsel /*vm=*/%vm54405, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v54434 = vsel /*vm=*/%vm54405, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v54438 = vsel /*vm=*/%vm54405, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v54442 = vsel /*vm=*/%vm54405, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v54446 = vadd.f32 -2.5, %v54402
%v54448 = vrsqrt.pop %v54402
%v54449 = vmul.f32 %v54448, %v54402
%vm54450 = vcmp.eq.f32.partialorder %v54402, inf
%v54451 = vsel /*vm=*/%vm54450, /*on_true_vy=*/%v54402, /*on_false_vx=*/%v54449
%vm54452 = vcmp.eq.f32.partialorder %v54402, 0.0
%v54453 = vand.u32 2147483648, %v54402
%v54454 = vsel /*vm=*/%vm54452, /*on_true_vy=*/%v54453, /*on_false_vx=*/%v54451
%v54457 = vadd.f32 -3.0, %v54454
%v54461 = vsel /*vm=*/%vm54405, /*on_true_vy=*/%v54446, /*on_false_vx=*/%v54457
%v54465 = vmul.f32 %v54461, %v54442
%v54469 = vadd.f32 %v54465, %v54438
%v54473 = vmul.f32 %v54469, %v54461
%v54477 = vadd.f32 %v54473, %v54434
%v54481 = vmul.f32 %v54477, %v54461
%v54485 = vadd.f32 %v54481, %v54430
%v54489 = vmul.f32 %v54485, %v54461
%v54493 = vadd.f32 %v54489, %v54426
%v54497 = vmul.f32 %v54493, %v54461
%v54501 = vadd.f32 %v54497, %v54422
%v54505 = vmul.f32 %v54501, %v54461
%v54509 = vadd.f32 %v54505, %v54418
%v54513 = vmul.f32 %v54509, %v54461
%v54517 = vadd.f32 %v54513, %v54414
%v54521 = vmul.f32 %v54517, %v54461
%v54525 = vadd.f32 %v54521, %v54410
%v54529 = vmul.f32 %v54525, %v54376
%v54533 = vsel /*vm=*/%vm54381, /*on_true_vy=*/%v54386, /*on_false_vx=*/%v54529
%v54537 = vmul.f32 1.4140625, %v54533
%v54540 = vpack.c.bf16 %v120417, %v54537
%120035 = vst [vmem:[%s280 + $0x1b8] sm:$0xf] /*vst_source=*/%v54540
%v54544 = vadd.s32 %v52697, %v2355
%v54554 = vadd.s32 %v54544, %v415
%vm54558 = vcmp.lt.u32.totalorder %v54554, %v54544
%vm54563 = vcmp.lt.u32.totalorder %v54544, %v2355
%v54568 = vadd.s32 %v52680, %v2342
%v54572 = vadd.s32 1, %v54568
%v54576 = vsel /*vm=*/%vm54563, /*on_true_vy=*/%v54572, /*on_false_vx=*/%v54568
%v54580 = vadd.s32 1, %v54576
%v54584 = vsel /*vm=*/%vm54558, /*on_true_vy=*/%v54580, /*on_false_vx=*/%v54576
%v54589 = vadd.s32 %v54584, %v10
%v54593 = vadd.s32 %v54554, %v9
%v54597 = vadd.s32 %v54593, %v54589
%v54599 = vshll.u32 %v54593, 13
%v54600 = vshrl.u32 %v54593, 19
%v54601 = vor.u32 %v54600, %v54599
%v54602 = vxor.u32 %v54601, %v54597
%v54605 = vadd.s32 %v54602, %v54597
%v54607 = vshll.u32 %v54602, 15
%v54608 = vshrl.u32 %v54602, 17
%v54609 = vor.u32 %v54608, %v54607
%v54610 = vxor.u32 %v54609, %v54605
%v54613 = vadd.s32 %v54610, %v54605
%v54615 = vshll.u32 %v54610, 26
%v54616 = vshrl.u32 %v54610, 6
%v54617 = vor.u32 %v54616, %v54615
%v54618 = vxor.u32 %v54617, %v54613
%v54621 = vadd.s32 %v54618, %v54613
%v54625 = vadd.s32 %v54621, %v9
%v54627 = vshll.u32 %v54618, 6
%v54628 = vshrl.u32 %v54618, 26
%v54629 = vor.u32 %v54628, %v54627
%v54630 = vxor.u32 %v54629, %v54621
%v54633 = vadd.s32 %v54630, %v8
%v54637 = vadd.s32 1, %v54633
%v54641 = vadd.s32 %v54637, %v54625
%v54643 = vshll.u32 %v54637, 17
%v54644 = vshrl.u32 %v54637, 15
%v54645 = vor.u32 %v54644, %v54643
%v54646 = vxor.u32 %v54645, %v54641
%v54649 = vadd.s32 %v54646, %v54641
%v54651 = vshll.u32 %v54646, 29
%v54652 = vshrl.u32 %v54646, 3
%v54653 = vor.u32 %v54652, %v54651
%v54654 = vxor.u32 %v54653, %v54649
%v54657 = vadd.s32 %v54654, %v54649
%v54659 = vshll.u32 %v54654, 16
%v54660 = vshrl.u32 %v54654, 16
%v54661 = vor.u32 %v54660, %v54659
%v54662 = vxor.u32 %v54661, %v54657
%v54665 = vadd.s32 %v54662, %v54657
%v54669 = vadd.s32 %v54665, %v8
%v54671 = vshll.u32 %v54662, 24
%v54672 = vshrl.u32 %v54662, 8
%v54673 = vor.u32 %v54672, %v54671
%v54674 = vxor.u32 %v54673, %v54665
%v54677 = vadd.s32 %v54674, %v10
%v54681 = vadd.s32 2, %v54677
%v54685 = vadd.s32 %v54681, %v54669
%v54687 = vshll.u32 %v54681, 13
%v54688 = vshrl.u32 %v54681, 19
%v54689 = vor.u32 %v54688, %v54687
%v54690 = vxor.u32 %v54689, %v54685
%v54693 = vadd.s32 %v54690, %v54685
%v54695 = vshll.u32 %v54690, 15
%v54696 = vshrl.u32 %v54690, 17
%v54697 = vor.u32 %v54696, %v54695
%v54698 = vxor.u32 %v54697, %v54693
%v54701 = vadd.s32 %v54698, %v54693
%v54703 = vshll.u32 %v54698, 26
%v54704 = vshrl.u32 %v54698, 6
%v54705 = vor.u32 %v54704, %v54703
%v54706 = vxor.u32 %v54705, %v54701
%v54709 = vadd.s32 %v54706, %v54701
%v54713 = vadd.s32 %v54709, %v10
%v54715 = vshll.u32 %v54706, 6
%v54716 = vshrl.u32 %v54706, 26
%v54717 = vor.u32 %v54716, %v54715
%v54718 = vxor.u32 %v54717, %v54709
%v54721 = vadd.s32 %v54718, %v9
%v54725 = vadd.s32 3, %v54721
%v54729 = vadd.s32 %v54725, %v54713
%v54731 = vshll.u32 %v54725, 17
%v54732 = vshrl.u32 %v54725, 15
%v54733 = vor.u32 %v54732, %v54731
%v54734 = vxor.u32 %v54733, %v54729
%v54737 = vadd.s32 %v54734, %v54729
%v54739 = vshll.u32 %v54734, 29
%v54740 = vshrl.u32 %v54734, 3
%v54741 = vor.u32 %v54740, %v54739
%v54742 = vxor.u32 %v54741, %v54737
%v54745 = vadd.s32 %v54742, %v54737
%v54747 = vshll.u32 %v54742, 16
%v54748 = vshrl.u32 %v54742, 16
%v54749 = vor.u32 %v54748, %v54747
%v54750 = vxor.u32 %v54749, %v54745
%v54753 = vadd.s32 %v54750, %v54745
%v54757 = vadd.s32 %v54753, %v9
%v54759 = vshll.u32 %v54750, 24
%v54760 = vshrl.u32 %v54750, 8
%v54761 = vor.u32 %v54760, %v54759
%v54762 = vxor.u32 %v54761, %v54753
%v54765 = vadd.s32 %v54762, %v8
%v54769 = vadd.s32 4, %v54765
%v54773 = vadd.s32 %v54769, %v54757
%v54775 = vshll.u32 %v54769, 13
%v54776 = vshrl.u32 %v54769, 19
%v54777 = vor.u32 %v54776, %v54775
%v54778 = vxor.u32 %v54777, %v54773
%v54781 = vadd.s32 %v54778, %v54773
%v54783 = vshll.u32 %v54778, 15
%v54784 = vshrl.u32 %v54778, 17
%v54785 = vor.u32 %v54784, %v54783
%v54786 = vxor.u32 %v54785, %v54781
%v54789 = vadd.s32 %v54786, %v54781
%v54791 = vshll.u32 %v54786, 26
%v54792 = vshrl.u32 %v54786, 6
%v54793 = vor.u32 %v54792, %v54791
%v54794 = vxor.u32 %v54793, %v54789
%v54797 = vadd.s32 %v54794, %v54789
%v54801 = vadd.s32 %v54797, %v8
%v54803 = vshll.u32 %v54794, 6
%v54804 = vshrl.u32 %v54794, 26
%v54805 = vor.u32 %v54804, %v54803
%v54806 = vxor.u32 %v54805, %v54797
%v54809 = vadd.s32 %v54806, %v10
%v54813 = vadd.s32 5, %v54809
%v54815 = vxor.u32 %v54813, %v54801
%v54816 = vand.u32.u8 255, %v54815
%v54817 = vand.u32 65535, %v54816
%v54818 = vshrl.u32 %v54817, 1
%v54819 = vor.u32 16256, %v54818
%v54820 = vand.u32.u16 65535, %v54819
%v120036 = vadd.low.f32.bf16 -1.0, %v54820
%v54829 = vmul.f32 2.0, %v120036
%v54833 = vadd.f32 -0.99609375, %v54829
%v54837 = vmax.f32 %v54833, -0.99609375
%v54839 = vand.u32 2147483647, %v54837
%vm54842 = vcmp.eq.f32.partialorder %v54839, 1.0
%v54847 = vmul.f32 inf, %v54837
%v54849 = vxor.u32 2147483648, %v54837
%v54852 = vmul.f32 %v54849, %v54837
%v54854 = vadd.f32 1.0, %v54852
%v54855 = vlog2.pop %v54854
%v54856 = vmul.f32 0.6931472, %v54855
%v54857 = vmul.f32 -0.5, %v54852
%v54858 = vadd.f32 1.0, %v54857
%v54859 = vmul.f32 %v54858, %v54852
%v54860 = vand.u32 2147483647, %v54852
%vm54861 = vcmp.lt.f32.partialorder %v54860, 0.0004427343
%v54862 = vsel /*vm=*/%vm54861, /*on_true_vy=*/%v54859, /*on_false_vx=*/%v54856
%v54863 = vxor.u32 2147483648, %v54862
%vm54866 = vcmp.lt.f32.partialorder %v54863, 5.0
%v54871 = vsel /*vm=*/%vm54866, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v54875 = vsel /*vm=*/%vm54866, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v54879 = vsel /*vm=*/%vm54866, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v54883 = vsel /*vm=*/%vm54866, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v54887 = vsel /*vm=*/%vm54866, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v54891 = vsel /*vm=*/%vm54866, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v54895 = vsel /*vm=*/%vm54866, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v54899 = vsel /*vm=*/%vm54866, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v54903 = vsel /*vm=*/%vm54866, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v54907 = vadd.f32 -2.5, %v54863
%v54909 = vrsqrt.pop %v54863
%v54910 = vmul.f32 %v54909, %v54863
%vm54911 = vcmp.eq.f32.partialorder %v54863, inf
%v54912 = vsel /*vm=*/%vm54911, /*on_true_vy=*/%v54863, /*on_false_vx=*/%v54910
%vm54913 = vcmp.eq.f32.partialorder %v54863, 0.0
%v54914 = vand.u32 2147483648, %v54863
%v54915 = vsel /*vm=*/%vm54913, /*on_true_vy=*/%v54914, /*on_false_vx=*/%v54912
%v54918 = vadd.f32 -3.0, %v54915
%v54922 = vsel /*vm=*/%vm54866, /*on_true_vy=*/%v54907, /*on_false_vx=*/%v54918
%v54926 = vmul.f32 %v54922, %v54903
%v54930 = vadd.f32 %v54926, %v54899
%v54934 = vmul.f32 %v54930, %v54922
%v54938 = vadd.f32 %v54934, %v54895
%v54942 = vmul.f32 %v54938, %v54922
%v54946 = vadd.f32 %v54942, %v54891
%v54950 = vmul.f32 %v54946, %v54922
%v54954 = vadd.f32 %v54950, %v54887
%v54958 = vmul.f32 %v54954, %v54922
%v54962 = vadd.f32 %v54958, %v54883
%v54966 = vmul.f32 %v54962, %v54922
%v54970 = vadd.f32 %v54966, %v54879
%v54974 = vmul.f32 %v54970, %v54922
%v54978 = vadd.f32 %v54974, %v54875
%v54982 = vmul.f32 %v54978, %v54922
%v54986 = vadd.f32 %v54982, %v54871
%v54990 = vmul.f32 %v54986, %v54837
%v54994 = vsel /*vm=*/%vm54842, /*on_true_vy=*/%v54847, /*on_false_vx=*/%v54990
%v54998 = vmul.f32 1.4140625, %v54994
%v55001 = vpack.c.bf16 %v120417, %v54998
%120037 = vst [vmem:[%s280 + $0x238] sm:$0xf] /*vst_source=*/%v55001
%v55005 = vadd.s32 %v52697, %v2842
%v55015 = vadd.s32 %v55005, %v415
%vm55019 = vcmp.lt.u32.totalorder %v55015, %v55005
%vm55024 = vcmp.lt.u32.totalorder %v55005, %v2842
%v55029 = vadd.s32 %v52680, %v2829
%v55033 = vadd.s32 1, %v55029
%v55037 = vsel /*vm=*/%vm55024, /*on_true_vy=*/%v55033, /*on_false_vx=*/%v55029
%v55041 = vadd.s32 1, %v55037
%v55045 = vsel /*vm=*/%vm55019, /*on_true_vy=*/%v55041, /*on_false_vx=*/%v55037
%v55050 = vadd.s32 %v55045, %v10
%v55054 = vadd.s32 %v55015, %v9
%v55058 = vadd.s32 %v55054, %v55050
%v55060 = vshll.u32 %v55054, 13
%v55061 = vshrl.u32 %v55054, 19
%v55062 = vor.u32 %v55061, %v55060
%v55063 = vxor.u32 %v55062, %v55058
%v55066 = vadd.s32 %v55063, %v55058
%v55068 = vshll.u32 %v55063, 15
%v55069 = vshrl.u32 %v55063, 17
%v55070 = vor.u32 %v55069, %v55068
%v55071 = vxor.u32 %v55070, %v55066
%v55074 = vadd.s32 %v55071, %v55066
%v55076 = vshll.u32 %v55071, 26
%v55077 = vshrl.u32 %v55071, 6
%v55078 = vor.u32 %v55077, %v55076
%v55079 = vxor.u32 %v55078, %v55074
%v55082 = vadd.s32 %v55079, %v55074
%v55086 = vadd.s32 %v55082, %v9
%v55088 = vshll.u32 %v55079, 6
%v55089 = vshrl.u32 %v55079, 26
%v55090 = vor.u32 %v55089, %v55088
%v55091 = vxor.u32 %v55090, %v55082
%v55094 = vadd.s32 %v55091, %v8
%v55098 = vadd.s32 1, %v55094
%v55102 = vadd.s32 %v55098, %v55086
%v55104 = vshll.u32 %v55098, 17
%v55105 = vshrl.u32 %v55098, 15
%v55106 = vor.u32 %v55105, %v55104
%v55107 = vxor.u32 %v55106, %v55102
%v55110 = vadd.s32 %v55107, %v55102
%v55112 = vshll.u32 %v55107, 29
%v55113 = vshrl.u32 %v55107, 3
%v55114 = vor.u32 %v55113, %v55112
%v55115 = vxor.u32 %v55114, %v55110
%v55118 = vadd.s32 %v55115, %v55110
%v55120 = vshll.u32 %v55115, 16
%v55121 = vshrl.u32 %v55115, 16
%v55122 = vor.u32 %v55121, %v55120
%v55123 = vxor.u32 %v55122, %v55118
%v55126 = vadd.s32 %v55123, %v55118
%v55130 = vadd.s32 %v55126, %v8
%v55132 = vshll.u32 %v55123, 24
%v55133 = vshrl.u32 %v55123, 8
%v55134 = vor.u32 %v55133, %v55132
%v55135 = vxor.u32 %v55134, %v55126
%v55138 = vadd.s32 %v55135, %v10
%v55142 = vadd.s32 2, %v55138
%v55146 = vadd.s32 %v55142, %v55130
%v55148 = vshll.u32 %v55142, 13
%v55149 = vshrl.u32 %v55142, 19
%v55150 = vor.u32 %v55149, %v55148
%v55151 = vxor.u32 %v55150, %v55146
%v55154 = vadd.s32 %v55151, %v55146
%v55156 = vshll.u32 %v55151, 15
%v55157 = vshrl.u32 %v55151, 17
%v55158 = vor.u32 %v55157, %v55156
%v55159 = vxor.u32 %v55158, %v55154
%v55162 = vadd.s32 %v55159, %v55154
%v55164 = vshll.u32 %v55159, 26
%v55165 = vshrl.u32 %v55159, 6
%v55166 = vor.u32 %v55165, %v55164
%v55167 = vxor.u32 %v55166, %v55162
%v55170 = vadd.s32 %v55167, %v55162
%v55174 = vadd.s32 %v55170, %v10
%v55176 = vshll.u32 %v55167, 6
%v55177 = vshrl.u32 %v55167, 26
%v55178 = vor.u32 %v55177, %v55176
%v55179 = vxor.u32 %v55178, %v55170
%v55182 = vadd.s32 %v55179, %v9
%v55186 = vadd.s32 3, %v55182
%v55190 = vadd.s32 %v55186, %v55174
%v55192 = vshll.u32 %v55186, 17
%v55193 = vshrl.u32 %v55186, 15
%v55194 = vor.u32 %v55193, %v55192
%v55195 = vxor.u32 %v55194, %v55190
%v55198 = vadd.s32 %v55195, %v55190
%v55200 = vshll.u32 %v55195, 29
%v55201 = vshrl.u32 %v55195, 3
%v55202 = vor.u32 %v55201, %v55200
%v55203 = vxor.u32 %v55202, %v55198
%v55206 = vadd.s32 %v55203, %v55198
%v55208 = vshll.u32 %v55203, 16
%v55209 = vshrl.u32 %v55203, 16
%v55210 = vor.u32 %v55209, %v55208
%v55211 = vxor.u32 %v55210, %v55206
%v55214 = vadd.s32 %v55211, %v55206
%v55218 = vadd.s32 %v55214, %v9
%v55220 = vshll.u32 %v55211, 24
%v55221 = vshrl.u32 %v55211, 8
%v55222 = vor.u32 %v55221, %v55220
%v55223 = vxor.u32 %v55222, %v55214
%v55226 = vadd.s32 %v55223, %v8
%v55230 = vadd.s32 4, %v55226
%v55234 = vadd.s32 %v55230, %v55218
%v55236 = vshll.u32 %v55230, 13
%v55237 = vshrl.u32 %v55230, 19
%v55238 = vor.u32 %v55237, %v55236
%v55239 = vxor.u32 %v55238, %v55234
%v55242 = vadd.s32 %v55239, %v55234
%v55244 = vshll.u32 %v55239, 15
%v55245 = vshrl.u32 %v55239, 17
%v55246 = vor.u32 %v55245, %v55244
%v55247 = vxor.u32 %v55246, %v55242
%v55250 = vadd.s32 %v55247, %v55242
%v55252 = vshll.u32 %v55247, 26
%v55253 = vshrl.u32 %v55247, 6
%v55254 = vor.u32 %v55253, %v55252
%v55255 = vxor.u32 %v55254, %v55250
%v55258 = vadd.s32 %v55255, %v55250
%v55262 = vadd.s32 %v55258, %v8
%v55264 = vshll.u32 %v55255, 6
%v55265 = vshrl.u32 %v55255, 26
%v55266 = vor.u32 %v55265, %v55264
%v55267 = vxor.u32 %v55266, %v55258
%v55270 = vadd.s32 %v55267, %v10
%v55274 = vadd.s32 5, %v55270
%v55276 = vxor.u32 %v55274, %v55262
%v55277 = vand.u32.u8 255, %v55276
%v55278 = vand.u32 65535, %v55277
%v55279 = vshrl.u32 %v55278, 1
%v55280 = vor.u32 16256, %v55279
%v55281 = vand.u32.u16 65535, %v55280
%v120038 = vadd.low.f32.bf16 -1.0, %v55281
%v55290 = vmul.f32 2.0, %v120038
%v55294 = vadd.f32 -0.99609375, %v55290
%v55298 = vmax.f32 %v55294, -0.99609375
%v55300 = vand.u32 2147483647, %v55298
%vm55303 = vcmp.eq.f32.partialorder %v55300, 1.0
%v55308 = vmul.f32 inf, %v55298
%v55310 = vxor.u32 2147483648, %v55298
%v55313 = vmul.f32 %v55310, %v55298
%v55315 = vadd.f32 1.0, %v55313
%v55316 = vlog2.pop %v55315
%v55317 = vmul.f32 0.6931472, %v55316
%v55318 = vmul.f32 -0.5, %v55313
%v55319 = vadd.f32 1.0, %v55318
%v55320 = vmul.f32 %v55319, %v55313
%v55321 = vand.u32 2147483647, %v55313
%vm55322 = vcmp.lt.f32.partialorder %v55321, 0.0004427343
%v55323 = vsel /*vm=*/%vm55322, /*on_true_vy=*/%v55320, /*on_false_vx=*/%v55317
%v55324 = vxor.u32 2147483648, %v55323
%vm55327 = vcmp.lt.f32.partialorder %v55324, 5.0
%v55332 = vsel /*vm=*/%vm55327, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v55336 = vsel /*vm=*/%vm55327, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v55340 = vsel /*vm=*/%vm55327, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v55344 = vsel /*vm=*/%vm55327, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v55348 = vsel /*vm=*/%vm55327, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v55352 = vsel /*vm=*/%vm55327, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v55356 = vsel /*vm=*/%vm55327, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v55360 = vsel /*vm=*/%vm55327, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v55364 = vsel /*vm=*/%vm55327, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v55368 = vadd.f32 -2.5, %v55324
%v55370 = vrsqrt.pop %v55324
%v55371 = vmul.f32 %v55370, %v55324
%vm55372 = vcmp.eq.f32.partialorder %v55324, inf
%v55373 = vsel /*vm=*/%vm55372, /*on_true_vy=*/%v55324, /*on_false_vx=*/%v55371
%vm55374 = vcmp.eq.f32.partialorder %v55324, 0.0
%v55375 = vand.u32 2147483648, %v55324
%v55376 = vsel /*vm=*/%vm55374, /*on_true_vy=*/%v55375, /*on_false_vx=*/%v55373
%v55379 = vadd.f32 -3.0, %v55376
%v55383 = vsel /*vm=*/%vm55327, /*on_true_vy=*/%v55368, /*on_false_vx=*/%v55379
%v55387 = vmul.f32 %v55383, %v55364
%v55391 = vadd.f32 %v55387, %v55360
%v55395 = vmul.f32 %v55391, %v55383
%v55399 = vadd.f32 %v55395, %v55356
%v55403 = vmul.f32 %v55399, %v55383
%v55407 = vadd.f32 %v55403, %v55352
%v55411 = vmul.f32 %v55407, %v55383
%v55415 = vadd.f32 %v55411, %v55348
%v55419 = vmul.f32 %v55415, %v55383
%v55423 = vadd.f32 %v55419, %v55344
%v55427 = vmul.f32 %v55423, %v55383
%v55431 = vadd.f32 %v55427, %v55340
%v55435 = vmul.f32 %v55431, %v55383
%v55439 = vadd.f32 %v55435, %v55336
%v55443 = vmul.f32 %v55439, %v55383
%v55447 = vadd.f32 %v55443, %v55332
%v55451 = vmul.f32 %v55447, %v55298
%v55455 = vsel /*vm=*/%vm55303, /*on_true_vy=*/%v55308, /*on_false_vx=*/%v55451
%v55459 = vmul.f32 1.4140625, %v55455
%v55462 = vpack.c.bf16 %v120417, %v55459
%120039 = vst [vmem:[%s280 + $0x2b8] sm:$0xf] /*vst_source=*/%v55462
%v55466 = vadd.s32 %v52697, %v3329
%v55476 = vadd.s32 %v55466, %v415
%vm55480 = vcmp.lt.u32.totalorder %v55476, %v55466
%vm55485 = vcmp.lt.u32.totalorder %v55466, %v3329
%v55490 = vadd.s32 %v52680, %v3316
%v55494 = vadd.s32 1, %v55490
%v55498 = vsel /*vm=*/%vm55485, /*on_true_vy=*/%v55494, /*on_false_vx=*/%v55490
%v55502 = vadd.s32 1, %v55498
%v55506 = vsel /*vm=*/%vm55480, /*on_true_vy=*/%v55502, /*on_false_vx=*/%v55498
%v55511 = vadd.s32 %v55506, %v10
%v55515 = vadd.s32 %v55476, %v9
%v55519 = vadd.s32 %v55515, %v55511
%v55521 = vshll.u32 %v55515, 13
%v55522 = vshrl.u32 %v55515, 19
%v55523 = vor.u32 %v55522, %v55521
%v55524 = vxor.u32 %v55523, %v55519
%v55527 = vadd.s32 %v55524, %v55519
%v55529 = vshll.u32 %v55524, 15
%v55530 = vshrl.u32 %v55524, 17
%v55531 = vor.u32 %v55530, %v55529
%v55532 = vxor.u32 %v55531, %v55527
%v55535 = vadd.s32 %v55532, %v55527
%v55537 = vshll.u32 %v55532, 26
%v55538 = vshrl.u32 %v55532, 6
%v55539 = vor.u32 %v55538, %v55537
%v55540 = vxor.u32 %v55539, %v55535
%v55543 = vadd.s32 %v55540, %v55535
%v55547 = vadd.s32 %v55543, %v9
%v55549 = vshll.u32 %v55540, 6
%v55550 = vshrl.u32 %v55540, 26
%v55551 = vor.u32 %v55550, %v55549
%v55552 = vxor.u32 %v55551, %v55543
%v55555 = vadd.s32 %v55552, %v8
%v55559 = vadd.s32 1, %v55555
%v55563 = vadd.s32 %v55559, %v55547
%v55565 = vshll.u32 %v55559, 17
%v55566 = vshrl.u32 %v55559, 15
%v55567 = vor.u32 %v55566, %v55565
%v55568 = vxor.u32 %v55567, %v55563
%v55571 = vadd.s32 %v55568, %v55563
%v55573 = vshll.u32 %v55568, 29
%v55574 = vshrl.u32 %v55568, 3
%v55575 = vor.u32 %v55574, %v55573
%v55576 = vxor.u32 %v55575, %v55571
%v55579 = vadd.s32 %v55576, %v55571
%v55581 = vshll.u32 %v55576, 16
%v55582 = vshrl.u32 %v55576, 16
%v55583 = vor.u32 %v55582, %v55581
%v55584 = vxor.u32 %v55583, %v55579
%v55587 = vadd.s32 %v55584, %v55579
%v55591 = vadd.s32 %v55587, %v8
%v55593 = vshll.u32 %v55584, 24
%v55594 = vshrl.u32 %v55584, 8
%v55595 = vor.u32 %v55594, %v55593
%v55596 = vxor.u32 %v55595, %v55587
%v55599 = vadd.s32 %v55596, %v10
%v55603 = vadd.s32 2, %v55599
%v55607 = vadd.s32 %v55603, %v55591
%v55609 = vshll.u32 %v55603, 13
%v55610 = vshrl.u32 %v55603, 19
%v55611 = vor.u32 %v55610, %v55609
%v55612 = vxor.u32 %v55611, %v55607
%v55615 = vadd.s32 %v55612, %v55607
%v55617 = vshll.u32 %v55612, 15
%v55618 = vshrl.u32 %v55612, 17
%v55619 = vor.u32 %v55618, %v55617
%v55620 = vxor.u32 %v55619, %v55615
%v55623 = vadd.s32 %v55620, %v55615
%v55625 = vshll.u32 %v55620, 26
%v55626 = vshrl.u32 %v55620, 6
%v55627 = vor.u32 %v55626, %v55625
%v55628 = vxor.u32 %v55627, %v55623
%v55631 = vadd.s32 %v55628, %v55623
%v55635 = vadd.s32 %v55631, %v10
%v55637 = vshll.u32 %v55628, 6
%v55638 = vshrl.u32 %v55628, 26
%v55639 = vor.u32 %v55638, %v55637
%v55640 = vxor.u32 %v55639, %v55631
%v55643 = vadd.s32 %v55640, %v9
%v55647 = vadd.s32 3, %v55643
%v55651 = vadd.s32 %v55647, %v55635
%v55653 = vshll.u32 %v55647, 17
%v55654 = vshrl.u32 %v55647, 15
%v55655 = vor.u32 %v55654, %v55653
%v55656 = vxor.u32 %v55655, %v55651
%v55659 = vadd.s32 %v55656, %v55651
%v55661 = vshll.u32 %v55656, 29
%v55662 = vshrl.u32 %v55656, 3
%v55663 = vor.u32 %v55662, %v55661
%v55664 = vxor.u32 %v55663, %v55659
%v55667 = vadd.s32 %v55664, %v55659
%v55669 = vshll.u32 %v55664, 16
%v55670 = vshrl.u32 %v55664, 16
%v55671 = vor.u32 %v55670, %v55669
%v55672 = vxor.u32 %v55671, %v55667
%v55675 = vadd.s32 %v55672, %v55667
%v55679 = vadd.s32 %v55675, %v9
%v55681 = vshll.u32 %v55672, 24
%v55682 = vshrl.u32 %v55672, 8
%v55683 = vor.u32 %v55682, %v55681
%v55684 = vxor.u32 %v55683, %v55675
%v55687 = vadd.s32 %v55684, %v8
%v55691 = vadd.s32 4, %v55687
%v55695 = vadd.s32 %v55691, %v55679
%v55697 = vshll.u32 %v55691, 13
%v55698 = vshrl.u32 %v55691, 19
%v55699 = vor.u32 %v55698, %v55697
%v55700 = vxor.u32 %v55699, %v55695
%v55703 = vadd.s32 %v55700, %v55695
%v55705 = vshll.u32 %v55700, 15
%v55706 = vshrl.u32 %v55700, 17
%v55707 = vor.u32 %v55706, %v55705
%v55708 = vxor.u32 %v55707, %v55703
%v55711 = vadd.s32 %v55708, %v55703
%v55713 = vshll.u32 %v55708, 26
%v55714 = vshrl.u32 %v55708, 6
%v55715 = vor.u32 %v55714, %v55713
%v55716 = vxor.u32 %v55715, %v55711
%v55719 = vadd.s32 %v55716, %v55711
%v55723 = vadd.s32 %v55719, %v8
%v55725 = vshll.u32 %v55716, 6
%v55726 = vshrl.u32 %v55716, 26
%v55727 = vor.u32 %v55726, %v55725
%v55728 = vxor.u32 %v55727, %v55719
%v55731 = vadd.s32 %v55728, %v10
%v55735 = vadd.s32 5, %v55731
%v55737 = vxor.u32 %v55735, %v55723
%v55738 = vand.u32.u8 255, %v55737
%v55739 = vand.u32 65535, %v55738
%v55740 = vshrl.u32 %v55739, 1
%v55741 = vor.u32 16256, %v55740
%v55742 = vand.u32.u16 65535, %v55741
%v120040 = vadd.low.f32.bf16 -1.0, %v55742
%v55751 = vmul.f32 2.0, %v120040
%v55755 = vadd.f32 -0.99609375, %v55751
%v55759 = vmax.f32 %v55755, -0.99609375
%v55761 = vand.u32 2147483647, %v55759
%vm55764 = vcmp.eq.f32.partialorder %v55761, 1.0
%v55769 = vmul.f32 inf, %v55759
%v55771 = vxor.u32 2147483648, %v55759
%v55774 = vmul.f32 %v55771, %v55759
%v55776 = vadd.f32 1.0, %v55774
%v55777 = vlog2.pop %v55776
%v55778 = vmul.f32 0.6931472, %v55777
%v55779 = vmul.f32 -0.5, %v55774
%v55780 = vadd.f32 1.0, %v55779
%v55781 = vmul.f32 %v55780, %v55774
%v55782 = vand.u32 2147483647, %v55774
%vm55783 = vcmp.lt.f32.partialorder %v55782, 0.0004427343
%v55784 = vsel /*vm=*/%vm55783, /*on_true_vy=*/%v55781, /*on_false_vx=*/%v55778
%v55785 = vxor.u32 2147483648, %v55784
%vm55788 = vcmp.lt.f32.partialorder %v55785, 5.0
%v55793 = vsel /*vm=*/%vm55788, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v55797 = vsel /*vm=*/%vm55788, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v55801 = vsel /*vm=*/%vm55788, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v55805 = vsel /*vm=*/%vm55788, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v55809 = vsel /*vm=*/%vm55788, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v55813 = vsel /*vm=*/%vm55788, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v55817 = vsel /*vm=*/%vm55788, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v55821 = vsel /*vm=*/%vm55788, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v55825 = vsel /*vm=*/%vm55788, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v55829 = vadd.f32 -2.5, %v55785
%v55831 = vrsqrt.pop %v55785
%v55832 = vmul.f32 %v55831, %v55785
%vm55833 = vcmp.eq.f32.partialorder %v55785, inf
%v55834 = vsel /*vm=*/%vm55833, /*on_true_vy=*/%v55785, /*on_false_vx=*/%v55832
%vm55835 = vcmp.eq.f32.partialorder %v55785, 0.0
%v55836 = vand.u32 2147483648, %v55785
%v55837 = vsel /*vm=*/%vm55835, /*on_true_vy=*/%v55836, /*on_false_vx=*/%v55834
%v55840 = vadd.f32 -3.0, %v55837
%v55844 = vsel /*vm=*/%vm55788, /*on_true_vy=*/%v55829, /*on_false_vx=*/%v55840
%v55848 = vmul.f32 %v55844, %v55825
%v55852 = vadd.f32 %v55848, %v55821
%v55856 = vmul.f32 %v55852, %v55844
%v55860 = vadd.f32 %v55856, %v55817
%v55864 = vmul.f32 %v55860, %v55844
%v55868 = vadd.f32 %v55864, %v55813
%v55872 = vmul.f32 %v55868, %v55844
%v55876 = vadd.f32 %v55872, %v55809
%v55880 = vmul.f32 %v55876, %v55844
%v55884 = vadd.f32 %v55880, %v55805
%v55888 = vmul.f32 %v55884, %v55844
%v55892 = vadd.f32 %v55888, %v55801
%v55896 = vmul.f32 %v55892, %v55844
%v55900 = vadd.f32 %v55896, %v55797
%v55904 = vmul.f32 %v55900, %v55844
%v55908 = vadd.f32 %v55904, %v55793
%v55912 = vmul.f32 %v55908, %v55759
%v55916 = vsel /*vm=*/%vm55764, /*on_true_vy=*/%v55769, /*on_false_vx=*/%v55912
%v55920 = vmul.f32 1.4140625, %v55916
%v55923 = vpack.c.bf16 %v120417, %v55920
%120041 = vst [vmem:[%s280 + $0x338] sm:$0xf] /*vst_source=*/%v55923
%v55927 = vadd.s32 %v52697, %v3816
%v55937 = vadd.s32 %v55927, %v415
%vm55941 = vcmp.lt.u32.totalorder %v55937, %v55927
%vm55946 = vcmp.lt.u32.totalorder %v55927, %v3816
%v55951 = vadd.s32 %v52680, %v3803
%v55955 = vadd.s32 1, %v55951
%v55959 = vsel /*vm=*/%vm55946, /*on_true_vy=*/%v55955, /*on_false_vx=*/%v55951
%v55963 = vadd.s32 1, %v55959
%v55967 = vsel /*vm=*/%vm55941, /*on_true_vy=*/%v55963, /*on_false_vx=*/%v55959
%v55972 = vadd.s32 %v55967, %v10
%v55976 = vadd.s32 %v55937, %v9
%v55980 = vadd.s32 %v55976, %v55972
%v55982 = vshll.u32 %v55976, 13
%v55983 = vshrl.u32 %v55976, 19
%v55984 = vor.u32 %v55983, %v55982
%v55985 = vxor.u32 %v55984, %v55980
%v55988 = vadd.s32 %v55985, %v55980
%v55990 = vshll.u32 %v55985, 15
%v55991 = vshrl.u32 %v55985, 17
%v55992 = vor.u32 %v55991, %v55990
%v55993 = vxor.u32 %v55992, %v55988
%v55996 = vadd.s32 %v55993, %v55988
%v55998 = vshll.u32 %v55993, 26
%v55999 = vshrl.u32 %v55993, 6
%v56000 = vor.u32 %v55999, %v55998
%v56001 = vxor.u32 %v56000, %v55996
%v56004 = vadd.s32 %v56001, %v55996
%v56008 = vadd.s32 %v56004, %v9
%v56010 = vshll.u32 %v56001, 6
%v56011 = vshrl.u32 %v56001, 26
%v56012 = vor.u32 %v56011, %v56010
%v56013 = vxor.u32 %v56012, %v56004
%v56016 = vadd.s32 %v56013, %v8
%v56020 = vadd.s32 1, %v56016
%v56024 = vadd.s32 %v56020, %v56008
%v56026 = vshll.u32 %v56020, 17
%v56027 = vshrl.u32 %v56020, 15
%v56028 = vor.u32 %v56027, %v56026
%v56029 = vxor.u32 %v56028, %v56024
%v56032 = vadd.s32 %v56029, %v56024
%v56034 = vshll.u32 %v56029, 29
%v56035 = vshrl.u32 %v56029, 3
%v56036 = vor.u32 %v56035, %v56034
%v56037 = vxor.u32 %v56036, %v56032
%v56040 = vadd.s32 %v56037, %v56032
%v56042 = vshll.u32 %v56037, 16
%v56043 = vshrl.u32 %v56037, 16
%v56044 = vor.u32 %v56043, %v56042
%v56045 = vxor.u32 %v56044, %v56040
%v56048 = vadd.s32 %v56045, %v56040
%v56052 = vadd.s32 %v56048, %v8
%v56054 = vshll.u32 %v56045, 24
%v56055 = vshrl.u32 %v56045, 8
%v56056 = vor.u32 %v56055, %v56054
%v56057 = vxor.u32 %v56056, %v56048
%v56060 = vadd.s32 %v56057, %v10
%v56064 = vadd.s32 2, %v56060
%v56068 = vadd.s32 %v56064, %v56052
%v56070 = vshll.u32 %v56064, 13
%v56071 = vshrl.u32 %v56064, 19
%v56072 = vor.u32 %v56071, %v56070
%v56073 = vxor.u32 %v56072, %v56068
%v56076 = vadd.s32 %v56073, %v56068
%v56078 = vshll.u32 %v56073, 15
%v56079 = vshrl.u32 %v56073, 17
%v56080 = vor.u32 %v56079, %v56078
%v56081 = vxor.u32 %v56080, %v56076
%v56084 = vadd.s32 %v56081, %v56076
%v56086 = vshll.u32 %v56081, 26
%v56087 = vshrl.u32 %v56081, 6
%v56088 = vor.u32 %v56087, %v56086
%v56089 = vxor.u32 %v56088, %v56084
%v56092 = vadd.s32 %v56089, %v56084
%v56096 = vadd.s32 %v56092, %v10
%v56098 = vshll.u32 %v56089, 6
%v56099 = vshrl.u32 %v56089, 26
%v56100 = vor.u32 %v56099, %v56098
%v56101 = vxor.u32 %v56100, %v56092
%v56104 = vadd.s32 %v56101, %v9
%v56108 = vadd.s32 3, %v56104
%v56112 = vadd.s32 %v56108, %v56096
%v56114 = vshll.u32 %v56108, 17
%v56115 = vshrl.u32 %v56108, 15
%v56116 = vor.u32 %v56115, %v56114
%v56117 = vxor.u32 %v56116, %v56112
%v56120 = vadd.s32 %v56117, %v56112
%v56122 = vshll.u32 %v56117, 29
%v56123 = vshrl.u32 %v56117, 3
%v56124 = vor.u32 %v56123, %v56122
%v56125 = vxor.u32 %v56124, %v56120
%v56128 = vadd.s32 %v56125, %v56120
%v56130 = vshll.u32 %v56125, 16
%v56131 = vshrl.u32 %v56125, 16
%v56132 = vor.u32 %v56131, %v56130
%v56133 = vxor.u32 %v56132, %v56128
%v56136 = vadd.s32 %v56133, %v56128
%v56140 = vadd.s32 %v56136, %v9
%v56142 = vshll.u32 %v56133, 24
%v56143 = vshrl.u32 %v56133, 8
%v56144 = vor.u32 %v56143, %v56142
%v56145 = vxor.u32 %v56144, %v56136
%v56148 = vadd.s32 %v56145, %v8
%v56152 = vadd.s32 4, %v56148
%v56156 = vadd.s32 %v56152, %v56140
%v56158 = vshll.u32 %v56152, 13
%v56159 = vshrl.u32 %v56152, 19
%v56160 = vor.u32 %v56159, %v56158
%v56161 = vxor.u32 %v56160, %v56156
%v56164 = vadd.s32 %v56161, %v56156
%v56166 = vshll.u32 %v56161, 15
%v56167 = vshrl.u32 %v56161, 17
%v56168 = vor.u32 %v56167, %v56166
%v56169 = vxor.u32 %v56168, %v56164
%v56172 = vadd.s32 %v56169, %v56164
%v56174 = vshll.u32 %v56169, 26
%v56175 = vshrl.u32 %v56169, 6
%v56176 = vor.u32 %v56175, %v56174
%v56177 = vxor.u32 %v56176, %v56172
%v56180 = vadd.s32 %v56177, %v56172
%v56184 = vadd.s32 %v56180, %v8
%v56186 = vshll.u32 %v56177, 6
%v56187 = vshrl.u32 %v56177, 26
%v56188 = vor.u32 %v56187, %v56186
%v56189 = vxor.u32 %v56188, %v56180
%v56192 = vadd.s32 %v56189, %v10
%v56196 = vadd.s32 5, %v56192
%v56198 = vxor.u32 %v56196, %v56184
%v56199 = vand.u32.u8 255, %v56198
%v56200 = vand.u32 65535, %v56199
%v56201 = vshrl.u32 %v56200, 1
%v56202 = vor.u32 16256, %v56201
%v56203 = vand.u32.u16 65535, %v56202
%v120042 = vadd.low.f32.bf16 -1.0, %v56203
%v56212 = vmul.f32 2.0, %v120042
%v56216 = vadd.f32 -0.99609375, %v56212
%v56220 = vmax.f32 %v56216, -0.99609375
%v56222 = vand.u32 2147483647, %v56220
%vm56225 = vcmp.eq.f32.partialorder %v56222, 1.0
%v56230 = vmul.f32 inf, %v56220
%v56232 = vxor.u32 2147483648, %v56220
%v56235 = vmul.f32 %v56232, %v56220
%v56237 = vadd.f32 1.0, %v56235
%v56238 = vlog2.pop %v56237
%v56239 = vmul.f32 0.6931472, %v56238
%v56240 = vmul.f32 -0.5, %v56235
%v56241 = vadd.f32 1.0, %v56240
%v56242 = vmul.f32 %v56241, %v56235
%v56243 = vand.u32 2147483647, %v56235
%vm56244 = vcmp.lt.f32.partialorder %v56243, 0.0004427343
%v56245 = vsel /*vm=*/%vm56244, /*on_true_vy=*/%v56242, /*on_false_vx=*/%v56239
%v56246 = vxor.u32 2147483648, %v56245
%vm56249 = vcmp.lt.f32.partialorder %v56246, 5.0
%v56254 = vsel /*vm=*/%vm56249, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v56258 = vsel /*vm=*/%vm56249, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v56262 = vsel /*vm=*/%vm56249, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v56266 = vsel /*vm=*/%vm56249, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v56270 = vsel /*vm=*/%vm56249, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v56274 = vsel /*vm=*/%vm56249, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v56278 = vsel /*vm=*/%vm56249, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v56282 = vsel /*vm=*/%vm56249, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v56286 = vsel /*vm=*/%vm56249, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v56290 = vadd.f32 -2.5, %v56246
%v56292 = vrsqrt.pop %v56246
%v56293 = vmul.f32 %v56292, %v56246
%vm56294 = vcmp.eq.f32.partialorder %v56246, inf
%v56295 = vsel /*vm=*/%vm56294, /*on_true_vy=*/%v56246, /*on_false_vx=*/%v56293
%vm56296 = vcmp.eq.f32.partialorder %v56246, 0.0
%v56297 = vand.u32 2147483648, %v56246
%v56298 = vsel /*vm=*/%vm56296, /*on_true_vy=*/%v56297, /*on_false_vx=*/%v56295
%v56301 = vadd.f32 -3.0, %v56298
%v56305 = vsel /*vm=*/%vm56249, /*on_true_vy=*/%v56290, /*on_false_vx=*/%v56301
%v56309 = vmul.f32 %v56305, %v56286
%v56313 = vadd.f32 %v56309, %v56282
%v56317 = vmul.f32 %v56313, %v56305
%v56321 = vadd.f32 %v56317, %v56278
%v56325 = vmul.f32 %v56321, %v56305
%v56329 = vadd.f32 %v56325, %v56274
%v56333 = vmul.f32 %v56329, %v56305
%v56337 = vadd.f32 %v56333, %v56270
%v56341 = vmul.f32 %v56337, %v56305
%v56345 = vadd.f32 %v56341, %v56266
%v56349 = vmul.f32 %v56345, %v56305
%v56353 = vadd.f32 %v56349, %v56262
%v56357 = vmul.f32 %v56353, %v56305
%v56361 = vadd.f32 %v56357, %v56258
%v56365 = vmul.f32 %v56361, %v56305
%v56369 = vadd.f32 %v56365, %v56254
%v56373 = vmul.f32 %v56369, %v56220
%v56377 = vsel /*vm=*/%vm56225, /*on_true_vy=*/%v56230, /*on_false_vx=*/%v56373
%v56381 = vmul.f32 1.4140625, %v56377
%v56384 = vpack.c.bf16 %v120417, %v56381
%120043 = vst [vmem:[%s280 + $0x3b8] sm:$0xf] /*vst_source=*/%v56384
%v56422 = vadd.s32 %v56419, %v408
%v56432 = vadd.s32 %v56422, %v415
%vm56436 = vcmp.lt.u32.totalorder %v56432, %v56422
%vm56441 = vcmp.lt.u32.totalorder %v56422, %v408
%v56446 = vadd.s32 %v56402, %v380
%v56450 = vadd.s32 1, %v56446
%v56454 = vsel /*vm=*/%vm56441, /*on_true_vy=*/%v56450, /*on_false_vx=*/%v56446
%v56458 = vadd.s32 1, %v56454
%v56462 = vsel /*vm=*/%vm56436, /*on_true_vy=*/%v56458, /*on_false_vx=*/%v56454
%v56467 = vadd.s32 %v56462, %v10
%v56471 = vadd.s32 %v56432, %v9
%v56475 = vadd.s32 %v56471, %v56467
%v56477 = vshll.u32 %v56471, 13
%v56478 = vshrl.u32 %v56471, 19
%v56479 = vor.u32 %v56478, %v56477
%v56480 = vxor.u32 %v56479, %v56475
%v56483 = vadd.s32 %v56480, %v56475
%v56485 = vshll.u32 %v56480, 15
%v56486 = vshrl.u32 %v56480, 17
%v56487 = vor.u32 %v56486, %v56485
%v56488 = vxor.u32 %v56487, %v56483
%v56491 = vadd.s32 %v56488, %v56483
%v56493 = vshll.u32 %v56488, 26
%v56494 = vshrl.u32 %v56488, 6
%v56495 = vor.u32 %v56494, %v56493
%v56496 = vxor.u32 %v56495, %v56491
%v56499 = vadd.s32 %v56496, %v56491
%v56503 = vadd.s32 %v56499, %v9
%v56505 = vshll.u32 %v56496, 6
%v56506 = vshrl.u32 %v56496, 26
%v56507 = vor.u32 %v56506, %v56505
%v56508 = vxor.u32 %v56507, %v56499
%v56511 = vadd.s32 %v56508, %v8
%v56515 = vadd.s32 1, %v56511
%v56519 = vadd.s32 %v56515, %v56503
%v56521 = vshll.u32 %v56515, 17
%v56522 = vshrl.u32 %v56515, 15
%v56523 = vor.u32 %v56522, %v56521
%v56524 = vxor.u32 %v56523, %v56519
%v56527 = vadd.s32 %v56524, %v56519
%v56529 = vshll.u32 %v56524, 29
%v56530 = vshrl.u32 %v56524, 3
%v56531 = vor.u32 %v56530, %v56529
%v56532 = vxor.u32 %v56531, %v56527
%v56535 = vadd.s32 %v56532, %v56527
%v56537 = vshll.u32 %v56532, 16
%v56538 = vshrl.u32 %v56532, 16
%v56539 = vor.u32 %v56538, %v56537
%v56540 = vxor.u32 %v56539, %v56535
%v56543 = vadd.s32 %v56540, %v56535
%v56547 = vadd.s32 %v56543, %v8
%v56549 = vshll.u32 %v56540, 24
%v56550 = vshrl.u32 %v56540, 8
%v56551 = vor.u32 %v56550, %v56549
%v56552 = vxor.u32 %v56551, %v56543
%v56555 = vadd.s32 %v56552, %v10
%v56559 = vadd.s32 2, %v56555
%v56563 = vadd.s32 %v56559, %v56547
%v56565 = vshll.u32 %v56559, 13
%v56566 = vshrl.u32 %v56559, 19
%v56567 = vor.u32 %v56566, %v56565
%v56568 = vxor.u32 %v56567, %v56563
%v56571 = vadd.s32 %v56568, %v56563
%v56573 = vshll.u32 %v56568, 15
%v56574 = vshrl.u32 %v56568, 17
%v56575 = vor.u32 %v56574, %v56573
%v56576 = vxor.u32 %v56575, %v56571
%v56579 = vadd.s32 %v56576, %v56571
%v56581 = vshll.u32 %v56576, 26
%v56582 = vshrl.u32 %v56576, 6
%v56583 = vor.u32 %v56582, %v56581
%v56584 = vxor.u32 %v56583, %v56579
%v56587 = vadd.s32 %v56584, %v56579
%v56591 = vadd.s32 %v56587, %v10
%v56593 = vshll.u32 %v56584, 6
%v56594 = vshrl.u32 %v56584, 26
%v56595 = vor.u32 %v56594, %v56593
%v56596 = vxor.u32 %v56595, %v56587
%v56599 = vadd.s32 %v56596, %v9
%v56603 = vadd.s32 3, %v56599
%v56607 = vadd.s32 %v56603, %v56591
%v56609 = vshll.u32 %v56603, 17
%v56610 = vshrl.u32 %v56603, 15
%v56611 = vor.u32 %v56610, %v56609
%v56612 = vxor.u32 %v56611, %v56607
%v56615 = vadd.s32 %v56612, %v56607
%v56617 = vshll.u32 %v56612, 29
%v56618 = vshrl.u32 %v56612, 3
%v56619 = vor.u32 %v56618, %v56617
%v56620 = vxor.u32 %v56619, %v56615
%v56623 = vadd.s32 %v56620, %v56615
%v56625 = vshll.u32 %v56620, 16
%v56626 = vshrl.u32 %v56620, 16
%v56627 = vor.u32 %v56626, %v56625
%v56628 = vxor.u32 %v56627, %v56623
%v56631 = vadd.s32 %v56628, %v56623
%v56635 = vadd.s32 %v56631, %v9
%v56637 = vshll.u32 %v56628, 24
%v56638 = vshrl.u32 %v56628, 8
%v56639 = vor.u32 %v56638, %v56637
%v56640 = vxor.u32 %v56639, %v56631
%v56643 = vadd.s32 %v56640, %v8
%v56647 = vadd.s32 4, %v56643
%v56651 = vadd.s32 %v56647, %v56635
%v56653 = vshll.u32 %v56647, 13
%v56654 = vshrl.u32 %v56647, 19
%v56655 = vor.u32 %v56654, %v56653
%v56656 = vxor.u32 %v56655, %v56651
%v56659 = vadd.s32 %v56656, %v56651
%v56661 = vshll.u32 %v56656, 15
%v56662 = vshrl.u32 %v56656, 17
%v56663 = vor.u32 %v56662, %v56661
%v56664 = vxor.u32 %v56663, %v56659
%v56667 = vadd.s32 %v56664, %v56659
%v56669 = vshll.u32 %v56664, 26
%v56670 = vshrl.u32 %v56664, 6
%v56671 = vor.u32 %v56670, %v56669
%v56672 = vxor.u32 %v56671, %v56667
%v56675 = vadd.s32 %v56672, %v56667
%v56679 = vadd.s32 %v56675, %v8
%v56681 = vshll.u32 %v56672, 6
%v56682 = vshrl.u32 %v56672, 26
%v56683 = vor.u32 %v56682, %v56681
%v56684 = vxor.u32 %v56683, %v56675
%v56687 = vadd.s32 %v56684, %v10
%v56691 = vadd.s32 5, %v56687
%v56693 = vxor.u32 %v56691, %v56679
%v56694 = vand.u32.u8 255, %v56693
%v56695 = vand.u32 65535, %v56694
%v56696 = vshrl.u32 %v56695, 1
%v56697 = vor.u32 16256, %v56696
%v56698 = vand.u32.u16 65535, %v56697
%v120048 = vadd.low.f32.bf16 -1.0, %v56698
%v56707 = vmul.f32 2.0, %v120048
%v56711 = vadd.f32 -0.99609375, %v56707
%v56715 = vmax.f32 %v56711, -0.99609375
%v56717 = vand.u32 2147483647, %v56715
%vm56720 = vcmp.eq.f32.partialorder %v56717, 1.0
%v56725 = vmul.f32 inf, %v56715
%v56727 = vxor.u32 2147483648, %v56715
%v56730 = vmul.f32 %v56727, %v56715
%v56732 = vadd.f32 1.0, %v56730
%v56733 = vlog2.pop %v56732
%v56734 = vmul.f32 0.6931472, %v56733
%v56735 = vmul.f32 -0.5, %v56730
%v56736 = vadd.f32 1.0, %v56735
%v56737 = vmul.f32 %v56736, %v56730
%v56738 = vand.u32 2147483647, %v56730
%vm56739 = vcmp.lt.f32.partialorder %v56738, 0.0004427343
%v56740 = vsel /*vm=*/%vm56739, /*on_true_vy=*/%v56737, /*on_false_vx=*/%v56734
%v56741 = vxor.u32 2147483648, %v56740
%vm56744 = vcmp.lt.f32.partialorder %v56741, 5.0
%v56749 = vsel /*vm=*/%vm56744, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v56753 = vsel /*vm=*/%vm56744, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v56757 = vsel /*vm=*/%vm56744, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v56761 = vsel /*vm=*/%vm56744, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v56765 = vsel /*vm=*/%vm56744, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v56769 = vsel /*vm=*/%vm56744, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v56773 = vsel /*vm=*/%vm56744, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v56777 = vsel /*vm=*/%vm56744, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v56781 = vsel /*vm=*/%vm56744, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v56785 = vadd.f32 -2.5, %v56741
%v56787 = vrsqrt.pop %v56741
%v56788 = vmul.f32 %v56787, %v56741
%vm56789 = vcmp.eq.f32.partialorder %v56741, inf
%v56790 = vsel /*vm=*/%vm56789, /*on_true_vy=*/%v56741, /*on_false_vx=*/%v56788
%vm56791 = vcmp.eq.f32.partialorder %v56741, 0.0
%v56792 = vand.u32 2147483648, %v56741
%v56793 = vsel /*vm=*/%vm56791, /*on_true_vy=*/%v56792, /*on_false_vx=*/%v56790
%v56796 = vadd.f32 -3.0, %v56793
%v56800 = vsel /*vm=*/%vm56744, /*on_true_vy=*/%v56785, /*on_false_vx=*/%v56796
%v56804 = vmul.f32 %v56800, %v56781
%v56808 = vadd.f32 %v56804, %v56777
%v56812 = vmul.f32 %v56808, %v56800
%v56816 = vadd.f32 %v56812, %v56773
%v56820 = vmul.f32 %v56816, %v56800
%v56824 = vadd.f32 %v56820, %v56769
%v56828 = vmul.f32 %v56824, %v56800
%v56832 = vadd.f32 %v56828, %v56765
%v56836 = vmul.f32 %v56832, %v56800
%v56840 = vadd.f32 %v56836, %v56761
%v56844 = vmul.f32 %v56840, %v56800
%v56848 = vadd.f32 %v56844, %v56757
%v56852 = vmul.f32 %v56848, %v56800
%v56856 = vadd.f32 %v56852, %v56753
%v56860 = vmul.f32 %v56856, %v56800
%v56864 = vadd.f32 %v56860, %v56749
%v56868 = vmul.f32 %v56864, %v56715
%v56872 = vsel /*vm=*/%vm56720, /*on_true_vy=*/%v56725, /*on_false_vx=*/%v56868
%v56876 = vmul.f32 1.4140625, %v56872
%v56879 = vpack.c.bf16 %v120417, %v56876
%120049 = vst [vmem:[%s280 + $0x3c] sm:$0xf] /*vst_source=*/%v56879
%v56883 = vadd.s32 %v56419, %v894
%v56893 = vadd.s32 %v56883, %v415
%vm56897 = vcmp.lt.u32.totalorder %v56893, %v56883
%vm56902 = vcmp.lt.u32.totalorder %v56883, %v894
%v56907 = vadd.s32 %v56402, %v881
%v56911 = vadd.s32 1, %v56907
%v56915 = vsel /*vm=*/%vm56902, /*on_true_vy=*/%v56911, /*on_false_vx=*/%v56907
%v56919 = vadd.s32 1, %v56915
%v56923 = vsel /*vm=*/%vm56897, /*on_true_vy=*/%v56919, /*on_false_vx=*/%v56915
%v56928 = vadd.s32 %v56923, %v10
%v56932 = vadd.s32 %v56893, %v9
%v56936 = vadd.s32 %v56932, %v56928
%v56938 = vshll.u32 %v56932, 13
%v56939 = vshrl.u32 %v56932, 19
%v56940 = vor.u32 %v56939, %v56938
%v56941 = vxor.u32 %v56940, %v56936
%v56944 = vadd.s32 %v56941, %v56936
%v56946 = vshll.u32 %v56941, 15
%v56947 = vshrl.u32 %v56941, 17
%v56948 = vor.u32 %v56947, %v56946
%v56949 = vxor.u32 %v56948, %v56944
%v56952 = vadd.s32 %v56949, %v56944
%v56954 = vshll.u32 %v56949, 26
%v56955 = vshrl.u32 %v56949, 6
%v56956 = vor.u32 %v56955, %v56954
%v56957 = vxor.u32 %v56956, %v56952
%v56960 = vadd.s32 %v56957, %v56952
%v56964 = vadd.s32 %v56960, %v9
%v56966 = vshll.u32 %v56957, 6
%v56967 = vshrl.u32 %v56957, 26
%v56968 = vor.u32 %v56967, %v56966
%v56969 = vxor.u32 %v56968, %v56960
%v56972 = vadd.s32 %v56969, %v8
%v56976 = vadd.s32 1, %v56972
%v56980 = vadd.s32 %v56976, %v56964
%v56982 = vshll.u32 %v56976, 17
%v56983 = vshrl.u32 %v56976, 15
%v56984 = vor.u32 %v56983, %v56982
%v56985 = vxor.u32 %v56984, %v56980
%v56988 = vadd.s32 %v56985, %v56980
%v56990 = vshll.u32 %v56985, 29
%v56991 = vshrl.u32 %v56985, 3
%v56992 = vor.u32 %v56991, %v56990
%v56993 = vxor.u32 %v56992, %v56988
%v56996 = vadd.s32 %v56993, %v56988
%v56998 = vshll.u32 %v56993, 16
%v56999 = vshrl.u32 %v56993, 16
%v57000 = vor.u32 %v56999, %v56998
%v57001 = vxor.u32 %v57000, %v56996
%v57004 = vadd.s32 %v57001, %v56996
%v57008 = vadd.s32 %v57004, %v8
%v57010 = vshll.u32 %v57001, 24
%v57011 = vshrl.u32 %v57001, 8
%v57012 = vor.u32 %v57011, %v57010
%v57013 = vxor.u32 %v57012, %v57004
%v57016 = vadd.s32 %v57013, %v10
%v57020 = vadd.s32 2, %v57016
%v57024 = vadd.s32 %v57020, %v57008
%v57026 = vshll.u32 %v57020, 13
%v57027 = vshrl.u32 %v57020, 19
%v57028 = vor.u32 %v57027, %v57026
%v57029 = vxor.u32 %v57028, %v57024
%v57032 = vadd.s32 %v57029, %v57024
%v57034 = vshll.u32 %v57029, 15
%v57035 = vshrl.u32 %v57029, 17
%v57036 = vor.u32 %v57035, %v57034
%v57037 = vxor.u32 %v57036, %v57032
%v57040 = vadd.s32 %v57037, %v57032
%v57042 = vshll.u32 %v57037, 26
%v57043 = vshrl.u32 %v57037, 6
%v57044 = vor.u32 %v57043, %v57042
%v57045 = vxor.u32 %v57044, %v57040
%v57048 = vadd.s32 %v57045, %v57040
%v57052 = vadd.s32 %v57048, %v10
%v57054 = vshll.u32 %v57045, 6
%v57055 = vshrl.u32 %v57045, 26
%v57056 = vor.u32 %v57055, %v57054
%v57057 = vxor.u32 %v57056, %v57048
%v57060 = vadd.s32 %v57057, %v9
%v57064 = vadd.s32 3, %v57060
%v57068 = vadd.s32 %v57064, %v57052
%v57070 = vshll.u32 %v57064, 17
%v57071 = vshrl.u32 %v57064, 15
%v57072 = vor.u32 %v57071, %v57070
%v57073 = vxor.u32 %v57072, %v57068
%v57076 = vadd.s32 %v57073, %v57068
%v57078 = vshll.u32 %v57073, 29
%v57079 = vshrl.u32 %v57073, 3
%v57080 = vor.u32 %v57079, %v57078
%v57081 = vxor.u32 %v57080, %v57076
%v57084 = vadd.s32 %v57081, %v57076
%v57086 = vshll.u32 %v57081, 16
%v57087 = vshrl.u32 %v57081, 16
%v57088 = vor.u32 %v57087, %v57086
%v57089 = vxor.u32 %v57088, %v57084
%v57092 = vadd.s32 %v57089, %v57084
%v57096 = vadd.s32 %v57092, %v9
%v57098 = vshll.u32 %v57089, 24
%v57099 = vshrl.u32 %v57089, 8
%v57100 = vor.u32 %v57099, %v57098
%v57101 = vxor.u32 %v57100, %v57092
%v57104 = vadd.s32 %v57101, %v8
%v57108 = vadd.s32 4, %v57104
%v57112 = vadd.s32 %v57108, %v57096
%v57114 = vshll.u32 %v57108, 13
%v57115 = vshrl.u32 %v57108, 19
%v57116 = vor.u32 %v57115, %v57114
%v57117 = vxor.u32 %v57116, %v57112
%v57120 = vadd.s32 %v57117, %v57112
%v57122 = vshll.u32 %v57117, 15
%v57123 = vshrl.u32 %v57117, 17
%v57124 = vor.u32 %v57123, %v57122
%v57125 = vxor.u32 %v57124, %v57120
%v57128 = vadd.s32 %v57125, %v57120
%v57130 = vshll.u32 %v57125, 26
%v57131 = vshrl.u32 %v57125, 6
%v57132 = vor.u32 %v57131, %v57130
%v57133 = vxor.u32 %v57132, %v57128
%v57136 = vadd.s32 %v57133, %v57128
%v57140 = vadd.s32 %v57136, %v8
%v57142 = vshll.u32 %v57133, 6
%v57143 = vshrl.u32 %v57133, 26
%v57144 = vor.u32 %v57143, %v57142
%v57145 = vxor.u32 %v57144, %v57136
%v57148 = vadd.s32 %v57145, %v10
%v57152 = vadd.s32 5, %v57148
%v57154 = vxor.u32 %v57152, %v57140
%v57155 = vand.u32.u8 255, %v57154
%v57156 = vand.u32 65535, %v57155
%v57157 = vshrl.u32 %v57156, 1
%v57158 = vor.u32 16256, %v57157
%v57159 = vand.u32.u16 65535, %v57158
%v120050 = vadd.low.f32.bf16 -1.0, %v57159
%v57168 = vmul.f32 2.0, %v120050
%v57172 = vadd.f32 -0.99609375, %v57168
%v57176 = vmax.f32 %v57172, -0.99609375
%v57178 = vand.u32 2147483647, %v57176
%vm57181 = vcmp.eq.f32.partialorder %v57178, 1.0
%v57186 = vmul.f32 inf, %v57176
%v57188 = vxor.u32 2147483648, %v57176
%v57191 = vmul.f32 %v57188, %v57176
%v57193 = vadd.f32 1.0, %v57191
%v57194 = vlog2.pop %v57193
%v57195 = vmul.f32 0.6931472, %v57194
%v57196 = vmul.f32 -0.5, %v57191
%v57197 = vadd.f32 1.0, %v57196
%v57198 = vmul.f32 %v57197, %v57191
%v57199 = vand.u32 2147483647, %v57191
%vm57200 = vcmp.lt.f32.partialorder %v57199, 0.0004427343
%v57201 = vsel /*vm=*/%vm57200, /*on_true_vy=*/%v57198, /*on_false_vx=*/%v57195
%v57202 = vxor.u32 2147483648, %v57201
%vm57205 = vcmp.lt.f32.partialorder %v57202, 5.0
%v57210 = vsel /*vm=*/%vm57205, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v57214 = vsel /*vm=*/%vm57205, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v57218 = vsel /*vm=*/%vm57205, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v57222 = vsel /*vm=*/%vm57205, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v57226 = vsel /*vm=*/%vm57205, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v57230 = vsel /*vm=*/%vm57205, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v57234 = vsel /*vm=*/%vm57205, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v57238 = vsel /*vm=*/%vm57205, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v57242 = vsel /*vm=*/%vm57205, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v57246 = vadd.f32 -2.5, %v57202
%v57248 = vrsqrt.pop %v57202
%v57249 = vmul.f32 %v57248, %v57202
%vm57250 = vcmp.eq.f32.partialorder %v57202, inf
%v57251 = vsel /*vm=*/%vm57250, /*on_true_vy=*/%v57202, /*on_false_vx=*/%v57249
%vm57252 = vcmp.eq.f32.partialorder %v57202, 0.0
%v57253 = vand.u32 2147483648, %v57202
%v57254 = vsel /*vm=*/%vm57252, /*on_true_vy=*/%v57253, /*on_false_vx=*/%v57251
%v57257 = vadd.f32 -3.0, %v57254
%v57261 = vsel /*vm=*/%vm57205, /*on_true_vy=*/%v57246, /*on_false_vx=*/%v57257
%v57265 = vmul.f32 %v57261, %v57242
%v57269 = vadd.f32 %v57265, %v57238
%v57273 = vmul.f32 %v57269, %v57261
%v57277 = vadd.f32 %v57273, %v57234
%v57281 = vmul.f32 %v57277, %v57261
%v57285 = vadd.f32 %v57281, %v57230
%v57289 = vmul.f32 %v57285, %v57261
%v57293 = vadd.f32 %v57289, %v57226
%v57297 = vmul.f32 %v57293, %v57261
%v57301 = vadd.f32 %v57297, %v57222
%v57305 = vmul.f32 %v57301, %v57261
%v57309 = vadd.f32 %v57305, %v57218
%v57313 = vmul.f32 %v57309, %v57261
%v57317 = vadd.f32 %v57313, %v57214
%v57321 = vmul.f32 %v57317, %v57261
%v57325 = vadd.f32 %v57321, %v57210
%v57329 = vmul.f32 %v57325, %v57176
%v57333 = vsel /*vm=*/%vm57181, /*on_true_vy=*/%v57186, /*on_false_vx=*/%v57329
%v57337 = vmul.f32 1.4140625, %v57333
%v57340 = vpack.c.bf16 %v120417, %v57337
%120051 = vst [vmem:[%s280 + $0xbc] sm:$0xf] /*vst_source=*/%v57340
%v57344 = vadd.s32 %v56419, %v1381
%v57354 = vadd.s32 %v57344, %v415
%vm57358 = vcmp.lt.u32.totalorder %v57354, %v57344
%vm57363 = vcmp.lt.u32.totalorder %v57344, %v1381
%v57368 = vadd.s32 %v56402, %v1368
%v57372 = vadd.s32 1, %v57368
%v57376 = vsel /*vm=*/%vm57363, /*on_true_vy=*/%v57372, /*on_false_vx=*/%v57368
%v57380 = vadd.s32 1, %v57376
%v57384 = vsel /*vm=*/%vm57358, /*on_true_vy=*/%v57380, /*on_false_vx=*/%v57376
%v57389 = vadd.s32 %v57384, %v10
%v57393 = vadd.s32 %v57354, %v9
%v57397 = vadd.s32 %v57393, %v57389
%v57399 = vshll.u32 %v57393, 13
%v57400 = vshrl.u32 %v57393, 19
%v57401 = vor.u32 %v57400, %v57399
%v57402 = vxor.u32 %v57401, %v57397
%v57405 = vadd.s32 %v57402, %v57397
%v57407 = vshll.u32 %v57402, 15
%v57408 = vshrl.u32 %v57402, 17
%v57409 = vor.u32 %v57408, %v57407
%v57410 = vxor.u32 %v57409, %v57405
%v57413 = vadd.s32 %v57410, %v57405
%v57415 = vshll.u32 %v57410, 26
%v57416 = vshrl.u32 %v57410, 6
%v57417 = vor.u32 %v57416, %v57415
%v57418 = vxor.u32 %v57417, %v57413
%v57421 = vadd.s32 %v57418, %v57413
%v57425 = vadd.s32 %v57421, %v9
%v57427 = vshll.u32 %v57418, 6
%v57428 = vshrl.u32 %v57418, 26
%v57429 = vor.u32 %v57428, %v57427
%v57430 = vxor.u32 %v57429, %v57421
%v57433 = vadd.s32 %v57430, %v8
%v57437 = vadd.s32 1, %v57433
%v57441 = vadd.s32 %v57437, %v57425
%v57443 = vshll.u32 %v57437, 17
%v57444 = vshrl.u32 %v57437, 15
%v57445 = vor.u32 %v57444, %v57443
%v57446 = vxor.u32 %v57445, %v57441
%v57449 = vadd.s32 %v57446, %v57441
%v57451 = vshll.u32 %v57446, 29
%v57452 = vshrl.u32 %v57446, 3
%v57453 = vor.u32 %v57452, %v57451
%v57454 = vxor.u32 %v57453, %v57449
%v57457 = vadd.s32 %v57454, %v57449
%v57459 = vshll.u32 %v57454, 16
%v57460 = vshrl.u32 %v57454, 16
%v57461 = vor.u32 %v57460, %v57459
%v57462 = vxor.u32 %v57461, %v57457
%v57465 = vadd.s32 %v57462, %v57457
%v57469 = vadd.s32 %v57465, %v8
%v57471 = vshll.u32 %v57462, 24
%v57472 = vshrl.u32 %v57462, 8
%v57473 = vor.u32 %v57472, %v57471
%v57474 = vxor.u32 %v57473, %v57465
%v57477 = vadd.s32 %v57474, %v10
%v57481 = vadd.s32 2, %v57477
%v57485 = vadd.s32 %v57481, %v57469
%v57487 = vshll.u32 %v57481, 13
%v57488 = vshrl.u32 %v57481, 19
%v57489 = vor.u32 %v57488, %v57487
%v57490 = vxor.u32 %v57489, %v57485
%v57493 = vadd.s32 %v57490, %v57485
%v57495 = vshll.u32 %v57490, 15
%v57496 = vshrl.u32 %v57490, 17
%v57497 = vor.u32 %v57496, %v57495
%v57498 = vxor.u32 %v57497, %v57493
%v57501 = vadd.s32 %v57498, %v57493
%v57503 = vshll.u32 %v57498, 26
%v57504 = vshrl.u32 %v57498, 6
%v57505 = vor.u32 %v57504, %v57503
%v57506 = vxor.u32 %v57505, %v57501
%v57509 = vadd.s32 %v57506, %v57501
%v57513 = vadd.s32 %v57509, %v10
%v57515 = vshll.u32 %v57506, 6
%v57516 = vshrl.u32 %v57506, 26
%v57517 = vor.u32 %v57516, %v57515
%v57518 = vxor.u32 %v57517, %v57509
%v57521 = vadd.s32 %v57518, %v9
%v57525 = vadd.s32 3, %v57521
%v57529 = vadd.s32 %v57525, %v57513
%v57531 = vshll.u32 %v57525, 17
%v57532 = vshrl.u32 %v57525, 15
%v57533 = vor.u32 %v57532, %v57531
%v57534 = vxor.u32 %v57533, %v57529
%v57537 = vadd.s32 %v57534, %v57529
%v57539 = vshll.u32 %v57534, 29
%v57540 = vshrl.u32 %v57534, 3
%v57541 = vor.u32 %v57540, %v57539
%v57542 = vxor.u32 %v57541, %v57537
%v57545 = vadd.s32 %v57542, %v57537
%v57547 = vshll.u32 %v57542, 16
%v57548 = vshrl.u32 %v57542, 16
%v57549 = vor.u32 %v57548, %v57547
%v57550 = vxor.u32 %v57549, %v57545
%v57553 = vadd.s32 %v57550, %v57545
%v57557 = vadd.s32 %v57553, %v9
%v57559 = vshll.u32 %v57550, 24
%v57560 = vshrl.u32 %v57550, 8
%v57561 = vor.u32 %v57560, %v57559
%v57562 = vxor.u32 %v57561, %v57553
%v57565 = vadd.s32 %v57562, %v8
%v57569 = vadd.s32 4, %v57565
%v57573 = vadd.s32 %v57569, %v57557
%v57575 = vshll.u32 %v57569, 13
%v57576 = vshrl.u32 %v57569, 19
%v57577 = vor.u32 %v57576, %v57575
%v57578 = vxor.u32 %v57577, %v57573
%v57581 = vadd.s32 %v57578, %v57573
%v57583 = vshll.u32 %v57578, 15
%v57584 = vshrl.u32 %v57578, 17
%v57585 = vor.u32 %v57584, %v57583
%v57586 = vxor.u32 %v57585, %v57581
%v57589 = vadd.s32 %v57586, %v57581
%v57591 = vshll.u32 %v57586, 26
%v57592 = vshrl.u32 %v57586, 6
%v57593 = vor.u32 %v57592, %v57591
%v57594 = vxor.u32 %v57593, %v57589
%v57597 = vadd.s32 %v57594, %v57589
%v57601 = vadd.s32 %v57597, %v8
%v57603 = vshll.u32 %v57594, 6
%v57604 = vshrl.u32 %v57594, 26
%v57605 = vor.u32 %v57604, %v57603
%v57606 = vxor.u32 %v57605, %v57597
%v57609 = vadd.s32 %v57606, %v10
%v57613 = vadd.s32 5, %v57609
%v57615 = vxor.u32 %v57613, %v57601
%v57616 = vand.u32.u8 255, %v57615
%v57617 = vand.u32 65535, %v57616
%v57618 = vshrl.u32 %v57617, 1
%v57619 = vor.u32 16256, %v57618
%v57620 = vand.u32.u16 65535, %v57619
%v120052 = vadd.low.f32.bf16 -1.0, %v57620
%v57629 = vmul.f32 2.0, %v120052
%v57633 = vadd.f32 -0.99609375, %v57629
%v57637 = vmax.f32 %v57633, -0.99609375
%v57639 = vand.u32 2147483647, %v57637
%vm57642 = vcmp.eq.f32.partialorder %v57639, 1.0
%v57647 = vmul.f32 inf, %v57637
%v57649 = vxor.u32 2147483648, %v57637
%v57652 = vmul.f32 %v57649, %v57637
%v57654 = vadd.f32 1.0, %v57652
%v57655 = vlog2.pop %v57654
%v57656 = vmul.f32 0.6931472, %v57655
%v57657 = vmul.f32 -0.5, %v57652
%v57658 = vadd.f32 1.0, %v57657
%v57659 = vmul.f32 %v57658, %v57652
%v57660 = vand.u32 2147483647, %v57652
%vm57661 = vcmp.lt.f32.partialorder %v57660, 0.0004427343
%v57662 = vsel /*vm=*/%vm57661, /*on_true_vy=*/%v57659, /*on_false_vx=*/%v57656
%v57663 = vxor.u32 2147483648, %v57662
%vm57666 = vcmp.lt.f32.partialorder %v57663, 5.0
%v57671 = vsel /*vm=*/%vm57666, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v57675 = vsel /*vm=*/%vm57666, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v57679 = vsel /*vm=*/%vm57666, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v57683 = vsel /*vm=*/%vm57666, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v57687 = vsel /*vm=*/%vm57666, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v57691 = vsel /*vm=*/%vm57666, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v57695 = vsel /*vm=*/%vm57666, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v57699 = vsel /*vm=*/%vm57666, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v57703 = vsel /*vm=*/%vm57666, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v57707 = vadd.f32 -2.5, %v57663
%v57709 = vrsqrt.pop %v57663
%v57710 = vmul.f32 %v57709, %v57663
%vm57711 = vcmp.eq.f32.partialorder %v57663, inf
%v57712 = vsel /*vm=*/%vm57711, /*on_true_vy=*/%v57663, /*on_false_vx=*/%v57710
%vm57713 = vcmp.eq.f32.partialorder %v57663, 0.0
%v57714 = vand.u32 2147483648, %v57663
%v57715 = vsel /*vm=*/%vm57713, /*on_true_vy=*/%v57714, /*on_false_vx=*/%v57712
%v57718 = vadd.f32 -3.0, %v57715
%v57722 = vsel /*vm=*/%vm57666, /*on_true_vy=*/%v57707, /*on_false_vx=*/%v57718
%v57726 = vmul.f32 %v57722, %v57703
%v57730 = vadd.f32 %v57726, %v57699
%v57734 = vmul.f32 %v57730, %v57722
%v57738 = vadd.f32 %v57734, %v57695
%v57742 = vmul.f32 %v57738, %v57722
%v57746 = vadd.f32 %v57742, %v57691
%v57750 = vmul.f32 %v57746, %v57722
%v57754 = vadd.f32 %v57750, %v57687
%v57758 = vmul.f32 %v57754, %v57722
%v57762 = vadd.f32 %v57758, %v57683
%v57766 = vmul.f32 %v57762, %v57722
%v57770 = vadd.f32 %v57766, %v57679
%v57774 = vmul.f32 %v57770, %v57722
%v57778 = vadd.f32 %v57774, %v57675
%v57782 = vmul.f32 %v57778, %v57722
%v57786 = vadd.f32 %v57782, %v57671
%v57790 = vmul.f32 %v57786, %v57637
%v57794 = vsel /*vm=*/%vm57642, /*on_true_vy=*/%v57647, /*on_false_vx=*/%v57790
%v57798 = vmul.f32 1.4140625, %v57794
%v57801 = vpack.c.bf16 %v120417, %v57798
%120053 = vst [vmem:[%s280 + $0x13c] sm:$0xf] /*vst_source=*/%v57801
%v57805 = vadd.s32 %v56419, %v1868
%v57815 = vadd.s32 %v57805, %v415
%vm57819 = vcmp.lt.u32.totalorder %v57815, %v57805
%vm57824 = vcmp.lt.u32.totalorder %v57805, %v1868
%v57829 = vadd.s32 %v56402, %v1855
%v57833 = vadd.s32 1, %v57829
%v57837 = vsel /*vm=*/%vm57824, /*on_true_vy=*/%v57833, /*on_false_vx=*/%v57829
%v57841 = vadd.s32 1, %v57837
%v57845 = vsel /*vm=*/%vm57819, /*on_true_vy=*/%v57841, /*on_false_vx=*/%v57837
%v57850 = vadd.s32 %v57845, %v10
%v57854 = vadd.s32 %v57815, %v9
%v57858 = vadd.s32 %v57854, %v57850
%v57860 = vshll.u32 %v57854, 13
%v57861 = vshrl.u32 %v57854, 19
%v57862 = vor.u32 %v57861, %v57860
%v57863 = vxor.u32 %v57862, %v57858
%v57866 = vadd.s32 %v57863, %v57858
%v57868 = vshll.u32 %v57863, 15
%v57869 = vshrl.u32 %v57863, 17
%v57870 = vor.u32 %v57869, %v57868
%v57871 = vxor.u32 %v57870, %v57866
%v57874 = vadd.s32 %v57871, %v57866
%v57876 = vshll.u32 %v57871, 26
%v57877 = vshrl.u32 %v57871, 6
%v57878 = vor.u32 %v57877, %v57876
%v57879 = vxor.u32 %v57878, %v57874
%v57882 = vadd.s32 %v57879, %v57874
%v57886 = vadd.s32 %v57882, %v9
%v57888 = vshll.u32 %v57879, 6
%v57889 = vshrl.u32 %v57879, 26
%v57890 = vor.u32 %v57889, %v57888
%v57891 = vxor.u32 %v57890, %v57882
%v57894 = vadd.s32 %v57891, %v8
%v57898 = vadd.s32 1, %v57894
%v57902 = vadd.s32 %v57898, %v57886
%v57904 = vshll.u32 %v57898, 17
%v57905 = vshrl.u32 %v57898, 15
%v57906 = vor.u32 %v57905, %v57904
%v57907 = vxor.u32 %v57906, %v57902
%v57910 = vadd.s32 %v57907, %v57902
%v57912 = vshll.u32 %v57907, 29
%v57913 = vshrl.u32 %v57907, 3
%v57914 = vor.u32 %v57913, %v57912
%v57915 = vxor.u32 %v57914, %v57910
%v57918 = vadd.s32 %v57915, %v57910
%v57920 = vshll.u32 %v57915, 16
%v57921 = vshrl.u32 %v57915, 16
%v57922 = vor.u32 %v57921, %v57920
%v57923 = vxor.u32 %v57922, %v57918
%v57926 = vadd.s32 %v57923, %v57918
%v57930 = vadd.s32 %v57926, %v8
%v57932 = vshll.u32 %v57923, 24
%v57933 = vshrl.u32 %v57923, 8
%v57934 = vor.u32 %v57933, %v57932
%v57935 = vxor.u32 %v57934, %v57926
%v57938 = vadd.s32 %v57935, %v10
%v57942 = vadd.s32 2, %v57938
%v57946 = vadd.s32 %v57942, %v57930
%v57948 = vshll.u32 %v57942, 13
%v57949 = vshrl.u32 %v57942, 19
%v57950 = vor.u32 %v57949, %v57948
%v57951 = vxor.u32 %v57950, %v57946
%v57954 = vadd.s32 %v57951, %v57946
%v57956 = vshll.u32 %v57951, 15
%v57957 = vshrl.u32 %v57951, 17
%v57958 = vor.u32 %v57957, %v57956
%v57959 = vxor.u32 %v57958, %v57954
%v57962 = vadd.s32 %v57959, %v57954
%v57964 = vshll.u32 %v57959, 26
%v57965 = vshrl.u32 %v57959, 6
%v57966 = vor.u32 %v57965, %v57964
%v57967 = vxor.u32 %v57966, %v57962
%v57970 = vadd.s32 %v57967, %v57962
%v57974 = vadd.s32 %v57970, %v10
%v57976 = vshll.u32 %v57967, 6
%v57977 = vshrl.u32 %v57967, 26
%v57978 = vor.u32 %v57977, %v57976
%v57979 = vxor.u32 %v57978, %v57970
%v57982 = vadd.s32 %v57979, %v9
%v57986 = vadd.s32 3, %v57982
%v57990 = vadd.s32 %v57986, %v57974
%v57992 = vshll.u32 %v57986, 17
%v57993 = vshrl.u32 %v57986, 15
%v57994 = vor.u32 %v57993, %v57992
%v57995 = vxor.u32 %v57994, %v57990
%v57998 = vadd.s32 %v57995, %v57990
%v58000 = vshll.u32 %v57995, 29
%v58001 = vshrl.u32 %v57995, 3
%v58002 = vor.u32 %v58001, %v58000
%v58003 = vxor.u32 %v58002, %v57998
%v58006 = vadd.s32 %v58003, %v57998
%v58008 = vshll.u32 %v58003, 16
%v58009 = vshrl.u32 %v58003, 16
%v58010 = vor.u32 %v58009, %v58008
%v58011 = vxor.u32 %v58010, %v58006
%v58014 = vadd.s32 %v58011, %v58006
%v58018 = vadd.s32 %v58014, %v9
%v58020 = vshll.u32 %v58011, 24
%v58021 = vshrl.u32 %v58011, 8
%v58022 = vor.u32 %v58021, %v58020
%v58023 = vxor.u32 %v58022, %v58014
%v58026 = vadd.s32 %v58023, %v8
%v58030 = vadd.s32 4, %v58026
%v58034 = vadd.s32 %v58030, %v58018
%v58036 = vshll.u32 %v58030, 13
%v58037 = vshrl.u32 %v58030, 19
%v58038 = vor.u32 %v58037, %v58036
%v58039 = vxor.u32 %v58038, %v58034
%v58042 = vadd.s32 %v58039, %v58034
%v58044 = vshll.u32 %v58039, 15
%v58045 = vshrl.u32 %v58039, 17
%v58046 = vor.u32 %v58045, %v58044
%v58047 = vxor.u32 %v58046, %v58042
%v58050 = vadd.s32 %v58047, %v58042
%v58052 = vshll.u32 %v58047, 26
%v58053 = vshrl.u32 %v58047, 6
%v58054 = vor.u32 %v58053, %v58052
%v58055 = vxor.u32 %v58054, %v58050
%v58058 = vadd.s32 %v58055, %v58050
%v58062 = vadd.s32 %v58058, %v8
%v58064 = vshll.u32 %v58055, 6
%v58065 = vshrl.u32 %v58055, 26
%v58066 = vor.u32 %v58065, %v58064
%v58067 = vxor.u32 %v58066, %v58058
%v58070 = vadd.s32 %v58067, %v10
%v58074 = vadd.s32 5, %v58070
%v58076 = vxor.u32 %v58074, %v58062
%v58077 = vand.u32.u8 255, %v58076
%v58078 = vand.u32 65535, %v58077
%v58079 = vshrl.u32 %v58078, 1
%v58080 = vor.u32 16256, %v58079
%v58081 = vand.u32.u16 65535, %v58080
%v120054 = vadd.low.f32.bf16 -1.0, %v58081
%v58090 = vmul.f32 2.0, %v120054
%v58094 = vadd.f32 -0.99609375, %v58090
%v58098 = vmax.f32 %v58094, -0.99609375
%v58100 = vand.u32 2147483647, %v58098
%vm58103 = vcmp.eq.f32.partialorder %v58100, 1.0
%v58108 = vmul.f32 inf, %v58098
%v58110 = vxor.u32 2147483648, %v58098
%v58113 = vmul.f32 %v58110, %v58098
%v58115 = vadd.f32 1.0, %v58113
%v58116 = vlog2.pop %v58115
%v58117 = vmul.f32 0.6931472, %v58116
%v58118 = vmul.f32 -0.5, %v58113
%v58119 = vadd.f32 1.0, %v58118
%v58120 = vmul.f32 %v58119, %v58113
%v58121 = vand.u32 2147483647, %v58113
%vm58122 = vcmp.lt.f32.partialorder %v58121, 0.0004427343
%v58123 = vsel /*vm=*/%vm58122, /*on_true_vy=*/%v58120, /*on_false_vx=*/%v58117
%v58124 = vxor.u32 2147483648, %v58123
%vm58127 = vcmp.lt.f32.partialorder %v58124, 5.0
%v58132 = vsel /*vm=*/%vm58127, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v58136 = vsel /*vm=*/%vm58127, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v58140 = vsel /*vm=*/%vm58127, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v58144 = vsel /*vm=*/%vm58127, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v58148 = vsel /*vm=*/%vm58127, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v58152 = vsel /*vm=*/%vm58127, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v58156 = vsel /*vm=*/%vm58127, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v58160 = vsel /*vm=*/%vm58127, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v58164 = vsel /*vm=*/%vm58127, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v58168 = vadd.f32 -2.5, %v58124
%v58170 = vrsqrt.pop %v58124
%v58171 = vmul.f32 %v58170, %v58124
%vm58172 = vcmp.eq.f32.partialorder %v58124, inf
%v58173 = vsel /*vm=*/%vm58172, /*on_true_vy=*/%v58124, /*on_false_vx=*/%v58171
%vm58174 = vcmp.eq.f32.partialorder %v58124, 0.0
%v58175 = vand.u32 2147483648, %v58124
%v58176 = vsel /*vm=*/%vm58174, /*on_true_vy=*/%v58175, /*on_false_vx=*/%v58173
%v58179 = vadd.f32 -3.0, %v58176
%v58183 = vsel /*vm=*/%vm58127, /*on_true_vy=*/%v58168, /*on_false_vx=*/%v58179
%v58187 = vmul.f32 %v58183, %v58164
%v58191 = vadd.f32 %v58187, %v58160
%v58195 = vmul.f32 %v58191, %v58183
%v58199 = vadd.f32 %v58195, %v58156
%v58203 = vmul.f32 %v58199, %v58183
%v58207 = vadd.f32 %v58203, %v58152
%v58211 = vmul.f32 %v58207, %v58183
%v58215 = vadd.f32 %v58211, %v58148
%v58219 = vmul.f32 %v58215, %v58183
%v58223 = vadd.f32 %v58219, %v58144
%v58227 = vmul.f32 %v58223, %v58183
%v58231 = vadd.f32 %v58227, %v58140
%v58235 = vmul.f32 %v58231, %v58183
%v58239 = vadd.f32 %v58235, %v58136
%v58243 = vmul.f32 %v58239, %v58183
%v58247 = vadd.f32 %v58243, %v58132
%v58251 = vmul.f32 %v58247, %v58098
%v58255 = vsel /*vm=*/%vm58103, /*on_true_vy=*/%v58108, /*on_false_vx=*/%v58251
%v58259 = vmul.f32 1.4140625, %v58255
%v58262 = vpack.c.bf16 %v120417, %v58259
%120055 = vst [vmem:[%s280 + $0x1bc] sm:$0xf] /*vst_source=*/%v58262
%v58266 = vadd.s32 %v56419, %v2355
%v58276 = vadd.s32 %v58266, %v415
%vm58280 = vcmp.lt.u32.totalorder %v58276, %v58266
%vm58285 = vcmp.lt.u32.totalorder %v58266, %v2355
%v58290 = vadd.s32 %v56402, %v2342
%v58294 = vadd.s32 1, %v58290
%v58298 = vsel /*vm=*/%vm58285, /*on_true_vy=*/%v58294, /*on_false_vx=*/%v58290
%v58302 = vadd.s32 1, %v58298
%v58306 = vsel /*vm=*/%vm58280, /*on_true_vy=*/%v58302, /*on_false_vx=*/%v58298
%v58311 = vadd.s32 %v58306, %v10
%v58315 = vadd.s32 %v58276, %v9
%v58319 = vadd.s32 %v58315, %v58311
%v58321 = vshll.u32 %v58315, 13
%v58322 = vshrl.u32 %v58315, 19
%v58323 = vor.u32 %v58322, %v58321
%v58324 = vxor.u32 %v58323, %v58319
%v58327 = vadd.s32 %v58324, %v58319
%v58329 = vshll.u32 %v58324, 15
%v58330 = vshrl.u32 %v58324, 17
%v58331 = vor.u32 %v58330, %v58329
%v58332 = vxor.u32 %v58331, %v58327
%v58335 = vadd.s32 %v58332, %v58327
%v58337 = vshll.u32 %v58332, 26
%v58338 = vshrl.u32 %v58332, 6
%v58339 = vor.u32 %v58338, %v58337
%v58340 = vxor.u32 %v58339, %v58335
%v58343 = vadd.s32 %v58340, %v58335
%v58347 = vadd.s32 %v58343, %v9
%v58349 = vshll.u32 %v58340, 6
%v58350 = vshrl.u32 %v58340, 26
%v58351 = vor.u32 %v58350, %v58349
%v58352 = vxor.u32 %v58351, %v58343
%v58355 = vadd.s32 %v58352, %v8
%v58359 = vadd.s32 1, %v58355
%v58363 = vadd.s32 %v58359, %v58347
%v58365 = vshll.u32 %v58359, 17
%v58366 = vshrl.u32 %v58359, 15
%v58367 = vor.u32 %v58366, %v58365
%v58368 = vxor.u32 %v58367, %v58363
%v58371 = vadd.s32 %v58368, %v58363
%v58373 = vshll.u32 %v58368, 29
%v58374 = vshrl.u32 %v58368, 3
%v58375 = vor.u32 %v58374, %v58373
%v58376 = vxor.u32 %v58375, %v58371
%v58379 = vadd.s32 %v58376, %v58371
%v58381 = vshll.u32 %v58376, 16
%v58382 = vshrl.u32 %v58376, 16
%v58383 = vor.u32 %v58382, %v58381
%v58384 = vxor.u32 %v58383, %v58379
%v58387 = vadd.s32 %v58384, %v58379
%v58391 = vadd.s32 %v58387, %v8
%v58393 = vshll.u32 %v58384, 24
%v58394 = vshrl.u32 %v58384, 8
%v58395 = vor.u32 %v58394, %v58393
%v58396 = vxor.u32 %v58395, %v58387
%v58399 = vadd.s32 %v58396, %v10
%v58403 = vadd.s32 2, %v58399
%v58407 = vadd.s32 %v58403, %v58391
%v58409 = vshll.u32 %v58403, 13
%v58410 = vshrl.u32 %v58403, 19
%v58411 = vor.u32 %v58410, %v58409
%v58412 = vxor.u32 %v58411, %v58407
%v58415 = vadd.s32 %v58412, %v58407
%v58417 = vshll.u32 %v58412, 15
%v58418 = vshrl.u32 %v58412, 17
%v58419 = vor.u32 %v58418, %v58417
%v58420 = vxor.u32 %v58419, %v58415
%v58423 = vadd.s32 %v58420, %v58415
%v58425 = vshll.u32 %v58420, 26
%v58426 = vshrl.u32 %v58420, 6
%v58427 = vor.u32 %v58426, %v58425
%v58428 = vxor.u32 %v58427, %v58423
%v58431 = vadd.s32 %v58428, %v58423
%v58435 = vadd.s32 %v58431, %v10
%v58437 = vshll.u32 %v58428, 6
%v58438 = vshrl.u32 %v58428, 26
%v58439 = vor.u32 %v58438, %v58437
%v58440 = vxor.u32 %v58439, %v58431
%v58443 = vadd.s32 %v58440, %v9
%v58447 = vadd.s32 3, %v58443
%v58451 = vadd.s32 %v58447, %v58435
%v58453 = vshll.u32 %v58447, 17
%v58454 = vshrl.u32 %v58447, 15
%v58455 = vor.u32 %v58454, %v58453
%v58456 = vxor.u32 %v58455, %v58451
%v58459 = vadd.s32 %v58456, %v58451
%v58461 = vshll.u32 %v58456, 29
%v58462 = vshrl.u32 %v58456, 3
%v58463 = vor.u32 %v58462, %v58461
%v58464 = vxor.u32 %v58463, %v58459
%v58467 = vadd.s32 %v58464, %v58459
%v58469 = vshll.u32 %v58464, 16
%v58470 = vshrl.u32 %v58464, 16
%v58471 = vor.u32 %v58470, %v58469
%v58472 = vxor.u32 %v58471, %v58467
%v58475 = vadd.s32 %v58472, %v58467
%v58479 = vadd.s32 %v58475, %v9
%v58481 = vshll.u32 %v58472, 24
%v58482 = vshrl.u32 %v58472, 8
%v58483 = vor.u32 %v58482, %v58481
%v58484 = vxor.u32 %v58483, %v58475
%v58487 = vadd.s32 %v58484, %v8
%v58491 = vadd.s32 4, %v58487
%v58495 = vadd.s32 %v58491, %v58479
%v58497 = vshll.u32 %v58491, 13
%v58498 = vshrl.u32 %v58491, 19
%v58499 = vor.u32 %v58498, %v58497
%v58500 = vxor.u32 %v58499, %v58495
%v58503 = vadd.s32 %v58500, %v58495
%v58505 = vshll.u32 %v58500, 15
%v58506 = vshrl.u32 %v58500, 17
%v58507 = vor.u32 %v58506, %v58505
%v58508 = vxor.u32 %v58507, %v58503
%v58511 = vadd.s32 %v58508, %v58503
%v58513 = vshll.u32 %v58508, 26
%v58514 = vshrl.u32 %v58508, 6
%v58515 = vor.u32 %v58514, %v58513
%v58516 = vxor.u32 %v58515, %v58511
%v58519 = vadd.s32 %v58516, %v58511
%v58523 = vadd.s32 %v58519, %v8
%v58525 = vshll.u32 %v58516, 6
%v58526 = vshrl.u32 %v58516, 26
%v58527 = vor.u32 %v58526, %v58525
%v58528 = vxor.u32 %v58527, %v58519
%v58531 = vadd.s32 %v58528, %v10
%v58535 = vadd.s32 5, %v58531
%v58537 = vxor.u32 %v58535, %v58523
%v58538 = vand.u32.u8 255, %v58537
%v58539 = vand.u32 65535, %v58538
%v58540 = vshrl.u32 %v58539, 1
%v58541 = vor.u32 16256, %v58540
%v58542 = vand.u32.u16 65535, %v58541
%v120056 = vadd.low.f32.bf16 -1.0, %v58542
%v58551 = vmul.f32 2.0, %v120056
%v58555 = vadd.f32 -0.99609375, %v58551
%v58559 = vmax.f32 %v58555, -0.99609375
%v58561 = vand.u32 2147483647, %v58559
%vm58564 = vcmp.eq.f32.partialorder %v58561, 1.0
%v58569 = vmul.f32 inf, %v58559
%v58571 = vxor.u32 2147483648, %v58559
%v58574 = vmul.f32 %v58571, %v58559
%v58576 = vadd.f32 1.0, %v58574
%v58577 = vlog2.pop %v58576
%v58578 = vmul.f32 0.6931472, %v58577
%v58579 = vmul.f32 -0.5, %v58574
%v58580 = vadd.f32 1.0, %v58579
%v58581 = vmul.f32 %v58580, %v58574
%v58582 = vand.u32 2147483647, %v58574
%vm58583 = vcmp.lt.f32.partialorder %v58582, 0.0004427343
%v58584 = vsel /*vm=*/%vm58583, /*on_true_vy=*/%v58581, /*on_false_vx=*/%v58578
%v58585 = vxor.u32 2147483648, %v58584
%vm58588 = vcmp.lt.f32.partialorder %v58585, 5.0
%v58593 = vsel /*vm=*/%vm58588, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v58597 = vsel /*vm=*/%vm58588, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v58601 = vsel /*vm=*/%vm58588, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v58605 = vsel /*vm=*/%vm58588, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v58609 = vsel /*vm=*/%vm58588, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v58613 = vsel /*vm=*/%vm58588, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v58617 = vsel /*vm=*/%vm58588, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v58621 = vsel /*vm=*/%vm58588, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v58625 = vsel /*vm=*/%vm58588, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v58629 = vadd.f32 -2.5, %v58585
%v58631 = vrsqrt.pop %v58585
%v58632 = vmul.f32 %v58631, %v58585
%vm58633 = vcmp.eq.f32.partialorder %v58585, inf
%v58634 = vsel /*vm=*/%vm58633, /*on_true_vy=*/%v58585, /*on_false_vx=*/%v58632
%vm58635 = vcmp.eq.f32.partialorder %v58585, 0.0
%v58636 = vand.u32 2147483648, %v58585
%v58637 = vsel /*vm=*/%vm58635, /*on_true_vy=*/%v58636, /*on_false_vx=*/%v58634
%v58640 = vadd.f32 -3.0, %v58637
%v58644 = vsel /*vm=*/%vm58588, /*on_true_vy=*/%v58629, /*on_false_vx=*/%v58640
%v58648 = vmul.f32 %v58644, %v58625
%v58652 = vadd.f32 %v58648, %v58621
%v58656 = vmul.f32 %v58652, %v58644
%v58660 = vadd.f32 %v58656, %v58617
%v58664 = vmul.f32 %v58660, %v58644
%v58668 = vadd.f32 %v58664, %v58613
%v58672 = vmul.f32 %v58668, %v58644
%v58676 = vadd.f32 %v58672, %v58609
%v58680 = vmul.f32 %v58676, %v58644
%v58684 = vadd.f32 %v58680, %v58605
%v58688 = vmul.f32 %v58684, %v58644
%v58692 = vadd.f32 %v58688, %v58601
%v58696 = vmul.f32 %v58692, %v58644
%v58700 = vadd.f32 %v58696, %v58597
%v58704 = vmul.f32 %v58700, %v58644
%v58708 = vadd.f32 %v58704, %v58593
%v58712 = vmul.f32 %v58708, %v58559
%v58716 = vsel /*vm=*/%vm58564, /*on_true_vy=*/%v58569, /*on_false_vx=*/%v58712
%v58720 = vmul.f32 1.4140625, %v58716
%v58723 = vpack.c.bf16 %v120417, %v58720
%120057 = vst [vmem:[%s280 + $0x23c] sm:$0xf] /*vst_source=*/%v58723
%v58727 = vadd.s32 %v56419, %v2842
%v58737 = vadd.s32 %v58727, %v415
%vm58741 = vcmp.lt.u32.totalorder %v58737, %v58727
%vm58746 = vcmp.lt.u32.totalorder %v58727, %v2842
%v58751 = vadd.s32 %v56402, %v2829
%v58755 = vadd.s32 1, %v58751
%v58759 = vsel /*vm=*/%vm58746, /*on_true_vy=*/%v58755, /*on_false_vx=*/%v58751
%v58763 = vadd.s32 1, %v58759
%v58767 = vsel /*vm=*/%vm58741, /*on_true_vy=*/%v58763, /*on_false_vx=*/%v58759
%v58772 = vadd.s32 %v58767, %v10
%v58776 = vadd.s32 %v58737, %v9
%v58780 = vadd.s32 %v58776, %v58772
%v58782 = vshll.u32 %v58776, 13
%v58783 = vshrl.u32 %v58776, 19
%v58784 = vor.u32 %v58783, %v58782
%v58785 = vxor.u32 %v58784, %v58780
%v58788 = vadd.s32 %v58785, %v58780
%v58790 = vshll.u32 %v58785, 15
%v58791 = vshrl.u32 %v58785, 17
%v58792 = vor.u32 %v58791, %v58790
%v58793 = vxor.u32 %v58792, %v58788
%v58796 = vadd.s32 %v58793, %v58788
%v58798 = vshll.u32 %v58793, 26
%v58799 = vshrl.u32 %v58793, 6
%v58800 = vor.u32 %v58799, %v58798
%v58801 = vxor.u32 %v58800, %v58796
%v58804 = vadd.s32 %v58801, %v58796
%v58808 = vadd.s32 %v58804, %v9
%v58810 = vshll.u32 %v58801, 6
%v58811 = vshrl.u32 %v58801, 26
%v58812 = vor.u32 %v58811, %v58810
%v58813 = vxor.u32 %v58812, %v58804
%v58816 = vadd.s32 %v58813, %v8
%v58820 = vadd.s32 1, %v58816
%v58824 = vadd.s32 %v58820, %v58808
%v58826 = vshll.u32 %v58820, 17
%v58827 = vshrl.u32 %v58820, 15
%v58828 = vor.u32 %v58827, %v58826
%v58829 = vxor.u32 %v58828, %v58824
%v58832 = vadd.s32 %v58829, %v58824
%v58834 = vshll.u32 %v58829, 29
%v58835 = vshrl.u32 %v58829, 3
%v58836 = vor.u32 %v58835, %v58834
%v58837 = vxor.u32 %v58836, %v58832
%v58840 = vadd.s32 %v58837, %v58832
%v58842 = vshll.u32 %v58837, 16
%v58843 = vshrl.u32 %v58837, 16
%v58844 = vor.u32 %v58843, %v58842
%v58845 = vxor.u32 %v58844, %v58840
%v58848 = vadd.s32 %v58845, %v58840
%v58852 = vadd.s32 %v58848, %v8
%v58854 = vshll.u32 %v58845, 24
%v58855 = vshrl.u32 %v58845, 8
%v58856 = vor.u32 %v58855, %v58854
%v58857 = vxor.u32 %v58856, %v58848
%v58860 = vadd.s32 %v58857, %v10
%v58864 = vadd.s32 2, %v58860
%v58868 = vadd.s32 %v58864, %v58852
%v58870 = vshll.u32 %v58864, 13
%v58871 = vshrl.u32 %v58864, 19
%v58872 = vor.u32 %v58871, %v58870
%v58873 = vxor.u32 %v58872, %v58868
%v58876 = vadd.s32 %v58873, %v58868
%v58878 = vshll.u32 %v58873, 15
%v58879 = vshrl.u32 %v58873, 17
%v58880 = vor.u32 %v58879, %v58878
%v58881 = vxor.u32 %v58880, %v58876
%v58884 = vadd.s32 %v58881, %v58876
%v58886 = vshll.u32 %v58881, 26
%v58887 = vshrl.u32 %v58881, 6
%v58888 = vor.u32 %v58887, %v58886
%v58889 = vxor.u32 %v58888, %v58884
%v58892 = vadd.s32 %v58889, %v58884
%v58896 = vadd.s32 %v58892, %v10
%v58898 = vshll.u32 %v58889, 6
%v58899 = vshrl.u32 %v58889, 26
%v58900 = vor.u32 %v58899, %v58898
%v58901 = vxor.u32 %v58900, %v58892
%v58904 = vadd.s32 %v58901, %v9
%v58908 = vadd.s32 3, %v58904
%v58912 = vadd.s32 %v58908, %v58896
%v58914 = vshll.u32 %v58908, 17
%v58915 = vshrl.u32 %v58908, 15
%v58916 = vor.u32 %v58915, %v58914
%v58917 = vxor.u32 %v58916, %v58912
%v58920 = vadd.s32 %v58917, %v58912
%v58922 = vshll.u32 %v58917, 29
%v58923 = vshrl.u32 %v58917, 3
%v58924 = vor.u32 %v58923, %v58922
%v58925 = vxor.u32 %v58924, %v58920
%v58928 = vadd.s32 %v58925, %v58920
%v58930 = vshll.u32 %v58925, 16
%v58931 = vshrl.u32 %v58925, 16
%v58932 = vor.u32 %v58931, %v58930
%v58933 = vxor.u32 %v58932, %v58928
%v58936 = vadd.s32 %v58933, %v58928
%v58940 = vadd.s32 %v58936, %v9
%v58942 = vshll.u32 %v58933, 24
%v58943 = vshrl.u32 %v58933, 8
%v58944 = vor.u32 %v58943, %v58942
%v58945 = vxor.u32 %v58944, %v58936
%v58948 = vadd.s32 %v58945, %v8
%v58952 = vadd.s32 4, %v58948
%v58956 = vadd.s32 %v58952, %v58940
%v58958 = vshll.u32 %v58952, 13
%v58959 = vshrl.u32 %v58952, 19
%v58960 = vor.u32 %v58959, %v58958
%v58961 = vxor.u32 %v58960, %v58956
%v58964 = vadd.s32 %v58961, %v58956
%v58966 = vshll.u32 %v58961, 15
%v58967 = vshrl.u32 %v58961, 17
%v58968 = vor.u32 %v58967, %v58966
%v58969 = vxor.u32 %v58968, %v58964
%v58972 = vadd.s32 %v58969, %v58964
%v58974 = vshll.u32 %v58969, 26
%v58975 = vshrl.u32 %v58969, 6
%v58976 = vor.u32 %v58975, %v58974
%v58977 = vxor.u32 %v58976, %v58972
%v58980 = vadd.s32 %v58977, %v58972
%v58984 = vadd.s32 %v58980, %v8
%v58986 = vshll.u32 %v58977, 6
%v58987 = vshrl.u32 %v58977, 26
%v58988 = vor.u32 %v58987, %v58986
%v58989 = vxor.u32 %v58988, %v58980
%v58992 = vadd.s32 %v58989, %v10
%v58996 = vadd.s32 5, %v58992
%v58998 = vxor.u32 %v58996, %v58984
%v58999 = vand.u32.u8 255, %v58998
%v59000 = vand.u32 65535, %v58999
%v59001 = vshrl.u32 %v59000, 1
%v59002 = vor.u32 16256, %v59001
%v59003 = vand.u32.u16 65535, %v59002
%v120058 = vadd.low.f32.bf16 -1.0, %v59003
%v59012 = vmul.f32 2.0, %v120058
%v59016 = vadd.f32 -0.99609375, %v59012
%v59020 = vmax.f32 %v59016, -0.99609375
%v59022 = vand.u32 2147483647, %v59020
%vm59025 = vcmp.eq.f32.partialorder %v59022, 1.0
%v59030 = vmul.f32 inf, %v59020
%v59032 = vxor.u32 2147483648, %v59020
%v59035 = vmul.f32 %v59032, %v59020
%v59037 = vadd.f32 1.0, %v59035
%v59038 = vlog2.pop %v59037
%v59039 = vmul.f32 0.6931472, %v59038
%v59040 = vmul.f32 -0.5, %v59035
%v59041 = vadd.f32 1.0, %v59040
%v59042 = vmul.f32 %v59041, %v59035
%v59043 = vand.u32 2147483647, %v59035
%vm59044 = vcmp.lt.f32.partialorder %v59043, 0.0004427343
%v59045 = vsel /*vm=*/%vm59044, /*on_true_vy=*/%v59042, /*on_false_vx=*/%v59039
%v59046 = vxor.u32 2147483648, %v59045
%vm59049 = vcmp.lt.f32.partialorder %v59046, 5.0
%v59054 = vsel /*vm=*/%vm59049, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v59058 = vsel /*vm=*/%vm59049, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v59062 = vsel /*vm=*/%vm59049, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v59066 = vsel /*vm=*/%vm59049, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v59070 = vsel /*vm=*/%vm59049, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v59074 = vsel /*vm=*/%vm59049, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v59078 = vsel /*vm=*/%vm59049, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v59082 = vsel /*vm=*/%vm59049, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v59086 = vsel /*vm=*/%vm59049, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v59090 = vadd.f32 -2.5, %v59046
%v59092 = vrsqrt.pop %v59046
%v59093 = vmul.f32 %v59092, %v59046
%vm59094 = vcmp.eq.f32.partialorder %v59046, inf
%v59095 = vsel /*vm=*/%vm59094, /*on_true_vy=*/%v59046, /*on_false_vx=*/%v59093
%vm59096 = vcmp.eq.f32.partialorder %v59046, 0.0
%v59097 = vand.u32 2147483648, %v59046
%v59098 = vsel /*vm=*/%vm59096, /*on_true_vy=*/%v59097, /*on_false_vx=*/%v59095
%v59101 = vadd.f32 -3.0, %v59098
%v59105 = vsel /*vm=*/%vm59049, /*on_true_vy=*/%v59090, /*on_false_vx=*/%v59101
%v59109 = vmul.f32 %v59105, %v59086
%v59113 = vadd.f32 %v59109, %v59082
%v59117 = vmul.f32 %v59113, %v59105
%v59121 = vadd.f32 %v59117, %v59078
%v59125 = vmul.f32 %v59121, %v59105
%v59129 = vadd.f32 %v59125, %v59074
%v59133 = vmul.f32 %v59129, %v59105
%v59137 = vadd.f32 %v59133, %v59070
%v59141 = vmul.f32 %v59137, %v59105
%v59145 = vadd.f32 %v59141, %v59066
%v59149 = vmul.f32 %v59145, %v59105
%v59153 = vadd.f32 %v59149, %v59062
%v59157 = vmul.f32 %v59153, %v59105
%v59161 = vadd.f32 %v59157, %v59058
%v59165 = vmul.f32 %v59161, %v59105
%v59169 = vadd.f32 %v59165, %v59054
%v59173 = vmul.f32 %v59169, %v59020
%v59177 = vsel /*vm=*/%vm59025, /*on_true_vy=*/%v59030, /*on_false_vx=*/%v59173
%v59181 = vmul.f32 1.4140625, %v59177
%v59184 = vpack.c.bf16 %v120417, %v59181
%120059 = vst [vmem:[%s280 + $0x2bc] sm:$0xf] /*vst_source=*/%v59184
%v59188 = vadd.s32 %v56419, %v3329
%v59198 = vadd.s32 %v59188, %v415
%vm59202 = vcmp.lt.u32.totalorder %v59198, %v59188
%vm59207 = vcmp.lt.u32.totalorder %v59188, %v3329
%v59212 = vadd.s32 %v56402, %v3316
%v59216 = vadd.s32 1, %v59212
%v59220 = vsel /*vm=*/%vm59207, /*on_true_vy=*/%v59216, /*on_false_vx=*/%v59212
%v59224 = vadd.s32 1, %v59220
%v59228 = vsel /*vm=*/%vm59202, /*on_true_vy=*/%v59224, /*on_false_vx=*/%v59220
%v59233 = vadd.s32 %v59228, %v10
%v59237 = vadd.s32 %v59198, %v9
%v59241 = vadd.s32 %v59237, %v59233
%v59243 = vshll.u32 %v59237, 13
%v59244 = vshrl.u32 %v59237, 19
%v59245 = vor.u32 %v59244, %v59243
%v59246 = vxor.u32 %v59245, %v59241
%v59249 = vadd.s32 %v59246, %v59241
%v59251 = vshll.u32 %v59246, 15
%v59252 = vshrl.u32 %v59246, 17
%v59253 = vor.u32 %v59252, %v59251
%v59254 = vxor.u32 %v59253, %v59249
%v59257 = vadd.s32 %v59254, %v59249
%v59259 = vshll.u32 %v59254, 26
%v59260 = vshrl.u32 %v59254, 6
%v59261 = vor.u32 %v59260, %v59259
%v59262 = vxor.u32 %v59261, %v59257
%v59265 = vadd.s32 %v59262, %v59257
%v59269 = vadd.s32 %v59265, %v9
%v59271 = vshll.u32 %v59262, 6
%v59272 = vshrl.u32 %v59262, 26
%v59273 = vor.u32 %v59272, %v59271
%v59274 = vxor.u32 %v59273, %v59265
%v59277 = vadd.s32 %v59274, %v8
%v59281 = vadd.s32 1, %v59277
%v59285 = vadd.s32 %v59281, %v59269
%v59287 = vshll.u32 %v59281, 17
%v59288 = vshrl.u32 %v59281, 15
%v59289 = vor.u32 %v59288, %v59287
%v59290 = vxor.u32 %v59289, %v59285
%v59293 = vadd.s32 %v59290, %v59285
%v59295 = vshll.u32 %v59290, 29
%v59296 = vshrl.u32 %v59290, 3
%v59297 = vor.u32 %v59296, %v59295
%v59298 = vxor.u32 %v59297, %v59293
%v59301 = vadd.s32 %v59298, %v59293
%v59303 = vshll.u32 %v59298, 16
%v59304 = vshrl.u32 %v59298, 16
%v59305 = vor.u32 %v59304, %v59303
%v59306 = vxor.u32 %v59305, %v59301
%v59309 = vadd.s32 %v59306, %v59301
%v59313 = vadd.s32 %v59309, %v8
%v59315 = vshll.u32 %v59306, 24
%v59316 = vshrl.u32 %v59306, 8
%v59317 = vor.u32 %v59316, %v59315
%v59318 = vxor.u32 %v59317, %v59309
%v59321 = vadd.s32 %v59318, %v10
%v59325 = vadd.s32 2, %v59321
%v59329 = vadd.s32 %v59325, %v59313
%v59331 = vshll.u32 %v59325, 13
%v59332 = vshrl.u32 %v59325, 19
%v59333 = vor.u32 %v59332, %v59331
%v59334 = vxor.u32 %v59333, %v59329
%v59337 = vadd.s32 %v59334, %v59329
%v59339 = vshll.u32 %v59334, 15
%v59340 = vshrl.u32 %v59334, 17
%v59341 = vor.u32 %v59340, %v59339
%v59342 = vxor.u32 %v59341, %v59337
%v59345 = vadd.s32 %v59342, %v59337
%v59347 = vshll.u32 %v59342, 26
%v59348 = vshrl.u32 %v59342, 6
%v59349 = vor.u32 %v59348, %v59347
%v59350 = vxor.u32 %v59349, %v59345
%v59353 = vadd.s32 %v59350, %v59345
%v59357 = vadd.s32 %v59353, %v10
%v59359 = vshll.u32 %v59350, 6
%v59360 = vshrl.u32 %v59350, 26
%v59361 = vor.u32 %v59360, %v59359
%v59362 = vxor.u32 %v59361, %v59353
%v59365 = vadd.s32 %v59362, %v9
%v59369 = vadd.s32 3, %v59365
%v59373 = vadd.s32 %v59369, %v59357
%v59375 = vshll.u32 %v59369, 17
%v59376 = vshrl.u32 %v59369, 15
%v59377 = vor.u32 %v59376, %v59375
%v59378 = vxor.u32 %v59377, %v59373
%v59381 = vadd.s32 %v59378, %v59373
%v59383 = vshll.u32 %v59378, 29
%v59384 = vshrl.u32 %v59378, 3
%v59385 = vor.u32 %v59384, %v59383
%v59386 = vxor.u32 %v59385, %v59381
%v59389 = vadd.s32 %v59386, %v59381
%v59391 = vshll.u32 %v59386, 16
%v59392 = vshrl.u32 %v59386, 16
%v59393 = vor.u32 %v59392, %v59391
%v59394 = vxor.u32 %v59393, %v59389
%v59397 = vadd.s32 %v59394, %v59389
%v59401 = vadd.s32 %v59397, %v9
%v59403 = vshll.u32 %v59394, 24
%v59404 = vshrl.u32 %v59394, 8
%v59405 = vor.u32 %v59404, %v59403
%v59406 = vxor.u32 %v59405, %v59397
%v59409 = vadd.s32 %v59406, %v8
%v59413 = vadd.s32 4, %v59409
%v59417 = vadd.s32 %v59413, %v59401
%v59419 = vshll.u32 %v59413, 13
%v59420 = vshrl.u32 %v59413, 19
%v59421 = vor.u32 %v59420, %v59419
%v59422 = vxor.u32 %v59421, %v59417
%v59425 = vadd.s32 %v59422, %v59417
%v59427 = vshll.u32 %v59422, 15
%v59428 = vshrl.u32 %v59422, 17
%v59429 = vor.u32 %v59428, %v59427
%v59430 = vxor.u32 %v59429, %v59425
%v59433 = vadd.s32 %v59430, %v59425
%v59435 = vshll.u32 %v59430, 26
%v59436 = vshrl.u32 %v59430, 6
%v59437 = vor.u32 %v59436, %v59435
%v59438 = vxor.u32 %v59437, %v59433
%v59441 = vadd.s32 %v59438, %v59433
%v59445 = vadd.s32 %v59441, %v8
%v59447 = vshll.u32 %v59438, 6
%v59448 = vshrl.u32 %v59438, 26
%v59449 = vor.u32 %v59448, %v59447
%v59450 = vxor.u32 %v59449, %v59441
%v59453 = vadd.s32 %v59450, %v10
%v59457 = vadd.s32 5, %v59453
%v59459 = vxor.u32 %v59457, %v59445
%v59460 = vand.u32.u8 255, %v59459
%v59461 = vand.u32 65535, %v59460
%v59462 = vshrl.u32 %v59461, 1
%v59463 = vor.u32 16256, %v59462
%v59464 = vand.u32.u16 65535, %v59463
%v120060 = vadd.low.f32.bf16 -1.0, %v59464
%v59473 = vmul.f32 2.0, %v120060
%v59477 = vadd.f32 -0.99609375, %v59473
%v59481 = vmax.f32 %v59477, -0.99609375
%v59483 = vand.u32 2147483647, %v59481
%vm59486 = vcmp.eq.f32.partialorder %v59483, 1.0
%v59491 = vmul.f32 inf, %v59481
%v59493 = vxor.u32 2147483648, %v59481
%v59496 = vmul.f32 %v59493, %v59481
%v59498 = vadd.f32 1.0, %v59496
%v59499 = vlog2.pop %v59498
%v59500 = vmul.f32 0.6931472, %v59499
%v59501 = vmul.f32 -0.5, %v59496
%v59502 = vadd.f32 1.0, %v59501
%v59503 = vmul.f32 %v59502, %v59496
%v59504 = vand.u32 2147483647, %v59496
%vm59505 = vcmp.lt.f32.partialorder %v59504, 0.0004427343
%v59506 = vsel /*vm=*/%vm59505, /*on_true_vy=*/%v59503, /*on_false_vx=*/%v59500
%v59507 = vxor.u32 2147483648, %v59506
%vm59510 = vcmp.lt.f32.partialorder %v59507, 5.0
%v59515 = vsel /*vm=*/%vm59510, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v59519 = vsel /*vm=*/%vm59510, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v59523 = vsel /*vm=*/%vm59510, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v59527 = vsel /*vm=*/%vm59510, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v59531 = vsel /*vm=*/%vm59510, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v59535 = vsel /*vm=*/%vm59510, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v59539 = vsel /*vm=*/%vm59510, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v59543 = vsel /*vm=*/%vm59510, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v59547 = vsel /*vm=*/%vm59510, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v59551 = vadd.f32 -2.5, %v59507
%v59553 = vrsqrt.pop %v59507
%v59554 = vmul.f32 %v59553, %v59507
%vm59555 = vcmp.eq.f32.partialorder %v59507, inf
%v59556 = vsel /*vm=*/%vm59555, /*on_true_vy=*/%v59507, /*on_false_vx=*/%v59554
%vm59557 = vcmp.eq.f32.partialorder %v59507, 0.0
%v59558 = vand.u32 2147483648, %v59507
%v59559 = vsel /*vm=*/%vm59557, /*on_true_vy=*/%v59558, /*on_false_vx=*/%v59556
%v59562 = vadd.f32 -3.0, %v59559
%v59566 = vsel /*vm=*/%vm59510, /*on_true_vy=*/%v59551, /*on_false_vx=*/%v59562
%v59570 = vmul.f32 %v59566, %v59547
%v59574 = vadd.f32 %v59570, %v59543
%v59578 = vmul.f32 %v59574, %v59566
%v59582 = vadd.f32 %v59578, %v59539
%v59586 = vmul.f32 %v59582, %v59566
%v59590 = vadd.f32 %v59586, %v59535
%v59594 = vmul.f32 %v59590, %v59566
%v59598 = vadd.f32 %v59594, %v59531
%v59602 = vmul.f32 %v59598, %v59566
%v59606 = vadd.f32 %v59602, %v59527
%v59610 = vmul.f32 %v59606, %v59566
%v59614 = vadd.f32 %v59610, %v59523
%v59618 = vmul.f32 %v59614, %v59566
%v59622 = vadd.f32 %v59618, %v59519
%v59626 = vmul.f32 %v59622, %v59566
%v59630 = vadd.f32 %v59626, %v59515
%v59634 = vmul.f32 %v59630, %v59481
%v59638 = vsel /*vm=*/%vm59486, /*on_true_vy=*/%v59491, /*on_false_vx=*/%v59634
%v59642 = vmul.f32 1.4140625, %v59638
%v59645 = vpack.c.bf16 %v120417, %v59642
%120061 = vst [vmem:[%s280 + $0x33c] sm:$0xf] /*vst_source=*/%v59645
%v59649 = vadd.s32 %v56419, %v3816
%v59659 = vadd.s32 %v59649, %v415
%vm59663 = vcmp.lt.u32.totalorder %v59659, %v59649
%vm59668 = vcmp.lt.u32.totalorder %v59649, %v3816
%v59673 = vadd.s32 %v56402, %v3803
%v59677 = vadd.s32 1, %v59673
%v59681 = vsel /*vm=*/%vm59668, /*on_true_vy=*/%v59677, /*on_false_vx=*/%v59673
%v59685 = vadd.s32 1, %v59681
%v59689 = vsel /*vm=*/%vm59663, /*on_true_vy=*/%v59685, /*on_false_vx=*/%v59681
%v59694 = vadd.s32 %v59689, %v10
%v59698 = vadd.s32 %v59659, %v9
%v59702 = vadd.s32 %v59698, %v59694
%v59704 = vshll.u32 %v59698, 13
%v59705 = vshrl.u32 %v59698, 19
%v59706 = vor.u32 %v59705, %v59704
%v59707 = vxor.u32 %v59706, %v59702
%v59710 = vadd.s32 %v59707, %v59702
%v59712 = vshll.u32 %v59707, 15
%v59713 = vshrl.u32 %v59707, 17
%v59714 = vor.u32 %v59713, %v59712
%v59715 = vxor.u32 %v59714, %v59710
%v59718 = vadd.s32 %v59715, %v59710
%v59720 = vshll.u32 %v59715, 26
%v59721 = vshrl.u32 %v59715, 6
%v59722 = vor.u32 %v59721, %v59720
%v59723 = vxor.u32 %v59722, %v59718
%v59726 = vadd.s32 %v59723, %v59718
%v59730 = vadd.s32 %v59726, %v9
%v59732 = vshll.u32 %v59723, 6
%v59733 = vshrl.u32 %v59723, 26
%v59734 = vor.u32 %v59733, %v59732
%v59735 = vxor.u32 %v59734, %v59726
%v59738 = vadd.s32 %v59735, %v8
%v59742 = vadd.s32 1, %v59738
%v59746 = vadd.s32 %v59742, %v59730
%v59748 = vshll.u32 %v59742, 17
%v59749 = vshrl.u32 %v59742, 15
%v59750 = vor.u32 %v59749, %v59748
%v59751 = vxor.u32 %v59750, %v59746
%v59754 = vadd.s32 %v59751, %v59746
%v59756 = vshll.u32 %v59751, 29
%v59757 = vshrl.u32 %v59751, 3
%v59758 = vor.u32 %v59757, %v59756
%v59759 = vxor.u32 %v59758, %v59754
%v59762 = vadd.s32 %v59759, %v59754
%v59764 = vshll.u32 %v59759, 16
%v59765 = vshrl.u32 %v59759, 16
%v59766 = vor.u32 %v59765, %v59764
%v59767 = vxor.u32 %v59766, %v59762
%v59770 = vadd.s32 %v59767, %v59762
%v59774 = vadd.s32 %v59770, %v8
%v59776 = vshll.u32 %v59767, 24
%v59777 = vshrl.u32 %v59767, 8
%v59778 = vor.u32 %v59777, %v59776
%v59779 = vxor.u32 %v59778, %v59770
%v59782 = vadd.s32 %v59779, %v10
%v59786 = vadd.s32 2, %v59782
%v59790 = vadd.s32 %v59786, %v59774
%v59792 = vshll.u32 %v59786, 13
%v59793 = vshrl.u32 %v59786, 19
%v59794 = vor.u32 %v59793, %v59792
%v59795 = vxor.u32 %v59794, %v59790
%v59798 = vadd.s32 %v59795, %v59790
%v59800 = vshll.u32 %v59795, 15
%v59801 = vshrl.u32 %v59795, 17
%v59802 = vor.u32 %v59801, %v59800
%v59803 = vxor.u32 %v59802, %v59798
%v59806 = vadd.s32 %v59803, %v59798
%v59808 = vshll.u32 %v59803, 26
%v59809 = vshrl.u32 %v59803, 6
%v59810 = vor.u32 %v59809, %v59808
%v59811 = vxor.u32 %v59810, %v59806
%v59814 = vadd.s32 %v59811, %v59806
%v59818 = vadd.s32 %v59814, %v10
%v59820 = vshll.u32 %v59811, 6
%v59821 = vshrl.u32 %v59811, 26
%v59822 = vor.u32 %v59821, %v59820
%v59823 = vxor.u32 %v59822, %v59814
%v59826 = vadd.s32 %v59823, %v9
%v59830 = vadd.s32 3, %v59826
%v59834 = vadd.s32 %v59830, %v59818
%v59836 = vshll.u32 %v59830, 17
%v59837 = vshrl.u32 %v59830, 15
%v59838 = vor.u32 %v59837, %v59836
%v59839 = vxor.u32 %v59838, %v59834
%v59842 = vadd.s32 %v59839, %v59834
%v59844 = vshll.u32 %v59839, 29
%v59845 = vshrl.u32 %v59839, 3
%v59846 = vor.u32 %v59845, %v59844
%v59847 = vxor.u32 %v59846, %v59842
%v59850 = vadd.s32 %v59847, %v59842
%v59852 = vshll.u32 %v59847, 16
%v59853 = vshrl.u32 %v59847, 16
%v59854 = vor.u32 %v59853, %v59852
%v59855 = vxor.u32 %v59854, %v59850
%v59858 = vadd.s32 %v59855, %v59850
%v59862 = vadd.s32 %v59858, %v9
%v59864 = vshll.u32 %v59855, 24
%v59865 = vshrl.u32 %v59855, 8
%v59866 = vor.u32 %v59865, %v59864
%v59867 = vxor.u32 %v59866, %v59858
%v59870 = vadd.s32 %v59867, %v8
%v59874 = vadd.s32 4, %v59870
%v59878 = vadd.s32 %v59874, %v59862
%v59880 = vshll.u32 %v59874, 13
%v59881 = vshrl.u32 %v59874, 19
%v59882 = vor.u32 %v59881, %v59880
%v59883 = vxor.u32 %v59882, %v59878
%v59886 = vadd.s32 %v59883, %v59878
%v59888 = vshll.u32 %v59883, 15
%v59889 = vshrl.u32 %v59883, 17
%v59890 = vor.u32 %v59889, %v59888
%v59891 = vxor.u32 %v59890, %v59886
%v59894 = vadd.s32 %v59891, %v59886
%v59896 = vshll.u32 %v59891, 26
%v59897 = vshrl.u32 %v59891, 6
%v59898 = vor.u32 %v59897, %v59896
%v59899 = vxor.u32 %v59898, %v59894
%v59902 = vadd.s32 %v59899, %v59894
%v59906 = vadd.s32 %v59902, %v8
%v59908 = vshll.u32 %v59899, 6
%v59909 = vshrl.u32 %v59899, 26
%v59910 = vor.u32 %v59909, %v59908
%v59911 = vxor.u32 %v59910, %v59902
%v59914 = vadd.s32 %v59911, %v10
%v59918 = vadd.s32 5, %v59914
%v59920 = vxor.u32 %v59918, %v59906
%v59921 = vand.u32.u8 255, %v59920
%v59922 = vand.u32 65535, %v59921
%v59923 = vshrl.u32 %v59922, 1
%v59924 = vor.u32 16256, %v59923
%v59925 = vand.u32.u16 65535, %v59924
%v120062 = vadd.low.f32.bf16 -1.0, %v59925
%v59934 = vmul.f32 2.0, %v120062
%v59938 = vadd.f32 -0.99609375, %v59934
%v59942 = vmax.f32 %v59938, -0.99609375
%v59944 = vand.u32 2147483647, %v59942
%vm59947 = vcmp.eq.f32.partialorder %v59944, 1.0
%v59952 = vmul.f32 inf, %v59942
%v59954 = vxor.u32 2147483648, %v59942
%v59957 = vmul.f32 %v59954, %v59942
%v59959 = vadd.f32 1.0, %v59957
%v59960 = vlog2.pop %v59959
%v59961 = vmul.f32 0.6931472, %v59960
%v59962 = vmul.f32 -0.5, %v59957
%v59963 = vadd.f32 1.0, %v59962
%v59964 = vmul.f32 %v59963, %v59957
%v59965 = vand.u32 2147483647, %v59957
%vm59966 = vcmp.lt.f32.partialorder %v59965, 0.0004427343
%v59967 = vsel /*vm=*/%vm59966, /*on_true_vy=*/%v59964, /*on_false_vx=*/%v59961
%v59968 = vxor.u32 2147483648, %v59967
%vm59971 = vcmp.lt.f32.partialorder %v59968, 5.0
%v59976 = vsel /*vm=*/%vm59971, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v59980 = vsel /*vm=*/%vm59971, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v59984 = vsel /*vm=*/%vm59971, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v59988 = vsel /*vm=*/%vm59971, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v59992 = vsel /*vm=*/%vm59971, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v59996 = vsel /*vm=*/%vm59971, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v60000 = vsel /*vm=*/%vm59971, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v60004 = vsel /*vm=*/%vm59971, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v60008 = vsel /*vm=*/%vm59971, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v60012 = vadd.f32 -2.5, %v59968
%v60014 = vrsqrt.pop %v59968
%v60015 = vmul.f32 %v60014, %v59968
%vm60016 = vcmp.eq.f32.partialorder %v59968, inf
%v60017 = vsel /*vm=*/%vm60016, /*on_true_vy=*/%v59968, /*on_false_vx=*/%v60015
%vm60018 = vcmp.eq.f32.partialorder %v59968, 0.0
%v60019 = vand.u32 2147483648, %v59968
%v60020 = vsel /*vm=*/%vm60018, /*on_true_vy=*/%v60019, /*on_false_vx=*/%v60017
%v60023 = vadd.f32 -3.0, %v60020
%v60027 = vsel /*vm=*/%vm59971, /*on_true_vy=*/%v60012, /*on_false_vx=*/%v60023
%v60031 = vmul.f32 %v60027, %v60008
%v60035 = vadd.f32 %v60031, %v60004
%v60039 = vmul.f32 %v60035, %v60027
%v60043 = vadd.f32 %v60039, %v60000
%v60047 = vmul.f32 %v60043, %v60027
%v60051 = vadd.f32 %v60047, %v59996
%v60055 = vmul.f32 %v60051, %v60027
%v60059 = vadd.f32 %v60055, %v59992
%v60063 = vmul.f32 %v60059, %v60027
%v60067 = vadd.f32 %v60063, %v59988
%v60071 = vmul.f32 %v60067, %v60027
%v60075 = vadd.f32 %v60071, %v59984
%v60079 = vmul.f32 %v60075, %v60027
%v60083 = vadd.f32 %v60079, %v59980
%v60087 = vmul.f32 %v60083, %v60027
%v60091 = vadd.f32 %v60087, %v59976
%v60095 = vmul.f32 %v60091, %v59942
%v60099 = vsel /*vm=*/%vm59947, /*on_true_vy=*/%v59952, /*on_false_vx=*/%v60095
%v60103 = vmul.f32 1.4140625, %v60099
%v60106 = vpack.c.bf16 %v120417, %v60103
%120063 = vst [vmem:[%s280 + $0x3bc] sm:$0xf] /*vst_source=*/%v60106
%v60144 = vadd.s32 %v60141, %v408
%v60154 = vadd.s32 %v60144, %v415
%vm60158 = vcmp.lt.u32.totalorder %v60154, %v60144
%vm60163 = vcmp.lt.u32.totalorder %v60144, %v408
%v60168 = vadd.s32 %v60124, %v380
%v60172 = vadd.s32 1, %v60168
%v60176 = vsel /*vm=*/%vm60163, /*on_true_vy=*/%v60172, /*on_false_vx=*/%v60168
%v60180 = vadd.s32 1, %v60176
%v60184 = vsel /*vm=*/%vm60158, /*on_true_vy=*/%v60180, /*on_false_vx=*/%v60176
%v60189 = vadd.s32 %v60184, %v10
%v60193 = vadd.s32 %v60154, %v9
%v60197 = vadd.s32 %v60193, %v60189
%v60199 = vshll.u32 %v60193, 13
%v60200 = vshrl.u32 %v60193, 19
%v60201 = vor.u32 %v60200, %v60199
%v60202 = vxor.u32 %v60201, %v60197
%v60205 = vadd.s32 %v60202, %v60197
%v60207 = vshll.u32 %v60202, 15
%v60208 = vshrl.u32 %v60202, 17
%v60209 = vor.u32 %v60208, %v60207
%v60210 = vxor.u32 %v60209, %v60205
%v60213 = vadd.s32 %v60210, %v60205
%v60215 = vshll.u32 %v60210, 26
%v60216 = vshrl.u32 %v60210, 6
%v60217 = vor.u32 %v60216, %v60215
%v60218 = vxor.u32 %v60217, %v60213
%v60221 = vadd.s32 %v60218, %v60213
%v60225 = vadd.s32 %v60221, %v9
%v60227 = vshll.u32 %v60218, 6
%v60228 = vshrl.u32 %v60218, 26
%v60229 = vor.u32 %v60228, %v60227
%v60230 = vxor.u32 %v60229, %v60221
%v60233 = vadd.s32 %v60230, %v8
%v60237 = vadd.s32 1, %v60233
%v60241 = vadd.s32 %v60237, %v60225
%v60243 = vshll.u32 %v60237, 17
%v60244 = vshrl.u32 %v60237, 15
%v60245 = vor.u32 %v60244, %v60243
%v60246 = vxor.u32 %v60245, %v60241
%v60249 = vadd.s32 %v60246, %v60241
%v60251 = vshll.u32 %v60246, 29
%v60252 = vshrl.u32 %v60246, 3
%v60253 = vor.u32 %v60252, %v60251
%v60254 = vxor.u32 %v60253, %v60249
%v60257 = vadd.s32 %v60254, %v60249
%v60259 = vshll.u32 %v60254, 16
%v60260 = vshrl.u32 %v60254, 16
%v60261 = vor.u32 %v60260, %v60259
%v60262 = vxor.u32 %v60261, %v60257
%v60265 = vadd.s32 %v60262, %v60257
%v60269 = vadd.s32 %v60265, %v8
%v60271 = vshll.u32 %v60262, 24
%v60272 = vshrl.u32 %v60262, 8
%v60273 = vor.u32 %v60272, %v60271
%v60274 = vxor.u32 %v60273, %v60265
%v60277 = vadd.s32 %v60274, %v10
%v60281 = vadd.s32 2, %v60277
%v60285 = vadd.s32 %v60281, %v60269
%v60287 = vshll.u32 %v60281, 13
%v60288 = vshrl.u32 %v60281, 19
%v60289 = vor.u32 %v60288, %v60287
%v60290 = vxor.u32 %v60289, %v60285
%v60293 = vadd.s32 %v60290, %v60285
%v60295 = vshll.u32 %v60290, 15
%v60296 = vshrl.u32 %v60290, 17
%v60297 = vor.u32 %v60296, %v60295
%v60298 = vxor.u32 %v60297, %v60293
%v60301 = vadd.s32 %v60298, %v60293
%v60303 = vshll.u32 %v60298, 26
%v60304 = vshrl.u32 %v60298, 6
%v60305 = vor.u32 %v60304, %v60303
%v60306 = vxor.u32 %v60305, %v60301
%v60309 = vadd.s32 %v60306, %v60301
%v60313 = vadd.s32 %v60309, %v10
%v60315 = vshll.u32 %v60306, 6
%v60316 = vshrl.u32 %v60306, 26
%v60317 = vor.u32 %v60316, %v60315
%v60318 = vxor.u32 %v60317, %v60309
%v60321 = vadd.s32 %v60318, %v9
%v60325 = vadd.s32 3, %v60321
%v60329 = vadd.s32 %v60325, %v60313
%v60331 = vshll.u32 %v60325, 17
%v60332 = vshrl.u32 %v60325, 15
%v60333 = vor.u32 %v60332, %v60331
%v60334 = vxor.u32 %v60333, %v60329
%v60337 = vadd.s32 %v60334, %v60329
%v60339 = vshll.u32 %v60334, 29
%v60340 = vshrl.u32 %v60334, 3
%v60341 = vor.u32 %v60340, %v60339
%v60342 = vxor.u32 %v60341, %v60337
%v60345 = vadd.s32 %v60342, %v60337
%v60347 = vshll.u32 %v60342, 16
%v60348 = vshrl.u32 %v60342, 16
%v60349 = vor.u32 %v60348, %v60347
%v60350 = vxor.u32 %v60349, %v60345
%v60353 = vadd.s32 %v60350, %v60345
%v60357 = vadd.s32 %v60353, %v9
%v60359 = vshll.u32 %v60350, 24
%v60360 = vshrl.u32 %v60350, 8
%v60361 = vor.u32 %v60360, %v60359
%v60362 = vxor.u32 %v60361, %v60353
%v60365 = vadd.s32 %v60362, %v8
%v60369 = vadd.s32 4, %v60365
%v60373 = vadd.s32 %v60369, %v60357
%v60375 = vshll.u32 %v60369, 13
%v60376 = vshrl.u32 %v60369, 19
%v60377 = vor.u32 %v60376, %v60375
%v60378 = vxor.u32 %v60377, %v60373
%v60381 = vadd.s32 %v60378, %v60373
%v60383 = vshll.u32 %v60378, 15
%v60384 = vshrl.u32 %v60378, 17
%v60385 = vor.u32 %v60384, %v60383
%v60386 = vxor.u32 %v60385, %v60381
%v60389 = vadd.s32 %v60386, %v60381
%v60391 = vshll.u32 %v60386, 26
%v60392 = vshrl.u32 %v60386, 6
%v60393 = vor.u32 %v60392, %v60391
%v60394 = vxor.u32 %v60393, %v60389
%v60397 = vadd.s32 %v60394, %v60389
%v60401 = vadd.s32 %v60397, %v8
%v60403 = vshll.u32 %v60394, 6
%v60404 = vshrl.u32 %v60394, 26
%v60405 = vor.u32 %v60404, %v60403
%v60406 = vxor.u32 %v60405, %v60397
%v60409 = vadd.s32 %v60406, %v10
%v60413 = vadd.s32 5, %v60409
%v60415 = vxor.u32 %v60413, %v60401
%v60416 = vand.u32.u8 255, %v60415
%v60417 = vand.u32 65535, %v60416
%v60418 = vshrl.u32 %v60417, 1
%v60419 = vor.u32 16256, %v60418
%v60420 = vand.u32.u16 65535, %v60419
%v120068 = vadd.low.f32.bf16 -1.0, %v60420
%v60429 = vmul.f32 2.0, %v120068
%v60433 = vadd.f32 -0.99609375, %v60429
%v60437 = vmax.f32 %v60433, -0.99609375
%v60439 = vand.u32 2147483647, %v60437
%vm60442 = vcmp.eq.f32.partialorder %v60439, 1.0
%v60447 = vmul.f32 inf, %v60437
%v60449 = vxor.u32 2147483648, %v60437
%v60452 = vmul.f32 %v60449, %v60437
%v60454 = vadd.f32 1.0, %v60452
%v60455 = vlog2.pop %v60454
%v60456 = vmul.f32 0.6931472, %v60455
%v60457 = vmul.f32 -0.5, %v60452
%v60458 = vadd.f32 1.0, %v60457
%v60459 = vmul.f32 %v60458, %v60452
%v60460 = vand.u32 2147483647, %v60452
%vm60461 = vcmp.lt.f32.partialorder %v60460, 0.0004427343
%v60462 = vsel /*vm=*/%vm60461, /*on_true_vy=*/%v60459, /*on_false_vx=*/%v60456
%v60463 = vxor.u32 2147483648, %v60462
%vm60466 = vcmp.lt.f32.partialorder %v60463, 5.0
%v60471 = vsel /*vm=*/%vm60466, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v60475 = vsel /*vm=*/%vm60466, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v60479 = vsel /*vm=*/%vm60466, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v60483 = vsel /*vm=*/%vm60466, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v60487 = vsel /*vm=*/%vm60466, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v60491 = vsel /*vm=*/%vm60466, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v60495 = vsel /*vm=*/%vm60466, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v60499 = vsel /*vm=*/%vm60466, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v60503 = vsel /*vm=*/%vm60466, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v60507 = vadd.f32 -2.5, %v60463
%v60509 = vrsqrt.pop %v60463
%v60510 = vmul.f32 %v60509, %v60463
%vm60511 = vcmp.eq.f32.partialorder %v60463, inf
%v60512 = vsel /*vm=*/%vm60511, /*on_true_vy=*/%v60463, /*on_false_vx=*/%v60510
%vm60513 = vcmp.eq.f32.partialorder %v60463, 0.0
%v60514 = vand.u32 2147483648, %v60463
%v60515 = vsel /*vm=*/%vm60513, /*on_true_vy=*/%v60514, /*on_false_vx=*/%v60512
%v60518 = vadd.f32 -3.0, %v60515
%v60522 = vsel /*vm=*/%vm60466, /*on_true_vy=*/%v60507, /*on_false_vx=*/%v60518
%v60526 = vmul.f32 %v60522, %v60503
%v60530 = vadd.f32 %v60526, %v60499
%v60534 = vmul.f32 %v60530, %v60522
%v60538 = vadd.f32 %v60534, %v60495
%v60542 = vmul.f32 %v60538, %v60522
%v60546 = vadd.f32 %v60542, %v60491
%v60550 = vmul.f32 %v60546, %v60522
%v60554 = vadd.f32 %v60550, %v60487
%v60558 = vmul.f32 %v60554, %v60522
%v60562 = vadd.f32 %v60558, %v60483
%v60566 = vmul.f32 %v60562, %v60522
%v60570 = vadd.f32 %v60566, %v60479
%v60574 = vmul.f32 %v60570, %v60522
%v60578 = vadd.f32 %v60574, %v60475
%v60582 = vmul.f32 %v60578, %v60522
%v60586 = vadd.f32 %v60582, %v60471
%v60590 = vmul.f32 %v60586, %v60437
%v60594 = vsel /*vm=*/%vm60442, /*on_true_vy=*/%v60447, /*on_false_vx=*/%v60590
%v60598 = vmul.f32 1.4140625, %v60594
%v60601 = vpack.c.bf16 %v120417, %v60598
%120069 = vst [vmem:[%s280 + $0x40] sm:$0xf] /*vst_source=*/%v60601
%v60605 = vadd.s32 %v60141, %v894
%v60615 = vadd.s32 %v60605, %v415
%vm60619 = vcmp.lt.u32.totalorder %v60615, %v60605
%vm60624 = vcmp.lt.u32.totalorder %v60605, %v894
%v60629 = vadd.s32 %v60124, %v881
%v60633 = vadd.s32 1, %v60629
%v60637 = vsel /*vm=*/%vm60624, /*on_true_vy=*/%v60633, /*on_false_vx=*/%v60629
%v60641 = vadd.s32 1, %v60637
%v60645 = vsel /*vm=*/%vm60619, /*on_true_vy=*/%v60641, /*on_false_vx=*/%v60637
%v60650 = vadd.s32 %v60645, %v10
%v60654 = vadd.s32 %v60615, %v9
%v60658 = vadd.s32 %v60654, %v60650
%v60660 = vshll.u32 %v60654, 13
%v60661 = vshrl.u32 %v60654, 19
%v60662 = vor.u32 %v60661, %v60660
%v60663 = vxor.u32 %v60662, %v60658
%v60666 = vadd.s32 %v60663, %v60658
%v60668 = vshll.u32 %v60663, 15
%v60669 = vshrl.u32 %v60663, 17
%v60670 = vor.u32 %v60669, %v60668
%v60671 = vxor.u32 %v60670, %v60666
%v60674 = vadd.s32 %v60671, %v60666
%v60676 = vshll.u32 %v60671, 26
%v60677 = vshrl.u32 %v60671, 6
%v60678 = vor.u32 %v60677, %v60676
%v60679 = vxor.u32 %v60678, %v60674
%v60682 = vadd.s32 %v60679, %v60674
%v60686 = vadd.s32 %v60682, %v9
%v60688 = vshll.u32 %v60679, 6
%v60689 = vshrl.u32 %v60679, 26
%v60690 = vor.u32 %v60689, %v60688
%v60691 = vxor.u32 %v60690, %v60682
%v60694 = vadd.s32 %v60691, %v8
%v60698 = vadd.s32 1, %v60694
%v60702 = vadd.s32 %v60698, %v60686
%v60704 = vshll.u32 %v60698, 17
%v60705 = vshrl.u32 %v60698, 15
%v60706 = vor.u32 %v60705, %v60704
%v60707 = vxor.u32 %v60706, %v60702
%v60710 = vadd.s32 %v60707, %v60702
%v60712 = vshll.u32 %v60707, 29
%v60713 = vshrl.u32 %v60707, 3
%v60714 = vor.u32 %v60713, %v60712
%v60715 = vxor.u32 %v60714, %v60710
%v60718 = vadd.s32 %v60715, %v60710
%v60720 = vshll.u32 %v60715, 16
%v60721 = vshrl.u32 %v60715, 16
%v60722 = vor.u32 %v60721, %v60720
%v60723 = vxor.u32 %v60722, %v60718
%v60726 = vadd.s32 %v60723, %v60718
%v60730 = vadd.s32 %v60726, %v8
%v60732 = vshll.u32 %v60723, 24
%v60733 = vshrl.u32 %v60723, 8
%v60734 = vor.u32 %v60733, %v60732
%v60735 = vxor.u32 %v60734, %v60726
%v60738 = vadd.s32 %v60735, %v10
%v60742 = vadd.s32 2, %v60738
%v60746 = vadd.s32 %v60742, %v60730
%v60748 = vshll.u32 %v60742, 13
%v60749 = vshrl.u32 %v60742, 19
%v60750 = vor.u32 %v60749, %v60748
%v60751 = vxor.u32 %v60750, %v60746
%v60754 = vadd.s32 %v60751, %v60746
%v60756 = vshll.u32 %v60751, 15
%v60757 = vshrl.u32 %v60751, 17
%v60758 = vor.u32 %v60757, %v60756
%v60759 = vxor.u32 %v60758, %v60754
%v60762 = vadd.s32 %v60759, %v60754
%v60764 = vshll.u32 %v60759, 26
%v60765 = vshrl.u32 %v60759, 6
%v60766 = vor.u32 %v60765, %v60764
%v60767 = vxor.u32 %v60766, %v60762
%v60770 = vadd.s32 %v60767, %v60762
%v60774 = vadd.s32 %v60770, %v10
%v60776 = vshll.u32 %v60767, 6
%v60777 = vshrl.u32 %v60767, 26
%v60778 = vor.u32 %v60777, %v60776
%v60779 = vxor.u32 %v60778, %v60770
%v60782 = vadd.s32 %v60779, %v9
%v60786 = vadd.s32 3, %v60782
%v60790 = vadd.s32 %v60786, %v60774
%v60792 = vshll.u32 %v60786, 17
%v60793 = vshrl.u32 %v60786, 15
%v60794 = vor.u32 %v60793, %v60792
%v60795 = vxor.u32 %v60794, %v60790
%v60798 = vadd.s32 %v60795, %v60790
%v60800 = vshll.u32 %v60795, 29
%v60801 = vshrl.u32 %v60795, 3
%v60802 = vor.u32 %v60801, %v60800
%v60803 = vxor.u32 %v60802, %v60798
%v60806 = vadd.s32 %v60803, %v60798
%v60808 = vshll.u32 %v60803, 16
%v60809 = vshrl.u32 %v60803, 16
%v60810 = vor.u32 %v60809, %v60808
%v60811 = vxor.u32 %v60810, %v60806
%v60814 = vadd.s32 %v60811, %v60806
%v60818 = vadd.s32 %v60814, %v9
%v60820 = vshll.u32 %v60811, 24
%v60821 = vshrl.u32 %v60811, 8
%v60822 = vor.u32 %v60821, %v60820
%v60823 = vxor.u32 %v60822, %v60814
%v60826 = vadd.s32 %v60823, %v8
%v60830 = vadd.s32 4, %v60826
%v60834 = vadd.s32 %v60830, %v60818
%v60836 = vshll.u32 %v60830, 13
%v60837 = vshrl.u32 %v60830, 19
%v60838 = vor.u32 %v60837, %v60836
%v60839 = vxor.u32 %v60838, %v60834
%v60842 = vadd.s32 %v60839, %v60834
%v60844 = vshll.u32 %v60839, 15
%v60845 = vshrl.u32 %v60839, 17
%v60846 = vor.u32 %v60845, %v60844
%v60847 = vxor.u32 %v60846, %v60842
%v60850 = vadd.s32 %v60847, %v60842
%v60852 = vshll.u32 %v60847, 26
%v60853 = vshrl.u32 %v60847, 6
%v60854 = vor.u32 %v60853, %v60852
%v60855 = vxor.u32 %v60854, %v60850
%v60858 = vadd.s32 %v60855, %v60850
%v60862 = vadd.s32 %v60858, %v8
%v60864 = vshll.u32 %v60855, 6
%v60865 = vshrl.u32 %v60855, 26
%v60866 = vor.u32 %v60865, %v60864
%v60867 = vxor.u32 %v60866, %v60858
%v60870 = vadd.s32 %v60867, %v10
%v60874 = vadd.s32 5, %v60870
%v60876 = vxor.u32 %v60874, %v60862
%v60877 = vand.u32.u8 255, %v60876
%v60878 = vand.u32 65535, %v60877
%v60879 = vshrl.u32 %v60878, 1
%v60880 = vor.u32 16256, %v60879
%v60881 = vand.u32.u16 65535, %v60880
%v120070 = vadd.low.f32.bf16 -1.0, %v60881
%v60890 = vmul.f32 2.0, %v120070
%v60894 = vadd.f32 -0.99609375, %v60890
%v60898 = vmax.f32 %v60894, -0.99609375
%v60900 = vand.u32 2147483647, %v60898
%vm60903 = vcmp.eq.f32.partialorder %v60900, 1.0
%v60908 = vmul.f32 inf, %v60898
%v60910 = vxor.u32 2147483648, %v60898
%v60913 = vmul.f32 %v60910, %v60898
%v60915 = vadd.f32 1.0, %v60913
%v60916 = vlog2.pop %v60915
%v60917 = vmul.f32 0.6931472, %v60916
%v60918 = vmul.f32 -0.5, %v60913
%v60919 = vadd.f32 1.0, %v60918
%v60920 = vmul.f32 %v60919, %v60913
%v60921 = vand.u32 2147483647, %v60913
%vm60922 = vcmp.lt.f32.partialorder %v60921, 0.0004427343
%v60923 = vsel /*vm=*/%vm60922, /*on_true_vy=*/%v60920, /*on_false_vx=*/%v60917
%v60924 = vxor.u32 2147483648, %v60923
%vm60927 = vcmp.lt.f32.partialorder %v60924, 5.0
%v60932 = vsel /*vm=*/%vm60927, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v60936 = vsel /*vm=*/%vm60927, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v60940 = vsel /*vm=*/%vm60927, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v60944 = vsel /*vm=*/%vm60927, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v60948 = vsel /*vm=*/%vm60927, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v60952 = vsel /*vm=*/%vm60927, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v60956 = vsel /*vm=*/%vm60927, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v60960 = vsel /*vm=*/%vm60927, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v60964 = vsel /*vm=*/%vm60927, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v60968 = vadd.f32 -2.5, %v60924
%v60970 = vrsqrt.pop %v60924
%v60971 = vmul.f32 %v60970, %v60924
%vm60972 = vcmp.eq.f32.partialorder %v60924, inf
%v60973 = vsel /*vm=*/%vm60972, /*on_true_vy=*/%v60924, /*on_false_vx=*/%v60971
%vm60974 = vcmp.eq.f32.partialorder %v60924, 0.0
%v60975 = vand.u32 2147483648, %v60924
%v60976 = vsel /*vm=*/%vm60974, /*on_true_vy=*/%v60975, /*on_false_vx=*/%v60973
%v60979 = vadd.f32 -3.0, %v60976
%v60983 = vsel /*vm=*/%vm60927, /*on_true_vy=*/%v60968, /*on_false_vx=*/%v60979
%v60987 = vmul.f32 %v60983, %v60964
%v60991 = vadd.f32 %v60987, %v60960
%v60995 = vmul.f32 %v60991, %v60983
%v60999 = vadd.f32 %v60995, %v60956
%v61003 = vmul.f32 %v60999, %v60983
%v61007 = vadd.f32 %v61003, %v60952
%v61011 = vmul.f32 %v61007, %v60983
%v61015 = vadd.f32 %v61011, %v60948
%v61019 = vmul.f32 %v61015, %v60983
%v61023 = vadd.f32 %v61019, %v60944
%v61027 = vmul.f32 %v61023, %v60983
%v61031 = vadd.f32 %v61027, %v60940
%v61035 = vmul.f32 %v61031, %v60983
%v61039 = vadd.f32 %v61035, %v60936
%v61043 = vmul.f32 %v61039, %v60983
%v61047 = vadd.f32 %v61043, %v60932
%v61051 = vmul.f32 %v61047, %v60898
%v61055 = vsel /*vm=*/%vm60903, /*on_true_vy=*/%v60908, /*on_false_vx=*/%v61051
%v61059 = vmul.f32 1.4140625, %v61055
%v61062 = vpack.c.bf16 %v120417, %v61059
%120071 = vst [vmem:[%s280 + $0xc0] sm:$0xf] /*vst_source=*/%v61062
%v61066 = vadd.s32 %v60141, %v1381
%v61076 = vadd.s32 %v61066, %v415
%vm61080 = vcmp.lt.u32.totalorder %v61076, %v61066
%vm61085 = vcmp.lt.u32.totalorder %v61066, %v1381
%v61090 = vadd.s32 %v60124, %v1368
%v61094 = vadd.s32 1, %v61090
%v61098 = vsel /*vm=*/%vm61085, /*on_true_vy=*/%v61094, /*on_false_vx=*/%v61090
%v61102 = vadd.s32 1, %v61098
%v61106 = vsel /*vm=*/%vm61080, /*on_true_vy=*/%v61102, /*on_false_vx=*/%v61098
%v61111 = vadd.s32 %v61106, %v10
%v61115 = vadd.s32 %v61076, %v9
%v61119 = vadd.s32 %v61115, %v61111
%v61121 = vshll.u32 %v61115, 13
%v61122 = vshrl.u32 %v61115, 19
%v61123 = vor.u32 %v61122, %v61121
%v61124 = vxor.u32 %v61123, %v61119
%v61127 = vadd.s32 %v61124, %v61119
%v61129 = vshll.u32 %v61124, 15
%v61130 = vshrl.u32 %v61124, 17
%v61131 = vor.u32 %v61130, %v61129
%v61132 = vxor.u32 %v61131, %v61127
%v61135 = vadd.s32 %v61132, %v61127
%v61137 = vshll.u32 %v61132, 26
%v61138 = vshrl.u32 %v61132, 6
%v61139 = vor.u32 %v61138, %v61137
%v61140 = vxor.u32 %v61139, %v61135
%v61143 = vadd.s32 %v61140, %v61135
%v61147 = vadd.s32 %v61143, %v9
%v61149 = vshll.u32 %v61140, 6
%v61150 = vshrl.u32 %v61140, 26
%v61151 = vor.u32 %v61150, %v61149
%v61152 = vxor.u32 %v61151, %v61143
%v61155 = vadd.s32 %v61152, %v8
%v61159 = vadd.s32 1, %v61155
%v61163 = vadd.s32 %v61159, %v61147
%v61165 = vshll.u32 %v61159, 17
%v61166 = vshrl.u32 %v61159, 15
%v61167 = vor.u32 %v61166, %v61165
%v61168 = vxor.u32 %v61167, %v61163
%v61171 = vadd.s32 %v61168, %v61163
%v61173 = vshll.u32 %v61168, 29
%v61174 = vshrl.u32 %v61168, 3
%v61175 = vor.u32 %v61174, %v61173
%v61176 = vxor.u32 %v61175, %v61171
%v61179 = vadd.s32 %v61176, %v61171
%v61181 = vshll.u32 %v61176, 16
%v61182 = vshrl.u32 %v61176, 16
%v61183 = vor.u32 %v61182, %v61181
%v61184 = vxor.u32 %v61183, %v61179
%v61187 = vadd.s32 %v61184, %v61179
%v61191 = vadd.s32 %v61187, %v8
%v61193 = vshll.u32 %v61184, 24
%v61194 = vshrl.u32 %v61184, 8
%v61195 = vor.u32 %v61194, %v61193
%v61196 = vxor.u32 %v61195, %v61187
%v61199 = vadd.s32 %v61196, %v10
%v61203 = vadd.s32 2, %v61199
%v61207 = vadd.s32 %v61203, %v61191
%v61209 = vshll.u32 %v61203, 13
%v61210 = vshrl.u32 %v61203, 19
%v61211 = vor.u32 %v61210, %v61209
%v61212 = vxor.u32 %v61211, %v61207
%v61215 = vadd.s32 %v61212, %v61207
%v61217 = vshll.u32 %v61212, 15
%v61218 = vshrl.u32 %v61212, 17
%v61219 = vor.u32 %v61218, %v61217
%v61220 = vxor.u32 %v61219, %v61215
%v61223 = vadd.s32 %v61220, %v61215
%v61225 = vshll.u32 %v61220, 26
%v61226 = vshrl.u32 %v61220, 6
%v61227 = vor.u32 %v61226, %v61225
%v61228 = vxor.u32 %v61227, %v61223
%v61231 = vadd.s32 %v61228, %v61223
%v61235 = vadd.s32 %v61231, %v10
%v61237 = vshll.u32 %v61228, 6
%v61238 = vshrl.u32 %v61228, 26
%v61239 = vor.u32 %v61238, %v61237
%v61240 = vxor.u32 %v61239, %v61231
%v61243 = vadd.s32 %v61240, %v9
%v61247 = vadd.s32 3, %v61243
%v61251 = vadd.s32 %v61247, %v61235
%v61253 = vshll.u32 %v61247, 17
%v61254 = vshrl.u32 %v61247, 15
%v61255 = vor.u32 %v61254, %v61253
%v61256 = vxor.u32 %v61255, %v61251
%v61259 = vadd.s32 %v61256, %v61251
%v61261 = vshll.u32 %v61256, 29
%v61262 = vshrl.u32 %v61256, 3
%v61263 = vor.u32 %v61262, %v61261
%v61264 = vxor.u32 %v61263, %v61259
%v61267 = vadd.s32 %v61264, %v61259
%v61269 = vshll.u32 %v61264, 16
%v61270 = vshrl.u32 %v61264, 16
%v61271 = vor.u32 %v61270, %v61269
%v61272 = vxor.u32 %v61271, %v61267
%v61275 = vadd.s32 %v61272, %v61267
%v61279 = vadd.s32 %v61275, %v9
%v61281 = vshll.u32 %v61272, 24
%v61282 = vshrl.u32 %v61272, 8
%v61283 = vor.u32 %v61282, %v61281
%v61284 = vxor.u32 %v61283, %v61275
%v61287 = vadd.s32 %v61284, %v8
%v61291 = vadd.s32 4, %v61287
%v61295 = vadd.s32 %v61291, %v61279
%v61297 = vshll.u32 %v61291, 13
%v61298 = vshrl.u32 %v61291, 19
%v61299 = vor.u32 %v61298, %v61297
%v61300 = vxor.u32 %v61299, %v61295
%v61303 = vadd.s32 %v61300, %v61295
%v61305 = vshll.u32 %v61300, 15
%v61306 = vshrl.u32 %v61300, 17
%v61307 = vor.u32 %v61306, %v61305
%v61308 = vxor.u32 %v61307, %v61303
%v61311 = vadd.s32 %v61308, %v61303
%v61313 = vshll.u32 %v61308, 26
%v61314 = vshrl.u32 %v61308, 6
%v61315 = vor.u32 %v61314, %v61313
%v61316 = vxor.u32 %v61315, %v61311
%v61319 = vadd.s32 %v61316, %v61311
%v61323 = vadd.s32 %v61319, %v8
%v61325 = vshll.u32 %v61316, 6
%v61326 = vshrl.u32 %v61316, 26
%v61327 = vor.u32 %v61326, %v61325
%v61328 = vxor.u32 %v61327, %v61319
%v61331 = vadd.s32 %v61328, %v10
%v61335 = vadd.s32 5, %v61331
%v61337 = vxor.u32 %v61335, %v61323
%v61338 = vand.u32.u8 255, %v61337
%v61339 = vand.u32 65535, %v61338
%v61340 = vshrl.u32 %v61339, 1
%v61341 = vor.u32 16256, %v61340
%v61342 = vand.u32.u16 65535, %v61341
%v120072 = vadd.low.f32.bf16 -1.0, %v61342
%v61351 = vmul.f32 2.0, %v120072
%v61355 = vadd.f32 -0.99609375, %v61351
%v61359 = vmax.f32 %v61355, -0.99609375
%v61361 = vand.u32 2147483647, %v61359
%vm61364 = vcmp.eq.f32.partialorder %v61361, 1.0
%v61369 = vmul.f32 inf, %v61359
%v61371 = vxor.u32 2147483648, %v61359
%v61374 = vmul.f32 %v61371, %v61359
%v61376 = vadd.f32 1.0, %v61374
%v61377 = vlog2.pop %v61376
%v61378 = vmul.f32 0.6931472, %v61377
%v61379 = vmul.f32 -0.5, %v61374
%v61380 = vadd.f32 1.0, %v61379
%v61381 = vmul.f32 %v61380, %v61374
%v61382 = vand.u32 2147483647, %v61374
%vm61383 = vcmp.lt.f32.partialorder %v61382, 0.0004427343
%v61384 = vsel /*vm=*/%vm61383, /*on_true_vy=*/%v61381, /*on_false_vx=*/%v61378
%v61385 = vxor.u32 2147483648, %v61384
%vm61388 = vcmp.lt.f32.partialorder %v61385, 5.0
%v61393 = vsel /*vm=*/%vm61388, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v61397 = vsel /*vm=*/%vm61388, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v61401 = vsel /*vm=*/%vm61388, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v61405 = vsel /*vm=*/%vm61388, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v61409 = vsel /*vm=*/%vm61388, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v61413 = vsel /*vm=*/%vm61388, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v61417 = vsel /*vm=*/%vm61388, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v61421 = vsel /*vm=*/%vm61388, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v61425 = vsel /*vm=*/%vm61388, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v61429 = vadd.f32 -2.5, %v61385
%v61431 = vrsqrt.pop %v61385
%v61432 = vmul.f32 %v61431, %v61385
%vm61433 = vcmp.eq.f32.partialorder %v61385, inf
%v61434 = vsel /*vm=*/%vm61433, /*on_true_vy=*/%v61385, /*on_false_vx=*/%v61432
%vm61435 = vcmp.eq.f32.partialorder %v61385, 0.0
%v61436 = vand.u32 2147483648, %v61385
%v61437 = vsel /*vm=*/%vm61435, /*on_true_vy=*/%v61436, /*on_false_vx=*/%v61434
%v61440 = vadd.f32 -3.0, %v61437
%v61444 = vsel /*vm=*/%vm61388, /*on_true_vy=*/%v61429, /*on_false_vx=*/%v61440
%v61448 = vmul.f32 %v61444, %v61425
%v61452 = vadd.f32 %v61448, %v61421
%v61456 = vmul.f32 %v61452, %v61444
%v61460 = vadd.f32 %v61456, %v61417
%v61464 = vmul.f32 %v61460, %v61444
%v61468 = vadd.f32 %v61464, %v61413
%v61472 = vmul.f32 %v61468, %v61444
%v61476 = vadd.f32 %v61472, %v61409
%v61480 = vmul.f32 %v61476, %v61444
%v61484 = vadd.f32 %v61480, %v61405
%v61488 = vmul.f32 %v61484, %v61444
%v61492 = vadd.f32 %v61488, %v61401
%v61496 = vmul.f32 %v61492, %v61444
%v61500 = vadd.f32 %v61496, %v61397
%v61504 = vmul.f32 %v61500, %v61444
%v61508 = vadd.f32 %v61504, %v61393
%v61512 = vmul.f32 %v61508, %v61359
%v61516 = vsel /*vm=*/%vm61364, /*on_true_vy=*/%v61369, /*on_false_vx=*/%v61512
%v61520 = vmul.f32 1.4140625, %v61516
%v61523 = vpack.c.bf16 %v120417, %v61520
%120073 = vst [vmem:[%s280 + $0x140] sm:$0xf] /*vst_source=*/%v61523
%v61527 = vadd.s32 %v60141, %v1868
%v61537 = vadd.s32 %v61527, %v415
%vm61541 = vcmp.lt.u32.totalorder %v61537, %v61527
%vm61546 = vcmp.lt.u32.totalorder %v61527, %v1868
%v61551 = vadd.s32 %v60124, %v1855
%v61555 = vadd.s32 1, %v61551
%v61559 = vsel /*vm=*/%vm61546, /*on_true_vy=*/%v61555, /*on_false_vx=*/%v61551
%v61563 = vadd.s32 1, %v61559
%v61567 = vsel /*vm=*/%vm61541, /*on_true_vy=*/%v61563, /*on_false_vx=*/%v61559
%v61572 = vadd.s32 %v61567, %v10
%v61576 = vadd.s32 %v61537, %v9
%v61580 = vadd.s32 %v61576, %v61572
%v61582 = vshll.u32 %v61576, 13
%v61583 = vshrl.u32 %v61576, 19
%v61584 = vor.u32 %v61583, %v61582
%v61585 = vxor.u32 %v61584, %v61580
%v61588 = vadd.s32 %v61585, %v61580
%v61590 = vshll.u32 %v61585, 15
%v61591 = vshrl.u32 %v61585, 17
%v61592 = vor.u32 %v61591, %v61590
%v61593 = vxor.u32 %v61592, %v61588
%v61596 = vadd.s32 %v61593, %v61588
%v61598 = vshll.u32 %v61593, 26
%v61599 = vshrl.u32 %v61593, 6
%v61600 = vor.u32 %v61599, %v61598
%v61601 = vxor.u32 %v61600, %v61596
%v61604 = vadd.s32 %v61601, %v61596
%v61608 = vadd.s32 %v61604, %v9
%v61610 = vshll.u32 %v61601, 6
%v61611 = vshrl.u32 %v61601, 26
%v61612 = vor.u32 %v61611, %v61610
%v61613 = vxor.u32 %v61612, %v61604
%v61616 = vadd.s32 %v61613, %v8
%v61620 = vadd.s32 1, %v61616
%v61624 = vadd.s32 %v61620, %v61608
%v61626 = vshll.u32 %v61620, 17
%v61627 = vshrl.u32 %v61620, 15
%v61628 = vor.u32 %v61627, %v61626
%v61629 = vxor.u32 %v61628, %v61624
%v61632 = vadd.s32 %v61629, %v61624
%v61634 = vshll.u32 %v61629, 29
%v61635 = vshrl.u32 %v61629, 3
%v61636 = vor.u32 %v61635, %v61634
%v61637 = vxor.u32 %v61636, %v61632
%v61640 = vadd.s32 %v61637, %v61632
%v61642 = vshll.u32 %v61637, 16
%v61643 = vshrl.u32 %v61637, 16
%v61644 = vor.u32 %v61643, %v61642
%v61645 = vxor.u32 %v61644, %v61640
%v61648 = vadd.s32 %v61645, %v61640
%v61652 = vadd.s32 %v61648, %v8
%v61654 = vshll.u32 %v61645, 24
%v61655 = vshrl.u32 %v61645, 8
%v61656 = vor.u32 %v61655, %v61654
%v61657 = vxor.u32 %v61656, %v61648
%v61660 = vadd.s32 %v61657, %v10
%v61664 = vadd.s32 2, %v61660
%v61668 = vadd.s32 %v61664, %v61652
%v61670 = vshll.u32 %v61664, 13
%v61671 = vshrl.u32 %v61664, 19
%v61672 = vor.u32 %v61671, %v61670
%v61673 = vxor.u32 %v61672, %v61668
%v61676 = vadd.s32 %v61673, %v61668
%v61678 = vshll.u32 %v61673, 15
%v61679 = vshrl.u32 %v61673, 17
%v61680 = vor.u32 %v61679, %v61678
%v61681 = vxor.u32 %v61680, %v61676
%v61684 = vadd.s32 %v61681, %v61676
%v61686 = vshll.u32 %v61681, 26
%v61687 = vshrl.u32 %v61681, 6
%v61688 = vor.u32 %v61687, %v61686
%v61689 = vxor.u32 %v61688, %v61684
%v61692 = vadd.s32 %v61689, %v61684
%v61696 = vadd.s32 %v61692, %v10
%v61698 = vshll.u32 %v61689, 6
%v61699 = vshrl.u32 %v61689, 26
%v61700 = vor.u32 %v61699, %v61698
%v61701 = vxor.u32 %v61700, %v61692
%v61704 = vadd.s32 %v61701, %v9
%v61708 = vadd.s32 3, %v61704
%v61712 = vadd.s32 %v61708, %v61696
%v61714 = vshll.u32 %v61708, 17
%v61715 = vshrl.u32 %v61708, 15
%v61716 = vor.u32 %v61715, %v61714
%v61717 = vxor.u32 %v61716, %v61712
%v61720 = vadd.s32 %v61717, %v61712
%v61722 = vshll.u32 %v61717, 29
%v61723 = vshrl.u32 %v61717, 3
%v61724 = vor.u32 %v61723, %v61722
%v61725 = vxor.u32 %v61724, %v61720
%v61728 = vadd.s32 %v61725, %v61720
%v61730 = vshll.u32 %v61725, 16
%v61731 = vshrl.u32 %v61725, 16
%v61732 = vor.u32 %v61731, %v61730
%v61733 = vxor.u32 %v61732, %v61728
%v61736 = vadd.s32 %v61733, %v61728
%v61740 = vadd.s32 %v61736, %v9
%v61742 = vshll.u32 %v61733, 24
%v61743 = vshrl.u32 %v61733, 8
%v61744 = vor.u32 %v61743, %v61742
%v61745 = vxor.u32 %v61744, %v61736
%v61748 = vadd.s32 %v61745, %v8
%v61752 = vadd.s32 4, %v61748
%v61756 = vadd.s32 %v61752, %v61740
%v61758 = vshll.u32 %v61752, 13
%v61759 = vshrl.u32 %v61752, 19
%v61760 = vor.u32 %v61759, %v61758
%v61761 = vxor.u32 %v61760, %v61756
%v61764 = vadd.s32 %v61761, %v61756
%v61766 = vshll.u32 %v61761, 15
%v61767 = vshrl.u32 %v61761, 17
%v61768 = vor.u32 %v61767, %v61766
%v61769 = vxor.u32 %v61768, %v61764
%v61772 = vadd.s32 %v61769, %v61764
%v61774 = vshll.u32 %v61769, 26
%v61775 = vshrl.u32 %v61769, 6
%v61776 = vor.u32 %v61775, %v61774
%v61777 = vxor.u32 %v61776, %v61772
%v61780 = vadd.s32 %v61777, %v61772
%v61784 = vadd.s32 %v61780, %v8
%v61786 = vshll.u32 %v61777, 6
%v61787 = vshrl.u32 %v61777, 26
%v61788 = vor.u32 %v61787, %v61786
%v61789 = vxor.u32 %v61788, %v61780
%v61792 = vadd.s32 %v61789, %v10
%v61796 = vadd.s32 5, %v61792
%v61798 = vxor.u32 %v61796, %v61784
%v61799 = vand.u32.u8 255, %v61798
%v61800 = vand.u32 65535, %v61799
%v61801 = vshrl.u32 %v61800, 1
%v61802 = vor.u32 16256, %v61801
%v61803 = vand.u32.u16 65535, %v61802
%v120074 = vadd.low.f32.bf16 -1.0, %v61803
%v61812 = vmul.f32 2.0, %v120074
%v61816 = vadd.f32 -0.99609375, %v61812
%v61820 = vmax.f32 %v61816, -0.99609375
%v61822 = vand.u32 2147483647, %v61820
%vm61825 = vcmp.eq.f32.partialorder %v61822, 1.0
%v61830 = vmul.f32 inf, %v61820
%v61832 = vxor.u32 2147483648, %v61820
%v61835 = vmul.f32 %v61832, %v61820
%v61837 = vadd.f32 1.0, %v61835
%v61838 = vlog2.pop %v61837
%v61839 = vmul.f32 0.6931472, %v61838
%v61840 = vmul.f32 -0.5, %v61835
%v61841 = vadd.f32 1.0, %v61840
%v61842 = vmul.f32 %v61841, %v61835
%v61843 = vand.u32 2147483647, %v61835
%vm61844 = vcmp.lt.f32.partialorder %v61843, 0.0004427343
%v61845 = vsel /*vm=*/%vm61844, /*on_true_vy=*/%v61842, /*on_false_vx=*/%v61839
%v61846 = vxor.u32 2147483648, %v61845
%vm61849 = vcmp.lt.f32.partialorder %v61846, 5.0
%v61854 = vsel /*vm=*/%vm61849, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v61858 = vsel /*vm=*/%vm61849, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v61862 = vsel /*vm=*/%vm61849, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v61866 = vsel /*vm=*/%vm61849, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v61870 = vsel /*vm=*/%vm61849, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v61874 = vsel /*vm=*/%vm61849, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v61878 = vsel /*vm=*/%vm61849, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v61882 = vsel /*vm=*/%vm61849, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v61886 = vsel /*vm=*/%vm61849, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v61890 = vadd.f32 -2.5, %v61846
%v61892 = vrsqrt.pop %v61846
%v61893 = vmul.f32 %v61892, %v61846
%vm61894 = vcmp.eq.f32.partialorder %v61846, inf
%v61895 = vsel /*vm=*/%vm61894, /*on_true_vy=*/%v61846, /*on_false_vx=*/%v61893
%vm61896 = vcmp.eq.f32.partialorder %v61846, 0.0
%v61897 = vand.u32 2147483648, %v61846
%v61898 = vsel /*vm=*/%vm61896, /*on_true_vy=*/%v61897, /*on_false_vx=*/%v61895
%v61901 = vadd.f32 -3.0, %v61898
%v61905 = vsel /*vm=*/%vm61849, /*on_true_vy=*/%v61890, /*on_false_vx=*/%v61901
%v61909 = vmul.f32 %v61905, %v61886
%v61913 = vadd.f32 %v61909, %v61882
%v61917 = vmul.f32 %v61913, %v61905
%v61921 = vadd.f32 %v61917, %v61878
%v61925 = vmul.f32 %v61921, %v61905
%v61929 = vadd.f32 %v61925, %v61874
%v61933 = vmul.f32 %v61929, %v61905
%v61937 = vadd.f32 %v61933, %v61870
%v61941 = vmul.f32 %v61937, %v61905
%v61945 = vadd.f32 %v61941, %v61866
%v61949 = vmul.f32 %v61945, %v61905
%v61953 = vadd.f32 %v61949, %v61862
%v61957 = vmul.f32 %v61953, %v61905
%v61961 = vadd.f32 %v61957, %v61858
%v61965 = vmul.f32 %v61961, %v61905
%v61969 = vadd.f32 %v61965, %v61854
%v61973 = vmul.f32 %v61969, %v61820
%v61977 = vsel /*vm=*/%vm61825, /*on_true_vy=*/%v61830, /*on_false_vx=*/%v61973
%v61981 = vmul.f32 1.4140625, %v61977
%v61984 = vpack.c.bf16 %v120417, %v61981
%120075 = vst [vmem:[%s280 + $0x1c0] sm:$0xf] /*vst_source=*/%v61984
%v61988 = vadd.s32 %v60141, %v2355
%v61998 = vadd.s32 %v61988, %v415
%vm62002 = vcmp.lt.u32.totalorder %v61998, %v61988
%vm62007 = vcmp.lt.u32.totalorder %v61988, %v2355
%v62012 = vadd.s32 %v60124, %v2342
%v62016 = vadd.s32 1, %v62012
%v62020 = vsel /*vm=*/%vm62007, /*on_true_vy=*/%v62016, /*on_false_vx=*/%v62012
%v62024 = vadd.s32 1, %v62020
%v62028 = vsel /*vm=*/%vm62002, /*on_true_vy=*/%v62024, /*on_false_vx=*/%v62020
%v62033 = vadd.s32 %v62028, %v10
%v62037 = vadd.s32 %v61998, %v9
%v62041 = vadd.s32 %v62037, %v62033
%v62043 = vshll.u32 %v62037, 13
%v62044 = vshrl.u32 %v62037, 19
%v62045 = vor.u32 %v62044, %v62043
%v62046 = vxor.u32 %v62045, %v62041
%v62049 = vadd.s32 %v62046, %v62041
%v62051 = vshll.u32 %v62046, 15
%v62052 = vshrl.u32 %v62046, 17
%v62053 = vor.u32 %v62052, %v62051
%v62054 = vxor.u32 %v62053, %v62049
%v62057 = vadd.s32 %v62054, %v62049
%v62059 = vshll.u32 %v62054, 26
%v62060 = vshrl.u32 %v62054, 6
%v62061 = vor.u32 %v62060, %v62059
%v62062 = vxor.u32 %v62061, %v62057
%v62065 = vadd.s32 %v62062, %v62057
%v62069 = vadd.s32 %v62065, %v9
%v62071 = vshll.u32 %v62062, 6
%v62072 = vshrl.u32 %v62062, 26
%v62073 = vor.u32 %v62072, %v62071
%v62074 = vxor.u32 %v62073, %v62065
%v62077 = vadd.s32 %v62074, %v8
%v62081 = vadd.s32 1, %v62077
%v62085 = vadd.s32 %v62081, %v62069
%v62087 = vshll.u32 %v62081, 17
%v62088 = vshrl.u32 %v62081, 15
%v62089 = vor.u32 %v62088, %v62087
%v62090 = vxor.u32 %v62089, %v62085
%v62093 = vadd.s32 %v62090, %v62085
%v62095 = vshll.u32 %v62090, 29
%v62096 = vshrl.u32 %v62090, 3
%v62097 = vor.u32 %v62096, %v62095
%v62098 = vxor.u32 %v62097, %v62093
%v62101 = vadd.s32 %v62098, %v62093
%v62103 = vshll.u32 %v62098, 16
%v62104 = vshrl.u32 %v62098, 16
%v62105 = vor.u32 %v62104, %v62103
%v62106 = vxor.u32 %v62105, %v62101
%v62109 = vadd.s32 %v62106, %v62101
%v62113 = vadd.s32 %v62109, %v8
%v62115 = vshll.u32 %v62106, 24
%v62116 = vshrl.u32 %v62106, 8
%v62117 = vor.u32 %v62116, %v62115
%v62118 = vxor.u32 %v62117, %v62109
%v62121 = vadd.s32 %v62118, %v10
%v62125 = vadd.s32 2, %v62121
%v62129 = vadd.s32 %v62125, %v62113
%v62131 = vshll.u32 %v62125, 13
%v62132 = vshrl.u32 %v62125, 19
%v62133 = vor.u32 %v62132, %v62131
%v62134 = vxor.u32 %v62133, %v62129
%v62137 = vadd.s32 %v62134, %v62129
%v62139 = vshll.u32 %v62134, 15
%v62140 = vshrl.u32 %v62134, 17
%v62141 = vor.u32 %v62140, %v62139
%v62142 = vxor.u32 %v62141, %v62137
%v62145 = vadd.s32 %v62142, %v62137
%v62147 = vshll.u32 %v62142, 26
%v62148 = vshrl.u32 %v62142, 6
%v62149 = vor.u32 %v62148, %v62147
%v62150 = vxor.u32 %v62149, %v62145
%v62153 = vadd.s32 %v62150, %v62145
%v62157 = vadd.s32 %v62153, %v10
%v62159 = vshll.u32 %v62150, 6
%v62160 = vshrl.u32 %v62150, 26
%v62161 = vor.u32 %v62160, %v62159
%v62162 = vxor.u32 %v62161, %v62153
%v62165 = vadd.s32 %v62162, %v9
%v62169 = vadd.s32 3, %v62165
%v62173 = vadd.s32 %v62169, %v62157
%v62175 = vshll.u32 %v62169, 17
%v62176 = vshrl.u32 %v62169, 15
%v62177 = vor.u32 %v62176, %v62175
%v62178 = vxor.u32 %v62177, %v62173
%v62181 = vadd.s32 %v62178, %v62173
%v62183 = vshll.u32 %v62178, 29
%v62184 = vshrl.u32 %v62178, 3
%v62185 = vor.u32 %v62184, %v62183
%v62186 = vxor.u32 %v62185, %v62181
%v62189 = vadd.s32 %v62186, %v62181
%v62191 = vshll.u32 %v62186, 16
%v62192 = vshrl.u32 %v62186, 16
%v62193 = vor.u32 %v62192, %v62191
%v62194 = vxor.u32 %v62193, %v62189
%v62197 = vadd.s32 %v62194, %v62189
%v62201 = vadd.s32 %v62197, %v9
%v62203 = vshll.u32 %v62194, 24
%v62204 = vshrl.u32 %v62194, 8
%v62205 = vor.u32 %v62204, %v62203
%v62206 = vxor.u32 %v62205, %v62197
%v62209 = vadd.s32 %v62206, %v8
%v62213 = vadd.s32 4, %v62209
%v62217 = vadd.s32 %v62213, %v62201
%v62219 = vshll.u32 %v62213, 13
%v62220 = vshrl.u32 %v62213, 19
%v62221 = vor.u32 %v62220, %v62219
%v62222 = vxor.u32 %v62221, %v62217
%v62225 = vadd.s32 %v62222, %v62217
%v62227 = vshll.u32 %v62222, 15
%v62228 = vshrl.u32 %v62222, 17
%v62229 = vor.u32 %v62228, %v62227
%v62230 = vxor.u32 %v62229, %v62225
%v62233 = vadd.s32 %v62230, %v62225
%v62235 = vshll.u32 %v62230, 26
%v62236 = vshrl.u32 %v62230, 6
%v62237 = vor.u32 %v62236, %v62235
%v62238 = vxor.u32 %v62237, %v62233
%v62241 = vadd.s32 %v62238, %v62233
%v62245 = vadd.s32 %v62241, %v8
%v62247 = vshll.u32 %v62238, 6
%v62248 = vshrl.u32 %v62238, 26
%v62249 = vor.u32 %v62248, %v62247
%v62250 = vxor.u32 %v62249, %v62241
%v62253 = vadd.s32 %v62250, %v10
%v62257 = vadd.s32 5, %v62253
%v62259 = vxor.u32 %v62257, %v62245
%v62260 = vand.u32.u8 255, %v62259
%v62261 = vand.u32 65535, %v62260
%v62262 = vshrl.u32 %v62261, 1
%v62263 = vor.u32 16256, %v62262
%v62264 = vand.u32.u16 65535, %v62263
%v120076 = vadd.low.f32.bf16 -1.0, %v62264
%v62273 = vmul.f32 2.0, %v120076
%v62277 = vadd.f32 -0.99609375, %v62273
%v62281 = vmax.f32 %v62277, -0.99609375
%v62283 = vand.u32 2147483647, %v62281
%vm62286 = vcmp.eq.f32.partialorder %v62283, 1.0
%v62291 = vmul.f32 inf, %v62281
%v62293 = vxor.u32 2147483648, %v62281
%v62296 = vmul.f32 %v62293, %v62281
%v62298 = vadd.f32 1.0, %v62296
%v62299 = vlog2.pop %v62298
%v62300 = vmul.f32 0.6931472, %v62299
%v62301 = vmul.f32 -0.5, %v62296
%v62302 = vadd.f32 1.0, %v62301
%v62303 = vmul.f32 %v62302, %v62296
%v62304 = vand.u32 2147483647, %v62296
%vm62305 = vcmp.lt.f32.partialorder %v62304, 0.0004427343
%v62306 = vsel /*vm=*/%vm62305, /*on_true_vy=*/%v62303, /*on_false_vx=*/%v62300
%v62307 = vxor.u32 2147483648, %v62306
%vm62310 = vcmp.lt.f32.partialorder %v62307, 5.0
%v62315 = vsel /*vm=*/%vm62310, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v62319 = vsel /*vm=*/%vm62310, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v62323 = vsel /*vm=*/%vm62310, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v62327 = vsel /*vm=*/%vm62310, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v62331 = vsel /*vm=*/%vm62310, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v62335 = vsel /*vm=*/%vm62310, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v62339 = vsel /*vm=*/%vm62310, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v62343 = vsel /*vm=*/%vm62310, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v62347 = vsel /*vm=*/%vm62310, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v62351 = vadd.f32 -2.5, %v62307
%v62353 = vrsqrt.pop %v62307
%v62354 = vmul.f32 %v62353, %v62307
%vm62355 = vcmp.eq.f32.partialorder %v62307, inf
%v62356 = vsel /*vm=*/%vm62355, /*on_true_vy=*/%v62307, /*on_false_vx=*/%v62354
%vm62357 = vcmp.eq.f32.partialorder %v62307, 0.0
%v62358 = vand.u32 2147483648, %v62307
%v62359 = vsel /*vm=*/%vm62357, /*on_true_vy=*/%v62358, /*on_false_vx=*/%v62356
%v62362 = vadd.f32 -3.0, %v62359
%v62366 = vsel /*vm=*/%vm62310, /*on_true_vy=*/%v62351, /*on_false_vx=*/%v62362
%v62370 = vmul.f32 %v62366, %v62347
%v62374 = vadd.f32 %v62370, %v62343
%v62378 = vmul.f32 %v62374, %v62366
%v62382 = vadd.f32 %v62378, %v62339
%v62386 = vmul.f32 %v62382, %v62366
%v62390 = vadd.f32 %v62386, %v62335
%v62394 = vmul.f32 %v62390, %v62366
%v62398 = vadd.f32 %v62394, %v62331
%v62402 = vmul.f32 %v62398, %v62366
%v62406 = vadd.f32 %v62402, %v62327
%v62410 = vmul.f32 %v62406, %v62366
%v62414 = vadd.f32 %v62410, %v62323
%v62418 = vmul.f32 %v62414, %v62366
%v62422 = vadd.f32 %v62418, %v62319
%v62426 = vmul.f32 %v62422, %v62366
%v62430 = vadd.f32 %v62426, %v62315
%v62434 = vmul.f32 %v62430, %v62281
%v62438 = vsel /*vm=*/%vm62286, /*on_true_vy=*/%v62291, /*on_false_vx=*/%v62434
%v62442 = vmul.f32 1.4140625, %v62438
%v62445 = vpack.c.bf16 %v120417, %v62442
%120077 = vst [vmem:[%s280 + $0x240] sm:$0xf] /*vst_source=*/%v62445
%v62449 = vadd.s32 %v60141, %v2842
%v62459 = vadd.s32 %v62449, %v415
%vm62463 = vcmp.lt.u32.totalorder %v62459, %v62449
%vm62468 = vcmp.lt.u32.totalorder %v62449, %v2842
%v62473 = vadd.s32 %v60124, %v2829
%v62477 = vadd.s32 1, %v62473
%v62481 = vsel /*vm=*/%vm62468, /*on_true_vy=*/%v62477, /*on_false_vx=*/%v62473
%v62485 = vadd.s32 1, %v62481
%v62489 = vsel /*vm=*/%vm62463, /*on_true_vy=*/%v62485, /*on_false_vx=*/%v62481
%v62494 = vadd.s32 %v62489, %v10
%v62498 = vadd.s32 %v62459, %v9
%v62502 = vadd.s32 %v62498, %v62494
%v62504 = vshll.u32 %v62498, 13
%v62505 = vshrl.u32 %v62498, 19
%v62506 = vor.u32 %v62505, %v62504
%v62507 = vxor.u32 %v62506, %v62502
%v62510 = vadd.s32 %v62507, %v62502
%v62512 = vshll.u32 %v62507, 15
%v62513 = vshrl.u32 %v62507, 17
%v62514 = vor.u32 %v62513, %v62512
%v62515 = vxor.u32 %v62514, %v62510
%v62518 = vadd.s32 %v62515, %v62510
%v62520 = vshll.u32 %v62515, 26
%v62521 = vshrl.u32 %v62515, 6
%v62522 = vor.u32 %v62521, %v62520
%v62523 = vxor.u32 %v62522, %v62518
%v62526 = vadd.s32 %v62523, %v62518
%v62530 = vadd.s32 %v62526, %v9
%v62532 = vshll.u32 %v62523, 6
%v62533 = vshrl.u32 %v62523, 26
%v62534 = vor.u32 %v62533, %v62532
%v62535 = vxor.u32 %v62534, %v62526
%v62538 = vadd.s32 %v62535, %v8
%v62542 = vadd.s32 1, %v62538
%v62546 = vadd.s32 %v62542, %v62530
%v62548 = vshll.u32 %v62542, 17
%v62549 = vshrl.u32 %v62542, 15
%v62550 = vor.u32 %v62549, %v62548
%v62551 = vxor.u32 %v62550, %v62546
%v62554 = vadd.s32 %v62551, %v62546
%v62556 = vshll.u32 %v62551, 29
%v62557 = vshrl.u32 %v62551, 3
%v62558 = vor.u32 %v62557, %v62556
%v62559 = vxor.u32 %v62558, %v62554
%v62562 = vadd.s32 %v62559, %v62554
%v62564 = vshll.u32 %v62559, 16
%v62565 = vshrl.u32 %v62559, 16
%v62566 = vor.u32 %v62565, %v62564
%v62567 = vxor.u32 %v62566, %v62562
%v62570 = vadd.s32 %v62567, %v62562
%v62574 = vadd.s32 %v62570, %v8
%v62576 = vshll.u32 %v62567, 24
%v62577 = vshrl.u32 %v62567, 8
%v62578 = vor.u32 %v62577, %v62576
%v62579 = vxor.u32 %v62578, %v62570
%v62582 = vadd.s32 %v62579, %v10
%v62586 = vadd.s32 2, %v62582
%v62590 = vadd.s32 %v62586, %v62574
%v62592 = vshll.u32 %v62586, 13
%v62593 = vshrl.u32 %v62586, 19
%v62594 = vor.u32 %v62593, %v62592
%v62595 = vxor.u32 %v62594, %v62590
%v62598 = vadd.s32 %v62595, %v62590
%v62600 = vshll.u32 %v62595, 15
%v62601 = vshrl.u32 %v62595, 17
%v62602 = vor.u32 %v62601, %v62600
%v62603 = vxor.u32 %v62602, %v62598
%v62606 = vadd.s32 %v62603, %v62598
%v62608 = vshll.u32 %v62603, 26
%v62609 = vshrl.u32 %v62603, 6
%v62610 = vor.u32 %v62609, %v62608
%v62611 = vxor.u32 %v62610, %v62606
%v62614 = vadd.s32 %v62611, %v62606
%v62618 = vadd.s32 %v62614, %v10
%v62620 = vshll.u32 %v62611, 6
%v62621 = vshrl.u32 %v62611, 26
%v62622 = vor.u32 %v62621, %v62620
%v62623 = vxor.u32 %v62622, %v62614
%v62626 = vadd.s32 %v62623, %v9
%v62630 = vadd.s32 3, %v62626
%v62634 = vadd.s32 %v62630, %v62618
%v62636 = vshll.u32 %v62630, 17
%v62637 = vshrl.u32 %v62630, 15
%v62638 = vor.u32 %v62637, %v62636
%v62639 = vxor.u32 %v62638, %v62634
%v62642 = vadd.s32 %v62639, %v62634
%v62644 = vshll.u32 %v62639, 29
%v62645 = vshrl.u32 %v62639, 3
%v62646 = vor.u32 %v62645, %v62644
%v62647 = vxor.u32 %v62646, %v62642
%v62650 = vadd.s32 %v62647, %v62642
%v62652 = vshll.u32 %v62647, 16
%v62653 = vshrl.u32 %v62647, 16
%v62654 = vor.u32 %v62653, %v62652
%v62655 = vxor.u32 %v62654, %v62650
%v62658 = vadd.s32 %v62655, %v62650
%v62662 = vadd.s32 %v62658, %v9
%v62664 = vshll.u32 %v62655, 24
%v62665 = vshrl.u32 %v62655, 8
%v62666 = vor.u32 %v62665, %v62664
%v62667 = vxor.u32 %v62666, %v62658
%v62670 = vadd.s32 %v62667, %v8
%v62674 = vadd.s32 4, %v62670
%v62678 = vadd.s32 %v62674, %v62662
%v62680 = vshll.u32 %v62674, 13
%v62681 = vshrl.u32 %v62674, 19
%v62682 = vor.u32 %v62681, %v62680
%v62683 = vxor.u32 %v62682, %v62678
%v62686 = vadd.s32 %v62683, %v62678
%v62688 = vshll.u32 %v62683, 15
%v62689 = vshrl.u32 %v62683, 17
%v62690 = vor.u32 %v62689, %v62688
%v62691 = vxor.u32 %v62690, %v62686
%v62694 = vadd.s32 %v62691, %v62686
%v62696 = vshll.u32 %v62691, 26
%v62697 = vshrl.u32 %v62691, 6
%v62698 = vor.u32 %v62697, %v62696
%v62699 = vxor.u32 %v62698, %v62694
%v62702 = vadd.s32 %v62699, %v62694
%v62706 = vadd.s32 %v62702, %v8
%v62708 = vshll.u32 %v62699, 6
%v62709 = vshrl.u32 %v62699, 26
%v62710 = vor.u32 %v62709, %v62708
%v62711 = vxor.u32 %v62710, %v62702
%v62714 = vadd.s32 %v62711, %v10
%v62718 = vadd.s32 5, %v62714
%v62720 = vxor.u32 %v62718, %v62706
%v62721 = vand.u32.u8 255, %v62720
%v62722 = vand.u32 65535, %v62721
%v62723 = vshrl.u32 %v62722, 1
%v62724 = vor.u32 16256, %v62723
%v62725 = vand.u32.u16 65535, %v62724
%v120078 = vadd.low.f32.bf16 -1.0, %v62725
%v62734 = vmul.f32 2.0, %v120078
%v62738 = vadd.f32 -0.99609375, %v62734
%v62742 = vmax.f32 %v62738, -0.99609375
%v62744 = vand.u32 2147483647, %v62742
%vm62747 = vcmp.eq.f32.partialorder %v62744, 1.0
%v62752 = vmul.f32 inf, %v62742
%v62754 = vxor.u32 2147483648, %v62742
%v62757 = vmul.f32 %v62754, %v62742
%v62759 = vadd.f32 1.0, %v62757
%v62760 = vlog2.pop %v62759
%v62761 = vmul.f32 0.6931472, %v62760
%v62762 = vmul.f32 -0.5, %v62757
%v62763 = vadd.f32 1.0, %v62762
%v62764 = vmul.f32 %v62763, %v62757
%v62765 = vand.u32 2147483647, %v62757
%vm62766 = vcmp.lt.f32.partialorder %v62765, 0.0004427343
%v62767 = vsel /*vm=*/%vm62766, /*on_true_vy=*/%v62764, /*on_false_vx=*/%v62761
%v62768 = vxor.u32 2147483648, %v62767
%vm62771 = vcmp.lt.f32.partialorder %v62768, 5.0
%v62776 = vsel /*vm=*/%vm62771, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v62780 = vsel /*vm=*/%vm62771, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v62784 = vsel /*vm=*/%vm62771, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v62788 = vsel /*vm=*/%vm62771, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v62792 = vsel /*vm=*/%vm62771, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v62796 = vsel /*vm=*/%vm62771, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v62800 = vsel /*vm=*/%vm62771, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v62804 = vsel /*vm=*/%vm62771, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v62808 = vsel /*vm=*/%vm62771, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v62812 = vadd.f32 -2.5, %v62768
%v62814 = vrsqrt.pop %v62768
%v62815 = vmul.f32 %v62814, %v62768
%vm62816 = vcmp.eq.f32.partialorder %v62768, inf
%v62817 = vsel /*vm=*/%vm62816, /*on_true_vy=*/%v62768, /*on_false_vx=*/%v62815
%vm62818 = vcmp.eq.f32.partialorder %v62768, 0.0
%v62819 = vand.u32 2147483648, %v62768
%v62820 = vsel /*vm=*/%vm62818, /*on_true_vy=*/%v62819, /*on_false_vx=*/%v62817
%v62823 = vadd.f32 -3.0, %v62820
%v62827 = vsel /*vm=*/%vm62771, /*on_true_vy=*/%v62812, /*on_false_vx=*/%v62823
%v62831 = vmul.f32 %v62827, %v62808
%v62835 = vadd.f32 %v62831, %v62804
%v62839 = vmul.f32 %v62835, %v62827
%v62843 = vadd.f32 %v62839, %v62800
%v62847 = vmul.f32 %v62843, %v62827
%v62851 = vadd.f32 %v62847, %v62796
%v62855 = vmul.f32 %v62851, %v62827
%v62859 = vadd.f32 %v62855, %v62792
%v62863 = vmul.f32 %v62859, %v62827
%v62867 = vadd.f32 %v62863, %v62788
%v62871 = vmul.f32 %v62867, %v62827
%v62875 = vadd.f32 %v62871, %v62784
%v62879 = vmul.f32 %v62875, %v62827
%v62883 = vadd.f32 %v62879, %v62780
%v62887 = vmul.f32 %v62883, %v62827
%v62891 = vadd.f32 %v62887, %v62776
%v62895 = vmul.f32 %v62891, %v62742
%v62899 = vsel /*vm=*/%vm62747, /*on_true_vy=*/%v62752, /*on_false_vx=*/%v62895
%v62903 = vmul.f32 1.4140625, %v62899
%v62906 = vpack.c.bf16 %v120417, %v62903
%120079 = vst [vmem:[%s280 + $0x2c0] sm:$0xf] /*vst_source=*/%v62906
%v62910 = vadd.s32 %v60141, %v3329
%v62920 = vadd.s32 %v62910, %v415
%vm62924 = vcmp.lt.u32.totalorder %v62920, %v62910
%vm62929 = vcmp.lt.u32.totalorder %v62910, %v3329
%v62934 = vadd.s32 %v60124, %v3316
%v62938 = vadd.s32 1, %v62934
%v62942 = vsel /*vm=*/%vm62929, /*on_true_vy=*/%v62938, /*on_false_vx=*/%v62934
%v62946 = vadd.s32 1, %v62942
%v62950 = vsel /*vm=*/%vm62924, /*on_true_vy=*/%v62946, /*on_false_vx=*/%v62942
%v62955 = vadd.s32 %v62950, %v10
%v62959 = vadd.s32 %v62920, %v9
%v62963 = vadd.s32 %v62959, %v62955
%v62965 = vshll.u32 %v62959, 13
%v62966 = vshrl.u32 %v62959, 19
%v62967 = vor.u32 %v62966, %v62965
%v62968 = vxor.u32 %v62967, %v62963
%v62971 = vadd.s32 %v62968, %v62963
%v62973 = vshll.u32 %v62968, 15
%v62974 = vshrl.u32 %v62968, 17
%v62975 = vor.u32 %v62974, %v62973
%v62976 = vxor.u32 %v62975, %v62971
%v62979 = vadd.s32 %v62976, %v62971
%v62981 = vshll.u32 %v62976, 26
%v62982 = vshrl.u32 %v62976, 6
%v62983 = vor.u32 %v62982, %v62981
%v62984 = vxor.u32 %v62983, %v62979
%v62987 = vadd.s32 %v62984, %v62979
%v62991 = vadd.s32 %v62987, %v9
%v62993 = vshll.u32 %v62984, 6
%v62994 = vshrl.u32 %v62984, 26
%v62995 = vor.u32 %v62994, %v62993
%v62996 = vxor.u32 %v62995, %v62987
%v62999 = vadd.s32 %v62996, %v8
%v63003 = vadd.s32 1, %v62999
%v63007 = vadd.s32 %v63003, %v62991
%v63009 = vshll.u32 %v63003, 17
%v63010 = vshrl.u32 %v63003, 15
%v63011 = vor.u32 %v63010, %v63009
%v63012 = vxor.u32 %v63011, %v63007
%v63015 = vadd.s32 %v63012, %v63007
%v63017 = vshll.u32 %v63012, 29
%v63018 = vshrl.u32 %v63012, 3
%v63019 = vor.u32 %v63018, %v63017
%v63020 = vxor.u32 %v63019, %v63015
%v63023 = vadd.s32 %v63020, %v63015
%v63025 = vshll.u32 %v63020, 16
%v63026 = vshrl.u32 %v63020, 16
%v63027 = vor.u32 %v63026, %v63025
%v63028 = vxor.u32 %v63027, %v63023
%v63031 = vadd.s32 %v63028, %v63023
%v63035 = vadd.s32 %v63031, %v8
%v63037 = vshll.u32 %v63028, 24
%v63038 = vshrl.u32 %v63028, 8
%v63039 = vor.u32 %v63038, %v63037
%v63040 = vxor.u32 %v63039, %v63031
%v63043 = vadd.s32 %v63040, %v10
%v63047 = vadd.s32 2, %v63043
%v63051 = vadd.s32 %v63047, %v63035
%v63053 = vshll.u32 %v63047, 13
%v63054 = vshrl.u32 %v63047, 19
%v63055 = vor.u32 %v63054, %v63053
%v63056 = vxor.u32 %v63055, %v63051
%v63059 = vadd.s32 %v63056, %v63051
%v63061 = vshll.u32 %v63056, 15
%v63062 = vshrl.u32 %v63056, 17
%v63063 = vor.u32 %v63062, %v63061
%v63064 = vxor.u32 %v63063, %v63059
%v63067 = vadd.s32 %v63064, %v63059
%v63069 = vshll.u32 %v63064, 26
%v63070 = vshrl.u32 %v63064, 6
%v63071 = vor.u32 %v63070, %v63069
%v63072 = vxor.u32 %v63071, %v63067
%v63075 = vadd.s32 %v63072, %v63067
%v63079 = vadd.s32 %v63075, %v10
%v63081 = vshll.u32 %v63072, 6
%v63082 = vshrl.u32 %v63072, 26
%v63083 = vor.u32 %v63082, %v63081
%v63084 = vxor.u32 %v63083, %v63075
%v63087 = vadd.s32 %v63084, %v9
%v63091 = vadd.s32 3, %v63087
%v63095 = vadd.s32 %v63091, %v63079
%v63097 = vshll.u32 %v63091, 17
%v63098 = vshrl.u32 %v63091, 15
%v63099 = vor.u32 %v63098, %v63097
%v63100 = vxor.u32 %v63099, %v63095
%v63103 = vadd.s32 %v63100, %v63095
%v63105 = vshll.u32 %v63100, 29
%v63106 = vshrl.u32 %v63100, 3
%v63107 = vor.u32 %v63106, %v63105
%v63108 = vxor.u32 %v63107, %v63103
%v63111 = vadd.s32 %v63108, %v63103
%v63113 = vshll.u32 %v63108, 16
%v63114 = vshrl.u32 %v63108, 16
%v63115 = vor.u32 %v63114, %v63113
%v63116 = vxor.u32 %v63115, %v63111
%v63119 = vadd.s32 %v63116, %v63111
%v63123 = vadd.s32 %v63119, %v9
%v63125 = vshll.u32 %v63116, 24
%v63126 = vshrl.u32 %v63116, 8
%v63127 = vor.u32 %v63126, %v63125
%v63128 = vxor.u32 %v63127, %v63119
%v63131 = vadd.s32 %v63128, %v8
%v63135 = vadd.s32 4, %v63131
%v63139 = vadd.s32 %v63135, %v63123
%v63141 = vshll.u32 %v63135, 13
%v63142 = vshrl.u32 %v63135, 19
%v63143 = vor.u32 %v63142, %v63141
%v63144 = vxor.u32 %v63143, %v63139
%v63147 = vadd.s32 %v63144, %v63139
%v63149 = vshll.u32 %v63144, 15
%v63150 = vshrl.u32 %v63144, 17
%v63151 = vor.u32 %v63150, %v63149
%v63152 = vxor.u32 %v63151, %v63147
%v63155 = vadd.s32 %v63152, %v63147
%v63157 = vshll.u32 %v63152, 26
%v63158 = vshrl.u32 %v63152, 6
%v63159 = vor.u32 %v63158, %v63157
%v63160 = vxor.u32 %v63159, %v63155
%v63163 = vadd.s32 %v63160, %v63155
%v63167 = vadd.s32 %v63163, %v8
%v63169 = vshll.u32 %v63160, 6
%v63170 = vshrl.u32 %v63160, 26
%v63171 = vor.u32 %v63170, %v63169
%v63172 = vxor.u32 %v63171, %v63163
%v63175 = vadd.s32 %v63172, %v10
%v63179 = vadd.s32 5, %v63175
%v63181 = vxor.u32 %v63179, %v63167
%v63182 = vand.u32.u8 255, %v63181
%v63183 = vand.u32 65535, %v63182
%v63184 = vshrl.u32 %v63183, 1
%v63185 = vor.u32 16256, %v63184
%v63186 = vand.u32.u16 65535, %v63185
%v120080 = vadd.low.f32.bf16 -1.0, %v63186
%v63195 = vmul.f32 2.0, %v120080
%v63199 = vadd.f32 -0.99609375, %v63195
%v63203 = vmax.f32 %v63199, -0.99609375
%v63205 = vand.u32 2147483647, %v63203
%vm63208 = vcmp.eq.f32.partialorder %v63205, 1.0
%v63213 = vmul.f32 inf, %v63203
%v63215 = vxor.u32 2147483648, %v63203
%v63218 = vmul.f32 %v63215, %v63203
%v63220 = vadd.f32 1.0, %v63218
%v63221 = vlog2.pop %v63220
%v63222 = vmul.f32 0.6931472, %v63221
%v63223 = vmul.f32 -0.5, %v63218
%v63224 = vadd.f32 1.0, %v63223
%v63225 = vmul.f32 %v63224, %v63218
%v63226 = vand.u32 2147483647, %v63218
%vm63227 = vcmp.lt.f32.partialorder %v63226, 0.0004427343
%v63228 = vsel /*vm=*/%vm63227, /*on_true_vy=*/%v63225, /*on_false_vx=*/%v63222
%v63229 = vxor.u32 2147483648, %v63228
%vm63232 = vcmp.lt.f32.partialorder %v63229, 5.0
%v63237 = vsel /*vm=*/%vm63232, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v63241 = vsel /*vm=*/%vm63232, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v63245 = vsel /*vm=*/%vm63232, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v63249 = vsel /*vm=*/%vm63232, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v63253 = vsel /*vm=*/%vm63232, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v63257 = vsel /*vm=*/%vm63232, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v63261 = vsel /*vm=*/%vm63232, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v63265 = vsel /*vm=*/%vm63232, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v63269 = vsel /*vm=*/%vm63232, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v63273 = vadd.f32 -2.5, %v63229
%v63275 = vrsqrt.pop %v63229
%v63276 = vmul.f32 %v63275, %v63229
%vm63277 = vcmp.eq.f32.partialorder %v63229, inf
%v63278 = vsel /*vm=*/%vm63277, /*on_true_vy=*/%v63229, /*on_false_vx=*/%v63276
%vm63279 = vcmp.eq.f32.partialorder %v63229, 0.0
%v63280 = vand.u32 2147483648, %v63229
%v63281 = vsel /*vm=*/%vm63279, /*on_true_vy=*/%v63280, /*on_false_vx=*/%v63278
%v63284 = vadd.f32 -3.0, %v63281
%v63288 = vsel /*vm=*/%vm63232, /*on_true_vy=*/%v63273, /*on_false_vx=*/%v63284
%v63292 = vmul.f32 %v63288, %v63269
%v63296 = vadd.f32 %v63292, %v63265
%v63300 = vmul.f32 %v63296, %v63288
%v63304 = vadd.f32 %v63300, %v63261
%v63308 = vmul.f32 %v63304, %v63288
%v63312 = vadd.f32 %v63308, %v63257
%v63316 = vmul.f32 %v63312, %v63288
%v63320 = vadd.f32 %v63316, %v63253
%v63324 = vmul.f32 %v63320, %v63288
%v63328 = vadd.f32 %v63324, %v63249
%v63332 = vmul.f32 %v63328, %v63288
%v63336 = vadd.f32 %v63332, %v63245
%v63340 = vmul.f32 %v63336, %v63288
%v63344 = vadd.f32 %v63340, %v63241
%v63348 = vmul.f32 %v63344, %v63288
%v63352 = vadd.f32 %v63348, %v63237
%v63356 = vmul.f32 %v63352, %v63203
%v63360 = vsel /*vm=*/%vm63208, /*on_true_vy=*/%v63213, /*on_false_vx=*/%v63356
%v63364 = vmul.f32 1.4140625, %v63360
%v63367 = vpack.c.bf16 %v120417, %v63364
%120081 = vst [vmem:[%s280 + $0x340] sm:$0xf] /*vst_source=*/%v63367
%v63371 = vadd.s32 %v60141, %v3816
%v63381 = vadd.s32 %v63371, %v415
%vm63385 = vcmp.lt.u32.totalorder %v63381, %v63371
%vm63390 = vcmp.lt.u32.totalorder %v63371, %v3816
%v63395 = vadd.s32 %v60124, %v3803
%v63399 = vadd.s32 1, %v63395
%v63403 = vsel /*vm=*/%vm63390, /*on_true_vy=*/%v63399, /*on_false_vx=*/%v63395
%v63407 = vadd.s32 1, %v63403
%v63411 = vsel /*vm=*/%vm63385, /*on_true_vy=*/%v63407, /*on_false_vx=*/%v63403
%v63416 = vadd.s32 %v63411, %v10
%v63420 = vadd.s32 %v63381, %v9
%v63424 = vadd.s32 %v63420, %v63416
%v63426 = vshll.u32 %v63420, 13
%v63427 = vshrl.u32 %v63420, 19
%v63428 = vor.u32 %v63427, %v63426
%v63429 = vxor.u32 %v63428, %v63424
%v63432 = vadd.s32 %v63429, %v63424
%v63434 = vshll.u32 %v63429, 15
%v63435 = vshrl.u32 %v63429, 17
%v63436 = vor.u32 %v63435, %v63434
%v63437 = vxor.u32 %v63436, %v63432
%v63440 = vadd.s32 %v63437, %v63432
%v63442 = vshll.u32 %v63437, 26
%v63443 = vshrl.u32 %v63437, 6
%v63444 = vor.u32 %v63443, %v63442
%v63445 = vxor.u32 %v63444, %v63440
%v63448 = vadd.s32 %v63445, %v63440
%v63452 = vadd.s32 %v63448, %v9
%v63454 = vshll.u32 %v63445, 6
%v63455 = vshrl.u32 %v63445, 26
%v63456 = vor.u32 %v63455, %v63454
%v63457 = vxor.u32 %v63456, %v63448
%v63460 = vadd.s32 %v63457, %v8
%v63464 = vadd.s32 1, %v63460
%v63468 = vadd.s32 %v63464, %v63452
%v63470 = vshll.u32 %v63464, 17
%v63471 = vshrl.u32 %v63464, 15
%v63472 = vor.u32 %v63471, %v63470
%v63473 = vxor.u32 %v63472, %v63468
%v63476 = vadd.s32 %v63473, %v63468
%v63478 = vshll.u32 %v63473, 29
%v63479 = vshrl.u32 %v63473, 3
%v63480 = vor.u32 %v63479, %v63478
%v63481 = vxor.u32 %v63480, %v63476
%v63484 = vadd.s32 %v63481, %v63476
%v63486 = vshll.u32 %v63481, 16
%v63487 = vshrl.u32 %v63481, 16
%v63488 = vor.u32 %v63487, %v63486
%v63489 = vxor.u32 %v63488, %v63484
%v63492 = vadd.s32 %v63489, %v63484
%v63496 = vadd.s32 %v63492, %v8
%v63498 = vshll.u32 %v63489, 24
%v63499 = vshrl.u32 %v63489, 8
%v63500 = vor.u32 %v63499, %v63498
%v63501 = vxor.u32 %v63500, %v63492
%v63504 = vadd.s32 %v63501, %v10
%v63508 = vadd.s32 2, %v63504
%v63512 = vadd.s32 %v63508, %v63496
%v63514 = vshll.u32 %v63508, 13
%v63515 = vshrl.u32 %v63508, 19
%v63516 = vor.u32 %v63515, %v63514
%v63517 = vxor.u32 %v63516, %v63512
%v63520 = vadd.s32 %v63517, %v63512
%v63522 = vshll.u32 %v63517, 15
%v63523 = vshrl.u32 %v63517, 17
%v63524 = vor.u32 %v63523, %v63522
%v63525 = vxor.u32 %v63524, %v63520
%v63528 = vadd.s32 %v63525, %v63520
%v63530 = vshll.u32 %v63525, 26
%v63531 = vshrl.u32 %v63525, 6
%v63532 = vor.u32 %v63531, %v63530
%v63533 = vxor.u32 %v63532, %v63528
%v63536 = vadd.s32 %v63533, %v63528
%v63540 = vadd.s32 %v63536, %v10
%v63542 = vshll.u32 %v63533, 6
%v63543 = vshrl.u32 %v63533, 26
%v63544 = vor.u32 %v63543, %v63542
%v63545 = vxor.u32 %v63544, %v63536
%v63548 = vadd.s32 %v63545, %v9
%v63552 = vadd.s32 3, %v63548
%v63556 = vadd.s32 %v63552, %v63540
%v63558 = vshll.u32 %v63552, 17
%v63559 = vshrl.u32 %v63552, 15
%v63560 = vor.u32 %v63559, %v63558
%v63561 = vxor.u32 %v63560, %v63556
%v63564 = vadd.s32 %v63561, %v63556
%v63566 = vshll.u32 %v63561, 29
%v63567 = vshrl.u32 %v63561, 3
%v63568 = vor.u32 %v63567, %v63566
%v63569 = vxor.u32 %v63568, %v63564
%v63572 = vadd.s32 %v63569, %v63564
%v63574 = vshll.u32 %v63569, 16
%v63575 = vshrl.u32 %v63569, 16
%v63576 = vor.u32 %v63575, %v63574
%v63577 = vxor.u32 %v63576, %v63572
%v63580 = vadd.s32 %v63577, %v63572
%v63584 = vadd.s32 %v63580, %v9
%v63586 = vshll.u32 %v63577, 24
%v63587 = vshrl.u32 %v63577, 8
%v63588 = vor.u32 %v63587, %v63586
%v63589 = vxor.u32 %v63588, %v63580
%v63592 = vadd.s32 %v63589, %v8
%v63596 = vadd.s32 4, %v63592
%v63600 = vadd.s32 %v63596, %v63584
%v63602 = vshll.u32 %v63596, 13
%v63603 = vshrl.u32 %v63596, 19
%v63604 = vor.u32 %v63603, %v63602
%v63605 = vxor.u32 %v63604, %v63600
%v63608 = vadd.s32 %v63605, %v63600
%v63610 = vshll.u32 %v63605, 15
%v63611 = vshrl.u32 %v63605, 17
%v63612 = vor.u32 %v63611, %v63610
%v63613 = vxor.u32 %v63612, %v63608
%v63616 = vadd.s32 %v63613, %v63608
%v63618 = vshll.u32 %v63613, 26
%v63619 = vshrl.u32 %v63613, 6
%v63620 = vor.u32 %v63619, %v63618
%v63621 = vxor.u32 %v63620, %v63616
%v63624 = vadd.s32 %v63621, %v63616
%v63628 = vadd.s32 %v63624, %v8
%v63630 = vshll.u32 %v63621, 6
%v63631 = vshrl.u32 %v63621, 26
%v63632 = vor.u32 %v63631, %v63630
%v63633 = vxor.u32 %v63632, %v63624
%v63636 = vadd.s32 %v63633, %v10
%v63640 = vadd.s32 5, %v63636
%v63642 = vxor.u32 %v63640, %v63628
%v63643 = vand.u32.u8 255, %v63642
%v63644 = vand.u32 65535, %v63643
%v63645 = vshrl.u32 %v63644, 1
%v63646 = vor.u32 16256, %v63645
%v63647 = vand.u32.u16 65535, %v63646
%v120082 = vadd.low.f32.bf16 -1.0, %v63647
%v63656 = vmul.f32 2.0, %v120082
%v63660 = vadd.f32 -0.99609375, %v63656
%v63664 = vmax.f32 %v63660, -0.99609375
%v63666 = vand.u32 2147483647, %v63664
%vm63669 = vcmp.eq.f32.partialorder %v63666, 1.0
%v63674 = vmul.f32 inf, %v63664
%v63676 = vxor.u32 2147483648, %v63664
%v63679 = vmul.f32 %v63676, %v63664
%v63681 = vadd.f32 1.0, %v63679
%v63682 = vlog2.pop %v63681
%v63683 = vmul.f32 0.6931472, %v63682
%v63684 = vmul.f32 -0.5, %v63679
%v63685 = vadd.f32 1.0, %v63684
%v63686 = vmul.f32 %v63685, %v63679
%v63687 = vand.u32 2147483647, %v63679
%vm63688 = vcmp.lt.f32.partialorder %v63687, 0.0004427343
%v63689 = vsel /*vm=*/%vm63688, /*on_true_vy=*/%v63686, /*on_false_vx=*/%v63683
%v63690 = vxor.u32 2147483648, %v63689
%vm63693 = vcmp.lt.f32.partialorder %v63690, 5.0
%v63698 = vsel /*vm=*/%vm63693, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v63702 = vsel /*vm=*/%vm63693, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v63706 = vsel /*vm=*/%vm63693, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v63710 = vsel /*vm=*/%vm63693, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v63714 = vsel /*vm=*/%vm63693, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v63718 = vsel /*vm=*/%vm63693, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v63722 = vsel /*vm=*/%vm63693, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v63726 = vsel /*vm=*/%vm63693, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v63730 = vsel /*vm=*/%vm63693, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v63734 = vadd.f32 -2.5, %v63690
%v63736 = vrsqrt.pop %v63690
%v63737 = vmul.f32 %v63736, %v63690
%vm63738 = vcmp.eq.f32.partialorder %v63690, inf
%v63739 = vsel /*vm=*/%vm63738, /*on_true_vy=*/%v63690, /*on_false_vx=*/%v63737
%vm63740 = vcmp.eq.f32.partialorder %v63690, 0.0
%v63741 = vand.u32 2147483648, %v63690
%v63742 = vsel /*vm=*/%vm63740, /*on_true_vy=*/%v63741, /*on_false_vx=*/%v63739
%v63745 = vadd.f32 -3.0, %v63742
%v63749 = vsel /*vm=*/%vm63693, /*on_true_vy=*/%v63734, /*on_false_vx=*/%v63745
%v63753 = vmul.f32 %v63749, %v63730
%v63757 = vadd.f32 %v63753, %v63726
%v63761 = vmul.f32 %v63757, %v63749
%v63765 = vadd.f32 %v63761, %v63722
%v63769 = vmul.f32 %v63765, %v63749
%v63773 = vadd.f32 %v63769, %v63718
%v63777 = vmul.f32 %v63773, %v63749
%v63781 = vadd.f32 %v63777, %v63714
%v63785 = vmul.f32 %v63781, %v63749
%v63789 = vadd.f32 %v63785, %v63710
%v63793 = vmul.f32 %v63789, %v63749
%v63797 = vadd.f32 %v63793, %v63706
%v63801 = vmul.f32 %v63797, %v63749
%v63805 = vadd.f32 %v63801, %v63702
%v63809 = vmul.f32 %v63805, %v63749
%v63813 = vadd.f32 %v63809, %v63698
%v63817 = vmul.f32 %v63813, %v63664
%v63821 = vsel /*vm=*/%vm63669, /*on_true_vy=*/%v63674, /*on_false_vx=*/%v63817
%v63825 = vmul.f32 1.4140625, %v63821
%v63828 = vpack.c.bf16 %v120417, %v63825
%120083 = vst [vmem:[%s280 + $0x3c0] sm:$0xf] /*vst_source=*/%v63828
%v63866 = vadd.s32 %v63863, %v408
%v63876 = vadd.s32 %v63866, %v415
%vm63880 = vcmp.lt.u32.totalorder %v63876, %v63866
%vm63885 = vcmp.lt.u32.totalorder %v63866, %v408
%v63890 = vadd.s32 %v63846, %v380
%v63894 = vadd.s32 1, %v63890
%v63898 = vsel /*vm=*/%vm63885, /*on_true_vy=*/%v63894, /*on_false_vx=*/%v63890
%v63902 = vadd.s32 1, %v63898
%v63906 = vsel /*vm=*/%vm63880, /*on_true_vy=*/%v63902, /*on_false_vx=*/%v63898
%v63911 = vadd.s32 %v63906, %v10
%v63915 = vadd.s32 %v63876, %v9
%v63919 = vadd.s32 %v63915, %v63911
%v63921 = vshll.u32 %v63915, 13
%v63922 = vshrl.u32 %v63915, 19
%v63923 = vor.u32 %v63922, %v63921
%v63924 = vxor.u32 %v63923, %v63919
%v63927 = vadd.s32 %v63924, %v63919
%v63929 = vshll.u32 %v63924, 15
%v63930 = vshrl.u32 %v63924, 17
%v63931 = vor.u32 %v63930, %v63929
%v63932 = vxor.u32 %v63931, %v63927
%v63935 = vadd.s32 %v63932, %v63927
%v63937 = vshll.u32 %v63932, 26
%v63938 = vshrl.u32 %v63932, 6
%v63939 = vor.u32 %v63938, %v63937
%v63940 = vxor.u32 %v63939, %v63935
%v63943 = vadd.s32 %v63940, %v63935
%v63947 = vadd.s32 %v63943, %v9
%v63949 = vshll.u32 %v63940, 6
%v63950 = vshrl.u32 %v63940, 26
%v63951 = vor.u32 %v63950, %v63949
%v63952 = vxor.u32 %v63951, %v63943
%v63955 = vadd.s32 %v63952, %v8
%v63959 = vadd.s32 1, %v63955
%v63963 = vadd.s32 %v63959, %v63947
%v63965 = vshll.u32 %v63959, 17
%v63966 = vshrl.u32 %v63959, 15
%v63967 = vor.u32 %v63966, %v63965
%v63968 = vxor.u32 %v63967, %v63963
%v63971 = vadd.s32 %v63968, %v63963
%v63973 = vshll.u32 %v63968, 29
%v63974 = vshrl.u32 %v63968, 3
%v63975 = vor.u32 %v63974, %v63973
%v63976 = vxor.u32 %v63975, %v63971
%v63979 = vadd.s32 %v63976, %v63971
%v63981 = vshll.u32 %v63976, 16
%v63982 = vshrl.u32 %v63976, 16
%v63983 = vor.u32 %v63982, %v63981
%v63984 = vxor.u32 %v63983, %v63979
%v63987 = vadd.s32 %v63984, %v63979
%v63991 = vadd.s32 %v63987, %v8
%v63993 = vshll.u32 %v63984, 24
%v63994 = vshrl.u32 %v63984, 8
%v63995 = vor.u32 %v63994, %v63993
%v63996 = vxor.u32 %v63995, %v63987
%v63999 = vadd.s32 %v63996, %v10
%v64003 = vadd.s32 2, %v63999
%v64007 = vadd.s32 %v64003, %v63991
%v64009 = vshll.u32 %v64003, 13
%v64010 = vshrl.u32 %v64003, 19
%v64011 = vor.u32 %v64010, %v64009
%v64012 = vxor.u32 %v64011, %v64007
%v64015 = vadd.s32 %v64012, %v64007
%v64017 = vshll.u32 %v64012, 15
%v64018 = vshrl.u32 %v64012, 17
%v64019 = vor.u32 %v64018, %v64017
%v64020 = vxor.u32 %v64019, %v64015
%v64023 = vadd.s32 %v64020, %v64015
%v64025 = vshll.u32 %v64020, 26
%v64026 = vshrl.u32 %v64020, 6
%v64027 = vor.u32 %v64026, %v64025
%v64028 = vxor.u32 %v64027, %v64023
%v64031 = vadd.s32 %v64028, %v64023
%v64035 = vadd.s32 %v64031, %v10
%v64037 = vshll.u32 %v64028, 6
%v64038 = vshrl.u32 %v64028, 26
%v64039 = vor.u32 %v64038, %v64037
%v64040 = vxor.u32 %v64039, %v64031
%v64043 = vadd.s32 %v64040, %v9
%v64047 = vadd.s32 3, %v64043
%v64051 = vadd.s32 %v64047, %v64035
%v64053 = vshll.u32 %v64047, 17
%v64054 = vshrl.u32 %v64047, 15
%v64055 = vor.u32 %v64054, %v64053
%v64056 = vxor.u32 %v64055, %v64051
%v64059 = vadd.s32 %v64056, %v64051
%v64061 = vshll.u32 %v64056, 29
%v64062 = vshrl.u32 %v64056, 3
%v64063 = vor.u32 %v64062, %v64061
%v64064 = vxor.u32 %v64063, %v64059
%v64067 = vadd.s32 %v64064, %v64059
%v64069 = vshll.u32 %v64064, 16
%v64070 = vshrl.u32 %v64064, 16
%v64071 = vor.u32 %v64070, %v64069
%v64072 = vxor.u32 %v64071, %v64067
%v64075 = vadd.s32 %v64072, %v64067
%v64079 = vadd.s32 %v64075, %v9
%v64081 = vshll.u32 %v64072, 24
%v64082 = vshrl.u32 %v64072, 8
%v64083 = vor.u32 %v64082, %v64081
%v64084 = vxor.u32 %v64083, %v64075
%v64087 = vadd.s32 %v64084, %v8
%v64091 = vadd.s32 4, %v64087
%v64095 = vadd.s32 %v64091, %v64079
%v64097 = vshll.u32 %v64091, 13
%v64098 = vshrl.u32 %v64091, 19
%v64099 = vor.u32 %v64098, %v64097
%v64100 = vxor.u32 %v64099, %v64095
%v64103 = vadd.s32 %v64100, %v64095
%v64105 = vshll.u32 %v64100, 15
%v64106 = vshrl.u32 %v64100, 17
%v64107 = vor.u32 %v64106, %v64105
%v64108 = vxor.u32 %v64107, %v64103
%v64111 = vadd.s32 %v64108, %v64103
%v64113 = vshll.u32 %v64108, 26
%v64114 = vshrl.u32 %v64108, 6
%v64115 = vor.u32 %v64114, %v64113
%v64116 = vxor.u32 %v64115, %v64111
%v64119 = vadd.s32 %v64116, %v64111
%v64123 = vadd.s32 %v64119, %v8
%v64125 = vshll.u32 %v64116, 6
%v64126 = vshrl.u32 %v64116, 26
%v64127 = vor.u32 %v64126, %v64125
%v64128 = vxor.u32 %v64127, %v64119
%v64131 = vadd.s32 %v64128, %v10
%v64135 = vadd.s32 5, %v64131
%v64137 = vxor.u32 %v64135, %v64123
%v64138 = vand.u32.u8 255, %v64137
%v64139 = vand.u32 65535, %v64138
%v64140 = vshrl.u32 %v64139, 1
%v64141 = vor.u32 16256, %v64140
%v64142 = vand.u32.u16 65535, %v64141
%v120088 = vadd.low.f32.bf16 -1.0, %v64142
%v64151 = vmul.f32 2.0, %v120088
%v64155 = vadd.f32 -0.99609375, %v64151
%v64159 = vmax.f32 %v64155, -0.99609375
%v64161 = vand.u32 2147483647, %v64159
%vm64164 = vcmp.eq.f32.partialorder %v64161, 1.0
%v64169 = vmul.f32 inf, %v64159
%v64171 = vxor.u32 2147483648, %v64159
%v64174 = vmul.f32 %v64171, %v64159
%v64176 = vadd.f32 1.0, %v64174
%v64177 = vlog2.pop %v64176
%v64178 = vmul.f32 0.6931472, %v64177
%v64179 = vmul.f32 -0.5, %v64174
%v64180 = vadd.f32 1.0, %v64179
%v64181 = vmul.f32 %v64180, %v64174
%v64182 = vand.u32 2147483647, %v64174
%vm64183 = vcmp.lt.f32.partialorder %v64182, 0.0004427343
%v64184 = vsel /*vm=*/%vm64183, /*on_true_vy=*/%v64181, /*on_false_vx=*/%v64178
%v64185 = vxor.u32 2147483648, %v64184
%vm64188 = vcmp.lt.f32.partialorder %v64185, 5.0
%v64193 = vsel /*vm=*/%vm64188, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v64197 = vsel /*vm=*/%vm64188, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v64201 = vsel /*vm=*/%vm64188, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v64205 = vsel /*vm=*/%vm64188, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v64209 = vsel /*vm=*/%vm64188, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v64213 = vsel /*vm=*/%vm64188, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v64217 = vsel /*vm=*/%vm64188, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v64221 = vsel /*vm=*/%vm64188, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v64225 = vsel /*vm=*/%vm64188, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v64229 = vadd.f32 -2.5, %v64185
%v64231 = vrsqrt.pop %v64185
%v64232 = vmul.f32 %v64231, %v64185
%vm64233 = vcmp.eq.f32.partialorder %v64185, inf
%v64234 = vsel /*vm=*/%vm64233, /*on_true_vy=*/%v64185, /*on_false_vx=*/%v64232
%vm64235 = vcmp.eq.f32.partialorder %v64185, 0.0
%v64236 = vand.u32 2147483648, %v64185
%v64237 = vsel /*vm=*/%vm64235, /*on_true_vy=*/%v64236, /*on_false_vx=*/%v64234
%v64240 = vadd.f32 -3.0, %v64237
%v64244 = vsel /*vm=*/%vm64188, /*on_true_vy=*/%v64229, /*on_false_vx=*/%v64240
%v64248 = vmul.f32 %v64244, %v64225
%v64252 = vadd.f32 %v64248, %v64221
%v64256 = vmul.f32 %v64252, %v64244
%v64260 = vadd.f32 %v64256, %v64217
%v64264 = vmul.f32 %v64260, %v64244
%v64268 = vadd.f32 %v64264, %v64213
%v64272 = vmul.f32 %v64268, %v64244
%v64276 = vadd.f32 %v64272, %v64209
%v64280 = vmul.f32 %v64276, %v64244
%v64284 = vadd.f32 %v64280, %v64205
%v64288 = vmul.f32 %v64284, %v64244
%v64292 = vadd.f32 %v64288, %v64201
%v64296 = vmul.f32 %v64292, %v64244
%v64300 = vadd.f32 %v64296, %v64197
%v64304 = vmul.f32 %v64300, %v64244
%v64308 = vadd.f32 %v64304, %v64193
%v64312 = vmul.f32 %v64308, %v64159
%v64316 = vsel /*vm=*/%vm64164, /*on_true_vy=*/%v64169, /*on_false_vx=*/%v64312
%v64320 = vmul.f32 1.4140625, %v64316
%v64323 = vpack.c.bf16 %v120417, %v64320
%120089 = vst [vmem:[%s280 + $0x44] sm:$0xf] /*vst_source=*/%v64323
%v64327 = vadd.s32 %v63863, %v894
%v64337 = vadd.s32 %v64327, %v415
%vm64341 = vcmp.lt.u32.totalorder %v64337, %v64327
%vm64346 = vcmp.lt.u32.totalorder %v64327, %v894
%v64351 = vadd.s32 %v63846, %v881
%v64355 = vadd.s32 1, %v64351
%v64359 = vsel /*vm=*/%vm64346, /*on_true_vy=*/%v64355, /*on_false_vx=*/%v64351
%v64363 = vadd.s32 1, %v64359
%v64367 = vsel /*vm=*/%vm64341, /*on_true_vy=*/%v64363, /*on_false_vx=*/%v64359
%v64372 = vadd.s32 %v64367, %v10
%v64376 = vadd.s32 %v64337, %v9
%v64380 = vadd.s32 %v64376, %v64372
%v64382 = vshll.u32 %v64376, 13
%v64383 = vshrl.u32 %v64376, 19
%v64384 = vor.u32 %v64383, %v64382
%v64385 = vxor.u32 %v64384, %v64380
%v64388 = vadd.s32 %v64385, %v64380
%v64390 = vshll.u32 %v64385, 15
%v64391 = vshrl.u32 %v64385, 17
%v64392 = vor.u32 %v64391, %v64390
%v64393 = vxor.u32 %v64392, %v64388
%v64396 = vadd.s32 %v64393, %v64388
%v64398 = vshll.u32 %v64393, 26
%v64399 = vshrl.u32 %v64393, 6
%v64400 = vor.u32 %v64399, %v64398
%v64401 = vxor.u32 %v64400, %v64396
%v64404 = vadd.s32 %v64401, %v64396
%v64408 = vadd.s32 %v64404, %v9
%v64410 = vshll.u32 %v64401, 6
%v64411 = vshrl.u32 %v64401, 26
%v64412 = vor.u32 %v64411, %v64410
%v64413 = vxor.u32 %v64412, %v64404
%v64416 = vadd.s32 %v64413, %v8
%v64420 = vadd.s32 1, %v64416
%v64424 = vadd.s32 %v64420, %v64408
%v64426 = vshll.u32 %v64420, 17
%v64427 = vshrl.u32 %v64420, 15
%v64428 = vor.u32 %v64427, %v64426
%v64429 = vxor.u32 %v64428, %v64424
%v64432 = vadd.s32 %v64429, %v64424
%v64434 = vshll.u32 %v64429, 29
%v64435 = vshrl.u32 %v64429, 3
%v64436 = vor.u32 %v64435, %v64434
%v64437 = vxor.u32 %v64436, %v64432
%v64440 = vadd.s32 %v64437, %v64432
%v64442 = vshll.u32 %v64437, 16
%v64443 = vshrl.u32 %v64437, 16
%v64444 = vor.u32 %v64443, %v64442
%v64445 = vxor.u32 %v64444, %v64440
%v64448 = vadd.s32 %v64445, %v64440
%v64452 = vadd.s32 %v64448, %v8
%v64454 = vshll.u32 %v64445, 24
%v64455 = vshrl.u32 %v64445, 8
%v64456 = vor.u32 %v64455, %v64454
%v64457 = vxor.u32 %v64456, %v64448
%v64460 = vadd.s32 %v64457, %v10
%v64464 = vadd.s32 2, %v64460
%v64468 = vadd.s32 %v64464, %v64452
%v64470 = vshll.u32 %v64464, 13
%v64471 = vshrl.u32 %v64464, 19
%v64472 = vor.u32 %v64471, %v64470
%v64473 = vxor.u32 %v64472, %v64468
%v64476 = vadd.s32 %v64473, %v64468
%v64478 = vshll.u32 %v64473, 15
%v64479 = vshrl.u32 %v64473, 17
%v64480 = vor.u32 %v64479, %v64478
%v64481 = vxor.u32 %v64480, %v64476
%v64484 = vadd.s32 %v64481, %v64476
%v64486 = vshll.u32 %v64481, 26
%v64487 = vshrl.u32 %v64481, 6
%v64488 = vor.u32 %v64487, %v64486
%v64489 = vxor.u32 %v64488, %v64484
%v64492 = vadd.s32 %v64489, %v64484
%v64496 = vadd.s32 %v64492, %v10
%v64498 = vshll.u32 %v64489, 6
%v64499 = vshrl.u32 %v64489, 26
%v64500 = vor.u32 %v64499, %v64498
%v64501 = vxor.u32 %v64500, %v64492
%v64504 = vadd.s32 %v64501, %v9
%v64508 = vadd.s32 3, %v64504
%v64512 = vadd.s32 %v64508, %v64496
%v64514 = vshll.u32 %v64508, 17
%v64515 = vshrl.u32 %v64508, 15
%v64516 = vor.u32 %v64515, %v64514
%v64517 = vxor.u32 %v64516, %v64512
%v64520 = vadd.s32 %v64517, %v64512
%v64522 = vshll.u32 %v64517, 29
%v64523 = vshrl.u32 %v64517, 3
%v64524 = vor.u32 %v64523, %v64522
%v64525 = vxor.u32 %v64524, %v64520
%v64528 = vadd.s32 %v64525, %v64520
%v64530 = vshll.u32 %v64525, 16
%v64531 = vshrl.u32 %v64525, 16
%v64532 = vor.u32 %v64531, %v64530
%v64533 = vxor.u32 %v64532, %v64528
%v64536 = vadd.s32 %v64533, %v64528
%v64540 = vadd.s32 %v64536, %v9
%v64542 = vshll.u32 %v64533, 24
%v64543 = vshrl.u32 %v64533, 8
%v64544 = vor.u32 %v64543, %v64542
%v64545 = vxor.u32 %v64544, %v64536
%v64548 = vadd.s32 %v64545, %v8
%v64552 = vadd.s32 4, %v64548
%v64556 = vadd.s32 %v64552, %v64540
%v64558 = vshll.u32 %v64552, 13
%v64559 = vshrl.u32 %v64552, 19
%v64560 = vor.u32 %v64559, %v64558
%v64561 = vxor.u32 %v64560, %v64556
%v64564 = vadd.s32 %v64561, %v64556
%v64566 = vshll.u32 %v64561, 15
%v64567 = vshrl.u32 %v64561, 17
%v64568 = vor.u32 %v64567, %v64566
%v64569 = vxor.u32 %v64568, %v64564
%v64572 = vadd.s32 %v64569, %v64564
%v64574 = vshll.u32 %v64569, 26
%v64575 = vshrl.u32 %v64569, 6
%v64576 = vor.u32 %v64575, %v64574
%v64577 = vxor.u32 %v64576, %v64572
%v64580 = vadd.s32 %v64577, %v64572
%v64584 = vadd.s32 %v64580, %v8
%v64586 = vshll.u32 %v64577, 6
%v64587 = vshrl.u32 %v64577, 26
%v64588 = vor.u32 %v64587, %v64586
%v64589 = vxor.u32 %v64588, %v64580
%v64592 = vadd.s32 %v64589, %v10
%v64596 = vadd.s32 5, %v64592
%v64598 = vxor.u32 %v64596, %v64584
%v64599 = vand.u32.u8 255, %v64598
%v64600 = vand.u32 65535, %v64599
%v64601 = vshrl.u32 %v64600, 1
%v64602 = vor.u32 16256, %v64601
%v64603 = vand.u32.u16 65535, %v64602
%v120090 = vadd.low.f32.bf16 -1.0, %v64603
%v64612 = vmul.f32 2.0, %v120090
%v64616 = vadd.f32 -0.99609375, %v64612
%v64620 = vmax.f32 %v64616, -0.99609375
%v64622 = vand.u32 2147483647, %v64620
%vm64625 = vcmp.eq.f32.partialorder %v64622, 1.0
%v64630 = vmul.f32 inf, %v64620
%v64632 = vxor.u32 2147483648, %v64620
%v64635 = vmul.f32 %v64632, %v64620
%v64637 = vadd.f32 1.0, %v64635
%v64638 = vlog2.pop %v64637
%v64639 = vmul.f32 0.6931472, %v64638
%v64640 = vmul.f32 -0.5, %v64635
%v64641 = vadd.f32 1.0, %v64640
%v64642 = vmul.f32 %v64641, %v64635
%v64643 = vand.u32 2147483647, %v64635
%vm64644 = vcmp.lt.f32.partialorder %v64643, 0.0004427343
%v64645 = vsel /*vm=*/%vm64644, /*on_true_vy=*/%v64642, /*on_false_vx=*/%v64639
%v64646 = vxor.u32 2147483648, %v64645
%vm64649 = vcmp.lt.f32.partialorder %v64646, 5.0
%v64654 = vsel /*vm=*/%vm64649, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v64658 = vsel /*vm=*/%vm64649, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v64662 = vsel /*vm=*/%vm64649, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v64666 = vsel /*vm=*/%vm64649, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v64670 = vsel /*vm=*/%vm64649, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v64674 = vsel /*vm=*/%vm64649, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v64678 = vsel /*vm=*/%vm64649, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v64682 = vsel /*vm=*/%vm64649, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v64686 = vsel /*vm=*/%vm64649, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v64690 = vadd.f32 -2.5, %v64646
%v64692 = vrsqrt.pop %v64646
%v64693 = vmul.f32 %v64692, %v64646
%vm64694 = vcmp.eq.f32.partialorder %v64646, inf
%v64695 = vsel /*vm=*/%vm64694, /*on_true_vy=*/%v64646, /*on_false_vx=*/%v64693
%vm64696 = vcmp.eq.f32.partialorder %v64646, 0.0
%v64697 = vand.u32 2147483648, %v64646
%v64698 = vsel /*vm=*/%vm64696, /*on_true_vy=*/%v64697, /*on_false_vx=*/%v64695
%v64701 = vadd.f32 -3.0, %v64698
%v64705 = vsel /*vm=*/%vm64649, /*on_true_vy=*/%v64690, /*on_false_vx=*/%v64701
%v64709 = vmul.f32 %v64705, %v64686
%v64713 = vadd.f32 %v64709, %v64682
%v64717 = vmul.f32 %v64713, %v64705
%v64721 = vadd.f32 %v64717, %v64678
%v64725 = vmul.f32 %v64721, %v64705
%v64729 = vadd.f32 %v64725, %v64674
%v64733 = vmul.f32 %v64729, %v64705
%v64737 = vadd.f32 %v64733, %v64670
%v64741 = vmul.f32 %v64737, %v64705
%v64745 = vadd.f32 %v64741, %v64666
%v64749 = vmul.f32 %v64745, %v64705
%v64753 = vadd.f32 %v64749, %v64662
%v64757 = vmul.f32 %v64753, %v64705
%v64761 = vadd.f32 %v64757, %v64658
%v64765 = vmul.f32 %v64761, %v64705
%v64769 = vadd.f32 %v64765, %v64654
%v64773 = vmul.f32 %v64769, %v64620
%v64777 = vsel /*vm=*/%vm64625, /*on_true_vy=*/%v64630, /*on_false_vx=*/%v64773
%v64781 = vmul.f32 1.4140625, %v64777
%v64784 = vpack.c.bf16 %v120417, %v64781
%120091 = vst [vmem:[%s280 + $0xc4] sm:$0xf] /*vst_source=*/%v64784
%v64788 = vadd.s32 %v63863, %v1381
%v64798 = vadd.s32 %v64788, %v415
%vm64802 = vcmp.lt.u32.totalorder %v64798, %v64788
%vm64807 = vcmp.lt.u32.totalorder %v64788, %v1381
%v64812 = vadd.s32 %v63846, %v1368
%v64816 = vadd.s32 1, %v64812
%v64820 = vsel /*vm=*/%vm64807, /*on_true_vy=*/%v64816, /*on_false_vx=*/%v64812
%v64824 = vadd.s32 1, %v64820
%v64828 = vsel /*vm=*/%vm64802, /*on_true_vy=*/%v64824, /*on_false_vx=*/%v64820
%v64833 = vadd.s32 %v64828, %v10
%v64837 = vadd.s32 %v64798, %v9
%v64841 = vadd.s32 %v64837, %v64833
%v64843 = vshll.u32 %v64837, 13
%v64844 = vshrl.u32 %v64837, 19
%v64845 = vor.u32 %v64844, %v64843
%v64846 = vxor.u32 %v64845, %v64841
%v64849 = vadd.s32 %v64846, %v64841
%v64851 = vshll.u32 %v64846, 15
%v64852 = vshrl.u32 %v64846, 17
%v64853 = vor.u32 %v64852, %v64851
%v64854 = vxor.u32 %v64853, %v64849
%v64857 = vadd.s32 %v64854, %v64849
%v64859 = vshll.u32 %v64854, 26
%v64860 = vshrl.u32 %v64854, 6
%v64861 = vor.u32 %v64860, %v64859
%v64862 = vxor.u32 %v64861, %v64857
%v64865 = vadd.s32 %v64862, %v64857
%v64869 = vadd.s32 %v64865, %v9
%v64871 = vshll.u32 %v64862, 6
%v64872 = vshrl.u32 %v64862, 26
%v64873 = vor.u32 %v64872, %v64871
%v64874 = vxor.u32 %v64873, %v64865
%v64877 = vadd.s32 %v64874, %v8
%v64881 = vadd.s32 1, %v64877
%v64885 = vadd.s32 %v64881, %v64869
%v64887 = vshll.u32 %v64881, 17
%v64888 = vshrl.u32 %v64881, 15
%v64889 = vor.u32 %v64888, %v64887
%v64890 = vxor.u32 %v64889, %v64885
%v64893 = vadd.s32 %v64890, %v64885
%v64895 = vshll.u32 %v64890, 29
%v64896 = vshrl.u32 %v64890, 3
%v64897 = vor.u32 %v64896, %v64895
%v64898 = vxor.u32 %v64897, %v64893
%v64901 = vadd.s32 %v64898, %v64893
%v64903 = vshll.u32 %v64898, 16
%v64904 = vshrl.u32 %v64898, 16
%v64905 = vor.u32 %v64904, %v64903
%v64906 = vxor.u32 %v64905, %v64901
%v64909 = vadd.s32 %v64906, %v64901
%v64913 = vadd.s32 %v64909, %v8
%v64915 = vshll.u32 %v64906, 24
%v64916 = vshrl.u32 %v64906, 8
%v64917 = vor.u32 %v64916, %v64915
%v64918 = vxor.u32 %v64917, %v64909
%v64921 = vadd.s32 %v64918, %v10
%v64925 = vadd.s32 2, %v64921
%v64929 = vadd.s32 %v64925, %v64913
%v64931 = vshll.u32 %v64925, 13
%v64932 = vshrl.u32 %v64925, 19
%v64933 = vor.u32 %v64932, %v64931
%v64934 = vxor.u32 %v64933, %v64929
%v64937 = vadd.s32 %v64934, %v64929
%v64939 = vshll.u32 %v64934, 15
%v64940 = vshrl.u32 %v64934, 17
%v64941 = vor.u32 %v64940, %v64939
%v64942 = vxor.u32 %v64941, %v64937
%v64945 = vadd.s32 %v64942, %v64937
%v64947 = vshll.u32 %v64942, 26
%v64948 = vshrl.u32 %v64942, 6
%v64949 = vor.u32 %v64948, %v64947
%v64950 = vxor.u32 %v64949, %v64945
%v64953 = vadd.s32 %v64950, %v64945
%v64957 = vadd.s32 %v64953, %v10
%v64959 = vshll.u32 %v64950, 6
%v64960 = vshrl.u32 %v64950, 26
%v64961 = vor.u32 %v64960, %v64959
%v64962 = vxor.u32 %v64961, %v64953
%v64965 = vadd.s32 %v64962, %v9
%v64969 = vadd.s32 3, %v64965
%v64973 = vadd.s32 %v64969, %v64957
%v64975 = vshll.u32 %v64969, 17
%v64976 = vshrl.u32 %v64969, 15
%v64977 = vor.u32 %v64976, %v64975
%v64978 = vxor.u32 %v64977, %v64973
%v64981 = vadd.s32 %v64978, %v64973
%v64983 = vshll.u32 %v64978, 29
%v64984 = vshrl.u32 %v64978, 3
%v64985 = vor.u32 %v64984, %v64983
%v64986 = vxor.u32 %v64985, %v64981
%v64989 = vadd.s32 %v64986, %v64981
%v64991 = vshll.u32 %v64986, 16
%v64992 = vshrl.u32 %v64986, 16
%v64993 = vor.u32 %v64992, %v64991
%v64994 = vxor.u32 %v64993, %v64989
%v64997 = vadd.s32 %v64994, %v64989
%v65001 = vadd.s32 %v64997, %v9
%v65003 = vshll.u32 %v64994, 24
%v65004 = vshrl.u32 %v64994, 8
%v65005 = vor.u32 %v65004, %v65003
%v65006 = vxor.u32 %v65005, %v64997
%v65009 = vadd.s32 %v65006, %v8
%v65013 = vadd.s32 4, %v65009
%v65017 = vadd.s32 %v65013, %v65001
%v65019 = vshll.u32 %v65013, 13
%v65020 = vshrl.u32 %v65013, 19
%v65021 = vor.u32 %v65020, %v65019
%v65022 = vxor.u32 %v65021, %v65017
%v65025 = vadd.s32 %v65022, %v65017
%v65027 = vshll.u32 %v65022, 15
%v65028 = vshrl.u32 %v65022, 17
%v65029 = vor.u32 %v65028, %v65027
%v65030 = vxor.u32 %v65029, %v65025
%v65033 = vadd.s32 %v65030, %v65025
%v65035 = vshll.u32 %v65030, 26
%v65036 = vshrl.u32 %v65030, 6
%v65037 = vor.u32 %v65036, %v65035
%v65038 = vxor.u32 %v65037, %v65033
%v65041 = vadd.s32 %v65038, %v65033
%v65045 = vadd.s32 %v65041, %v8
%v65047 = vshll.u32 %v65038, 6
%v65048 = vshrl.u32 %v65038, 26
%v65049 = vor.u32 %v65048, %v65047
%v65050 = vxor.u32 %v65049, %v65041
%v65053 = vadd.s32 %v65050, %v10
%v65057 = vadd.s32 5, %v65053
%v65059 = vxor.u32 %v65057, %v65045
%v65060 = vand.u32.u8 255, %v65059
%v65061 = vand.u32 65535, %v65060
%v65062 = vshrl.u32 %v65061, 1
%v65063 = vor.u32 16256, %v65062
%v65064 = vand.u32.u16 65535, %v65063
%v120092 = vadd.low.f32.bf16 -1.0, %v65064
%v65073 = vmul.f32 2.0, %v120092
%v65077 = vadd.f32 -0.99609375, %v65073
%v65081 = vmax.f32 %v65077, -0.99609375
%v65083 = vand.u32 2147483647, %v65081
%vm65086 = vcmp.eq.f32.partialorder %v65083, 1.0
%v65091 = vmul.f32 inf, %v65081
%v65093 = vxor.u32 2147483648, %v65081
%v65096 = vmul.f32 %v65093, %v65081
%v65098 = vadd.f32 1.0, %v65096
%v65099 = vlog2.pop %v65098
%v65100 = vmul.f32 0.6931472, %v65099
%v65101 = vmul.f32 -0.5, %v65096
%v65102 = vadd.f32 1.0, %v65101
%v65103 = vmul.f32 %v65102, %v65096
%v65104 = vand.u32 2147483647, %v65096
%vm65105 = vcmp.lt.f32.partialorder %v65104, 0.0004427343
%v65106 = vsel /*vm=*/%vm65105, /*on_true_vy=*/%v65103, /*on_false_vx=*/%v65100
%v65107 = vxor.u32 2147483648, %v65106
%vm65110 = vcmp.lt.f32.partialorder %v65107, 5.0
%v65115 = vsel /*vm=*/%vm65110, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v65119 = vsel /*vm=*/%vm65110, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v65123 = vsel /*vm=*/%vm65110, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v65127 = vsel /*vm=*/%vm65110, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v65131 = vsel /*vm=*/%vm65110, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v65135 = vsel /*vm=*/%vm65110, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v65139 = vsel /*vm=*/%vm65110, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v65143 = vsel /*vm=*/%vm65110, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v65147 = vsel /*vm=*/%vm65110, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v65151 = vadd.f32 -2.5, %v65107
%v65153 = vrsqrt.pop %v65107
%v65154 = vmul.f32 %v65153, %v65107
%vm65155 = vcmp.eq.f32.partialorder %v65107, inf
%v65156 = vsel /*vm=*/%vm65155, /*on_true_vy=*/%v65107, /*on_false_vx=*/%v65154
%vm65157 = vcmp.eq.f32.partialorder %v65107, 0.0
%v65158 = vand.u32 2147483648, %v65107
%v65159 = vsel /*vm=*/%vm65157, /*on_true_vy=*/%v65158, /*on_false_vx=*/%v65156
%v65162 = vadd.f32 -3.0, %v65159
%v65166 = vsel /*vm=*/%vm65110, /*on_true_vy=*/%v65151, /*on_false_vx=*/%v65162
%v65170 = vmul.f32 %v65166, %v65147
%v65174 = vadd.f32 %v65170, %v65143
%v65178 = vmul.f32 %v65174, %v65166
%v65182 = vadd.f32 %v65178, %v65139
%v65186 = vmul.f32 %v65182, %v65166
%v65190 = vadd.f32 %v65186, %v65135
%v65194 = vmul.f32 %v65190, %v65166
%v65198 = vadd.f32 %v65194, %v65131
%v65202 = vmul.f32 %v65198, %v65166
%v65206 = vadd.f32 %v65202, %v65127
%v65210 = vmul.f32 %v65206, %v65166
%v65214 = vadd.f32 %v65210, %v65123
%v65218 = vmul.f32 %v65214, %v65166
%v65222 = vadd.f32 %v65218, %v65119
%v65226 = vmul.f32 %v65222, %v65166
%v65230 = vadd.f32 %v65226, %v65115
%v65234 = vmul.f32 %v65230, %v65081
%v65238 = vsel /*vm=*/%vm65086, /*on_true_vy=*/%v65091, /*on_false_vx=*/%v65234
%v65242 = vmul.f32 1.4140625, %v65238
%v65245 = vpack.c.bf16 %v120417, %v65242
%120093 = vst [vmem:[%s280 + $0x144] sm:$0xf] /*vst_source=*/%v65245
%v65249 = vadd.s32 %v63863, %v1868
%v65259 = vadd.s32 %v65249, %v415
%vm65263 = vcmp.lt.u32.totalorder %v65259, %v65249
%vm65268 = vcmp.lt.u32.totalorder %v65249, %v1868
%v65273 = vadd.s32 %v63846, %v1855
%v65277 = vadd.s32 1, %v65273
%v65281 = vsel /*vm=*/%vm65268, /*on_true_vy=*/%v65277, /*on_false_vx=*/%v65273
%v65285 = vadd.s32 1, %v65281
%v65289 = vsel /*vm=*/%vm65263, /*on_true_vy=*/%v65285, /*on_false_vx=*/%v65281
%v65294 = vadd.s32 %v65289, %v10
%v65298 = vadd.s32 %v65259, %v9
%v65302 = vadd.s32 %v65298, %v65294
%v65304 = vshll.u32 %v65298, 13
%v65305 = vshrl.u32 %v65298, 19
%v65306 = vor.u32 %v65305, %v65304
%v65307 = vxor.u32 %v65306, %v65302
%v65310 = vadd.s32 %v65307, %v65302
%v65312 = vshll.u32 %v65307, 15
%v65313 = vshrl.u32 %v65307, 17
%v65314 = vor.u32 %v65313, %v65312
%v65315 = vxor.u32 %v65314, %v65310
%v65318 = vadd.s32 %v65315, %v65310
%v65320 = vshll.u32 %v65315, 26
%v65321 = vshrl.u32 %v65315, 6
%v65322 = vor.u32 %v65321, %v65320
%v65323 = vxor.u32 %v65322, %v65318
%v65326 = vadd.s32 %v65323, %v65318
%v65330 = vadd.s32 %v65326, %v9
%v65332 = vshll.u32 %v65323, 6
%v65333 = vshrl.u32 %v65323, 26
%v65334 = vor.u32 %v65333, %v65332
%v65335 = vxor.u32 %v65334, %v65326
%v65338 = vadd.s32 %v65335, %v8
%v65342 = vadd.s32 1, %v65338
%v65346 = vadd.s32 %v65342, %v65330
%v65348 = vshll.u32 %v65342, 17
%v65349 = vshrl.u32 %v65342, 15
%v65350 = vor.u32 %v65349, %v65348
%v65351 = vxor.u32 %v65350, %v65346
%v65354 = vadd.s32 %v65351, %v65346
%v65356 = vshll.u32 %v65351, 29
%v65357 = vshrl.u32 %v65351, 3
%v65358 = vor.u32 %v65357, %v65356
%v65359 = vxor.u32 %v65358, %v65354
%v65362 = vadd.s32 %v65359, %v65354
%v65364 = vshll.u32 %v65359, 16
%v65365 = vshrl.u32 %v65359, 16
%v65366 = vor.u32 %v65365, %v65364
%v65367 = vxor.u32 %v65366, %v65362
%v65370 = vadd.s32 %v65367, %v65362
%v65374 = vadd.s32 %v65370, %v8
%v65376 = vshll.u32 %v65367, 24
%v65377 = vshrl.u32 %v65367, 8
%v65378 = vor.u32 %v65377, %v65376
%v65379 = vxor.u32 %v65378, %v65370
%v65382 = vadd.s32 %v65379, %v10
%v65386 = vadd.s32 2, %v65382
%v65390 = vadd.s32 %v65386, %v65374
%v65392 = vshll.u32 %v65386, 13
%v65393 = vshrl.u32 %v65386, 19
%v65394 = vor.u32 %v65393, %v65392
%v65395 = vxor.u32 %v65394, %v65390
%v65398 = vadd.s32 %v65395, %v65390
%v65400 = vshll.u32 %v65395, 15
%v65401 = vshrl.u32 %v65395, 17
%v65402 = vor.u32 %v65401, %v65400
%v65403 = vxor.u32 %v65402, %v65398
%v65406 = vadd.s32 %v65403, %v65398
%v65408 = vshll.u32 %v65403, 26
%v65409 = vshrl.u32 %v65403, 6
%v65410 = vor.u32 %v65409, %v65408
%v65411 = vxor.u32 %v65410, %v65406
%v65414 = vadd.s32 %v65411, %v65406
%v65418 = vadd.s32 %v65414, %v10
%v65420 = vshll.u32 %v65411, 6
%v65421 = vshrl.u32 %v65411, 26
%v65422 = vor.u32 %v65421, %v65420
%v65423 = vxor.u32 %v65422, %v65414
%v65426 = vadd.s32 %v65423, %v9
%v65430 = vadd.s32 3, %v65426
%v65434 = vadd.s32 %v65430, %v65418
%v65436 = vshll.u32 %v65430, 17
%v65437 = vshrl.u32 %v65430, 15
%v65438 = vor.u32 %v65437, %v65436
%v65439 = vxor.u32 %v65438, %v65434
%v65442 = vadd.s32 %v65439, %v65434
%v65444 = vshll.u32 %v65439, 29
%v65445 = vshrl.u32 %v65439, 3
%v65446 = vor.u32 %v65445, %v65444
%v65447 = vxor.u32 %v65446, %v65442
%v65450 = vadd.s32 %v65447, %v65442
%v65452 = vshll.u32 %v65447, 16
%v65453 = vshrl.u32 %v65447, 16
%v65454 = vor.u32 %v65453, %v65452
%v65455 = vxor.u32 %v65454, %v65450
%v65458 = vadd.s32 %v65455, %v65450
%v65462 = vadd.s32 %v65458, %v9
%v65464 = vshll.u32 %v65455, 24
%v65465 = vshrl.u32 %v65455, 8
%v65466 = vor.u32 %v65465, %v65464
%v65467 = vxor.u32 %v65466, %v65458
%v65470 = vadd.s32 %v65467, %v8
%v65474 = vadd.s32 4, %v65470
%v65478 = vadd.s32 %v65474, %v65462
%v65480 = vshll.u32 %v65474, 13
%v65481 = vshrl.u32 %v65474, 19
%v65482 = vor.u32 %v65481, %v65480
%v65483 = vxor.u32 %v65482, %v65478
%v65486 = vadd.s32 %v65483, %v65478
%v65488 = vshll.u32 %v65483, 15
%v65489 = vshrl.u32 %v65483, 17
%v65490 = vor.u32 %v65489, %v65488
%v65491 = vxor.u32 %v65490, %v65486
%v65494 = vadd.s32 %v65491, %v65486
%v65496 = vshll.u32 %v65491, 26
%v65497 = vshrl.u32 %v65491, 6
%v65498 = vor.u32 %v65497, %v65496
%v65499 = vxor.u32 %v65498, %v65494
%v65502 = vadd.s32 %v65499, %v65494
%v65506 = vadd.s32 %v65502, %v8
%v65508 = vshll.u32 %v65499, 6
%v65509 = vshrl.u32 %v65499, 26
%v65510 = vor.u32 %v65509, %v65508
%v65511 = vxor.u32 %v65510, %v65502
%v65514 = vadd.s32 %v65511, %v10
%v65518 = vadd.s32 5, %v65514
%v65520 = vxor.u32 %v65518, %v65506
%v65521 = vand.u32.u8 255, %v65520
%v65522 = vand.u32 65535, %v65521
%v65523 = vshrl.u32 %v65522, 1
%v65524 = vor.u32 16256, %v65523
%v65525 = vand.u32.u16 65535, %v65524
%v120094 = vadd.low.f32.bf16 -1.0, %v65525
%v65534 = vmul.f32 2.0, %v120094
%v65538 = vadd.f32 -0.99609375, %v65534
%v65542 = vmax.f32 %v65538, -0.99609375
%v65544 = vand.u32 2147483647, %v65542
%vm65547 = vcmp.eq.f32.partialorder %v65544, 1.0
%v65552 = vmul.f32 inf, %v65542
%v65554 = vxor.u32 2147483648, %v65542
%v65557 = vmul.f32 %v65554, %v65542
%v65559 = vadd.f32 1.0, %v65557
%v65560 = vlog2.pop %v65559
%v65561 = vmul.f32 0.6931472, %v65560
%v65562 = vmul.f32 -0.5, %v65557
%v65563 = vadd.f32 1.0, %v65562
%v65564 = vmul.f32 %v65563, %v65557
%v65565 = vand.u32 2147483647, %v65557
%vm65566 = vcmp.lt.f32.partialorder %v65565, 0.0004427343
%v65567 = vsel /*vm=*/%vm65566, /*on_true_vy=*/%v65564, /*on_false_vx=*/%v65561
%v65568 = vxor.u32 2147483648, %v65567
%vm65571 = vcmp.lt.f32.partialorder %v65568, 5.0
%v65576 = vsel /*vm=*/%vm65571, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v65580 = vsel /*vm=*/%vm65571, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v65584 = vsel /*vm=*/%vm65571, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v65588 = vsel /*vm=*/%vm65571, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v65592 = vsel /*vm=*/%vm65571, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v65596 = vsel /*vm=*/%vm65571, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v65600 = vsel /*vm=*/%vm65571, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v65604 = vsel /*vm=*/%vm65571, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v65608 = vsel /*vm=*/%vm65571, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v65612 = vadd.f32 -2.5, %v65568
%v65614 = vrsqrt.pop %v65568
%v65615 = vmul.f32 %v65614, %v65568
%vm65616 = vcmp.eq.f32.partialorder %v65568, inf
%v65617 = vsel /*vm=*/%vm65616, /*on_true_vy=*/%v65568, /*on_false_vx=*/%v65615
%vm65618 = vcmp.eq.f32.partialorder %v65568, 0.0
%v65619 = vand.u32 2147483648, %v65568
%v65620 = vsel /*vm=*/%vm65618, /*on_true_vy=*/%v65619, /*on_false_vx=*/%v65617
%v65623 = vadd.f32 -3.0, %v65620
%v65627 = vsel /*vm=*/%vm65571, /*on_true_vy=*/%v65612, /*on_false_vx=*/%v65623
%v65631 = vmul.f32 %v65627, %v65608
%v65635 = vadd.f32 %v65631, %v65604
%v65639 = vmul.f32 %v65635, %v65627
%v65643 = vadd.f32 %v65639, %v65600
%v65647 = vmul.f32 %v65643, %v65627
%v65651 = vadd.f32 %v65647, %v65596
%v65655 = vmul.f32 %v65651, %v65627
%v65659 = vadd.f32 %v65655, %v65592
%v65663 = vmul.f32 %v65659, %v65627
%v65667 = vadd.f32 %v65663, %v65588
%v65671 = vmul.f32 %v65667, %v65627
%v65675 = vadd.f32 %v65671, %v65584
%v65679 = vmul.f32 %v65675, %v65627
%v65683 = vadd.f32 %v65679, %v65580
%v65687 = vmul.f32 %v65683, %v65627
%v65691 = vadd.f32 %v65687, %v65576
%v65695 = vmul.f32 %v65691, %v65542
%v65699 = vsel /*vm=*/%vm65547, /*on_true_vy=*/%v65552, /*on_false_vx=*/%v65695
%v65703 = vmul.f32 1.4140625, %v65699
%v65706 = vpack.c.bf16 %v120417, %v65703
%120095 = vst [vmem:[%s280 + $0x1c4] sm:$0xf] /*vst_source=*/%v65706
%v65710 = vadd.s32 %v63863, %v2355
%v65720 = vadd.s32 %v65710, %v415
%vm65724 = vcmp.lt.u32.totalorder %v65720, %v65710
%vm65729 = vcmp.lt.u32.totalorder %v65710, %v2355
%v65734 = vadd.s32 %v63846, %v2342
%v65738 = vadd.s32 1, %v65734
%v65742 = vsel /*vm=*/%vm65729, /*on_true_vy=*/%v65738, /*on_false_vx=*/%v65734
%v65746 = vadd.s32 1, %v65742
%v65750 = vsel /*vm=*/%vm65724, /*on_true_vy=*/%v65746, /*on_false_vx=*/%v65742
%v65755 = vadd.s32 %v65750, %v10
%v65759 = vadd.s32 %v65720, %v9
%v65763 = vadd.s32 %v65759, %v65755
%v65765 = vshll.u32 %v65759, 13
%v65766 = vshrl.u32 %v65759, 19
%v65767 = vor.u32 %v65766, %v65765
%v65768 = vxor.u32 %v65767, %v65763
%v65771 = vadd.s32 %v65768, %v65763
%v65773 = vshll.u32 %v65768, 15
%v65774 = vshrl.u32 %v65768, 17
%v65775 = vor.u32 %v65774, %v65773
%v65776 = vxor.u32 %v65775, %v65771
%v65779 = vadd.s32 %v65776, %v65771
%v65781 = vshll.u32 %v65776, 26
%v65782 = vshrl.u32 %v65776, 6
%v65783 = vor.u32 %v65782, %v65781
%v65784 = vxor.u32 %v65783, %v65779
%v65787 = vadd.s32 %v65784, %v65779
%v65791 = vadd.s32 %v65787, %v9
%v65793 = vshll.u32 %v65784, 6
%v65794 = vshrl.u32 %v65784, 26
%v65795 = vor.u32 %v65794, %v65793
%v65796 = vxor.u32 %v65795, %v65787
%v65799 = vadd.s32 %v65796, %v8
%v65803 = vadd.s32 1, %v65799
%v65807 = vadd.s32 %v65803, %v65791
%v65809 = vshll.u32 %v65803, 17
%v65810 = vshrl.u32 %v65803, 15
%v65811 = vor.u32 %v65810, %v65809
%v65812 = vxor.u32 %v65811, %v65807
%v65815 = vadd.s32 %v65812, %v65807
%v65817 = vshll.u32 %v65812, 29
%v65818 = vshrl.u32 %v65812, 3
%v65819 = vor.u32 %v65818, %v65817
%v65820 = vxor.u32 %v65819, %v65815
%v65823 = vadd.s32 %v65820, %v65815
%v65825 = vshll.u32 %v65820, 16
%v65826 = vshrl.u32 %v65820, 16
%v65827 = vor.u32 %v65826, %v65825
%v65828 = vxor.u32 %v65827, %v65823
%v65831 = vadd.s32 %v65828, %v65823
%v65835 = vadd.s32 %v65831, %v8
%v65837 = vshll.u32 %v65828, 24
%v65838 = vshrl.u32 %v65828, 8
%v65839 = vor.u32 %v65838, %v65837
%v65840 = vxor.u32 %v65839, %v65831
%v65843 = vadd.s32 %v65840, %v10
%v65847 = vadd.s32 2, %v65843
%v65851 = vadd.s32 %v65847, %v65835
%v65853 = vshll.u32 %v65847, 13
%v65854 = vshrl.u32 %v65847, 19
%v65855 = vor.u32 %v65854, %v65853
%v65856 = vxor.u32 %v65855, %v65851
%v65859 = vadd.s32 %v65856, %v65851
%v65861 = vshll.u32 %v65856, 15
%v65862 = vshrl.u32 %v65856, 17
%v65863 = vor.u32 %v65862, %v65861
%v65864 = vxor.u32 %v65863, %v65859
%v65867 = vadd.s32 %v65864, %v65859
%v65869 = vshll.u32 %v65864, 26
%v65870 = vshrl.u32 %v65864, 6
%v65871 = vor.u32 %v65870, %v65869
%v65872 = vxor.u32 %v65871, %v65867
%v65875 = vadd.s32 %v65872, %v65867
%v65879 = vadd.s32 %v65875, %v10
%v65881 = vshll.u32 %v65872, 6
%v65882 = vshrl.u32 %v65872, 26
%v65883 = vor.u32 %v65882, %v65881
%v65884 = vxor.u32 %v65883, %v65875
%v65887 = vadd.s32 %v65884, %v9
%v65891 = vadd.s32 3, %v65887
%v65895 = vadd.s32 %v65891, %v65879
%v65897 = vshll.u32 %v65891, 17
%v65898 = vshrl.u32 %v65891, 15
%v65899 = vor.u32 %v65898, %v65897
%v65900 = vxor.u32 %v65899, %v65895
%v65903 = vadd.s32 %v65900, %v65895
%v65905 = vshll.u32 %v65900, 29
%v65906 = vshrl.u32 %v65900, 3
%v65907 = vor.u32 %v65906, %v65905
%v65908 = vxor.u32 %v65907, %v65903
%v65911 = vadd.s32 %v65908, %v65903
%v65913 = vshll.u32 %v65908, 16
%v65914 = vshrl.u32 %v65908, 16
%v65915 = vor.u32 %v65914, %v65913
%v65916 = vxor.u32 %v65915, %v65911
%v65919 = vadd.s32 %v65916, %v65911
%v65923 = vadd.s32 %v65919, %v9
%v65925 = vshll.u32 %v65916, 24
%v65926 = vshrl.u32 %v65916, 8
%v65927 = vor.u32 %v65926, %v65925
%v65928 = vxor.u32 %v65927, %v65919
%v65931 = vadd.s32 %v65928, %v8
%v65935 = vadd.s32 4, %v65931
%v65939 = vadd.s32 %v65935, %v65923
%v65941 = vshll.u32 %v65935, 13
%v65942 = vshrl.u32 %v65935, 19
%v65943 = vor.u32 %v65942, %v65941
%v65944 = vxor.u32 %v65943, %v65939
%v65947 = vadd.s32 %v65944, %v65939
%v65949 = vshll.u32 %v65944, 15
%v65950 = vshrl.u32 %v65944, 17
%v65951 = vor.u32 %v65950, %v65949
%v65952 = vxor.u32 %v65951, %v65947
%v65955 = vadd.s32 %v65952, %v65947
%v65957 = vshll.u32 %v65952, 26
%v65958 = vshrl.u32 %v65952, 6
%v65959 = vor.u32 %v65958, %v65957
%v65960 = vxor.u32 %v65959, %v65955
%v65963 = vadd.s32 %v65960, %v65955
%v65967 = vadd.s32 %v65963, %v8
%v65969 = vshll.u32 %v65960, 6
%v65970 = vshrl.u32 %v65960, 26
%v65971 = vor.u32 %v65970, %v65969
%v65972 = vxor.u32 %v65971, %v65963
%v65975 = vadd.s32 %v65972, %v10
%v65979 = vadd.s32 5, %v65975
%v65981 = vxor.u32 %v65979, %v65967
%v65982 = vand.u32.u8 255, %v65981
%v65983 = vand.u32 65535, %v65982
%v65984 = vshrl.u32 %v65983, 1
%v65985 = vor.u32 16256, %v65984
%v65986 = vand.u32.u16 65535, %v65985
%v120096 = vadd.low.f32.bf16 -1.0, %v65986
%v65995 = vmul.f32 2.0, %v120096
%v65999 = vadd.f32 -0.99609375, %v65995
%v66003 = vmax.f32 %v65999, -0.99609375
%v66005 = vand.u32 2147483647, %v66003
%vm66008 = vcmp.eq.f32.partialorder %v66005, 1.0
%v66013 = vmul.f32 inf, %v66003
%v66015 = vxor.u32 2147483648, %v66003
%v66018 = vmul.f32 %v66015, %v66003
%v66020 = vadd.f32 1.0, %v66018
%v66021 = vlog2.pop %v66020
%v66022 = vmul.f32 0.6931472, %v66021
%v66023 = vmul.f32 -0.5, %v66018
%v66024 = vadd.f32 1.0, %v66023
%v66025 = vmul.f32 %v66024, %v66018
%v66026 = vand.u32 2147483647, %v66018
%vm66027 = vcmp.lt.f32.partialorder %v66026, 0.0004427343
%v66028 = vsel /*vm=*/%vm66027, /*on_true_vy=*/%v66025, /*on_false_vx=*/%v66022
%v66029 = vxor.u32 2147483648, %v66028
%vm66032 = vcmp.lt.f32.partialorder %v66029, 5.0
%v66037 = vsel /*vm=*/%vm66032, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v66041 = vsel /*vm=*/%vm66032, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v66045 = vsel /*vm=*/%vm66032, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v66049 = vsel /*vm=*/%vm66032, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v66053 = vsel /*vm=*/%vm66032, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v66057 = vsel /*vm=*/%vm66032, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v66061 = vsel /*vm=*/%vm66032, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v66065 = vsel /*vm=*/%vm66032, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v66069 = vsel /*vm=*/%vm66032, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v66073 = vadd.f32 -2.5, %v66029
%v66075 = vrsqrt.pop %v66029
%v66076 = vmul.f32 %v66075, %v66029
%vm66077 = vcmp.eq.f32.partialorder %v66029, inf
%v66078 = vsel /*vm=*/%vm66077, /*on_true_vy=*/%v66029, /*on_false_vx=*/%v66076
%vm66079 = vcmp.eq.f32.partialorder %v66029, 0.0
%v66080 = vand.u32 2147483648, %v66029
%v66081 = vsel /*vm=*/%vm66079, /*on_true_vy=*/%v66080, /*on_false_vx=*/%v66078
%v66084 = vadd.f32 -3.0, %v66081
%v66088 = vsel /*vm=*/%vm66032, /*on_true_vy=*/%v66073, /*on_false_vx=*/%v66084
%v66092 = vmul.f32 %v66088, %v66069
%v66096 = vadd.f32 %v66092, %v66065
%v66100 = vmul.f32 %v66096, %v66088
%v66104 = vadd.f32 %v66100, %v66061
%v66108 = vmul.f32 %v66104, %v66088
%v66112 = vadd.f32 %v66108, %v66057
%v66116 = vmul.f32 %v66112, %v66088
%v66120 = vadd.f32 %v66116, %v66053
%v66124 = vmul.f32 %v66120, %v66088
%v66128 = vadd.f32 %v66124, %v66049
%v66132 = vmul.f32 %v66128, %v66088
%v66136 = vadd.f32 %v66132, %v66045
%v66140 = vmul.f32 %v66136, %v66088
%v66144 = vadd.f32 %v66140, %v66041
%v66148 = vmul.f32 %v66144, %v66088
%v66152 = vadd.f32 %v66148, %v66037
%v66156 = vmul.f32 %v66152, %v66003
%v66160 = vsel /*vm=*/%vm66008, /*on_true_vy=*/%v66013, /*on_false_vx=*/%v66156
%v66164 = vmul.f32 1.4140625, %v66160
%v66167 = vpack.c.bf16 %v120417, %v66164
%120097 = vst [vmem:[%s280 + $0x244] sm:$0xf] /*vst_source=*/%v66167
%v66171 = vadd.s32 %v63863, %v2842
%v66181 = vadd.s32 %v66171, %v415
%vm66185 = vcmp.lt.u32.totalorder %v66181, %v66171
%vm66190 = vcmp.lt.u32.totalorder %v66171, %v2842
%v66195 = vadd.s32 %v63846, %v2829
%v66199 = vadd.s32 1, %v66195
%v66203 = vsel /*vm=*/%vm66190, /*on_true_vy=*/%v66199, /*on_false_vx=*/%v66195
%v66207 = vadd.s32 1, %v66203
%v66211 = vsel /*vm=*/%vm66185, /*on_true_vy=*/%v66207, /*on_false_vx=*/%v66203
%v66216 = vadd.s32 %v66211, %v10
%v66220 = vadd.s32 %v66181, %v9
%v66224 = vadd.s32 %v66220, %v66216
%v66226 = vshll.u32 %v66220, 13
%v66227 = vshrl.u32 %v66220, 19
%v66228 = vor.u32 %v66227, %v66226
%v66229 = vxor.u32 %v66228, %v66224
%v66232 = vadd.s32 %v66229, %v66224
%v66234 = vshll.u32 %v66229, 15
%v66235 = vshrl.u32 %v66229, 17
%v66236 = vor.u32 %v66235, %v66234
%v66237 = vxor.u32 %v66236, %v66232
%v66240 = vadd.s32 %v66237, %v66232
%v66242 = vshll.u32 %v66237, 26
%v66243 = vshrl.u32 %v66237, 6
%v66244 = vor.u32 %v66243, %v66242
%v66245 = vxor.u32 %v66244, %v66240
%v66248 = vadd.s32 %v66245, %v66240
%v66252 = vadd.s32 %v66248, %v9
%v66254 = vshll.u32 %v66245, 6
%v66255 = vshrl.u32 %v66245, 26
%v66256 = vor.u32 %v66255, %v66254
%v66257 = vxor.u32 %v66256, %v66248
%v66260 = vadd.s32 %v66257, %v8
%v66264 = vadd.s32 1, %v66260
%v66268 = vadd.s32 %v66264, %v66252
%v66270 = vshll.u32 %v66264, 17
%v66271 = vshrl.u32 %v66264, 15
%v66272 = vor.u32 %v66271, %v66270
%v66273 = vxor.u32 %v66272, %v66268
%v66276 = vadd.s32 %v66273, %v66268
%v66278 = vshll.u32 %v66273, 29
%v66279 = vshrl.u32 %v66273, 3
%v66280 = vor.u32 %v66279, %v66278
%v66281 = vxor.u32 %v66280, %v66276
%v66284 = vadd.s32 %v66281, %v66276
%v66286 = vshll.u32 %v66281, 16
%v66287 = vshrl.u32 %v66281, 16
%v66288 = vor.u32 %v66287, %v66286
%v66289 = vxor.u32 %v66288, %v66284
%v66292 = vadd.s32 %v66289, %v66284
%v66296 = vadd.s32 %v66292, %v8
%v66298 = vshll.u32 %v66289, 24
%v66299 = vshrl.u32 %v66289, 8
%v66300 = vor.u32 %v66299, %v66298
%v66301 = vxor.u32 %v66300, %v66292
%v66304 = vadd.s32 %v66301, %v10
%v66308 = vadd.s32 2, %v66304
%v66312 = vadd.s32 %v66308, %v66296
%v66314 = vshll.u32 %v66308, 13
%v66315 = vshrl.u32 %v66308, 19
%v66316 = vor.u32 %v66315, %v66314
%v66317 = vxor.u32 %v66316, %v66312
%v66320 = vadd.s32 %v66317, %v66312
%v66322 = vshll.u32 %v66317, 15
%v66323 = vshrl.u32 %v66317, 17
%v66324 = vor.u32 %v66323, %v66322
%v66325 = vxor.u32 %v66324, %v66320
%v66328 = vadd.s32 %v66325, %v66320
%v66330 = vshll.u32 %v66325, 26
%v66331 = vshrl.u32 %v66325, 6
%v66332 = vor.u32 %v66331, %v66330
%v66333 = vxor.u32 %v66332, %v66328
%v66336 = vadd.s32 %v66333, %v66328
%v66340 = vadd.s32 %v66336, %v10
%v66342 = vshll.u32 %v66333, 6
%v66343 = vshrl.u32 %v66333, 26
%v66344 = vor.u32 %v66343, %v66342
%v66345 = vxor.u32 %v66344, %v66336
%v66348 = vadd.s32 %v66345, %v9
%v66352 = vadd.s32 3, %v66348
%v66356 = vadd.s32 %v66352, %v66340
%v66358 = vshll.u32 %v66352, 17
%v66359 = vshrl.u32 %v66352, 15
%v66360 = vor.u32 %v66359, %v66358
%v66361 = vxor.u32 %v66360, %v66356
%v66364 = vadd.s32 %v66361, %v66356
%v66366 = vshll.u32 %v66361, 29
%v66367 = vshrl.u32 %v66361, 3
%v66368 = vor.u32 %v66367, %v66366
%v66369 = vxor.u32 %v66368, %v66364
%v66372 = vadd.s32 %v66369, %v66364
%v66374 = vshll.u32 %v66369, 16
%v66375 = vshrl.u32 %v66369, 16
%v66376 = vor.u32 %v66375, %v66374
%v66377 = vxor.u32 %v66376, %v66372
%v66380 = vadd.s32 %v66377, %v66372
%v66384 = vadd.s32 %v66380, %v9
%v66386 = vshll.u32 %v66377, 24
%v66387 = vshrl.u32 %v66377, 8
%v66388 = vor.u32 %v66387, %v66386
%v66389 = vxor.u32 %v66388, %v66380
%v66392 = vadd.s32 %v66389, %v8
%v66396 = vadd.s32 4, %v66392
%v66400 = vadd.s32 %v66396, %v66384
%v66402 = vshll.u32 %v66396, 13
%v66403 = vshrl.u32 %v66396, 19
%v66404 = vor.u32 %v66403, %v66402
%v66405 = vxor.u32 %v66404, %v66400
%v66408 = vadd.s32 %v66405, %v66400
%v66410 = vshll.u32 %v66405, 15
%v66411 = vshrl.u32 %v66405, 17
%v66412 = vor.u32 %v66411, %v66410
%v66413 = vxor.u32 %v66412, %v66408
%v66416 = vadd.s32 %v66413, %v66408
%v66418 = vshll.u32 %v66413, 26
%v66419 = vshrl.u32 %v66413, 6
%v66420 = vor.u32 %v66419, %v66418
%v66421 = vxor.u32 %v66420, %v66416
%v66424 = vadd.s32 %v66421, %v66416
%v66428 = vadd.s32 %v66424, %v8
%v66430 = vshll.u32 %v66421, 6
%v66431 = vshrl.u32 %v66421, 26
%v66432 = vor.u32 %v66431, %v66430
%v66433 = vxor.u32 %v66432, %v66424
%v66436 = vadd.s32 %v66433, %v10
%v66440 = vadd.s32 5, %v66436
%v66442 = vxor.u32 %v66440, %v66428
%v66443 = vand.u32.u8 255, %v66442
%v66444 = vand.u32 65535, %v66443
%v66445 = vshrl.u32 %v66444, 1
%v66446 = vor.u32 16256, %v66445
%v66447 = vand.u32.u16 65535, %v66446
%v120098 = vadd.low.f32.bf16 -1.0, %v66447
%v66456 = vmul.f32 2.0, %v120098
%v66460 = vadd.f32 -0.99609375, %v66456
%v66464 = vmax.f32 %v66460, -0.99609375
%v66466 = vand.u32 2147483647, %v66464
%vm66469 = vcmp.eq.f32.partialorder %v66466, 1.0
%v66474 = vmul.f32 inf, %v66464
%v66476 = vxor.u32 2147483648, %v66464
%v66479 = vmul.f32 %v66476, %v66464
%v66481 = vadd.f32 1.0, %v66479
%v66482 = vlog2.pop %v66481
%v66483 = vmul.f32 0.6931472, %v66482
%v66484 = vmul.f32 -0.5, %v66479
%v66485 = vadd.f32 1.0, %v66484
%v66486 = vmul.f32 %v66485, %v66479
%v66487 = vand.u32 2147483647, %v66479
%vm66488 = vcmp.lt.f32.partialorder %v66487, 0.0004427343
%v66489 = vsel /*vm=*/%vm66488, /*on_true_vy=*/%v66486, /*on_false_vx=*/%v66483
%v66490 = vxor.u32 2147483648, %v66489
%vm66493 = vcmp.lt.f32.partialorder %v66490, 5.0
%v66498 = vsel /*vm=*/%vm66493, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v66502 = vsel /*vm=*/%vm66493, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v66506 = vsel /*vm=*/%vm66493, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v66510 = vsel /*vm=*/%vm66493, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v66514 = vsel /*vm=*/%vm66493, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v66518 = vsel /*vm=*/%vm66493, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v66522 = vsel /*vm=*/%vm66493, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v66526 = vsel /*vm=*/%vm66493, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v66530 = vsel /*vm=*/%vm66493, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v66534 = vadd.f32 -2.5, %v66490
%v66536 = vrsqrt.pop %v66490
%v66537 = vmul.f32 %v66536, %v66490
%vm66538 = vcmp.eq.f32.partialorder %v66490, inf
%v66539 = vsel /*vm=*/%vm66538, /*on_true_vy=*/%v66490, /*on_false_vx=*/%v66537
%vm66540 = vcmp.eq.f32.partialorder %v66490, 0.0
%v66541 = vand.u32 2147483648, %v66490
%v66542 = vsel /*vm=*/%vm66540, /*on_true_vy=*/%v66541, /*on_false_vx=*/%v66539
%v66545 = vadd.f32 -3.0, %v66542
%v66549 = vsel /*vm=*/%vm66493, /*on_true_vy=*/%v66534, /*on_false_vx=*/%v66545
%v66553 = vmul.f32 %v66549, %v66530
%v66557 = vadd.f32 %v66553, %v66526
%v66561 = vmul.f32 %v66557, %v66549
%v66565 = vadd.f32 %v66561, %v66522
%v66569 = vmul.f32 %v66565, %v66549
%v66573 = vadd.f32 %v66569, %v66518
%v66577 = vmul.f32 %v66573, %v66549
%v66581 = vadd.f32 %v66577, %v66514
%v66585 = vmul.f32 %v66581, %v66549
%v66589 = vadd.f32 %v66585, %v66510
%v66593 = vmul.f32 %v66589, %v66549
%v66597 = vadd.f32 %v66593, %v66506
%v66601 = vmul.f32 %v66597, %v66549
%v66605 = vadd.f32 %v66601, %v66502
%v66609 = vmul.f32 %v66605, %v66549
%v66613 = vadd.f32 %v66609, %v66498
%v66617 = vmul.f32 %v66613, %v66464
%v66621 = vsel /*vm=*/%vm66469, /*on_true_vy=*/%v66474, /*on_false_vx=*/%v66617
%v66625 = vmul.f32 1.4140625, %v66621
%v66628 = vpack.c.bf16 %v120417, %v66625
%120099 = vst [vmem:[%s280 + $0x2c4] sm:$0xf] /*vst_source=*/%v66628
%v66632 = vadd.s32 %v63863, %v3329
%v66642 = vadd.s32 %v66632, %v415
%vm66646 = vcmp.lt.u32.totalorder %v66642, %v66632
%vm66651 = vcmp.lt.u32.totalorder %v66632, %v3329
%v66656 = vadd.s32 %v63846, %v3316
%v66660 = vadd.s32 1, %v66656
%v66664 = vsel /*vm=*/%vm66651, /*on_true_vy=*/%v66660, /*on_false_vx=*/%v66656
%v66668 = vadd.s32 1, %v66664
%v66672 = vsel /*vm=*/%vm66646, /*on_true_vy=*/%v66668, /*on_false_vx=*/%v66664
%v66677 = vadd.s32 %v66672, %v10
%v66681 = vadd.s32 %v66642, %v9
%v66685 = vadd.s32 %v66681, %v66677
%v66687 = vshll.u32 %v66681, 13
%v66688 = vshrl.u32 %v66681, 19
%v66689 = vor.u32 %v66688, %v66687
%v66690 = vxor.u32 %v66689, %v66685
%v66693 = vadd.s32 %v66690, %v66685
%v66695 = vshll.u32 %v66690, 15
%v66696 = vshrl.u32 %v66690, 17
%v66697 = vor.u32 %v66696, %v66695
%v66698 = vxor.u32 %v66697, %v66693
%v66701 = vadd.s32 %v66698, %v66693
%v66703 = vshll.u32 %v66698, 26
%v66704 = vshrl.u32 %v66698, 6
%v66705 = vor.u32 %v66704, %v66703
%v66706 = vxor.u32 %v66705, %v66701
%v66709 = vadd.s32 %v66706, %v66701
%v66713 = vadd.s32 %v66709, %v9
%v66715 = vshll.u32 %v66706, 6
%v66716 = vshrl.u32 %v66706, 26
%v66717 = vor.u32 %v66716, %v66715
%v66718 = vxor.u32 %v66717, %v66709
%v66721 = vadd.s32 %v66718, %v8
%v66725 = vadd.s32 1, %v66721
%v66729 = vadd.s32 %v66725, %v66713
%v66731 = vshll.u32 %v66725, 17
%v66732 = vshrl.u32 %v66725, 15
%v66733 = vor.u32 %v66732, %v66731
%v66734 = vxor.u32 %v66733, %v66729
%v66737 = vadd.s32 %v66734, %v66729
%v66739 = vshll.u32 %v66734, 29
%v66740 = vshrl.u32 %v66734, 3
%v66741 = vor.u32 %v66740, %v66739
%v66742 = vxor.u32 %v66741, %v66737
%v66745 = vadd.s32 %v66742, %v66737
%v66747 = vshll.u32 %v66742, 16
%v66748 = vshrl.u32 %v66742, 16
%v66749 = vor.u32 %v66748, %v66747
%v66750 = vxor.u32 %v66749, %v66745
%v66753 = vadd.s32 %v66750, %v66745
%v66757 = vadd.s32 %v66753, %v8
%v66759 = vshll.u32 %v66750, 24
%v66760 = vshrl.u32 %v66750, 8
%v66761 = vor.u32 %v66760, %v66759
%v66762 = vxor.u32 %v66761, %v66753
%v66765 = vadd.s32 %v66762, %v10
%v66769 = vadd.s32 2, %v66765
%v66773 = vadd.s32 %v66769, %v66757
%v66775 = vshll.u32 %v66769, 13
%v66776 = vshrl.u32 %v66769, 19
%v66777 = vor.u32 %v66776, %v66775
%v66778 = vxor.u32 %v66777, %v66773
%v66781 = vadd.s32 %v66778, %v66773
%v66783 = vshll.u32 %v66778, 15
%v66784 = vshrl.u32 %v66778, 17
%v66785 = vor.u32 %v66784, %v66783
%v66786 = vxor.u32 %v66785, %v66781
%v66789 = vadd.s32 %v66786, %v66781
%v66791 = vshll.u32 %v66786, 26
%v66792 = vshrl.u32 %v66786, 6
%v66793 = vor.u32 %v66792, %v66791
%v66794 = vxor.u32 %v66793, %v66789
%v66797 = vadd.s32 %v66794, %v66789
%v66801 = vadd.s32 %v66797, %v10
%v66803 = vshll.u32 %v66794, 6
%v66804 = vshrl.u32 %v66794, 26
%v66805 = vor.u32 %v66804, %v66803
%v66806 = vxor.u32 %v66805, %v66797
%v66809 = vadd.s32 %v66806, %v9
%v66813 = vadd.s32 3, %v66809
%v66817 = vadd.s32 %v66813, %v66801
%v66819 = vshll.u32 %v66813, 17
%v66820 = vshrl.u32 %v66813, 15
%v66821 = vor.u32 %v66820, %v66819
%v66822 = vxor.u32 %v66821, %v66817
%v66825 = vadd.s32 %v66822, %v66817
%v66827 = vshll.u32 %v66822, 29
%v66828 = vshrl.u32 %v66822, 3
%v66829 = vor.u32 %v66828, %v66827
%v66830 = vxor.u32 %v66829, %v66825
%v66833 = vadd.s32 %v66830, %v66825
%v66835 = vshll.u32 %v66830, 16
%v66836 = vshrl.u32 %v66830, 16
%v66837 = vor.u32 %v66836, %v66835
%v66838 = vxor.u32 %v66837, %v66833
%v66841 = vadd.s32 %v66838, %v66833
%v66845 = vadd.s32 %v66841, %v9
%v66847 = vshll.u32 %v66838, 24
%v66848 = vshrl.u32 %v66838, 8
%v66849 = vor.u32 %v66848, %v66847
%v66850 = vxor.u32 %v66849, %v66841
%v66853 = vadd.s32 %v66850, %v8
%v66857 = vadd.s32 4, %v66853
%v66861 = vadd.s32 %v66857, %v66845
%v66863 = vshll.u32 %v66857, 13
%v66864 = vshrl.u32 %v66857, 19
%v66865 = vor.u32 %v66864, %v66863
%v66866 = vxor.u32 %v66865, %v66861
%v66869 = vadd.s32 %v66866, %v66861
%v66871 = vshll.u32 %v66866, 15
%v66872 = vshrl.u32 %v66866, 17
%v66873 = vor.u32 %v66872, %v66871
%v66874 = vxor.u32 %v66873, %v66869
%v66877 = vadd.s32 %v66874, %v66869
%v66879 = vshll.u32 %v66874, 26
%v66880 = vshrl.u32 %v66874, 6
%v66881 = vor.u32 %v66880, %v66879
%v66882 = vxor.u32 %v66881, %v66877
%v66885 = vadd.s32 %v66882, %v66877
%v66889 = vadd.s32 %v66885, %v8
%v66891 = vshll.u32 %v66882, 6
%v66892 = vshrl.u32 %v66882, 26
%v66893 = vor.u32 %v66892, %v66891
%v66894 = vxor.u32 %v66893, %v66885
%v66897 = vadd.s32 %v66894, %v10
%v66901 = vadd.s32 5, %v66897
%v66903 = vxor.u32 %v66901, %v66889
%v66904 = vand.u32.u8 255, %v66903
%v66905 = vand.u32 65535, %v66904
%v66906 = vshrl.u32 %v66905, 1
%v66907 = vor.u32 16256, %v66906
%v66908 = vand.u32.u16 65535, %v66907
%v120100 = vadd.low.f32.bf16 -1.0, %v66908
%v66917 = vmul.f32 2.0, %v120100
%v66921 = vadd.f32 -0.99609375, %v66917
%v66925 = vmax.f32 %v66921, -0.99609375
%v66927 = vand.u32 2147483647, %v66925
%vm66930 = vcmp.eq.f32.partialorder %v66927, 1.0
%v66935 = vmul.f32 inf, %v66925
%v66937 = vxor.u32 2147483648, %v66925
%v66940 = vmul.f32 %v66937, %v66925
%v66942 = vadd.f32 1.0, %v66940
%v66943 = vlog2.pop %v66942
%v66944 = vmul.f32 0.6931472, %v66943
%v66945 = vmul.f32 -0.5, %v66940
%v66946 = vadd.f32 1.0, %v66945
%v66947 = vmul.f32 %v66946, %v66940
%v66948 = vand.u32 2147483647, %v66940
%vm66949 = vcmp.lt.f32.partialorder %v66948, 0.0004427343
%v66950 = vsel /*vm=*/%vm66949, /*on_true_vy=*/%v66947, /*on_false_vx=*/%v66944
%v66951 = vxor.u32 2147483648, %v66950
%vm66954 = vcmp.lt.f32.partialorder %v66951, 5.0
%v66959 = vsel /*vm=*/%vm66954, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v66963 = vsel /*vm=*/%vm66954, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v66967 = vsel /*vm=*/%vm66954, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v66971 = vsel /*vm=*/%vm66954, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v66975 = vsel /*vm=*/%vm66954, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v66979 = vsel /*vm=*/%vm66954, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v66983 = vsel /*vm=*/%vm66954, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v66987 = vsel /*vm=*/%vm66954, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v66991 = vsel /*vm=*/%vm66954, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v66995 = vadd.f32 -2.5, %v66951
%v66997 = vrsqrt.pop %v66951
%v66998 = vmul.f32 %v66997, %v66951
%vm66999 = vcmp.eq.f32.partialorder %v66951, inf
%v67000 = vsel /*vm=*/%vm66999, /*on_true_vy=*/%v66951, /*on_false_vx=*/%v66998
%vm67001 = vcmp.eq.f32.partialorder %v66951, 0.0
%v67002 = vand.u32 2147483648, %v66951
%v67003 = vsel /*vm=*/%vm67001, /*on_true_vy=*/%v67002, /*on_false_vx=*/%v67000
%v67006 = vadd.f32 -3.0, %v67003
%v67010 = vsel /*vm=*/%vm66954, /*on_true_vy=*/%v66995, /*on_false_vx=*/%v67006
%v67014 = vmul.f32 %v67010, %v66991
%v67018 = vadd.f32 %v67014, %v66987
%v67022 = vmul.f32 %v67018, %v67010
%v67026 = vadd.f32 %v67022, %v66983
%v67030 = vmul.f32 %v67026, %v67010
%v67034 = vadd.f32 %v67030, %v66979
%v67038 = vmul.f32 %v67034, %v67010
%v67042 = vadd.f32 %v67038, %v66975
%v67046 = vmul.f32 %v67042, %v67010
%v67050 = vadd.f32 %v67046, %v66971
%v67054 = vmul.f32 %v67050, %v67010
%v67058 = vadd.f32 %v67054, %v66967
%v67062 = vmul.f32 %v67058, %v67010
%v67066 = vadd.f32 %v67062, %v66963
%v67070 = vmul.f32 %v67066, %v67010
%v67074 = vadd.f32 %v67070, %v66959
%v67078 = vmul.f32 %v67074, %v66925
%v67082 = vsel /*vm=*/%vm66930, /*on_true_vy=*/%v66935, /*on_false_vx=*/%v67078
%v67086 = vmul.f32 1.4140625, %v67082
%v67089 = vpack.c.bf16 %v120417, %v67086
%120101 = vst [vmem:[%s280 + $0x344] sm:$0xf] /*vst_source=*/%v67089
%v67093 = vadd.s32 %v63863, %v3816
%v67103 = vadd.s32 %v67093, %v415
%vm67107 = vcmp.lt.u32.totalorder %v67103, %v67093
%vm67112 = vcmp.lt.u32.totalorder %v67093, %v3816
%v67117 = vadd.s32 %v63846, %v3803
%v67121 = vadd.s32 1, %v67117
%v67125 = vsel /*vm=*/%vm67112, /*on_true_vy=*/%v67121, /*on_false_vx=*/%v67117
%v67129 = vadd.s32 1, %v67125
%v67133 = vsel /*vm=*/%vm67107, /*on_true_vy=*/%v67129, /*on_false_vx=*/%v67125
%v67138 = vadd.s32 %v67133, %v10
%v67142 = vadd.s32 %v67103, %v9
%v67146 = vadd.s32 %v67142, %v67138
%v67148 = vshll.u32 %v67142, 13
%v67149 = vshrl.u32 %v67142, 19
%v67150 = vor.u32 %v67149, %v67148
%v67151 = vxor.u32 %v67150, %v67146
%v67154 = vadd.s32 %v67151, %v67146
%v67156 = vshll.u32 %v67151, 15
%v67157 = vshrl.u32 %v67151, 17
%v67158 = vor.u32 %v67157, %v67156
%v67159 = vxor.u32 %v67158, %v67154
%v67162 = vadd.s32 %v67159, %v67154
%v67164 = vshll.u32 %v67159, 26
%v67165 = vshrl.u32 %v67159, 6
%v67166 = vor.u32 %v67165, %v67164
%v67167 = vxor.u32 %v67166, %v67162
%v67170 = vadd.s32 %v67167, %v67162
%v67174 = vadd.s32 %v67170, %v9
%v67176 = vshll.u32 %v67167, 6
%v67177 = vshrl.u32 %v67167, 26
%v67178 = vor.u32 %v67177, %v67176
%v67179 = vxor.u32 %v67178, %v67170
%v67182 = vadd.s32 %v67179, %v8
%v67186 = vadd.s32 1, %v67182
%v67190 = vadd.s32 %v67186, %v67174
%v67192 = vshll.u32 %v67186, 17
%v67193 = vshrl.u32 %v67186, 15
%v67194 = vor.u32 %v67193, %v67192
%v67195 = vxor.u32 %v67194, %v67190
%v67198 = vadd.s32 %v67195, %v67190
%v67200 = vshll.u32 %v67195, 29
%v67201 = vshrl.u32 %v67195, 3
%v67202 = vor.u32 %v67201, %v67200
%v67203 = vxor.u32 %v67202, %v67198
%v67206 = vadd.s32 %v67203, %v67198
%v67208 = vshll.u32 %v67203, 16
%v67209 = vshrl.u32 %v67203, 16
%v67210 = vor.u32 %v67209, %v67208
%v67211 = vxor.u32 %v67210, %v67206
%v67214 = vadd.s32 %v67211, %v67206
%v67218 = vadd.s32 %v67214, %v8
%v67220 = vshll.u32 %v67211, 24
%v67221 = vshrl.u32 %v67211, 8
%v67222 = vor.u32 %v67221, %v67220
%v67223 = vxor.u32 %v67222, %v67214
%v67226 = vadd.s32 %v67223, %v10
%v67230 = vadd.s32 2, %v67226
%v67234 = vadd.s32 %v67230, %v67218
%v67236 = vshll.u32 %v67230, 13
%v67237 = vshrl.u32 %v67230, 19
%v67238 = vor.u32 %v67237, %v67236
%v67239 = vxor.u32 %v67238, %v67234
%v67242 = vadd.s32 %v67239, %v67234
%v67244 = vshll.u32 %v67239, 15
%v67245 = vshrl.u32 %v67239, 17
%v67246 = vor.u32 %v67245, %v67244
%v67247 = vxor.u32 %v67246, %v67242
%v67250 = vadd.s32 %v67247, %v67242
%v67252 = vshll.u32 %v67247, 26
%v67253 = vshrl.u32 %v67247, 6
%v67254 = vor.u32 %v67253, %v67252
%v67255 = vxor.u32 %v67254, %v67250
%v67258 = vadd.s32 %v67255, %v67250
%v67262 = vadd.s32 %v67258, %v10
%v67264 = vshll.u32 %v67255, 6
%v67265 = vshrl.u32 %v67255, 26
%v67266 = vor.u32 %v67265, %v67264
%v67267 = vxor.u32 %v67266, %v67258
%v67270 = vadd.s32 %v67267, %v9
%v67274 = vadd.s32 3, %v67270
%v67278 = vadd.s32 %v67274, %v67262
%v67280 = vshll.u32 %v67274, 17
%v67281 = vshrl.u32 %v67274, 15
%v67282 = vor.u32 %v67281, %v67280
%v67283 = vxor.u32 %v67282, %v67278
%v67286 = vadd.s32 %v67283, %v67278
%v67288 = vshll.u32 %v67283, 29
%v67289 = vshrl.u32 %v67283, 3
%v67290 = vor.u32 %v67289, %v67288
%v67291 = vxor.u32 %v67290, %v67286
%v67294 = vadd.s32 %v67291, %v67286
%v67296 = vshll.u32 %v67291, 16
%v67297 = vshrl.u32 %v67291, 16
%v67298 = vor.u32 %v67297, %v67296
%v67299 = vxor.u32 %v67298, %v67294
%v67302 = vadd.s32 %v67299, %v67294
%v67306 = vadd.s32 %v67302, %v9
%v67308 = vshll.u32 %v67299, 24
%v67309 = vshrl.u32 %v67299, 8
%v67310 = vor.u32 %v67309, %v67308
%v67311 = vxor.u32 %v67310, %v67302
%v67314 = vadd.s32 %v67311, %v8
%v67318 = vadd.s32 4, %v67314
%v67322 = vadd.s32 %v67318, %v67306
%v67324 = vshll.u32 %v67318, 13
%v67325 = vshrl.u32 %v67318, 19
%v67326 = vor.u32 %v67325, %v67324
%v67327 = vxor.u32 %v67326, %v67322
%v67330 = vadd.s32 %v67327, %v67322
%v67332 = vshll.u32 %v67327, 15
%v67333 = vshrl.u32 %v67327, 17
%v67334 = vor.u32 %v67333, %v67332
%v67335 = vxor.u32 %v67334, %v67330
%v67338 = vadd.s32 %v67335, %v67330
%v67340 = vshll.u32 %v67335, 26
%v67341 = vshrl.u32 %v67335, 6
%v67342 = vor.u32 %v67341, %v67340
%v67343 = vxor.u32 %v67342, %v67338
%v67346 = vadd.s32 %v67343, %v67338
%v67350 = vadd.s32 %v67346, %v8
%v67352 = vshll.u32 %v67343, 6
%v67353 = vshrl.u32 %v67343, 26
%v67354 = vor.u32 %v67353, %v67352
%v67355 = vxor.u32 %v67354, %v67346
%v67358 = vadd.s32 %v67355, %v10
%v67362 = vadd.s32 5, %v67358
%v67364 = vxor.u32 %v67362, %v67350
%v67365 = vand.u32.u8 255, %v67364
%v67366 = vand.u32 65535, %v67365
%v67367 = vshrl.u32 %v67366, 1
%v67368 = vor.u32 16256, %v67367
%v67369 = vand.u32.u16 65535, %v67368
%v120102 = vadd.low.f32.bf16 -1.0, %v67369
%v67378 = vmul.f32 2.0, %v120102
%v67382 = vadd.f32 -0.99609375, %v67378
%v67386 = vmax.f32 %v67382, -0.99609375
%v67388 = vand.u32 2147483647, %v67386
%vm67391 = vcmp.eq.f32.partialorder %v67388, 1.0
%v67396 = vmul.f32 inf, %v67386
%v67398 = vxor.u32 2147483648, %v67386
%v67401 = vmul.f32 %v67398, %v67386
%v67403 = vadd.f32 1.0, %v67401
%v67404 = vlog2.pop %v67403
%v67405 = vmul.f32 0.6931472, %v67404
%v67406 = vmul.f32 -0.5, %v67401
%v67407 = vadd.f32 1.0, %v67406
%v67408 = vmul.f32 %v67407, %v67401
%v67409 = vand.u32 2147483647, %v67401
%vm67410 = vcmp.lt.f32.partialorder %v67409, 0.0004427343
%v67411 = vsel /*vm=*/%vm67410, /*on_true_vy=*/%v67408, /*on_false_vx=*/%v67405
%v67412 = vxor.u32 2147483648, %v67411
%vm67415 = vcmp.lt.f32.partialorder %v67412, 5.0
%v67420 = vsel /*vm=*/%vm67415, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v67424 = vsel /*vm=*/%vm67415, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v67428 = vsel /*vm=*/%vm67415, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v67432 = vsel /*vm=*/%vm67415, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v67436 = vsel /*vm=*/%vm67415, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v67440 = vsel /*vm=*/%vm67415, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v67444 = vsel /*vm=*/%vm67415, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v67448 = vsel /*vm=*/%vm67415, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v67452 = vsel /*vm=*/%vm67415, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v67456 = vadd.f32 -2.5, %v67412
%v67458 = vrsqrt.pop %v67412
%v67459 = vmul.f32 %v67458, %v67412
%vm67460 = vcmp.eq.f32.partialorder %v67412, inf
%v67461 = vsel /*vm=*/%vm67460, /*on_true_vy=*/%v67412, /*on_false_vx=*/%v67459
%vm67462 = vcmp.eq.f32.partialorder %v67412, 0.0
%v67463 = vand.u32 2147483648, %v67412
%v67464 = vsel /*vm=*/%vm67462, /*on_true_vy=*/%v67463, /*on_false_vx=*/%v67461
%v67467 = vadd.f32 -3.0, %v67464
%v67471 = vsel /*vm=*/%vm67415, /*on_true_vy=*/%v67456, /*on_false_vx=*/%v67467
%v67475 = vmul.f32 %v67471, %v67452
%v67479 = vadd.f32 %v67475, %v67448
%v67483 = vmul.f32 %v67479, %v67471
%v67487 = vadd.f32 %v67483, %v67444
%v67491 = vmul.f32 %v67487, %v67471
%v67495 = vadd.f32 %v67491, %v67440
%v67499 = vmul.f32 %v67495, %v67471
%v67503 = vadd.f32 %v67499, %v67436
%v67507 = vmul.f32 %v67503, %v67471
%v67511 = vadd.f32 %v67507, %v67432
%v67515 = vmul.f32 %v67511, %v67471
%v67519 = vadd.f32 %v67515, %v67428
%v67523 = vmul.f32 %v67519, %v67471
%v67527 = vadd.f32 %v67523, %v67424
%v67531 = vmul.f32 %v67527, %v67471
%v67535 = vadd.f32 %v67531, %v67420
%v67539 = vmul.f32 %v67535, %v67386
%v67543 = vsel /*vm=*/%vm67391, /*on_true_vy=*/%v67396, /*on_false_vx=*/%v67539
%v67547 = vmul.f32 1.4140625, %v67543
%v67550 = vpack.c.bf16 %v120417, %v67547
%120103 = vst [vmem:[%s280 + $0x3c4] sm:$0xf] /*vst_source=*/%v67550
%v67588 = vadd.s32 %v67585, %v408
%v67598 = vadd.s32 %v67588, %v415
%vm67602 = vcmp.lt.u32.totalorder %v67598, %v67588
%vm67607 = vcmp.lt.u32.totalorder %v67588, %v408
%v67612 = vadd.s32 %v67568, %v380
%v67616 = vadd.s32 1, %v67612
%v67620 = vsel /*vm=*/%vm67607, /*on_true_vy=*/%v67616, /*on_false_vx=*/%v67612
%v67624 = vadd.s32 1, %v67620
%v67628 = vsel /*vm=*/%vm67602, /*on_true_vy=*/%v67624, /*on_false_vx=*/%v67620
%v67633 = vadd.s32 %v67628, %v10
%v67637 = vadd.s32 %v67598, %v9
%v67641 = vadd.s32 %v67637, %v67633
%v67643 = vshll.u32 %v67637, 13
%v67644 = vshrl.u32 %v67637, 19
%v67645 = vor.u32 %v67644, %v67643
%v67646 = vxor.u32 %v67645, %v67641
%v67649 = vadd.s32 %v67646, %v67641
%v67651 = vshll.u32 %v67646, 15
%v67652 = vshrl.u32 %v67646, 17
%v67653 = vor.u32 %v67652, %v67651
%v67654 = vxor.u32 %v67653, %v67649
%v67657 = vadd.s32 %v67654, %v67649
%v67659 = vshll.u32 %v67654, 26
%v67660 = vshrl.u32 %v67654, 6
%v67661 = vor.u32 %v67660, %v67659
%v67662 = vxor.u32 %v67661, %v67657
%v67665 = vadd.s32 %v67662, %v67657
%v67669 = vadd.s32 %v67665, %v9
%v67671 = vshll.u32 %v67662, 6
%v67672 = vshrl.u32 %v67662, 26
%v67673 = vor.u32 %v67672, %v67671
%v67674 = vxor.u32 %v67673, %v67665
%v67677 = vadd.s32 %v67674, %v8
%v67681 = vadd.s32 1, %v67677
%v67685 = vadd.s32 %v67681, %v67669
%v67687 = vshll.u32 %v67681, 17
%v67688 = vshrl.u32 %v67681, 15
%v67689 = vor.u32 %v67688, %v67687
%v67690 = vxor.u32 %v67689, %v67685
%v67693 = vadd.s32 %v67690, %v67685
%v67695 = vshll.u32 %v67690, 29
%v67696 = vshrl.u32 %v67690, 3
%v67697 = vor.u32 %v67696, %v67695
%v67698 = vxor.u32 %v67697, %v67693
%v67701 = vadd.s32 %v67698, %v67693
%v67703 = vshll.u32 %v67698, 16
%v67704 = vshrl.u32 %v67698, 16
%v67705 = vor.u32 %v67704, %v67703
%v67706 = vxor.u32 %v67705, %v67701
%v67709 = vadd.s32 %v67706, %v67701
%v67713 = vadd.s32 %v67709, %v8
%v67715 = vshll.u32 %v67706, 24
%v67716 = vshrl.u32 %v67706, 8
%v67717 = vor.u32 %v67716, %v67715
%v67718 = vxor.u32 %v67717, %v67709
%v67721 = vadd.s32 %v67718, %v10
%v67725 = vadd.s32 2, %v67721
%v67729 = vadd.s32 %v67725, %v67713
%v67731 = vshll.u32 %v67725, 13
%v67732 = vshrl.u32 %v67725, 19
%v67733 = vor.u32 %v67732, %v67731
%v67734 = vxor.u32 %v67733, %v67729
%v67737 = vadd.s32 %v67734, %v67729
%v67739 = vshll.u32 %v67734, 15
%v67740 = vshrl.u32 %v67734, 17
%v67741 = vor.u32 %v67740, %v67739
%v67742 = vxor.u32 %v67741, %v67737
%v67745 = vadd.s32 %v67742, %v67737
%v67747 = vshll.u32 %v67742, 26
%v67748 = vshrl.u32 %v67742, 6
%v67749 = vor.u32 %v67748, %v67747
%v67750 = vxor.u32 %v67749, %v67745
%v67753 = vadd.s32 %v67750, %v67745
%v67757 = vadd.s32 %v67753, %v10
%v67759 = vshll.u32 %v67750, 6
%v67760 = vshrl.u32 %v67750, 26
%v67761 = vor.u32 %v67760, %v67759
%v67762 = vxor.u32 %v67761, %v67753
%v67765 = vadd.s32 %v67762, %v9
%v67769 = vadd.s32 3, %v67765
%v67773 = vadd.s32 %v67769, %v67757
%v67775 = vshll.u32 %v67769, 17
%v67776 = vshrl.u32 %v67769, 15
%v67777 = vor.u32 %v67776, %v67775
%v67778 = vxor.u32 %v67777, %v67773
%v67781 = vadd.s32 %v67778, %v67773
%v67783 = vshll.u32 %v67778, 29
%v67784 = vshrl.u32 %v67778, 3
%v67785 = vor.u32 %v67784, %v67783
%v67786 = vxor.u32 %v67785, %v67781
%v67789 = vadd.s32 %v67786, %v67781
%v67791 = vshll.u32 %v67786, 16
%v67792 = vshrl.u32 %v67786, 16
%v67793 = vor.u32 %v67792, %v67791
%v67794 = vxor.u32 %v67793, %v67789
%v67797 = vadd.s32 %v67794, %v67789
%v67801 = vadd.s32 %v67797, %v9
%v67803 = vshll.u32 %v67794, 24
%v67804 = vshrl.u32 %v67794, 8
%v67805 = vor.u32 %v67804, %v67803
%v67806 = vxor.u32 %v67805, %v67797
%v67809 = vadd.s32 %v67806, %v8
%v67813 = vadd.s32 4, %v67809
%v67817 = vadd.s32 %v67813, %v67801
%v67819 = vshll.u32 %v67813, 13
%v67820 = vshrl.u32 %v67813, 19
%v67821 = vor.u32 %v67820, %v67819
%v67822 = vxor.u32 %v67821, %v67817
%v67825 = vadd.s32 %v67822, %v67817
%v67827 = vshll.u32 %v67822, 15
%v67828 = vshrl.u32 %v67822, 17
%v67829 = vor.u32 %v67828, %v67827
%v67830 = vxor.u32 %v67829, %v67825
%v67833 = vadd.s32 %v67830, %v67825
%v67835 = vshll.u32 %v67830, 26
%v67836 = vshrl.u32 %v67830, 6
%v67837 = vor.u32 %v67836, %v67835
%v67838 = vxor.u32 %v67837, %v67833
%v67841 = vadd.s32 %v67838, %v67833
%v67845 = vadd.s32 %v67841, %v8
%v67847 = vshll.u32 %v67838, 6
%v67848 = vshrl.u32 %v67838, 26
%v67849 = vor.u32 %v67848, %v67847
%v67850 = vxor.u32 %v67849, %v67841
%v67853 = vadd.s32 %v67850, %v10
%v67857 = vadd.s32 5, %v67853
%v67859 = vxor.u32 %v67857, %v67845
%v67860 = vand.u32.u8 255, %v67859
%v67861 = vand.u32 65535, %v67860
%v67862 = vshrl.u32 %v67861, 1
%v67863 = vor.u32 16256, %v67862
%v67864 = vand.u32.u16 65535, %v67863
%v120108 = vadd.low.f32.bf16 -1.0, %v67864
%v67873 = vmul.f32 2.0, %v120108
%v67877 = vadd.f32 -0.99609375, %v67873
%v67881 = vmax.f32 %v67877, -0.99609375
%v67883 = vand.u32 2147483647, %v67881
%vm67886 = vcmp.eq.f32.partialorder %v67883, 1.0
%v67891 = vmul.f32 inf, %v67881
%v67893 = vxor.u32 2147483648, %v67881
%v67896 = vmul.f32 %v67893, %v67881
%v67898 = vadd.f32 1.0, %v67896
%v67899 = vlog2.pop %v67898
%v67900 = vmul.f32 0.6931472, %v67899
%v67901 = vmul.f32 -0.5, %v67896
%v67902 = vadd.f32 1.0, %v67901
%v67903 = vmul.f32 %v67902, %v67896
%v67904 = vand.u32 2147483647, %v67896
%vm67905 = vcmp.lt.f32.partialorder %v67904, 0.0004427343
%v67906 = vsel /*vm=*/%vm67905, /*on_true_vy=*/%v67903, /*on_false_vx=*/%v67900
%v67907 = vxor.u32 2147483648, %v67906
%vm67910 = vcmp.lt.f32.partialorder %v67907, 5.0
%v67915 = vsel /*vm=*/%vm67910, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v67919 = vsel /*vm=*/%vm67910, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v67923 = vsel /*vm=*/%vm67910, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v67927 = vsel /*vm=*/%vm67910, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v67931 = vsel /*vm=*/%vm67910, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v67935 = vsel /*vm=*/%vm67910, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v67939 = vsel /*vm=*/%vm67910, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v67943 = vsel /*vm=*/%vm67910, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v67947 = vsel /*vm=*/%vm67910, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v67951 = vadd.f32 -2.5, %v67907
%v67953 = vrsqrt.pop %v67907
%v67954 = vmul.f32 %v67953, %v67907
%vm67955 = vcmp.eq.f32.partialorder %v67907, inf
%v67956 = vsel /*vm=*/%vm67955, /*on_true_vy=*/%v67907, /*on_false_vx=*/%v67954
%vm67957 = vcmp.eq.f32.partialorder %v67907, 0.0
%v67958 = vand.u32 2147483648, %v67907
%v67959 = vsel /*vm=*/%vm67957, /*on_true_vy=*/%v67958, /*on_false_vx=*/%v67956
%v67962 = vadd.f32 -3.0, %v67959
%v67966 = vsel /*vm=*/%vm67910, /*on_true_vy=*/%v67951, /*on_false_vx=*/%v67962
%v67970 = vmul.f32 %v67966, %v67947
%v67974 = vadd.f32 %v67970, %v67943
%v67978 = vmul.f32 %v67974, %v67966
%v67982 = vadd.f32 %v67978, %v67939
%v67986 = vmul.f32 %v67982, %v67966
%v67990 = vadd.f32 %v67986, %v67935
%v67994 = vmul.f32 %v67990, %v67966
%v67998 = vadd.f32 %v67994, %v67931
%v68002 = vmul.f32 %v67998, %v67966
%v68006 = vadd.f32 %v68002, %v67927
%v68010 = vmul.f32 %v68006, %v67966
%v68014 = vadd.f32 %v68010, %v67923
%v68018 = vmul.f32 %v68014, %v67966
%v68022 = vadd.f32 %v68018, %v67919
%v68026 = vmul.f32 %v68022, %v67966
%v68030 = vadd.f32 %v68026, %v67915
%v68034 = vmul.f32 %v68030, %v67881
%v68038 = vsel /*vm=*/%vm67886, /*on_true_vy=*/%v67891, /*on_false_vx=*/%v68034
%v68042 = vmul.f32 1.4140625, %v68038
%v68045 = vpack.c.bf16 %v120417, %v68042
%120109 = vst [vmem:[%s280 + $0x48] sm:$0xf] /*vst_source=*/%v68045
%v68049 = vadd.s32 %v67585, %v894
%v68059 = vadd.s32 %v68049, %v415
%vm68063 = vcmp.lt.u32.totalorder %v68059, %v68049
%vm68068 = vcmp.lt.u32.totalorder %v68049, %v894
%v68073 = vadd.s32 %v67568, %v881
%v68077 = vadd.s32 1, %v68073
%v68081 = vsel /*vm=*/%vm68068, /*on_true_vy=*/%v68077, /*on_false_vx=*/%v68073
%v68085 = vadd.s32 1, %v68081
%v68089 = vsel /*vm=*/%vm68063, /*on_true_vy=*/%v68085, /*on_false_vx=*/%v68081
%v68094 = vadd.s32 %v68089, %v10
%v68098 = vadd.s32 %v68059, %v9
%v68102 = vadd.s32 %v68098, %v68094
%v68104 = vshll.u32 %v68098, 13
%v68105 = vshrl.u32 %v68098, 19
%v68106 = vor.u32 %v68105, %v68104
%v68107 = vxor.u32 %v68106, %v68102
%v68110 = vadd.s32 %v68107, %v68102
%v68112 = vshll.u32 %v68107, 15
%v68113 = vshrl.u32 %v68107, 17
%v68114 = vor.u32 %v68113, %v68112
%v68115 = vxor.u32 %v68114, %v68110
%v68118 = vadd.s32 %v68115, %v68110
%v68120 = vshll.u32 %v68115, 26
%v68121 = vshrl.u32 %v68115, 6
%v68122 = vor.u32 %v68121, %v68120
%v68123 = vxor.u32 %v68122, %v68118
%v68126 = vadd.s32 %v68123, %v68118
%v68130 = vadd.s32 %v68126, %v9
%v68132 = vshll.u32 %v68123, 6
%v68133 = vshrl.u32 %v68123, 26
%v68134 = vor.u32 %v68133, %v68132
%v68135 = vxor.u32 %v68134, %v68126
%v68138 = vadd.s32 %v68135, %v8
%v68142 = vadd.s32 1, %v68138
%v68146 = vadd.s32 %v68142, %v68130
%v68148 = vshll.u32 %v68142, 17
%v68149 = vshrl.u32 %v68142, 15
%v68150 = vor.u32 %v68149, %v68148
%v68151 = vxor.u32 %v68150, %v68146
%v68154 = vadd.s32 %v68151, %v68146
%v68156 = vshll.u32 %v68151, 29
%v68157 = vshrl.u32 %v68151, 3
%v68158 = vor.u32 %v68157, %v68156
%v68159 = vxor.u32 %v68158, %v68154
%v68162 = vadd.s32 %v68159, %v68154
%v68164 = vshll.u32 %v68159, 16
%v68165 = vshrl.u32 %v68159, 16
%v68166 = vor.u32 %v68165, %v68164
%v68167 = vxor.u32 %v68166, %v68162
%v68170 = vadd.s32 %v68167, %v68162
%v68174 = vadd.s32 %v68170, %v8
%v68176 = vshll.u32 %v68167, 24
%v68177 = vshrl.u32 %v68167, 8
%v68178 = vor.u32 %v68177, %v68176
%v68179 = vxor.u32 %v68178, %v68170
%v68182 = vadd.s32 %v68179, %v10
%v68186 = vadd.s32 2, %v68182
%v68190 = vadd.s32 %v68186, %v68174
%v68192 = vshll.u32 %v68186, 13
%v68193 = vshrl.u32 %v68186, 19
%v68194 = vor.u32 %v68193, %v68192
%v68195 = vxor.u32 %v68194, %v68190
%v68198 = vadd.s32 %v68195, %v68190
%v68200 = vshll.u32 %v68195, 15
%v68201 = vshrl.u32 %v68195, 17
%v68202 = vor.u32 %v68201, %v68200
%v68203 = vxor.u32 %v68202, %v68198
%v68206 = vadd.s32 %v68203, %v68198
%v68208 = vshll.u32 %v68203, 26
%v68209 = vshrl.u32 %v68203, 6
%v68210 = vor.u32 %v68209, %v68208
%v68211 = vxor.u32 %v68210, %v68206
%v68214 = vadd.s32 %v68211, %v68206
%v68218 = vadd.s32 %v68214, %v10
%v68220 = vshll.u32 %v68211, 6
%v68221 = vshrl.u32 %v68211, 26
%v68222 = vor.u32 %v68221, %v68220
%v68223 = vxor.u32 %v68222, %v68214
%v68226 = vadd.s32 %v68223, %v9
%v68230 = vadd.s32 3, %v68226
%v68234 = vadd.s32 %v68230, %v68218
%v68236 = vshll.u32 %v68230, 17
%v68237 = vshrl.u32 %v68230, 15
%v68238 = vor.u32 %v68237, %v68236
%v68239 = vxor.u32 %v68238, %v68234
%v68242 = vadd.s32 %v68239, %v68234
%v68244 = vshll.u32 %v68239, 29
%v68245 = vshrl.u32 %v68239, 3
%v68246 = vor.u32 %v68245, %v68244
%v68247 = vxor.u32 %v68246, %v68242
%v68250 = vadd.s32 %v68247, %v68242
%v68252 = vshll.u32 %v68247, 16
%v68253 = vshrl.u32 %v68247, 16
%v68254 = vor.u32 %v68253, %v68252
%v68255 = vxor.u32 %v68254, %v68250
%v68258 = vadd.s32 %v68255, %v68250
%v68262 = vadd.s32 %v68258, %v9
%v68264 = vshll.u32 %v68255, 24
%v68265 = vshrl.u32 %v68255, 8
%v68266 = vor.u32 %v68265, %v68264
%v68267 = vxor.u32 %v68266, %v68258
%v68270 = vadd.s32 %v68267, %v8
%v68274 = vadd.s32 4, %v68270
%v68278 = vadd.s32 %v68274, %v68262
%v68280 = vshll.u32 %v68274, 13
%v68281 = vshrl.u32 %v68274, 19
%v68282 = vor.u32 %v68281, %v68280
%v68283 = vxor.u32 %v68282, %v68278
%v68286 = vadd.s32 %v68283, %v68278
%v68288 = vshll.u32 %v68283, 15
%v68289 = vshrl.u32 %v68283, 17
%v68290 = vor.u32 %v68289, %v68288
%v68291 = vxor.u32 %v68290, %v68286
%v68294 = vadd.s32 %v68291, %v68286
%v68296 = vshll.u32 %v68291, 26
%v68297 = vshrl.u32 %v68291, 6
%v68298 = vor.u32 %v68297, %v68296
%v68299 = vxor.u32 %v68298, %v68294
%v68302 = vadd.s32 %v68299, %v68294
%v68306 = vadd.s32 %v68302, %v8
%v68308 = vshll.u32 %v68299, 6
%v68309 = vshrl.u32 %v68299, 26
%v68310 = vor.u32 %v68309, %v68308
%v68311 = vxor.u32 %v68310, %v68302
%v68314 = vadd.s32 %v68311, %v10
%v68318 = vadd.s32 5, %v68314
%v68320 = vxor.u32 %v68318, %v68306
%v68321 = vand.u32.u8 255, %v68320
%v68322 = vand.u32 65535, %v68321
%v68323 = vshrl.u32 %v68322, 1
%v68324 = vor.u32 16256, %v68323
%v68325 = vand.u32.u16 65535, %v68324
%v120110 = vadd.low.f32.bf16 -1.0, %v68325
%v68334 = vmul.f32 2.0, %v120110
%v68338 = vadd.f32 -0.99609375, %v68334
%v68342 = vmax.f32 %v68338, -0.99609375
%v68344 = vand.u32 2147483647, %v68342
%vm68347 = vcmp.eq.f32.partialorder %v68344, 1.0
%v68352 = vmul.f32 inf, %v68342
%v68354 = vxor.u32 2147483648, %v68342
%v68357 = vmul.f32 %v68354, %v68342
%v68359 = vadd.f32 1.0, %v68357
%v68360 = vlog2.pop %v68359
%v68361 = vmul.f32 0.6931472, %v68360
%v68362 = vmul.f32 -0.5, %v68357
%v68363 = vadd.f32 1.0, %v68362
%v68364 = vmul.f32 %v68363, %v68357
%v68365 = vand.u32 2147483647, %v68357
%vm68366 = vcmp.lt.f32.partialorder %v68365, 0.0004427343
%v68367 = vsel /*vm=*/%vm68366, /*on_true_vy=*/%v68364, /*on_false_vx=*/%v68361
%v68368 = vxor.u32 2147483648, %v68367
%vm68371 = vcmp.lt.f32.partialorder %v68368, 5.0
%v68376 = vsel /*vm=*/%vm68371, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v68380 = vsel /*vm=*/%vm68371, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v68384 = vsel /*vm=*/%vm68371, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v68388 = vsel /*vm=*/%vm68371, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v68392 = vsel /*vm=*/%vm68371, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v68396 = vsel /*vm=*/%vm68371, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v68400 = vsel /*vm=*/%vm68371, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v68404 = vsel /*vm=*/%vm68371, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v68408 = vsel /*vm=*/%vm68371, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v68412 = vadd.f32 -2.5, %v68368
%v68414 = vrsqrt.pop %v68368
%v68415 = vmul.f32 %v68414, %v68368
%vm68416 = vcmp.eq.f32.partialorder %v68368, inf
%v68417 = vsel /*vm=*/%vm68416, /*on_true_vy=*/%v68368, /*on_false_vx=*/%v68415
%vm68418 = vcmp.eq.f32.partialorder %v68368, 0.0
%v68419 = vand.u32 2147483648, %v68368
%v68420 = vsel /*vm=*/%vm68418, /*on_true_vy=*/%v68419, /*on_false_vx=*/%v68417
%v68423 = vadd.f32 -3.0, %v68420
%v68427 = vsel /*vm=*/%vm68371, /*on_true_vy=*/%v68412, /*on_false_vx=*/%v68423
%v68431 = vmul.f32 %v68427, %v68408
%v68435 = vadd.f32 %v68431, %v68404
%v68439 = vmul.f32 %v68435, %v68427
%v68443 = vadd.f32 %v68439, %v68400
%v68447 = vmul.f32 %v68443, %v68427
%v68451 = vadd.f32 %v68447, %v68396
%v68455 = vmul.f32 %v68451, %v68427
%v68459 = vadd.f32 %v68455, %v68392
%v68463 = vmul.f32 %v68459, %v68427
%v68467 = vadd.f32 %v68463, %v68388
%v68471 = vmul.f32 %v68467, %v68427
%v68475 = vadd.f32 %v68471, %v68384
%v68479 = vmul.f32 %v68475, %v68427
%v68483 = vadd.f32 %v68479, %v68380
%v68487 = vmul.f32 %v68483, %v68427
%v68491 = vadd.f32 %v68487, %v68376
%v68495 = vmul.f32 %v68491, %v68342
%v68499 = vsel /*vm=*/%vm68347, /*on_true_vy=*/%v68352, /*on_false_vx=*/%v68495
%v68503 = vmul.f32 1.4140625, %v68499
%v68506 = vpack.c.bf16 %v120417, %v68503
%120111 = vst [vmem:[%s280 + $0xc8] sm:$0xf] /*vst_source=*/%v68506
%v68510 = vadd.s32 %v67585, %v1381
%v68520 = vadd.s32 %v68510, %v415
%vm68524 = vcmp.lt.u32.totalorder %v68520, %v68510
%vm68529 = vcmp.lt.u32.totalorder %v68510, %v1381
%v68534 = vadd.s32 %v67568, %v1368
%v68538 = vadd.s32 1, %v68534
%v68542 = vsel /*vm=*/%vm68529, /*on_true_vy=*/%v68538, /*on_false_vx=*/%v68534
%v68546 = vadd.s32 1, %v68542
%v68550 = vsel /*vm=*/%vm68524, /*on_true_vy=*/%v68546, /*on_false_vx=*/%v68542
%v68555 = vadd.s32 %v68550, %v10
%v68559 = vadd.s32 %v68520, %v9
%v68563 = vadd.s32 %v68559, %v68555
%v68565 = vshll.u32 %v68559, 13
%v68566 = vshrl.u32 %v68559, 19
%v68567 = vor.u32 %v68566, %v68565
%v68568 = vxor.u32 %v68567, %v68563
%v68571 = vadd.s32 %v68568, %v68563
%v68573 = vshll.u32 %v68568, 15
%v68574 = vshrl.u32 %v68568, 17
%v68575 = vor.u32 %v68574, %v68573
%v68576 = vxor.u32 %v68575, %v68571
%v68579 = vadd.s32 %v68576, %v68571
%v68581 = vshll.u32 %v68576, 26
%v68582 = vshrl.u32 %v68576, 6
%v68583 = vor.u32 %v68582, %v68581
%v68584 = vxor.u32 %v68583, %v68579
%v68587 = vadd.s32 %v68584, %v68579
%v68591 = vadd.s32 %v68587, %v9
%v68593 = vshll.u32 %v68584, 6
%v68594 = vshrl.u32 %v68584, 26
%v68595 = vor.u32 %v68594, %v68593
%v68596 = vxor.u32 %v68595, %v68587
%v68599 = vadd.s32 %v68596, %v8
%v68603 = vadd.s32 1, %v68599
%v68607 = vadd.s32 %v68603, %v68591
%v68609 = vshll.u32 %v68603, 17
%v68610 = vshrl.u32 %v68603, 15
%v68611 = vor.u32 %v68610, %v68609
%v68612 = vxor.u32 %v68611, %v68607
%v68615 = vadd.s32 %v68612, %v68607
%v68617 = vshll.u32 %v68612, 29
%v68618 = vshrl.u32 %v68612, 3
%v68619 = vor.u32 %v68618, %v68617
%v68620 = vxor.u32 %v68619, %v68615
%v68623 = vadd.s32 %v68620, %v68615
%v68625 = vshll.u32 %v68620, 16
%v68626 = vshrl.u32 %v68620, 16
%v68627 = vor.u32 %v68626, %v68625
%v68628 = vxor.u32 %v68627, %v68623
%v68631 = vadd.s32 %v68628, %v68623
%v68635 = vadd.s32 %v68631, %v8
%v68637 = vshll.u32 %v68628, 24
%v68638 = vshrl.u32 %v68628, 8
%v68639 = vor.u32 %v68638, %v68637
%v68640 = vxor.u32 %v68639, %v68631
%v68643 = vadd.s32 %v68640, %v10
%v68647 = vadd.s32 2, %v68643
%v68651 = vadd.s32 %v68647, %v68635
%v68653 = vshll.u32 %v68647, 13
%v68654 = vshrl.u32 %v68647, 19
%v68655 = vor.u32 %v68654, %v68653
%v68656 = vxor.u32 %v68655, %v68651
%v68659 = vadd.s32 %v68656, %v68651
%v68661 = vshll.u32 %v68656, 15
%v68662 = vshrl.u32 %v68656, 17
%v68663 = vor.u32 %v68662, %v68661
%v68664 = vxor.u32 %v68663, %v68659
%v68667 = vadd.s32 %v68664, %v68659
%v68669 = vshll.u32 %v68664, 26
%v68670 = vshrl.u32 %v68664, 6
%v68671 = vor.u32 %v68670, %v68669
%v68672 = vxor.u32 %v68671, %v68667
%v68675 = vadd.s32 %v68672, %v68667
%v68679 = vadd.s32 %v68675, %v10
%v68681 = vshll.u32 %v68672, 6
%v68682 = vshrl.u32 %v68672, 26
%v68683 = vor.u32 %v68682, %v68681
%v68684 = vxor.u32 %v68683, %v68675
%v68687 = vadd.s32 %v68684, %v9
%v68691 = vadd.s32 3, %v68687
%v68695 = vadd.s32 %v68691, %v68679
%v68697 = vshll.u32 %v68691, 17
%v68698 = vshrl.u32 %v68691, 15
%v68699 = vor.u32 %v68698, %v68697
%v68700 = vxor.u32 %v68699, %v68695
%v68703 = vadd.s32 %v68700, %v68695
%v68705 = vshll.u32 %v68700, 29
%v68706 = vshrl.u32 %v68700, 3
%v68707 = vor.u32 %v68706, %v68705
%v68708 = vxor.u32 %v68707, %v68703
%v68711 = vadd.s32 %v68708, %v68703
%v68713 = vshll.u32 %v68708, 16
%v68714 = vshrl.u32 %v68708, 16
%v68715 = vor.u32 %v68714, %v68713
%v68716 = vxor.u32 %v68715, %v68711
%v68719 = vadd.s32 %v68716, %v68711
%v68723 = vadd.s32 %v68719, %v9
%v68725 = vshll.u32 %v68716, 24
%v68726 = vshrl.u32 %v68716, 8
%v68727 = vor.u32 %v68726, %v68725
%v68728 = vxor.u32 %v68727, %v68719
%v68731 = vadd.s32 %v68728, %v8
%v68735 = vadd.s32 4, %v68731
%v68739 = vadd.s32 %v68735, %v68723
%v68741 = vshll.u32 %v68735, 13
%v68742 = vshrl.u32 %v68735, 19
%v68743 = vor.u32 %v68742, %v68741
%v68744 = vxor.u32 %v68743, %v68739
%v68747 = vadd.s32 %v68744, %v68739
%v68749 = vshll.u32 %v68744, 15
%v68750 = vshrl.u32 %v68744, 17
%v68751 = vor.u32 %v68750, %v68749
%v68752 = vxor.u32 %v68751, %v68747
%v68755 = vadd.s32 %v68752, %v68747
%v68757 = vshll.u32 %v68752, 26
%v68758 = vshrl.u32 %v68752, 6
%v68759 = vor.u32 %v68758, %v68757
%v68760 = vxor.u32 %v68759, %v68755
%v68763 = vadd.s32 %v68760, %v68755
%v68767 = vadd.s32 %v68763, %v8
%v68769 = vshll.u32 %v68760, 6
%v68770 = vshrl.u32 %v68760, 26
%v68771 = vor.u32 %v68770, %v68769
%v68772 = vxor.u32 %v68771, %v68763
%v68775 = vadd.s32 %v68772, %v10
%v68779 = vadd.s32 5, %v68775
%v68781 = vxor.u32 %v68779, %v68767
%v68782 = vand.u32.u8 255, %v68781
%v68783 = vand.u32 65535, %v68782
%v68784 = vshrl.u32 %v68783, 1
%v68785 = vor.u32 16256, %v68784
%v68786 = vand.u32.u16 65535, %v68785
%v120112 = vadd.low.f32.bf16 -1.0, %v68786
%v68795 = vmul.f32 2.0, %v120112
%v68799 = vadd.f32 -0.99609375, %v68795
%v68803 = vmax.f32 %v68799, -0.99609375
%v68805 = vand.u32 2147483647, %v68803
%vm68808 = vcmp.eq.f32.partialorder %v68805, 1.0
%v68813 = vmul.f32 inf, %v68803
%v68815 = vxor.u32 2147483648, %v68803
%v68818 = vmul.f32 %v68815, %v68803
%v68820 = vadd.f32 1.0, %v68818
%v68821 = vlog2.pop %v68820
%v68822 = vmul.f32 0.6931472, %v68821
%v68823 = vmul.f32 -0.5, %v68818
%v68824 = vadd.f32 1.0, %v68823
%v68825 = vmul.f32 %v68824, %v68818
%v68826 = vand.u32 2147483647, %v68818
%vm68827 = vcmp.lt.f32.partialorder %v68826, 0.0004427343
%v68828 = vsel /*vm=*/%vm68827, /*on_true_vy=*/%v68825, /*on_false_vx=*/%v68822
%v68829 = vxor.u32 2147483648, %v68828
%vm68832 = vcmp.lt.f32.partialorder %v68829, 5.0
%v68837 = vsel /*vm=*/%vm68832, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v68841 = vsel /*vm=*/%vm68832, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v68845 = vsel /*vm=*/%vm68832, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v68849 = vsel /*vm=*/%vm68832, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v68853 = vsel /*vm=*/%vm68832, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v68857 = vsel /*vm=*/%vm68832, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v68861 = vsel /*vm=*/%vm68832, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v68865 = vsel /*vm=*/%vm68832, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v68869 = vsel /*vm=*/%vm68832, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v68873 = vadd.f32 -2.5, %v68829
%v68875 = vrsqrt.pop %v68829
%v68876 = vmul.f32 %v68875, %v68829
%vm68877 = vcmp.eq.f32.partialorder %v68829, inf
%v68878 = vsel /*vm=*/%vm68877, /*on_true_vy=*/%v68829, /*on_false_vx=*/%v68876
%vm68879 = vcmp.eq.f32.partialorder %v68829, 0.0
%v68880 = vand.u32 2147483648, %v68829
%v68881 = vsel /*vm=*/%vm68879, /*on_true_vy=*/%v68880, /*on_false_vx=*/%v68878
%v68884 = vadd.f32 -3.0, %v68881
%v68888 = vsel /*vm=*/%vm68832, /*on_true_vy=*/%v68873, /*on_false_vx=*/%v68884
%v68892 = vmul.f32 %v68888, %v68869
%v68896 = vadd.f32 %v68892, %v68865
%v68900 = vmul.f32 %v68896, %v68888
%v68904 = vadd.f32 %v68900, %v68861
%v68908 = vmul.f32 %v68904, %v68888
%v68912 = vadd.f32 %v68908, %v68857
%v68916 = vmul.f32 %v68912, %v68888
%v68920 = vadd.f32 %v68916, %v68853
%v68924 = vmul.f32 %v68920, %v68888
%v68928 = vadd.f32 %v68924, %v68849
%v68932 = vmul.f32 %v68928, %v68888
%v68936 = vadd.f32 %v68932, %v68845
%v68940 = vmul.f32 %v68936, %v68888
%v68944 = vadd.f32 %v68940, %v68841
%v68948 = vmul.f32 %v68944, %v68888
%v68952 = vadd.f32 %v68948, %v68837
%v68956 = vmul.f32 %v68952, %v68803
%v68960 = vsel /*vm=*/%vm68808, /*on_true_vy=*/%v68813, /*on_false_vx=*/%v68956
%v68964 = vmul.f32 1.4140625, %v68960
%v68967 = vpack.c.bf16 %v120417, %v68964
%120113 = vst [vmem:[%s280 + $0x148] sm:$0xf] /*vst_source=*/%v68967
%v68971 = vadd.s32 %v67585, %v1868
%v68981 = vadd.s32 %v68971, %v415
%vm68985 = vcmp.lt.u32.totalorder %v68981, %v68971
%vm68990 = vcmp.lt.u32.totalorder %v68971, %v1868
%v68995 = vadd.s32 %v67568, %v1855
%v68999 = vadd.s32 1, %v68995
%v69003 = vsel /*vm=*/%vm68990, /*on_true_vy=*/%v68999, /*on_false_vx=*/%v68995
%v69007 = vadd.s32 1, %v69003
%v69011 = vsel /*vm=*/%vm68985, /*on_true_vy=*/%v69007, /*on_false_vx=*/%v69003
%v69016 = vadd.s32 %v69011, %v10
%v69020 = vadd.s32 %v68981, %v9
%v69024 = vadd.s32 %v69020, %v69016
%v69026 = vshll.u32 %v69020, 13
%v69027 = vshrl.u32 %v69020, 19
%v69028 = vor.u32 %v69027, %v69026
%v69029 = vxor.u32 %v69028, %v69024
%v69032 = vadd.s32 %v69029, %v69024
%v69034 = vshll.u32 %v69029, 15
%v69035 = vshrl.u32 %v69029, 17
%v69036 = vor.u32 %v69035, %v69034
%v69037 = vxor.u32 %v69036, %v69032
%v69040 = vadd.s32 %v69037, %v69032
%v69042 = vshll.u32 %v69037, 26
%v69043 = vshrl.u32 %v69037, 6
%v69044 = vor.u32 %v69043, %v69042
%v69045 = vxor.u32 %v69044, %v69040
%v69048 = vadd.s32 %v69045, %v69040
%v69052 = vadd.s32 %v69048, %v9
%v69054 = vshll.u32 %v69045, 6
%v69055 = vshrl.u32 %v69045, 26
%v69056 = vor.u32 %v69055, %v69054
%v69057 = vxor.u32 %v69056, %v69048
%v69060 = vadd.s32 %v69057, %v8
%v69064 = vadd.s32 1, %v69060
%v69068 = vadd.s32 %v69064, %v69052
%v69070 = vshll.u32 %v69064, 17
%v69071 = vshrl.u32 %v69064, 15
%v69072 = vor.u32 %v69071, %v69070
%v69073 = vxor.u32 %v69072, %v69068
%v69076 = vadd.s32 %v69073, %v69068
%v69078 = vshll.u32 %v69073, 29
%v69079 = vshrl.u32 %v69073, 3
%v69080 = vor.u32 %v69079, %v69078
%v69081 = vxor.u32 %v69080, %v69076
%v69084 = vadd.s32 %v69081, %v69076
%v69086 = vshll.u32 %v69081, 16
%v69087 = vshrl.u32 %v69081, 16
%v69088 = vor.u32 %v69087, %v69086
%v69089 = vxor.u32 %v69088, %v69084
%v69092 = vadd.s32 %v69089, %v69084
%v69096 = vadd.s32 %v69092, %v8
%v69098 = vshll.u32 %v69089, 24
%v69099 = vshrl.u32 %v69089, 8
%v69100 = vor.u32 %v69099, %v69098
%v69101 = vxor.u32 %v69100, %v69092
%v69104 = vadd.s32 %v69101, %v10
%v69108 = vadd.s32 2, %v69104
%v69112 = vadd.s32 %v69108, %v69096
%v69114 = vshll.u32 %v69108, 13
%v69115 = vshrl.u32 %v69108, 19
%v69116 = vor.u32 %v69115, %v69114
%v69117 = vxor.u32 %v69116, %v69112
%v69120 = vadd.s32 %v69117, %v69112
%v69122 = vshll.u32 %v69117, 15
%v69123 = vshrl.u32 %v69117, 17
%v69124 = vor.u32 %v69123, %v69122
%v69125 = vxor.u32 %v69124, %v69120
%v69128 = vadd.s32 %v69125, %v69120
%v69130 = vshll.u32 %v69125, 26
%v69131 = vshrl.u32 %v69125, 6
%v69132 = vor.u32 %v69131, %v69130
%v69133 = vxor.u32 %v69132, %v69128
%v69136 = vadd.s32 %v69133, %v69128
%v69140 = vadd.s32 %v69136, %v10
%v69142 = vshll.u32 %v69133, 6
%v69143 = vshrl.u32 %v69133, 26
%v69144 = vor.u32 %v69143, %v69142
%v69145 = vxor.u32 %v69144, %v69136
%v69148 = vadd.s32 %v69145, %v9
%v69152 = vadd.s32 3, %v69148
%v69156 = vadd.s32 %v69152, %v69140
%v69158 = vshll.u32 %v69152, 17
%v69159 = vshrl.u32 %v69152, 15
%v69160 = vor.u32 %v69159, %v69158
%v69161 = vxor.u32 %v69160, %v69156
%v69164 = vadd.s32 %v69161, %v69156
%v69166 = vshll.u32 %v69161, 29
%v69167 = vshrl.u32 %v69161, 3
%v69168 = vor.u32 %v69167, %v69166
%v69169 = vxor.u32 %v69168, %v69164
%v69172 = vadd.s32 %v69169, %v69164
%v69174 = vshll.u32 %v69169, 16
%v69175 = vshrl.u32 %v69169, 16
%v69176 = vor.u32 %v69175, %v69174
%v69177 = vxor.u32 %v69176, %v69172
%v69180 = vadd.s32 %v69177, %v69172
%v69184 = vadd.s32 %v69180, %v9
%v69186 = vshll.u32 %v69177, 24
%v69187 = vshrl.u32 %v69177, 8
%v69188 = vor.u32 %v69187, %v69186
%v69189 = vxor.u32 %v69188, %v69180
%v69192 = vadd.s32 %v69189, %v8
%v69196 = vadd.s32 4, %v69192
%v69200 = vadd.s32 %v69196, %v69184
%v69202 = vshll.u32 %v69196, 13
%v69203 = vshrl.u32 %v69196, 19
%v69204 = vor.u32 %v69203, %v69202
%v69205 = vxor.u32 %v69204, %v69200
%v69208 = vadd.s32 %v69205, %v69200
%v69210 = vshll.u32 %v69205, 15
%v69211 = vshrl.u32 %v69205, 17
%v69212 = vor.u32 %v69211, %v69210
%v69213 = vxor.u32 %v69212, %v69208
%v69216 = vadd.s32 %v69213, %v69208
%v69218 = vshll.u32 %v69213, 26
%v69219 = vshrl.u32 %v69213, 6
%v69220 = vor.u32 %v69219, %v69218
%v69221 = vxor.u32 %v69220, %v69216
%v69224 = vadd.s32 %v69221, %v69216
%v69228 = vadd.s32 %v69224, %v8
%v69230 = vshll.u32 %v69221, 6
%v69231 = vshrl.u32 %v69221, 26
%v69232 = vor.u32 %v69231, %v69230
%v69233 = vxor.u32 %v69232, %v69224
%v69236 = vadd.s32 %v69233, %v10
%v69240 = vadd.s32 5, %v69236
%v69242 = vxor.u32 %v69240, %v69228
%v69243 = vand.u32.u8 255, %v69242
%v69244 = vand.u32 65535, %v69243
%v69245 = vshrl.u32 %v69244, 1
%v69246 = vor.u32 16256, %v69245
%v69247 = vand.u32.u16 65535, %v69246
%v120114 = vadd.low.f32.bf16 -1.0, %v69247
%v69256 = vmul.f32 2.0, %v120114
%v69260 = vadd.f32 -0.99609375, %v69256
%v69264 = vmax.f32 %v69260, -0.99609375
%v69266 = vand.u32 2147483647, %v69264
%vm69269 = vcmp.eq.f32.partialorder %v69266, 1.0
%v69274 = vmul.f32 inf, %v69264
%v69276 = vxor.u32 2147483648, %v69264
%v69279 = vmul.f32 %v69276, %v69264
%v69281 = vadd.f32 1.0, %v69279
%v69282 = vlog2.pop %v69281
%v69283 = vmul.f32 0.6931472, %v69282
%v69284 = vmul.f32 -0.5, %v69279
%v69285 = vadd.f32 1.0, %v69284
%v69286 = vmul.f32 %v69285, %v69279
%v69287 = vand.u32 2147483647, %v69279
%vm69288 = vcmp.lt.f32.partialorder %v69287, 0.0004427343
%v69289 = vsel /*vm=*/%vm69288, /*on_true_vy=*/%v69286, /*on_false_vx=*/%v69283
%v69290 = vxor.u32 2147483648, %v69289
%vm69293 = vcmp.lt.f32.partialorder %v69290, 5.0
%v69298 = vsel /*vm=*/%vm69293, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v69302 = vsel /*vm=*/%vm69293, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v69306 = vsel /*vm=*/%vm69293, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v69310 = vsel /*vm=*/%vm69293, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v69314 = vsel /*vm=*/%vm69293, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v69318 = vsel /*vm=*/%vm69293, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v69322 = vsel /*vm=*/%vm69293, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v69326 = vsel /*vm=*/%vm69293, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v69330 = vsel /*vm=*/%vm69293, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v69334 = vadd.f32 -2.5, %v69290
%v69336 = vrsqrt.pop %v69290
%v69337 = vmul.f32 %v69336, %v69290
%vm69338 = vcmp.eq.f32.partialorder %v69290, inf
%v69339 = vsel /*vm=*/%vm69338, /*on_true_vy=*/%v69290, /*on_false_vx=*/%v69337
%vm69340 = vcmp.eq.f32.partialorder %v69290, 0.0
%v69341 = vand.u32 2147483648, %v69290
%v69342 = vsel /*vm=*/%vm69340, /*on_true_vy=*/%v69341, /*on_false_vx=*/%v69339
%v69345 = vadd.f32 -3.0, %v69342
%v69349 = vsel /*vm=*/%vm69293, /*on_true_vy=*/%v69334, /*on_false_vx=*/%v69345
%v69353 = vmul.f32 %v69349, %v69330
%v69357 = vadd.f32 %v69353, %v69326
%v69361 = vmul.f32 %v69357, %v69349
%v69365 = vadd.f32 %v69361, %v69322
%v69369 = vmul.f32 %v69365, %v69349
%v69373 = vadd.f32 %v69369, %v69318
%v69377 = vmul.f32 %v69373, %v69349
%v69381 = vadd.f32 %v69377, %v69314
%v69385 = vmul.f32 %v69381, %v69349
%v69389 = vadd.f32 %v69385, %v69310
%v69393 = vmul.f32 %v69389, %v69349
%v69397 = vadd.f32 %v69393, %v69306
%v69401 = vmul.f32 %v69397, %v69349
%v69405 = vadd.f32 %v69401, %v69302
%v69409 = vmul.f32 %v69405, %v69349
%v69413 = vadd.f32 %v69409, %v69298
%v69417 = vmul.f32 %v69413, %v69264
%v69421 = vsel /*vm=*/%vm69269, /*on_true_vy=*/%v69274, /*on_false_vx=*/%v69417
%v69425 = vmul.f32 1.4140625, %v69421
%v69428 = vpack.c.bf16 %v120417, %v69425
%120115 = vst [vmem:[%s280 + $0x1c8] sm:$0xf] /*vst_source=*/%v69428
%v69432 = vadd.s32 %v67585, %v2355
%v69442 = vadd.s32 %v69432, %v415
%vm69446 = vcmp.lt.u32.totalorder %v69442, %v69432
%vm69451 = vcmp.lt.u32.totalorder %v69432, %v2355
%v69456 = vadd.s32 %v67568, %v2342
%v69460 = vadd.s32 1, %v69456
%v69464 = vsel /*vm=*/%vm69451, /*on_true_vy=*/%v69460, /*on_false_vx=*/%v69456
%v69468 = vadd.s32 1, %v69464
%v69472 = vsel /*vm=*/%vm69446, /*on_true_vy=*/%v69468, /*on_false_vx=*/%v69464
%v69477 = vadd.s32 %v69472, %v10
%v69481 = vadd.s32 %v69442, %v9
%v69485 = vadd.s32 %v69481, %v69477
%v69487 = vshll.u32 %v69481, 13
%v69488 = vshrl.u32 %v69481, 19
%v69489 = vor.u32 %v69488, %v69487
%v69490 = vxor.u32 %v69489, %v69485
%v69493 = vadd.s32 %v69490, %v69485
%v69495 = vshll.u32 %v69490, 15
%v69496 = vshrl.u32 %v69490, 17
%v69497 = vor.u32 %v69496, %v69495
%v69498 = vxor.u32 %v69497, %v69493
%v69501 = vadd.s32 %v69498, %v69493
%v69503 = vshll.u32 %v69498, 26
%v69504 = vshrl.u32 %v69498, 6
%v69505 = vor.u32 %v69504, %v69503
%v69506 = vxor.u32 %v69505, %v69501
%v69509 = vadd.s32 %v69506, %v69501
%v69513 = vadd.s32 %v69509, %v9
%v69515 = vshll.u32 %v69506, 6
%v69516 = vshrl.u32 %v69506, 26
%v69517 = vor.u32 %v69516, %v69515
%v69518 = vxor.u32 %v69517, %v69509
%v69521 = vadd.s32 %v69518, %v8
%v69525 = vadd.s32 1, %v69521
%v69529 = vadd.s32 %v69525, %v69513
%v69531 = vshll.u32 %v69525, 17
%v69532 = vshrl.u32 %v69525, 15
%v69533 = vor.u32 %v69532, %v69531
%v69534 = vxor.u32 %v69533, %v69529
%v69537 = vadd.s32 %v69534, %v69529
%v69539 = vshll.u32 %v69534, 29
%v69540 = vshrl.u32 %v69534, 3
%v69541 = vor.u32 %v69540, %v69539
%v69542 = vxor.u32 %v69541, %v69537
%v69545 = vadd.s32 %v69542, %v69537
%v69547 = vshll.u32 %v69542, 16
%v69548 = vshrl.u32 %v69542, 16
%v69549 = vor.u32 %v69548, %v69547
%v69550 = vxor.u32 %v69549, %v69545
%v69553 = vadd.s32 %v69550, %v69545
%v69557 = vadd.s32 %v69553, %v8
%v69559 = vshll.u32 %v69550, 24
%v69560 = vshrl.u32 %v69550, 8
%v69561 = vor.u32 %v69560, %v69559
%v69562 = vxor.u32 %v69561, %v69553
%v69565 = vadd.s32 %v69562, %v10
%v69569 = vadd.s32 2, %v69565
%v69573 = vadd.s32 %v69569, %v69557
%v69575 = vshll.u32 %v69569, 13
%v69576 = vshrl.u32 %v69569, 19
%v69577 = vor.u32 %v69576, %v69575
%v69578 = vxor.u32 %v69577, %v69573
%v69581 = vadd.s32 %v69578, %v69573
%v69583 = vshll.u32 %v69578, 15
%v69584 = vshrl.u32 %v69578, 17
%v69585 = vor.u32 %v69584, %v69583
%v69586 = vxor.u32 %v69585, %v69581
%v69589 = vadd.s32 %v69586, %v69581
%v69591 = vshll.u32 %v69586, 26
%v69592 = vshrl.u32 %v69586, 6
%v69593 = vor.u32 %v69592, %v69591
%v69594 = vxor.u32 %v69593, %v69589
%v69597 = vadd.s32 %v69594, %v69589
%v69601 = vadd.s32 %v69597, %v10
%v69603 = vshll.u32 %v69594, 6
%v69604 = vshrl.u32 %v69594, 26
%v69605 = vor.u32 %v69604, %v69603
%v69606 = vxor.u32 %v69605, %v69597
%v69609 = vadd.s32 %v69606, %v9
%v69613 = vadd.s32 3, %v69609
%v69617 = vadd.s32 %v69613, %v69601
%v69619 = vshll.u32 %v69613, 17
%v69620 = vshrl.u32 %v69613, 15
%v69621 = vor.u32 %v69620, %v69619
%v69622 = vxor.u32 %v69621, %v69617
%v69625 = vadd.s32 %v69622, %v69617
%v69627 = vshll.u32 %v69622, 29
%v69628 = vshrl.u32 %v69622, 3
%v69629 = vor.u32 %v69628, %v69627
%v69630 = vxor.u32 %v69629, %v69625
%v69633 = vadd.s32 %v69630, %v69625
%v69635 = vshll.u32 %v69630, 16
%v69636 = vshrl.u32 %v69630, 16
%v69637 = vor.u32 %v69636, %v69635
%v69638 = vxor.u32 %v69637, %v69633
%v69641 = vadd.s32 %v69638, %v69633
%v69645 = vadd.s32 %v69641, %v9
%v69647 = vshll.u32 %v69638, 24
%v69648 = vshrl.u32 %v69638, 8
%v69649 = vor.u32 %v69648, %v69647
%v69650 = vxor.u32 %v69649, %v69641
%v69653 = vadd.s32 %v69650, %v8
%v69657 = vadd.s32 4, %v69653
%v69661 = vadd.s32 %v69657, %v69645
%v69663 = vshll.u32 %v69657, 13
%v69664 = vshrl.u32 %v69657, 19
%v69665 = vor.u32 %v69664, %v69663
%v69666 = vxor.u32 %v69665, %v69661
%v69669 = vadd.s32 %v69666, %v69661
%v69671 = vshll.u32 %v69666, 15
%v69672 = vshrl.u32 %v69666, 17
%v69673 = vor.u32 %v69672, %v69671
%v69674 = vxor.u32 %v69673, %v69669
%v69677 = vadd.s32 %v69674, %v69669
%v69679 = vshll.u32 %v69674, 26
%v69680 = vshrl.u32 %v69674, 6
%v69681 = vor.u32 %v69680, %v69679
%v69682 = vxor.u32 %v69681, %v69677
%v69685 = vadd.s32 %v69682, %v69677
%v69689 = vadd.s32 %v69685, %v8
%v69691 = vshll.u32 %v69682, 6
%v69692 = vshrl.u32 %v69682, 26
%v69693 = vor.u32 %v69692, %v69691
%v69694 = vxor.u32 %v69693, %v69685
%v69697 = vadd.s32 %v69694, %v10
%v69701 = vadd.s32 5, %v69697
%v69703 = vxor.u32 %v69701, %v69689
%v69704 = vand.u32.u8 255, %v69703
%v69705 = vand.u32 65535, %v69704
%v69706 = vshrl.u32 %v69705, 1
%v69707 = vor.u32 16256, %v69706
%v69708 = vand.u32.u16 65535, %v69707
%v120116 = vadd.low.f32.bf16 -1.0, %v69708
%v69717 = vmul.f32 2.0, %v120116
%v69721 = vadd.f32 -0.99609375, %v69717
%v69725 = vmax.f32 %v69721, -0.99609375
%v69727 = vand.u32 2147483647, %v69725
%vm69730 = vcmp.eq.f32.partialorder %v69727, 1.0
%v69735 = vmul.f32 inf, %v69725
%v69737 = vxor.u32 2147483648, %v69725
%v69740 = vmul.f32 %v69737, %v69725
%v69742 = vadd.f32 1.0, %v69740
%v69743 = vlog2.pop %v69742
%v69744 = vmul.f32 0.6931472, %v69743
%v69745 = vmul.f32 -0.5, %v69740
%v69746 = vadd.f32 1.0, %v69745
%v69747 = vmul.f32 %v69746, %v69740
%v69748 = vand.u32 2147483647, %v69740
%vm69749 = vcmp.lt.f32.partialorder %v69748, 0.0004427343
%v69750 = vsel /*vm=*/%vm69749, /*on_true_vy=*/%v69747, /*on_false_vx=*/%v69744
%v69751 = vxor.u32 2147483648, %v69750
%vm69754 = vcmp.lt.f32.partialorder %v69751, 5.0
%v69759 = vsel /*vm=*/%vm69754, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v69763 = vsel /*vm=*/%vm69754, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v69767 = vsel /*vm=*/%vm69754, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v69771 = vsel /*vm=*/%vm69754, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v69775 = vsel /*vm=*/%vm69754, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v69779 = vsel /*vm=*/%vm69754, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v69783 = vsel /*vm=*/%vm69754, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v69787 = vsel /*vm=*/%vm69754, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v69791 = vsel /*vm=*/%vm69754, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v69795 = vadd.f32 -2.5, %v69751
%v69797 = vrsqrt.pop %v69751
%v69798 = vmul.f32 %v69797, %v69751
%vm69799 = vcmp.eq.f32.partialorder %v69751, inf
%v69800 = vsel /*vm=*/%vm69799, /*on_true_vy=*/%v69751, /*on_false_vx=*/%v69798
%vm69801 = vcmp.eq.f32.partialorder %v69751, 0.0
%v69802 = vand.u32 2147483648, %v69751
%v69803 = vsel /*vm=*/%vm69801, /*on_true_vy=*/%v69802, /*on_false_vx=*/%v69800
%v69806 = vadd.f32 -3.0, %v69803
%v69810 = vsel /*vm=*/%vm69754, /*on_true_vy=*/%v69795, /*on_false_vx=*/%v69806
%v69814 = vmul.f32 %v69810, %v69791
%v69818 = vadd.f32 %v69814, %v69787
%v69822 = vmul.f32 %v69818, %v69810
%v69826 = vadd.f32 %v69822, %v69783
%v69830 = vmul.f32 %v69826, %v69810
%v69834 = vadd.f32 %v69830, %v69779
%v69838 = vmul.f32 %v69834, %v69810
%v69842 = vadd.f32 %v69838, %v69775
%v69846 = vmul.f32 %v69842, %v69810
%v69850 = vadd.f32 %v69846, %v69771
%v69854 = vmul.f32 %v69850, %v69810
%v69858 = vadd.f32 %v69854, %v69767
%v69862 = vmul.f32 %v69858, %v69810
%v69866 = vadd.f32 %v69862, %v69763
%v69870 = vmul.f32 %v69866, %v69810
%v69874 = vadd.f32 %v69870, %v69759
%v69878 = vmul.f32 %v69874, %v69725
%v69882 = vsel /*vm=*/%vm69730, /*on_true_vy=*/%v69735, /*on_false_vx=*/%v69878
%v69886 = vmul.f32 1.4140625, %v69882
%v69889 = vpack.c.bf16 %v120417, %v69886
%120117 = vst [vmem:[%s280 + $0x248] sm:$0xf] /*vst_source=*/%v69889
%v69893 = vadd.s32 %v67585, %v2842
%v69903 = vadd.s32 %v69893, %v415
%vm69907 = vcmp.lt.u32.totalorder %v69903, %v69893
%vm69912 = vcmp.lt.u32.totalorder %v69893, %v2842
%v69917 = vadd.s32 %v67568, %v2829
%v69921 = vadd.s32 1, %v69917
%v69925 = vsel /*vm=*/%vm69912, /*on_true_vy=*/%v69921, /*on_false_vx=*/%v69917
%v69929 = vadd.s32 1, %v69925
%v69933 = vsel /*vm=*/%vm69907, /*on_true_vy=*/%v69929, /*on_false_vx=*/%v69925
%v69938 = vadd.s32 %v69933, %v10
%v69942 = vadd.s32 %v69903, %v9
%v69946 = vadd.s32 %v69942, %v69938
%v69948 = vshll.u32 %v69942, 13
%v69949 = vshrl.u32 %v69942, 19
%v69950 = vor.u32 %v69949, %v69948
%v69951 = vxor.u32 %v69950, %v69946
%v69954 = vadd.s32 %v69951, %v69946
%v69956 = vshll.u32 %v69951, 15
%v69957 = vshrl.u32 %v69951, 17
%v69958 = vor.u32 %v69957, %v69956
%v69959 = vxor.u32 %v69958, %v69954
%v69962 = vadd.s32 %v69959, %v69954
%v69964 = vshll.u32 %v69959, 26
%v69965 = vshrl.u32 %v69959, 6
%v69966 = vor.u32 %v69965, %v69964
%v69967 = vxor.u32 %v69966, %v69962
%v69970 = vadd.s32 %v69967, %v69962
%v69974 = vadd.s32 %v69970, %v9
%v69976 = vshll.u32 %v69967, 6
%v69977 = vshrl.u32 %v69967, 26
%v69978 = vor.u32 %v69977, %v69976
%v69979 = vxor.u32 %v69978, %v69970
%v69982 = vadd.s32 %v69979, %v8
%v69986 = vadd.s32 1, %v69982
%v69990 = vadd.s32 %v69986, %v69974
%v69992 = vshll.u32 %v69986, 17
%v69993 = vshrl.u32 %v69986, 15
%v69994 = vor.u32 %v69993, %v69992
%v69995 = vxor.u32 %v69994, %v69990
%v69998 = vadd.s32 %v69995, %v69990
%v70000 = vshll.u32 %v69995, 29
%v70001 = vshrl.u32 %v69995, 3
%v70002 = vor.u32 %v70001, %v70000
%v70003 = vxor.u32 %v70002, %v69998
%v70006 = vadd.s32 %v70003, %v69998
%v70008 = vshll.u32 %v70003, 16
%v70009 = vshrl.u32 %v70003, 16
%v70010 = vor.u32 %v70009, %v70008
%v70011 = vxor.u32 %v70010, %v70006
%v70014 = vadd.s32 %v70011, %v70006
%v70018 = vadd.s32 %v70014, %v8
%v70020 = vshll.u32 %v70011, 24
%v70021 = vshrl.u32 %v70011, 8
%v70022 = vor.u32 %v70021, %v70020
%v70023 = vxor.u32 %v70022, %v70014
%v70026 = vadd.s32 %v70023, %v10
%v70030 = vadd.s32 2, %v70026
%v70034 = vadd.s32 %v70030, %v70018
%v70036 = vshll.u32 %v70030, 13
%v70037 = vshrl.u32 %v70030, 19
%v70038 = vor.u32 %v70037, %v70036
%v70039 = vxor.u32 %v70038, %v70034
%v70042 = vadd.s32 %v70039, %v70034
%v70044 = vshll.u32 %v70039, 15
%v70045 = vshrl.u32 %v70039, 17
%v70046 = vor.u32 %v70045, %v70044
%v70047 = vxor.u32 %v70046, %v70042
%v70050 = vadd.s32 %v70047, %v70042
%v70052 = vshll.u32 %v70047, 26
%v70053 = vshrl.u32 %v70047, 6
%v70054 = vor.u32 %v70053, %v70052
%v70055 = vxor.u32 %v70054, %v70050
%v70058 = vadd.s32 %v70055, %v70050
%v70062 = vadd.s32 %v70058, %v10
%v70064 = vshll.u32 %v70055, 6
%v70065 = vshrl.u32 %v70055, 26
%v70066 = vor.u32 %v70065, %v70064
%v70067 = vxor.u32 %v70066, %v70058
%v70070 = vadd.s32 %v70067, %v9
%v70074 = vadd.s32 3, %v70070
%v70078 = vadd.s32 %v70074, %v70062
%v70080 = vshll.u32 %v70074, 17
%v70081 = vshrl.u32 %v70074, 15
%v70082 = vor.u32 %v70081, %v70080
%v70083 = vxor.u32 %v70082, %v70078
%v70086 = vadd.s32 %v70083, %v70078
%v70088 = vshll.u32 %v70083, 29
%v70089 = vshrl.u32 %v70083, 3
%v70090 = vor.u32 %v70089, %v70088
%v70091 = vxor.u32 %v70090, %v70086
%v70094 = vadd.s32 %v70091, %v70086
%v70096 = vshll.u32 %v70091, 16
%v70097 = vshrl.u32 %v70091, 16
%v70098 = vor.u32 %v70097, %v70096
%v70099 = vxor.u32 %v70098, %v70094
%v70102 = vadd.s32 %v70099, %v70094
%v70106 = vadd.s32 %v70102, %v9
%v70108 = vshll.u32 %v70099, 24
%v70109 = vshrl.u32 %v70099, 8
%v70110 = vor.u32 %v70109, %v70108
%v70111 = vxor.u32 %v70110, %v70102
%v70114 = vadd.s32 %v70111, %v8
%v70118 = vadd.s32 4, %v70114
%v70122 = vadd.s32 %v70118, %v70106
%v70124 = vshll.u32 %v70118, 13
%v70125 = vshrl.u32 %v70118, 19
%v70126 = vor.u32 %v70125, %v70124
%v70127 = vxor.u32 %v70126, %v70122
%v70130 = vadd.s32 %v70127, %v70122
%v70132 = vshll.u32 %v70127, 15
%v70133 = vshrl.u32 %v70127, 17
%v70134 = vor.u32 %v70133, %v70132
%v70135 = vxor.u32 %v70134, %v70130
%v70138 = vadd.s32 %v70135, %v70130
%v70140 = vshll.u32 %v70135, 26
%v70141 = vshrl.u32 %v70135, 6
%v70142 = vor.u32 %v70141, %v70140
%v70143 = vxor.u32 %v70142, %v70138
%v70146 = vadd.s32 %v70143, %v70138
%v70150 = vadd.s32 %v70146, %v8
%v70152 = vshll.u32 %v70143, 6
%v70153 = vshrl.u32 %v70143, 26
%v70154 = vor.u32 %v70153, %v70152
%v70155 = vxor.u32 %v70154, %v70146
%v70158 = vadd.s32 %v70155, %v10
%v70162 = vadd.s32 5, %v70158
%v70164 = vxor.u32 %v70162, %v70150
%v70165 = vand.u32.u8 255, %v70164
%v70166 = vand.u32 65535, %v70165
%v70167 = vshrl.u32 %v70166, 1
%v70168 = vor.u32 16256, %v70167
%v70169 = vand.u32.u16 65535, %v70168
%v120118 = vadd.low.f32.bf16 -1.0, %v70169
%v70178 = vmul.f32 2.0, %v120118
%v70182 = vadd.f32 -0.99609375, %v70178
%v70186 = vmax.f32 %v70182, -0.99609375
%v70188 = vand.u32 2147483647, %v70186
%vm70191 = vcmp.eq.f32.partialorder %v70188, 1.0
%v70196 = vmul.f32 inf, %v70186
%v70198 = vxor.u32 2147483648, %v70186
%v70201 = vmul.f32 %v70198, %v70186
%v70203 = vadd.f32 1.0, %v70201
%v70204 = vlog2.pop %v70203
%v70205 = vmul.f32 0.6931472, %v70204
%v70206 = vmul.f32 -0.5, %v70201
%v70207 = vadd.f32 1.0, %v70206
%v70208 = vmul.f32 %v70207, %v70201
%v70209 = vand.u32 2147483647, %v70201
%vm70210 = vcmp.lt.f32.partialorder %v70209, 0.0004427343
%v70211 = vsel /*vm=*/%vm70210, /*on_true_vy=*/%v70208, /*on_false_vx=*/%v70205
%v70212 = vxor.u32 2147483648, %v70211
%vm70215 = vcmp.lt.f32.partialorder %v70212, 5.0
%v70220 = vsel /*vm=*/%vm70215, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v70224 = vsel /*vm=*/%vm70215, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v70228 = vsel /*vm=*/%vm70215, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v70232 = vsel /*vm=*/%vm70215, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v70236 = vsel /*vm=*/%vm70215, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v70240 = vsel /*vm=*/%vm70215, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v70244 = vsel /*vm=*/%vm70215, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v70248 = vsel /*vm=*/%vm70215, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v70252 = vsel /*vm=*/%vm70215, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v70256 = vadd.f32 -2.5, %v70212
%v70258 = vrsqrt.pop %v70212
%v70259 = vmul.f32 %v70258, %v70212
%vm70260 = vcmp.eq.f32.partialorder %v70212, inf
%v70261 = vsel /*vm=*/%vm70260, /*on_true_vy=*/%v70212, /*on_false_vx=*/%v70259
%vm70262 = vcmp.eq.f32.partialorder %v70212, 0.0
%v70263 = vand.u32 2147483648, %v70212
%v70264 = vsel /*vm=*/%vm70262, /*on_true_vy=*/%v70263, /*on_false_vx=*/%v70261
%v70267 = vadd.f32 -3.0, %v70264
%v70271 = vsel /*vm=*/%vm70215, /*on_true_vy=*/%v70256, /*on_false_vx=*/%v70267
%v70275 = vmul.f32 %v70271, %v70252
%v70279 = vadd.f32 %v70275, %v70248
%v70283 = vmul.f32 %v70279, %v70271
%v70287 = vadd.f32 %v70283, %v70244
%v70291 = vmul.f32 %v70287, %v70271
%v70295 = vadd.f32 %v70291, %v70240
%v70299 = vmul.f32 %v70295, %v70271
%v70303 = vadd.f32 %v70299, %v70236
%v70307 = vmul.f32 %v70303, %v70271
%v70311 = vadd.f32 %v70307, %v70232
%v70315 = vmul.f32 %v70311, %v70271
%v70319 = vadd.f32 %v70315, %v70228
%v70323 = vmul.f32 %v70319, %v70271
%v70327 = vadd.f32 %v70323, %v70224
%v70331 = vmul.f32 %v70327, %v70271
%v70335 = vadd.f32 %v70331, %v70220
%v70339 = vmul.f32 %v70335, %v70186
%v70343 = vsel /*vm=*/%vm70191, /*on_true_vy=*/%v70196, /*on_false_vx=*/%v70339
%v70347 = vmul.f32 1.4140625, %v70343
%v70350 = vpack.c.bf16 %v120417, %v70347
%120119 = vst [vmem:[%s280 + $0x2c8] sm:$0xf] /*vst_source=*/%v70350
%v70354 = vadd.s32 %v67585, %v3329
%v70364 = vadd.s32 %v70354, %v415
%vm70368 = vcmp.lt.u32.totalorder %v70364, %v70354
%vm70373 = vcmp.lt.u32.totalorder %v70354, %v3329
%v70378 = vadd.s32 %v67568, %v3316
%v70382 = vadd.s32 1, %v70378
%v70386 = vsel /*vm=*/%vm70373, /*on_true_vy=*/%v70382, /*on_false_vx=*/%v70378
%v70390 = vadd.s32 1, %v70386
%v70394 = vsel /*vm=*/%vm70368, /*on_true_vy=*/%v70390, /*on_false_vx=*/%v70386
%v70399 = vadd.s32 %v70394, %v10
%v70403 = vadd.s32 %v70364, %v9
%v70407 = vadd.s32 %v70403, %v70399
%v70409 = vshll.u32 %v70403, 13
%v70410 = vshrl.u32 %v70403, 19
%v70411 = vor.u32 %v70410, %v70409
%v70412 = vxor.u32 %v70411, %v70407
%v70415 = vadd.s32 %v70412, %v70407
%v70417 = vshll.u32 %v70412, 15
%v70418 = vshrl.u32 %v70412, 17
%v70419 = vor.u32 %v70418, %v70417
%v70420 = vxor.u32 %v70419, %v70415
%v70423 = vadd.s32 %v70420, %v70415
%v70425 = vshll.u32 %v70420, 26
%v70426 = vshrl.u32 %v70420, 6
%v70427 = vor.u32 %v70426, %v70425
%v70428 = vxor.u32 %v70427, %v70423
%v70431 = vadd.s32 %v70428, %v70423
%v70435 = vadd.s32 %v70431, %v9
%v70437 = vshll.u32 %v70428, 6
%v70438 = vshrl.u32 %v70428, 26
%v70439 = vor.u32 %v70438, %v70437
%v70440 = vxor.u32 %v70439, %v70431
%v70443 = vadd.s32 %v70440, %v8
%v70447 = vadd.s32 1, %v70443
%v70451 = vadd.s32 %v70447, %v70435
%v70453 = vshll.u32 %v70447, 17
%v70454 = vshrl.u32 %v70447, 15
%v70455 = vor.u32 %v70454, %v70453
%v70456 = vxor.u32 %v70455, %v70451
%v70459 = vadd.s32 %v70456, %v70451
%v70461 = vshll.u32 %v70456, 29
%v70462 = vshrl.u32 %v70456, 3
%v70463 = vor.u32 %v70462, %v70461
%v70464 = vxor.u32 %v70463, %v70459
%v70467 = vadd.s32 %v70464, %v70459
%v70469 = vshll.u32 %v70464, 16
%v70470 = vshrl.u32 %v70464, 16
%v70471 = vor.u32 %v70470, %v70469
%v70472 = vxor.u32 %v70471, %v70467
%v70475 = vadd.s32 %v70472, %v70467
%v70479 = vadd.s32 %v70475, %v8
%v70481 = vshll.u32 %v70472, 24
%v70482 = vshrl.u32 %v70472, 8
%v70483 = vor.u32 %v70482, %v70481
%v70484 = vxor.u32 %v70483, %v70475
%v70487 = vadd.s32 %v70484, %v10
%v70491 = vadd.s32 2, %v70487
%v70495 = vadd.s32 %v70491, %v70479
%v70497 = vshll.u32 %v70491, 13
%v70498 = vshrl.u32 %v70491, 19
%v70499 = vor.u32 %v70498, %v70497
%v70500 = vxor.u32 %v70499, %v70495
%v70503 = vadd.s32 %v70500, %v70495
%v70505 = vshll.u32 %v70500, 15
%v70506 = vshrl.u32 %v70500, 17
%v70507 = vor.u32 %v70506, %v70505
%v70508 = vxor.u32 %v70507, %v70503
%v70511 = vadd.s32 %v70508, %v70503
%v70513 = vshll.u32 %v70508, 26
%v70514 = vshrl.u32 %v70508, 6
%v70515 = vor.u32 %v70514, %v70513
%v70516 = vxor.u32 %v70515, %v70511
%v70519 = vadd.s32 %v70516, %v70511
%v70523 = vadd.s32 %v70519, %v10
%v70525 = vshll.u32 %v70516, 6
%v70526 = vshrl.u32 %v70516, 26
%v70527 = vor.u32 %v70526, %v70525
%v70528 = vxor.u32 %v70527, %v70519
%v70531 = vadd.s32 %v70528, %v9
%v70535 = vadd.s32 3, %v70531
%v70539 = vadd.s32 %v70535, %v70523
%v70541 = vshll.u32 %v70535, 17
%v70542 = vshrl.u32 %v70535, 15
%v70543 = vor.u32 %v70542, %v70541
%v70544 = vxor.u32 %v70543, %v70539
%v70547 = vadd.s32 %v70544, %v70539
%v70549 = vshll.u32 %v70544, 29
%v70550 = vshrl.u32 %v70544, 3
%v70551 = vor.u32 %v70550, %v70549
%v70552 = vxor.u32 %v70551, %v70547
%v70555 = vadd.s32 %v70552, %v70547
%v70557 = vshll.u32 %v70552, 16
%v70558 = vshrl.u32 %v70552, 16
%v70559 = vor.u32 %v70558, %v70557
%v70560 = vxor.u32 %v70559, %v70555
%v70563 = vadd.s32 %v70560, %v70555
%v70567 = vadd.s32 %v70563, %v9
%v70569 = vshll.u32 %v70560, 24
%v70570 = vshrl.u32 %v70560, 8
%v70571 = vor.u32 %v70570, %v70569
%v70572 = vxor.u32 %v70571, %v70563
%v70575 = vadd.s32 %v70572, %v8
%v70579 = vadd.s32 4, %v70575
%v70583 = vadd.s32 %v70579, %v70567
%v70585 = vshll.u32 %v70579, 13
%v70586 = vshrl.u32 %v70579, 19
%v70587 = vor.u32 %v70586, %v70585
%v70588 = vxor.u32 %v70587, %v70583
%v70591 = vadd.s32 %v70588, %v70583
%v70593 = vshll.u32 %v70588, 15
%v70594 = vshrl.u32 %v70588, 17
%v70595 = vor.u32 %v70594, %v70593
%v70596 = vxor.u32 %v70595, %v70591
%v70599 = vadd.s32 %v70596, %v70591
%v70601 = vshll.u32 %v70596, 26
%v70602 = vshrl.u32 %v70596, 6
%v70603 = vor.u32 %v70602, %v70601
%v70604 = vxor.u32 %v70603, %v70599
%v70607 = vadd.s32 %v70604, %v70599
%v70611 = vadd.s32 %v70607, %v8
%v70613 = vshll.u32 %v70604, 6
%v70614 = vshrl.u32 %v70604, 26
%v70615 = vor.u32 %v70614, %v70613
%v70616 = vxor.u32 %v70615, %v70607
%v70619 = vadd.s32 %v70616, %v10
%v70623 = vadd.s32 5, %v70619
%v70625 = vxor.u32 %v70623, %v70611
%v70626 = vand.u32.u8 255, %v70625
%v70627 = vand.u32 65535, %v70626
%v70628 = vshrl.u32 %v70627, 1
%v70629 = vor.u32 16256, %v70628
%v70630 = vand.u32.u16 65535, %v70629
%v120120 = vadd.low.f32.bf16 -1.0, %v70630
%v70639 = vmul.f32 2.0, %v120120
%v70643 = vadd.f32 -0.99609375, %v70639
%v70647 = vmax.f32 %v70643, -0.99609375
%v70649 = vand.u32 2147483647, %v70647
%vm70652 = vcmp.eq.f32.partialorder %v70649, 1.0
%v70657 = vmul.f32 inf, %v70647
%v70659 = vxor.u32 2147483648, %v70647
%v70662 = vmul.f32 %v70659, %v70647
%v70664 = vadd.f32 1.0, %v70662
%v70665 = vlog2.pop %v70664
%v70666 = vmul.f32 0.6931472, %v70665
%v70667 = vmul.f32 -0.5, %v70662
%v70668 = vadd.f32 1.0, %v70667
%v70669 = vmul.f32 %v70668, %v70662
%v70670 = vand.u32 2147483647, %v70662
%vm70671 = vcmp.lt.f32.partialorder %v70670, 0.0004427343
%v70672 = vsel /*vm=*/%vm70671, /*on_true_vy=*/%v70669, /*on_false_vx=*/%v70666
%v70673 = vxor.u32 2147483648, %v70672
%vm70676 = vcmp.lt.f32.partialorder %v70673, 5.0
%v70681 = vsel /*vm=*/%vm70676, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v70685 = vsel /*vm=*/%vm70676, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v70689 = vsel /*vm=*/%vm70676, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v70693 = vsel /*vm=*/%vm70676, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v70697 = vsel /*vm=*/%vm70676, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v70701 = vsel /*vm=*/%vm70676, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v70705 = vsel /*vm=*/%vm70676, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v70709 = vsel /*vm=*/%vm70676, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v70713 = vsel /*vm=*/%vm70676, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v70717 = vadd.f32 -2.5, %v70673
%v70719 = vrsqrt.pop %v70673
%v70720 = vmul.f32 %v70719, %v70673
%vm70721 = vcmp.eq.f32.partialorder %v70673, inf
%v70722 = vsel /*vm=*/%vm70721, /*on_true_vy=*/%v70673, /*on_false_vx=*/%v70720
%vm70723 = vcmp.eq.f32.partialorder %v70673, 0.0
%v70724 = vand.u32 2147483648, %v70673
%v70725 = vsel /*vm=*/%vm70723, /*on_true_vy=*/%v70724, /*on_false_vx=*/%v70722
%v70728 = vadd.f32 -3.0, %v70725
%v70732 = vsel /*vm=*/%vm70676, /*on_true_vy=*/%v70717, /*on_false_vx=*/%v70728
%v70736 = vmul.f32 %v70732, %v70713
%v70740 = vadd.f32 %v70736, %v70709
%v70744 = vmul.f32 %v70740, %v70732
%v70748 = vadd.f32 %v70744, %v70705
%v70752 = vmul.f32 %v70748, %v70732
%v70756 = vadd.f32 %v70752, %v70701
%v70760 = vmul.f32 %v70756, %v70732
%v70764 = vadd.f32 %v70760, %v70697
%v70768 = vmul.f32 %v70764, %v70732
%v70772 = vadd.f32 %v70768, %v70693
%v70776 = vmul.f32 %v70772, %v70732
%v70780 = vadd.f32 %v70776, %v70689
%v70784 = vmul.f32 %v70780, %v70732
%v70788 = vadd.f32 %v70784, %v70685
%v70792 = vmul.f32 %v70788, %v70732
%v70796 = vadd.f32 %v70792, %v70681
%v70800 = vmul.f32 %v70796, %v70647
%v70804 = vsel /*vm=*/%vm70652, /*on_true_vy=*/%v70657, /*on_false_vx=*/%v70800
%v70808 = vmul.f32 1.4140625, %v70804
%v70811 = vpack.c.bf16 %v120417, %v70808
%120121 = vst [vmem:[%s280 + $0x348] sm:$0xf] /*vst_source=*/%v70811
%v70815 = vadd.s32 %v67585, %v3816
%v70825 = vadd.s32 %v70815, %v415
%vm70829 = vcmp.lt.u32.totalorder %v70825, %v70815
%vm70834 = vcmp.lt.u32.totalorder %v70815, %v3816
%v70839 = vadd.s32 %v67568, %v3803
%v70843 = vadd.s32 1, %v70839
%v70847 = vsel /*vm=*/%vm70834, /*on_true_vy=*/%v70843, /*on_false_vx=*/%v70839
%v70851 = vadd.s32 1, %v70847
%v70855 = vsel /*vm=*/%vm70829, /*on_true_vy=*/%v70851, /*on_false_vx=*/%v70847
%v70860 = vadd.s32 %v70855, %v10
%v70864 = vadd.s32 %v70825, %v9
%v70868 = vadd.s32 %v70864, %v70860
%v70870 = vshll.u32 %v70864, 13
%v70871 = vshrl.u32 %v70864, 19
%v70872 = vor.u32 %v70871, %v70870
%v70873 = vxor.u32 %v70872, %v70868
%v70876 = vadd.s32 %v70873, %v70868
%v70878 = vshll.u32 %v70873, 15
%v70879 = vshrl.u32 %v70873, 17
%v70880 = vor.u32 %v70879, %v70878
%v70881 = vxor.u32 %v70880, %v70876
%v70884 = vadd.s32 %v70881, %v70876
%v70886 = vshll.u32 %v70881, 26
%v70887 = vshrl.u32 %v70881, 6
%v70888 = vor.u32 %v70887, %v70886
%v70889 = vxor.u32 %v70888, %v70884
%v70892 = vadd.s32 %v70889, %v70884
%v70896 = vadd.s32 %v70892, %v9
%v70898 = vshll.u32 %v70889, 6
%v70899 = vshrl.u32 %v70889, 26
%v70900 = vor.u32 %v70899, %v70898
%v70901 = vxor.u32 %v70900, %v70892
%v70904 = vadd.s32 %v70901, %v8
%v70908 = vadd.s32 1, %v70904
%v70912 = vadd.s32 %v70908, %v70896
%v70914 = vshll.u32 %v70908, 17
%v70915 = vshrl.u32 %v70908, 15
%v70916 = vor.u32 %v70915, %v70914
%v70917 = vxor.u32 %v70916, %v70912
%v70920 = vadd.s32 %v70917, %v70912
%v70922 = vshll.u32 %v70917, 29
%v70923 = vshrl.u32 %v70917, 3
%v70924 = vor.u32 %v70923, %v70922
%v70925 = vxor.u32 %v70924, %v70920
%v70928 = vadd.s32 %v70925, %v70920
%v70930 = vshll.u32 %v70925, 16
%v70931 = vshrl.u32 %v70925, 16
%v70932 = vor.u32 %v70931, %v70930
%v70933 = vxor.u32 %v70932, %v70928
%v70936 = vadd.s32 %v70933, %v70928
%v70940 = vadd.s32 %v70936, %v8
%v70942 = vshll.u32 %v70933, 24
%v70943 = vshrl.u32 %v70933, 8
%v70944 = vor.u32 %v70943, %v70942
%v70945 = vxor.u32 %v70944, %v70936
%v70948 = vadd.s32 %v70945, %v10
%v70952 = vadd.s32 2, %v70948
%v70956 = vadd.s32 %v70952, %v70940
%v70958 = vshll.u32 %v70952, 13
%v70959 = vshrl.u32 %v70952, 19
%v70960 = vor.u32 %v70959, %v70958
%v70961 = vxor.u32 %v70960, %v70956
%v70964 = vadd.s32 %v70961, %v70956
%v70966 = vshll.u32 %v70961, 15
%v70967 = vshrl.u32 %v70961, 17
%v70968 = vor.u32 %v70967, %v70966
%v70969 = vxor.u32 %v70968, %v70964
%v70972 = vadd.s32 %v70969, %v70964
%v70974 = vshll.u32 %v70969, 26
%v70975 = vshrl.u32 %v70969, 6
%v70976 = vor.u32 %v70975, %v70974
%v70977 = vxor.u32 %v70976, %v70972
%v70980 = vadd.s32 %v70977, %v70972
%v70984 = vadd.s32 %v70980, %v10
%v70986 = vshll.u32 %v70977, 6
%v70987 = vshrl.u32 %v70977, 26
%v70988 = vor.u32 %v70987, %v70986
%v70989 = vxor.u32 %v70988, %v70980
%v70992 = vadd.s32 %v70989, %v9
%v70996 = vadd.s32 3, %v70992
%v71000 = vadd.s32 %v70996, %v70984
%v71002 = vshll.u32 %v70996, 17
%v71003 = vshrl.u32 %v70996, 15
%v71004 = vor.u32 %v71003, %v71002
%v71005 = vxor.u32 %v71004, %v71000
%v71008 = vadd.s32 %v71005, %v71000
%v71010 = vshll.u32 %v71005, 29
%v71011 = vshrl.u32 %v71005, 3
%v71012 = vor.u32 %v71011, %v71010
%v71013 = vxor.u32 %v71012, %v71008
%v71016 = vadd.s32 %v71013, %v71008
%v71018 = vshll.u32 %v71013, 16
%v71019 = vshrl.u32 %v71013, 16
%v71020 = vor.u32 %v71019, %v71018
%v71021 = vxor.u32 %v71020, %v71016
%v71024 = vadd.s32 %v71021, %v71016
%v71028 = vadd.s32 %v71024, %v9
%v71030 = vshll.u32 %v71021, 24
%v71031 = vshrl.u32 %v71021, 8
%v71032 = vor.u32 %v71031, %v71030
%v71033 = vxor.u32 %v71032, %v71024
%v71036 = vadd.s32 %v71033, %v8
%v71040 = vadd.s32 4, %v71036
%v71044 = vadd.s32 %v71040, %v71028
%v71046 = vshll.u32 %v71040, 13
%v71047 = vshrl.u32 %v71040, 19
%v71048 = vor.u32 %v71047, %v71046
%v71049 = vxor.u32 %v71048, %v71044
%v71052 = vadd.s32 %v71049, %v71044
%v71054 = vshll.u32 %v71049, 15
%v71055 = vshrl.u32 %v71049, 17
%v71056 = vor.u32 %v71055, %v71054
%v71057 = vxor.u32 %v71056, %v71052
%v71060 = vadd.s32 %v71057, %v71052
%v71062 = vshll.u32 %v71057, 26
%v71063 = vshrl.u32 %v71057, 6
%v71064 = vor.u32 %v71063, %v71062
%v71065 = vxor.u32 %v71064, %v71060
%v71068 = vadd.s32 %v71065, %v71060
%v71072 = vadd.s32 %v71068, %v8
%v71074 = vshll.u32 %v71065, 6
%v71075 = vshrl.u32 %v71065, 26
%v71076 = vor.u32 %v71075, %v71074
%v71077 = vxor.u32 %v71076, %v71068
%v71080 = vadd.s32 %v71077, %v10
%v71084 = vadd.s32 5, %v71080
%v71086 = vxor.u32 %v71084, %v71072
%v71087 = vand.u32.u8 255, %v71086
%v71088 = vand.u32 65535, %v71087
%v71089 = vshrl.u32 %v71088, 1
%v71090 = vor.u32 16256, %v71089
%v71091 = vand.u32.u16 65535, %v71090
%v120122 = vadd.low.f32.bf16 -1.0, %v71091
%v71100 = vmul.f32 2.0, %v120122
%v71104 = vadd.f32 -0.99609375, %v71100
%v71108 = vmax.f32 %v71104, -0.99609375
%v71110 = vand.u32 2147483647, %v71108
%vm71113 = vcmp.eq.f32.partialorder %v71110, 1.0
%v71118 = vmul.f32 inf, %v71108
%v71120 = vxor.u32 2147483648, %v71108
%v71123 = vmul.f32 %v71120, %v71108
%v71125 = vadd.f32 1.0, %v71123
%v71126 = vlog2.pop %v71125
%v71127 = vmul.f32 0.6931472, %v71126
%v71128 = vmul.f32 -0.5, %v71123
%v71129 = vadd.f32 1.0, %v71128
%v71130 = vmul.f32 %v71129, %v71123
%v71131 = vand.u32 2147483647, %v71123
%vm71132 = vcmp.lt.f32.partialorder %v71131, 0.0004427343
%v71133 = vsel /*vm=*/%vm71132, /*on_true_vy=*/%v71130, /*on_false_vx=*/%v71127
%v71134 = vxor.u32 2147483648, %v71133
%vm71137 = vcmp.lt.f32.partialorder %v71134, 5.0
%v71142 = vsel /*vm=*/%vm71137, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v71146 = vsel /*vm=*/%vm71137, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v71150 = vsel /*vm=*/%vm71137, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v71154 = vsel /*vm=*/%vm71137, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v71158 = vsel /*vm=*/%vm71137, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v71162 = vsel /*vm=*/%vm71137, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v71166 = vsel /*vm=*/%vm71137, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v71170 = vsel /*vm=*/%vm71137, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v71174 = vsel /*vm=*/%vm71137, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v71178 = vadd.f32 -2.5, %v71134
%v71180 = vrsqrt.pop %v71134
%v71181 = vmul.f32 %v71180, %v71134
%vm71182 = vcmp.eq.f32.partialorder %v71134, inf
%v71183 = vsel /*vm=*/%vm71182, /*on_true_vy=*/%v71134, /*on_false_vx=*/%v71181
%vm71184 = vcmp.eq.f32.partialorder %v71134, 0.0
%v71185 = vand.u32 2147483648, %v71134
%v71186 = vsel /*vm=*/%vm71184, /*on_true_vy=*/%v71185, /*on_false_vx=*/%v71183
%v71189 = vadd.f32 -3.0, %v71186
%v71193 = vsel /*vm=*/%vm71137, /*on_true_vy=*/%v71178, /*on_false_vx=*/%v71189
%v71197 = vmul.f32 %v71193, %v71174
%v71201 = vadd.f32 %v71197, %v71170
%v71205 = vmul.f32 %v71201, %v71193
%v71209 = vadd.f32 %v71205, %v71166
%v71213 = vmul.f32 %v71209, %v71193
%v71217 = vadd.f32 %v71213, %v71162
%v71221 = vmul.f32 %v71217, %v71193
%v71225 = vadd.f32 %v71221, %v71158
%v71229 = vmul.f32 %v71225, %v71193
%v71233 = vadd.f32 %v71229, %v71154
%v71237 = vmul.f32 %v71233, %v71193
%v71241 = vadd.f32 %v71237, %v71150
%v71245 = vmul.f32 %v71241, %v71193
%v71249 = vadd.f32 %v71245, %v71146
%v71253 = vmul.f32 %v71249, %v71193
%v71257 = vadd.f32 %v71253, %v71142
%v71261 = vmul.f32 %v71257, %v71108
%v71265 = vsel /*vm=*/%vm71113, /*on_true_vy=*/%v71118, /*on_false_vx=*/%v71261
%v71269 = vmul.f32 1.4140625, %v71265
%v71272 = vpack.c.bf16 %v120417, %v71269
%120123 = vst [vmem:[%s280 + $0x3c8] sm:$0xf] /*vst_source=*/%v71272
%v71310 = vadd.s32 %v71307, %v408
%v71320 = vadd.s32 %v71310, %v415
%vm71324 = vcmp.lt.u32.totalorder %v71320, %v71310
%vm71329 = vcmp.lt.u32.totalorder %v71310, %v408
%v71334 = vadd.s32 %v71290, %v380
%v71338 = vadd.s32 1, %v71334
%v71342 = vsel /*vm=*/%vm71329, /*on_true_vy=*/%v71338, /*on_false_vx=*/%v71334
%v71346 = vadd.s32 1, %v71342
%v71350 = vsel /*vm=*/%vm71324, /*on_true_vy=*/%v71346, /*on_false_vx=*/%v71342
%v71355 = vadd.s32 %v71350, %v10
%v71359 = vadd.s32 %v71320, %v9
%v71363 = vadd.s32 %v71359, %v71355
%v71365 = vshll.u32 %v71359, 13
%v71366 = vshrl.u32 %v71359, 19
%v71367 = vor.u32 %v71366, %v71365
%v71368 = vxor.u32 %v71367, %v71363
%v71371 = vadd.s32 %v71368, %v71363
%v71373 = vshll.u32 %v71368, 15
%v71374 = vshrl.u32 %v71368, 17
%v71375 = vor.u32 %v71374, %v71373
%v71376 = vxor.u32 %v71375, %v71371
%v71379 = vadd.s32 %v71376, %v71371
%v71381 = vshll.u32 %v71376, 26
%v71382 = vshrl.u32 %v71376, 6
%v71383 = vor.u32 %v71382, %v71381
%v71384 = vxor.u32 %v71383, %v71379
%v71387 = vadd.s32 %v71384, %v71379
%v71391 = vadd.s32 %v71387, %v9
%v71393 = vshll.u32 %v71384, 6
%v71394 = vshrl.u32 %v71384, 26
%v71395 = vor.u32 %v71394, %v71393
%v71396 = vxor.u32 %v71395, %v71387
%v71399 = vadd.s32 %v71396, %v8
%v71403 = vadd.s32 1, %v71399
%v71407 = vadd.s32 %v71403, %v71391
%v71409 = vshll.u32 %v71403, 17
%v71410 = vshrl.u32 %v71403, 15
%v71411 = vor.u32 %v71410, %v71409
%v71412 = vxor.u32 %v71411, %v71407
%v71415 = vadd.s32 %v71412, %v71407
%v71417 = vshll.u32 %v71412, 29
%v71418 = vshrl.u32 %v71412, 3
%v71419 = vor.u32 %v71418, %v71417
%v71420 = vxor.u32 %v71419, %v71415
%v71423 = vadd.s32 %v71420, %v71415
%v71425 = vshll.u32 %v71420, 16
%v71426 = vshrl.u32 %v71420, 16
%v71427 = vor.u32 %v71426, %v71425
%v71428 = vxor.u32 %v71427, %v71423
%v71431 = vadd.s32 %v71428, %v71423
%v71435 = vadd.s32 %v71431, %v8
%v71437 = vshll.u32 %v71428, 24
%v71438 = vshrl.u32 %v71428, 8
%v71439 = vor.u32 %v71438, %v71437
%v71440 = vxor.u32 %v71439, %v71431
%v71443 = vadd.s32 %v71440, %v10
%v71447 = vadd.s32 2, %v71443
%v71451 = vadd.s32 %v71447, %v71435
%v71453 = vshll.u32 %v71447, 13
%v71454 = vshrl.u32 %v71447, 19
%v71455 = vor.u32 %v71454, %v71453
%v71456 = vxor.u32 %v71455, %v71451
%v71459 = vadd.s32 %v71456, %v71451
%v71461 = vshll.u32 %v71456, 15
%v71462 = vshrl.u32 %v71456, 17
%v71463 = vor.u32 %v71462, %v71461
%v71464 = vxor.u32 %v71463, %v71459
%v71467 = vadd.s32 %v71464, %v71459
%v71469 = vshll.u32 %v71464, 26
%v71470 = vshrl.u32 %v71464, 6
%v71471 = vor.u32 %v71470, %v71469
%v71472 = vxor.u32 %v71471, %v71467
%v71475 = vadd.s32 %v71472, %v71467
%v71479 = vadd.s32 %v71475, %v10
%v71481 = vshll.u32 %v71472, 6
%v71482 = vshrl.u32 %v71472, 26
%v71483 = vor.u32 %v71482, %v71481
%v71484 = vxor.u32 %v71483, %v71475
%v71487 = vadd.s32 %v71484, %v9
%v71491 = vadd.s32 3, %v71487
%v71495 = vadd.s32 %v71491, %v71479
%v71497 = vshll.u32 %v71491, 17
%v71498 = vshrl.u32 %v71491, 15
%v71499 = vor.u32 %v71498, %v71497
%v71500 = vxor.u32 %v71499, %v71495
%v71503 = vadd.s32 %v71500, %v71495
%v71505 = vshll.u32 %v71500, 29
%v71506 = vshrl.u32 %v71500, 3
%v71507 = vor.u32 %v71506, %v71505
%v71508 = vxor.u32 %v71507, %v71503
%v71511 = vadd.s32 %v71508, %v71503
%v71513 = vshll.u32 %v71508, 16
%v71514 = vshrl.u32 %v71508, 16
%v71515 = vor.u32 %v71514, %v71513
%v71516 = vxor.u32 %v71515, %v71511
%v71519 = vadd.s32 %v71516, %v71511
%v71523 = vadd.s32 %v71519, %v9
%v71525 = vshll.u32 %v71516, 24
%v71526 = vshrl.u32 %v71516, 8
%v71527 = vor.u32 %v71526, %v71525
%v71528 = vxor.u32 %v71527, %v71519
%v71531 = vadd.s32 %v71528, %v8
%v71535 = vadd.s32 4, %v71531
%v71539 = vadd.s32 %v71535, %v71523
%v71541 = vshll.u32 %v71535, 13
%v71542 = vshrl.u32 %v71535, 19
%v71543 = vor.u32 %v71542, %v71541
%v71544 = vxor.u32 %v71543, %v71539
%v71547 = vadd.s32 %v71544, %v71539
%v71549 = vshll.u32 %v71544, 15
%v71550 = vshrl.u32 %v71544, 17
%v71551 = vor.u32 %v71550, %v71549
%v71552 = vxor.u32 %v71551, %v71547
%v71555 = vadd.s32 %v71552, %v71547
%v71557 = vshll.u32 %v71552, 26
%v71558 = vshrl.u32 %v71552, 6
%v71559 = vor.u32 %v71558, %v71557
%v71560 = vxor.u32 %v71559, %v71555
%v71563 = vadd.s32 %v71560, %v71555
%v71567 = vadd.s32 %v71563, %v8
%v71569 = vshll.u32 %v71560, 6
%v71570 = vshrl.u32 %v71560, 26
%v71571 = vor.u32 %v71570, %v71569
%v71572 = vxor.u32 %v71571, %v71563
%v71575 = vadd.s32 %v71572, %v10
%v71579 = vadd.s32 5, %v71575
%v71581 = vxor.u32 %v71579, %v71567
%v71582 = vand.u32.u8 255, %v71581
%v71583 = vand.u32 65535, %v71582
%v71584 = vshrl.u32 %v71583, 1
%v71585 = vor.u32 16256, %v71584
%v71586 = vand.u32.u16 65535, %v71585
%v120128 = vadd.low.f32.bf16 -1.0, %v71586
%v71595 = vmul.f32 2.0, %v120128
%v71599 = vadd.f32 -0.99609375, %v71595
%v71603 = vmax.f32 %v71599, -0.99609375
%v71605 = vand.u32 2147483647, %v71603
%vm71608 = vcmp.eq.f32.partialorder %v71605, 1.0
%v71613 = vmul.f32 inf, %v71603
%v71615 = vxor.u32 2147483648, %v71603
%v71618 = vmul.f32 %v71615, %v71603
%v71620 = vadd.f32 1.0, %v71618
%v71621 = vlog2.pop %v71620
%v71622 = vmul.f32 0.6931472, %v71621
%v71623 = vmul.f32 -0.5, %v71618
%v71624 = vadd.f32 1.0, %v71623
%v71625 = vmul.f32 %v71624, %v71618
%v71626 = vand.u32 2147483647, %v71618
%vm71627 = vcmp.lt.f32.partialorder %v71626, 0.0004427343
%v71628 = vsel /*vm=*/%vm71627, /*on_true_vy=*/%v71625, /*on_false_vx=*/%v71622
%v71629 = vxor.u32 2147483648, %v71628
%vm71632 = vcmp.lt.f32.partialorder %v71629, 5.0
%v71637 = vsel /*vm=*/%vm71632, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v71641 = vsel /*vm=*/%vm71632, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v71645 = vsel /*vm=*/%vm71632, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v71649 = vsel /*vm=*/%vm71632, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v71653 = vsel /*vm=*/%vm71632, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v71657 = vsel /*vm=*/%vm71632, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v71661 = vsel /*vm=*/%vm71632, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v71665 = vsel /*vm=*/%vm71632, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v71669 = vsel /*vm=*/%vm71632, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v71673 = vadd.f32 -2.5, %v71629
%v71675 = vrsqrt.pop %v71629
%v71676 = vmul.f32 %v71675, %v71629
%vm71677 = vcmp.eq.f32.partialorder %v71629, inf
%v71678 = vsel /*vm=*/%vm71677, /*on_true_vy=*/%v71629, /*on_false_vx=*/%v71676
%vm71679 = vcmp.eq.f32.partialorder %v71629, 0.0
%v71680 = vand.u32 2147483648, %v71629
%v71681 = vsel /*vm=*/%vm71679, /*on_true_vy=*/%v71680, /*on_false_vx=*/%v71678
%v71684 = vadd.f32 -3.0, %v71681
%v71688 = vsel /*vm=*/%vm71632, /*on_true_vy=*/%v71673, /*on_false_vx=*/%v71684
%v71692 = vmul.f32 %v71688, %v71669
%v71696 = vadd.f32 %v71692, %v71665
%v71700 = vmul.f32 %v71696, %v71688
%v71704 = vadd.f32 %v71700, %v71661
%v71708 = vmul.f32 %v71704, %v71688
%v71712 = vadd.f32 %v71708, %v71657
%v71716 = vmul.f32 %v71712, %v71688
%v71720 = vadd.f32 %v71716, %v71653
%v71724 = vmul.f32 %v71720, %v71688
%v71728 = vadd.f32 %v71724, %v71649
%v71732 = vmul.f32 %v71728, %v71688
%v71736 = vadd.f32 %v71732, %v71645
%v71740 = vmul.f32 %v71736, %v71688
%v71744 = vadd.f32 %v71740, %v71641
%v71748 = vmul.f32 %v71744, %v71688
%v71752 = vadd.f32 %v71748, %v71637
%v71756 = vmul.f32 %v71752, %v71603
%v71760 = vsel /*vm=*/%vm71608, /*on_true_vy=*/%v71613, /*on_false_vx=*/%v71756
%v71764 = vmul.f32 1.4140625, %v71760
%v71767 = vpack.c.bf16 %v120417, %v71764
%120129 = vst [vmem:[%s280 + $0x4c] sm:$0xf] /*vst_source=*/%v71767
%v71771 = vadd.s32 %v71307, %v894
%v71781 = vadd.s32 %v71771, %v415
%vm71785 = vcmp.lt.u32.totalorder %v71781, %v71771
%vm71790 = vcmp.lt.u32.totalorder %v71771, %v894
%v71795 = vadd.s32 %v71290, %v881
%v71799 = vadd.s32 1, %v71795
%v71803 = vsel /*vm=*/%vm71790, /*on_true_vy=*/%v71799, /*on_false_vx=*/%v71795
%v71807 = vadd.s32 1, %v71803
%v71811 = vsel /*vm=*/%vm71785, /*on_true_vy=*/%v71807, /*on_false_vx=*/%v71803
%v71816 = vadd.s32 %v71811, %v10
%v71820 = vadd.s32 %v71781, %v9
%v71824 = vadd.s32 %v71820, %v71816
%v71826 = vshll.u32 %v71820, 13
%v71827 = vshrl.u32 %v71820, 19
%v71828 = vor.u32 %v71827, %v71826
%v71829 = vxor.u32 %v71828, %v71824
%v71832 = vadd.s32 %v71829, %v71824
%v71834 = vshll.u32 %v71829, 15
%v71835 = vshrl.u32 %v71829, 17
%v71836 = vor.u32 %v71835, %v71834
%v71837 = vxor.u32 %v71836, %v71832
%v71840 = vadd.s32 %v71837, %v71832
%v71842 = vshll.u32 %v71837, 26
%v71843 = vshrl.u32 %v71837, 6
%v71844 = vor.u32 %v71843, %v71842
%v71845 = vxor.u32 %v71844, %v71840
%v71848 = vadd.s32 %v71845, %v71840
%v71852 = vadd.s32 %v71848, %v9
%v71854 = vshll.u32 %v71845, 6
%v71855 = vshrl.u32 %v71845, 26
%v71856 = vor.u32 %v71855, %v71854
%v71857 = vxor.u32 %v71856, %v71848
%v71860 = vadd.s32 %v71857, %v8
%v71864 = vadd.s32 1, %v71860
%v71868 = vadd.s32 %v71864, %v71852
%v71870 = vshll.u32 %v71864, 17
%v71871 = vshrl.u32 %v71864, 15
%v71872 = vor.u32 %v71871, %v71870
%v71873 = vxor.u32 %v71872, %v71868
%v71876 = vadd.s32 %v71873, %v71868
%v71878 = vshll.u32 %v71873, 29
%v71879 = vshrl.u32 %v71873, 3
%v71880 = vor.u32 %v71879, %v71878
%v71881 = vxor.u32 %v71880, %v71876
%v71884 = vadd.s32 %v71881, %v71876
%v71886 = vshll.u32 %v71881, 16
%v71887 = vshrl.u32 %v71881, 16
%v71888 = vor.u32 %v71887, %v71886
%v71889 = vxor.u32 %v71888, %v71884
%v71892 = vadd.s32 %v71889, %v71884
%v71896 = vadd.s32 %v71892, %v8
%v71898 = vshll.u32 %v71889, 24
%v71899 = vshrl.u32 %v71889, 8
%v71900 = vor.u32 %v71899, %v71898
%v71901 = vxor.u32 %v71900, %v71892
%v71904 = vadd.s32 %v71901, %v10
%v71908 = vadd.s32 2, %v71904
%v71912 = vadd.s32 %v71908, %v71896
%v71914 = vshll.u32 %v71908, 13
%v71915 = vshrl.u32 %v71908, 19
%v71916 = vor.u32 %v71915, %v71914
%v71917 = vxor.u32 %v71916, %v71912
%v71920 = vadd.s32 %v71917, %v71912
%v71922 = vshll.u32 %v71917, 15
%v71923 = vshrl.u32 %v71917, 17
%v71924 = vor.u32 %v71923, %v71922
%v71925 = vxor.u32 %v71924, %v71920
%v71928 = vadd.s32 %v71925, %v71920
%v71930 = vshll.u32 %v71925, 26
%v71931 = vshrl.u32 %v71925, 6
%v71932 = vor.u32 %v71931, %v71930
%v71933 = vxor.u32 %v71932, %v71928
%v71936 = vadd.s32 %v71933, %v71928
%v71940 = vadd.s32 %v71936, %v10
%v71942 = vshll.u32 %v71933, 6
%v71943 = vshrl.u32 %v71933, 26
%v71944 = vor.u32 %v71943, %v71942
%v71945 = vxor.u32 %v71944, %v71936
%v71948 = vadd.s32 %v71945, %v9
%v71952 = vadd.s32 3, %v71948
%v71956 = vadd.s32 %v71952, %v71940
%v71958 = vshll.u32 %v71952, 17
%v71959 = vshrl.u32 %v71952, 15
%v71960 = vor.u32 %v71959, %v71958
%v71961 = vxor.u32 %v71960, %v71956
%v71964 = vadd.s32 %v71961, %v71956
%v71966 = vshll.u32 %v71961, 29
%v71967 = vshrl.u32 %v71961, 3
%v71968 = vor.u32 %v71967, %v71966
%v71969 = vxor.u32 %v71968, %v71964
%v71972 = vadd.s32 %v71969, %v71964
%v71974 = vshll.u32 %v71969, 16
%v71975 = vshrl.u32 %v71969, 16
%v71976 = vor.u32 %v71975, %v71974
%v71977 = vxor.u32 %v71976, %v71972
%v71980 = vadd.s32 %v71977, %v71972
%v71984 = vadd.s32 %v71980, %v9
%v71986 = vshll.u32 %v71977, 24
%v71987 = vshrl.u32 %v71977, 8
%v71988 = vor.u32 %v71987, %v71986
%v71989 = vxor.u32 %v71988, %v71980
%v71992 = vadd.s32 %v71989, %v8
%v71996 = vadd.s32 4, %v71992
%v72000 = vadd.s32 %v71996, %v71984
%v72002 = vshll.u32 %v71996, 13
%v72003 = vshrl.u32 %v71996, 19
%v72004 = vor.u32 %v72003, %v72002
%v72005 = vxor.u32 %v72004, %v72000
%v72008 = vadd.s32 %v72005, %v72000
%v72010 = vshll.u32 %v72005, 15
%v72011 = vshrl.u32 %v72005, 17
%v72012 = vor.u32 %v72011, %v72010
%v72013 = vxor.u32 %v72012, %v72008
%v72016 = vadd.s32 %v72013, %v72008
%v72018 = vshll.u32 %v72013, 26
%v72019 = vshrl.u32 %v72013, 6
%v72020 = vor.u32 %v72019, %v72018
%v72021 = vxor.u32 %v72020, %v72016
%v72024 = vadd.s32 %v72021, %v72016
%v72028 = vadd.s32 %v72024, %v8
%v72030 = vshll.u32 %v72021, 6
%v72031 = vshrl.u32 %v72021, 26
%v72032 = vor.u32 %v72031, %v72030
%v72033 = vxor.u32 %v72032, %v72024
%v72036 = vadd.s32 %v72033, %v10
%v72040 = vadd.s32 5, %v72036
%v72042 = vxor.u32 %v72040, %v72028
%v72043 = vand.u32.u8 255, %v72042
%v72044 = vand.u32 65535, %v72043
%v72045 = vshrl.u32 %v72044, 1
%v72046 = vor.u32 16256, %v72045
%v72047 = vand.u32.u16 65535, %v72046
%v120130 = vadd.low.f32.bf16 -1.0, %v72047
%v72056 = vmul.f32 2.0, %v120130
%v72060 = vadd.f32 -0.99609375, %v72056
%v72064 = vmax.f32 %v72060, -0.99609375
%v72066 = vand.u32 2147483647, %v72064
%vm72069 = vcmp.eq.f32.partialorder %v72066, 1.0
%v72074 = vmul.f32 inf, %v72064
%v72076 = vxor.u32 2147483648, %v72064
%v72079 = vmul.f32 %v72076, %v72064
%v72081 = vadd.f32 1.0, %v72079
%v72082 = vlog2.pop %v72081
%v72083 = vmul.f32 0.6931472, %v72082
%v72084 = vmul.f32 -0.5, %v72079
%v72085 = vadd.f32 1.0, %v72084
%v72086 = vmul.f32 %v72085, %v72079
%v72087 = vand.u32 2147483647, %v72079
%vm72088 = vcmp.lt.f32.partialorder %v72087, 0.0004427343
%v72089 = vsel /*vm=*/%vm72088, /*on_true_vy=*/%v72086, /*on_false_vx=*/%v72083
%v72090 = vxor.u32 2147483648, %v72089
%vm72093 = vcmp.lt.f32.partialorder %v72090, 5.0
%v72098 = vsel /*vm=*/%vm72093, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v72102 = vsel /*vm=*/%vm72093, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v72106 = vsel /*vm=*/%vm72093, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v72110 = vsel /*vm=*/%vm72093, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v72114 = vsel /*vm=*/%vm72093, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v72118 = vsel /*vm=*/%vm72093, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v72122 = vsel /*vm=*/%vm72093, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v72126 = vsel /*vm=*/%vm72093, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v72130 = vsel /*vm=*/%vm72093, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v72134 = vadd.f32 -2.5, %v72090
%v72136 = vrsqrt.pop %v72090
%v72137 = vmul.f32 %v72136, %v72090
%vm72138 = vcmp.eq.f32.partialorder %v72090, inf
%v72139 = vsel /*vm=*/%vm72138, /*on_true_vy=*/%v72090, /*on_false_vx=*/%v72137
%vm72140 = vcmp.eq.f32.partialorder %v72090, 0.0
%v72141 = vand.u32 2147483648, %v72090
%v72142 = vsel /*vm=*/%vm72140, /*on_true_vy=*/%v72141, /*on_false_vx=*/%v72139
%v72145 = vadd.f32 -3.0, %v72142
%v72149 = vsel /*vm=*/%vm72093, /*on_true_vy=*/%v72134, /*on_false_vx=*/%v72145
%v72153 = vmul.f32 %v72149, %v72130
%v72157 = vadd.f32 %v72153, %v72126
%v72161 = vmul.f32 %v72157, %v72149
%v72165 = vadd.f32 %v72161, %v72122
%v72169 = vmul.f32 %v72165, %v72149
%v72173 = vadd.f32 %v72169, %v72118
%v72177 = vmul.f32 %v72173, %v72149
%v72181 = vadd.f32 %v72177, %v72114
%v72185 = vmul.f32 %v72181, %v72149
%v72189 = vadd.f32 %v72185, %v72110
%v72193 = vmul.f32 %v72189, %v72149
%v72197 = vadd.f32 %v72193, %v72106
%v72201 = vmul.f32 %v72197, %v72149
%v72205 = vadd.f32 %v72201, %v72102
%v72209 = vmul.f32 %v72205, %v72149
%v72213 = vadd.f32 %v72209, %v72098
%v72217 = vmul.f32 %v72213, %v72064
%v72221 = vsel /*vm=*/%vm72069, /*on_true_vy=*/%v72074, /*on_false_vx=*/%v72217
%v72225 = vmul.f32 1.4140625, %v72221
%v72228 = vpack.c.bf16 %v120417, %v72225
%120131 = vst [vmem:[%s280 + $0xcc] sm:$0xf] /*vst_source=*/%v72228
%v72232 = vadd.s32 %v71307, %v1381
%v72242 = vadd.s32 %v72232, %v415
%vm72246 = vcmp.lt.u32.totalorder %v72242, %v72232
%vm72251 = vcmp.lt.u32.totalorder %v72232, %v1381
%v72256 = vadd.s32 %v71290, %v1368
%v72260 = vadd.s32 1, %v72256
%v72264 = vsel /*vm=*/%vm72251, /*on_true_vy=*/%v72260, /*on_false_vx=*/%v72256
%v72268 = vadd.s32 1, %v72264
%v72272 = vsel /*vm=*/%vm72246, /*on_true_vy=*/%v72268, /*on_false_vx=*/%v72264
%v72277 = vadd.s32 %v72272, %v10
%v72281 = vadd.s32 %v72242, %v9
%v72285 = vadd.s32 %v72281, %v72277
%v72287 = vshll.u32 %v72281, 13
%v72288 = vshrl.u32 %v72281, 19
%v72289 = vor.u32 %v72288, %v72287
%v72290 = vxor.u32 %v72289, %v72285
%v72293 = vadd.s32 %v72290, %v72285
%v72295 = vshll.u32 %v72290, 15
%v72296 = vshrl.u32 %v72290, 17
%v72297 = vor.u32 %v72296, %v72295
%v72298 = vxor.u32 %v72297, %v72293
%v72301 = vadd.s32 %v72298, %v72293
%v72303 = vshll.u32 %v72298, 26
%v72304 = vshrl.u32 %v72298, 6
%v72305 = vor.u32 %v72304, %v72303
%v72306 = vxor.u32 %v72305, %v72301
%v72309 = vadd.s32 %v72306, %v72301
%v72313 = vadd.s32 %v72309, %v9
%v72315 = vshll.u32 %v72306, 6
%v72316 = vshrl.u32 %v72306, 26
%v72317 = vor.u32 %v72316, %v72315
%v72318 = vxor.u32 %v72317, %v72309
%v72321 = vadd.s32 %v72318, %v8
%v72325 = vadd.s32 1, %v72321
%v72329 = vadd.s32 %v72325, %v72313
%v72331 = vshll.u32 %v72325, 17
%v72332 = vshrl.u32 %v72325, 15
%v72333 = vor.u32 %v72332, %v72331
%v72334 = vxor.u32 %v72333, %v72329
%v72337 = vadd.s32 %v72334, %v72329
%v72339 = vshll.u32 %v72334, 29
%v72340 = vshrl.u32 %v72334, 3
%v72341 = vor.u32 %v72340, %v72339
%v72342 = vxor.u32 %v72341, %v72337
%v72345 = vadd.s32 %v72342, %v72337
%v72347 = vshll.u32 %v72342, 16
%v72348 = vshrl.u32 %v72342, 16
%v72349 = vor.u32 %v72348, %v72347
%v72350 = vxor.u32 %v72349, %v72345
%v72353 = vadd.s32 %v72350, %v72345
%v72357 = vadd.s32 %v72353, %v8
%v72359 = vshll.u32 %v72350, 24
%v72360 = vshrl.u32 %v72350, 8
%v72361 = vor.u32 %v72360, %v72359
%v72362 = vxor.u32 %v72361, %v72353
%v72365 = vadd.s32 %v72362, %v10
%v72369 = vadd.s32 2, %v72365
%v72373 = vadd.s32 %v72369, %v72357
%v72375 = vshll.u32 %v72369, 13
%v72376 = vshrl.u32 %v72369, 19
%v72377 = vor.u32 %v72376, %v72375
%v72378 = vxor.u32 %v72377, %v72373
%v72381 = vadd.s32 %v72378, %v72373
%v72383 = vshll.u32 %v72378, 15
%v72384 = vshrl.u32 %v72378, 17
%v72385 = vor.u32 %v72384, %v72383
%v72386 = vxor.u32 %v72385, %v72381
%v72389 = vadd.s32 %v72386, %v72381
%v72391 = vshll.u32 %v72386, 26
%v72392 = vshrl.u32 %v72386, 6
%v72393 = vor.u32 %v72392, %v72391
%v72394 = vxor.u32 %v72393, %v72389
%v72397 = vadd.s32 %v72394, %v72389
%v72401 = vadd.s32 %v72397, %v10
%v72403 = vshll.u32 %v72394, 6
%v72404 = vshrl.u32 %v72394, 26
%v72405 = vor.u32 %v72404, %v72403
%v72406 = vxor.u32 %v72405, %v72397
%v72409 = vadd.s32 %v72406, %v9
%v72413 = vadd.s32 3, %v72409
%v72417 = vadd.s32 %v72413, %v72401
%v72419 = vshll.u32 %v72413, 17
%v72420 = vshrl.u32 %v72413, 15
%v72421 = vor.u32 %v72420, %v72419
%v72422 = vxor.u32 %v72421, %v72417
%v72425 = vadd.s32 %v72422, %v72417
%v72427 = vshll.u32 %v72422, 29
%v72428 = vshrl.u32 %v72422, 3
%v72429 = vor.u32 %v72428, %v72427
%v72430 = vxor.u32 %v72429, %v72425
%v72433 = vadd.s32 %v72430, %v72425
%v72435 = vshll.u32 %v72430, 16
%v72436 = vshrl.u32 %v72430, 16
%v72437 = vor.u32 %v72436, %v72435
%v72438 = vxor.u32 %v72437, %v72433
%v72441 = vadd.s32 %v72438, %v72433
%v72445 = vadd.s32 %v72441, %v9
%v72447 = vshll.u32 %v72438, 24
%v72448 = vshrl.u32 %v72438, 8
%v72449 = vor.u32 %v72448, %v72447
%v72450 = vxor.u32 %v72449, %v72441
%v72453 = vadd.s32 %v72450, %v8
%v72457 = vadd.s32 4, %v72453
%v72461 = vadd.s32 %v72457, %v72445
%v72463 = vshll.u32 %v72457, 13
%v72464 = vshrl.u32 %v72457, 19
%v72465 = vor.u32 %v72464, %v72463
%v72466 = vxor.u32 %v72465, %v72461
%v72469 = vadd.s32 %v72466, %v72461
%v72471 = vshll.u32 %v72466, 15
%v72472 = vshrl.u32 %v72466, 17
%v72473 = vor.u32 %v72472, %v72471
%v72474 = vxor.u32 %v72473, %v72469
%v72477 = vadd.s32 %v72474, %v72469
%v72479 = vshll.u32 %v72474, 26
%v72480 = vshrl.u32 %v72474, 6
%v72481 = vor.u32 %v72480, %v72479
%v72482 = vxor.u32 %v72481, %v72477
%v72485 = vadd.s32 %v72482, %v72477
%v72489 = vadd.s32 %v72485, %v8
%v72491 = vshll.u32 %v72482, 6
%v72492 = vshrl.u32 %v72482, 26
%v72493 = vor.u32 %v72492, %v72491
%v72494 = vxor.u32 %v72493, %v72485
%v72497 = vadd.s32 %v72494, %v10
%v72501 = vadd.s32 5, %v72497
%v72503 = vxor.u32 %v72501, %v72489
%v72504 = vand.u32.u8 255, %v72503
%v72505 = vand.u32 65535, %v72504
%v72506 = vshrl.u32 %v72505, 1
%v72507 = vor.u32 16256, %v72506
%v72508 = vand.u32.u16 65535, %v72507
%v120132 = vadd.low.f32.bf16 -1.0, %v72508
%v72517 = vmul.f32 2.0, %v120132
%v72521 = vadd.f32 -0.99609375, %v72517
%v72525 = vmax.f32 %v72521, -0.99609375
%v72527 = vand.u32 2147483647, %v72525
%vm72530 = vcmp.eq.f32.partialorder %v72527, 1.0
%v72535 = vmul.f32 inf, %v72525
%v72537 = vxor.u32 2147483648, %v72525
%v72540 = vmul.f32 %v72537, %v72525
%v72542 = vadd.f32 1.0, %v72540
%v72543 = vlog2.pop %v72542
%v72544 = vmul.f32 0.6931472, %v72543
%v72545 = vmul.f32 -0.5, %v72540
%v72546 = vadd.f32 1.0, %v72545
%v72547 = vmul.f32 %v72546, %v72540
%v72548 = vand.u32 2147483647, %v72540
%vm72549 = vcmp.lt.f32.partialorder %v72548, 0.0004427343
%v72550 = vsel /*vm=*/%vm72549, /*on_true_vy=*/%v72547, /*on_false_vx=*/%v72544
%v72551 = vxor.u32 2147483648, %v72550
%vm72554 = vcmp.lt.f32.partialorder %v72551, 5.0
%v72559 = vsel /*vm=*/%vm72554, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v72563 = vsel /*vm=*/%vm72554, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v72567 = vsel /*vm=*/%vm72554, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v72571 = vsel /*vm=*/%vm72554, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v72575 = vsel /*vm=*/%vm72554, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v72579 = vsel /*vm=*/%vm72554, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v72583 = vsel /*vm=*/%vm72554, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v72587 = vsel /*vm=*/%vm72554, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v72591 = vsel /*vm=*/%vm72554, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v72595 = vadd.f32 -2.5, %v72551
%v72597 = vrsqrt.pop %v72551
%v72598 = vmul.f32 %v72597, %v72551
%vm72599 = vcmp.eq.f32.partialorder %v72551, inf
%v72600 = vsel /*vm=*/%vm72599, /*on_true_vy=*/%v72551, /*on_false_vx=*/%v72598
%vm72601 = vcmp.eq.f32.partialorder %v72551, 0.0
%v72602 = vand.u32 2147483648, %v72551
%v72603 = vsel /*vm=*/%vm72601, /*on_true_vy=*/%v72602, /*on_false_vx=*/%v72600
%v72606 = vadd.f32 -3.0, %v72603
%v72610 = vsel /*vm=*/%vm72554, /*on_true_vy=*/%v72595, /*on_false_vx=*/%v72606
%v72614 = vmul.f32 %v72610, %v72591
%v72618 = vadd.f32 %v72614, %v72587
%v72622 = vmul.f32 %v72618, %v72610
%v72626 = vadd.f32 %v72622, %v72583
%v72630 = vmul.f32 %v72626, %v72610
%v72634 = vadd.f32 %v72630, %v72579
%v72638 = vmul.f32 %v72634, %v72610
%v72642 = vadd.f32 %v72638, %v72575
%v72646 = vmul.f32 %v72642, %v72610
%v72650 = vadd.f32 %v72646, %v72571
%v72654 = vmul.f32 %v72650, %v72610
%v72658 = vadd.f32 %v72654, %v72567
%v72662 = vmul.f32 %v72658, %v72610
%v72666 = vadd.f32 %v72662, %v72563
%v72670 = vmul.f32 %v72666, %v72610
%v72674 = vadd.f32 %v72670, %v72559
%v72678 = vmul.f32 %v72674, %v72525
%v72682 = vsel /*vm=*/%vm72530, /*on_true_vy=*/%v72535, /*on_false_vx=*/%v72678
%v72686 = vmul.f32 1.4140625, %v72682
%v72689 = vpack.c.bf16 %v120417, %v72686
%120133 = vst [vmem:[%s280 + $0x14c] sm:$0xf] /*vst_source=*/%v72689
%v72693 = vadd.s32 %v71307, %v1868
%v72703 = vadd.s32 %v72693, %v415
%vm72707 = vcmp.lt.u32.totalorder %v72703, %v72693
%vm72712 = vcmp.lt.u32.totalorder %v72693, %v1868
%v72717 = vadd.s32 %v71290, %v1855
%v72721 = vadd.s32 1, %v72717
%v72725 = vsel /*vm=*/%vm72712, /*on_true_vy=*/%v72721, /*on_false_vx=*/%v72717
%v72729 = vadd.s32 1, %v72725
%v72733 = vsel /*vm=*/%vm72707, /*on_true_vy=*/%v72729, /*on_false_vx=*/%v72725
%v72738 = vadd.s32 %v72733, %v10
%v72742 = vadd.s32 %v72703, %v9
%v72746 = vadd.s32 %v72742, %v72738
%v72748 = vshll.u32 %v72742, 13
%v72749 = vshrl.u32 %v72742, 19
%v72750 = vor.u32 %v72749, %v72748
%v72751 = vxor.u32 %v72750, %v72746
%v72754 = vadd.s32 %v72751, %v72746
%v72756 = vshll.u32 %v72751, 15
%v72757 = vshrl.u32 %v72751, 17
%v72758 = vor.u32 %v72757, %v72756
%v72759 = vxor.u32 %v72758, %v72754
%v72762 = vadd.s32 %v72759, %v72754
%v72764 = vshll.u32 %v72759, 26
%v72765 = vshrl.u32 %v72759, 6
%v72766 = vor.u32 %v72765, %v72764
%v72767 = vxor.u32 %v72766, %v72762
%v72770 = vadd.s32 %v72767, %v72762
%v72774 = vadd.s32 %v72770, %v9
%v72776 = vshll.u32 %v72767, 6
%v72777 = vshrl.u32 %v72767, 26
%v72778 = vor.u32 %v72777, %v72776
%v72779 = vxor.u32 %v72778, %v72770
%v72782 = vadd.s32 %v72779, %v8
%v72786 = vadd.s32 1, %v72782
%v72790 = vadd.s32 %v72786, %v72774
%v72792 = vshll.u32 %v72786, 17
%v72793 = vshrl.u32 %v72786, 15
%v72794 = vor.u32 %v72793, %v72792
%v72795 = vxor.u32 %v72794, %v72790
%v72798 = vadd.s32 %v72795, %v72790
%v72800 = vshll.u32 %v72795, 29
%v72801 = vshrl.u32 %v72795, 3
%v72802 = vor.u32 %v72801, %v72800
%v72803 = vxor.u32 %v72802, %v72798
%v72806 = vadd.s32 %v72803, %v72798
%v72808 = vshll.u32 %v72803, 16
%v72809 = vshrl.u32 %v72803, 16
%v72810 = vor.u32 %v72809, %v72808
%v72811 = vxor.u32 %v72810, %v72806
%v72814 = vadd.s32 %v72811, %v72806
%v72818 = vadd.s32 %v72814, %v8
%v72820 = vshll.u32 %v72811, 24
%v72821 = vshrl.u32 %v72811, 8
%v72822 = vor.u32 %v72821, %v72820
%v72823 = vxor.u32 %v72822, %v72814
%v72826 = vadd.s32 %v72823, %v10
%v72830 = vadd.s32 2, %v72826
%v72834 = vadd.s32 %v72830, %v72818
%v72836 = vshll.u32 %v72830, 13
%v72837 = vshrl.u32 %v72830, 19
%v72838 = vor.u32 %v72837, %v72836
%v72839 = vxor.u32 %v72838, %v72834
%v72842 = vadd.s32 %v72839, %v72834
%v72844 = vshll.u32 %v72839, 15
%v72845 = vshrl.u32 %v72839, 17
%v72846 = vor.u32 %v72845, %v72844
%v72847 = vxor.u32 %v72846, %v72842
%v72850 = vadd.s32 %v72847, %v72842
%v72852 = vshll.u32 %v72847, 26
%v72853 = vshrl.u32 %v72847, 6
%v72854 = vor.u32 %v72853, %v72852
%v72855 = vxor.u32 %v72854, %v72850
%v72858 = vadd.s32 %v72855, %v72850
%v72862 = vadd.s32 %v72858, %v10
%v72864 = vshll.u32 %v72855, 6
%v72865 = vshrl.u32 %v72855, 26
%v72866 = vor.u32 %v72865, %v72864
%v72867 = vxor.u32 %v72866, %v72858
%v72870 = vadd.s32 %v72867, %v9
%v72874 = vadd.s32 3, %v72870
%v72878 = vadd.s32 %v72874, %v72862
%v72880 = vshll.u32 %v72874, 17
%v72881 = vshrl.u32 %v72874, 15
%v72882 = vor.u32 %v72881, %v72880
%v72883 = vxor.u32 %v72882, %v72878
%v72886 = vadd.s32 %v72883, %v72878
%v72888 = vshll.u32 %v72883, 29
%v72889 = vshrl.u32 %v72883, 3
%v72890 = vor.u32 %v72889, %v72888
%v72891 = vxor.u32 %v72890, %v72886
%v72894 = vadd.s32 %v72891, %v72886
%v72896 = vshll.u32 %v72891, 16
%v72897 = vshrl.u32 %v72891, 16
%v72898 = vor.u32 %v72897, %v72896
%v72899 = vxor.u32 %v72898, %v72894
%v72902 = vadd.s32 %v72899, %v72894
%v72906 = vadd.s32 %v72902, %v9
%v72908 = vshll.u32 %v72899, 24
%v72909 = vshrl.u32 %v72899, 8
%v72910 = vor.u32 %v72909, %v72908
%v72911 = vxor.u32 %v72910, %v72902
%v72914 = vadd.s32 %v72911, %v8
%v72918 = vadd.s32 4, %v72914
%v72922 = vadd.s32 %v72918, %v72906
%v72924 = vshll.u32 %v72918, 13
%v72925 = vshrl.u32 %v72918, 19
%v72926 = vor.u32 %v72925, %v72924
%v72927 = vxor.u32 %v72926, %v72922
%v72930 = vadd.s32 %v72927, %v72922
%v72932 = vshll.u32 %v72927, 15
%v72933 = vshrl.u32 %v72927, 17
%v72934 = vor.u32 %v72933, %v72932
%v72935 = vxor.u32 %v72934, %v72930
%v72938 = vadd.s32 %v72935, %v72930
%v72940 = vshll.u32 %v72935, 26
%v72941 = vshrl.u32 %v72935, 6
%v72942 = vor.u32 %v72941, %v72940
%v72943 = vxor.u32 %v72942, %v72938
%v72946 = vadd.s32 %v72943, %v72938
%v72950 = vadd.s32 %v72946, %v8
%v72952 = vshll.u32 %v72943, 6
%v72953 = vshrl.u32 %v72943, 26
%v72954 = vor.u32 %v72953, %v72952
%v72955 = vxor.u32 %v72954, %v72946
%v72958 = vadd.s32 %v72955, %v10
%v72962 = vadd.s32 5, %v72958
%v72964 = vxor.u32 %v72962, %v72950
%v72965 = vand.u32.u8 255, %v72964
%v72966 = vand.u32 65535, %v72965
%v72967 = vshrl.u32 %v72966, 1
%v72968 = vor.u32 16256, %v72967
%v72969 = vand.u32.u16 65535, %v72968
%v120134 = vadd.low.f32.bf16 -1.0, %v72969
%v72978 = vmul.f32 2.0, %v120134
%v72982 = vadd.f32 -0.99609375, %v72978
%v72986 = vmax.f32 %v72982, -0.99609375
%v72988 = vand.u32 2147483647, %v72986
%vm72991 = vcmp.eq.f32.partialorder %v72988, 1.0
%v72996 = vmul.f32 inf, %v72986
%v72998 = vxor.u32 2147483648, %v72986
%v73001 = vmul.f32 %v72998, %v72986
%v73003 = vadd.f32 1.0, %v73001
%v73004 = vlog2.pop %v73003
%v73005 = vmul.f32 0.6931472, %v73004
%v73006 = vmul.f32 -0.5, %v73001
%v73007 = vadd.f32 1.0, %v73006
%v73008 = vmul.f32 %v73007, %v73001
%v73009 = vand.u32 2147483647, %v73001
%vm73010 = vcmp.lt.f32.partialorder %v73009, 0.0004427343
%v73011 = vsel /*vm=*/%vm73010, /*on_true_vy=*/%v73008, /*on_false_vx=*/%v73005
%v73012 = vxor.u32 2147483648, %v73011
%vm73015 = vcmp.lt.f32.partialorder %v73012, 5.0
%v73020 = vsel /*vm=*/%vm73015, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v73024 = vsel /*vm=*/%vm73015, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v73028 = vsel /*vm=*/%vm73015, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v73032 = vsel /*vm=*/%vm73015, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v73036 = vsel /*vm=*/%vm73015, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v73040 = vsel /*vm=*/%vm73015, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v73044 = vsel /*vm=*/%vm73015, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v73048 = vsel /*vm=*/%vm73015, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v73052 = vsel /*vm=*/%vm73015, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v73056 = vadd.f32 -2.5, %v73012
%v73058 = vrsqrt.pop %v73012
%v73059 = vmul.f32 %v73058, %v73012
%vm73060 = vcmp.eq.f32.partialorder %v73012, inf
%v73061 = vsel /*vm=*/%vm73060, /*on_true_vy=*/%v73012, /*on_false_vx=*/%v73059
%vm73062 = vcmp.eq.f32.partialorder %v73012, 0.0
%v73063 = vand.u32 2147483648, %v73012
%v73064 = vsel /*vm=*/%vm73062, /*on_true_vy=*/%v73063, /*on_false_vx=*/%v73061
%v73067 = vadd.f32 -3.0, %v73064
%v73071 = vsel /*vm=*/%vm73015, /*on_true_vy=*/%v73056, /*on_false_vx=*/%v73067
%v73075 = vmul.f32 %v73071, %v73052
%v73079 = vadd.f32 %v73075, %v73048
%v73083 = vmul.f32 %v73079, %v73071
%v73087 = vadd.f32 %v73083, %v73044
%v73091 = vmul.f32 %v73087, %v73071
%v73095 = vadd.f32 %v73091, %v73040
%v73099 = vmul.f32 %v73095, %v73071
%v73103 = vadd.f32 %v73099, %v73036
%v73107 = vmul.f32 %v73103, %v73071
%v73111 = vadd.f32 %v73107, %v73032
%v73115 = vmul.f32 %v73111, %v73071
%v73119 = vadd.f32 %v73115, %v73028
%v73123 = vmul.f32 %v73119, %v73071
%v73127 = vadd.f32 %v73123, %v73024
%v73131 = vmul.f32 %v73127, %v73071
%v73135 = vadd.f32 %v73131, %v73020
%v73139 = vmul.f32 %v73135, %v72986
%v73143 = vsel /*vm=*/%vm72991, /*on_true_vy=*/%v72996, /*on_false_vx=*/%v73139
%v73147 = vmul.f32 1.4140625, %v73143
%v73150 = vpack.c.bf16 %v120417, %v73147
%120135 = vst [vmem:[%s280 + $0x1cc] sm:$0xf] /*vst_source=*/%v73150
%v73154 = vadd.s32 %v71307, %v2355
%v73164 = vadd.s32 %v73154, %v415
%vm73168 = vcmp.lt.u32.totalorder %v73164, %v73154
%vm73173 = vcmp.lt.u32.totalorder %v73154, %v2355
%v73178 = vadd.s32 %v71290, %v2342
%v73182 = vadd.s32 1, %v73178
%v73186 = vsel /*vm=*/%vm73173, /*on_true_vy=*/%v73182, /*on_false_vx=*/%v73178
%v73190 = vadd.s32 1, %v73186
%v73194 = vsel /*vm=*/%vm73168, /*on_true_vy=*/%v73190, /*on_false_vx=*/%v73186
%v73199 = vadd.s32 %v73194, %v10
%v73203 = vadd.s32 %v73164, %v9
%v73207 = vadd.s32 %v73203, %v73199
%v73209 = vshll.u32 %v73203, 13
%v73210 = vshrl.u32 %v73203, 19
%v73211 = vor.u32 %v73210, %v73209
%v73212 = vxor.u32 %v73211, %v73207
%v73215 = vadd.s32 %v73212, %v73207
%v73217 = vshll.u32 %v73212, 15
%v73218 = vshrl.u32 %v73212, 17
%v73219 = vor.u32 %v73218, %v73217
%v73220 = vxor.u32 %v73219, %v73215
%v73223 = vadd.s32 %v73220, %v73215
%v73225 = vshll.u32 %v73220, 26
%v73226 = vshrl.u32 %v73220, 6
%v73227 = vor.u32 %v73226, %v73225
%v73228 = vxor.u32 %v73227, %v73223
%v73231 = vadd.s32 %v73228, %v73223
%v73235 = vadd.s32 %v73231, %v9
%v73237 = vshll.u32 %v73228, 6
%v73238 = vshrl.u32 %v73228, 26
%v73239 = vor.u32 %v73238, %v73237
%v73240 = vxor.u32 %v73239, %v73231
%v73243 = vadd.s32 %v73240, %v8
%v73247 = vadd.s32 1, %v73243
%v73251 = vadd.s32 %v73247, %v73235
%v73253 = vshll.u32 %v73247, 17
%v73254 = vshrl.u32 %v73247, 15
%v73255 = vor.u32 %v73254, %v73253
%v73256 = vxor.u32 %v73255, %v73251
%v73259 = vadd.s32 %v73256, %v73251
%v73261 = vshll.u32 %v73256, 29
%v73262 = vshrl.u32 %v73256, 3
%v73263 = vor.u32 %v73262, %v73261
%v73264 = vxor.u32 %v73263, %v73259
%v73267 = vadd.s32 %v73264, %v73259
%v73269 = vshll.u32 %v73264, 16
%v73270 = vshrl.u32 %v73264, 16
%v73271 = vor.u32 %v73270, %v73269
%v73272 = vxor.u32 %v73271, %v73267
%v73275 = vadd.s32 %v73272, %v73267
%v73279 = vadd.s32 %v73275, %v8
%v73281 = vshll.u32 %v73272, 24
%v73282 = vshrl.u32 %v73272, 8
%v73283 = vor.u32 %v73282, %v73281
%v73284 = vxor.u32 %v73283, %v73275
%v73287 = vadd.s32 %v73284, %v10
%v73291 = vadd.s32 2, %v73287
%v73295 = vadd.s32 %v73291, %v73279
%v73297 = vshll.u32 %v73291, 13
%v73298 = vshrl.u32 %v73291, 19
%v73299 = vor.u32 %v73298, %v73297
%v73300 = vxor.u32 %v73299, %v73295
%v73303 = vadd.s32 %v73300, %v73295
%v73305 = vshll.u32 %v73300, 15
%v73306 = vshrl.u32 %v73300, 17
%v73307 = vor.u32 %v73306, %v73305
%v73308 = vxor.u32 %v73307, %v73303
%v73311 = vadd.s32 %v73308, %v73303
%v73313 = vshll.u32 %v73308, 26
%v73314 = vshrl.u32 %v73308, 6
%v73315 = vor.u32 %v73314, %v73313
%v73316 = vxor.u32 %v73315, %v73311
%v73319 = vadd.s32 %v73316, %v73311
%v73323 = vadd.s32 %v73319, %v10
%v73325 = vshll.u32 %v73316, 6
%v73326 = vshrl.u32 %v73316, 26
%v73327 = vor.u32 %v73326, %v73325
%v73328 = vxor.u32 %v73327, %v73319
%v73331 = vadd.s32 %v73328, %v9
%v73335 = vadd.s32 3, %v73331
%v73339 = vadd.s32 %v73335, %v73323
%v73341 = vshll.u32 %v73335, 17
%v73342 = vshrl.u32 %v73335, 15
%v73343 = vor.u32 %v73342, %v73341
%v73344 = vxor.u32 %v73343, %v73339
%v73347 = vadd.s32 %v73344, %v73339
%v73349 = vshll.u32 %v73344, 29
%v73350 = vshrl.u32 %v73344, 3
%v73351 = vor.u32 %v73350, %v73349
%v73352 = vxor.u32 %v73351, %v73347
%v73355 = vadd.s32 %v73352, %v73347
%v73357 = vshll.u32 %v73352, 16
%v73358 = vshrl.u32 %v73352, 16
%v73359 = vor.u32 %v73358, %v73357
%v73360 = vxor.u32 %v73359, %v73355
%v73363 = vadd.s32 %v73360, %v73355
%v73367 = vadd.s32 %v73363, %v9
%v73369 = vshll.u32 %v73360, 24
%v73370 = vshrl.u32 %v73360, 8
%v73371 = vor.u32 %v73370, %v73369
%v73372 = vxor.u32 %v73371, %v73363
%v73375 = vadd.s32 %v73372, %v8
%v73379 = vadd.s32 4, %v73375
%v73383 = vadd.s32 %v73379, %v73367
%v73385 = vshll.u32 %v73379, 13
%v73386 = vshrl.u32 %v73379, 19
%v73387 = vor.u32 %v73386, %v73385
%v73388 = vxor.u32 %v73387, %v73383
%v73391 = vadd.s32 %v73388, %v73383
%v73393 = vshll.u32 %v73388, 15
%v73394 = vshrl.u32 %v73388, 17
%v73395 = vor.u32 %v73394, %v73393
%v73396 = vxor.u32 %v73395, %v73391
%v73399 = vadd.s32 %v73396, %v73391
%v73401 = vshll.u32 %v73396, 26
%v73402 = vshrl.u32 %v73396, 6
%v73403 = vor.u32 %v73402, %v73401
%v73404 = vxor.u32 %v73403, %v73399
%v73407 = vadd.s32 %v73404, %v73399
%v73411 = vadd.s32 %v73407, %v8
%v73413 = vshll.u32 %v73404, 6
%v73414 = vshrl.u32 %v73404, 26
%v73415 = vor.u32 %v73414, %v73413
%v73416 = vxor.u32 %v73415, %v73407
%v73419 = vadd.s32 %v73416, %v10
%v73423 = vadd.s32 5, %v73419
%v73425 = vxor.u32 %v73423, %v73411
%v73426 = vand.u32.u8 255, %v73425
%v73427 = vand.u32 65535, %v73426
%v73428 = vshrl.u32 %v73427, 1
%v73429 = vor.u32 16256, %v73428
%v73430 = vand.u32.u16 65535, %v73429
%v120136 = vadd.low.f32.bf16 -1.0, %v73430
%v73439 = vmul.f32 2.0, %v120136
%v73443 = vadd.f32 -0.99609375, %v73439
%v73447 = vmax.f32 %v73443, -0.99609375
%v73449 = vand.u32 2147483647, %v73447
%vm73452 = vcmp.eq.f32.partialorder %v73449, 1.0
%v73457 = vmul.f32 inf, %v73447
%v73459 = vxor.u32 2147483648, %v73447
%v73462 = vmul.f32 %v73459, %v73447
%v73464 = vadd.f32 1.0, %v73462
%v73465 = vlog2.pop %v73464
%v73466 = vmul.f32 0.6931472, %v73465
%v73467 = vmul.f32 -0.5, %v73462
%v73468 = vadd.f32 1.0, %v73467
%v73469 = vmul.f32 %v73468, %v73462
%v73470 = vand.u32 2147483647, %v73462
%vm73471 = vcmp.lt.f32.partialorder %v73470, 0.0004427343
%v73472 = vsel /*vm=*/%vm73471, /*on_true_vy=*/%v73469, /*on_false_vx=*/%v73466
%v73473 = vxor.u32 2147483648, %v73472
%vm73476 = vcmp.lt.f32.partialorder %v73473, 5.0
%v73481 = vsel /*vm=*/%vm73476, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v73485 = vsel /*vm=*/%vm73476, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v73489 = vsel /*vm=*/%vm73476, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v73493 = vsel /*vm=*/%vm73476, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v73497 = vsel /*vm=*/%vm73476, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v73501 = vsel /*vm=*/%vm73476, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v73505 = vsel /*vm=*/%vm73476, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v73509 = vsel /*vm=*/%vm73476, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v73513 = vsel /*vm=*/%vm73476, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v73517 = vadd.f32 -2.5, %v73473
%v73519 = vrsqrt.pop %v73473
%v73520 = vmul.f32 %v73519, %v73473
%vm73521 = vcmp.eq.f32.partialorder %v73473, inf
%v73522 = vsel /*vm=*/%vm73521, /*on_true_vy=*/%v73473, /*on_false_vx=*/%v73520
%vm73523 = vcmp.eq.f32.partialorder %v73473, 0.0
%v73524 = vand.u32 2147483648, %v73473
%v73525 = vsel /*vm=*/%vm73523, /*on_true_vy=*/%v73524, /*on_false_vx=*/%v73522
%v73528 = vadd.f32 -3.0, %v73525
%v73532 = vsel /*vm=*/%vm73476, /*on_true_vy=*/%v73517, /*on_false_vx=*/%v73528
%v73536 = vmul.f32 %v73532, %v73513
%v73540 = vadd.f32 %v73536, %v73509
%v73544 = vmul.f32 %v73540, %v73532
%v73548 = vadd.f32 %v73544, %v73505
%v73552 = vmul.f32 %v73548, %v73532
%v73556 = vadd.f32 %v73552, %v73501
%v73560 = vmul.f32 %v73556, %v73532
%v73564 = vadd.f32 %v73560, %v73497
%v73568 = vmul.f32 %v73564, %v73532
%v73572 = vadd.f32 %v73568, %v73493
%v73576 = vmul.f32 %v73572, %v73532
%v73580 = vadd.f32 %v73576, %v73489
%v73584 = vmul.f32 %v73580, %v73532
%v73588 = vadd.f32 %v73584, %v73485
%v73592 = vmul.f32 %v73588, %v73532
%v73596 = vadd.f32 %v73592, %v73481
%v73600 = vmul.f32 %v73596, %v73447
%v73604 = vsel /*vm=*/%vm73452, /*on_true_vy=*/%v73457, /*on_false_vx=*/%v73600
%v73608 = vmul.f32 1.4140625, %v73604
%v73611 = vpack.c.bf16 %v120417, %v73608
%120137 = vst [vmem:[%s280 + $0x24c] sm:$0xf] /*vst_source=*/%v73611
%v73615 = vadd.s32 %v71307, %v2842
%v73625 = vadd.s32 %v73615, %v415
%vm73629 = vcmp.lt.u32.totalorder %v73625, %v73615
%vm73634 = vcmp.lt.u32.totalorder %v73615, %v2842
%v73639 = vadd.s32 %v71290, %v2829
%v73643 = vadd.s32 1, %v73639
%v73647 = vsel /*vm=*/%vm73634, /*on_true_vy=*/%v73643, /*on_false_vx=*/%v73639
%v73651 = vadd.s32 1, %v73647
%v73655 = vsel /*vm=*/%vm73629, /*on_true_vy=*/%v73651, /*on_false_vx=*/%v73647
%v73660 = vadd.s32 %v73655, %v10
%v73664 = vadd.s32 %v73625, %v9
%v73668 = vadd.s32 %v73664, %v73660
%v73670 = vshll.u32 %v73664, 13
%v73671 = vshrl.u32 %v73664, 19
%v73672 = vor.u32 %v73671, %v73670
%v73673 = vxor.u32 %v73672, %v73668
%v73676 = vadd.s32 %v73673, %v73668
%v73678 = vshll.u32 %v73673, 15
%v73679 = vshrl.u32 %v73673, 17
%v73680 = vor.u32 %v73679, %v73678
%v73681 = vxor.u32 %v73680, %v73676
%v73684 = vadd.s32 %v73681, %v73676
%v73686 = vshll.u32 %v73681, 26
%v73687 = vshrl.u32 %v73681, 6
%v73688 = vor.u32 %v73687, %v73686
%v73689 = vxor.u32 %v73688, %v73684
%v73692 = vadd.s32 %v73689, %v73684
%v73696 = vadd.s32 %v73692, %v9
%v73698 = vshll.u32 %v73689, 6
%v73699 = vshrl.u32 %v73689, 26
%v73700 = vor.u32 %v73699, %v73698
%v73701 = vxor.u32 %v73700, %v73692
%v73704 = vadd.s32 %v73701, %v8
%v73708 = vadd.s32 1, %v73704
%v73712 = vadd.s32 %v73708, %v73696
%v73714 = vshll.u32 %v73708, 17
%v73715 = vshrl.u32 %v73708, 15
%v73716 = vor.u32 %v73715, %v73714
%v73717 = vxor.u32 %v73716, %v73712
%v73720 = vadd.s32 %v73717, %v73712
%v73722 = vshll.u32 %v73717, 29
%v73723 = vshrl.u32 %v73717, 3
%v73724 = vor.u32 %v73723, %v73722
%v73725 = vxor.u32 %v73724, %v73720
%v73728 = vadd.s32 %v73725, %v73720
%v73730 = vshll.u32 %v73725, 16
%v73731 = vshrl.u32 %v73725, 16
%v73732 = vor.u32 %v73731, %v73730
%v73733 = vxor.u32 %v73732, %v73728
%v73736 = vadd.s32 %v73733, %v73728
%v73740 = vadd.s32 %v73736, %v8
%v73742 = vshll.u32 %v73733, 24
%v73743 = vshrl.u32 %v73733, 8
%v73744 = vor.u32 %v73743, %v73742
%v73745 = vxor.u32 %v73744, %v73736
%v73748 = vadd.s32 %v73745, %v10
%v73752 = vadd.s32 2, %v73748
%v73756 = vadd.s32 %v73752, %v73740
%v73758 = vshll.u32 %v73752, 13
%v73759 = vshrl.u32 %v73752, 19
%v73760 = vor.u32 %v73759, %v73758
%v73761 = vxor.u32 %v73760, %v73756
%v73764 = vadd.s32 %v73761, %v73756
%v73766 = vshll.u32 %v73761, 15
%v73767 = vshrl.u32 %v73761, 17
%v73768 = vor.u32 %v73767, %v73766
%v73769 = vxor.u32 %v73768, %v73764
%v73772 = vadd.s32 %v73769, %v73764
%v73774 = vshll.u32 %v73769, 26
%v73775 = vshrl.u32 %v73769, 6
%v73776 = vor.u32 %v73775, %v73774
%v73777 = vxor.u32 %v73776, %v73772
%v73780 = vadd.s32 %v73777, %v73772
%v73784 = vadd.s32 %v73780, %v10
%v73786 = vshll.u32 %v73777, 6
%v73787 = vshrl.u32 %v73777, 26
%v73788 = vor.u32 %v73787, %v73786
%v73789 = vxor.u32 %v73788, %v73780
%v73792 = vadd.s32 %v73789, %v9
%v73796 = vadd.s32 3, %v73792
%v73800 = vadd.s32 %v73796, %v73784
%v73802 = vshll.u32 %v73796, 17
%v73803 = vshrl.u32 %v73796, 15
%v73804 = vor.u32 %v73803, %v73802
%v73805 = vxor.u32 %v73804, %v73800
%v73808 = vadd.s32 %v73805, %v73800
%v73810 = vshll.u32 %v73805, 29
%v73811 = vshrl.u32 %v73805, 3
%v73812 = vor.u32 %v73811, %v73810
%v73813 = vxor.u32 %v73812, %v73808
%v73816 = vadd.s32 %v73813, %v73808
%v73818 = vshll.u32 %v73813, 16
%v73819 = vshrl.u32 %v73813, 16
%v73820 = vor.u32 %v73819, %v73818
%v73821 = vxor.u32 %v73820, %v73816
%v73824 = vadd.s32 %v73821, %v73816
%v73828 = vadd.s32 %v73824, %v9
%v73830 = vshll.u32 %v73821, 24
%v73831 = vshrl.u32 %v73821, 8
%v73832 = vor.u32 %v73831, %v73830
%v73833 = vxor.u32 %v73832, %v73824
%v73836 = vadd.s32 %v73833, %v8
%v73840 = vadd.s32 4, %v73836
%v73844 = vadd.s32 %v73840, %v73828
%v73846 = vshll.u32 %v73840, 13
%v73847 = vshrl.u32 %v73840, 19
%v73848 = vor.u32 %v73847, %v73846
%v73849 = vxor.u32 %v73848, %v73844
%v73852 = vadd.s32 %v73849, %v73844
%v73854 = vshll.u32 %v73849, 15
%v73855 = vshrl.u32 %v73849, 17
%v73856 = vor.u32 %v73855, %v73854
%v73857 = vxor.u32 %v73856, %v73852
%v73860 = vadd.s32 %v73857, %v73852
%v73862 = vshll.u32 %v73857, 26
%v73863 = vshrl.u32 %v73857, 6
%v73864 = vor.u32 %v73863, %v73862
%v73865 = vxor.u32 %v73864, %v73860
%v73868 = vadd.s32 %v73865, %v73860
%v73872 = vadd.s32 %v73868, %v8
%v73874 = vshll.u32 %v73865, 6
%v73875 = vshrl.u32 %v73865, 26
%v73876 = vor.u32 %v73875, %v73874
%v73877 = vxor.u32 %v73876, %v73868
%v73880 = vadd.s32 %v73877, %v10
%v73884 = vadd.s32 5, %v73880
%v73886 = vxor.u32 %v73884, %v73872
%v73887 = vand.u32.u8 255, %v73886
%v73888 = vand.u32 65535, %v73887
%v73889 = vshrl.u32 %v73888, 1
%v73890 = vor.u32 16256, %v73889
%v73891 = vand.u32.u16 65535, %v73890
%v120138 = vadd.low.f32.bf16 -1.0, %v73891
%v73900 = vmul.f32 2.0, %v120138
%v73904 = vadd.f32 -0.99609375, %v73900
%v73908 = vmax.f32 %v73904, -0.99609375
%v73910 = vand.u32 2147483647, %v73908
%vm73913 = vcmp.eq.f32.partialorder %v73910, 1.0
%v73918 = vmul.f32 inf, %v73908
%v73920 = vxor.u32 2147483648, %v73908
%v73923 = vmul.f32 %v73920, %v73908
%v73925 = vadd.f32 1.0, %v73923
%v73926 = vlog2.pop %v73925
%v73927 = vmul.f32 0.6931472, %v73926
%v73928 = vmul.f32 -0.5, %v73923
%v73929 = vadd.f32 1.0, %v73928
%v73930 = vmul.f32 %v73929, %v73923
%v73931 = vand.u32 2147483647, %v73923
%vm73932 = vcmp.lt.f32.partialorder %v73931, 0.0004427343
%v73933 = vsel /*vm=*/%vm73932, /*on_true_vy=*/%v73930, /*on_false_vx=*/%v73927
%v73934 = vxor.u32 2147483648, %v73933
%vm73937 = vcmp.lt.f32.partialorder %v73934, 5.0
%v73942 = vsel /*vm=*/%vm73937, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v73946 = vsel /*vm=*/%vm73937, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v73950 = vsel /*vm=*/%vm73937, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v73954 = vsel /*vm=*/%vm73937, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v73958 = vsel /*vm=*/%vm73937, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v73962 = vsel /*vm=*/%vm73937, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v73966 = vsel /*vm=*/%vm73937, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v73970 = vsel /*vm=*/%vm73937, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v73974 = vsel /*vm=*/%vm73937, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v73978 = vadd.f32 -2.5, %v73934
%v73980 = vrsqrt.pop %v73934
%v73981 = vmul.f32 %v73980, %v73934
%vm73982 = vcmp.eq.f32.partialorder %v73934, inf
%v73983 = vsel /*vm=*/%vm73982, /*on_true_vy=*/%v73934, /*on_false_vx=*/%v73981
%vm73984 = vcmp.eq.f32.partialorder %v73934, 0.0
%v73985 = vand.u32 2147483648, %v73934
%v73986 = vsel /*vm=*/%vm73984, /*on_true_vy=*/%v73985, /*on_false_vx=*/%v73983
%v73989 = vadd.f32 -3.0, %v73986
%v73993 = vsel /*vm=*/%vm73937, /*on_true_vy=*/%v73978, /*on_false_vx=*/%v73989
%v73997 = vmul.f32 %v73993, %v73974
%v74001 = vadd.f32 %v73997, %v73970
%v74005 = vmul.f32 %v74001, %v73993
%v74009 = vadd.f32 %v74005, %v73966
%v74013 = vmul.f32 %v74009, %v73993
%v74017 = vadd.f32 %v74013, %v73962
%v74021 = vmul.f32 %v74017, %v73993
%v74025 = vadd.f32 %v74021, %v73958
%v74029 = vmul.f32 %v74025, %v73993
%v74033 = vadd.f32 %v74029, %v73954
%v74037 = vmul.f32 %v74033, %v73993
%v74041 = vadd.f32 %v74037, %v73950
%v74045 = vmul.f32 %v74041, %v73993
%v74049 = vadd.f32 %v74045, %v73946
%v74053 = vmul.f32 %v74049, %v73993
%v74057 = vadd.f32 %v74053, %v73942
%v74061 = vmul.f32 %v74057, %v73908
%v74065 = vsel /*vm=*/%vm73913, /*on_true_vy=*/%v73918, /*on_false_vx=*/%v74061
%v74069 = vmul.f32 1.4140625, %v74065
%v74072 = vpack.c.bf16 %v120417, %v74069
%120139 = vst [vmem:[%s280 + $0x2cc] sm:$0xf] /*vst_source=*/%v74072
%v74076 = vadd.s32 %v71307, %v3329
%v74086 = vadd.s32 %v74076, %v415
%vm74090 = vcmp.lt.u32.totalorder %v74086, %v74076
%vm74095 = vcmp.lt.u32.totalorder %v74076, %v3329
%v74100 = vadd.s32 %v71290, %v3316
%v74104 = vadd.s32 1, %v74100
%v74108 = vsel /*vm=*/%vm74095, /*on_true_vy=*/%v74104, /*on_false_vx=*/%v74100
%v74112 = vadd.s32 1, %v74108
%v74116 = vsel /*vm=*/%vm74090, /*on_true_vy=*/%v74112, /*on_false_vx=*/%v74108
%v74121 = vadd.s32 %v74116, %v10
%v74125 = vadd.s32 %v74086, %v9
%v74129 = vadd.s32 %v74125, %v74121
%v74131 = vshll.u32 %v74125, 13
%v74132 = vshrl.u32 %v74125, 19
%v74133 = vor.u32 %v74132, %v74131
%v74134 = vxor.u32 %v74133, %v74129
%v74137 = vadd.s32 %v74134, %v74129
%v74139 = vshll.u32 %v74134, 15
%v74140 = vshrl.u32 %v74134, 17
%v74141 = vor.u32 %v74140, %v74139
%v74142 = vxor.u32 %v74141, %v74137
%v74145 = vadd.s32 %v74142, %v74137
%v74147 = vshll.u32 %v74142, 26
%v74148 = vshrl.u32 %v74142, 6
%v74149 = vor.u32 %v74148, %v74147
%v74150 = vxor.u32 %v74149, %v74145
%v74153 = vadd.s32 %v74150, %v74145
%v74157 = vadd.s32 %v74153, %v9
%v74159 = vshll.u32 %v74150, 6
%v74160 = vshrl.u32 %v74150, 26
%v74161 = vor.u32 %v74160, %v74159
%v74162 = vxor.u32 %v74161, %v74153
%v74165 = vadd.s32 %v74162, %v8
%v74169 = vadd.s32 1, %v74165
%v74173 = vadd.s32 %v74169, %v74157
%v74175 = vshll.u32 %v74169, 17
%v74176 = vshrl.u32 %v74169, 15
%v74177 = vor.u32 %v74176, %v74175
%v74178 = vxor.u32 %v74177, %v74173
%v74181 = vadd.s32 %v74178, %v74173
%v74183 = vshll.u32 %v74178, 29
%v74184 = vshrl.u32 %v74178, 3
%v74185 = vor.u32 %v74184, %v74183
%v74186 = vxor.u32 %v74185, %v74181
%v74189 = vadd.s32 %v74186, %v74181
%v74191 = vshll.u32 %v74186, 16
%v74192 = vshrl.u32 %v74186, 16
%v74193 = vor.u32 %v74192, %v74191
%v74194 = vxor.u32 %v74193, %v74189
%v74197 = vadd.s32 %v74194, %v74189
%v74201 = vadd.s32 %v74197, %v8
%v74203 = vshll.u32 %v74194, 24
%v74204 = vshrl.u32 %v74194, 8
%v74205 = vor.u32 %v74204, %v74203
%v74206 = vxor.u32 %v74205, %v74197
%v74209 = vadd.s32 %v74206, %v10
%v74213 = vadd.s32 2, %v74209
%v74217 = vadd.s32 %v74213, %v74201
%v74219 = vshll.u32 %v74213, 13
%v74220 = vshrl.u32 %v74213, 19
%v74221 = vor.u32 %v74220, %v74219
%v74222 = vxor.u32 %v74221, %v74217
%v74225 = vadd.s32 %v74222, %v74217
%v74227 = vshll.u32 %v74222, 15
%v74228 = vshrl.u32 %v74222, 17
%v74229 = vor.u32 %v74228, %v74227
%v74230 = vxor.u32 %v74229, %v74225
%v74233 = vadd.s32 %v74230, %v74225
%v74235 = vshll.u32 %v74230, 26
%v74236 = vshrl.u32 %v74230, 6
%v74237 = vor.u32 %v74236, %v74235
%v74238 = vxor.u32 %v74237, %v74233
%v74241 = vadd.s32 %v74238, %v74233
%v74245 = vadd.s32 %v74241, %v10
%v74247 = vshll.u32 %v74238, 6
%v74248 = vshrl.u32 %v74238, 26
%v74249 = vor.u32 %v74248, %v74247
%v74250 = vxor.u32 %v74249, %v74241
%v74253 = vadd.s32 %v74250, %v9
%v74257 = vadd.s32 3, %v74253
%v74261 = vadd.s32 %v74257, %v74245
%v74263 = vshll.u32 %v74257, 17
%v74264 = vshrl.u32 %v74257, 15
%v74265 = vor.u32 %v74264, %v74263
%v74266 = vxor.u32 %v74265, %v74261
%v74269 = vadd.s32 %v74266, %v74261
%v74271 = vshll.u32 %v74266, 29
%v74272 = vshrl.u32 %v74266, 3
%v74273 = vor.u32 %v74272, %v74271
%v74274 = vxor.u32 %v74273, %v74269
%v74277 = vadd.s32 %v74274, %v74269
%v74279 = vshll.u32 %v74274, 16
%v74280 = vshrl.u32 %v74274, 16
%v74281 = vor.u32 %v74280, %v74279
%v74282 = vxor.u32 %v74281, %v74277
%v74285 = vadd.s32 %v74282, %v74277
%v74289 = vadd.s32 %v74285, %v9
%v74291 = vshll.u32 %v74282, 24
%v74292 = vshrl.u32 %v74282, 8
%v74293 = vor.u32 %v74292, %v74291
%v74294 = vxor.u32 %v74293, %v74285
%v74297 = vadd.s32 %v74294, %v8
%v74301 = vadd.s32 4, %v74297
%v74305 = vadd.s32 %v74301, %v74289
%v74307 = vshll.u32 %v74301, 13
%v74308 = vshrl.u32 %v74301, 19
%v74309 = vor.u32 %v74308, %v74307
%v74310 = vxor.u32 %v74309, %v74305
%v74313 = vadd.s32 %v74310, %v74305
%v74315 = vshll.u32 %v74310, 15
%v74316 = vshrl.u32 %v74310, 17
%v74317 = vor.u32 %v74316, %v74315
%v74318 = vxor.u32 %v74317, %v74313
%v74321 = vadd.s32 %v74318, %v74313
%v74323 = vshll.u32 %v74318, 26
%v74324 = vshrl.u32 %v74318, 6
%v74325 = vor.u32 %v74324, %v74323
%v74326 = vxor.u32 %v74325, %v74321
%v74329 = vadd.s32 %v74326, %v74321
%v74333 = vadd.s32 %v74329, %v8
%v74335 = vshll.u32 %v74326, 6
%v74336 = vshrl.u32 %v74326, 26
%v74337 = vor.u32 %v74336, %v74335
%v74338 = vxor.u32 %v74337, %v74329
%v74341 = vadd.s32 %v74338, %v10
%v74345 = vadd.s32 5, %v74341
%v74347 = vxor.u32 %v74345, %v74333
%v74348 = vand.u32.u8 255, %v74347
%v74349 = vand.u32 65535, %v74348
%v74350 = vshrl.u32 %v74349, 1
%v74351 = vor.u32 16256, %v74350
%v74352 = vand.u32.u16 65535, %v74351
%v120140 = vadd.low.f32.bf16 -1.0, %v74352
%v74361 = vmul.f32 2.0, %v120140
%v74365 = vadd.f32 -0.99609375, %v74361
%v74369 = vmax.f32 %v74365, -0.99609375
%v74371 = vand.u32 2147483647, %v74369
%vm74374 = vcmp.eq.f32.partialorder %v74371, 1.0
%v74379 = vmul.f32 inf, %v74369
%v74381 = vxor.u32 2147483648, %v74369
%v74384 = vmul.f32 %v74381, %v74369
%v74386 = vadd.f32 1.0, %v74384
%v74387 = vlog2.pop %v74386
%v74388 = vmul.f32 0.6931472, %v74387
%v74389 = vmul.f32 -0.5, %v74384
%v74390 = vadd.f32 1.0, %v74389
%v74391 = vmul.f32 %v74390, %v74384
%v74392 = vand.u32 2147483647, %v74384
%vm74393 = vcmp.lt.f32.partialorder %v74392, 0.0004427343
%v74394 = vsel /*vm=*/%vm74393, /*on_true_vy=*/%v74391, /*on_false_vx=*/%v74388
%v74395 = vxor.u32 2147483648, %v74394
%vm74398 = vcmp.lt.f32.partialorder %v74395, 5.0
%v74403 = vsel /*vm=*/%vm74398, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v74407 = vsel /*vm=*/%vm74398, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v74411 = vsel /*vm=*/%vm74398, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v74415 = vsel /*vm=*/%vm74398, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v74419 = vsel /*vm=*/%vm74398, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v74423 = vsel /*vm=*/%vm74398, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v74427 = vsel /*vm=*/%vm74398, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v74431 = vsel /*vm=*/%vm74398, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v74435 = vsel /*vm=*/%vm74398, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v74439 = vadd.f32 -2.5, %v74395
%v74441 = vrsqrt.pop %v74395
%v74442 = vmul.f32 %v74441, %v74395
%vm74443 = vcmp.eq.f32.partialorder %v74395, inf
%v74444 = vsel /*vm=*/%vm74443, /*on_true_vy=*/%v74395, /*on_false_vx=*/%v74442
%vm74445 = vcmp.eq.f32.partialorder %v74395, 0.0
%v74446 = vand.u32 2147483648, %v74395
%v74447 = vsel /*vm=*/%vm74445, /*on_true_vy=*/%v74446, /*on_false_vx=*/%v74444
%v74450 = vadd.f32 -3.0, %v74447
%v74454 = vsel /*vm=*/%vm74398, /*on_true_vy=*/%v74439, /*on_false_vx=*/%v74450
%v74458 = vmul.f32 %v74454, %v74435
%v74462 = vadd.f32 %v74458, %v74431
%v74466 = vmul.f32 %v74462, %v74454
%v74470 = vadd.f32 %v74466, %v74427
%v74474 = vmul.f32 %v74470, %v74454
%v74478 = vadd.f32 %v74474, %v74423
%v74482 = vmul.f32 %v74478, %v74454
%v74486 = vadd.f32 %v74482, %v74419
%v74490 = vmul.f32 %v74486, %v74454
%v74494 = vadd.f32 %v74490, %v74415
%v74498 = vmul.f32 %v74494, %v74454
%v74502 = vadd.f32 %v74498, %v74411
%v74506 = vmul.f32 %v74502, %v74454
%v74510 = vadd.f32 %v74506, %v74407
%v74514 = vmul.f32 %v74510, %v74454
%v74518 = vadd.f32 %v74514, %v74403
%v74522 = vmul.f32 %v74518, %v74369
%v74526 = vsel /*vm=*/%vm74374, /*on_true_vy=*/%v74379, /*on_false_vx=*/%v74522
%v74530 = vmul.f32 1.4140625, %v74526
%v74533 = vpack.c.bf16 %v120417, %v74530
%120141 = vst [vmem:[%s280 + $0x34c] sm:$0xf] /*vst_source=*/%v74533
%v74537 = vadd.s32 %v71307, %v3816
%v74547 = vadd.s32 %v74537, %v415
%vm74551 = vcmp.lt.u32.totalorder %v74547, %v74537
%vm74556 = vcmp.lt.u32.totalorder %v74537, %v3816
%v74561 = vadd.s32 %v71290, %v3803
%v74565 = vadd.s32 1, %v74561
%v74569 = vsel /*vm=*/%vm74556, /*on_true_vy=*/%v74565, /*on_false_vx=*/%v74561
%v74573 = vadd.s32 1, %v74569
%v74577 = vsel /*vm=*/%vm74551, /*on_true_vy=*/%v74573, /*on_false_vx=*/%v74569
%v74582 = vadd.s32 %v74577, %v10
%v74586 = vadd.s32 %v74547, %v9
%v74590 = vadd.s32 %v74586, %v74582
%v74592 = vshll.u32 %v74586, 13
%v74593 = vshrl.u32 %v74586, 19
%v74594 = vor.u32 %v74593, %v74592
%v74595 = vxor.u32 %v74594, %v74590
%v74598 = vadd.s32 %v74595, %v74590
%v74600 = vshll.u32 %v74595, 15
%v74601 = vshrl.u32 %v74595, 17
%v74602 = vor.u32 %v74601, %v74600
%v74603 = vxor.u32 %v74602, %v74598
%v74606 = vadd.s32 %v74603, %v74598
%v74608 = vshll.u32 %v74603, 26
%v74609 = vshrl.u32 %v74603, 6
%v74610 = vor.u32 %v74609, %v74608
%v74611 = vxor.u32 %v74610, %v74606
%v74614 = vadd.s32 %v74611, %v74606
%v74618 = vadd.s32 %v74614, %v9
%v74620 = vshll.u32 %v74611, 6
%v74621 = vshrl.u32 %v74611, 26
%v74622 = vor.u32 %v74621, %v74620
%v74623 = vxor.u32 %v74622, %v74614
%v74626 = vadd.s32 %v74623, %v8
%v74630 = vadd.s32 1, %v74626
%v74634 = vadd.s32 %v74630, %v74618
%v74636 = vshll.u32 %v74630, 17
%v74637 = vshrl.u32 %v74630, 15
%v74638 = vor.u32 %v74637, %v74636
%v74639 = vxor.u32 %v74638, %v74634
%v74642 = vadd.s32 %v74639, %v74634
%v74644 = vshll.u32 %v74639, 29
%v74645 = vshrl.u32 %v74639, 3
%v74646 = vor.u32 %v74645, %v74644
%v74647 = vxor.u32 %v74646, %v74642
%v74650 = vadd.s32 %v74647, %v74642
%v74652 = vshll.u32 %v74647, 16
%v74653 = vshrl.u32 %v74647, 16
%v74654 = vor.u32 %v74653, %v74652
%v74655 = vxor.u32 %v74654, %v74650
%v74658 = vadd.s32 %v74655, %v74650
%v74662 = vadd.s32 %v74658, %v8
%v74664 = vshll.u32 %v74655, 24
%v74665 = vshrl.u32 %v74655, 8
%v74666 = vor.u32 %v74665, %v74664
%v74667 = vxor.u32 %v74666, %v74658
%v74670 = vadd.s32 %v74667, %v10
%v74674 = vadd.s32 2, %v74670
%v74678 = vadd.s32 %v74674, %v74662
%v74680 = vshll.u32 %v74674, 13
%v74681 = vshrl.u32 %v74674, 19
%v74682 = vor.u32 %v74681, %v74680
%v74683 = vxor.u32 %v74682, %v74678
%v74686 = vadd.s32 %v74683, %v74678
%v74688 = vshll.u32 %v74683, 15
%v74689 = vshrl.u32 %v74683, 17
%v74690 = vor.u32 %v74689, %v74688
%v74691 = vxor.u32 %v74690, %v74686
%v74694 = vadd.s32 %v74691, %v74686
%v74696 = vshll.u32 %v74691, 26
%v74697 = vshrl.u32 %v74691, 6
%v74698 = vor.u32 %v74697, %v74696
%v74699 = vxor.u32 %v74698, %v74694
%v74702 = vadd.s32 %v74699, %v74694
%v74706 = vadd.s32 %v74702, %v10
%v74708 = vshll.u32 %v74699, 6
%v74709 = vshrl.u32 %v74699, 26
%v74710 = vor.u32 %v74709, %v74708
%v74711 = vxor.u32 %v74710, %v74702
%v74714 = vadd.s32 %v74711, %v9
%v74718 = vadd.s32 3, %v74714
%v74722 = vadd.s32 %v74718, %v74706
%v74724 = vshll.u32 %v74718, 17
%v74725 = vshrl.u32 %v74718, 15
%v74726 = vor.u32 %v74725, %v74724
%v74727 = vxor.u32 %v74726, %v74722
%v74730 = vadd.s32 %v74727, %v74722
%v74732 = vshll.u32 %v74727, 29
%v74733 = vshrl.u32 %v74727, 3
%v74734 = vor.u32 %v74733, %v74732
%v74735 = vxor.u32 %v74734, %v74730
%v74738 = vadd.s32 %v74735, %v74730
%v74740 = vshll.u32 %v74735, 16
%v74741 = vshrl.u32 %v74735, 16
%v74742 = vor.u32 %v74741, %v74740
%v74743 = vxor.u32 %v74742, %v74738
%v74746 = vadd.s32 %v74743, %v74738
%v74750 = vadd.s32 %v74746, %v9
%v74752 = vshll.u32 %v74743, 24
%v74753 = vshrl.u32 %v74743, 8
%v74754 = vor.u32 %v74753, %v74752
%v74755 = vxor.u32 %v74754, %v74746
%v74758 = vadd.s32 %v74755, %v8
%v74762 = vadd.s32 4, %v74758
%v74766 = vadd.s32 %v74762, %v74750
%v74768 = vshll.u32 %v74762, 13
%v74769 = vshrl.u32 %v74762, 19
%v74770 = vor.u32 %v74769, %v74768
%v74771 = vxor.u32 %v74770, %v74766
%v74774 = vadd.s32 %v74771, %v74766
%v74776 = vshll.u32 %v74771, 15
%v74777 = vshrl.u32 %v74771, 17
%v74778 = vor.u32 %v74777, %v74776
%v74779 = vxor.u32 %v74778, %v74774
%v74782 = vadd.s32 %v74779, %v74774
%v74784 = vshll.u32 %v74779, 26
%v74785 = vshrl.u32 %v74779, 6
%v74786 = vor.u32 %v74785, %v74784
%v74787 = vxor.u32 %v74786, %v74782
%v74790 = vadd.s32 %v74787, %v74782
%v74794 = vadd.s32 %v74790, %v8
%v74796 = vshll.u32 %v74787, 6
%v74797 = vshrl.u32 %v74787, 26
%v74798 = vor.u32 %v74797, %v74796
%v74799 = vxor.u32 %v74798, %v74790
%v74802 = vadd.s32 %v74799, %v10
%v74806 = vadd.s32 5, %v74802
%v74808 = vxor.u32 %v74806, %v74794
%v74809 = vand.u32.u8 255, %v74808
%v74810 = vand.u32 65535, %v74809
%v74811 = vshrl.u32 %v74810, 1
%v74812 = vor.u32 16256, %v74811
%v74813 = vand.u32.u16 65535, %v74812
%v120142 = vadd.low.f32.bf16 -1.0, %v74813
%v74822 = vmul.f32 2.0, %v120142
%v74826 = vadd.f32 -0.99609375, %v74822
%v74830 = vmax.f32 %v74826, -0.99609375
%v74832 = vand.u32 2147483647, %v74830
%vm74835 = vcmp.eq.f32.partialorder %v74832, 1.0
%v74840 = vmul.f32 inf, %v74830
%v74842 = vxor.u32 2147483648, %v74830
%v74845 = vmul.f32 %v74842, %v74830
%v74847 = vadd.f32 1.0, %v74845
%v74848 = vlog2.pop %v74847
%v74849 = vmul.f32 0.6931472, %v74848
%v74850 = vmul.f32 -0.5, %v74845
%v74851 = vadd.f32 1.0, %v74850
%v74852 = vmul.f32 %v74851, %v74845
%v74853 = vand.u32 2147483647, %v74845
%vm74854 = vcmp.lt.f32.partialorder %v74853, 0.0004427343
%v74855 = vsel /*vm=*/%vm74854, /*on_true_vy=*/%v74852, /*on_false_vx=*/%v74849
%v74856 = vxor.u32 2147483648, %v74855
%vm74859 = vcmp.lt.f32.partialorder %v74856, 5.0
%v74864 = vsel /*vm=*/%vm74859, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v74868 = vsel /*vm=*/%vm74859, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v74872 = vsel /*vm=*/%vm74859, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v74876 = vsel /*vm=*/%vm74859, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v74880 = vsel /*vm=*/%vm74859, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v74884 = vsel /*vm=*/%vm74859, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v74888 = vsel /*vm=*/%vm74859, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v74892 = vsel /*vm=*/%vm74859, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v74896 = vsel /*vm=*/%vm74859, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v74900 = vadd.f32 -2.5, %v74856
%v74902 = vrsqrt.pop %v74856
%v74903 = vmul.f32 %v74902, %v74856
%vm74904 = vcmp.eq.f32.partialorder %v74856, inf
%v74905 = vsel /*vm=*/%vm74904, /*on_true_vy=*/%v74856, /*on_false_vx=*/%v74903
%vm74906 = vcmp.eq.f32.partialorder %v74856, 0.0
%v74907 = vand.u32 2147483648, %v74856
%v74908 = vsel /*vm=*/%vm74906, /*on_true_vy=*/%v74907, /*on_false_vx=*/%v74905
%v74911 = vadd.f32 -3.0, %v74908
%v74915 = vsel /*vm=*/%vm74859, /*on_true_vy=*/%v74900, /*on_false_vx=*/%v74911
%v74919 = vmul.f32 %v74915, %v74896
%v74923 = vadd.f32 %v74919, %v74892
%v74927 = vmul.f32 %v74923, %v74915
%v74931 = vadd.f32 %v74927, %v74888
%v74935 = vmul.f32 %v74931, %v74915
%v74939 = vadd.f32 %v74935, %v74884
%v74943 = vmul.f32 %v74939, %v74915
%v74947 = vadd.f32 %v74943, %v74880
%v74951 = vmul.f32 %v74947, %v74915
%v74955 = vadd.f32 %v74951, %v74876
%v74959 = vmul.f32 %v74955, %v74915
%v74963 = vadd.f32 %v74959, %v74872
%v74967 = vmul.f32 %v74963, %v74915
%v74971 = vadd.f32 %v74967, %v74868
%v74975 = vmul.f32 %v74971, %v74915
%v74979 = vadd.f32 %v74975, %v74864
%v74983 = vmul.f32 %v74979, %v74830
%v74987 = vsel /*vm=*/%vm74835, /*on_true_vy=*/%v74840, /*on_false_vx=*/%v74983
%v74991 = vmul.f32 1.4140625, %v74987
%v74994 = vpack.c.bf16 %v120417, %v74991
%120143 = vst [vmem:[%s280 + $0x3cc] sm:$0xf] /*vst_source=*/%v74994
%v75032 = vadd.s32 %v75029, %v408
%v75042 = vadd.s32 %v75032, %v415
%vm75046 = vcmp.lt.u32.totalorder %v75042, %v75032
%vm75051 = vcmp.lt.u32.totalorder %v75032, %v408
%v75056 = vadd.s32 %v75012, %v380
%v75060 = vadd.s32 1, %v75056
%v75064 = vsel /*vm=*/%vm75051, /*on_true_vy=*/%v75060, /*on_false_vx=*/%v75056
%v75068 = vadd.s32 1, %v75064
%v75072 = vsel /*vm=*/%vm75046, /*on_true_vy=*/%v75068, /*on_false_vx=*/%v75064
%v75077 = vadd.s32 %v75072, %v10
%v75081 = vadd.s32 %v75042, %v9
%v75085 = vadd.s32 %v75081, %v75077
%v75087 = vshll.u32 %v75081, 13
%v75088 = vshrl.u32 %v75081, 19
%v75089 = vor.u32 %v75088, %v75087
%v75090 = vxor.u32 %v75089, %v75085
%v75093 = vadd.s32 %v75090, %v75085
%v75095 = vshll.u32 %v75090, 15
%v75096 = vshrl.u32 %v75090, 17
%v75097 = vor.u32 %v75096, %v75095
%v75098 = vxor.u32 %v75097, %v75093
%v75101 = vadd.s32 %v75098, %v75093
%v75103 = vshll.u32 %v75098, 26
%v75104 = vshrl.u32 %v75098, 6
%v75105 = vor.u32 %v75104, %v75103
%v75106 = vxor.u32 %v75105, %v75101
%v75109 = vadd.s32 %v75106, %v75101
%v75113 = vadd.s32 %v75109, %v9
%v75115 = vshll.u32 %v75106, 6
%v75116 = vshrl.u32 %v75106, 26
%v75117 = vor.u32 %v75116, %v75115
%v75118 = vxor.u32 %v75117, %v75109
%v75121 = vadd.s32 %v75118, %v8
%v75125 = vadd.s32 1, %v75121
%v75129 = vadd.s32 %v75125, %v75113
%v75131 = vshll.u32 %v75125, 17
%v75132 = vshrl.u32 %v75125, 15
%v75133 = vor.u32 %v75132, %v75131
%v75134 = vxor.u32 %v75133, %v75129
%v75137 = vadd.s32 %v75134, %v75129
%v75139 = vshll.u32 %v75134, 29
%v75140 = vshrl.u32 %v75134, 3
%v75141 = vor.u32 %v75140, %v75139
%v75142 = vxor.u32 %v75141, %v75137
%v75145 = vadd.s32 %v75142, %v75137
%v75147 = vshll.u32 %v75142, 16
%v75148 = vshrl.u32 %v75142, 16
%v75149 = vor.u32 %v75148, %v75147
%v75150 = vxor.u32 %v75149, %v75145
%v75153 = vadd.s32 %v75150, %v75145
%v75157 = vadd.s32 %v75153, %v8
%v75159 = vshll.u32 %v75150, 24
%v75160 = vshrl.u32 %v75150, 8
%v75161 = vor.u32 %v75160, %v75159
%v75162 = vxor.u32 %v75161, %v75153
%v75165 = vadd.s32 %v75162, %v10
%v75169 = vadd.s32 2, %v75165
%v75173 = vadd.s32 %v75169, %v75157
%v75175 = vshll.u32 %v75169, 13
%v75176 = vshrl.u32 %v75169, 19
%v75177 = vor.u32 %v75176, %v75175
%v75178 = vxor.u32 %v75177, %v75173
%v75181 = vadd.s32 %v75178, %v75173
%v75183 = vshll.u32 %v75178, 15
%v75184 = vshrl.u32 %v75178, 17
%v75185 = vor.u32 %v75184, %v75183
%v75186 = vxor.u32 %v75185, %v75181
%v75189 = vadd.s32 %v75186, %v75181
%v75191 = vshll.u32 %v75186, 26
%v75192 = vshrl.u32 %v75186, 6
%v75193 = vor.u32 %v75192, %v75191
%v75194 = vxor.u32 %v75193, %v75189
%v75197 = vadd.s32 %v75194, %v75189
%v75201 = vadd.s32 %v75197, %v10
%v75203 = vshll.u32 %v75194, 6
%v75204 = vshrl.u32 %v75194, 26
%v75205 = vor.u32 %v75204, %v75203
%v75206 = vxor.u32 %v75205, %v75197
%v75209 = vadd.s32 %v75206, %v9
%v75213 = vadd.s32 3, %v75209
%v75217 = vadd.s32 %v75213, %v75201
%v75219 = vshll.u32 %v75213, 17
%v75220 = vshrl.u32 %v75213, 15
%v75221 = vor.u32 %v75220, %v75219
%v75222 = vxor.u32 %v75221, %v75217
%v75225 = vadd.s32 %v75222, %v75217
%v75227 = vshll.u32 %v75222, 29
%v75228 = vshrl.u32 %v75222, 3
%v75229 = vor.u32 %v75228, %v75227
%v75230 = vxor.u32 %v75229, %v75225
%v75233 = vadd.s32 %v75230, %v75225
%v75235 = vshll.u32 %v75230, 16
%v75236 = vshrl.u32 %v75230, 16
%v75237 = vor.u32 %v75236, %v75235
%v75238 = vxor.u32 %v75237, %v75233
%v75241 = vadd.s32 %v75238, %v75233
%v75245 = vadd.s32 %v75241, %v9
%v75247 = vshll.u32 %v75238, 24
%v75248 = vshrl.u32 %v75238, 8
%v75249 = vor.u32 %v75248, %v75247
%v75250 = vxor.u32 %v75249, %v75241
%v75253 = vadd.s32 %v75250, %v8
%v75257 = vadd.s32 4, %v75253
%v75261 = vadd.s32 %v75257, %v75245
%v75263 = vshll.u32 %v75257, 13
%v75264 = vshrl.u32 %v75257, 19
%v75265 = vor.u32 %v75264, %v75263
%v75266 = vxor.u32 %v75265, %v75261
%v75269 = vadd.s32 %v75266, %v75261
%v75271 = vshll.u32 %v75266, 15
%v75272 = vshrl.u32 %v75266, 17
%v75273 = vor.u32 %v75272, %v75271
%v75274 = vxor.u32 %v75273, %v75269
%v75277 = vadd.s32 %v75274, %v75269
%v75279 = vshll.u32 %v75274, 26
%v75280 = vshrl.u32 %v75274, 6
%v75281 = vor.u32 %v75280, %v75279
%v75282 = vxor.u32 %v75281, %v75277
%v75285 = vadd.s32 %v75282, %v75277
%v75289 = vadd.s32 %v75285, %v8
%v75291 = vshll.u32 %v75282, 6
%v75292 = vshrl.u32 %v75282, 26
%v75293 = vor.u32 %v75292, %v75291
%v75294 = vxor.u32 %v75293, %v75285
%v75297 = vadd.s32 %v75294, %v10
%v75301 = vadd.s32 5, %v75297
%v75303 = vxor.u32 %v75301, %v75289
%v75304 = vand.u32.u8 255, %v75303
%v75305 = vand.u32 65535, %v75304
%v75306 = vshrl.u32 %v75305, 1
%v75307 = vor.u32 16256, %v75306
%v75308 = vand.u32.u16 65535, %v75307
%v120148 = vadd.low.f32.bf16 -1.0, %v75308
%v75317 = vmul.f32 2.0, %v120148
%v75321 = vadd.f32 -0.99609375, %v75317
%v75325 = vmax.f32 %v75321, -0.99609375
%v75327 = vand.u32 2147483647, %v75325
%vm75330 = vcmp.eq.f32.partialorder %v75327, 1.0
%v75335 = vmul.f32 inf, %v75325
%v75337 = vxor.u32 2147483648, %v75325
%v75340 = vmul.f32 %v75337, %v75325
%v75342 = vadd.f32 1.0, %v75340
%v75343 = vlog2.pop %v75342
%v75344 = vmul.f32 0.6931472, %v75343
%v75345 = vmul.f32 -0.5, %v75340
%v75346 = vadd.f32 1.0, %v75345
%v75347 = vmul.f32 %v75346, %v75340
%v75348 = vand.u32 2147483647, %v75340
%vm75349 = vcmp.lt.f32.partialorder %v75348, 0.0004427343
%v75350 = vsel /*vm=*/%vm75349, /*on_true_vy=*/%v75347, /*on_false_vx=*/%v75344
%v75351 = vxor.u32 2147483648, %v75350
%vm75354 = vcmp.lt.f32.partialorder %v75351, 5.0
%v75359 = vsel /*vm=*/%vm75354, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v75363 = vsel /*vm=*/%vm75354, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v75367 = vsel /*vm=*/%vm75354, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v75371 = vsel /*vm=*/%vm75354, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v75375 = vsel /*vm=*/%vm75354, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v75379 = vsel /*vm=*/%vm75354, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v75383 = vsel /*vm=*/%vm75354, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v75387 = vsel /*vm=*/%vm75354, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v75391 = vsel /*vm=*/%vm75354, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v75395 = vadd.f32 -2.5, %v75351
%v75397 = vrsqrt.pop %v75351
%v75398 = vmul.f32 %v75397, %v75351
%vm75399 = vcmp.eq.f32.partialorder %v75351, inf
%v75400 = vsel /*vm=*/%vm75399, /*on_true_vy=*/%v75351, /*on_false_vx=*/%v75398
%vm75401 = vcmp.eq.f32.partialorder %v75351, 0.0
%v75402 = vand.u32 2147483648, %v75351
%v75403 = vsel /*vm=*/%vm75401, /*on_true_vy=*/%v75402, /*on_false_vx=*/%v75400
%v75406 = vadd.f32 -3.0, %v75403
%v75410 = vsel /*vm=*/%vm75354, /*on_true_vy=*/%v75395, /*on_false_vx=*/%v75406
%v75414 = vmul.f32 %v75410, %v75391
%v75418 = vadd.f32 %v75414, %v75387
%v75422 = vmul.f32 %v75418, %v75410
%v75426 = vadd.f32 %v75422, %v75383
%v75430 = vmul.f32 %v75426, %v75410
%v75434 = vadd.f32 %v75430, %v75379
%v75438 = vmul.f32 %v75434, %v75410
%v75442 = vadd.f32 %v75438, %v75375
%v75446 = vmul.f32 %v75442, %v75410
%v75450 = vadd.f32 %v75446, %v75371
%v75454 = vmul.f32 %v75450, %v75410
%v75458 = vadd.f32 %v75454, %v75367
%v75462 = vmul.f32 %v75458, %v75410
%v75466 = vadd.f32 %v75462, %v75363
%v75470 = vmul.f32 %v75466, %v75410
%v75474 = vadd.f32 %v75470, %v75359
%v75478 = vmul.f32 %v75474, %v75325
%v75482 = vsel /*vm=*/%vm75330, /*on_true_vy=*/%v75335, /*on_false_vx=*/%v75478
%v75486 = vmul.f32 1.4140625, %v75482
%v75489 = vpack.c.bf16 %v120417, %v75486
%120149 = vst [vmem:[%s280 + $0x50] sm:$0xf] /*vst_source=*/%v75489
%v75493 = vadd.s32 %v75029, %v894
%v75503 = vadd.s32 %v75493, %v415
%vm75507 = vcmp.lt.u32.totalorder %v75503, %v75493
%vm75512 = vcmp.lt.u32.totalorder %v75493, %v894
%v75517 = vadd.s32 %v75012, %v881
%v75521 = vadd.s32 1, %v75517
%v75525 = vsel /*vm=*/%vm75512, /*on_true_vy=*/%v75521, /*on_false_vx=*/%v75517
%v75529 = vadd.s32 1, %v75525
%v75533 = vsel /*vm=*/%vm75507, /*on_true_vy=*/%v75529, /*on_false_vx=*/%v75525
%v75538 = vadd.s32 %v75533, %v10
%v75542 = vadd.s32 %v75503, %v9
%v75546 = vadd.s32 %v75542, %v75538
%v75548 = vshll.u32 %v75542, 13
%v75549 = vshrl.u32 %v75542, 19
%v75550 = vor.u32 %v75549, %v75548
%v75551 = vxor.u32 %v75550, %v75546
%v75554 = vadd.s32 %v75551, %v75546
%v75556 = vshll.u32 %v75551, 15
%v75557 = vshrl.u32 %v75551, 17
%v75558 = vor.u32 %v75557, %v75556
%v75559 = vxor.u32 %v75558, %v75554
%v75562 = vadd.s32 %v75559, %v75554
%v75564 = vshll.u32 %v75559, 26
%v75565 = vshrl.u32 %v75559, 6
%v75566 = vor.u32 %v75565, %v75564
%v75567 = vxor.u32 %v75566, %v75562
%v75570 = vadd.s32 %v75567, %v75562
%v75574 = vadd.s32 %v75570, %v9
%v75576 = vshll.u32 %v75567, 6
%v75577 = vshrl.u32 %v75567, 26
%v75578 = vor.u32 %v75577, %v75576
%v75579 = vxor.u32 %v75578, %v75570
%v75582 = vadd.s32 %v75579, %v8
%v75586 = vadd.s32 1, %v75582
%v75590 = vadd.s32 %v75586, %v75574
%v75592 = vshll.u32 %v75586, 17
%v75593 = vshrl.u32 %v75586, 15
%v75594 = vor.u32 %v75593, %v75592
%v75595 = vxor.u32 %v75594, %v75590
%v75598 = vadd.s32 %v75595, %v75590
%v75600 = vshll.u32 %v75595, 29
%v75601 = vshrl.u32 %v75595, 3
%v75602 = vor.u32 %v75601, %v75600
%v75603 = vxor.u32 %v75602, %v75598
%v75606 = vadd.s32 %v75603, %v75598
%v75608 = vshll.u32 %v75603, 16
%v75609 = vshrl.u32 %v75603, 16
%v75610 = vor.u32 %v75609, %v75608
%v75611 = vxor.u32 %v75610, %v75606
%v75614 = vadd.s32 %v75611, %v75606
%v75618 = vadd.s32 %v75614, %v8
%v75620 = vshll.u32 %v75611, 24
%v75621 = vshrl.u32 %v75611, 8
%v75622 = vor.u32 %v75621, %v75620
%v75623 = vxor.u32 %v75622, %v75614
%v75626 = vadd.s32 %v75623, %v10
%v75630 = vadd.s32 2, %v75626
%v75634 = vadd.s32 %v75630, %v75618
%v75636 = vshll.u32 %v75630, 13
%v75637 = vshrl.u32 %v75630, 19
%v75638 = vor.u32 %v75637, %v75636
%v75639 = vxor.u32 %v75638, %v75634
%v75642 = vadd.s32 %v75639, %v75634
%v75644 = vshll.u32 %v75639, 15
%v75645 = vshrl.u32 %v75639, 17
%v75646 = vor.u32 %v75645, %v75644
%v75647 = vxor.u32 %v75646, %v75642
%v75650 = vadd.s32 %v75647, %v75642
%v75652 = vshll.u32 %v75647, 26
%v75653 = vshrl.u32 %v75647, 6
%v75654 = vor.u32 %v75653, %v75652
%v75655 = vxor.u32 %v75654, %v75650
%v75658 = vadd.s32 %v75655, %v75650
%v75662 = vadd.s32 %v75658, %v10
%v75664 = vshll.u32 %v75655, 6
%v75665 = vshrl.u32 %v75655, 26
%v75666 = vor.u32 %v75665, %v75664
%v75667 = vxor.u32 %v75666, %v75658
%v75670 = vadd.s32 %v75667, %v9
%v75674 = vadd.s32 3, %v75670
%v75678 = vadd.s32 %v75674, %v75662
%v75680 = vshll.u32 %v75674, 17
%v75681 = vshrl.u32 %v75674, 15
%v75682 = vor.u32 %v75681, %v75680
%v75683 = vxor.u32 %v75682, %v75678
%v75686 = vadd.s32 %v75683, %v75678
%v75688 = vshll.u32 %v75683, 29
%v75689 = vshrl.u32 %v75683, 3
%v75690 = vor.u32 %v75689, %v75688
%v75691 = vxor.u32 %v75690, %v75686
%v75694 = vadd.s32 %v75691, %v75686
%v75696 = vshll.u32 %v75691, 16
%v75697 = vshrl.u32 %v75691, 16
%v75698 = vor.u32 %v75697, %v75696
%v75699 = vxor.u32 %v75698, %v75694
%v75702 = vadd.s32 %v75699, %v75694
%v75706 = vadd.s32 %v75702, %v9
%v75708 = vshll.u32 %v75699, 24
%v75709 = vshrl.u32 %v75699, 8
%v75710 = vor.u32 %v75709, %v75708
%v75711 = vxor.u32 %v75710, %v75702
%v75714 = vadd.s32 %v75711, %v8
%v75718 = vadd.s32 4, %v75714
%v75722 = vadd.s32 %v75718, %v75706
%v75724 = vshll.u32 %v75718, 13
%v75725 = vshrl.u32 %v75718, 19
%v75726 = vor.u32 %v75725, %v75724
%v75727 = vxor.u32 %v75726, %v75722
%v75730 = vadd.s32 %v75727, %v75722
%v75732 = vshll.u32 %v75727, 15
%v75733 = vshrl.u32 %v75727, 17
%v75734 = vor.u32 %v75733, %v75732
%v75735 = vxor.u32 %v75734, %v75730
%v75738 = vadd.s32 %v75735, %v75730
%v75740 = vshll.u32 %v75735, 26
%v75741 = vshrl.u32 %v75735, 6
%v75742 = vor.u32 %v75741, %v75740
%v75743 = vxor.u32 %v75742, %v75738
%v75746 = vadd.s32 %v75743, %v75738
%v75750 = vadd.s32 %v75746, %v8
%v75752 = vshll.u32 %v75743, 6
%v75753 = vshrl.u32 %v75743, 26
%v75754 = vor.u32 %v75753, %v75752
%v75755 = vxor.u32 %v75754, %v75746
%v75758 = vadd.s32 %v75755, %v10
%v75762 = vadd.s32 5, %v75758
%v75764 = vxor.u32 %v75762, %v75750
%v75765 = vand.u32.u8 255, %v75764
%v75766 = vand.u32 65535, %v75765
%v75767 = vshrl.u32 %v75766, 1
%v75768 = vor.u32 16256, %v75767
%v75769 = vand.u32.u16 65535, %v75768
%v120150 = vadd.low.f32.bf16 -1.0, %v75769
%v75778 = vmul.f32 2.0, %v120150
%v75782 = vadd.f32 -0.99609375, %v75778
%v75786 = vmax.f32 %v75782, -0.99609375
%v75788 = vand.u32 2147483647, %v75786
%vm75791 = vcmp.eq.f32.partialorder %v75788, 1.0
%v75796 = vmul.f32 inf, %v75786
%v75798 = vxor.u32 2147483648, %v75786
%v75801 = vmul.f32 %v75798, %v75786
%v75803 = vadd.f32 1.0, %v75801
%v75804 = vlog2.pop %v75803
%v75805 = vmul.f32 0.6931472, %v75804
%v75806 = vmul.f32 -0.5, %v75801
%v75807 = vadd.f32 1.0, %v75806
%v75808 = vmul.f32 %v75807, %v75801
%v75809 = vand.u32 2147483647, %v75801
%vm75810 = vcmp.lt.f32.partialorder %v75809, 0.0004427343
%v75811 = vsel /*vm=*/%vm75810, /*on_true_vy=*/%v75808, /*on_false_vx=*/%v75805
%v75812 = vxor.u32 2147483648, %v75811
%vm75815 = vcmp.lt.f32.partialorder %v75812, 5.0
%v75820 = vsel /*vm=*/%vm75815, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v75824 = vsel /*vm=*/%vm75815, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v75828 = vsel /*vm=*/%vm75815, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v75832 = vsel /*vm=*/%vm75815, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v75836 = vsel /*vm=*/%vm75815, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v75840 = vsel /*vm=*/%vm75815, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v75844 = vsel /*vm=*/%vm75815, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v75848 = vsel /*vm=*/%vm75815, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v75852 = vsel /*vm=*/%vm75815, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v75856 = vadd.f32 -2.5, %v75812
%v75858 = vrsqrt.pop %v75812
%v75859 = vmul.f32 %v75858, %v75812
%vm75860 = vcmp.eq.f32.partialorder %v75812, inf
%v75861 = vsel /*vm=*/%vm75860, /*on_true_vy=*/%v75812, /*on_false_vx=*/%v75859
%vm75862 = vcmp.eq.f32.partialorder %v75812, 0.0
%v75863 = vand.u32 2147483648, %v75812
%v75864 = vsel /*vm=*/%vm75862, /*on_true_vy=*/%v75863, /*on_false_vx=*/%v75861
%v75867 = vadd.f32 -3.0, %v75864
%v75871 = vsel /*vm=*/%vm75815, /*on_true_vy=*/%v75856, /*on_false_vx=*/%v75867
%v75875 = vmul.f32 %v75871, %v75852
%v75879 = vadd.f32 %v75875, %v75848
%v75883 = vmul.f32 %v75879, %v75871
%v75887 = vadd.f32 %v75883, %v75844
%v75891 = vmul.f32 %v75887, %v75871
%v75895 = vadd.f32 %v75891, %v75840
%v75899 = vmul.f32 %v75895, %v75871
%v75903 = vadd.f32 %v75899, %v75836
%v75907 = vmul.f32 %v75903, %v75871
%v75911 = vadd.f32 %v75907, %v75832
%v75915 = vmul.f32 %v75911, %v75871
%v75919 = vadd.f32 %v75915, %v75828
%v75923 = vmul.f32 %v75919, %v75871
%v75927 = vadd.f32 %v75923, %v75824
%v75931 = vmul.f32 %v75927, %v75871
%v75935 = vadd.f32 %v75931, %v75820
%v75939 = vmul.f32 %v75935, %v75786
%v75943 = vsel /*vm=*/%vm75791, /*on_true_vy=*/%v75796, /*on_false_vx=*/%v75939
%v75947 = vmul.f32 1.4140625, %v75943
%v75950 = vpack.c.bf16 %v120417, %v75947
%120151 = vst [vmem:[%s280 + $0xd0] sm:$0xf] /*vst_source=*/%v75950
%v75954 = vadd.s32 %v75029, %v1381
%v75964 = vadd.s32 %v75954, %v415
%vm75968 = vcmp.lt.u32.totalorder %v75964, %v75954
%vm75973 = vcmp.lt.u32.totalorder %v75954, %v1381
%v75978 = vadd.s32 %v75012, %v1368
%v75982 = vadd.s32 1, %v75978
%v75986 = vsel /*vm=*/%vm75973, /*on_true_vy=*/%v75982, /*on_false_vx=*/%v75978
%v75990 = vadd.s32 1, %v75986
%v75994 = vsel /*vm=*/%vm75968, /*on_true_vy=*/%v75990, /*on_false_vx=*/%v75986
%v75999 = vadd.s32 %v75994, %v10
%v76003 = vadd.s32 %v75964, %v9
%v76007 = vadd.s32 %v76003, %v75999
%v76009 = vshll.u32 %v76003, 13
%v76010 = vshrl.u32 %v76003, 19
%v76011 = vor.u32 %v76010, %v76009
%v76012 = vxor.u32 %v76011, %v76007
%v76015 = vadd.s32 %v76012, %v76007
%v76017 = vshll.u32 %v76012, 15
%v76018 = vshrl.u32 %v76012, 17
%v76019 = vor.u32 %v76018, %v76017
%v76020 = vxor.u32 %v76019, %v76015
%v76023 = vadd.s32 %v76020, %v76015
%v76025 = vshll.u32 %v76020, 26
%v76026 = vshrl.u32 %v76020, 6
%v76027 = vor.u32 %v76026, %v76025
%v76028 = vxor.u32 %v76027, %v76023
%v76031 = vadd.s32 %v76028, %v76023
%v76035 = vadd.s32 %v76031, %v9
%v76037 = vshll.u32 %v76028, 6
%v76038 = vshrl.u32 %v76028, 26
%v76039 = vor.u32 %v76038, %v76037
%v76040 = vxor.u32 %v76039, %v76031
%v76043 = vadd.s32 %v76040, %v8
%v76047 = vadd.s32 1, %v76043
%v76051 = vadd.s32 %v76047, %v76035
%v76053 = vshll.u32 %v76047, 17
%v76054 = vshrl.u32 %v76047, 15
%v76055 = vor.u32 %v76054, %v76053
%v76056 = vxor.u32 %v76055, %v76051
%v76059 = vadd.s32 %v76056, %v76051
%v76061 = vshll.u32 %v76056, 29
%v76062 = vshrl.u32 %v76056, 3
%v76063 = vor.u32 %v76062, %v76061
%v76064 = vxor.u32 %v76063, %v76059
%v76067 = vadd.s32 %v76064, %v76059
%v76069 = vshll.u32 %v76064, 16
%v76070 = vshrl.u32 %v76064, 16
%v76071 = vor.u32 %v76070, %v76069
%v76072 = vxor.u32 %v76071, %v76067
%v76075 = vadd.s32 %v76072, %v76067
%v76079 = vadd.s32 %v76075, %v8
%v76081 = vshll.u32 %v76072, 24
%v76082 = vshrl.u32 %v76072, 8
%v76083 = vor.u32 %v76082, %v76081
%v76084 = vxor.u32 %v76083, %v76075
%v76087 = vadd.s32 %v76084, %v10
%v76091 = vadd.s32 2, %v76087
%v76095 = vadd.s32 %v76091, %v76079
%v76097 = vshll.u32 %v76091, 13
%v76098 = vshrl.u32 %v76091, 19
%v76099 = vor.u32 %v76098, %v76097
%v76100 = vxor.u32 %v76099, %v76095
%v76103 = vadd.s32 %v76100, %v76095
%v76105 = vshll.u32 %v76100, 15
%v76106 = vshrl.u32 %v76100, 17
%v76107 = vor.u32 %v76106, %v76105
%v76108 = vxor.u32 %v76107, %v76103
%v76111 = vadd.s32 %v76108, %v76103
%v76113 = vshll.u32 %v76108, 26
%v76114 = vshrl.u32 %v76108, 6
%v76115 = vor.u32 %v76114, %v76113
%v76116 = vxor.u32 %v76115, %v76111
%v76119 = vadd.s32 %v76116, %v76111
%v76123 = vadd.s32 %v76119, %v10
%v76125 = vshll.u32 %v76116, 6
%v76126 = vshrl.u32 %v76116, 26
%v76127 = vor.u32 %v76126, %v76125
%v76128 = vxor.u32 %v76127, %v76119
%v76131 = vadd.s32 %v76128, %v9
%v76135 = vadd.s32 3, %v76131
%v76139 = vadd.s32 %v76135, %v76123
%v76141 = vshll.u32 %v76135, 17
%v76142 = vshrl.u32 %v76135, 15
%v76143 = vor.u32 %v76142, %v76141
%v76144 = vxor.u32 %v76143, %v76139
%v76147 = vadd.s32 %v76144, %v76139
%v76149 = vshll.u32 %v76144, 29
%v76150 = vshrl.u32 %v76144, 3
%v76151 = vor.u32 %v76150, %v76149
%v76152 = vxor.u32 %v76151, %v76147
%v76155 = vadd.s32 %v76152, %v76147
%v76157 = vshll.u32 %v76152, 16
%v76158 = vshrl.u32 %v76152, 16
%v76159 = vor.u32 %v76158, %v76157
%v76160 = vxor.u32 %v76159, %v76155
%v76163 = vadd.s32 %v76160, %v76155
%v76167 = vadd.s32 %v76163, %v9
%v76169 = vshll.u32 %v76160, 24
%v76170 = vshrl.u32 %v76160, 8
%v76171 = vor.u32 %v76170, %v76169
%v76172 = vxor.u32 %v76171, %v76163
%v76175 = vadd.s32 %v76172, %v8
%v76179 = vadd.s32 4, %v76175
%v76183 = vadd.s32 %v76179, %v76167
%v76185 = vshll.u32 %v76179, 13
%v76186 = vshrl.u32 %v76179, 19
%v76187 = vor.u32 %v76186, %v76185
%v76188 = vxor.u32 %v76187, %v76183
%v76191 = vadd.s32 %v76188, %v76183
%v76193 = vshll.u32 %v76188, 15
%v76194 = vshrl.u32 %v76188, 17
%v76195 = vor.u32 %v76194, %v76193
%v76196 = vxor.u32 %v76195, %v76191
%v76199 = vadd.s32 %v76196, %v76191
%v76201 = vshll.u32 %v76196, 26
%v76202 = vshrl.u32 %v76196, 6
%v76203 = vor.u32 %v76202, %v76201
%v76204 = vxor.u32 %v76203, %v76199
%v76207 = vadd.s32 %v76204, %v76199
%v76211 = vadd.s32 %v76207, %v8
%v76213 = vshll.u32 %v76204, 6
%v76214 = vshrl.u32 %v76204, 26
%v76215 = vor.u32 %v76214, %v76213
%v76216 = vxor.u32 %v76215, %v76207
%v76219 = vadd.s32 %v76216, %v10
%v76223 = vadd.s32 5, %v76219
%v76225 = vxor.u32 %v76223, %v76211
%v76226 = vand.u32.u8 255, %v76225
%v76227 = vand.u32 65535, %v76226
%v76228 = vshrl.u32 %v76227, 1
%v76229 = vor.u32 16256, %v76228
%v76230 = vand.u32.u16 65535, %v76229
%v120152 = vadd.low.f32.bf16 -1.0, %v76230
%v76239 = vmul.f32 2.0, %v120152
%v76243 = vadd.f32 -0.99609375, %v76239
%v76247 = vmax.f32 %v76243, -0.99609375
%v76249 = vand.u32 2147483647, %v76247
%vm76252 = vcmp.eq.f32.partialorder %v76249, 1.0
%v76257 = vmul.f32 inf, %v76247
%v76259 = vxor.u32 2147483648, %v76247
%v76262 = vmul.f32 %v76259, %v76247
%v76264 = vadd.f32 1.0, %v76262
%v76265 = vlog2.pop %v76264
%v76266 = vmul.f32 0.6931472, %v76265
%v76267 = vmul.f32 -0.5, %v76262
%v76268 = vadd.f32 1.0, %v76267
%v76269 = vmul.f32 %v76268, %v76262
%v76270 = vand.u32 2147483647, %v76262
%vm76271 = vcmp.lt.f32.partialorder %v76270, 0.0004427343
%v76272 = vsel /*vm=*/%vm76271, /*on_true_vy=*/%v76269, /*on_false_vx=*/%v76266
%v76273 = vxor.u32 2147483648, %v76272
%vm76276 = vcmp.lt.f32.partialorder %v76273, 5.0
%v76281 = vsel /*vm=*/%vm76276, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v76285 = vsel /*vm=*/%vm76276, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v76289 = vsel /*vm=*/%vm76276, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v76293 = vsel /*vm=*/%vm76276, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v76297 = vsel /*vm=*/%vm76276, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v76301 = vsel /*vm=*/%vm76276, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v76305 = vsel /*vm=*/%vm76276, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v76309 = vsel /*vm=*/%vm76276, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v76313 = vsel /*vm=*/%vm76276, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v76317 = vadd.f32 -2.5, %v76273
%v76319 = vrsqrt.pop %v76273
%v76320 = vmul.f32 %v76319, %v76273
%vm76321 = vcmp.eq.f32.partialorder %v76273, inf
%v76322 = vsel /*vm=*/%vm76321, /*on_true_vy=*/%v76273, /*on_false_vx=*/%v76320
%vm76323 = vcmp.eq.f32.partialorder %v76273, 0.0
%v76324 = vand.u32 2147483648, %v76273
%v76325 = vsel /*vm=*/%vm76323, /*on_true_vy=*/%v76324, /*on_false_vx=*/%v76322
%v76328 = vadd.f32 -3.0, %v76325
%v76332 = vsel /*vm=*/%vm76276, /*on_true_vy=*/%v76317, /*on_false_vx=*/%v76328
%v76336 = vmul.f32 %v76332, %v76313
%v76340 = vadd.f32 %v76336, %v76309
%v76344 = vmul.f32 %v76340, %v76332
%v76348 = vadd.f32 %v76344, %v76305
%v76352 = vmul.f32 %v76348, %v76332
%v76356 = vadd.f32 %v76352, %v76301
%v76360 = vmul.f32 %v76356, %v76332
%v76364 = vadd.f32 %v76360, %v76297
%v76368 = vmul.f32 %v76364, %v76332
%v76372 = vadd.f32 %v76368, %v76293
%v76376 = vmul.f32 %v76372, %v76332
%v76380 = vadd.f32 %v76376, %v76289
%v76384 = vmul.f32 %v76380, %v76332
%v76388 = vadd.f32 %v76384, %v76285
%v76392 = vmul.f32 %v76388, %v76332
%v76396 = vadd.f32 %v76392, %v76281
%v76400 = vmul.f32 %v76396, %v76247
%v76404 = vsel /*vm=*/%vm76252, /*on_true_vy=*/%v76257, /*on_false_vx=*/%v76400
%v76408 = vmul.f32 1.4140625, %v76404
%v76411 = vpack.c.bf16 %v120417, %v76408
%120153 = vst [vmem:[%s280 + $0x150] sm:$0xf] /*vst_source=*/%v76411
%v76415 = vadd.s32 %v75029, %v1868
%v76425 = vadd.s32 %v76415, %v415
%vm76429 = vcmp.lt.u32.totalorder %v76425, %v76415
%vm76434 = vcmp.lt.u32.totalorder %v76415, %v1868
%v76439 = vadd.s32 %v75012, %v1855
%v76443 = vadd.s32 1, %v76439
%v76447 = vsel /*vm=*/%vm76434, /*on_true_vy=*/%v76443, /*on_false_vx=*/%v76439
%v76451 = vadd.s32 1, %v76447
%v76455 = vsel /*vm=*/%vm76429, /*on_true_vy=*/%v76451, /*on_false_vx=*/%v76447
%v76460 = vadd.s32 %v76455, %v10
%v76464 = vadd.s32 %v76425, %v9
%v76468 = vadd.s32 %v76464, %v76460
%v76470 = vshll.u32 %v76464, 13
%v76471 = vshrl.u32 %v76464, 19
%v76472 = vor.u32 %v76471, %v76470
%v76473 = vxor.u32 %v76472, %v76468
%v76476 = vadd.s32 %v76473, %v76468
%v76478 = vshll.u32 %v76473, 15
%v76479 = vshrl.u32 %v76473, 17
%v76480 = vor.u32 %v76479, %v76478
%v76481 = vxor.u32 %v76480, %v76476
%v76484 = vadd.s32 %v76481, %v76476
%v76486 = vshll.u32 %v76481, 26
%v76487 = vshrl.u32 %v76481, 6
%v76488 = vor.u32 %v76487, %v76486
%v76489 = vxor.u32 %v76488, %v76484
%v76492 = vadd.s32 %v76489, %v76484
%v76496 = vadd.s32 %v76492, %v9
%v76498 = vshll.u32 %v76489, 6
%v76499 = vshrl.u32 %v76489, 26
%v76500 = vor.u32 %v76499, %v76498
%v76501 = vxor.u32 %v76500, %v76492
%v76504 = vadd.s32 %v76501, %v8
%v76508 = vadd.s32 1, %v76504
%v76512 = vadd.s32 %v76508, %v76496
%v76514 = vshll.u32 %v76508, 17
%v76515 = vshrl.u32 %v76508, 15
%v76516 = vor.u32 %v76515, %v76514
%v76517 = vxor.u32 %v76516, %v76512
%v76520 = vadd.s32 %v76517, %v76512
%v76522 = vshll.u32 %v76517, 29
%v76523 = vshrl.u32 %v76517, 3
%v76524 = vor.u32 %v76523, %v76522
%v76525 = vxor.u32 %v76524, %v76520
%v76528 = vadd.s32 %v76525, %v76520
%v76530 = vshll.u32 %v76525, 16
%v76531 = vshrl.u32 %v76525, 16
%v76532 = vor.u32 %v76531, %v76530
%v76533 = vxor.u32 %v76532, %v76528
%v76536 = vadd.s32 %v76533, %v76528
%v76540 = vadd.s32 %v76536, %v8
%v76542 = vshll.u32 %v76533, 24
%v76543 = vshrl.u32 %v76533, 8
%v76544 = vor.u32 %v76543, %v76542
%v76545 = vxor.u32 %v76544, %v76536
%v76548 = vadd.s32 %v76545, %v10
%v76552 = vadd.s32 2, %v76548
%v76556 = vadd.s32 %v76552, %v76540
%v76558 = vshll.u32 %v76552, 13
%v76559 = vshrl.u32 %v76552, 19
%v76560 = vor.u32 %v76559, %v76558
%v76561 = vxor.u32 %v76560, %v76556
%v76564 = vadd.s32 %v76561, %v76556
%v76566 = vshll.u32 %v76561, 15
%v76567 = vshrl.u32 %v76561, 17
%v76568 = vor.u32 %v76567, %v76566
%v76569 = vxor.u32 %v76568, %v76564
%v76572 = vadd.s32 %v76569, %v76564
%v76574 = vshll.u32 %v76569, 26
%v76575 = vshrl.u32 %v76569, 6
%v76576 = vor.u32 %v76575, %v76574
%v76577 = vxor.u32 %v76576, %v76572
%v76580 = vadd.s32 %v76577, %v76572
%v76584 = vadd.s32 %v76580, %v10
%v76586 = vshll.u32 %v76577, 6
%v76587 = vshrl.u32 %v76577, 26
%v76588 = vor.u32 %v76587, %v76586
%v76589 = vxor.u32 %v76588, %v76580
%v76592 = vadd.s32 %v76589, %v9
%v76596 = vadd.s32 3, %v76592
%v76600 = vadd.s32 %v76596, %v76584
%v76602 = vshll.u32 %v76596, 17
%v76603 = vshrl.u32 %v76596, 15
%v76604 = vor.u32 %v76603, %v76602
%v76605 = vxor.u32 %v76604, %v76600
%v76608 = vadd.s32 %v76605, %v76600
%v76610 = vshll.u32 %v76605, 29
%v76611 = vshrl.u32 %v76605, 3
%v76612 = vor.u32 %v76611, %v76610
%v76613 = vxor.u32 %v76612, %v76608
%v76616 = vadd.s32 %v76613, %v76608
%v76618 = vshll.u32 %v76613, 16
%v76619 = vshrl.u32 %v76613, 16
%v76620 = vor.u32 %v76619, %v76618
%v76621 = vxor.u32 %v76620, %v76616
%v76624 = vadd.s32 %v76621, %v76616
%v76628 = vadd.s32 %v76624, %v9
%v76630 = vshll.u32 %v76621, 24
%v76631 = vshrl.u32 %v76621, 8
%v76632 = vor.u32 %v76631, %v76630
%v76633 = vxor.u32 %v76632, %v76624
%v76636 = vadd.s32 %v76633, %v8
%v76640 = vadd.s32 4, %v76636
%v76644 = vadd.s32 %v76640, %v76628
%v76646 = vshll.u32 %v76640, 13
%v76647 = vshrl.u32 %v76640, 19
%v76648 = vor.u32 %v76647, %v76646
%v76649 = vxor.u32 %v76648, %v76644
%v76652 = vadd.s32 %v76649, %v76644
%v76654 = vshll.u32 %v76649, 15
%v76655 = vshrl.u32 %v76649, 17
%v76656 = vor.u32 %v76655, %v76654
%v76657 = vxor.u32 %v76656, %v76652
%v76660 = vadd.s32 %v76657, %v76652
%v76662 = vshll.u32 %v76657, 26
%v76663 = vshrl.u32 %v76657, 6
%v76664 = vor.u32 %v76663, %v76662
%v76665 = vxor.u32 %v76664, %v76660
%v76668 = vadd.s32 %v76665, %v76660
%v76672 = vadd.s32 %v76668, %v8
%v76674 = vshll.u32 %v76665, 6
%v76675 = vshrl.u32 %v76665, 26
%v76676 = vor.u32 %v76675, %v76674
%v76677 = vxor.u32 %v76676, %v76668
%v76680 = vadd.s32 %v76677, %v10
%v76684 = vadd.s32 5, %v76680
%v76686 = vxor.u32 %v76684, %v76672
%v76687 = vand.u32.u8 255, %v76686
%v76688 = vand.u32 65535, %v76687
%v76689 = vshrl.u32 %v76688, 1
%v76690 = vor.u32 16256, %v76689
%v76691 = vand.u32.u16 65535, %v76690
%v120154 = vadd.low.f32.bf16 -1.0, %v76691
%v76700 = vmul.f32 2.0, %v120154
%v76704 = vadd.f32 -0.99609375, %v76700
%v76708 = vmax.f32 %v76704, -0.99609375
%v76710 = vand.u32 2147483647, %v76708
%vm76713 = vcmp.eq.f32.partialorder %v76710, 1.0
%v76718 = vmul.f32 inf, %v76708
%v76720 = vxor.u32 2147483648, %v76708
%v76723 = vmul.f32 %v76720, %v76708
%v76725 = vadd.f32 1.0, %v76723
%v76726 = vlog2.pop %v76725
%v76727 = vmul.f32 0.6931472, %v76726
%v76728 = vmul.f32 -0.5, %v76723
%v76729 = vadd.f32 1.0, %v76728
%v76730 = vmul.f32 %v76729, %v76723
%v76731 = vand.u32 2147483647, %v76723
%vm76732 = vcmp.lt.f32.partialorder %v76731, 0.0004427343
%v76733 = vsel /*vm=*/%vm76732, /*on_true_vy=*/%v76730, /*on_false_vx=*/%v76727
%v76734 = vxor.u32 2147483648, %v76733
%vm76737 = vcmp.lt.f32.partialorder %v76734, 5.0
%v76742 = vsel /*vm=*/%vm76737, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v76746 = vsel /*vm=*/%vm76737, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v76750 = vsel /*vm=*/%vm76737, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v76754 = vsel /*vm=*/%vm76737, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v76758 = vsel /*vm=*/%vm76737, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v76762 = vsel /*vm=*/%vm76737, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v76766 = vsel /*vm=*/%vm76737, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v76770 = vsel /*vm=*/%vm76737, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v76774 = vsel /*vm=*/%vm76737, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v76778 = vadd.f32 -2.5, %v76734
%v76780 = vrsqrt.pop %v76734
%v76781 = vmul.f32 %v76780, %v76734
%vm76782 = vcmp.eq.f32.partialorder %v76734, inf
%v76783 = vsel /*vm=*/%vm76782, /*on_true_vy=*/%v76734, /*on_false_vx=*/%v76781
%vm76784 = vcmp.eq.f32.partialorder %v76734, 0.0
%v76785 = vand.u32 2147483648, %v76734
%v76786 = vsel /*vm=*/%vm76784, /*on_true_vy=*/%v76785, /*on_false_vx=*/%v76783
%v76789 = vadd.f32 -3.0, %v76786
%v76793 = vsel /*vm=*/%vm76737, /*on_true_vy=*/%v76778, /*on_false_vx=*/%v76789
%v76797 = vmul.f32 %v76793, %v76774
%v76801 = vadd.f32 %v76797, %v76770
%v76805 = vmul.f32 %v76801, %v76793
%v76809 = vadd.f32 %v76805, %v76766
%v76813 = vmul.f32 %v76809, %v76793
%v76817 = vadd.f32 %v76813, %v76762
%v76821 = vmul.f32 %v76817, %v76793
%v76825 = vadd.f32 %v76821, %v76758
%v76829 = vmul.f32 %v76825, %v76793
%v76833 = vadd.f32 %v76829, %v76754
%v76837 = vmul.f32 %v76833, %v76793
%v76841 = vadd.f32 %v76837, %v76750
%v76845 = vmul.f32 %v76841, %v76793
%v76849 = vadd.f32 %v76845, %v76746
%v76853 = vmul.f32 %v76849, %v76793
%v76857 = vadd.f32 %v76853, %v76742
%v76861 = vmul.f32 %v76857, %v76708
%v76865 = vsel /*vm=*/%vm76713, /*on_true_vy=*/%v76718, /*on_false_vx=*/%v76861
%v76869 = vmul.f32 1.4140625, %v76865
%v76872 = vpack.c.bf16 %v120417, %v76869
%120155 = vst [vmem:[%s280 + $0x1d0] sm:$0xf] /*vst_source=*/%v76872
%v76876 = vadd.s32 %v75029, %v2355
%v76886 = vadd.s32 %v76876, %v415
%vm76890 = vcmp.lt.u32.totalorder %v76886, %v76876
%vm76895 = vcmp.lt.u32.totalorder %v76876, %v2355
%v76900 = vadd.s32 %v75012, %v2342
%v76904 = vadd.s32 1, %v76900
%v76908 = vsel /*vm=*/%vm76895, /*on_true_vy=*/%v76904, /*on_false_vx=*/%v76900
%v76912 = vadd.s32 1, %v76908
%v76916 = vsel /*vm=*/%vm76890, /*on_true_vy=*/%v76912, /*on_false_vx=*/%v76908
%v76921 = vadd.s32 %v76916, %v10
%v76925 = vadd.s32 %v76886, %v9
%v76929 = vadd.s32 %v76925, %v76921
%v76931 = vshll.u32 %v76925, 13
%v76932 = vshrl.u32 %v76925, 19
%v76933 = vor.u32 %v76932, %v76931
%v76934 = vxor.u32 %v76933, %v76929
%v76937 = vadd.s32 %v76934, %v76929
%v76939 = vshll.u32 %v76934, 15
%v76940 = vshrl.u32 %v76934, 17
%v76941 = vor.u32 %v76940, %v76939
%v76942 = vxor.u32 %v76941, %v76937
%v76945 = vadd.s32 %v76942, %v76937
%v76947 = vshll.u32 %v76942, 26
%v76948 = vshrl.u32 %v76942, 6
%v76949 = vor.u32 %v76948, %v76947
%v76950 = vxor.u32 %v76949, %v76945
%v76953 = vadd.s32 %v76950, %v76945
%v76957 = vadd.s32 %v76953, %v9
%v76959 = vshll.u32 %v76950, 6
%v76960 = vshrl.u32 %v76950, 26
%v76961 = vor.u32 %v76960, %v76959
%v76962 = vxor.u32 %v76961, %v76953
%v76965 = vadd.s32 %v76962, %v8
%v76969 = vadd.s32 1, %v76965
%v76973 = vadd.s32 %v76969, %v76957
%v76975 = vshll.u32 %v76969, 17
%v76976 = vshrl.u32 %v76969, 15
%v76977 = vor.u32 %v76976, %v76975
%v76978 = vxor.u32 %v76977, %v76973
%v76981 = vadd.s32 %v76978, %v76973
%v76983 = vshll.u32 %v76978, 29
%v76984 = vshrl.u32 %v76978, 3
%v76985 = vor.u32 %v76984, %v76983
%v76986 = vxor.u32 %v76985, %v76981
%v76989 = vadd.s32 %v76986, %v76981
%v76991 = vshll.u32 %v76986, 16
%v76992 = vshrl.u32 %v76986, 16
%v76993 = vor.u32 %v76992, %v76991
%v76994 = vxor.u32 %v76993, %v76989
%v76997 = vadd.s32 %v76994, %v76989
%v77001 = vadd.s32 %v76997, %v8
%v77003 = vshll.u32 %v76994, 24
%v77004 = vshrl.u32 %v76994, 8
%v77005 = vor.u32 %v77004, %v77003
%v77006 = vxor.u32 %v77005, %v76997
%v77009 = vadd.s32 %v77006, %v10
%v77013 = vadd.s32 2, %v77009
%v77017 = vadd.s32 %v77013, %v77001
%v77019 = vshll.u32 %v77013, 13
%v77020 = vshrl.u32 %v77013, 19
%v77021 = vor.u32 %v77020, %v77019
%v77022 = vxor.u32 %v77021, %v77017
%v77025 = vadd.s32 %v77022, %v77017
%v77027 = vshll.u32 %v77022, 15
%v77028 = vshrl.u32 %v77022, 17
%v77029 = vor.u32 %v77028, %v77027
%v77030 = vxor.u32 %v77029, %v77025
%v77033 = vadd.s32 %v77030, %v77025
%v77035 = vshll.u32 %v77030, 26
%v77036 = vshrl.u32 %v77030, 6
%v77037 = vor.u32 %v77036, %v77035
%v77038 = vxor.u32 %v77037, %v77033
%v77041 = vadd.s32 %v77038, %v77033
%v77045 = vadd.s32 %v77041, %v10
%v77047 = vshll.u32 %v77038, 6
%v77048 = vshrl.u32 %v77038, 26
%v77049 = vor.u32 %v77048, %v77047
%v77050 = vxor.u32 %v77049, %v77041
%v77053 = vadd.s32 %v77050, %v9
%v77057 = vadd.s32 3, %v77053
%v77061 = vadd.s32 %v77057, %v77045
%v77063 = vshll.u32 %v77057, 17
%v77064 = vshrl.u32 %v77057, 15
%v77065 = vor.u32 %v77064, %v77063
%v77066 = vxor.u32 %v77065, %v77061
%v77069 = vadd.s32 %v77066, %v77061
%v77071 = vshll.u32 %v77066, 29
%v77072 = vshrl.u32 %v77066, 3
%v77073 = vor.u32 %v77072, %v77071
%v77074 = vxor.u32 %v77073, %v77069
%v77077 = vadd.s32 %v77074, %v77069
%v77079 = vshll.u32 %v77074, 16
%v77080 = vshrl.u32 %v77074, 16
%v77081 = vor.u32 %v77080, %v77079
%v77082 = vxor.u32 %v77081, %v77077
%v77085 = vadd.s32 %v77082, %v77077
%v77089 = vadd.s32 %v77085, %v9
%v77091 = vshll.u32 %v77082, 24
%v77092 = vshrl.u32 %v77082, 8
%v77093 = vor.u32 %v77092, %v77091
%v77094 = vxor.u32 %v77093, %v77085
%v77097 = vadd.s32 %v77094, %v8
%v77101 = vadd.s32 4, %v77097
%v77105 = vadd.s32 %v77101, %v77089
%v77107 = vshll.u32 %v77101, 13
%v77108 = vshrl.u32 %v77101, 19
%v77109 = vor.u32 %v77108, %v77107
%v77110 = vxor.u32 %v77109, %v77105
%v77113 = vadd.s32 %v77110, %v77105
%v77115 = vshll.u32 %v77110, 15
%v77116 = vshrl.u32 %v77110, 17
%v77117 = vor.u32 %v77116, %v77115
%v77118 = vxor.u32 %v77117, %v77113
%v77121 = vadd.s32 %v77118, %v77113
%v77123 = vshll.u32 %v77118, 26
%v77124 = vshrl.u32 %v77118, 6
%v77125 = vor.u32 %v77124, %v77123
%v77126 = vxor.u32 %v77125, %v77121
%v77129 = vadd.s32 %v77126, %v77121
%v77133 = vadd.s32 %v77129, %v8
%v77135 = vshll.u32 %v77126, 6
%v77136 = vshrl.u32 %v77126, 26
%v77137 = vor.u32 %v77136, %v77135
%v77138 = vxor.u32 %v77137, %v77129
%v77141 = vadd.s32 %v77138, %v10
%v77145 = vadd.s32 5, %v77141
%v77147 = vxor.u32 %v77145, %v77133
%v77148 = vand.u32.u8 255, %v77147
%v77149 = vand.u32 65535, %v77148
%v77150 = vshrl.u32 %v77149, 1
%v77151 = vor.u32 16256, %v77150
%v77152 = vand.u32.u16 65535, %v77151
%v120156 = vadd.low.f32.bf16 -1.0, %v77152
%v77161 = vmul.f32 2.0, %v120156
%v77165 = vadd.f32 -0.99609375, %v77161
%v77169 = vmax.f32 %v77165, -0.99609375
%v77171 = vand.u32 2147483647, %v77169
%vm77174 = vcmp.eq.f32.partialorder %v77171, 1.0
%v77179 = vmul.f32 inf, %v77169
%v77181 = vxor.u32 2147483648, %v77169
%v77184 = vmul.f32 %v77181, %v77169
%v77186 = vadd.f32 1.0, %v77184
%v77187 = vlog2.pop %v77186
%v77188 = vmul.f32 0.6931472, %v77187
%v77189 = vmul.f32 -0.5, %v77184
%v77190 = vadd.f32 1.0, %v77189
%v77191 = vmul.f32 %v77190, %v77184
%v77192 = vand.u32 2147483647, %v77184
%vm77193 = vcmp.lt.f32.partialorder %v77192, 0.0004427343
%v77194 = vsel /*vm=*/%vm77193, /*on_true_vy=*/%v77191, /*on_false_vx=*/%v77188
%v77195 = vxor.u32 2147483648, %v77194
%vm77198 = vcmp.lt.f32.partialorder %v77195, 5.0
%v77203 = vsel /*vm=*/%vm77198, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v77207 = vsel /*vm=*/%vm77198, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v77211 = vsel /*vm=*/%vm77198, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v77215 = vsel /*vm=*/%vm77198, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v77219 = vsel /*vm=*/%vm77198, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v77223 = vsel /*vm=*/%vm77198, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v77227 = vsel /*vm=*/%vm77198, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v77231 = vsel /*vm=*/%vm77198, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v77235 = vsel /*vm=*/%vm77198, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v77239 = vadd.f32 -2.5, %v77195
%v77241 = vrsqrt.pop %v77195
%v77242 = vmul.f32 %v77241, %v77195
%vm77243 = vcmp.eq.f32.partialorder %v77195, inf
%v77244 = vsel /*vm=*/%vm77243, /*on_true_vy=*/%v77195, /*on_false_vx=*/%v77242
%vm77245 = vcmp.eq.f32.partialorder %v77195, 0.0
%v77246 = vand.u32 2147483648, %v77195
%v77247 = vsel /*vm=*/%vm77245, /*on_true_vy=*/%v77246, /*on_false_vx=*/%v77244
%v77250 = vadd.f32 -3.0, %v77247
%v77254 = vsel /*vm=*/%vm77198, /*on_true_vy=*/%v77239, /*on_false_vx=*/%v77250
%v77258 = vmul.f32 %v77254, %v77235
%v77262 = vadd.f32 %v77258, %v77231
%v77266 = vmul.f32 %v77262, %v77254
%v77270 = vadd.f32 %v77266, %v77227
%v77274 = vmul.f32 %v77270, %v77254
%v77278 = vadd.f32 %v77274, %v77223
%v77282 = vmul.f32 %v77278, %v77254
%v77286 = vadd.f32 %v77282, %v77219
%v77290 = vmul.f32 %v77286, %v77254
%v77294 = vadd.f32 %v77290, %v77215
%v77298 = vmul.f32 %v77294, %v77254
%v77302 = vadd.f32 %v77298, %v77211
%v77306 = vmul.f32 %v77302, %v77254
%v77310 = vadd.f32 %v77306, %v77207
%v77314 = vmul.f32 %v77310, %v77254
%v77318 = vadd.f32 %v77314, %v77203
%v77322 = vmul.f32 %v77318, %v77169
%v77326 = vsel /*vm=*/%vm77174, /*on_true_vy=*/%v77179, /*on_false_vx=*/%v77322
%v77330 = vmul.f32 1.4140625, %v77326
%v77333 = vpack.c.bf16 %v120417, %v77330
%120157 = vst [vmem:[%s280 + $0x250] sm:$0xf] /*vst_source=*/%v77333
%v77337 = vadd.s32 %v75029, %v2842
%v77347 = vadd.s32 %v77337, %v415
%vm77351 = vcmp.lt.u32.totalorder %v77347, %v77337
%vm77356 = vcmp.lt.u32.totalorder %v77337, %v2842
%v77361 = vadd.s32 %v75012, %v2829
%v77365 = vadd.s32 1, %v77361
%v77369 = vsel /*vm=*/%vm77356, /*on_true_vy=*/%v77365, /*on_false_vx=*/%v77361
%v77373 = vadd.s32 1, %v77369
%v77377 = vsel /*vm=*/%vm77351, /*on_true_vy=*/%v77373, /*on_false_vx=*/%v77369
%v77382 = vadd.s32 %v77377, %v10
%v77386 = vadd.s32 %v77347, %v9
%v77390 = vadd.s32 %v77386, %v77382
%v77392 = vshll.u32 %v77386, 13
%v77393 = vshrl.u32 %v77386, 19
%v77394 = vor.u32 %v77393, %v77392
%v77395 = vxor.u32 %v77394, %v77390
%v77398 = vadd.s32 %v77395, %v77390
%v77400 = vshll.u32 %v77395, 15
%v77401 = vshrl.u32 %v77395, 17
%v77402 = vor.u32 %v77401, %v77400
%v77403 = vxor.u32 %v77402, %v77398
%v77406 = vadd.s32 %v77403, %v77398
%v77408 = vshll.u32 %v77403, 26
%v77409 = vshrl.u32 %v77403, 6
%v77410 = vor.u32 %v77409, %v77408
%v77411 = vxor.u32 %v77410, %v77406
%v77414 = vadd.s32 %v77411, %v77406
%v77418 = vadd.s32 %v77414, %v9
%v77420 = vshll.u32 %v77411, 6
%v77421 = vshrl.u32 %v77411, 26
%v77422 = vor.u32 %v77421, %v77420
%v77423 = vxor.u32 %v77422, %v77414
%v77426 = vadd.s32 %v77423, %v8
%v77430 = vadd.s32 1, %v77426
%v77434 = vadd.s32 %v77430, %v77418
%v77436 = vshll.u32 %v77430, 17
%v77437 = vshrl.u32 %v77430, 15
%v77438 = vor.u32 %v77437, %v77436
%v77439 = vxor.u32 %v77438, %v77434
%v77442 = vadd.s32 %v77439, %v77434
%v77444 = vshll.u32 %v77439, 29
%v77445 = vshrl.u32 %v77439, 3
%v77446 = vor.u32 %v77445, %v77444
%v77447 = vxor.u32 %v77446, %v77442
%v77450 = vadd.s32 %v77447, %v77442
%v77452 = vshll.u32 %v77447, 16
%v77453 = vshrl.u32 %v77447, 16
%v77454 = vor.u32 %v77453, %v77452
%v77455 = vxor.u32 %v77454, %v77450
%v77458 = vadd.s32 %v77455, %v77450
%v77462 = vadd.s32 %v77458, %v8
%v77464 = vshll.u32 %v77455, 24
%v77465 = vshrl.u32 %v77455, 8
%v77466 = vor.u32 %v77465, %v77464
%v77467 = vxor.u32 %v77466, %v77458
%v77470 = vadd.s32 %v77467, %v10
%v77474 = vadd.s32 2, %v77470
%v77478 = vadd.s32 %v77474, %v77462
%v77480 = vshll.u32 %v77474, 13
%v77481 = vshrl.u32 %v77474, 19
%v77482 = vor.u32 %v77481, %v77480
%v77483 = vxor.u32 %v77482, %v77478
%v77486 = vadd.s32 %v77483, %v77478
%v77488 = vshll.u32 %v77483, 15
%v77489 = vshrl.u32 %v77483, 17
%v77490 = vor.u32 %v77489, %v77488
%v77491 = vxor.u32 %v77490, %v77486
%v77494 = vadd.s32 %v77491, %v77486
%v77496 = vshll.u32 %v77491, 26
%v77497 = vshrl.u32 %v77491, 6
%v77498 = vor.u32 %v77497, %v77496
%v77499 = vxor.u32 %v77498, %v77494
%v77502 = vadd.s32 %v77499, %v77494
%v77506 = vadd.s32 %v77502, %v10
%v77508 = vshll.u32 %v77499, 6
%v77509 = vshrl.u32 %v77499, 26
%v77510 = vor.u32 %v77509, %v77508
%v77511 = vxor.u32 %v77510, %v77502
%v77514 = vadd.s32 %v77511, %v9
%v77518 = vadd.s32 3, %v77514
%v77522 = vadd.s32 %v77518, %v77506
%v77524 = vshll.u32 %v77518, 17
%v77525 = vshrl.u32 %v77518, 15
%v77526 = vor.u32 %v77525, %v77524
%v77527 = vxor.u32 %v77526, %v77522
%v77530 = vadd.s32 %v77527, %v77522
%v77532 = vshll.u32 %v77527, 29
%v77533 = vshrl.u32 %v77527, 3
%v77534 = vor.u32 %v77533, %v77532
%v77535 = vxor.u32 %v77534, %v77530
%v77538 = vadd.s32 %v77535, %v77530
%v77540 = vshll.u32 %v77535, 16
%v77541 = vshrl.u32 %v77535, 16
%v77542 = vor.u32 %v77541, %v77540
%v77543 = vxor.u32 %v77542, %v77538
%v77546 = vadd.s32 %v77543, %v77538
%v77550 = vadd.s32 %v77546, %v9
%v77552 = vshll.u32 %v77543, 24
%v77553 = vshrl.u32 %v77543, 8
%v77554 = vor.u32 %v77553, %v77552
%v77555 = vxor.u32 %v77554, %v77546
%v77558 = vadd.s32 %v77555, %v8
%v77562 = vadd.s32 4, %v77558
%v77566 = vadd.s32 %v77562, %v77550
%v77568 = vshll.u32 %v77562, 13
%v77569 = vshrl.u32 %v77562, 19
%v77570 = vor.u32 %v77569, %v77568
%v77571 = vxor.u32 %v77570, %v77566
%v77574 = vadd.s32 %v77571, %v77566
%v77576 = vshll.u32 %v77571, 15
%v77577 = vshrl.u32 %v77571, 17
%v77578 = vor.u32 %v77577, %v77576
%v77579 = vxor.u32 %v77578, %v77574
%v77582 = vadd.s32 %v77579, %v77574
%v77584 = vshll.u32 %v77579, 26
%v77585 = vshrl.u32 %v77579, 6
%v77586 = vor.u32 %v77585, %v77584
%v77587 = vxor.u32 %v77586, %v77582
%v77590 = vadd.s32 %v77587, %v77582
%v77594 = vadd.s32 %v77590, %v8
%v77596 = vshll.u32 %v77587, 6
%v77597 = vshrl.u32 %v77587, 26
%v77598 = vor.u32 %v77597, %v77596
%v77599 = vxor.u32 %v77598, %v77590
%v77602 = vadd.s32 %v77599, %v10
%v77606 = vadd.s32 5, %v77602
%v77608 = vxor.u32 %v77606, %v77594
%v77609 = vand.u32.u8 255, %v77608
%v77610 = vand.u32 65535, %v77609
%v77611 = vshrl.u32 %v77610, 1
%v77612 = vor.u32 16256, %v77611
%v77613 = vand.u32.u16 65535, %v77612
%v120158 = vadd.low.f32.bf16 -1.0, %v77613
%v77622 = vmul.f32 2.0, %v120158
%v77626 = vadd.f32 -0.99609375, %v77622
%v77630 = vmax.f32 %v77626, -0.99609375
%v77632 = vand.u32 2147483647, %v77630
%vm77635 = vcmp.eq.f32.partialorder %v77632, 1.0
%v77640 = vmul.f32 inf, %v77630
%v77642 = vxor.u32 2147483648, %v77630
%v77645 = vmul.f32 %v77642, %v77630
%v77647 = vadd.f32 1.0, %v77645
%v77648 = vlog2.pop %v77647
%v77649 = vmul.f32 0.6931472, %v77648
%v77650 = vmul.f32 -0.5, %v77645
%v77651 = vadd.f32 1.0, %v77650
%v77652 = vmul.f32 %v77651, %v77645
%v77653 = vand.u32 2147483647, %v77645
%vm77654 = vcmp.lt.f32.partialorder %v77653, 0.0004427343
%v77655 = vsel /*vm=*/%vm77654, /*on_true_vy=*/%v77652, /*on_false_vx=*/%v77649
%v77656 = vxor.u32 2147483648, %v77655
%vm77659 = vcmp.lt.f32.partialorder %v77656, 5.0
%v77664 = vsel /*vm=*/%vm77659, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v77668 = vsel /*vm=*/%vm77659, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v77672 = vsel /*vm=*/%vm77659, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v77676 = vsel /*vm=*/%vm77659, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v77680 = vsel /*vm=*/%vm77659, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v77684 = vsel /*vm=*/%vm77659, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v77688 = vsel /*vm=*/%vm77659, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v77692 = vsel /*vm=*/%vm77659, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v77696 = vsel /*vm=*/%vm77659, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v77700 = vadd.f32 -2.5, %v77656
%v77702 = vrsqrt.pop %v77656
%v77703 = vmul.f32 %v77702, %v77656
%vm77704 = vcmp.eq.f32.partialorder %v77656, inf
%v77705 = vsel /*vm=*/%vm77704, /*on_true_vy=*/%v77656, /*on_false_vx=*/%v77703
%vm77706 = vcmp.eq.f32.partialorder %v77656, 0.0
%v77707 = vand.u32 2147483648, %v77656
%v77708 = vsel /*vm=*/%vm77706, /*on_true_vy=*/%v77707, /*on_false_vx=*/%v77705
%v77711 = vadd.f32 -3.0, %v77708
%v77715 = vsel /*vm=*/%vm77659, /*on_true_vy=*/%v77700, /*on_false_vx=*/%v77711
%v77719 = vmul.f32 %v77715, %v77696
%v77723 = vadd.f32 %v77719, %v77692
%v77727 = vmul.f32 %v77723, %v77715
%v77731 = vadd.f32 %v77727, %v77688
%v77735 = vmul.f32 %v77731, %v77715
%v77739 = vadd.f32 %v77735, %v77684
%v77743 = vmul.f32 %v77739, %v77715
%v77747 = vadd.f32 %v77743, %v77680
%v77751 = vmul.f32 %v77747, %v77715
%v77755 = vadd.f32 %v77751, %v77676
%v77759 = vmul.f32 %v77755, %v77715
%v77763 = vadd.f32 %v77759, %v77672
%v77767 = vmul.f32 %v77763, %v77715
%v77771 = vadd.f32 %v77767, %v77668
%v77775 = vmul.f32 %v77771, %v77715
%v77779 = vadd.f32 %v77775, %v77664
%v77783 = vmul.f32 %v77779, %v77630
%v77787 = vsel /*vm=*/%vm77635, /*on_true_vy=*/%v77640, /*on_false_vx=*/%v77783
%v77791 = vmul.f32 1.4140625, %v77787
%v77794 = vpack.c.bf16 %v120417, %v77791
%120159 = vst [vmem:[%s280 + $0x2d0] sm:$0xf] /*vst_source=*/%v77794
%v77798 = vadd.s32 %v75029, %v3329
%v77808 = vadd.s32 %v77798, %v415
%vm77812 = vcmp.lt.u32.totalorder %v77808, %v77798
%vm77817 = vcmp.lt.u32.totalorder %v77798, %v3329
%v77822 = vadd.s32 %v75012, %v3316
%v77826 = vadd.s32 1, %v77822
%v77830 = vsel /*vm=*/%vm77817, /*on_true_vy=*/%v77826, /*on_false_vx=*/%v77822
%v77834 = vadd.s32 1, %v77830
%v77838 = vsel /*vm=*/%vm77812, /*on_true_vy=*/%v77834, /*on_false_vx=*/%v77830
%v77843 = vadd.s32 %v77838, %v10
%v77847 = vadd.s32 %v77808, %v9
%v77851 = vadd.s32 %v77847, %v77843
%v77853 = vshll.u32 %v77847, 13
%v77854 = vshrl.u32 %v77847, 19
%v77855 = vor.u32 %v77854, %v77853
%v77856 = vxor.u32 %v77855, %v77851
%v77859 = vadd.s32 %v77856, %v77851
%v77861 = vshll.u32 %v77856, 15
%v77862 = vshrl.u32 %v77856, 17
%v77863 = vor.u32 %v77862, %v77861
%v77864 = vxor.u32 %v77863, %v77859
%v77867 = vadd.s32 %v77864, %v77859
%v77869 = vshll.u32 %v77864, 26
%v77870 = vshrl.u32 %v77864, 6
%v77871 = vor.u32 %v77870, %v77869
%v77872 = vxor.u32 %v77871, %v77867
%v77875 = vadd.s32 %v77872, %v77867
%v77879 = vadd.s32 %v77875, %v9
%v77881 = vshll.u32 %v77872, 6
%v77882 = vshrl.u32 %v77872, 26
%v77883 = vor.u32 %v77882, %v77881
%v77884 = vxor.u32 %v77883, %v77875
%v77887 = vadd.s32 %v77884, %v8
%v77891 = vadd.s32 1, %v77887
%v77895 = vadd.s32 %v77891, %v77879
%v77897 = vshll.u32 %v77891, 17
%v77898 = vshrl.u32 %v77891, 15
%v77899 = vor.u32 %v77898, %v77897
%v77900 = vxor.u32 %v77899, %v77895
%v77903 = vadd.s32 %v77900, %v77895
%v77905 = vshll.u32 %v77900, 29
%v77906 = vshrl.u32 %v77900, 3
%v77907 = vor.u32 %v77906, %v77905
%v77908 = vxor.u32 %v77907, %v77903
%v77911 = vadd.s32 %v77908, %v77903
%v77913 = vshll.u32 %v77908, 16
%v77914 = vshrl.u32 %v77908, 16
%v77915 = vor.u32 %v77914, %v77913
%v77916 = vxor.u32 %v77915, %v77911
%v77919 = vadd.s32 %v77916, %v77911
%v77923 = vadd.s32 %v77919, %v8
%v77925 = vshll.u32 %v77916, 24
%v77926 = vshrl.u32 %v77916, 8
%v77927 = vor.u32 %v77926, %v77925
%v77928 = vxor.u32 %v77927, %v77919
%v77931 = vadd.s32 %v77928, %v10
%v77935 = vadd.s32 2, %v77931
%v77939 = vadd.s32 %v77935, %v77923
%v77941 = vshll.u32 %v77935, 13
%v77942 = vshrl.u32 %v77935, 19
%v77943 = vor.u32 %v77942, %v77941
%v77944 = vxor.u32 %v77943, %v77939
%v77947 = vadd.s32 %v77944, %v77939
%v77949 = vshll.u32 %v77944, 15
%v77950 = vshrl.u32 %v77944, 17
%v77951 = vor.u32 %v77950, %v77949
%v77952 = vxor.u32 %v77951, %v77947
%v77955 = vadd.s32 %v77952, %v77947
%v77957 = vshll.u32 %v77952, 26
%v77958 = vshrl.u32 %v77952, 6
%v77959 = vor.u32 %v77958, %v77957
%v77960 = vxor.u32 %v77959, %v77955
%v77963 = vadd.s32 %v77960, %v77955
%v77967 = vadd.s32 %v77963, %v10
%v77969 = vshll.u32 %v77960, 6
%v77970 = vshrl.u32 %v77960, 26
%v77971 = vor.u32 %v77970, %v77969
%v77972 = vxor.u32 %v77971, %v77963
%v77975 = vadd.s32 %v77972, %v9
%v77979 = vadd.s32 3, %v77975
%v77983 = vadd.s32 %v77979, %v77967
%v77985 = vshll.u32 %v77979, 17
%v77986 = vshrl.u32 %v77979, 15
%v77987 = vor.u32 %v77986, %v77985
%v77988 = vxor.u32 %v77987, %v77983
%v77991 = vadd.s32 %v77988, %v77983
%v77993 = vshll.u32 %v77988, 29
%v77994 = vshrl.u32 %v77988, 3
%v77995 = vor.u32 %v77994, %v77993
%v77996 = vxor.u32 %v77995, %v77991
%v77999 = vadd.s32 %v77996, %v77991
%v78001 = vshll.u32 %v77996, 16
%v78002 = vshrl.u32 %v77996, 16
%v78003 = vor.u32 %v78002, %v78001
%v78004 = vxor.u32 %v78003, %v77999
%v78007 = vadd.s32 %v78004, %v77999
%v78011 = vadd.s32 %v78007, %v9
%v78013 = vshll.u32 %v78004, 24
%v78014 = vshrl.u32 %v78004, 8
%v78015 = vor.u32 %v78014, %v78013
%v78016 = vxor.u32 %v78015, %v78007
%v78019 = vadd.s32 %v78016, %v8
%v78023 = vadd.s32 4, %v78019
%v78027 = vadd.s32 %v78023, %v78011
%v78029 = vshll.u32 %v78023, 13
%v78030 = vshrl.u32 %v78023, 19
%v78031 = vor.u32 %v78030, %v78029
%v78032 = vxor.u32 %v78031, %v78027
%v78035 = vadd.s32 %v78032, %v78027
%v78037 = vshll.u32 %v78032, 15
%v78038 = vshrl.u32 %v78032, 17
%v78039 = vor.u32 %v78038, %v78037
%v78040 = vxor.u32 %v78039, %v78035
%v78043 = vadd.s32 %v78040, %v78035
%v78045 = vshll.u32 %v78040, 26
%v78046 = vshrl.u32 %v78040, 6
%v78047 = vor.u32 %v78046, %v78045
%v78048 = vxor.u32 %v78047, %v78043
%v78051 = vadd.s32 %v78048, %v78043
%v78055 = vadd.s32 %v78051, %v8
%v78057 = vshll.u32 %v78048, 6
%v78058 = vshrl.u32 %v78048, 26
%v78059 = vor.u32 %v78058, %v78057
%v78060 = vxor.u32 %v78059, %v78051
%v78063 = vadd.s32 %v78060, %v10
%v78067 = vadd.s32 5, %v78063
%v78069 = vxor.u32 %v78067, %v78055
%v78070 = vand.u32.u8 255, %v78069
%v78071 = vand.u32 65535, %v78070
%v78072 = vshrl.u32 %v78071, 1
%v78073 = vor.u32 16256, %v78072
%v78074 = vand.u32.u16 65535, %v78073
%v120160 = vadd.low.f32.bf16 -1.0, %v78074
%v78083 = vmul.f32 2.0, %v120160
%v78087 = vadd.f32 -0.99609375, %v78083
%v78091 = vmax.f32 %v78087, -0.99609375
%v78093 = vand.u32 2147483647, %v78091
%vm78096 = vcmp.eq.f32.partialorder %v78093, 1.0
%v78101 = vmul.f32 inf, %v78091
%v78103 = vxor.u32 2147483648, %v78091
%v78106 = vmul.f32 %v78103, %v78091
%v78108 = vadd.f32 1.0, %v78106
%v78109 = vlog2.pop %v78108
%v78110 = vmul.f32 0.6931472, %v78109
%v78111 = vmul.f32 -0.5, %v78106
%v78112 = vadd.f32 1.0, %v78111
%v78113 = vmul.f32 %v78112, %v78106
%v78114 = vand.u32 2147483647, %v78106
%vm78115 = vcmp.lt.f32.partialorder %v78114, 0.0004427343
%v78116 = vsel /*vm=*/%vm78115, /*on_true_vy=*/%v78113, /*on_false_vx=*/%v78110
%v78117 = vxor.u32 2147483648, %v78116
%vm78120 = vcmp.lt.f32.partialorder %v78117, 5.0
%v78125 = vsel /*vm=*/%vm78120, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v78129 = vsel /*vm=*/%vm78120, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v78133 = vsel /*vm=*/%vm78120, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v78137 = vsel /*vm=*/%vm78120, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v78141 = vsel /*vm=*/%vm78120, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v78145 = vsel /*vm=*/%vm78120, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v78149 = vsel /*vm=*/%vm78120, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v78153 = vsel /*vm=*/%vm78120, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v78157 = vsel /*vm=*/%vm78120, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v78161 = vadd.f32 -2.5, %v78117
%v78163 = vrsqrt.pop %v78117
%v78164 = vmul.f32 %v78163, %v78117
%vm78165 = vcmp.eq.f32.partialorder %v78117, inf
%v78166 = vsel /*vm=*/%vm78165, /*on_true_vy=*/%v78117, /*on_false_vx=*/%v78164
%vm78167 = vcmp.eq.f32.partialorder %v78117, 0.0
%v78168 = vand.u32 2147483648, %v78117
%v78169 = vsel /*vm=*/%vm78167, /*on_true_vy=*/%v78168, /*on_false_vx=*/%v78166
%v78172 = vadd.f32 -3.0, %v78169
%v78176 = vsel /*vm=*/%vm78120, /*on_true_vy=*/%v78161, /*on_false_vx=*/%v78172
%v78180 = vmul.f32 %v78176, %v78157
%v78184 = vadd.f32 %v78180, %v78153
%v78188 = vmul.f32 %v78184, %v78176
%v78192 = vadd.f32 %v78188, %v78149
%v78196 = vmul.f32 %v78192, %v78176
%v78200 = vadd.f32 %v78196, %v78145
%v78204 = vmul.f32 %v78200, %v78176
%v78208 = vadd.f32 %v78204, %v78141
%v78212 = vmul.f32 %v78208, %v78176
%v78216 = vadd.f32 %v78212, %v78137
%v78220 = vmul.f32 %v78216, %v78176
%v78224 = vadd.f32 %v78220, %v78133
%v78228 = vmul.f32 %v78224, %v78176
%v78232 = vadd.f32 %v78228, %v78129
%v78236 = vmul.f32 %v78232, %v78176
%v78240 = vadd.f32 %v78236, %v78125
%v78244 = vmul.f32 %v78240, %v78091
%v78248 = vsel /*vm=*/%vm78096, /*on_true_vy=*/%v78101, /*on_false_vx=*/%v78244
%v78252 = vmul.f32 1.4140625, %v78248
%v78255 = vpack.c.bf16 %v120417, %v78252
%120161 = vst [vmem:[%s280 + $0x350] sm:$0xf] /*vst_source=*/%v78255
%v78259 = vadd.s32 %v75029, %v3816
%v78269 = vadd.s32 %v78259, %v415
%vm78273 = vcmp.lt.u32.totalorder %v78269, %v78259
%vm78278 = vcmp.lt.u32.totalorder %v78259, %v3816
%v78283 = vadd.s32 %v75012, %v3803
%v78287 = vadd.s32 1, %v78283
%v78291 = vsel /*vm=*/%vm78278, /*on_true_vy=*/%v78287, /*on_false_vx=*/%v78283
%v78295 = vadd.s32 1, %v78291
%v78299 = vsel /*vm=*/%vm78273, /*on_true_vy=*/%v78295, /*on_false_vx=*/%v78291
%v78304 = vadd.s32 %v78299, %v10
%v78308 = vadd.s32 %v78269, %v9
%v78312 = vadd.s32 %v78308, %v78304
%v78314 = vshll.u32 %v78308, 13
%v78315 = vshrl.u32 %v78308, 19
%v78316 = vor.u32 %v78315, %v78314
%v78317 = vxor.u32 %v78316, %v78312
%v78320 = vadd.s32 %v78317, %v78312
%v78322 = vshll.u32 %v78317, 15
%v78323 = vshrl.u32 %v78317, 17
%v78324 = vor.u32 %v78323, %v78322
%v78325 = vxor.u32 %v78324, %v78320
%v78328 = vadd.s32 %v78325, %v78320
%v78330 = vshll.u32 %v78325, 26
%v78331 = vshrl.u32 %v78325, 6
%v78332 = vor.u32 %v78331, %v78330
%v78333 = vxor.u32 %v78332, %v78328
%v78336 = vadd.s32 %v78333, %v78328
%v78340 = vadd.s32 %v78336, %v9
%v78342 = vshll.u32 %v78333, 6
%v78343 = vshrl.u32 %v78333, 26
%v78344 = vor.u32 %v78343, %v78342
%v78345 = vxor.u32 %v78344, %v78336
%v78348 = vadd.s32 %v78345, %v8
%v78352 = vadd.s32 1, %v78348
%v78356 = vadd.s32 %v78352, %v78340
%v78358 = vshll.u32 %v78352, 17
%v78359 = vshrl.u32 %v78352, 15
%v78360 = vor.u32 %v78359, %v78358
%v78361 = vxor.u32 %v78360, %v78356
%v78364 = vadd.s32 %v78361, %v78356
%v78366 = vshll.u32 %v78361, 29
%v78367 = vshrl.u32 %v78361, 3
%v78368 = vor.u32 %v78367, %v78366
%v78369 = vxor.u32 %v78368, %v78364
%v78372 = vadd.s32 %v78369, %v78364
%v78374 = vshll.u32 %v78369, 16
%v78375 = vshrl.u32 %v78369, 16
%v78376 = vor.u32 %v78375, %v78374
%v78377 = vxor.u32 %v78376, %v78372
%v78380 = vadd.s32 %v78377, %v78372
%v78384 = vadd.s32 %v78380, %v8
%v78386 = vshll.u32 %v78377, 24
%v78387 = vshrl.u32 %v78377, 8
%v78388 = vor.u32 %v78387, %v78386
%v78389 = vxor.u32 %v78388, %v78380
%v78392 = vadd.s32 %v78389, %v10
%v78396 = vadd.s32 2, %v78392
%v78400 = vadd.s32 %v78396, %v78384
%v78402 = vshll.u32 %v78396, 13
%v78403 = vshrl.u32 %v78396, 19
%v78404 = vor.u32 %v78403, %v78402
%v78405 = vxor.u32 %v78404, %v78400
%v78408 = vadd.s32 %v78405, %v78400
%v78410 = vshll.u32 %v78405, 15
%v78411 = vshrl.u32 %v78405, 17
%v78412 = vor.u32 %v78411, %v78410
%v78413 = vxor.u32 %v78412, %v78408
%v78416 = vadd.s32 %v78413, %v78408
%v78418 = vshll.u32 %v78413, 26
%v78419 = vshrl.u32 %v78413, 6
%v78420 = vor.u32 %v78419, %v78418
%v78421 = vxor.u32 %v78420, %v78416
%v78424 = vadd.s32 %v78421, %v78416
%v78428 = vadd.s32 %v78424, %v10
%v78430 = vshll.u32 %v78421, 6
%v78431 = vshrl.u32 %v78421, 26
%v78432 = vor.u32 %v78431, %v78430
%v78433 = vxor.u32 %v78432, %v78424
%v78436 = vadd.s32 %v78433, %v9
%v78440 = vadd.s32 3, %v78436
%v78444 = vadd.s32 %v78440, %v78428
%v78446 = vshll.u32 %v78440, 17
%v78447 = vshrl.u32 %v78440, 15
%v78448 = vor.u32 %v78447, %v78446
%v78449 = vxor.u32 %v78448, %v78444
%v78452 = vadd.s32 %v78449, %v78444
%v78454 = vshll.u32 %v78449, 29
%v78455 = vshrl.u32 %v78449, 3
%v78456 = vor.u32 %v78455, %v78454
%v78457 = vxor.u32 %v78456, %v78452
%v78460 = vadd.s32 %v78457, %v78452
%v78462 = vshll.u32 %v78457, 16
%v78463 = vshrl.u32 %v78457, 16
%v78464 = vor.u32 %v78463, %v78462
%v78465 = vxor.u32 %v78464, %v78460
%v78468 = vadd.s32 %v78465, %v78460
%v78472 = vadd.s32 %v78468, %v9
%v78474 = vshll.u32 %v78465, 24
%v78475 = vshrl.u32 %v78465, 8
%v78476 = vor.u32 %v78475, %v78474
%v78477 = vxor.u32 %v78476, %v78468
%v78480 = vadd.s32 %v78477, %v8
%v78484 = vadd.s32 4, %v78480
%v78488 = vadd.s32 %v78484, %v78472
%v78490 = vshll.u32 %v78484, 13
%v78491 = vshrl.u32 %v78484, 19
%v78492 = vor.u32 %v78491, %v78490
%v78493 = vxor.u32 %v78492, %v78488
%v78496 = vadd.s32 %v78493, %v78488
%v78498 = vshll.u32 %v78493, 15
%v78499 = vshrl.u32 %v78493, 17
%v78500 = vor.u32 %v78499, %v78498
%v78501 = vxor.u32 %v78500, %v78496
%v78504 = vadd.s32 %v78501, %v78496
%v78506 = vshll.u32 %v78501, 26
%v78507 = vshrl.u32 %v78501, 6
%v78508 = vor.u32 %v78507, %v78506
%v78509 = vxor.u32 %v78508, %v78504
%v78512 = vadd.s32 %v78509, %v78504
%v78516 = vadd.s32 %v78512, %v8
%v78518 = vshll.u32 %v78509, 6
%v78519 = vshrl.u32 %v78509, 26
%v78520 = vor.u32 %v78519, %v78518
%v78521 = vxor.u32 %v78520, %v78512
%v78524 = vadd.s32 %v78521, %v10
%v78528 = vadd.s32 5, %v78524
%v78530 = vxor.u32 %v78528, %v78516
%v78531 = vand.u32.u8 255, %v78530
%v78532 = vand.u32 65535, %v78531
%v78533 = vshrl.u32 %v78532, 1
%v78534 = vor.u32 16256, %v78533
%v78535 = vand.u32.u16 65535, %v78534
%v120162 = vadd.low.f32.bf16 -1.0, %v78535
%v78544 = vmul.f32 2.0, %v120162
%v78548 = vadd.f32 -0.99609375, %v78544
%v78552 = vmax.f32 %v78548, -0.99609375
%v78554 = vand.u32 2147483647, %v78552
%vm78557 = vcmp.eq.f32.partialorder %v78554, 1.0
%v78562 = vmul.f32 inf, %v78552
%v78564 = vxor.u32 2147483648, %v78552
%v78567 = vmul.f32 %v78564, %v78552
%v78569 = vadd.f32 1.0, %v78567
%v78570 = vlog2.pop %v78569
%v78571 = vmul.f32 0.6931472, %v78570
%v78572 = vmul.f32 -0.5, %v78567
%v78573 = vadd.f32 1.0, %v78572
%v78574 = vmul.f32 %v78573, %v78567
%v78575 = vand.u32 2147483647, %v78567
%vm78576 = vcmp.lt.f32.partialorder %v78575, 0.0004427343
%v78577 = vsel /*vm=*/%vm78576, /*on_true_vy=*/%v78574, /*on_false_vx=*/%v78571
%v78578 = vxor.u32 2147483648, %v78577
%vm78581 = vcmp.lt.f32.partialorder %v78578, 5.0
%v78586 = vsel /*vm=*/%vm78581, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v78590 = vsel /*vm=*/%vm78581, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v78594 = vsel /*vm=*/%vm78581, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v78598 = vsel /*vm=*/%vm78581, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v78602 = vsel /*vm=*/%vm78581, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v78606 = vsel /*vm=*/%vm78581, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v78610 = vsel /*vm=*/%vm78581, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v78614 = vsel /*vm=*/%vm78581, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v78618 = vsel /*vm=*/%vm78581, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v78622 = vadd.f32 -2.5, %v78578
%v78624 = vrsqrt.pop %v78578
%v78625 = vmul.f32 %v78624, %v78578
%vm78626 = vcmp.eq.f32.partialorder %v78578, inf
%v78627 = vsel /*vm=*/%vm78626, /*on_true_vy=*/%v78578, /*on_false_vx=*/%v78625
%vm78628 = vcmp.eq.f32.partialorder %v78578, 0.0
%v78629 = vand.u32 2147483648, %v78578
%v78630 = vsel /*vm=*/%vm78628, /*on_true_vy=*/%v78629, /*on_false_vx=*/%v78627
%v78633 = vadd.f32 -3.0, %v78630
%v78637 = vsel /*vm=*/%vm78581, /*on_true_vy=*/%v78622, /*on_false_vx=*/%v78633
%v78641 = vmul.f32 %v78637, %v78618
%v78645 = vadd.f32 %v78641, %v78614
%v78649 = vmul.f32 %v78645, %v78637
%v78653 = vadd.f32 %v78649, %v78610
%v78657 = vmul.f32 %v78653, %v78637
%v78661 = vadd.f32 %v78657, %v78606
%v78665 = vmul.f32 %v78661, %v78637
%v78669 = vadd.f32 %v78665, %v78602
%v78673 = vmul.f32 %v78669, %v78637
%v78677 = vadd.f32 %v78673, %v78598
%v78681 = vmul.f32 %v78677, %v78637
%v78685 = vadd.f32 %v78681, %v78594
%v78689 = vmul.f32 %v78685, %v78637
%v78693 = vadd.f32 %v78689, %v78590
%v78697 = vmul.f32 %v78693, %v78637
%v78701 = vadd.f32 %v78697, %v78586
%v78705 = vmul.f32 %v78701, %v78552
%v78709 = vsel /*vm=*/%vm78557, /*on_true_vy=*/%v78562, /*on_false_vx=*/%v78705
%v78713 = vmul.f32 1.4140625, %v78709
%v78716 = vpack.c.bf16 %v120417, %v78713
%120163 = vst [vmem:[%s280 + $0x3d0] sm:$0xf] /*vst_source=*/%v78716
%v78754 = vadd.s32 %v78751, %v408
%v78764 = vadd.s32 %v78754, %v415
%vm78768 = vcmp.lt.u32.totalorder %v78764, %v78754
%vm78773 = vcmp.lt.u32.totalorder %v78754, %v408
%v78778 = vadd.s32 %v78734, %v380
%v78782 = vadd.s32 1, %v78778
%v78786 = vsel /*vm=*/%vm78773, /*on_true_vy=*/%v78782, /*on_false_vx=*/%v78778
%v78790 = vadd.s32 1, %v78786
%v78794 = vsel /*vm=*/%vm78768, /*on_true_vy=*/%v78790, /*on_false_vx=*/%v78786
%v78799 = vadd.s32 %v78794, %v10
%v78803 = vadd.s32 %v78764, %v9
%v78807 = vadd.s32 %v78803, %v78799
%v78809 = vshll.u32 %v78803, 13
%v78810 = vshrl.u32 %v78803, 19
%v78811 = vor.u32 %v78810, %v78809
%v78812 = vxor.u32 %v78811, %v78807
%v78815 = vadd.s32 %v78812, %v78807
%v78817 = vshll.u32 %v78812, 15
%v78818 = vshrl.u32 %v78812, 17
%v78819 = vor.u32 %v78818, %v78817
%v78820 = vxor.u32 %v78819, %v78815
%v78823 = vadd.s32 %v78820, %v78815
%v78825 = vshll.u32 %v78820, 26
%v78826 = vshrl.u32 %v78820, 6
%v78827 = vor.u32 %v78826, %v78825
%v78828 = vxor.u32 %v78827, %v78823
%v78831 = vadd.s32 %v78828, %v78823
%v78835 = vadd.s32 %v78831, %v9
%v78837 = vshll.u32 %v78828, 6
%v78838 = vshrl.u32 %v78828, 26
%v78839 = vor.u32 %v78838, %v78837
%v78840 = vxor.u32 %v78839, %v78831
%v78843 = vadd.s32 %v78840, %v8
%v78847 = vadd.s32 1, %v78843
%v78851 = vadd.s32 %v78847, %v78835
%v78853 = vshll.u32 %v78847, 17
%v78854 = vshrl.u32 %v78847, 15
%v78855 = vor.u32 %v78854, %v78853
%v78856 = vxor.u32 %v78855, %v78851
%v78859 = vadd.s32 %v78856, %v78851
%v78861 = vshll.u32 %v78856, 29
%v78862 = vshrl.u32 %v78856, 3
%v78863 = vor.u32 %v78862, %v78861
%v78864 = vxor.u32 %v78863, %v78859
%v78867 = vadd.s32 %v78864, %v78859
%v78869 = vshll.u32 %v78864, 16
%v78870 = vshrl.u32 %v78864, 16
%v78871 = vor.u32 %v78870, %v78869
%v78872 = vxor.u32 %v78871, %v78867
%v78875 = vadd.s32 %v78872, %v78867
%v78879 = vadd.s32 %v78875, %v8
%v78881 = vshll.u32 %v78872, 24
%v78882 = vshrl.u32 %v78872, 8
%v78883 = vor.u32 %v78882, %v78881
%v78884 = vxor.u32 %v78883, %v78875
%v78887 = vadd.s32 %v78884, %v10
%v78891 = vadd.s32 2, %v78887
%v78895 = vadd.s32 %v78891, %v78879
%v78897 = vshll.u32 %v78891, 13
%v78898 = vshrl.u32 %v78891, 19
%v78899 = vor.u32 %v78898, %v78897
%v78900 = vxor.u32 %v78899, %v78895
%v78903 = vadd.s32 %v78900, %v78895
%v78905 = vshll.u32 %v78900, 15
%v78906 = vshrl.u32 %v78900, 17
%v78907 = vor.u32 %v78906, %v78905
%v78908 = vxor.u32 %v78907, %v78903
%v78911 = vadd.s32 %v78908, %v78903
%v78913 = vshll.u32 %v78908, 26
%v78914 = vshrl.u32 %v78908, 6
%v78915 = vor.u32 %v78914, %v78913
%v78916 = vxor.u32 %v78915, %v78911
%v78919 = vadd.s32 %v78916, %v78911
%v78923 = vadd.s32 %v78919, %v10
%v78925 = vshll.u32 %v78916, 6
%v78926 = vshrl.u32 %v78916, 26
%v78927 = vor.u32 %v78926, %v78925
%v78928 = vxor.u32 %v78927, %v78919
%v78931 = vadd.s32 %v78928, %v9
%v78935 = vadd.s32 3, %v78931
%v78939 = vadd.s32 %v78935, %v78923
%v78941 = vshll.u32 %v78935, 17
%v78942 = vshrl.u32 %v78935, 15
%v78943 = vor.u32 %v78942, %v78941
%v78944 = vxor.u32 %v78943, %v78939
%v78947 = vadd.s32 %v78944, %v78939
%v78949 = vshll.u32 %v78944, 29
%v78950 = vshrl.u32 %v78944, 3
%v78951 = vor.u32 %v78950, %v78949
%v78952 = vxor.u32 %v78951, %v78947
%v78955 = vadd.s32 %v78952, %v78947
%v78957 = vshll.u32 %v78952, 16
%v78958 = vshrl.u32 %v78952, 16
%v78959 = vor.u32 %v78958, %v78957
%v78960 = vxor.u32 %v78959, %v78955
%v78963 = vadd.s32 %v78960, %v78955
%v78967 = vadd.s32 %v78963, %v9
%v78969 = vshll.u32 %v78960, 24
%v78970 = vshrl.u32 %v78960, 8
%v78971 = vor.u32 %v78970, %v78969
%v78972 = vxor.u32 %v78971, %v78963
%v78975 = vadd.s32 %v78972, %v8
%v78979 = vadd.s32 4, %v78975
%v78983 = vadd.s32 %v78979, %v78967
%v78985 = vshll.u32 %v78979, 13
%v78986 = vshrl.u32 %v78979, 19
%v78987 = vor.u32 %v78986, %v78985
%v78988 = vxor.u32 %v78987, %v78983
%v78991 = vadd.s32 %v78988, %v78983
%v78993 = vshll.u32 %v78988, 15
%v78994 = vshrl.u32 %v78988, 17
%v78995 = vor.u32 %v78994, %v78993
%v78996 = vxor.u32 %v78995, %v78991
%v78999 = vadd.s32 %v78996, %v78991
%v79001 = vshll.u32 %v78996, 26
%v79002 = vshrl.u32 %v78996, 6
%v79003 = vor.u32 %v79002, %v79001
%v79004 = vxor.u32 %v79003, %v78999
%v79007 = vadd.s32 %v79004, %v78999
%v79011 = vadd.s32 %v79007, %v8
%v79013 = vshll.u32 %v79004, 6
%v79014 = vshrl.u32 %v79004, 26
%v79015 = vor.u32 %v79014, %v79013
%v79016 = vxor.u32 %v79015, %v79007
%v79019 = vadd.s32 %v79016, %v10
%v79023 = vadd.s32 5, %v79019
%v79025 = vxor.u32 %v79023, %v79011
%v79026 = vand.u32.u8 255, %v79025
%v79027 = vand.u32 65535, %v79026
%v79028 = vshrl.u32 %v79027, 1
%v79029 = vor.u32 16256, %v79028
%v79030 = vand.u32.u16 65535, %v79029
%v120168 = vadd.low.f32.bf16 -1.0, %v79030
%v79039 = vmul.f32 2.0, %v120168
%v79043 = vadd.f32 -0.99609375, %v79039
%v79047 = vmax.f32 %v79043, -0.99609375
%v79049 = vand.u32 2147483647, %v79047
%vm79052 = vcmp.eq.f32.partialorder %v79049, 1.0
%v79057 = vmul.f32 inf, %v79047
%v79059 = vxor.u32 2147483648, %v79047
%v79062 = vmul.f32 %v79059, %v79047
%v79064 = vadd.f32 1.0, %v79062
%v79065 = vlog2.pop %v79064
%v79066 = vmul.f32 0.6931472, %v79065
%v79067 = vmul.f32 -0.5, %v79062
%v79068 = vadd.f32 1.0, %v79067
%v79069 = vmul.f32 %v79068, %v79062
%v79070 = vand.u32 2147483647, %v79062
%vm79071 = vcmp.lt.f32.partialorder %v79070, 0.0004427343
%v79072 = vsel /*vm=*/%vm79071, /*on_true_vy=*/%v79069, /*on_false_vx=*/%v79066
%v79073 = vxor.u32 2147483648, %v79072
%vm79076 = vcmp.lt.f32.partialorder %v79073, 5.0
%v79081 = vsel /*vm=*/%vm79076, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v79085 = vsel /*vm=*/%vm79076, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v79089 = vsel /*vm=*/%vm79076, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v79093 = vsel /*vm=*/%vm79076, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v79097 = vsel /*vm=*/%vm79076, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v79101 = vsel /*vm=*/%vm79076, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v79105 = vsel /*vm=*/%vm79076, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v79109 = vsel /*vm=*/%vm79076, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v79113 = vsel /*vm=*/%vm79076, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v79117 = vadd.f32 -2.5, %v79073
%v79119 = vrsqrt.pop %v79073
%v79120 = vmul.f32 %v79119, %v79073
%vm79121 = vcmp.eq.f32.partialorder %v79073, inf
%v79122 = vsel /*vm=*/%vm79121, /*on_true_vy=*/%v79073, /*on_false_vx=*/%v79120
%vm79123 = vcmp.eq.f32.partialorder %v79073, 0.0
%v79124 = vand.u32 2147483648, %v79073
%v79125 = vsel /*vm=*/%vm79123, /*on_true_vy=*/%v79124, /*on_false_vx=*/%v79122
%v79128 = vadd.f32 -3.0, %v79125
%v79132 = vsel /*vm=*/%vm79076, /*on_true_vy=*/%v79117, /*on_false_vx=*/%v79128
%v79136 = vmul.f32 %v79132, %v79113
%v79140 = vadd.f32 %v79136, %v79109
%v79144 = vmul.f32 %v79140, %v79132
%v79148 = vadd.f32 %v79144, %v79105
%v79152 = vmul.f32 %v79148, %v79132
%v79156 = vadd.f32 %v79152, %v79101
%v79160 = vmul.f32 %v79156, %v79132
%v79164 = vadd.f32 %v79160, %v79097
%v79168 = vmul.f32 %v79164, %v79132
%v79172 = vadd.f32 %v79168, %v79093
%v79176 = vmul.f32 %v79172, %v79132
%v79180 = vadd.f32 %v79176, %v79089
%v79184 = vmul.f32 %v79180, %v79132
%v79188 = vadd.f32 %v79184, %v79085
%v79192 = vmul.f32 %v79188, %v79132
%v79196 = vadd.f32 %v79192, %v79081
%v79200 = vmul.f32 %v79196, %v79047
%v79204 = vsel /*vm=*/%vm79052, /*on_true_vy=*/%v79057, /*on_false_vx=*/%v79200
%v79208 = vmul.f32 1.4140625, %v79204
%v79211 = vpack.c.bf16 %v120417, %v79208
%120169 = vst [vmem:[%s280 + $0x54] sm:$0xf] /*vst_source=*/%v79211
%v79215 = vadd.s32 %v78751, %v894
%v79225 = vadd.s32 %v79215, %v415
%vm79229 = vcmp.lt.u32.totalorder %v79225, %v79215
%vm79234 = vcmp.lt.u32.totalorder %v79215, %v894
%v79239 = vadd.s32 %v78734, %v881
%v79243 = vadd.s32 1, %v79239
%v79247 = vsel /*vm=*/%vm79234, /*on_true_vy=*/%v79243, /*on_false_vx=*/%v79239
%v79251 = vadd.s32 1, %v79247
%v79255 = vsel /*vm=*/%vm79229, /*on_true_vy=*/%v79251, /*on_false_vx=*/%v79247
%v79260 = vadd.s32 %v79255, %v10
%v79264 = vadd.s32 %v79225, %v9
%v79268 = vadd.s32 %v79264, %v79260
%v79270 = vshll.u32 %v79264, 13
%v79271 = vshrl.u32 %v79264, 19
%v79272 = vor.u32 %v79271, %v79270
%v79273 = vxor.u32 %v79272, %v79268
%v79276 = vadd.s32 %v79273, %v79268
%v79278 = vshll.u32 %v79273, 15
%v79279 = vshrl.u32 %v79273, 17
%v79280 = vor.u32 %v79279, %v79278
%v79281 = vxor.u32 %v79280, %v79276
%v79284 = vadd.s32 %v79281, %v79276
%v79286 = vshll.u32 %v79281, 26
%v79287 = vshrl.u32 %v79281, 6
%v79288 = vor.u32 %v79287, %v79286
%v79289 = vxor.u32 %v79288, %v79284
%v79292 = vadd.s32 %v79289, %v79284
%v79296 = vadd.s32 %v79292, %v9
%v79298 = vshll.u32 %v79289, 6
%v79299 = vshrl.u32 %v79289, 26
%v79300 = vor.u32 %v79299, %v79298
%v79301 = vxor.u32 %v79300, %v79292
%v79304 = vadd.s32 %v79301, %v8
%v79308 = vadd.s32 1, %v79304
%v79312 = vadd.s32 %v79308, %v79296
%v79314 = vshll.u32 %v79308, 17
%v79315 = vshrl.u32 %v79308, 15
%v79316 = vor.u32 %v79315, %v79314
%v79317 = vxor.u32 %v79316, %v79312
%v79320 = vadd.s32 %v79317, %v79312
%v79322 = vshll.u32 %v79317, 29
%v79323 = vshrl.u32 %v79317, 3
%v79324 = vor.u32 %v79323, %v79322
%v79325 = vxor.u32 %v79324, %v79320
%v79328 = vadd.s32 %v79325, %v79320
%v79330 = vshll.u32 %v79325, 16
%v79331 = vshrl.u32 %v79325, 16
%v79332 = vor.u32 %v79331, %v79330
%v79333 = vxor.u32 %v79332, %v79328
%v79336 = vadd.s32 %v79333, %v79328
%v79340 = vadd.s32 %v79336, %v8
%v79342 = vshll.u32 %v79333, 24
%v79343 = vshrl.u32 %v79333, 8
%v79344 = vor.u32 %v79343, %v79342
%v79345 = vxor.u32 %v79344, %v79336
%v79348 = vadd.s32 %v79345, %v10
%v79352 = vadd.s32 2, %v79348
%v79356 = vadd.s32 %v79352, %v79340
%v79358 = vshll.u32 %v79352, 13
%v79359 = vshrl.u32 %v79352, 19
%v79360 = vor.u32 %v79359, %v79358
%v79361 = vxor.u32 %v79360, %v79356
%v79364 = vadd.s32 %v79361, %v79356
%v79366 = vshll.u32 %v79361, 15
%v79367 = vshrl.u32 %v79361, 17
%v79368 = vor.u32 %v79367, %v79366
%v79369 = vxor.u32 %v79368, %v79364
%v79372 = vadd.s32 %v79369, %v79364
%v79374 = vshll.u32 %v79369, 26
%v79375 = vshrl.u32 %v79369, 6
%v79376 = vor.u32 %v79375, %v79374
%v79377 = vxor.u32 %v79376, %v79372
%v79380 = vadd.s32 %v79377, %v79372
%v79384 = vadd.s32 %v79380, %v10
%v79386 = vshll.u32 %v79377, 6
%v79387 = vshrl.u32 %v79377, 26
%v79388 = vor.u32 %v79387, %v79386
%v79389 = vxor.u32 %v79388, %v79380
%v79392 = vadd.s32 %v79389, %v9
%v79396 = vadd.s32 3, %v79392
%v79400 = vadd.s32 %v79396, %v79384
%v79402 = vshll.u32 %v79396, 17
%v79403 = vshrl.u32 %v79396, 15
%v79404 = vor.u32 %v79403, %v79402
%v79405 = vxor.u32 %v79404, %v79400
%v79408 = vadd.s32 %v79405, %v79400
%v79410 = vshll.u32 %v79405, 29
%v79411 = vshrl.u32 %v79405, 3
%v79412 = vor.u32 %v79411, %v79410
%v79413 = vxor.u32 %v79412, %v79408
%v79416 = vadd.s32 %v79413, %v79408
%v79418 = vshll.u32 %v79413, 16
%v79419 = vshrl.u32 %v79413, 16
%v79420 = vor.u32 %v79419, %v79418
%v79421 = vxor.u32 %v79420, %v79416
%v79424 = vadd.s32 %v79421, %v79416
%v79428 = vadd.s32 %v79424, %v9
%v79430 = vshll.u32 %v79421, 24
%v79431 = vshrl.u32 %v79421, 8
%v79432 = vor.u32 %v79431, %v79430
%v79433 = vxor.u32 %v79432, %v79424
%v79436 = vadd.s32 %v79433, %v8
%v79440 = vadd.s32 4, %v79436
%v79444 = vadd.s32 %v79440, %v79428
%v79446 = vshll.u32 %v79440, 13
%v79447 = vshrl.u32 %v79440, 19
%v79448 = vor.u32 %v79447, %v79446
%v79449 = vxor.u32 %v79448, %v79444
%v79452 = vadd.s32 %v79449, %v79444
%v79454 = vshll.u32 %v79449, 15
%v79455 = vshrl.u32 %v79449, 17
%v79456 = vor.u32 %v79455, %v79454
%v79457 = vxor.u32 %v79456, %v79452
%v79460 = vadd.s32 %v79457, %v79452
%v79462 = vshll.u32 %v79457, 26
%v79463 = vshrl.u32 %v79457, 6
%v79464 = vor.u32 %v79463, %v79462
%v79465 = vxor.u32 %v79464, %v79460
%v79468 = vadd.s32 %v79465, %v79460
%v79472 = vadd.s32 %v79468, %v8
%v79474 = vshll.u32 %v79465, 6
%v79475 = vshrl.u32 %v79465, 26
%v79476 = vor.u32 %v79475, %v79474
%v79477 = vxor.u32 %v79476, %v79468
%v79480 = vadd.s32 %v79477, %v10
%v79484 = vadd.s32 5, %v79480
%v79486 = vxor.u32 %v79484, %v79472
%v79487 = vand.u32.u8 255, %v79486
%v79488 = vand.u32 65535, %v79487
%v79489 = vshrl.u32 %v79488, 1
%v79490 = vor.u32 16256, %v79489
%v79491 = vand.u32.u16 65535, %v79490
%v120170 = vadd.low.f32.bf16 -1.0, %v79491
%v79500 = vmul.f32 2.0, %v120170
%v79504 = vadd.f32 -0.99609375, %v79500
%v79508 = vmax.f32 %v79504, -0.99609375
%v79510 = vand.u32 2147483647, %v79508
%vm79513 = vcmp.eq.f32.partialorder %v79510, 1.0
%v79518 = vmul.f32 inf, %v79508
%v79520 = vxor.u32 2147483648, %v79508
%v79523 = vmul.f32 %v79520, %v79508
%v79525 = vadd.f32 1.0, %v79523
%v79526 = vlog2.pop %v79525
%v79527 = vmul.f32 0.6931472, %v79526
%v79528 = vmul.f32 -0.5, %v79523
%v79529 = vadd.f32 1.0, %v79528
%v79530 = vmul.f32 %v79529, %v79523
%v79531 = vand.u32 2147483647, %v79523
%vm79532 = vcmp.lt.f32.partialorder %v79531, 0.0004427343
%v79533 = vsel /*vm=*/%vm79532, /*on_true_vy=*/%v79530, /*on_false_vx=*/%v79527
%v79534 = vxor.u32 2147483648, %v79533
%vm79537 = vcmp.lt.f32.partialorder %v79534, 5.0
%v79542 = vsel /*vm=*/%vm79537, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v79546 = vsel /*vm=*/%vm79537, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v79550 = vsel /*vm=*/%vm79537, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v79554 = vsel /*vm=*/%vm79537, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v79558 = vsel /*vm=*/%vm79537, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v79562 = vsel /*vm=*/%vm79537, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v79566 = vsel /*vm=*/%vm79537, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v79570 = vsel /*vm=*/%vm79537, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v79574 = vsel /*vm=*/%vm79537, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v79578 = vadd.f32 -2.5, %v79534
%v79580 = vrsqrt.pop %v79534
%v79581 = vmul.f32 %v79580, %v79534
%vm79582 = vcmp.eq.f32.partialorder %v79534, inf
%v79583 = vsel /*vm=*/%vm79582, /*on_true_vy=*/%v79534, /*on_false_vx=*/%v79581
%vm79584 = vcmp.eq.f32.partialorder %v79534, 0.0
%v79585 = vand.u32 2147483648, %v79534
%v79586 = vsel /*vm=*/%vm79584, /*on_true_vy=*/%v79585, /*on_false_vx=*/%v79583
%v79589 = vadd.f32 -3.0, %v79586
%v79593 = vsel /*vm=*/%vm79537, /*on_true_vy=*/%v79578, /*on_false_vx=*/%v79589
%v79597 = vmul.f32 %v79593, %v79574
%v79601 = vadd.f32 %v79597, %v79570
%v79605 = vmul.f32 %v79601, %v79593
%v79609 = vadd.f32 %v79605, %v79566
%v79613 = vmul.f32 %v79609, %v79593
%v79617 = vadd.f32 %v79613, %v79562
%v79621 = vmul.f32 %v79617, %v79593
%v79625 = vadd.f32 %v79621, %v79558
%v79629 = vmul.f32 %v79625, %v79593
%v79633 = vadd.f32 %v79629, %v79554
%v79637 = vmul.f32 %v79633, %v79593
%v79641 = vadd.f32 %v79637, %v79550
%v79645 = vmul.f32 %v79641, %v79593
%v79649 = vadd.f32 %v79645, %v79546
%v79653 = vmul.f32 %v79649, %v79593
%v79657 = vadd.f32 %v79653, %v79542
%v79661 = vmul.f32 %v79657, %v79508
%v79665 = vsel /*vm=*/%vm79513, /*on_true_vy=*/%v79518, /*on_false_vx=*/%v79661
%v79669 = vmul.f32 1.4140625, %v79665
%v79672 = vpack.c.bf16 %v120417, %v79669
%120171 = vst [vmem:[%s280 + $0xd4] sm:$0xf] /*vst_source=*/%v79672
%v79676 = vadd.s32 %v78751, %v1381
%v79686 = vadd.s32 %v79676, %v415
%vm79690 = vcmp.lt.u32.totalorder %v79686, %v79676
%vm79695 = vcmp.lt.u32.totalorder %v79676, %v1381
%v79700 = vadd.s32 %v78734, %v1368
%v79704 = vadd.s32 1, %v79700
%v79708 = vsel /*vm=*/%vm79695, /*on_true_vy=*/%v79704, /*on_false_vx=*/%v79700
%v79712 = vadd.s32 1, %v79708
%v79716 = vsel /*vm=*/%vm79690, /*on_true_vy=*/%v79712, /*on_false_vx=*/%v79708
%v79721 = vadd.s32 %v79716, %v10
%v79725 = vadd.s32 %v79686, %v9
%v79729 = vadd.s32 %v79725, %v79721
%v79731 = vshll.u32 %v79725, 13
%v79732 = vshrl.u32 %v79725, 19
%v79733 = vor.u32 %v79732, %v79731
%v79734 = vxor.u32 %v79733, %v79729
%v79737 = vadd.s32 %v79734, %v79729
%v79739 = vshll.u32 %v79734, 15
%v79740 = vshrl.u32 %v79734, 17
%v79741 = vor.u32 %v79740, %v79739
%v79742 = vxor.u32 %v79741, %v79737
%v79745 = vadd.s32 %v79742, %v79737
%v79747 = vshll.u32 %v79742, 26
%v79748 = vshrl.u32 %v79742, 6
%v79749 = vor.u32 %v79748, %v79747
%v79750 = vxor.u32 %v79749, %v79745
%v79753 = vadd.s32 %v79750, %v79745
%v79757 = vadd.s32 %v79753, %v9
%v79759 = vshll.u32 %v79750, 6
%v79760 = vshrl.u32 %v79750, 26
%v79761 = vor.u32 %v79760, %v79759
%v79762 = vxor.u32 %v79761, %v79753
%v79765 = vadd.s32 %v79762, %v8
%v79769 = vadd.s32 1, %v79765
%v79773 = vadd.s32 %v79769, %v79757
%v79775 = vshll.u32 %v79769, 17
%v79776 = vshrl.u32 %v79769, 15
%v79777 = vor.u32 %v79776, %v79775
%v79778 = vxor.u32 %v79777, %v79773
%v79781 = vadd.s32 %v79778, %v79773
%v79783 = vshll.u32 %v79778, 29
%v79784 = vshrl.u32 %v79778, 3
%v79785 = vor.u32 %v79784, %v79783
%v79786 = vxor.u32 %v79785, %v79781
%v79789 = vadd.s32 %v79786, %v79781
%v79791 = vshll.u32 %v79786, 16
%v79792 = vshrl.u32 %v79786, 16
%v79793 = vor.u32 %v79792, %v79791
%v79794 = vxor.u32 %v79793, %v79789
%v79797 = vadd.s32 %v79794, %v79789
%v79801 = vadd.s32 %v79797, %v8
%v79803 = vshll.u32 %v79794, 24
%v79804 = vshrl.u32 %v79794, 8
%v79805 = vor.u32 %v79804, %v79803
%v79806 = vxor.u32 %v79805, %v79797
%v79809 = vadd.s32 %v79806, %v10
%v79813 = vadd.s32 2, %v79809
%v79817 = vadd.s32 %v79813, %v79801
%v79819 = vshll.u32 %v79813, 13
%v79820 = vshrl.u32 %v79813, 19
%v79821 = vor.u32 %v79820, %v79819
%v79822 = vxor.u32 %v79821, %v79817
%v79825 = vadd.s32 %v79822, %v79817
%v79827 = vshll.u32 %v79822, 15
%v79828 = vshrl.u32 %v79822, 17
%v79829 = vor.u32 %v79828, %v79827
%v79830 = vxor.u32 %v79829, %v79825
%v79833 = vadd.s32 %v79830, %v79825
%v79835 = vshll.u32 %v79830, 26
%v79836 = vshrl.u32 %v79830, 6
%v79837 = vor.u32 %v79836, %v79835
%v79838 = vxor.u32 %v79837, %v79833
%v79841 = vadd.s32 %v79838, %v79833
%v79845 = vadd.s32 %v79841, %v10
%v79847 = vshll.u32 %v79838, 6
%v79848 = vshrl.u32 %v79838, 26
%v79849 = vor.u32 %v79848, %v79847
%v79850 = vxor.u32 %v79849, %v79841
%v79853 = vadd.s32 %v79850, %v9
%v79857 = vadd.s32 3, %v79853
%v79861 = vadd.s32 %v79857, %v79845
%v79863 = vshll.u32 %v79857, 17
%v79864 = vshrl.u32 %v79857, 15
%v79865 = vor.u32 %v79864, %v79863
%v79866 = vxor.u32 %v79865, %v79861
%v79869 = vadd.s32 %v79866, %v79861
%v79871 = vshll.u32 %v79866, 29
%v79872 = vshrl.u32 %v79866, 3
%v79873 = vor.u32 %v79872, %v79871
%v79874 = vxor.u32 %v79873, %v79869
%v79877 = vadd.s32 %v79874, %v79869
%v79879 = vshll.u32 %v79874, 16
%v79880 = vshrl.u32 %v79874, 16
%v79881 = vor.u32 %v79880, %v79879
%v79882 = vxor.u32 %v79881, %v79877
%v79885 = vadd.s32 %v79882, %v79877
%v79889 = vadd.s32 %v79885, %v9
%v79891 = vshll.u32 %v79882, 24
%v79892 = vshrl.u32 %v79882, 8
%v79893 = vor.u32 %v79892, %v79891
%v79894 = vxor.u32 %v79893, %v79885
%v79897 = vadd.s32 %v79894, %v8
%v79901 = vadd.s32 4, %v79897
%v79905 = vadd.s32 %v79901, %v79889
%v79907 = vshll.u32 %v79901, 13
%v79908 = vshrl.u32 %v79901, 19
%v79909 = vor.u32 %v79908, %v79907
%v79910 = vxor.u32 %v79909, %v79905
%v79913 = vadd.s32 %v79910, %v79905
%v79915 = vshll.u32 %v79910, 15
%v79916 = vshrl.u32 %v79910, 17
%v79917 = vor.u32 %v79916, %v79915
%v79918 = vxor.u32 %v79917, %v79913
%v79921 = vadd.s32 %v79918, %v79913
%v79923 = vshll.u32 %v79918, 26
%v79924 = vshrl.u32 %v79918, 6
%v79925 = vor.u32 %v79924, %v79923
%v79926 = vxor.u32 %v79925, %v79921
%v79929 = vadd.s32 %v79926, %v79921
%v79933 = vadd.s32 %v79929, %v8
%v79935 = vshll.u32 %v79926, 6
%v79936 = vshrl.u32 %v79926, 26
%v79937 = vor.u32 %v79936, %v79935
%v79938 = vxor.u32 %v79937, %v79929
%v79941 = vadd.s32 %v79938, %v10
%v79945 = vadd.s32 5, %v79941
%v79947 = vxor.u32 %v79945, %v79933
%v79948 = vand.u32.u8 255, %v79947
%v79949 = vand.u32 65535, %v79948
%v79950 = vshrl.u32 %v79949, 1
%v79951 = vor.u32 16256, %v79950
%v79952 = vand.u32.u16 65535, %v79951
%v120172 = vadd.low.f32.bf16 -1.0, %v79952
%v79961 = vmul.f32 2.0, %v120172
%v79965 = vadd.f32 -0.99609375, %v79961
%v79969 = vmax.f32 %v79965, -0.99609375
%v79971 = vand.u32 2147483647, %v79969
%vm79974 = vcmp.eq.f32.partialorder %v79971, 1.0
%v79979 = vmul.f32 inf, %v79969
%v79981 = vxor.u32 2147483648, %v79969
%v79984 = vmul.f32 %v79981, %v79969
%v79986 = vadd.f32 1.0, %v79984
%v79987 = vlog2.pop %v79986
%v79988 = vmul.f32 0.6931472, %v79987
%v79989 = vmul.f32 -0.5, %v79984
%v79990 = vadd.f32 1.0, %v79989
%v79991 = vmul.f32 %v79990, %v79984
%v79992 = vand.u32 2147483647, %v79984
%vm79993 = vcmp.lt.f32.partialorder %v79992, 0.0004427343
%v79994 = vsel /*vm=*/%vm79993, /*on_true_vy=*/%v79991, /*on_false_vx=*/%v79988
%v79995 = vxor.u32 2147483648, %v79994
%vm79998 = vcmp.lt.f32.partialorder %v79995, 5.0
%v80003 = vsel /*vm=*/%vm79998, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v80007 = vsel /*vm=*/%vm79998, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v80011 = vsel /*vm=*/%vm79998, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v80015 = vsel /*vm=*/%vm79998, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v80019 = vsel /*vm=*/%vm79998, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v80023 = vsel /*vm=*/%vm79998, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v80027 = vsel /*vm=*/%vm79998, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v80031 = vsel /*vm=*/%vm79998, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v80035 = vsel /*vm=*/%vm79998, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v80039 = vadd.f32 -2.5, %v79995
%v80041 = vrsqrt.pop %v79995
%v80042 = vmul.f32 %v80041, %v79995
%vm80043 = vcmp.eq.f32.partialorder %v79995, inf
%v80044 = vsel /*vm=*/%vm80043, /*on_true_vy=*/%v79995, /*on_false_vx=*/%v80042
%vm80045 = vcmp.eq.f32.partialorder %v79995, 0.0
%v80046 = vand.u32 2147483648, %v79995
%v80047 = vsel /*vm=*/%vm80045, /*on_true_vy=*/%v80046, /*on_false_vx=*/%v80044
%v80050 = vadd.f32 -3.0, %v80047
%v80054 = vsel /*vm=*/%vm79998, /*on_true_vy=*/%v80039, /*on_false_vx=*/%v80050
%v80058 = vmul.f32 %v80054, %v80035
%v80062 = vadd.f32 %v80058, %v80031
%v80066 = vmul.f32 %v80062, %v80054
%v80070 = vadd.f32 %v80066, %v80027
%v80074 = vmul.f32 %v80070, %v80054
%v80078 = vadd.f32 %v80074, %v80023
%v80082 = vmul.f32 %v80078, %v80054
%v80086 = vadd.f32 %v80082, %v80019
%v80090 = vmul.f32 %v80086, %v80054
%v80094 = vadd.f32 %v80090, %v80015
%v80098 = vmul.f32 %v80094, %v80054
%v80102 = vadd.f32 %v80098, %v80011
%v80106 = vmul.f32 %v80102, %v80054
%v80110 = vadd.f32 %v80106, %v80007
%v80114 = vmul.f32 %v80110, %v80054
%v80118 = vadd.f32 %v80114, %v80003
%v80122 = vmul.f32 %v80118, %v79969
%v80126 = vsel /*vm=*/%vm79974, /*on_true_vy=*/%v79979, /*on_false_vx=*/%v80122
%v80130 = vmul.f32 1.4140625, %v80126
%v80133 = vpack.c.bf16 %v120417, %v80130
%120173 = vst [vmem:[%s280 + $0x154] sm:$0xf] /*vst_source=*/%v80133
%v80137 = vadd.s32 %v78751, %v1868
%v80147 = vadd.s32 %v80137, %v415
%vm80151 = vcmp.lt.u32.totalorder %v80147, %v80137
%vm80156 = vcmp.lt.u32.totalorder %v80137, %v1868
%v80161 = vadd.s32 %v78734, %v1855
%v80165 = vadd.s32 1, %v80161
%v80169 = vsel /*vm=*/%vm80156, /*on_true_vy=*/%v80165, /*on_false_vx=*/%v80161
%v80173 = vadd.s32 1, %v80169
%v80177 = vsel /*vm=*/%vm80151, /*on_true_vy=*/%v80173, /*on_false_vx=*/%v80169
%v80182 = vadd.s32 %v80177, %v10
%v80186 = vadd.s32 %v80147, %v9
%v80190 = vadd.s32 %v80186, %v80182
%v80192 = vshll.u32 %v80186, 13
%v80193 = vshrl.u32 %v80186, 19
%v80194 = vor.u32 %v80193, %v80192
%v80195 = vxor.u32 %v80194, %v80190
%v80198 = vadd.s32 %v80195, %v80190
%v80200 = vshll.u32 %v80195, 15
%v80201 = vshrl.u32 %v80195, 17
%v80202 = vor.u32 %v80201, %v80200
%v80203 = vxor.u32 %v80202, %v80198
%v80206 = vadd.s32 %v80203, %v80198
%v80208 = vshll.u32 %v80203, 26
%v80209 = vshrl.u32 %v80203, 6
%v80210 = vor.u32 %v80209, %v80208
%v80211 = vxor.u32 %v80210, %v80206
%v80214 = vadd.s32 %v80211, %v80206
%v80218 = vadd.s32 %v80214, %v9
%v80220 = vshll.u32 %v80211, 6
%v80221 = vshrl.u32 %v80211, 26
%v80222 = vor.u32 %v80221, %v80220
%v80223 = vxor.u32 %v80222, %v80214
%v80226 = vadd.s32 %v80223, %v8
%v80230 = vadd.s32 1, %v80226
%v80234 = vadd.s32 %v80230, %v80218
%v80236 = vshll.u32 %v80230, 17
%v80237 = vshrl.u32 %v80230, 15
%v80238 = vor.u32 %v80237, %v80236
%v80239 = vxor.u32 %v80238, %v80234
%v80242 = vadd.s32 %v80239, %v80234
%v80244 = vshll.u32 %v80239, 29
%v80245 = vshrl.u32 %v80239, 3
%v80246 = vor.u32 %v80245, %v80244
%v80247 = vxor.u32 %v80246, %v80242
%v80250 = vadd.s32 %v80247, %v80242
%v80252 = vshll.u32 %v80247, 16
%v80253 = vshrl.u32 %v80247, 16
%v80254 = vor.u32 %v80253, %v80252
%v80255 = vxor.u32 %v80254, %v80250
%v80258 = vadd.s32 %v80255, %v80250
%v80262 = vadd.s32 %v80258, %v8
%v80264 = vshll.u32 %v80255, 24
%v80265 = vshrl.u32 %v80255, 8
%v80266 = vor.u32 %v80265, %v80264
%v80267 = vxor.u32 %v80266, %v80258
%v80270 = vadd.s32 %v80267, %v10
%v80274 = vadd.s32 2, %v80270
%v80278 = vadd.s32 %v80274, %v80262
%v80280 = vshll.u32 %v80274, 13
%v80281 = vshrl.u32 %v80274, 19
%v80282 = vor.u32 %v80281, %v80280
%v80283 = vxor.u32 %v80282, %v80278
%v80286 = vadd.s32 %v80283, %v80278
%v80288 = vshll.u32 %v80283, 15
%v80289 = vshrl.u32 %v80283, 17
%v80290 = vor.u32 %v80289, %v80288
%v80291 = vxor.u32 %v80290, %v80286
%v80294 = vadd.s32 %v80291, %v80286
%v80296 = vshll.u32 %v80291, 26
%v80297 = vshrl.u32 %v80291, 6
%v80298 = vor.u32 %v80297, %v80296
%v80299 = vxor.u32 %v80298, %v80294
%v80302 = vadd.s32 %v80299, %v80294
%v80306 = vadd.s32 %v80302, %v10
%v80308 = vshll.u32 %v80299, 6
%v80309 = vshrl.u32 %v80299, 26
%v80310 = vor.u32 %v80309, %v80308
%v80311 = vxor.u32 %v80310, %v80302
%v80314 = vadd.s32 %v80311, %v9
%v80318 = vadd.s32 3, %v80314
%v80322 = vadd.s32 %v80318, %v80306
%v80324 = vshll.u32 %v80318, 17
%v80325 = vshrl.u32 %v80318, 15
%v80326 = vor.u32 %v80325, %v80324
%v80327 = vxor.u32 %v80326, %v80322
%v80330 = vadd.s32 %v80327, %v80322
%v80332 = vshll.u32 %v80327, 29
%v80333 = vshrl.u32 %v80327, 3
%v80334 = vor.u32 %v80333, %v80332
%v80335 = vxor.u32 %v80334, %v80330
%v80338 = vadd.s32 %v80335, %v80330
%v80340 = vshll.u32 %v80335, 16
%v80341 = vshrl.u32 %v80335, 16
%v80342 = vor.u32 %v80341, %v80340
%v80343 = vxor.u32 %v80342, %v80338
%v80346 = vadd.s32 %v80343, %v80338
%v80350 = vadd.s32 %v80346, %v9
%v80352 = vshll.u32 %v80343, 24
%v80353 = vshrl.u32 %v80343, 8
%v80354 = vor.u32 %v80353, %v80352
%v80355 = vxor.u32 %v80354, %v80346
%v80358 = vadd.s32 %v80355, %v8
%v80362 = vadd.s32 4, %v80358
%v80366 = vadd.s32 %v80362, %v80350
%v80368 = vshll.u32 %v80362, 13
%v80369 = vshrl.u32 %v80362, 19
%v80370 = vor.u32 %v80369, %v80368
%v80371 = vxor.u32 %v80370, %v80366
%v80374 = vadd.s32 %v80371, %v80366
%v80376 = vshll.u32 %v80371, 15
%v80377 = vshrl.u32 %v80371, 17
%v80378 = vor.u32 %v80377, %v80376
%v80379 = vxor.u32 %v80378, %v80374
%v80382 = vadd.s32 %v80379, %v80374
%v80384 = vshll.u32 %v80379, 26
%v80385 = vshrl.u32 %v80379, 6
%v80386 = vor.u32 %v80385, %v80384
%v80387 = vxor.u32 %v80386, %v80382
%v80390 = vadd.s32 %v80387, %v80382
%v80394 = vadd.s32 %v80390, %v8
%v80396 = vshll.u32 %v80387, 6
%v80397 = vshrl.u32 %v80387, 26
%v80398 = vor.u32 %v80397, %v80396
%v80399 = vxor.u32 %v80398, %v80390
%v80402 = vadd.s32 %v80399, %v10
%v80406 = vadd.s32 5, %v80402
%v80408 = vxor.u32 %v80406, %v80394
%v80409 = vand.u32.u8 255, %v80408
%v80410 = vand.u32 65535, %v80409
%v80411 = vshrl.u32 %v80410, 1
%v80412 = vor.u32 16256, %v80411
%v80413 = vand.u32.u16 65535, %v80412
%v120174 = vadd.low.f32.bf16 -1.0, %v80413
%v80422 = vmul.f32 2.0, %v120174
%v80426 = vadd.f32 -0.99609375, %v80422
%v80430 = vmax.f32 %v80426, -0.99609375
%v80432 = vand.u32 2147483647, %v80430
%vm80435 = vcmp.eq.f32.partialorder %v80432, 1.0
%v80440 = vmul.f32 inf, %v80430
%v80442 = vxor.u32 2147483648, %v80430
%v80445 = vmul.f32 %v80442, %v80430
%v80447 = vadd.f32 1.0, %v80445
%v80448 = vlog2.pop %v80447
%v80449 = vmul.f32 0.6931472, %v80448
%v80450 = vmul.f32 -0.5, %v80445
%v80451 = vadd.f32 1.0, %v80450
%v80452 = vmul.f32 %v80451, %v80445
%v80453 = vand.u32 2147483647, %v80445
%vm80454 = vcmp.lt.f32.partialorder %v80453, 0.0004427343
%v80455 = vsel /*vm=*/%vm80454, /*on_true_vy=*/%v80452, /*on_false_vx=*/%v80449
%v80456 = vxor.u32 2147483648, %v80455
%vm80459 = vcmp.lt.f32.partialorder %v80456, 5.0
%v80464 = vsel /*vm=*/%vm80459, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v80468 = vsel /*vm=*/%vm80459, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v80472 = vsel /*vm=*/%vm80459, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v80476 = vsel /*vm=*/%vm80459, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v80480 = vsel /*vm=*/%vm80459, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v80484 = vsel /*vm=*/%vm80459, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v80488 = vsel /*vm=*/%vm80459, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v80492 = vsel /*vm=*/%vm80459, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v80496 = vsel /*vm=*/%vm80459, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v80500 = vadd.f32 -2.5, %v80456
%v80502 = vrsqrt.pop %v80456
%v80503 = vmul.f32 %v80502, %v80456
%vm80504 = vcmp.eq.f32.partialorder %v80456, inf
%v80505 = vsel /*vm=*/%vm80504, /*on_true_vy=*/%v80456, /*on_false_vx=*/%v80503
%vm80506 = vcmp.eq.f32.partialorder %v80456, 0.0
%v80507 = vand.u32 2147483648, %v80456
%v80508 = vsel /*vm=*/%vm80506, /*on_true_vy=*/%v80507, /*on_false_vx=*/%v80505
%v80511 = vadd.f32 -3.0, %v80508
%v80515 = vsel /*vm=*/%vm80459, /*on_true_vy=*/%v80500, /*on_false_vx=*/%v80511
%v80519 = vmul.f32 %v80515, %v80496
%v80523 = vadd.f32 %v80519, %v80492
%v80527 = vmul.f32 %v80523, %v80515
%v80531 = vadd.f32 %v80527, %v80488
%v80535 = vmul.f32 %v80531, %v80515
%v80539 = vadd.f32 %v80535, %v80484
%v80543 = vmul.f32 %v80539, %v80515
%v80547 = vadd.f32 %v80543, %v80480
%v80551 = vmul.f32 %v80547, %v80515
%v80555 = vadd.f32 %v80551, %v80476
%v80559 = vmul.f32 %v80555, %v80515
%v80563 = vadd.f32 %v80559, %v80472
%v80567 = vmul.f32 %v80563, %v80515
%v80571 = vadd.f32 %v80567, %v80468
%v80575 = vmul.f32 %v80571, %v80515
%v80579 = vadd.f32 %v80575, %v80464
%v80583 = vmul.f32 %v80579, %v80430
%v80587 = vsel /*vm=*/%vm80435, /*on_true_vy=*/%v80440, /*on_false_vx=*/%v80583
%v80591 = vmul.f32 1.4140625, %v80587
%v80594 = vpack.c.bf16 %v120417, %v80591
%120175 = vst [vmem:[%s280 + $0x1d4] sm:$0xf] /*vst_source=*/%v80594
%v80598 = vadd.s32 %v78751, %v2355
%v80608 = vadd.s32 %v80598, %v415
%vm80612 = vcmp.lt.u32.totalorder %v80608, %v80598
%vm80617 = vcmp.lt.u32.totalorder %v80598, %v2355
%v80622 = vadd.s32 %v78734, %v2342
%v80626 = vadd.s32 1, %v80622
%v80630 = vsel /*vm=*/%vm80617, /*on_true_vy=*/%v80626, /*on_false_vx=*/%v80622
%v80634 = vadd.s32 1, %v80630
%v80638 = vsel /*vm=*/%vm80612, /*on_true_vy=*/%v80634, /*on_false_vx=*/%v80630
%v80643 = vadd.s32 %v80638, %v10
%v80647 = vadd.s32 %v80608, %v9
%v80651 = vadd.s32 %v80647, %v80643
%v80653 = vshll.u32 %v80647, 13
%v80654 = vshrl.u32 %v80647, 19
%v80655 = vor.u32 %v80654, %v80653
%v80656 = vxor.u32 %v80655, %v80651
%v80659 = vadd.s32 %v80656, %v80651
%v80661 = vshll.u32 %v80656, 15
%v80662 = vshrl.u32 %v80656, 17
%v80663 = vor.u32 %v80662, %v80661
%v80664 = vxor.u32 %v80663, %v80659
%v80667 = vadd.s32 %v80664, %v80659
%v80669 = vshll.u32 %v80664, 26
%v80670 = vshrl.u32 %v80664, 6
%v80671 = vor.u32 %v80670, %v80669
%v80672 = vxor.u32 %v80671, %v80667
%v80675 = vadd.s32 %v80672, %v80667
%v80679 = vadd.s32 %v80675, %v9
%v80681 = vshll.u32 %v80672, 6
%v80682 = vshrl.u32 %v80672, 26
%v80683 = vor.u32 %v80682, %v80681
%v80684 = vxor.u32 %v80683, %v80675
%v80687 = vadd.s32 %v80684, %v8
%v80691 = vadd.s32 1, %v80687
%v80695 = vadd.s32 %v80691, %v80679
%v80697 = vshll.u32 %v80691, 17
%v80698 = vshrl.u32 %v80691, 15
%v80699 = vor.u32 %v80698, %v80697
%v80700 = vxor.u32 %v80699, %v80695
%v80703 = vadd.s32 %v80700, %v80695
%v80705 = vshll.u32 %v80700, 29
%v80706 = vshrl.u32 %v80700, 3
%v80707 = vor.u32 %v80706, %v80705
%v80708 = vxor.u32 %v80707, %v80703
%v80711 = vadd.s32 %v80708, %v80703
%v80713 = vshll.u32 %v80708, 16
%v80714 = vshrl.u32 %v80708, 16
%v80715 = vor.u32 %v80714, %v80713
%v80716 = vxor.u32 %v80715, %v80711
%v80719 = vadd.s32 %v80716, %v80711
%v80723 = vadd.s32 %v80719, %v8
%v80725 = vshll.u32 %v80716, 24
%v80726 = vshrl.u32 %v80716, 8
%v80727 = vor.u32 %v80726, %v80725
%v80728 = vxor.u32 %v80727, %v80719
%v80731 = vadd.s32 %v80728, %v10
%v80735 = vadd.s32 2, %v80731
%v80739 = vadd.s32 %v80735, %v80723
%v80741 = vshll.u32 %v80735, 13
%v80742 = vshrl.u32 %v80735, 19
%v80743 = vor.u32 %v80742, %v80741
%v80744 = vxor.u32 %v80743, %v80739
%v80747 = vadd.s32 %v80744, %v80739
%v80749 = vshll.u32 %v80744, 15
%v80750 = vshrl.u32 %v80744, 17
%v80751 = vor.u32 %v80750, %v80749
%v80752 = vxor.u32 %v80751, %v80747
%v80755 = vadd.s32 %v80752, %v80747
%v80757 = vshll.u32 %v80752, 26
%v80758 = vshrl.u32 %v80752, 6
%v80759 = vor.u32 %v80758, %v80757
%v80760 = vxor.u32 %v80759, %v80755
%v80763 = vadd.s32 %v80760, %v80755
%v80767 = vadd.s32 %v80763, %v10
%v80769 = vshll.u32 %v80760, 6
%v80770 = vshrl.u32 %v80760, 26
%v80771 = vor.u32 %v80770, %v80769
%v80772 = vxor.u32 %v80771, %v80763
%v80775 = vadd.s32 %v80772, %v9
%v80779 = vadd.s32 3, %v80775
%v80783 = vadd.s32 %v80779, %v80767
%v80785 = vshll.u32 %v80779, 17
%v80786 = vshrl.u32 %v80779, 15
%v80787 = vor.u32 %v80786, %v80785
%v80788 = vxor.u32 %v80787, %v80783
%v80791 = vadd.s32 %v80788, %v80783
%v80793 = vshll.u32 %v80788, 29
%v80794 = vshrl.u32 %v80788, 3
%v80795 = vor.u32 %v80794, %v80793
%v80796 = vxor.u32 %v80795, %v80791
%v80799 = vadd.s32 %v80796, %v80791
%v80801 = vshll.u32 %v80796, 16
%v80802 = vshrl.u32 %v80796, 16
%v80803 = vor.u32 %v80802, %v80801
%v80804 = vxor.u32 %v80803, %v80799
%v80807 = vadd.s32 %v80804, %v80799
%v80811 = vadd.s32 %v80807, %v9
%v80813 = vshll.u32 %v80804, 24
%v80814 = vshrl.u32 %v80804, 8
%v80815 = vor.u32 %v80814, %v80813
%v80816 = vxor.u32 %v80815, %v80807
%v80819 = vadd.s32 %v80816, %v8
%v80823 = vadd.s32 4, %v80819
%v80827 = vadd.s32 %v80823, %v80811
%v80829 = vshll.u32 %v80823, 13
%v80830 = vshrl.u32 %v80823, 19
%v80831 = vor.u32 %v80830, %v80829
%v80832 = vxor.u32 %v80831, %v80827
%v80835 = vadd.s32 %v80832, %v80827
%v80837 = vshll.u32 %v80832, 15
%v80838 = vshrl.u32 %v80832, 17
%v80839 = vor.u32 %v80838, %v80837
%v80840 = vxor.u32 %v80839, %v80835
%v80843 = vadd.s32 %v80840, %v80835
%v80845 = vshll.u32 %v80840, 26
%v80846 = vshrl.u32 %v80840, 6
%v80847 = vor.u32 %v80846, %v80845
%v80848 = vxor.u32 %v80847, %v80843
%v80851 = vadd.s32 %v80848, %v80843
%v80855 = vadd.s32 %v80851, %v8
%v80857 = vshll.u32 %v80848, 6
%v80858 = vshrl.u32 %v80848, 26
%v80859 = vor.u32 %v80858, %v80857
%v80860 = vxor.u32 %v80859, %v80851
%v80863 = vadd.s32 %v80860, %v10
%v80867 = vadd.s32 5, %v80863
%v80869 = vxor.u32 %v80867, %v80855
%v80870 = vand.u32.u8 255, %v80869
%v80871 = vand.u32 65535, %v80870
%v80872 = vshrl.u32 %v80871, 1
%v80873 = vor.u32 16256, %v80872
%v80874 = vand.u32.u16 65535, %v80873
%v120176 = vadd.low.f32.bf16 -1.0, %v80874
%v80883 = vmul.f32 2.0, %v120176
%v80887 = vadd.f32 -0.99609375, %v80883
%v80891 = vmax.f32 %v80887, -0.99609375
%v80893 = vand.u32 2147483647, %v80891
%vm80896 = vcmp.eq.f32.partialorder %v80893, 1.0
%v80901 = vmul.f32 inf, %v80891
%v80903 = vxor.u32 2147483648, %v80891
%v80906 = vmul.f32 %v80903, %v80891
%v80908 = vadd.f32 1.0, %v80906
%v80909 = vlog2.pop %v80908
%v80910 = vmul.f32 0.6931472, %v80909
%v80911 = vmul.f32 -0.5, %v80906
%v80912 = vadd.f32 1.0, %v80911
%v80913 = vmul.f32 %v80912, %v80906
%v80914 = vand.u32 2147483647, %v80906
%vm80915 = vcmp.lt.f32.partialorder %v80914, 0.0004427343
%v80916 = vsel /*vm=*/%vm80915, /*on_true_vy=*/%v80913, /*on_false_vx=*/%v80910
%v80917 = vxor.u32 2147483648, %v80916
%vm80920 = vcmp.lt.f32.partialorder %v80917, 5.0
%v80925 = vsel /*vm=*/%vm80920, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v80929 = vsel /*vm=*/%vm80920, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v80933 = vsel /*vm=*/%vm80920, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v80937 = vsel /*vm=*/%vm80920, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v80941 = vsel /*vm=*/%vm80920, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v80945 = vsel /*vm=*/%vm80920, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v80949 = vsel /*vm=*/%vm80920, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v80953 = vsel /*vm=*/%vm80920, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v80957 = vsel /*vm=*/%vm80920, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v80961 = vadd.f32 -2.5, %v80917
%v80963 = vrsqrt.pop %v80917
%v80964 = vmul.f32 %v80963, %v80917
%vm80965 = vcmp.eq.f32.partialorder %v80917, inf
%v80966 = vsel /*vm=*/%vm80965, /*on_true_vy=*/%v80917, /*on_false_vx=*/%v80964
%vm80967 = vcmp.eq.f32.partialorder %v80917, 0.0
%v80968 = vand.u32 2147483648, %v80917
%v80969 = vsel /*vm=*/%vm80967, /*on_true_vy=*/%v80968, /*on_false_vx=*/%v80966
%v80972 = vadd.f32 -3.0, %v80969
%v80976 = vsel /*vm=*/%vm80920, /*on_true_vy=*/%v80961, /*on_false_vx=*/%v80972
%v80980 = vmul.f32 %v80976, %v80957
%v80984 = vadd.f32 %v80980, %v80953
%v80988 = vmul.f32 %v80984, %v80976
%v80992 = vadd.f32 %v80988, %v80949
%v80996 = vmul.f32 %v80992, %v80976
%v81000 = vadd.f32 %v80996, %v80945
%v81004 = vmul.f32 %v81000, %v80976
%v81008 = vadd.f32 %v81004, %v80941
%v81012 = vmul.f32 %v81008, %v80976
%v81016 = vadd.f32 %v81012, %v80937
%v81020 = vmul.f32 %v81016, %v80976
%v81024 = vadd.f32 %v81020, %v80933
%v81028 = vmul.f32 %v81024, %v80976
%v81032 = vadd.f32 %v81028, %v80929
%v81036 = vmul.f32 %v81032, %v80976
%v81040 = vadd.f32 %v81036, %v80925
%v81044 = vmul.f32 %v81040, %v80891
%v81048 = vsel /*vm=*/%vm80896, /*on_true_vy=*/%v80901, /*on_false_vx=*/%v81044
%v81052 = vmul.f32 1.4140625, %v81048
%v81055 = vpack.c.bf16 %v120417, %v81052
%120177 = vst [vmem:[%s280 + $0x254] sm:$0xf] /*vst_source=*/%v81055
%v81059 = vadd.s32 %v78751, %v2842
%v81069 = vadd.s32 %v81059, %v415
%vm81073 = vcmp.lt.u32.totalorder %v81069, %v81059
%vm81078 = vcmp.lt.u32.totalorder %v81059, %v2842
%v81083 = vadd.s32 %v78734, %v2829
%v81087 = vadd.s32 1, %v81083
%v81091 = vsel /*vm=*/%vm81078, /*on_true_vy=*/%v81087, /*on_false_vx=*/%v81083
%v81095 = vadd.s32 1, %v81091
%v81099 = vsel /*vm=*/%vm81073, /*on_true_vy=*/%v81095, /*on_false_vx=*/%v81091
%v81104 = vadd.s32 %v81099, %v10
%v81108 = vadd.s32 %v81069, %v9
%v81112 = vadd.s32 %v81108, %v81104
%v81114 = vshll.u32 %v81108, 13
%v81115 = vshrl.u32 %v81108, 19
%v81116 = vor.u32 %v81115, %v81114
%v81117 = vxor.u32 %v81116, %v81112
%v81120 = vadd.s32 %v81117, %v81112
%v81122 = vshll.u32 %v81117, 15
%v81123 = vshrl.u32 %v81117, 17
%v81124 = vor.u32 %v81123, %v81122
%v81125 = vxor.u32 %v81124, %v81120
%v81128 = vadd.s32 %v81125, %v81120
%v81130 = vshll.u32 %v81125, 26
%v81131 = vshrl.u32 %v81125, 6
%v81132 = vor.u32 %v81131, %v81130
%v81133 = vxor.u32 %v81132, %v81128
%v81136 = vadd.s32 %v81133, %v81128
%v81140 = vadd.s32 %v81136, %v9
%v81142 = vshll.u32 %v81133, 6
%v81143 = vshrl.u32 %v81133, 26
%v81144 = vor.u32 %v81143, %v81142
%v81145 = vxor.u32 %v81144, %v81136
%v81148 = vadd.s32 %v81145, %v8
%v81152 = vadd.s32 1, %v81148
%v81156 = vadd.s32 %v81152, %v81140
%v81158 = vshll.u32 %v81152, 17
%v81159 = vshrl.u32 %v81152, 15
%v81160 = vor.u32 %v81159, %v81158
%v81161 = vxor.u32 %v81160, %v81156
%v81164 = vadd.s32 %v81161, %v81156
%v81166 = vshll.u32 %v81161, 29
%v81167 = vshrl.u32 %v81161, 3
%v81168 = vor.u32 %v81167, %v81166
%v81169 = vxor.u32 %v81168, %v81164
%v81172 = vadd.s32 %v81169, %v81164
%v81174 = vshll.u32 %v81169, 16
%v81175 = vshrl.u32 %v81169, 16
%v81176 = vor.u32 %v81175, %v81174
%v81177 = vxor.u32 %v81176, %v81172
%v81180 = vadd.s32 %v81177, %v81172
%v81184 = vadd.s32 %v81180, %v8
%v81186 = vshll.u32 %v81177, 24
%v81187 = vshrl.u32 %v81177, 8
%v81188 = vor.u32 %v81187, %v81186
%v81189 = vxor.u32 %v81188, %v81180
%v81192 = vadd.s32 %v81189, %v10
%v81196 = vadd.s32 2, %v81192
%v81200 = vadd.s32 %v81196, %v81184
%v81202 = vshll.u32 %v81196, 13
%v81203 = vshrl.u32 %v81196, 19
%v81204 = vor.u32 %v81203, %v81202
%v81205 = vxor.u32 %v81204, %v81200
%v81208 = vadd.s32 %v81205, %v81200
%v81210 = vshll.u32 %v81205, 15
%v81211 = vshrl.u32 %v81205, 17
%v81212 = vor.u32 %v81211, %v81210
%v81213 = vxor.u32 %v81212, %v81208
%v81216 = vadd.s32 %v81213, %v81208
%v81218 = vshll.u32 %v81213, 26
%v81219 = vshrl.u32 %v81213, 6
%v81220 = vor.u32 %v81219, %v81218
%v81221 = vxor.u32 %v81220, %v81216
%v81224 = vadd.s32 %v81221, %v81216
%v81228 = vadd.s32 %v81224, %v10
%v81230 = vshll.u32 %v81221, 6
%v81231 = vshrl.u32 %v81221, 26
%v81232 = vor.u32 %v81231, %v81230
%v81233 = vxor.u32 %v81232, %v81224
%v81236 = vadd.s32 %v81233, %v9
%v81240 = vadd.s32 3, %v81236
%v81244 = vadd.s32 %v81240, %v81228
%v81246 = vshll.u32 %v81240, 17
%v81247 = vshrl.u32 %v81240, 15
%v81248 = vor.u32 %v81247, %v81246
%v81249 = vxor.u32 %v81248, %v81244
%v81252 = vadd.s32 %v81249, %v81244
%v81254 = vshll.u32 %v81249, 29
%v81255 = vshrl.u32 %v81249, 3
%v81256 = vor.u32 %v81255, %v81254
%v81257 = vxor.u32 %v81256, %v81252
%v81260 = vadd.s32 %v81257, %v81252
%v81262 = vshll.u32 %v81257, 16
%v81263 = vshrl.u32 %v81257, 16
%v81264 = vor.u32 %v81263, %v81262
%v81265 = vxor.u32 %v81264, %v81260
%v81268 = vadd.s32 %v81265, %v81260
%v81272 = vadd.s32 %v81268, %v9
%v81274 = vshll.u32 %v81265, 24
%v81275 = vshrl.u32 %v81265, 8
%v81276 = vor.u32 %v81275, %v81274
%v81277 = vxor.u32 %v81276, %v81268
%v81280 = vadd.s32 %v81277, %v8
%v81284 = vadd.s32 4, %v81280
%v81288 = vadd.s32 %v81284, %v81272
%v81290 = vshll.u32 %v81284, 13
%v81291 = vshrl.u32 %v81284, 19
%v81292 = vor.u32 %v81291, %v81290
%v81293 = vxor.u32 %v81292, %v81288
%v81296 = vadd.s32 %v81293, %v81288
%v81298 = vshll.u32 %v81293, 15
%v81299 = vshrl.u32 %v81293, 17
%v81300 = vor.u32 %v81299, %v81298
%v81301 = vxor.u32 %v81300, %v81296
%v81304 = vadd.s32 %v81301, %v81296
%v81306 = vshll.u32 %v81301, 26
%v81307 = vshrl.u32 %v81301, 6
%v81308 = vor.u32 %v81307, %v81306
%v81309 = vxor.u32 %v81308, %v81304
%v81312 = vadd.s32 %v81309, %v81304
%v81316 = vadd.s32 %v81312, %v8
%v81318 = vshll.u32 %v81309, 6
%v81319 = vshrl.u32 %v81309, 26
%v81320 = vor.u32 %v81319, %v81318
%v81321 = vxor.u32 %v81320, %v81312
%v81324 = vadd.s32 %v81321, %v10
%v81328 = vadd.s32 5, %v81324
%v81330 = vxor.u32 %v81328, %v81316
%v81331 = vand.u32.u8 255, %v81330
%v81332 = vand.u32 65535, %v81331
%v81333 = vshrl.u32 %v81332, 1
%v81334 = vor.u32 16256, %v81333
%v81335 = vand.u32.u16 65535, %v81334
%v120178 = vadd.low.f32.bf16 -1.0, %v81335
%v81344 = vmul.f32 2.0, %v120178
%v81348 = vadd.f32 -0.99609375, %v81344
%v81352 = vmax.f32 %v81348, -0.99609375
%v81354 = vand.u32 2147483647, %v81352
%vm81357 = vcmp.eq.f32.partialorder %v81354, 1.0
%v81362 = vmul.f32 inf, %v81352
%v81364 = vxor.u32 2147483648, %v81352
%v81367 = vmul.f32 %v81364, %v81352
%v81369 = vadd.f32 1.0, %v81367
%v81370 = vlog2.pop %v81369
%v81371 = vmul.f32 0.6931472, %v81370
%v81372 = vmul.f32 -0.5, %v81367
%v81373 = vadd.f32 1.0, %v81372
%v81374 = vmul.f32 %v81373, %v81367
%v81375 = vand.u32 2147483647, %v81367
%vm81376 = vcmp.lt.f32.partialorder %v81375, 0.0004427343
%v81377 = vsel /*vm=*/%vm81376, /*on_true_vy=*/%v81374, /*on_false_vx=*/%v81371
%v81378 = vxor.u32 2147483648, %v81377
%vm81381 = vcmp.lt.f32.partialorder %v81378, 5.0
%v81386 = vsel /*vm=*/%vm81381, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v81390 = vsel /*vm=*/%vm81381, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v81394 = vsel /*vm=*/%vm81381, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v81398 = vsel /*vm=*/%vm81381, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v81402 = vsel /*vm=*/%vm81381, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v81406 = vsel /*vm=*/%vm81381, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v81410 = vsel /*vm=*/%vm81381, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v81414 = vsel /*vm=*/%vm81381, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v81418 = vsel /*vm=*/%vm81381, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v81422 = vadd.f32 -2.5, %v81378
%v81424 = vrsqrt.pop %v81378
%v81425 = vmul.f32 %v81424, %v81378
%vm81426 = vcmp.eq.f32.partialorder %v81378, inf
%v81427 = vsel /*vm=*/%vm81426, /*on_true_vy=*/%v81378, /*on_false_vx=*/%v81425
%vm81428 = vcmp.eq.f32.partialorder %v81378, 0.0
%v81429 = vand.u32 2147483648, %v81378
%v81430 = vsel /*vm=*/%vm81428, /*on_true_vy=*/%v81429, /*on_false_vx=*/%v81427
%v81433 = vadd.f32 -3.0, %v81430
%v81437 = vsel /*vm=*/%vm81381, /*on_true_vy=*/%v81422, /*on_false_vx=*/%v81433
%v81441 = vmul.f32 %v81437, %v81418
%v81445 = vadd.f32 %v81441, %v81414
%v81449 = vmul.f32 %v81445, %v81437
%v81453 = vadd.f32 %v81449, %v81410
%v81457 = vmul.f32 %v81453, %v81437
%v81461 = vadd.f32 %v81457, %v81406
%v81465 = vmul.f32 %v81461, %v81437
%v81469 = vadd.f32 %v81465, %v81402
%v81473 = vmul.f32 %v81469, %v81437
%v81477 = vadd.f32 %v81473, %v81398
%v81481 = vmul.f32 %v81477, %v81437
%v81485 = vadd.f32 %v81481, %v81394
%v81489 = vmul.f32 %v81485, %v81437
%v81493 = vadd.f32 %v81489, %v81390
%v81497 = vmul.f32 %v81493, %v81437
%v81501 = vadd.f32 %v81497, %v81386
%v81505 = vmul.f32 %v81501, %v81352
%v81509 = vsel /*vm=*/%vm81357, /*on_true_vy=*/%v81362, /*on_false_vx=*/%v81505
%v81513 = vmul.f32 1.4140625, %v81509
%v81516 = vpack.c.bf16 %v120417, %v81513
%120179 = vst [vmem:[%s280 + $0x2d4] sm:$0xf] /*vst_source=*/%v81516
%v81520 = vadd.s32 %v78751, %v3329
%v81530 = vadd.s32 %v81520, %v415
%vm81534 = vcmp.lt.u32.totalorder %v81530, %v81520
%vm81539 = vcmp.lt.u32.totalorder %v81520, %v3329
%v81544 = vadd.s32 %v78734, %v3316
%v81548 = vadd.s32 1, %v81544
%v81552 = vsel /*vm=*/%vm81539, /*on_true_vy=*/%v81548, /*on_false_vx=*/%v81544
%v81556 = vadd.s32 1, %v81552
%v81560 = vsel /*vm=*/%vm81534, /*on_true_vy=*/%v81556, /*on_false_vx=*/%v81552
%v81565 = vadd.s32 %v81560, %v10
%v81569 = vadd.s32 %v81530, %v9
%v81573 = vadd.s32 %v81569, %v81565
%v81575 = vshll.u32 %v81569, 13
%v81576 = vshrl.u32 %v81569, 19
%v81577 = vor.u32 %v81576, %v81575
%v81578 = vxor.u32 %v81577, %v81573
%v81581 = vadd.s32 %v81578, %v81573
%v81583 = vshll.u32 %v81578, 15
%v81584 = vshrl.u32 %v81578, 17
%v81585 = vor.u32 %v81584, %v81583
%v81586 = vxor.u32 %v81585, %v81581
%v81589 = vadd.s32 %v81586, %v81581
%v81591 = vshll.u32 %v81586, 26
%v81592 = vshrl.u32 %v81586, 6
%v81593 = vor.u32 %v81592, %v81591
%v81594 = vxor.u32 %v81593, %v81589
%v81597 = vadd.s32 %v81594, %v81589
%v81601 = vadd.s32 %v81597, %v9
%v81603 = vshll.u32 %v81594, 6
%v81604 = vshrl.u32 %v81594, 26
%v81605 = vor.u32 %v81604, %v81603
%v81606 = vxor.u32 %v81605, %v81597
%v81609 = vadd.s32 %v81606, %v8
%v81613 = vadd.s32 1, %v81609
%v81617 = vadd.s32 %v81613, %v81601
%v81619 = vshll.u32 %v81613, 17
%v81620 = vshrl.u32 %v81613, 15
%v81621 = vor.u32 %v81620, %v81619
%v81622 = vxor.u32 %v81621, %v81617
%v81625 = vadd.s32 %v81622, %v81617
%v81627 = vshll.u32 %v81622, 29
%v81628 = vshrl.u32 %v81622, 3
%v81629 = vor.u32 %v81628, %v81627
%v81630 = vxor.u32 %v81629, %v81625
%v81633 = vadd.s32 %v81630, %v81625
%v81635 = vshll.u32 %v81630, 16
%v81636 = vshrl.u32 %v81630, 16
%v81637 = vor.u32 %v81636, %v81635
%v81638 = vxor.u32 %v81637, %v81633
%v81641 = vadd.s32 %v81638, %v81633
%v81645 = vadd.s32 %v81641, %v8
%v81647 = vshll.u32 %v81638, 24
%v81648 = vshrl.u32 %v81638, 8
%v81649 = vor.u32 %v81648, %v81647
%v81650 = vxor.u32 %v81649, %v81641
%v81653 = vadd.s32 %v81650, %v10
%v81657 = vadd.s32 2, %v81653
%v81661 = vadd.s32 %v81657, %v81645
%v81663 = vshll.u32 %v81657, 13
%v81664 = vshrl.u32 %v81657, 19
%v81665 = vor.u32 %v81664, %v81663
%v81666 = vxor.u32 %v81665, %v81661
%v81669 = vadd.s32 %v81666, %v81661
%v81671 = vshll.u32 %v81666, 15
%v81672 = vshrl.u32 %v81666, 17
%v81673 = vor.u32 %v81672, %v81671
%v81674 = vxor.u32 %v81673, %v81669
%v81677 = vadd.s32 %v81674, %v81669
%v81679 = vshll.u32 %v81674, 26
%v81680 = vshrl.u32 %v81674, 6
%v81681 = vor.u32 %v81680, %v81679
%v81682 = vxor.u32 %v81681, %v81677
%v81685 = vadd.s32 %v81682, %v81677
%v81689 = vadd.s32 %v81685, %v10
%v81691 = vshll.u32 %v81682, 6
%v81692 = vshrl.u32 %v81682, 26
%v81693 = vor.u32 %v81692, %v81691
%v81694 = vxor.u32 %v81693, %v81685
%v81697 = vadd.s32 %v81694, %v9
%v81701 = vadd.s32 3, %v81697
%v81705 = vadd.s32 %v81701, %v81689
%v81707 = vshll.u32 %v81701, 17
%v81708 = vshrl.u32 %v81701, 15
%v81709 = vor.u32 %v81708, %v81707
%v81710 = vxor.u32 %v81709, %v81705
%v81713 = vadd.s32 %v81710, %v81705
%v81715 = vshll.u32 %v81710, 29
%v81716 = vshrl.u32 %v81710, 3
%v81717 = vor.u32 %v81716, %v81715
%v81718 = vxor.u32 %v81717, %v81713
%v81721 = vadd.s32 %v81718, %v81713
%v81723 = vshll.u32 %v81718, 16
%v81724 = vshrl.u32 %v81718, 16
%v81725 = vor.u32 %v81724, %v81723
%v81726 = vxor.u32 %v81725, %v81721
%v81729 = vadd.s32 %v81726, %v81721
%v81733 = vadd.s32 %v81729, %v9
%v81735 = vshll.u32 %v81726, 24
%v81736 = vshrl.u32 %v81726, 8
%v81737 = vor.u32 %v81736, %v81735
%v81738 = vxor.u32 %v81737, %v81729
%v81741 = vadd.s32 %v81738, %v8
%v81745 = vadd.s32 4, %v81741
%v81749 = vadd.s32 %v81745, %v81733
%v81751 = vshll.u32 %v81745, 13
%v81752 = vshrl.u32 %v81745, 19
%v81753 = vor.u32 %v81752, %v81751
%v81754 = vxor.u32 %v81753, %v81749
%v81757 = vadd.s32 %v81754, %v81749
%v81759 = vshll.u32 %v81754, 15
%v81760 = vshrl.u32 %v81754, 17
%v81761 = vor.u32 %v81760, %v81759
%v81762 = vxor.u32 %v81761, %v81757
%v81765 = vadd.s32 %v81762, %v81757
%v81767 = vshll.u32 %v81762, 26
%v81768 = vshrl.u32 %v81762, 6
%v81769 = vor.u32 %v81768, %v81767
%v81770 = vxor.u32 %v81769, %v81765
%v81773 = vadd.s32 %v81770, %v81765
%v81777 = vadd.s32 %v81773, %v8
%v81779 = vshll.u32 %v81770, 6
%v81780 = vshrl.u32 %v81770, 26
%v81781 = vor.u32 %v81780, %v81779
%v81782 = vxor.u32 %v81781, %v81773
%v81785 = vadd.s32 %v81782, %v10
%v81789 = vadd.s32 5, %v81785
%v81791 = vxor.u32 %v81789, %v81777
%v81792 = vand.u32.u8 255, %v81791
%v81793 = vand.u32 65535, %v81792
%v81794 = vshrl.u32 %v81793, 1
%v81795 = vor.u32 16256, %v81794
%v81796 = vand.u32.u16 65535, %v81795
%v120180 = vadd.low.f32.bf16 -1.0, %v81796
%v81805 = vmul.f32 2.0, %v120180
%v81809 = vadd.f32 -0.99609375, %v81805
%v81813 = vmax.f32 %v81809, -0.99609375
%v81815 = vand.u32 2147483647, %v81813
%vm81818 = vcmp.eq.f32.partialorder %v81815, 1.0
%v81823 = vmul.f32 inf, %v81813
%v81825 = vxor.u32 2147483648, %v81813
%v81828 = vmul.f32 %v81825, %v81813
%v81830 = vadd.f32 1.0, %v81828
%v81831 = vlog2.pop %v81830
%v81832 = vmul.f32 0.6931472, %v81831
%v81833 = vmul.f32 -0.5, %v81828
%v81834 = vadd.f32 1.0, %v81833
%v81835 = vmul.f32 %v81834, %v81828
%v81836 = vand.u32 2147483647, %v81828
%vm81837 = vcmp.lt.f32.partialorder %v81836, 0.0004427343
%v81838 = vsel /*vm=*/%vm81837, /*on_true_vy=*/%v81835, /*on_false_vx=*/%v81832
%v81839 = vxor.u32 2147483648, %v81838
%vm81842 = vcmp.lt.f32.partialorder %v81839, 5.0
%v81847 = vsel /*vm=*/%vm81842, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v81851 = vsel /*vm=*/%vm81842, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v81855 = vsel /*vm=*/%vm81842, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v81859 = vsel /*vm=*/%vm81842, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v81863 = vsel /*vm=*/%vm81842, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v81867 = vsel /*vm=*/%vm81842, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v81871 = vsel /*vm=*/%vm81842, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v81875 = vsel /*vm=*/%vm81842, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v81879 = vsel /*vm=*/%vm81842, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v81883 = vadd.f32 -2.5, %v81839
%v81885 = vrsqrt.pop %v81839
%v81886 = vmul.f32 %v81885, %v81839
%vm81887 = vcmp.eq.f32.partialorder %v81839, inf
%v81888 = vsel /*vm=*/%vm81887, /*on_true_vy=*/%v81839, /*on_false_vx=*/%v81886
%vm81889 = vcmp.eq.f32.partialorder %v81839, 0.0
%v81890 = vand.u32 2147483648, %v81839
%v81891 = vsel /*vm=*/%vm81889, /*on_true_vy=*/%v81890, /*on_false_vx=*/%v81888
%v81894 = vadd.f32 -3.0, %v81891
%v81898 = vsel /*vm=*/%vm81842, /*on_true_vy=*/%v81883, /*on_false_vx=*/%v81894
%v81902 = vmul.f32 %v81898, %v81879
%v81906 = vadd.f32 %v81902, %v81875
%v81910 = vmul.f32 %v81906, %v81898
%v81914 = vadd.f32 %v81910, %v81871
%v81918 = vmul.f32 %v81914, %v81898
%v81922 = vadd.f32 %v81918, %v81867
%v81926 = vmul.f32 %v81922, %v81898
%v81930 = vadd.f32 %v81926, %v81863
%v81934 = vmul.f32 %v81930, %v81898
%v81938 = vadd.f32 %v81934, %v81859
%v81942 = vmul.f32 %v81938, %v81898
%v81946 = vadd.f32 %v81942, %v81855
%v81950 = vmul.f32 %v81946, %v81898
%v81954 = vadd.f32 %v81950, %v81851
%v81958 = vmul.f32 %v81954, %v81898
%v81962 = vadd.f32 %v81958, %v81847
%v81966 = vmul.f32 %v81962, %v81813
%v81970 = vsel /*vm=*/%vm81818, /*on_true_vy=*/%v81823, /*on_false_vx=*/%v81966
%v81974 = vmul.f32 1.4140625, %v81970
%v81977 = vpack.c.bf16 %v120417, %v81974
%120181 = vst [vmem:[%s280 + $0x354] sm:$0xf] /*vst_source=*/%v81977
%v81981 = vadd.s32 %v78751, %v3816
%v81991 = vadd.s32 %v81981, %v415
%vm81995 = vcmp.lt.u32.totalorder %v81991, %v81981
%vm82000 = vcmp.lt.u32.totalorder %v81981, %v3816
%v82005 = vadd.s32 %v78734, %v3803
%v82009 = vadd.s32 1, %v82005
%v82013 = vsel /*vm=*/%vm82000, /*on_true_vy=*/%v82009, /*on_false_vx=*/%v82005
%v82017 = vadd.s32 1, %v82013
%v82021 = vsel /*vm=*/%vm81995, /*on_true_vy=*/%v82017, /*on_false_vx=*/%v82013
%v82026 = vadd.s32 %v82021, %v10
%v82030 = vadd.s32 %v81991, %v9
%v82034 = vadd.s32 %v82030, %v82026
%v82036 = vshll.u32 %v82030, 13
%v82037 = vshrl.u32 %v82030, 19
%v82038 = vor.u32 %v82037, %v82036
%v82039 = vxor.u32 %v82038, %v82034
%v82042 = vadd.s32 %v82039, %v82034
%v82044 = vshll.u32 %v82039, 15
%v82045 = vshrl.u32 %v82039, 17
%v82046 = vor.u32 %v82045, %v82044
%v82047 = vxor.u32 %v82046, %v82042
%v82050 = vadd.s32 %v82047, %v82042
%v82052 = vshll.u32 %v82047, 26
%v82053 = vshrl.u32 %v82047, 6
%v82054 = vor.u32 %v82053, %v82052
%v82055 = vxor.u32 %v82054, %v82050
%v82058 = vadd.s32 %v82055, %v82050
%v82062 = vadd.s32 %v82058, %v9
%v82064 = vshll.u32 %v82055, 6
%v82065 = vshrl.u32 %v82055, 26
%v82066 = vor.u32 %v82065, %v82064
%v82067 = vxor.u32 %v82066, %v82058
%v82070 = vadd.s32 %v82067, %v8
%v82074 = vadd.s32 1, %v82070
%v82078 = vadd.s32 %v82074, %v82062
%v82080 = vshll.u32 %v82074, 17
%v82081 = vshrl.u32 %v82074, 15
%v82082 = vor.u32 %v82081, %v82080
%v82083 = vxor.u32 %v82082, %v82078
%v82086 = vadd.s32 %v82083, %v82078
%v82088 = vshll.u32 %v82083, 29
%v82089 = vshrl.u32 %v82083, 3
%v82090 = vor.u32 %v82089, %v82088
%v82091 = vxor.u32 %v82090, %v82086
%v82094 = vadd.s32 %v82091, %v82086
%v82096 = vshll.u32 %v82091, 16
%v82097 = vshrl.u32 %v82091, 16
%v82098 = vor.u32 %v82097, %v82096
%v82099 = vxor.u32 %v82098, %v82094
%v82102 = vadd.s32 %v82099, %v82094
%v82106 = vadd.s32 %v82102, %v8
%v82108 = vshll.u32 %v82099, 24
%v82109 = vshrl.u32 %v82099, 8
%v82110 = vor.u32 %v82109, %v82108
%v82111 = vxor.u32 %v82110, %v82102
%v82114 = vadd.s32 %v82111, %v10
%v82118 = vadd.s32 2, %v82114
%v82122 = vadd.s32 %v82118, %v82106
%v82124 = vshll.u32 %v82118, 13
%v82125 = vshrl.u32 %v82118, 19
%v82126 = vor.u32 %v82125, %v82124
%v82127 = vxor.u32 %v82126, %v82122
%v82130 = vadd.s32 %v82127, %v82122
%v82132 = vshll.u32 %v82127, 15
%v82133 = vshrl.u32 %v82127, 17
%v82134 = vor.u32 %v82133, %v82132
%v82135 = vxor.u32 %v82134, %v82130
%v82138 = vadd.s32 %v82135, %v82130
%v82140 = vshll.u32 %v82135, 26
%v82141 = vshrl.u32 %v82135, 6
%v82142 = vor.u32 %v82141, %v82140
%v82143 = vxor.u32 %v82142, %v82138
%v82146 = vadd.s32 %v82143, %v82138
%v82150 = vadd.s32 %v82146, %v10
%v82152 = vshll.u32 %v82143, 6
%v82153 = vshrl.u32 %v82143, 26
%v82154 = vor.u32 %v82153, %v82152
%v82155 = vxor.u32 %v82154, %v82146
%v82158 = vadd.s32 %v82155, %v9
%v82162 = vadd.s32 3, %v82158
%v82166 = vadd.s32 %v82162, %v82150
%v82168 = vshll.u32 %v82162, 17
%v82169 = vshrl.u32 %v82162, 15
%v82170 = vor.u32 %v82169, %v82168
%v82171 = vxor.u32 %v82170, %v82166
%v82174 = vadd.s32 %v82171, %v82166
%v82176 = vshll.u32 %v82171, 29
%v82177 = vshrl.u32 %v82171, 3
%v82178 = vor.u32 %v82177, %v82176
%v82179 = vxor.u32 %v82178, %v82174
%v82182 = vadd.s32 %v82179, %v82174
%v82184 = vshll.u32 %v82179, 16
%v82185 = vshrl.u32 %v82179, 16
%v82186 = vor.u32 %v82185, %v82184
%v82187 = vxor.u32 %v82186, %v82182
%v82190 = vadd.s32 %v82187, %v82182
%v82194 = vadd.s32 %v82190, %v9
%v82196 = vshll.u32 %v82187, 24
%v82197 = vshrl.u32 %v82187, 8
%v82198 = vor.u32 %v82197, %v82196
%v82199 = vxor.u32 %v82198, %v82190
%v82202 = vadd.s32 %v82199, %v8
%v82206 = vadd.s32 4, %v82202
%v82210 = vadd.s32 %v82206, %v82194
%v82212 = vshll.u32 %v82206, 13
%v82213 = vshrl.u32 %v82206, 19
%v82214 = vor.u32 %v82213, %v82212
%v82215 = vxor.u32 %v82214, %v82210
%v82218 = vadd.s32 %v82215, %v82210
%v82220 = vshll.u32 %v82215, 15
%v82221 = vshrl.u32 %v82215, 17
%v82222 = vor.u32 %v82221, %v82220
%v82223 = vxor.u32 %v82222, %v82218
%v82226 = vadd.s32 %v82223, %v82218
%v82228 = vshll.u32 %v82223, 26
%v82229 = vshrl.u32 %v82223, 6
%v82230 = vor.u32 %v82229, %v82228
%v82231 = vxor.u32 %v82230, %v82226
%v82234 = vadd.s32 %v82231, %v82226
%v82238 = vadd.s32 %v82234, %v8
%v82240 = vshll.u32 %v82231, 6
%v82241 = vshrl.u32 %v82231, 26
%v82242 = vor.u32 %v82241, %v82240
%v82243 = vxor.u32 %v82242, %v82234
%v82246 = vadd.s32 %v82243, %v10
%v82250 = vadd.s32 5, %v82246
%v82252 = vxor.u32 %v82250, %v82238
%v82253 = vand.u32.u8 255, %v82252
%v82254 = vand.u32 65535, %v82253
%v82255 = vshrl.u32 %v82254, 1
%v82256 = vor.u32 16256, %v82255
%v82257 = vand.u32.u16 65535, %v82256
%v120182 = vadd.low.f32.bf16 -1.0, %v82257
%v82266 = vmul.f32 2.0, %v120182
%v82270 = vadd.f32 -0.99609375, %v82266
%v82274 = vmax.f32 %v82270, -0.99609375
%v82276 = vand.u32 2147483647, %v82274
%vm82279 = vcmp.eq.f32.partialorder %v82276, 1.0
%v82284 = vmul.f32 inf, %v82274
%v82286 = vxor.u32 2147483648, %v82274
%v82289 = vmul.f32 %v82286, %v82274
%v82291 = vadd.f32 1.0, %v82289
%v82292 = vlog2.pop %v82291
%v82293 = vmul.f32 0.6931472, %v82292
%v82294 = vmul.f32 -0.5, %v82289
%v82295 = vadd.f32 1.0, %v82294
%v82296 = vmul.f32 %v82295, %v82289
%v82297 = vand.u32 2147483647, %v82289
%vm82298 = vcmp.lt.f32.partialorder %v82297, 0.0004427343
%v82299 = vsel /*vm=*/%vm82298, /*on_true_vy=*/%v82296, /*on_false_vx=*/%v82293
%v82300 = vxor.u32 2147483648, %v82299
%vm82303 = vcmp.lt.f32.partialorder %v82300, 5.0
%v82308 = vsel /*vm=*/%vm82303, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v82312 = vsel /*vm=*/%vm82303, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v82316 = vsel /*vm=*/%vm82303, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v82320 = vsel /*vm=*/%vm82303, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v82324 = vsel /*vm=*/%vm82303, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v82328 = vsel /*vm=*/%vm82303, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v82332 = vsel /*vm=*/%vm82303, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v82336 = vsel /*vm=*/%vm82303, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v82340 = vsel /*vm=*/%vm82303, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v82344 = vadd.f32 -2.5, %v82300
%v82346 = vrsqrt.pop %v82300
%v82347 = vmul.f32 %v82346, %v82300
%vm82348 = vcmp.eq.f32.partialorder %v82300, inf
%v82349 = vsel /*vm=*/%vm82348, /*on_true_vy=*/%v82300, /*on_false_vx=*/%v82347
%vm82350 = vcmp.eq.f32.partialorder %v82300, 0.0
%v82351 = vand.u32 2147483648, %v82300
%v82352 = vsel /*vm=*/%vm82350, /*on_true_vy=*/%v82351, /*on_false_vx=*/%v82349
%v82355 = vadd.f32 -3.0, %v82352
%v82359 = vsel /*vm=*/%vm82303, /*on_true_vy=*/%v82344, /*on_false_vx=*/%v82355
%v82363 = vmul.f32 %v82359, %v82340
%v82367 = vadd.f32 %v82363, %v82336
%v82371 = vmul.f32 %v82367, %v82359
%v82375 = vadd.f32 %v82371, %v82332
%v82379 = vmul.f32 %v82375, %v82359
%v82383 = vadd.f32 %v82379, %v82328
%v82387 = vmul.f32 %v82383, %v82359
%v82391 = vadd.f32 %v82387, %v82324
%v82395 = vmul.f32 %v82391, %v82359
%v82399 = vadd.f32 %v82395, %v82320
%v82403 = vmul.f32 %v82399, %v82359
%v82407 = vadd.f32 %v82403, %v82316
%v82411 = vmul.f32 %v82407, %v82359
%v82415 = vadd.f32 %v82411, %v82312
%v82419 = vmul.f32 %v82415, %v82359
%v82423 = vadd.f32 %v82419, %v82308
%v82427 = vmul.f32 %v82423, %v82274
%v82431 = vsel /*vm=*/%vm82279, /*on_true_vy=*/%v82284, /*on_false_vx=*/%v82427
%v82435 = vmul.f32 1.4140625, %v82431
%v82438 = vpack.c.bf16 %v120417, %v82435
%120183 = vst [vmem:[%s280 + $0x3d4] sm:$0xf] /*vst_source=*/%v82438
%v82476 = vadd.s32 %v82473, %v408
%v82486 = vadd.s32 %v82476, %v415
%vm82490 = vcmp.lt.u32.totalorder %v82486, %v82476
%vm82495 = vcmp.lt.u32.totalorder %v82476, %v408
%v82500 = vadd.s32 %v82456, %v380
%v82504 = vadd.s32 1, %v82500
%v82508 = vsel /*vm=*/%vm82495, /*on_true_vy=*/%v82504, /*on_false_vx=*/%v82500
%v82512 = vadd.s32 1, %v82508
%v82516 = vsel /*vm=*/%vm82490, /*on_true_vy=*/%v82512, /*on_false_vx=*/%v82508
%v82521 = vadd.s32 %v82516, %v10
%v82525 = vadd.s32 %v82486, %v9
%v82529 = vadd.s32 %v82525, %v82521
%v82531 = vshll.u32 %v82525, 13
%v82532 = vshrl.u32 %v82525, 19
%v82533 = vor.u32 %v82532, %v82531
%v82534 = vxor.u32 %v82533, %v82529
%v82537 = vadd.s32 %v82534, %v82529
%v82539 = vshll.u32 %v82534, 15
%v82540 = vshrl.u32 %v82534, 17
%v82541 = vor.u32 %v82540, %v82539
%v82542 = vxor.u32 %v82541, %v82537
%v82545 = vadd.s32 %v82542, %v82537
%v82547 = vshll.u32 %v82542, 26
%v82548 = vshrl.u32 %v82542, 6
%v82549 = vor.u32 %v82548, %v82547
%v82550 = vxor.u32 %v82549, %v82545
%v82553 = vadd.s32 %v82550, %v82545
%v82557 = vadd.s32 %v82553, %v9
%v82559 = vshll.u32 %v82550, 6
%v82560 = vshrl.u32 %v82550, 26
%v82561 = vor.u32 %v82560, %v82559
%v82562 = vxor.u32 %v82561, %v82553
%v82565 = vadd.s32 %v82562, %v8
%v82569 = vadd.s32 1, %v82565
%v82573 = vadd.s32 %v82569, %v82557
%v82575 = vshll.u32 %v82569, 17
%v82576 = vshrl.u32 %v82569, 15
%v82577 = vor.u32 %v82576, %v82575
%v82578 = vxor.u32 %v82577, %v82573
%v82581 = vadd.s32 %v82578, %v82573
%v82583 = vshll.u32 %v82578, 29
%v82584 = vshrl.u32 %v82578, 3
%v82585 = vor.u32 %v82584, %v82583
%v82586 = vxor.u32 %v82585, %v82581
%v82589 = vadd.s32 %v82586, %v82581
%v82591 = vshll.u32 %v82586, 16
%v82592 = vshrl.u32 %v82586, 16
%v82593 = vor.u32 %v82592, %v82591
%v82594 = vxor.u32 %v82593, %v82589
%v82597 = vadd.s32 %v82594, %v82589
%v82601 = vadd.s32 %v82597, %v8
%v82603 = vshll.u32 %v82594, 24
%v82604 = vshrl.u32 %v82594, 8
%v82605 = vor.u32 %v82604, %v82603
%v82606 = vxor.u32 %v82605, %v82597
%v82609 = vadd.s32 %v82606, %v10
%v82613 = vadd.s32 2, %v82609
%v82617 = vadd.s32 %v82613, %v82601
%v82619 = vshll.u32 %v82613, 13
%v82620 = vshrl.u32 %v82613, 19
%v82621 = vor.u32 %v82620, %v82619
%v82622 = vxor.u32 %v82621, %v82617
%v82625 = vadd.s32 %v82622, %v82617
%v82627 = vshll.u32 %v82622, 15
%v82628 = vshrl.u32 %v82622, 17
%v82629 = vor.u32 %v82628, %v82627
%v82630 = vxor.u32 %v82629, %v82625
%v82633 = vadd.s32 %v82630, %v82625
%v82635 = vshll.u32 %v82630, 26
%v82636 = vshrl.u32 %v82630, 6
%v82637 = vor.u32 %v82636, %v82635
%v82638 = vxor.u32 %v82637, %v82633
%v82641 = vadd.s32 %v82638, %v82633
%v82645 = vadd.s32 %v82641, %v10
%v82647 = vshll.u32 %v82638, 6
%v82648 = vshrl.u32 %v82638, 26
%v82649 = vor.u32 %v82648, %v82647
%v82650 = vxor.u32 %v82649, %v82641
%v82653 = vadd.s32 %v82650, %v9
%v82657 = vadd.s32 3, %v82653
%v82661 = vadd.s32 %v82657, %v82645
%v82663 = vshll.u32 %v82657, 17
%v82664 = vshrl.u32 %v82657, 15
%v82665 = vor.u32 %v82664, %v82663
%v82666 = vxor.u32 %v82665, %v82661
%v82669 = vadd.s32 %v82666, %v82661
%v82671 = vshll.u32 %v82666, 29
%v82672 = vshrl.u32 %v82666, 3
%v82673 = vor.u32 %v82672, %v82671
%v82674 = vxor.u32 %v82673, %v82669
%v82677 = vadd.s32 %v82674, %v82669
%v82679 = vshll.u32 %v82674, 16
%v82680 = vshrl.u32 %v82674, 16
%v82681 = vor.u32 %v82680, %v82679
%v82682 = vxor.u32 %v82681, %v82677
%v82685 = vadd.s32 %v82682, %v82677
%v82689 = vadd.s32 %v82685, %v9
%v82691 = vshll.u32 %v82682, 24
%v82692 = vshrl.u32 %v82682, 8
%v82693 = vor.u32 %v82692, %v82691
%v82694 = vxor.u32 %v82693, %v82685
%v82697 = vadd.s32 %v82694, %v8
%v82701 = vadd.s32 4, %v82697
%v82705 = vadd.s32 %v82701, %v82689
%v82707 = vshll.u32 %v82701, 13
%v82708 = vshrl.u32 %v82701, 19
%v82709 = vor.u32 %v82708, %v82707
%v82710 = vxor.u32 %v82709, %v82705
%v82713 = vadd.s32 %v82710, %v82705
%v82715 = vshll.u32 %v82710, 15
%v82716 = vshrl.u32 %v82710, 17
%v82717 = vor.u32 %v82716, %v82715
%v82718 = vxor.u32 %v82717, %v82713
%v82721 = vadd.s32 %v82718, %v82713
%v82723 = vshll.u32 %v82718, 26
%v82724 = vshrl.u32 %v82718, 6
%v82725 = vor.u32 %v82724, %v82723
%v82726 = vxor.u32 %v82725, %v82721
%v82729 = vadd.s32 %v82726, %v82721
%v82733 = vadd.s32 %v82729, %v8
%v82735 = vshll.u32 %v82726, 6
%v82736 = vshrl.u32 %v82726, 26
%v82737 = vor.u32 %v82736, %v82735
%v82738 = vxor.u32 %v82737, %v82729
%v82741 = vadd.s32 %v82738, %v10
%v82745 = vadd.s32 5, %v82741
%v82747 = vxor.u32 %v82745, %v82733
%v82748 = vand.u32.u8 255, %v82747
%v82749 = vand.u32 65535, %v82748
%v82750 = vshrl.u32 %v82749, 1
%v82751 = vor.u32 16256, %v82750
%v82752 = vand.u32.u16 65535, %v82751
%v120188 = vadd.low.f32.bf16 -1.0, %v82752
%v82761 = vmul.f32 2.0, %v120188
%v82765 = vadd.f32 -0.99609375, %v82761
%v82769 = vmax.f32 %v82765, -0.99609375
%v82771 = vand.u32 2147483647, %v82769
%vm82774 = vcmp.eq.f32.partialorder %v82771, 1.0
%v82779 = vmul.f32 inf, %v82769
%v82781 = vxor.u32 2147483648, %v82769
%v82784 = vmul.f32 %v82781, %v82769
%v82786 = vadd.f32 1.0, %v82784
%v82787 = vlog2.pop %v82786
%v82788 = vmul.f32 0.6931472, %v82787
%v82789 = vmul.f32 -0.5, %v82784
%v82790 = vadd.f32 1.0, %v82789
%v82791 = vmul.f32 %v82790, %v82784
%v82792 = vand.u32 2147483647, %v82784
%vm82793 = vcmp.lt.f32.partialorder %v82792, 0.0004427343
%v82794 = vsel /*vm=*/%vm82793, /*on_true_vy=*/%v82791, /*on_false_vx=*/%v82788
%v82795 = vxor.u32 2147483648, %v82794
%vm82798 = vcmp.lt.f32.partialorder %v82795, 5.0
%v82803 = vsel /*vm=*/%vm82798, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v82807 = vsel /*vm=*/%vm82798, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v82811 = vsel /*vm=*/%vm82798, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v82815 = vsel /*vm=*/%vm82798, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v82819 = vsel /*vm=*/%vm82798, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v82823 = vsel /*vm=*/%vm82798, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v82827 = vsel /*vm=*/%vm82798, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v82831 = vsel /*vm=*/%vm82798, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v82835 = vsel /*vm=*/%vm82798, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v82839 = vadd.f32 -2.5, %v82795
%v82841 = vrsqrt.pop %v82795
%v82842 = vmul.f32 %v82841, %v82795
%vm82843 = vcmp.eq.f32.partialorder %v82795, inf
%v82844 = vsel /*vm=*/%vm82843, /*on_true_vy=*/%v82795, /*on_false_vx=*/%v82842
%vm82845 = vcmp.eq.f32.partialorder %v82795, 0.0
%v82846 = vand.u32 2147483648, %v82795
%v82847 = vsel /*vm=*/%vm82845, /*on_true_vy=*/%v82846, /*on_false_vx=*/%v82844
%v82850 = vadd.f32 -3.0, %v82847
%v82854 = vsel /*vm=*/%vm82798, /*on_true_vy=*/%v82839, /*on_false_vx=*/%v82850
%v82858 = vmul.f32 %v82854, %v82835
%v82862 = vadd.f32 %v82858, %v82831
%v82866 = vmul.f32 %v82862, %v82854
%v82870 = vadd.f32 %v82866, %v82827
%v82874 = vmul.f32 %v82870, %v82854
%v82878 = vadd.f32 %v82874, %v82823
%v82882 = vmul.f32 %v82878, %v82854
%v82886 = vadd.f32 %v82882, %v82819
%v82890 = vmul.f32 %v82886, %v82854
%v82894 = vadd.f32 %v82890, %v82815
%v82898 = vmul.f32 %v82894, %v82854
%v82902 = vadd.f32 %v82898, %v82811
%v82906 = vmul.f32 %v82902, %v82854
%v82910 = vadd.f32 %v82906, %v82807
%v82914 = vmul.f32 %v82910, %v82854
%v82918 = vadd.f32 %v82914, %v82803
%v82922 = vmul.f32 %v82918, %v82769
%v82926 = vsel /*vm=*/%vm82774, /*on_true_vy=*/%v82779, /*on_false_vx=*/%v82922
%v82930 = vmul.f32 1.4140625, %v82926
%v82933 = vpack.c.bf16 %v120417, %v82930
%120189 = vst [vmem:[%s280 + $0x58] sm:$0xf] /*vst_source=*/%v82933
%v82937 = vadd.s32 %v82473, %v894
%v82947 = vadd.s32 %v82937, %v415
%vm82951 = vcmp.lt.u32.totalorder %v82947, %v82937
%vm82956 = vcmp.lt.u32.totalorder %v82937, %v894
%v82961 = vadd.s32 %v82456, %v881
%v82965 = vadd.s32 1, %v82961
%v82969 = vsel /*vm=*/%vm82956, /*on_true_vy=*/%v82965, /*on_false_vx=*/%v82961
%v82973 = vadd.s32 1, %v82969
%v82977 = vsel /*vm=*/%vm82951, /*on_true_vy=*/%v82973, /*on_false_vx=*/%v82969
%v82982 = vadd.s32 %v82977, %v10
%v82986 = vadd.s32 %v82947, %v9
%v82990 = vadd.s32 %v82986, %v82982
%v82992 = vshll.u32 %v82986, 13
%v82993 = vshrl.u32 %v82986, 19
%v82994 = vor.u32 %v82993, %v82992
%v82995 = vxor.u32 %v82994, %v82990
%v82998 = vadd.s32 %v82995, %v82990
%v83000 = vshll.u32 %v82995, 15
%v83001 = vshrl.u32 %v82995, 17
%v83002 = vor.u32 %v83001, %v83000
%v83003 = vxor.u32 %v83002, %v82998
%v83006 = vadd.s32 %v83003, %v82998
%v83008 = vshll.u32 %v83003, 26
%v83009 = vshrl.u32 %v83003, 6
%v83010 = vor.u32 %v83009, %v83008
%v83011 = vxor.u32 %v83010, %v83006
%v83014 = vadd.s32 %v83011, %v83006
%v83018 = vadd.s32 %v83014, %v9
%v83020 = vshll.u32 %v83011, 6
%v83021 = vshrl.u32 %v83011, 26
%v83022 = vor.u32 %v83021, %v83020
%v83023 = vxor.u32 %v83022, %v83014
%v83026 = vadd.s32 %v83023, %v8
%v83030 = vadd.s32 1, %v83026
%v83034 = vadd.s32 %v83030, %v83018
%v83036 = vshll.u32 %v83030, 17
%v83037 = vshrl.u32 %v83030, 15
%v83038 = vor.u32 %v83037, %v83036
%v83039 = vxor.u32 %v83038, %v83034
%v83042 = vadd.s32 %v83039, %v83034
%v83044 = vshll.u32 %v83039, 29
%v83045 = vshrl.u32 %v83039, 3
%v83046 = vor.u32 %v83045, %v83044
%v83047 = vxor.u32 %v83046, %v83042
%v83050 = vadd.s32 %v83047, %v83042
%v83052 = vshll.u32 %v83047, 16
%v83053 = vshrl.u32 %v83047, 16
%v83054 = vor.u32 %v83053, %v83052
%v83055 = vxor.u32 %v83054, %v83050
%v83058 = vadd.s32 %v83055, %v83050
%v83062 = vadd.s32 %v83058, %v8
%v83064 = vshll.u32 %v83055, 24
%v83065 = vshrl.u32 %v83055, 8
%v83066 = vor.u32 %v83065, %v83064
%v83067 = vxor.u32 %v83066, %v83058
%v83070 = vadd.s32 %v83067, %v10
%v83074 = vadd.s32 2, %v83070
%v83078 = vadd.s32 %v83074, %v83062
%v83080 = vshll.u32 %v83074, 13
%v83081 = vshrl.u32 %v83074, 19
%v83082 = vor.u32 %v83081, %v83080
%v83083 = vxor.u32 %v83082, %v83078
%v83086 = vadd.s32 %v83083, %v83078
%v83088 = vshll.u32 %v83083, 15
%v83089 = vshrl.u32 %v83083, 17
%v83090 = vor.u32 %v83089, %v83088
%v83091 = vxor.u32 %v83090, %v83086
%v83094 = vadd.s32 %v83091, %v83086
%v83096 = vshll.u32 %v83091, 26
%v83097 = vshrl.u32 %v83091, 6
%v83098 = vor.u32 %v83097, %v83096
%v83099 = vxor.u32 %v83098, %v83094
%v83102 = vadd.s32 %v83099, %v83094
%v83106 = vadd.s32 %v83102, %v10
%v83108 = vshll.u32 %v83099, 6
%v83109 = vshrl.u32 %v83099, 26
%v83110 = vor.u32 %v83109, %v83108
%v83111 = vxor.u32 %v83110, %v83102
%v83114 = vadd.s32 %v83111, %v9
%v83118 = vadd.s32 3, %v83114
%v83122 = vadd.s32 %v83118, %v83106
%v83124 = vshll.u32 %v83118, 17
%v83125 = vshrl.u32 %v83118, 15
%v83126 = vor.u32 %v83125, %v83124
%v83127 = vxor.u32 %v83126, %v83122
%v83130 = vadd.s32 %v83127, %v83122
%v83132 = vshll.u32 %v83127, 29
%v83133 = vshrl.u32 %v83127, 3
%v83134 = vor.u32 %v83133, %v83132
%v83135 = vxor.u32 %v83134, %v83130
%v83138 = vadd.s32 %v83135, %v83130
%v83140 = vshll.u32 %v83135, 16
%v83141 = vshrl.u32 %v83135, 16
%v83142 = vor.u32 %v83141, %v83140
%v83143 = vxor.u32 %v83142, %v83138
%v83146 = vadd.s32 %v83143, %v83138
%v83150 = vadd.s32 %v83146, %v9
%v83152 = vshll.u32 %v83143, 24
%v83153 = vshrl.u32 %v83143, 8
%v83154 = vor.u32 %v83153, %v83152
%v83155 = vxor.u32 %v83154, %v83146
%v83158 = vadd.s32 %v83155, %v8
%v83162 = vadd.s32 4, %v83158
%v83166 = vadd.s32 %v83162, %v83150
%v83168 = vshll.u32 %v83162, 13
%v83169 = vshrl.u32 %v83162, 19
%v83170 = vor.u32 %v83169, %v83168
%v83171 = vxor.u32 %v83170, %v83166
%v83174 = vadd.s32 %v83171, %v83166
%v83176 = vshll.u32 %v83171, 15
%v83177 = vshrl.u32 %v83171, 17
%v83178 = vor.u32 %v83177, %v83176
%v83179 = vxor.u32 %v83178, %v83174
%v83182 = vadd.s32 %v83179, %v83174
%v83184 = vshll.u32 %v83179, 26
%v83185 = vshrl.u32 %v83179, 6
%v83186 = vor.u32 %v83185, %v83184
%v83187 = vxor.u32 %v83186, %v83182
%v83190 = vadd.s32 %v83187, %v83182
%v83194 = vadd.s32 %v83190, %v8
%v83196 = vshll.u32 %v83187, 6
%v83197 = vshrl.u32 %v83187, 26
%v83198 = vor.u32 %v83197, %v83196
%v83199 = vxor.u32 %v83198, %v83190
%v83202 = vadd.s32 %v83199, %v10
%v83206 = vadd.s32 5, %v83202
%v83208 = vxor.u32 %v83206, %v83194
%v83209 = vand.u32.u8 255, %v83208
%v83210 = vand.u32 65535, %v83209
%v83211 = vshrl.u32 %v83210, 1
%v83212 = vor.u32 16256, %v83211
%v83213 = vand.u32.u16 65535, %v83212
%v120190 = vadd.low.f32.bf16 -1.0, %v83213
%v83222 = vmul.f32 2.0, %v120190
%v83226 = vadd.f32 -0.99609375, %v83222
%v83230 = vmax.f32 %v83226, -0.99609375
%v83232 = vand.u32 2147483647, %v83230
%vm83235 = vcmp.eq.f32.partialorder %v83232, 1.0
%v83240 = vmul.f32 inf, %v83230
%v83242 = vxor.u32 2147483648, %v83230
%v83245 = vmul.f32 %v83242, %v83230
%v83247 = vadd.f32 1.0, %v83245
%v83248 = vlog2.pop %v83247
%v83249 = vmul.f32 0.6931472, %v83248
%v83250 = vmul.f32 -0.5, %v83245
%v83251 = vadd.f32 1.0, %v83250
%v83252 = vmul.f32 %v83251, %v83245
%v83253 = vand.u32 2147483647, %v83245
%vm83254 = vcmp.lt.f32.partialorder %v83253, 0.0004427343
%v83255 = vsel /*vm=*/%vm83254, /*on_true_vy=*/%v83252, /*on_false_vx=*/%v83249
%v83256 = vxor.u32 2147483648, %v83255
%vm83259 = vcmp.lt.f32.partialorder %v83256, 5.0
%v83264 = vsel /*vm=*/%vm83259, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v83268 = vsel /*vm=*/%vm83259, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v83272 = vsel /*vm=*/%vm83259, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v83276 = vsel /*vm=*/%vm83259, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v83280 = vsel /*vm=*/%vm83259, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v83284 = vsel /*vm=*/%vm83259, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v83288 = vsel /*vm=*/%vm83259, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v83292 = vsel /*vm=*/%vm83259, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v83296 = vsel /*vm=*/%vm83259, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v83300 = vadd.f32 -2.5, %v83256
%v83302 = vrsqrt.pop %v83256
%v83303 = vmul.f32 %v83302, %v83256
%vm83304 = vcmp.eq.f32.partialorder %v83256, inf
%v83305 = vsel /*vm=*/%vm83304, /*on_true_vy=*/%v83256, /*on_false_vx=*/%v83303
%vm83306 = vcmp.eq.f32.partialorder %v83256, 0.0
%v83307 = vand.u32 2147483648, %v83256
%v83308 = vsel /*vm=*/%vm83306, /*on_true_vy=*/%v83307, /*on_false_vx=*/%v83305
%v83311 = vadd.f32 -3.0, %v83308
%v83315 = vsel /*vm=*/%vm83259, /*on_true_vy=*/%v83300, /*on_false_vx=*/%v83311
%v83319 = vmul.f32 %v83315, %v83296
%v83323 = vadd.f32 %v83319, %v83292
%v83327 = vmul.f32 %v83323, %v83315
%v83331 = vadd.f32 %v83327, %v83288
%v83335 = vmul.f32 %v83331, %v83315
%v83339 = vadd.f32 %v83335, %v83284
%v83343 = vmul.f32 %v83339, %v83315
%v83347 = vadd.f32 %v83343, %v83280
%v83351 = vmul.f32 %v83347, %v83315
%v83355 = vadd.f32 %v83351, %v83276
%v83359 = vmul.f32 %v83355, %v83315
%v83363 = vadd.f32 %v83359, %v83272
%v83367 = vmul.f32 %v83363, %v83315
%v83371 = vadd.f32 %v83367, %v83268
%v83375 = vmul.f32 %v83371, %v83315
%v83379 = vadd.f32 %v83375, %v83264
%v83383 = vmul.f32 %v83379, %v83230
%v83387 = vsel /*vm=*/%vm83235, /*on_true_vy=*/%v83240, /*on_false_vx=*/%v83383
%v83391 = vmul.f32 1.4140625, %v83387
%v83394 = vpack.c.bf16 %v120417, %v83391
%120191 = vst [vmem:[%s280 + $0xd8] sm:$0xf] /*vst_source=*/%v83394
%v83398 = vadd.s32 %v82473, %v1381
%v83408 = vadd.s32 %v83398, %v415
%vm83412 = vcmp.lt.u32.totalorder %v83408, %v83398
%vm83417 = vcmp.lt.u32.totalorder %v83398, %v1381
%v83422 = vadd.s32 %v82456, %v1368
%v83426 = vadd.s32 1, %v83422
%v83430 = vsel /*vm=*/%vm83417, /*on_true_vy=*/%v83426, /*on_false_vx=*/%v83422
%v83434 = vadd.s32 1, %v83430
%v83438 = vsel /*vm=*/%vm83412, /*on_true_vy=*/%v83434, /*on_false_vx=*/%v83430
%v83443 = vadd.s32 %v83438, %v10
%v83447 = vadd.s32 %v83408, %v9
%v83451 = vadd.s32 %v83447, %v83443
%v83453 = vshll.u32 %v83447, 13
%v83454 = vshrl.u32 %v83447, 19
%v83455 = vor.u32 %v83454, %v83453
%v83456 = vxor.u32 %v83455, %v83451
%v83459 = vadd.s32 %v83456, %v83451
%v83461 = vshll.u32 %v83456, 15
%v83462 = vshrl.u32 %v83456, 17
%v83463 = vor.u32 %v83462, %v83461
%v83464 = vxor.u32 %v83463, %v83459
%v83467 = vadd.s32 %v83464, %v83459
%v83469 = vshll.u32 %v83464, 26
%v83470 = vshrl.u32 %v83464, 6
%v83471 = vor.u32 %v83470, %v83469
%v83472 = vxor.u32 %v83471, %v83467
%v83475 = vadd.s32 %v83472, %v83467
%v83479 = vadd.s32 %v83475, %v9
%v83481 = vshll.u32 %v83472, 6
%v83482 = vshrl.u32 %v83472, 26
%v83483 = vor.u32 %v83482, %v83481
%v83484 = vxor.u32 %v83483, %v83475
%v83487 = vadd.s32 %v83484, %v8
%v83491 = vadd.s32 1, %v83487
%v83495 = vadd.s32 %v83491, %v83479
%v83497 = vshll.u32 %v83491, 17
%v83498 = vshrl.u32 %v83491, 15
%v83499 = vor.u32 %v83498, %v83497
%v83500 = vxor.u32 %v83499, %v83495
%v83503 = vadd.s32 %v83500, %v83495
%v83505 = vshll.u32 %v83500, 29
%v83506 = vshrl.u32 %v83500, 3
%v83507 = vor.u32 %v83506, %v83505
%v83508 = vxor.u32 %v83507, %v83503
%v83511 = vadd.s32 %v83508, %v83503
%v83513 = vshll.u32 %v83508, 16
%v83514 = vshrl.u32 %v83508, 16
%v83515 = vor.u32 %v83514, %v83513
%v83516 = vxor.u32 %v83515, %v83511
%v83519 = vadd.s32 %v83516, %v83511
%v83523 = vadd.s32 %v83519, %v8
%v83525 = vshll.u32 %v83516, 24
%v83526 = vshrl.u32 %v83516, 8
%v83527 = vor.u32 %v83526, %v83525
%v83528 = vxor.u32 %v83527, %v83519
%v83531 = vadd.s32 %v83528, %v10
%v83535 = vadd.s32 2, %v83531
%v83539 = vadd.s32 %v83535, %v83523
%v83541 = vshll.u32 %v83535, 13
%v83542 = vshrl.u32 %v83535, 19
%v83543 = vor.u32 %v83542, %v83541
%v83544 = vxor.u32 %v83543, %v83539
%v83547 = vadd.s32 %v83544, %v83539
%v83549 = vshll.u32 %v83544, 15
%v83550 = vshrl.u32 %v83544, 17
%v83551 = vor.u32 %v83550, %v83549
%v83552 = vxor.u32 %v83551, %v83547
%v83555 = vadd.s32 %v83552, %v83547
%v83557 = vshll.u32 %v83552, 26
%v83558 = vshrl.u32 %v83552, 6
%v83559 = vor.u32 %v83558, %v83557
%v83560 = vxor.u32 %v83559, %v83555
%v83563 = vadd.s32 %v83560, %v83555
%v83567 = vadd.s32 %v83563, %v10
%v83569 = vshll.u32 %v83560, 6
%v83570 = vshrl.u32 %v83560, 26
%v83571 = vor.u32 %v83570, %v83569
%v83572 = vxor.u32 %v83571, %v83563
%v83575 = vadd.s32 %v83572, %v9
%v83579 = vadd.s32 3, %v83575
%v83583 = vadd.s32 %v83579, %v83567
%v83585 = vshll.u32 %v83579, 17
%v83586 = vshrl.u32 %v83579, 15
%v83587 = vor.u32 %v83586, %v83585
%v83588 = vxor.u32 %v83587, %v83583
%v83591 = vadd.s32 %v83588, %v83583
%v83593 = vshll.u32 %v83588, 29
%v83594 = vshrl.u32 %v83588, 3
%v83595 = vor.u32 %v83594, %v83593
%v83596 = vxor.u32 %v83595, %v83591
%v83599 = vadd.s32 %v83596, %v83591
%v83601 = vshll.u32 %v83596, 16
%v83602 = vshrl.u32 %v83596, 16
%v83603 = vor.u32 %v83602, %v83601
%v83604 = vxor.u32 %v83603, %v83599
%v83607 = vadd.s32 %v83604, %v83599
%v83611 = vadd.s32 %v83607, %v9
%v83613 = vshll.u32 %v83604, 24
%v83614 = vshrl.u32 %v83604, 8
%v83615 = vor.u32 %v83614, %v83613
%v83616 = vxor.u32 %v83615, %v83607
%v83619 = vadd.s32 %v83616, %v8
%v83623 = vadd.s32 4, %v83619
%v83627 = vadd.s32 %v83623, %v83611
%v83629 = vshll.u32 %v83623, 13
%v83630 = vshrl.u32 %v83623, 19
%v83631 = vor.u32 %v83630, %v83629
%v83632 = vxor.u32 %v83631, %v83627
%v83635 = vadd.s32 %v83632, %v83627
%v83637 = vshll.u32 %v83632, 15
%v83638 = vshrl.u32 %v83632, 17
%v83639 = vor.u32 %v83638, %v83637
%v83640 = vxor.u32 %v83639, %v83635
%v83643 = vadd.s32 %v83640, %v83635
%v83645 = vshll.u32 %v83640, 26
%v83646 = vshrl.u32 %v83640, 6
%v83647 = vor.u32 %v83646, %v83645
%v83648 = vxor.u32 %v83647, %v83643
%v83651 = vadd.s32 %v83648, %v83643
%v83655 = vadd.s32 %v83651, %v8
%v83657 = vshll.u32 %v83648, 6
%v83658 = vshrl.u32 %v83648, 26
%v83659 = vor.u32 %v83658, %v83657
%v83660 = vxor.u32 %v83659, %v83651
%v83663 = vadd.s32 %v83660, %v10
%v83667 = vadd.s32 5, %v83663
%v83669 = vxor.u32 %v83667, %v83655
%v83670 = vand.u32.u8 255, %v83669
%v83671 = vand.u32 65535, %v83670
%v83672 = vshrl.u32 %v83671, 1
%v83673 = vor.u32 16256, %v83672
%v83674 = vand.u32.u16 65535, %v83673
%v120192 = vadd.low.f32.bf16 -1.0, %v83674
%v83683 = vmul.f32 2.0, %v120192
%v83687 = vadd.f32 -0.99609375, %v83683
%v83691 = vmax.f32 %v83687, -0.99609375
%v83693 = vand.u32 2147483647, %v83691
%vm83696 = vcmp.eq.f32.partialorder %v83693, 1.0
%v83701 = vmul.f32 inf, %v83691
%v83703 = vxor.u32 2147483648, %v83691
%v83706 = vmul.f32 %v83703, %v83691
%v83708 = vadd.f32 1.0, %v83706
%v83709 = vlog2.pop %v83708
%v83710 = vmul.f32 0.6931472, %v83709
%v83711 = vmul.f32 -0.5, %v83706
%v83712 = vadd.f32 1.0, %v83711
%v83713 = vmul.f32 %v83712, %v83706
%v83714 = vand.u32 2147483647, %v83706
%vm83715 = vcmp.lt.f32.partialorder %v83714, 0.0004427343
%v83716 = vsel /*vm=*/%vm83715, /*on_true_vy=*/%v83713, /*on_false_vx=*/%v83710
%v83717 = vxor.u32 2147483648, %v83716
%vm83720 = vcmp.lt.f32.partialorder %v83717, 5.0
%v83725 = vsel /*vm=*/%vm83720, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v83729 = vsel /*vm=*/%vm83720, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v83733 = vsel /*vm=*/%vm83720, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v83737 = vsel /*vm=*/%vm83720, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v83741 = vsel /*vm=*/%vm83720, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v83745 = vsel /*vm=*/%vm83720, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v83749 = vsel /*vm=*/%vm83720, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v83753 = vsel /*vm=*/%vm83720, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v83757 = vsel /*vm=*/%vm83720, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v83761 = vadd.f32 -2.5, %v83717
%v83763 = vrsqrt.pop %v83717
%v83764 = vmul.f32 %v83763, %v83717
%vm83765 = vcmp.eq.f32.partialorder %v83717, inf
%v83766 = vsel /*vm=*/%vm83765, /*on_true_vy=*/%v83717, /*on_false_vx=*/%v83764
%vm83767 = vcmp.eq.f32.partialorder %v83717, 0.0
%v83768 = vand.u32 2147483648, %v83717
%v83769 = vsel /*vm=*/%vm83767, /*on_true_vy=*/%v83768, /*on_false_vx=*/%v83766
%v83772 = vadd.f32 -3.0, %v83769
%v83776 = vsel /*vm=*/%vm83720, /*on_true_vy=*/%v83761, /*on_false_vx=*/%v83772
%v83780 = vmul.f32 %v83776, %v83757
%v83784 = vadd.f32 %v83780, %v83753
%v83788 = vmul.f32 %v83784, %v83776
%v83792 = vadd.f32 %v83788, %v83749
%v83796 = vmul.f32 %v83792, %v83776
%v83800 = vadd.f32 %v83796, %v83745
%v83804 = vmul.f32 %v83800, %v83776
%v83808 = vadd.f32 %v83804, %v83741
%v83812 = vmul.f32 %v83808, %v83776
%v83816 = vadd.f32 %v83812, %v83737
%v83820 = vmul.f32 %v83816, %v83776
%v83824 = vadd.f32 %v83820, %v83733
%v83828 = vmul.f32 %v83824, %v83776
%v83832 = vadd.f32 %v83828, %v83729
%v83836 = vmul.f32 %v83832, %v83776
%v83840 = vadd.f32 %v83836, %v83725
%v83844 = vmul.f32 %v83840, %v83691
%v83848 = vsel /*vm=*/%vm83696, /*on_true_vy=*/%v83701, /*on_false_vx=*/%v83844
%v83852 = vmul.f32 1.4140625, %v83848
%v83855 = vpack.c.bf16 %v120417, %v83852
%120193 = vst [vmem:[%s280 + $0x158] sm:$0xf] /*vst_source=*/%v83855
%v83859 = vadd.s32 %v82473, %v1868
%v83869 = vadd.s32 %v83859, %v415
%vm83873 = vcmp.lt.u32.totalorder %v83869, %v83859
%vm83878 = vcmp.lt.u32.totalorder %v83859, %v1868
%v83883 = vadd.s32 %v82456, %v1855
%v83887 = vadd.s32 1, %v83883
%v83891 = vsel /*vm=*/%vm83878, /*on_true_vy=*/%v83887, /*on_false_vx=*/%v83883
%v83895 = vadd.s32 1, %v83891
%v83899 = vsel /*vm=*/%vm83873, /*on_true_vy=*/%v83895, /*on_false_vx=*/%v83891
%v83904 = vadd.s32 %v83899, %v10
%v83908 = vadd.s32 %v83869, %v9
%v83912 = vadd.s32 %v83908, %v83904
%v83914 = vshll.u32 %v83908, 13
%v83915 = vshrl.u32 %v83908, 19
%v83916 = vor.u32 %v83915, %v83914
%v83917 = vxor.u32 %v83916, %v83912
%v83920 = vadd.s32 %v83917, %v83912
%v83922 = vshll.u32 %v83917, 15
%v83923 = vshrl.u32 %v83917, 17
%v83924 = vor.u32 %v83923, %v83922
%v83925 = vxor.u32 %v83924, %v83920
%v83928 = vadd.s32 %v83925, %v83920
%v83930 = vshll.u32 %v83925, 26
%v83931 = vshrl.u32 %v83925, 6
%v83932 = vor.u32 %v83931, %v83930
%v83933 = vxor.u32 %v83932, %v83928
%v83936 = vadd.s32 %v83933, %v83928
%v83940 = vadd.s32 %v83936, %v9
%v83942 = vshll.u32 %v83933, 6
%v83943 = vshrl.u32 %v83933, 26
%v83944 = vor.u32 %v83943, %v83942
%v83945 = vxor.u32 %v83944, %v83936
%v83948 = vadd.s32 %v83945, %v8
%v83952 = vadd.s32 1, %v83948
%v83956 = vadd.s32 %v83952, %v83940
%v83958 = vshll.u32 %v83952, 17
%v83959 = vshrl.u32 %v83952, 15
%v83960 = vor.u32 %v83959, %v83958
%v83961 = vxor.u32 %v83960, %v83956
%v83964 = vadd.s32 %v83961, %v83956
%v83966 = vshll.u32 %v83961, 29
%v83967 = vshrl.u32 %v83961, 3
%v83968 = vor.u32 %v83967, %v83966
%v83969 = vxor.u32 %v83968, %v83964
%v83972 = vadd.s32 %v83969, %v83964
%v83974 = vshll.u32 %v83969, 16
%v83975 = vshrl.u32 %v83969, 16
%v83976 = vor.u32 %v83975, %v83974
%v83977 = vxor.u32 %v83976, %v83972
%v83980 = vadd.s32 %v83977, %v83972
%v83984 = vadd.s32 %v83980, %v8
%v83986 = vshll.u32 %v83977, 24
%v83987 = vshrl.u32 %v83977, 8
%v83988 = vor.u32 %v83987, %v83986
%v83989 = vxor.u32 %v83988, %v83980
%v83992 = vadd.s32 %v83989, %v10
%v83996 = vadd.s32 2, %v83992
%v84000 = vadd.s32 %v83996, %v83984
%v84002 = vshll.u32 %v83996, 13
%v84003 = vshrl.u32 %v83996, 19
%v84004 = vor.u32 %v84003, %v84002
%v84005 = vxor.u32 %v84004, %v84000
%v84008 = vadd.s32 %v84005, %v84000
%v84010 = vshll.u32 %v84005, 15
%v84011 = vshrl.u32 %v84005, 17
%v84012 = vor.u32 %v84011, %v84010
%v84013 = vxor.u32 %v84012, %v84008
%v84016 = vadd.s32 %v84013, %v84008
%v84018 = vshll.u32 %v84013, 26
%v84019 = vshrl.u32 %v84013, 6
%v84020 = vor.u32 %v84019, %v84018
%v84021 = vxor.u32 %v84020, %v84016
%v84024 = vadd.s32 %v84021, %v84016
%v84028 = vadd.s32 %v84024, %v10
%v84030 = vshll.u32 %v84021, 6
%v84031 = vshrl.u32 %v84021, 26
%v84032 = vor.u32 %v84031, %v84030
%v84033 = vxor.u32 %v84032, %v84024
%v84036 = vadd.s32 %v84033, %v9
%v84040 = vadd.s32 3, %v84036
%v84044 = vadd.s32 %v84040, %v84028
%v84046 = vshll.u32 %v84040, 17
%v84047 = vshrl.u32 %v84040, 15
%v84048 = vor.u32 %v84047, %v84046
%v84049 = vxor.u32 %v84048, %v84044
%v84052 = vadd.s32 %v84049, %v84044
%v84054 = vshll.u32 %v84049, 29
%v84055 = vshrl.u32 %v84049, 3
%v84056 = vor.u32 %v84055, %v84054
%v84057 = vxor.u32 %v84056, %v84052
%v84060 = vadd.s32 %v84057, %v84052
%v84062 = vshll.u32 %v84057, 16
%v84063 = vshrl.u32 %v84057, 16
%v84064 = vor.u32 %v84063, %v84062
%v84065 = vxor.u32 %v84064, %v84060
%v84068 = vadd.s32 %v84065, %v84060
%v84072 = vadd.s32 %v84068, %v9
%v84074 = vshll.u32 %v84065, 24
%v84075 = vshrl.u32 %v84065, 8
%v84076 = vor.u32 %v84075, %v84074
%v84077 = vxor.u32 %v84076, %v84068
%v84080 = vadd.s32 %v84077, %v8
%v84084 = vadd.s32 4, %v84080
%v84088 = vadd.s32 %v84084, %v84072
%v84090 = vshll.u32 %v84084, 13
%v84091 = vshrl.u32 %v84084, 19
%v84092 = vor.u32 %v84091, %v84090
%v84093 = vxor.u32 %v84092, %v84088
%v84096 = vadd.s32 %v84093, %v84088
%v84098 = vshll.u32 %v84093, 15
%v84099 = vshrl.u32 %v84093, 17
%v84100 = vor.u32 %v84099, %v84098
%v84101 = vxor.u32 %v84100, %v84096
%v84104 = vadd.s32 %v84101, %v84096
%v84106 = vshll.u32 %v84101, 26
%v84107 = vshrl.u32 %v84101, 6
%v84108 = vor.u32 %v84107, %v84106
%v84109 = vxor.u32 %v84108, %v84104
%v84112 = vadd.s32 %v84109, %v84104
%v84116 = vadd.s32 %v84112, %v8
%v84118 = vshll.u32 %v84109, 6
%v84119 = vshrl.u32 %v84109, 26
%v84120 = vor.u32 %v84119, %v84118
%v84121 = vxor.u32 %v84120, %v84112
%v84124 = vadd.s32 %v84121, %v10
%v84128 = vadd.s32 5, %v84124
%v84130 = vxor.u32 %v84128, %v84116
%v84131 = vand.u32.u8 255, %v84130
%v84132 = vand.u32 65535, %v84131
%v84133 = vshrl.u32 %v84132, 1
%v84134 = vor.u32 16256, %v84133
%v84135 = vand.u32.u16 65535, %v84134
%v120194 = vadd.low.f32.bf16 -1.0, %v84135
%v84144 = vmul.f32 2.0, %v120194
%v84148 = vadd.f32 -0.99609375, %v84144
%v84152 = vmax.f32 %v84148, -0.99609375
%v84154 = vand.u32 2147483647, %v84152
%vm84157 = vcmp.eq.f32.partialorder %v84154, 1.0
%v84162 = vmul.f32 inf, %v84152
%v84164 = vxor.u32 2147483648, %v84152
%v84167 = vmul.f32 %v84164, %v84152
%v84169 = vadd.f32 1.0, %v84167
%v84170 = vlog2.pop %v84169
%v84171 = vmul.f32 0.6931472, %v84170
%v84172 = vmul.f32 -0.5, %v84167
%v84173 = vadd.f32 1.0, %v84172
%v84174 = vmul.f32 %v84173, %v84167
%v84175 = vand.u32 2147483647, %v84167
%vm84176 = vcmp.lt.f32.partialorder %v84175, 0.0004427343
%v84177 = vsel /*vm=*/%vm84176, /*on_true_vy=*/%v84174, /*on_false_vx=*/%v84171
%v84178 = vxor.u32 2147483648, %v84177
%vm84181 = vcmp.lt.f32.partialorder %v84178, 5.0
%v84186 = vsel /*vm=*/%vm84181, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v84190 = vsel /*vm=*/%vm84181, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v84194 = vsel /*vm=*/%vm84181, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v84198 = vsel /*vm=*/%vm84181, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v84202 = vsel /*vm=*/%vm84181, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v84206 = vsel /*vm=*/%vm84181, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v84210 = vsel /*vm=*/%vm84181, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v84214 = vsel /*vm=*/%vm84181, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v84218 = vsel /*vm=*/%vm84181, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v84222 = vadd.f32 -2.5, %v84178
%v84224 = vrsqrt.pop %v84178
%v84225 = vmul.f32 %v84224, %v84178
%vm84226 = vcmp.eq.f32.partialorder %v84178, inf
%v84227 = vsel /*vm=*/%vm84226, /*on_true_vy=*/%v84178, /*on_false_vx=*/%v84225
%vm84228 = vcmp.eq.f32.partialorder %v84178, 0.0
%v84229 = vand.u32 2147483648, %v84178
%v84230 = vsel /*vm=*/%vm84228, /*on_true_vy=*/%v84229, /*on_false_vx=*/%v84227
%v84233 = vadd.f32 -3.0, %v84230
%v84237 = vsel /*vm=*/%vm84181, /*on_true_vy=*/%v84222, /*on_false_vx=*/%v84233
%v84241 = vmul.f32 %v84237, %v84218
%v84245 = vadd.f32 %v84241, %v84214
%v84249 = vmul.f32 %v84245, %v84237
%v84253 = vadd.f32 %v84249, %v84210
%v84257 = vmul.f32 %v84253, %v84237
%v84261 = vadd.f32 %v84257, %v84206
%v84265 = vmul.f32 %v84261, %v84237
%v84269 = vadd.f32 %v84265, %v84202
%v84273 = vmul.f32 %v84269, %v84237
%v84277 = vadd.f32 %v84273, %v84198
%v84281 = vmul.f32 %v84277, %v84237
%v84285 = vadd.f32 %v84281, %v84194
%v84289 = vmul.f32 %v84285, %v84237
%v84293 = vadd.f32 %v84289, %v84190
%v84297 = vmul.f32 %v84293, %v84237
%v84301 = vadd.f32 %v84297, %v84186
%v84305 = vmul.f32 %v84301, %v84152
%v84309 = vsel /*vm=*/%vm84157, /*on_true_vy=*/%v84162, /*on_false_vx=*/%v84305
%v84313 = vmul.f32 1.4140625, %v84309
%v84316 = vpack.c.bf16 %v120417, %v84313
%120195 = vst [vmem:[%s280 + $0x1d8] sm:$0xf] /*vst_source=*/%v84316
%v84320 = vadd.s32 %v82473, %v2355
%v84330 = vadd.s32 %v84320, %v415
%vm84334 = vcmp.lt.u32.totalorder %v84330, %v84320
%vm84339 = vcmp.lt.u32.totalorder %v84320, %v2355
%v84344 = vadd.s32 %v82456, %v2342
%v84348 = vadd.s32 1, %v84344
%v84352 = vsel /*vm=*/%vm84339, /*on_true_vy=*/%v84348, /*on_false_vx=*/%v84344
%v84356 = vadd.s32 1, %v84352
%v84360 = vsel /*vm=*/%vm84334, /*on_true_vy=*/%v84356, /*on_false_vx=*/%v84352
%v84365 = vadd.s32 %v84360, %v10
%v84369 = vadd.s32 %v84330, %v9
%v84373 = vadd.s32 %v84369, %v84365
%v84375 = vshll.u32 %v84369, 13
%v84376 = vshrl.u32 %v84369, 19
%v84377 = vor.u32 %v84376, %v84375
%v84378 = vxor.u32 %v84377, %v84373
%v84381 = vadd.s32 %v84378, %v84373
%v84383 = vshll.u32 %v84378, 15
%v84384 = vshrl.u32 %v84378, 17
%v84385 = vor.u32 %v84384, %v84383
%v84386 = vxor.u32 %v84385, %v84381
%v84389 = vadd.s32 %v84386, %v84381
%v84391 = vshll.u32 %v84386, 26
%v84392 = vshrl.u32 %v84386, 6
%v84393 = vor.u32 %v84392, %v84391
%v84394 = vxor.u32 %v84393, %v84389
%v84397 = vadd.s32 %v84394, %v84389
%v84401 = vadd.s32 %v84397, %v9
%v84403 = vshll.u32 %v84394, 6
%v84404 = vshrl.u32 %v84394, 26
%v84405 = vor.u32 %v84404, %v84403
%v84406 = vxor.u32 %v84405, %v84397
%v84409 = vadd.s32 %v84406, %v8
%v84413 = vadd.s32 1, %v84409
%v84417 = vadd.s32 %v84413, %v84401
%v84419 = vshll.u32 %v84413, 17
%v84420 = vshrl.u32 %v84413, 15
%v84421 = vor.u32 %v84420, %v84419
%v84422 = vxor.u32 %v84421, %v84417
%v84425 = vadd.s32 %v84422, %v84417
%v84427 = vshll.u32 %v84422, 29
%v84428 = vshrl.u32 %v84422, 3
%v84429 = vor.u32 %v84428, %v84427
%v84430 = vxor.u32 %v84429, %v84425
%v84433 = vadd.s32 %v84430, %v84425
%v84435 = vshll.u32 %v84430, 16
%v84436 = vshrl.u32 %v84430, 16
%v84437 = vor.u32 %v84436, %v84435
%v84438 = vxor.u32 %v84437, %v84433
%v84441 = vadd.s32 %v84438, %v84433
%v84445 = vadd.s32 %v84441, %v8
%v84447 = vshll.u32 %v84438, 24
%v84448 = vshrl.u32 %v84438, 8
%v84449 = vor.u32 %v84448, %v84447
%v84450 = vxor.u32 %v84449, %v84441
%v84453 = vadd.s32 %v84450, %v10
%v84457 = vadd.s32 2, %v84453
%v84461 = vadd.s32 %v84457, %v84445
%v84463 = vshll.u32 %v84457, 13
%v84464 = vshrl.u32 %v84457, 19
%v84465 = vor.u32 %v84464, %v84463
%v84466 = vxor.u32 %v84465, %v84461
%v84469 = vadd.s32 %v84466, %v84461
%v84471 = vshll.u32 %v84466, 15
%v84472 = vshrl.u32 %v84466, 17
%v84473 = vor.u32 %v84472, %v84471
%v84474 = vxor.u32 %v84473, %v84469
%v84477 = vadd.s32 %v84474, %v84469
%v84479 = vshll.u32 %v84474, 26
%v84480 = vshrl.u32 %v84474, 6
%v84481 = vor.u32 %v84480, %v84479
%v84482 = vxor.u32 %v84481, %v84477
%v84485 = vadd.s32 %v84482, %v84477
%v84489 = vadd.s32 %v84485, %v10
%v84491 = vshll.u32 %v84482, 6
%v84492 = vshrl.u32 %v84482, 26
%v84493 = vor.u32 %v84492, %v84491
%v84494 = vxor.u32 %v84493, %v84485
%v84497 = vadd.s32 %v84494, %v9
%v84501 = vadd.s32 3, %v84497
%v84505 = vadd.s32 %v84501, %v84489
%v84507 = vshll.u32 %v84501, 17
%v84508 = vshrl.u32 %v84501, 15
%v84509 = vor.u32 %v84508, %v84507
%v84510 = vxor.u32 %v84509, %v84505
%v84513 = vadd.s32 %v84510, %v84505
%v84515 = vshll.u32 %v84510, 29
%v84516 = vshrl.u32 %v84510, 3
%v84517 = vor.u32 %v84516, %v84515
%v84518 = vxor.u32 %v84517, %v84513
%v84521 = vadd.s32 %v84518, %v84513
%v84523 = vshll.u32 %v84518, 16
%v84524 = vshrl.u32 %v84518, 16
%v84525 = vor.u32 %v84524, %v84523
%v84526 = vxor.u32 %v84525, %v84521
%v84529 = vadd.s32 %v84526, %v84521
%v84533 = vadd.s32 %v84529, %v9
%v84535 = vshll.u32 %v84526, 24
%v84536 = vshrl.u32 %v84526, 8
%v84537 = vor.u32 %v84536, %v84535
%v84538 = vxor.u32 %v84537, %v84529
%v84541 = vadd.s32 %v84538, %v8
%v84545 = vadd.s32 4, %v84541
%v84549 = vadd.s32 %v84545, %v84533
%v84551 = vshll.u32 %v84545, 13
%v84552 = vshrl.u32 %v84545, 19
%v84553 = vor.u32 %v84552, %v84551
%v84554 = vxor.u32 %v84553, %v84549
%v84557 = vadd.s32 %v84554, %v84549
%v84559 = vshll.u32 %v84554, 15
%v84560 = vshrl.u32 %v84554, 17
%v84561 = vor.u32 %v84560, %v84559
%v84562 = vxor.u32 %v84561, %v84557
%v84565 = vadd.s32 %v84562, %v84557
%v84567 = vshll.u32 %v84562, 26
%v84568 = vshrl.u32 %v84562, 6
%v84569 = vor.u32 %v84568, %v84567
%v84570 = vxor.u32 %v84569, %v84565
%v84573 = vadd.s32 %v84570, %v84565
%v84577 = vadd.s32 %v84573, %v8
%v84579 = vshll.u32 %v84570, 6
%v84580 = vshrl.u32 %v84570, 26
%v84581 = vor.u32 %v84580, %v84579
%v84582 = vxor.u32 %v84581, %v84573
%v84585 = vadd.s32 %v84582, %v10
%v84589 = vadd.s32 5, %v84585
%v84591 = vxor.u32 %v84589, %v84577
%v84592 = vand.u32.u8 255, %v84591
%v84593 = vand.u32 65535, %v84592
%v84594 = vshrl.u32 %v84593, 1
%v84595 = vor.u32 16256, %v84594
%v84596 = vand.u32.u16 65535, %v84595
%v120196 = vadd.low.f32.bf16 -1.0, %v84596
%v84605 = vmul.f32 2.0, %v120196
%v84609 = vadd.f32 -0.99609375, %v84605
%v84613 = vmax.f32 %v84609, -0.99609375
%v84615 = vand.u32 2147483647, %v84613
%vm84618 = vcmp.eq.f32.partialorder %v84615, 1.0
%v84623 = vmul.f32 inf, %v84613
%v84625 = vxor.u32 2147483648, %v84613
%v84628 = vmul.f32 %v84625, %v84613
%v84630 = vadd.f32 1.0, %v84628
%v84631 = vlog2.pop %v84630
%v84632 = vmul.f32 0.6931472, %v84631
%v84633 = vmul.f32 -0.5, %v84628
%v84634 = vadd.f32 1.0, %v84633
%v84635 = vmul.f32 %v84634, %v84628
%v84636 = vand.u32 2147483647, %v84628
%vm84637 = vcmp.lt.f32.partialorder %v84636, 0.0004427343
%v84638 = vsel /*vm=*/%vm84637, /*on_true_vy=*/%v84635, /*on_false_vx=*/%v84632
%v84639 = vxor.u32 2147483648, %v84638
%vm84642 = vcmp.lt.f32.partialorder %v84639, 5.0
%v84647 = vsel /*vm=*/%vm84642, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v84651 = vsel /*vm=*/%vm84642, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v84655 = vsel /*vm=*/%vm84642, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v84659 = vsel /*vm=*/%vm84642, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v84663 = vsel /*vm=*/%vm84642, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v84667 = vsel /*vm=*/%vm84642, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v84671 = vsel /*vm=*/%vm84642, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v84675 = vsel /*vm=*/%vm84642, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v84679 = vsel /*vm=*/%vm84642, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v84683 = vadd.f32 -2.5, %v84639
%v84685 = vrsqrt.pop %v84639
%v84686 = vmul.f32 %v84685, %v84639
%vm84687 = vcmp.eq.f32.partialorder %v84639, inf
%v84688 = vsel /*vm=*/%vm84687, /*on_true_vy=*/%v84639, /*on_false_vx=*/%v84686
%vm84689 = vcmp.eq.f32.partialorder %v84639, 0.0
%v84690 = vand.u32 2147483648, %v84639
%v84691 = vsel /*vm=*/%vm84689, /*on_true_vy=*/%v84690, /*on_false_vx=*/%v84688
%v84694 = vadd.f32 -3.0, %v84691
%v84698 = vsel /*vm=*/%vm84642, /*on_true_vy=*/%v84683, /*on_false_vx=*/%v84694
%v84702 = vmul.f32 %v84698, %v84679
%v84706 = vadd.f32 %v84702, %v84675
%v84710 = vmul.f32 %v84706, %v84698
%v84714 = vadd.f32 %v84710, %v84671
%v84718 = vmul.f32 %v84714, %v84698
%v84722 = vadd.f32 %v84718, %v84667
%v84726 = vmul.f32 %v84722, %v84698
%v84730 = vadd.f32 %v84726, %v84663
%v84734 = vmul.f32 %v84730, %v84698
%v84738 = vadd.f32 %v84734, %v84659
%v84742 = vmul.f32 %v84738, %v84698
%v84746 = vadd.f32 %v84742, %v84655
%v84750 = vmul.f32 %v84746, %v84698
%v84754 = vadd.f32 %v84750, %v84651
%v84758 = vmul.f32 %v84754, %v84698
%v84762 = vadd.f32 %v84758, %v84647
%v84766 = vmul.f32 %v84762, %v84613
%v84770 = vsel /*vm=*/%vm84618, /*on_true_vy=*/%v84623, /*on_false_vx=*/%v84766
%v84774 = vmul.f32 1.4140625, %v84770
%v84777 = vpack.c.bf16 %v120417, %v84774
%120197 = vst [vmem:[%s280 + $0x258] sm:$0xf] /*vst_source=*/%v84777
%v84781 = vadd.s32 %v82473, %v2842
%v84791 = vadd.s32 %v84781, %v415
%vm84795 = vcmp.lt.u32.totalorder %v84791, %v84781
%vm84800 = vcmp.lt.u32.totalorder %v84781, %v2842
%v84805 = vadd.s32 %v82456, %v2829
%v84809 = vadd.s32 1, %v84805
%v84813 = vsel /*vm=*/%vm84800, /*on_true_vy=*/%v84809, /*on_false_vx=*/%v84805
%v84817 = vadd.s32 1, %v84813
%v84821 = vsel /*vm=*/%vm84795, /*on_true_vy=*/%v84817, /*on_false_vx=*/%v84813
%v84826 = vadd.s32 %v84821, %v10
%v84830 = vadd.s32 %v84791, %v9
%v84834 = vadd.s32 %v84830, %v84826
%v84836 = vshll.u32 %v84830, 13
%v84837 = vshrl.u32 %v84830, 19
%v84838 = vor.u32 %v84837, %v84836
%v84839 = vxor.u32 %v84838, %v84834
%v84842 = vadd.s32 %v84839, %v84834
%v84844 = vshll.u32 %v84839, 15
%v84845 = vshrl.u32 %v84839, 17
%v84846 = vor.u32 %v84845, %v84844
%v84847 = vxor.u32 %v84846, %v84842
%v84850 = vadd.s32 %v84847, %v84842
%v84852 = vshll.u32 %v84847, 26
%v84853 = vshrl.u32 %v84847, 6
%v84854 = vor.u32 %v84853, %v84852
%v84855 = vxor.u32 %v84854, %v84850
%v84858 = vadd.s32 %v84855, %v84850
%v84862 = vadd.s32 %v84858, %v9
%v84864 = vshll.u32 %v84855, 6
%v84865 = vshrl.u32 %v84855, 26
%v84866 = vor.u32 %v84865, %v84864
%v84867 = vxor.u32 %v84866, %v84858
%v84870 = vadd.s32 %v84867, %v8
%v84874 = vadd.s32 1, %v84870
%v84878 = vadd.s32 %v84874, %v84862
%v84880 = vshll.u32 %v84874, 17
%v84881 = vshrl.u32 %v84874, 15
%v84882 = vor.u32 %v84881, %v84880
%v84883 = vxor.u32 %v84882, %v84878
%v84886 = vadd.s32 %v84883, %v84878
%v84888 = vshll.u32 %v84883, 29
%v84889 = vshrl.u32 %v84883, 3
%v84890 = vor.u32 %v84889, %v84888
%v84891 = vxor.u32 %v84890, %v84886
%v84894 = vadd.s32 %v84891, %v84886
%v84896 = vshll.u32 %v84891, 16
%v84897 = vshrl.u32 %v84891, 16
%v84898 = vor.u32 %v84897, %v84896
%v84899 = vxor.u32 %v84898, %v84894
%v84902 = vadd.s32 %v84899, %v84894
%v84906 = vadd.s32 %v84902, %v8
%v84908 = vshll.u32 %v84899, 24
%v84909 = vshrl.u32 %v84899, 8
%v84910 = vor.u32 %v84909, %v84908
%v84911 = vxor.u32 %v84910, %v84902
%v84914 = vadd.s32 %v84911, %v10
%v84918 = vadd.s32 2, %v84914
%v84922 = vadd.s32 %v84918, %v84906
%v84924 = vshll.u32 %v84918, 13
%v84925 = vshrl.u32 %v84918, 19
%v84926 = vor.u32 %v84925, %v84924
%v84927 = vxor.u32 %v84926, %v84922
%v84930 = vadd.s32 %v84927, %v84922
%v84932 = vshll.u32 %v84927, 15
%v84933 = vshrl.u32 %v84927, 17
%v84934 = vor.u32 %v84933, %v84932
%v84935 = vxor.u32 %v84934, %v84930
%v84938 = vadd.s32 %v84935, %v84930
%v84940 = vshll.u32 %v84935, 26
%v84941 = vshrl.u32 %v84935, 6
%v84942 = vor.u32 %v84941, %v84940
%v84943 = vxor.u32 %v84942, %v84938
%v84946 = vadd.s32 %v84943, %v84938
%v84950 = vadd.s32 %v84946, %v10
%v84952 = vshll.u32 %v84943, 6
%v84953 = vshrl.u32 %v84943, 26
%v84954 = vor.u32 %v84953, %v84952
%v84955 = vxor.u32 %v84954, %v84946
%v84958 = vadd.s32 %v84955, %v9
%v84962 = vadd.s32 3, %v84958
%v84966 = vadd.s32 %v84962, %v84950
%v84968 = vshll.u32 %v84962, 17
%v84969 = vshrl.u32 %v84962, 15
%v84970 = vor.u32 %v84969, %v84968
%v84971 = vxor.u32 %v84970, %v84966
%v84974 = vadd.s32 %v84971, %v84966
%v84976 = vshll.u32 %v84971, 29
%v84977 = vshrl.u32 %v84971, 3
%v84978 = vor.u32 %v84977, %v84976
%v84979 = vxor.u32 %v84978, %v84974
%v84982 = vadd.s32 %v84979, %v84974
%v84984 = vshll.u32 %v84979, 16
%v84985 = vshrl.u32 %v84979, 16
%v84986 = vor.u32 %v84985, %v84984
%v84987 = vxor.u32 %v84986, %v84982
%v84990 = vadd.s32 %v84987, %v84982
%v84994 = vadd.s32 %v84990, %v9
%v84996 = vshll.u32 %v84987, 24
%v84997 = vshrl.u32 %v84987, 8
%v84998 = vor.u32 %v84997, %v84996
%v84999 = vxor.u32 %v84998, %v84990
%v85002 = vadd.s32 %v84999, %v8
%v85006 = vadd.s32 4, %v85002
%v85010 = vadd.s32 %v85006, %v84994
%v85012 = vshll.u32 %v85006, 13
%v85013 = vshrl.u32 %v85006, 19
%v85014 = vor.u32 %v85013, %v85012
%v85015 = vxor.u32 %v85014, %v85010
%v85018 = vadd.s32 %v85015, %v85010
%v85020 = vshll.u32 %v85015, 15
%v85021 = vshrl.u32 %v85015, 17
%v85022 = vor.u32 %v85021, %v85020
%v85023 = vxor.u32 %v85022, %v85018
%v85026 = vadd.s32 %v85023, %v85018
%v85028 = vshll.u32 %v85023, 26
%v85029 = vshrl.u32 %v85023, 6
%v85030 = vor.u32 %v85029, %v85028
%v85031 = vxor.u32 %v85030, %v85026
%v85034 = vadd.s32 %v85031, %v85026
%v85038 = vadd.s32 %v85034, %v8
%v85040 = vshll.u32 %v85031, 6
%v85041 = vshrl.u32 %v85031, 26
%v85042 = vor.u32 %v85041, %v85040
%v85043 = vxor.u32 %v85042, %v85034
%v85046 = vadd.s32 %v85043, %v10
%v85050 = vadd.s32 5, %v85046
%v85052 = vxor.u32 %v85050, %v85038
%v85053 = vand.u32.u8 255, %v85052
%v85054 = vand.u32 65535, %v85053
%v85055 = vshrl.u32 %v85054, 1
%v85056 = vor.u32 16256, %v85055
%v85057 = vand.u32.u16 65535, %v85056
%v120198 = vadd.low.f32.bf16 -1.0, %v85057
%v85066 = vmul.f32 2.0, %v120198
%v85070 = vadd.f32 -0.99609375, %v85066
%v85074 = vmax.f32 %v85070, -0.99609375
%v85076 = vand.u32 2147483647, %v85074
%vm85079 = vcmp.eq.f32.partialorder %v85076, 1.0
%v85084 = vmul.f32 inf, %v85074
%v85086 = vxor.u32 2147483648, %v85074
%v85089 = vmul.f32 %v85086, %v85074
%v85091 = vadd.f32 1.0, %v85089
%v85092 = vlog2.pop %v85091
%v85093 = vmul.f32 0.6931472, %v85092
%v85094 = vmul.f32 -0.5, %v85089
%v85095 = vadd.f32 1.0, %v85094
%v85096 = vmul.f32 %v85095, %v85089
%v85097 = vand.u32 2147483647, %v85089
%vm85098 = vcmp.lt.f32.partialorder %v85097, 0.0004427343
%v85099 = vsel /*vm=*/%vm85098, /*on_true_vy=*/%v85096, /*on_false_vx=*/%v85093
%v85100 = vxor.u32 2147483648, %v85099
%vm85103 = vcmp.lt.f32.partialorder %v85100, 5.0
%v85108 = vsel /*vm=*/%vm85103, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v85112 = vsel /*vm=*/%vm85103, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v85116 = vsel /*vm=*/%vm85103, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v85120 = vsel /*vm=*/%vm85103, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v85124 = vsel /*vm=*/%vm85103, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v85128 = vsel /*vm=*/%vm85103, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v85132 = vsel /*vm=*/%vm85103, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v85136 = vsel /*vm=*/%vm85103, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v85140 = vsel /*vm=*/%vm85103, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v85144 = vadd.f32 -2.5, %v85100
%v85146 = vrsqrt.pop %v85100
%v85147 = vmul.f32 %v85146, %v85100
%vm85148 = vcmp.eq.f32.partialorder %v85100, inf
%v85149 = vsel /*vm=*/%vm85148, /*on_true_vy=*/%v85100, /*on_false_vx=*/%v85147
%vm85150 = vcmp.eq.f32.partialorder %v85100, 0.0
%v85151 = vand.u32 2147483648, %v85100
%v85152 = vsel /*vm=*/%vm85150, /*on_true_vy=*/%v85151, /*on_false_vx=*/%v85149
%v85155 = vadd.f32 -3.0, %v85152
%v85159 = vsel /*vm=*/%vm85103, /*on_true_vy=*/%v85144, /*on_false_vx=*/%v85155
%v85163 = vmul.f32 %v85159, %v85140
%v85167 = vadd.f32 %v85163, %v85136
%v85171 = vmul.f32 %v85167, %v85159
%v85175 = vadd.f32 %v85171, %v85132
%v85179 = vmul.f32 %v85175, %v85159
%v85183 = vadd.f32 %v85179, %v85128
%v85187 = vmul.f32 %v85183, %v85159
%v85191 = vadd.f32 %v85187, %v85124
%v85195 = vmul.f32 %v85191, %v85159
%v85199 = vadd.f32 %v85195, %v85120
%v85203 = vmul.f32 %v85199, %v85159
%v85207 = vadd.f32 %v85203, %v85116
%v85211 = vmul.f32 %v85207, %v85159
%v85215 = vadd.f32 %v85211, %v85112
%v85219 = vmul.f32 %v85215, %v85159
%v85223 = vadd.f32 %v85219, %v85108
%v85227 = vmul.f32 %v85223, %v85074
%v85231 = vsel /*vm=*/%vm85079, /*on_true_vy=*/%v85084, /*on_false_vx=*/%v85227
%v85235 = vmul.f32 1.4140625, %v85231
%v85238 = vpack.c.bf16 %v120417, %v85235
%120199 = vst [vmem:[%s280 + $0x2d8] sm:$0xf] /*vst_source=*/%v85238
%v85242 = vadd.s32 %v82473, %v3329
%v85252 = vadd.s32 %v85242, %v415
%vm85256 = vcmp.lt.u32.totalorder %v85252, %v85242
%vm85261 = vcmp.lt.u32.totalorder %v85242, %v3329
%v85266 = vadd.s32 %v82456, %v3316
%v85270 = vadd.s32 1, %v85266
%v85274 = vsel /*vm=*/%vm85261, /*on_true_vy=*/%v85270, /*on_false_vx=*/%v85266
%v85278 = vadd.s32 1, %v85274
%v85282 = vsel /*vm=*/%vm85256, /*on_true_vy=*/%v85278, /*on_false_vx=*/%v85274
%v85287 = vadd.s32 %v85282, %v10
%v85291 = vadd.s32 %v85252, %v9
%v85295 = vadd.s32 %v85291, %v85287
%v85297 = vshll.u32 %v85291, 13
%v85298 = vshrl.u32 %v85291, 19
%v85299 = vor.u32 %v85298, %v85297
%v85300 = vxor.u32 %v85299, %v85295
%v85303 = vadd.s32 %v85300, %v85295
%v85305 = vshll.u32 %v85300, 15
%v85306 = vshrl.u32 %v85300, 17
%v85307 = vor.u32 %v85306, %v85305
%v85308 = vxor.u32 %v85307, %v85303
%v85311 = vadd.s32 %v85308, %v85303
%v85313 = vshll.u32 %v85308, 26
%v85314 = vshrl.u32 %v85308, 6
%v85315 = vor.u32 %v85314, %v85313
%v85316 = vxor.u32 %v85315, %v85311
%v85319 = vadd.s32 %v85316, %v85311
%v85323 = vadd.s32 %v85319, %v9
%v85325 = vshll.u32 %v85316, 6
%v85326 = vshrl.u32 %v85316, 26
%v85327 = vor.u32 %v85326, %v85325
%v85328 = vxor.u32 %v85327, %v85319
%v85331 = vadd.s32 %v85328, %v8
%v85335 = vadd.s32 1, %v85331
%v85339 = vadd.s32 %v85335, %v85323
%v85341 = vshll.u32 %v85335, 17
%v85342 = vshrl.u32 %v85335, 15
%v85343 = vor.u32 %v85342, %v85341
%v85344 = vxor.u32 %v85343, %v85339
%v85347 = vadd.s32 %v85344, %v85339
%v85349 = vshll.u32 %v85344, 29
%v85350 = vshrl.u32 %v85344, 3
%v85351 = vor.u32 %v85350, %v85349
%v85352 = vxor.u32 %v85351, %v85347
%v85355 = vadd.s32 %v85352, %v85347
%v85357 = vshll.u32 %v85352, 16
%v85358 = vshrl.u32 %v85352, 16
%v85359 = vor.u32 %v85358, %v85357
%v85360 = vxor.u32 %v85359, %v85355
%v85363 = vadd.s32 %v85360, %v85355
%v85367 = vadd.s32 %v85363, %v8
%v85369 = vshll.u32 %v85360, 24
%v85370 = vshrl.u32 %v85360, 8
%v85371 = vor.u32 %v85370, %v85369
%v85372 = vxor.u32 %v85371, %v85363
%v85375 = vadd.s32 %v85372, %v10
%v85379 = vadd.s32 2, %v85375
%v85383 = vadd.s32 %v85379, %v85367
%v85385 = vshll.u32 %v85379, 13
%v85386 = vshrl.u32 %v85379, 19
%v85387 = vor.u32 %v85386, %v85385
%v85388 = vxor.u32 %v85387, %v85383
%v85391 = vadd.s32 %v85388, %v85383
%v85393 = vshll.u32 %v85388, 15
%v85394 = vshrl.u32 %v85388, 17
%v85395 = vor.u32 %v85394, %v85393
%v85396 = vxor.u32 %v85395, %v85391
%v85399 = vadd.s32 %v85396, %v85391
%v85401 = vshll.u32 %v85396, 26
%v85402 = vshrl.u32 %v85396, 6
%v85403 = vor.u32 %v85402, %v85401
%v85404 = vxor.u32 %v85403, %v85399
%v85407 = vadd.s32 %v85404, %v85399
%v85411 = vadd.s32 %v85407, %v10
%v85413 = vshll.u32 %v85404, 6
%v85414 = vshrl.u32 %v85404, 26
%v85415 = vor.u32 %v85414, %v85413
%v85416 = vxor.u32 %v85415, %v85407
%v85419 = vadd.s32 %v85416, %v9
%v85423 = vadd.s32 3, %v85419
%v85427 = vadd.s32 %v85423, %v85411
%v85429 = vshll.u32 %v85423, 17
%v85430 = vshrl.u32 %v85423, 15
%v85431 = vor.u32 %v85430, %v85429
%v85432 = vxor.u32 %v85431, %v85427
%v85435 = vadd.s32 %v85432, %v85427
%v85437 = vshll.u32 %v85432, 29
%v85438 = vshrl.u32 %v85432, 3
%v85439 = vor.u32 %v85438, %v85437
%v85440 = vxor.u32 %v85439, %v85435
%v85443 = vadd.s32 %v85440, %v85435
%v85445 = vshll.u32 %v85440, 16
%v85446 = vshrl.u32 %v85440, 16
%v85447 = vor.u32 %v85446, %v85445
%v85448 = vxor.u32 %v85447, %v85443
%v85451 = vadd.s32 %v85448, %v85443
%v85455 = vadd.s32 %v85451, %v9
%v85457 = vshll.u32 %v85448, 24
%v85458 = vshrl.u32 %v85448, 8
%v85459 = vor.u32 %v85458, %v85457
%v85460 = vxor.u32 %v85459, %v85451
%v85463 = vadd.s32 %v85460, %v8
%v85467 = vadd.s32 4, %v85463
%v85471 = vadd.s32 %v85467, %v85455
%v85473 = vshll.u32 %v85467, 13
%v85474 = vshrl.u32 %v85467, 19
%v85475 = vor.u32 %v85474, %v85473
%v85476 = vxor.u32 %v85475, %v85471
%v85479 = vadd.s32 %v85476, %v85471
%v85481 = vshll.u32 %v85476, 15
%v85482 = vshrl.u32 %v85476, 17
%v85483 = vor.u32 %v85482, %v85481
%v85484 = vxor.u32 %v85483, %v85479
%v85487 = vadd.s32 %v85484, %v85479
%v85489 = vshll.u32 %v85484, 26
%v85490 = vshrl.u32 %v85484, 6
%v85491 = vor.u32 %v85490, %v85489
%v85492 = vxor.u32 %v85491, %v85487
%v85495 = vadd.s32 %v85492, %v85487
%v85499 = vadd.s32 %v85495, %v8
%v85501 = vshll.u32 %v85492, 6
%v85502 = vshrl.u32 %v85492, 26
%v85503 = vor.u32 %v85502, %v85501
%v85504 = vxor.u32 %v85503, %v85495
%v85507 = vadd.s32 %v85504, %v10
%v85511 = vadd.s32 5, %v85507
%v85513 = vxor.u32 %v85511, %v85499
%v85514 = vand.u32.u8 255, %v85513
%v85515 = vand.u32 65535, %v85514
%v85516 = vshrl.u32 %v85515, 1
%v85517 = vor.u32 16256, %v85516
%v85518 = vand.u32.u16 65535, %v85517
%v120200 = vadd.low.f32.bf16 -1.0, %v85518
%v85527 = vmul.f32 2.0, %v120200
%v85531 = vadd.f32 -0.99609375, %v85527
%v85535 = vmax.f32 %v85531, -0.99609375
%v85537 = vand.u32 2147483647, %v85535
%vm85540 = vcmp.eq.f32.partialorder %v85537, 1.0
%v85545 = vmul.f32 inf, %v85535
%v85547 = vxor.u32 2147483648, %v85535
%v85550 = vmul.f32 %v85547, %v85535
%v85552 = vadd.f32 1.0, %v85550
%v85553 = vlog2.pop %v85552
%v85554 = vmul.f32 0.6931472, %v85553
%v85555 = vmul.f32 -0.5, %v85550
%v85556 = vadd.f32 1.0, %v85555
%v85557 = vmul.f32 %v85556, %v85550
%v85558 = vand.u32 2147483647, %v85550
%vm85559 = vcmp.lt.f32.partialorder %v85558, 0.0004427343
%v85560 = vsel /*vm=*/%vm85559, /*on_true_vy=*/%v85557, /*on_false_vx=*/%v85554
%v85561 = vxor.u32 2147483648, %v85560
%vm85564 = vcmp.lt.f32.partialorder %v85561, 5.0
%v85569 = vsel /*vm=*/%vm85564, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v85573 = vsel /*vm=*/%vm85564, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v85577 = vsel /*vm=*/%vm85564, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v85581 = vsel /*vm=*/%vm85564, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v85585 = vsel /*vm=*/%vm85564, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v85589 = vsel /*vm=*/%vm85564, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v85593 = vsel /*vm=*/%vm85564, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v85597 = vsel /*vm=*/%vm85564, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v85601 = vsel /*vm=*/%vm85564, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v85605 = vadd.f32 -2.5, %v85561
%v85607 = vrsqrt.pop %v85561
%v85608 = vmul.f32 %v85607, %v85561
%vm85609 = vcmp.eq.f32.partialorder %v85561, inf
%v85610 = vsel /*vm=*/%vm85609, /*on_true_vy=*/%v85561, /*on_false_vx=*/%v85608
%vm85611 = vcmp.eq.f32.partialorder %v85561, 0.0
%v85612 = vand.u32 2147483648, %v85561
%v85613 = vsel /*vm=*/%vm85611, /*on_true_vy=*/%v85612, /*on_false_vx=*/%v85610
%v85616 = vadd.f32 -3.0, %v85613
%v85620 = vsel /*vm=*/%vm85564, /*on_true_vy=*/%v85605, /*on_false_vx=*/%v85616
%v85624 = vmul.f32 %v85620, %v85601
%v85628 = vadd.f32 %v85624, %v85597
%v85632 = vmul.f32 %v85628, %v85620
%v85636 = vadd.f32 %v85632, %v85593
%v85640 = vmul.f32 %v85636, %v85620
%v85644 = vadd.f32 %v85640, %v85589
%v85648 = vmul.f32 %v85644, %v85620
%v85652 = vadd.f32 %v85648, %v85585
%v85656 = vmul.f32 %v85652, %v85620
%v85660 = vadd.f32 %v85656, %v85581
%v85664 = vmul.f32 %v85660, %v85620
%v85668 = vadd.f32 %v85664, %v85577
%v85672 = vmul.f32 %v85668, %v85620
%v85676 = vadd.f32 %v85672, %v85573
%v85680 = vmul.f32 %v85676, %v85620
%v85684 = vadd.f32 %v85680, %v85569
%v85688 = vmul.f32 %v85684, %v85535
%v85692 = vsel /*vm=*/%vm85540, /*on_true_vy=*/%v85545, /*on_false_vx=*/%v85688
%v85696 = vmul.f32 1.4140625, %v85692
%v85699 = vpack.c.bf16 %v120417, %v85696
%120201 = vst [vmem:[%s280 + $0x358] sm:$0xf] /*vst_source=*/%v85699
%v85703 = vadd.s32 %v82473, %v3816
%v85713 = vadd.s32 %v85703, %v415
%vm85717 = vcmp.lt.u32.totalorder %v85713, %v85703
%vm85722 = vcmp.lt.u32.totalorder %v85703, %v3816
%v85727 = vadd.s32 %v82456, %v3803
%v85731 = vadd.s32 1, %v85727
%v85735 = vsel /*vm=*/%vm85722, /*on_true_vy=*/%v85731, /*on_false_vx=*/%v85727
%v85739 = vadd.s32 1, %v85735
%v85743 = vsel /*vm=*/%vm85717, /*on_true_vy=*/%v85739, /*on_false_vx=*/%v85735
%v85748 = vadd.s32 %v85743, %v10
%v85752 = vadd.s32 %v85713, %v9
%v85756 = vadd.s32 %v85752, %v85748
%v85758 = vshll.u32 %v85752, 13
%v85759 = vshrl.u32 %v85752, 19
%v85760 = vor.u32 %v85759, %v85758
%v85761 = vxor.u32 %v85760, %v85756
%v85764 = vadd.s32 %v85761, %v85756
%v85766 = vshll.u32 %v85761, 15
%v85767 = vshrl.u32 %v85761, 17
%v85768 = vor.u32 %v85767, %v85766
%v85769 = vxor.u32 %v85768, %v85764
%v85772 = vadd.s32 %v85769, %v85764
%v85774 = vshll.u32 %v85769, 26
%v85775 = vshrl.u32 %v85769, 6
%v85776 = vor.u32 %v85775, %v85774
%v85777 = vxor.u32 %v85776, %v85772
%v85780 = vadd.s32 %v85777, %v85772
%v85784 = vadd.s32 %v85780, %v9
%v85786 = vshll.u32 %v85777, 6
%v85787 = vshrl.u32 %v85777, 26
%v85788 = vor.u32 %v85787, %v85786
%v85789 = vxor.u32 %v85788, %v85780
%v85792 = vadd.s32 %v85789, %v8
%v85796 = vadd.s32 1, %v85792
%v85800 = vadd.s32 %v85796, %v85784
%v85802 = vshll.u32 %v85796, 17
%v85803 = vshrl.u32 %v85796, 15
%v85804 = vor.u32 %v85803, %v85802
%v85805 = vxor.u32 %v85804, %v85800
%v85808 = vadd.s32 %v85805, %v85800
%v85810 = vshll.u32 %v85805, 29
%v85811 = vshrl.u32 %v85805, 3
%v85812 = vor.u32 %v85811, %v85810
%v85813 = vxor.u32 %v85812, %v85808
%v85816 = vadd.s32 %v85813, %v85808
%v85818 = vshll.u32 %v85813, 16
%v85819 = vshrl.u32 %v85813, 16
%v85820 = vor.u32 %v85819, %v85818
%v85821 = vxor.u32 %v85820, %v85816
%v85824 = vadd.s32 %v85821, %v85816
%v85828 = vadd.s32 %v85824, %v8
%v85830 = vshll.u32 %v85821, 24
%v85831 = vshrl.u32 %v85821, 8
%v85832 = vor.u32 %v85831, %v85830
%v85833 = vxor.u32 %v85832, %v85824
%v85836 = vadd.s32 %v85833, %v10
%v85840 = vadd.s32 2, %v85836
%v85844 = vadd.s32 %v85840, %v85828
%v85846 = vshll.u32 %v85840, 13
%v85847 = vshrl.u32 %v85840, 19
%v85848 = vor.u32 %v85847, %v85846
%v85849 = vxor.u32 %v85848, %v85844
%v85852 = vadd.s32 %v85849, %v85844
%v85854 = vshll.u32 %v85849, 15
%v85855 = vshrl.u32 %v85849, 17
%v85856 = vor.u32 %v85855, %v85854
%v85857 = vxor.u32 %v85856, %v85852
%v85860 = vadd.s32 %v85857, %v85852
%v85862 = vshll.u32 %v85857, 26
%v85863 = vshrl.u32 %v85857, 6
%v85864 = vor.u32 %v85863, %v85862
%v85865 = vxor.u32 %v85864, %v85860
%v85868 = vadd.s32 %v85865, %v85860
%v85872 = vadd.s32 %v85868, %v10
%v85874 = vshll.u32 %v85865, 6
%v85875 = vshrl.u32 %v85865, 26
%v85876 = vor.u32 %v85875, %v85874
%v85877 = vxor.u32 %v85876, %v85868
%v85880 = vadd.s32 %v85877, %v9
%v85884 = vadd.s32 3, %v85880
%v85888 = vadd.s32 %v85884, %v85872
%v85890 = vshll.u32 %v85884, 17
%v85891 = vshrl.u32 %v85884, 15
%v85892 = vor.u32 %v85891, %v85890
%v85893 = vxor.u32 %v85892, %v85888
%v85896 = vadd.s32 %v85893, %v85888
%v85898 = vshll.u32 %v85893, 29
%v85899 = vshrl.u32 %v85893, 3
%v85900 = vor.u32 %v85899, %v85898
%v85901 = vxor.u32 %v85900, %v85896
%v85904 = vadd.s32 %v85901, %v85896
%v85906 = vshll.u32 %v85901, 16
%v85907 = vshrl.u32 %v85901, 16
%v85908 = vor.u32 %v85907, %v85906
%v85909 = vxor.u32 %v85908, %v85904
%v85912 = vadd.s32 %v85909, %v85904
%v85916 = vadd.s32 %v85912, %v9
%v85918 = vshll.u32 %v85909, 24
%v85919 = vshrl.u32 %v85909, 8
%v85920 = vor.u32 %v85919, %v85918
%v85921 = vxor.u32 %v85920, %v85912
%v85924 = vadd.s32 %v85921, %v8
%v85928 = vadd.s32 4, %v85924
%v85932 = vadd.s32 %v85928, %v85916
%v85934 = vshll.u32 %v85928, 13
%v85935 = vshrl.u32 %v85928, 19
%v85936 = vor.u32 %v85935, %v85934
%v85937 = vxor.u32 %v85936, %v85932
%v85940 = vadd.s32 %v85937, %v85932
%v85942 = vshll.u32 %v85937, 15
%v85943 = vshrl.u32 %v85937, 17
%v85944 = vor.u32 %v85943, %v85942
%v85945 = vxor.u32 %v85944, %v85940
%v85948 = vadd.s32 %v85945, %v85940
%v85950 = vshll.u32 %v85945, 26
%v85951 = vshrl.u32 %v85945, 6
%v85952 = vor.u32 %v85951, %v85950
%v85953 = vxor.u32 %v85952, %v85948
%v85956 = vadd.s32 %v85953, %v85948
%v85960 = vadd.s32 %v85956, %v8
%v85962 = vshll.u32 %v85953, 6
%v85963 = vshrl.u32 %v85953, 26
%v85964 = vor.u32 %v85963, %v85962
%v85965 = vxor.u32 %v85964, %v85956
%v85968 = vadd.s32 %v85965, %v10
%v85972 = vadd.s32 5, %v85968
%v85974 = vxor.u32 %v85972, %v85960
%v85975 = vand.u32.u8 255, %v85974
%v85976 = vand.u32 65535, %v85975
%v85977 = vshrl.u32 %v85976, 1
%v85978 = vor.u32 16256, %v85977
%v85979 = vand.u32.u16 65535, %v85978
%v120202 = vadd.low.f32.bf16 -1.0, %v85979
%v85988 = vmul.f32 2.0, %v120202
%v85992 = vadd.f32 -0.99609375, %v85988
%v85996 = vmax.f32 %v85992, -0.99609375
%v85998 = vand.u32 2147483647, %v85996
%vm86001 = vcmp.eq.f32.partialorder %v85998, 1.0
%v86006 = vmul.f32 inf, %v85996
%v86008 = vxor.u32 2147483648, %v85996
%v86011 = vmul.f32 %v86008, %v85996
%v86013 = vadd.f32 1.0, %v86011
%v86014 = vlog2.pop %v86013
%v86015 = vmul.f32 0.6931472, %v86014
%v86016 = vmul.f32 -0.5, %v86011
%v86017 = vadd.f32 1.0, %v86016
%v86018 = vmul.f32 %v86017, %v86011
%v86019 = vand.u32 2147483647, %v86011
%vm86020 = vcmp.lt.f32.partialorder %v86019, 0.0004427343
%v86021 = vsel /*vm=*/%vm86020, /*on_true_vy=*/%v86018, /*on_false_vx=*/%v86015
%v86022 = vxor.u32 2147483648, %v86021
%vm86025 = vcmp.lt.f32.partialorder %v86022, 5.0
%v86030 = vsel /*vm=*/%vm86025, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v86034 = vsel /*vm=*/%vm86025, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v86038 = vsel /*vm=*/%vm86025, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v86042 = vsel /*vm=*/%vm86025, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v86046 = vsel /*vm=*/%vm86025, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v86050 = vsel /*vm=*/%vm86025, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v86054 = vsel /*vm=*/%vm86025, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v86058 = vsel /*vm=*/%vm86025, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v86062 = vsel /*vm=*/%vm86025, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v86066 = vadd.f32 -2.5, %v86022
%v86068 = vrsqrt.pop %v86022
%v86069 = vmul.f32 %v86068, %v86022
%vm86070 = vcmp.eq.f32.partialorder %v86022, inf
%v86071 = vsel /*vm=*/%vm86070, /*on_true_vy=*/%v86022, /*on_false_vx=*/%v86069
%vm86072 = vcmp.eq.f32.partialorder %v86022, 0.0
%v86073 = vand.u32 2147483648, %v86022
%v86074 = vsel /*vm=*/%vm86072, /*on_true_vy=*/%v86073, /*on_false_vx=*/%v86071
%v86077 = vadd.f32 -3.0, %v86074
%v86081 = vsel /*vm=*/%vm86025, /*on_true_vy=*/%v86066, /*on_false_vx=*/%v86077
%v86085 = vmul.f32 %v86081, %v86062
%v86089 = vadd.f32 %v86085, %v86058
%v86093 = vmul.f32 %v86089, %v86081
%v86097 = vadd.f32 %v86093, %v86054
%v86101 = vmul.f32 %v86097, %v86081
%v86105 = vadd.f32 %v86101, %v86050
%v86109 = vmul.f32 %v86105, %v86081
%v86113 = vadd.f32 %v86109, %v86046
%v86117 = vmul.f32 %v86113, %v86081
%v86121 = vadd.f32 %v86117, %v86042
%v86125 = vmul.f32 %v86121, %v86081
%v86129 = vadd.f32 %v86125, %v86038
%v86133 = vmul.f32 %v86129, %v86081
%v86137 = vadd.f32 %v86133, %v86034
%v86141 = vmul.f32 %v86137, %v86081
%v86145 = vadd.f32 %v86141, %v86030
%v86149 = vmul.f32 %v86145, %v85996
%v86153 = vsel /*vm=*/%vm86001, /*on_true_vy=*/%v86006, /*on_false_vx=*/%v86149
%v86157 = vmul.f32 1.4140625, %v86153
%v86160 = vpack.c.bf16 %v120417, %v86157
%120203 = vst [vmem:[%s280 + $0x3d8] sm:$0xf] /*vst_source=*/%v86160
%v86198 = vadd.s32 %v86195, %v408
%v86208 = vadd.s32 %v86198, %v415
%vm86212 = vcmp.lt.u32.totalorder %v86208, %v86198
%vm86217 = vcmp.lt.u32.totalorder %v86198, %v408
%v86222 = vadd.s32 %v86178, %v380
%v86226 = vadd.s32 1, %v86222
%v86230 = vsel /*vm=*/%vm86217, /*on_true_vy=*/%v86226, /*on_false_vx=*/%v86222
%v86234 = vadd.s32 1, %v86230
%v86238 = vsel /*vm=*/%vm86212, /*on_true_vy=*/%v86234, /*on_false_vx=*/%v86230
%v86243 = vadd.s32 %v86238, %v10
%v86247 = vadd.s32 %v86208, %v9
%v86251 = vadd.s32 %v86247, %v86243
%v86253 = vshll.u32 %v86247, 13
%v86254 = vshrl.u32 %v86247, 19
%v86255 = vor.u32 %v86254, %v86253
%v86256 = vxor.u32 %v86255, %v86251
%v86259 = vadd.s32 %v86256, %v86251
%v86261 = vshll.u32 %v86256, 15
%v86262 = vshrl.u32 %v86256, 17
%v86263 = vor.u32 %v86262, %v86261
%v86264 = vxor.u32 %v86263, %v86259
%v86267 = vadd.s32 %v86264, %v86259
%v86269 = vshll.u32 %v86264, 26
%v86270 = vshrl.u32 %v86264, 6
%v86271 = vor.u32 %v86270, %v86269
%v86272 = vxor.u32 %v86271, %v86267
%v86275 = vadd.s32 %v86272, %v86267
%v86279 = vadd.s32 %v86275, %v9
%v86281 = vshll.u32 %v86272, 6
%v86282 = vshrl.u32 %v86272, 26
%v86283 = vor.u32 %v86282, %v86281
%v86284 = vxor.u32 %v86283, %v86275
%v86287 = vadd.s32 %v86284, %v8
%v86291 = vadd.s32 1, %v86287
%v86295 = vadd.s32 %v86291, %v86279
%v86297 = vshll.u32 %v86291, 17
%v86298 = vshrl.u32 %v86291, 15
%v86299 = vor.u32 %v86298, %v86297
%v86300 = vxor.u32 %v86299, %v86295
%v86303 = vadd.s32 %v86300, %v86295
%v86305 = vshll.u32 %v86300, 29
%v86306 = vshrl.u32 %v86300, 3
%v86307 = vor.u32 %v86306, %v86305
%v86308 = vxor.u32 %v86307, %v86303
%v86311 = vadd.s32 %v86308, %v86303
%v86313 = vshll.u32 %v86308, 16
%v86314 = vshrl.u32 %v86308, 16
%v86315 = vor.u32 %v86314, %v86313
%v86316 = vxor.u32 %v86315, %v86311
%v86319 = vadd.s32 %v86316, %v86311
%v86323 = vadd.s32 %v86319, %v8
%v86325 = vshll.u32 %v86316, 24
%v86326 = vshrl.u32 %v86316, 8
%v86327 = vor.u32 %v86326, %v86325
%v86328 = vxor.u32 %v86327, %v86319
%v86331 = vadd.s32 %v86328, %v10
%v86335 = vadd.s32 2, %v86331
%v86339 = vadd.s32 %v86335, %v86323
%v86341 = vshll.u32 %v86335, 13
%v86342 = vshrl.u32 %v86335, 19
%v86343 = vor.u32 %v86342, %v86341
%v86344 = vxor.u32 %v86343, %v86339
%v86347 = vadd.s32 %v86344, %v86339
%v86349 = vshll.u32 %v86344, 15
%v86350 = vshrl.u32 %v86344, 17
%v86351 = vor.u32 %v86350, %v86349
%v86352 = vxor.u32 %v86351, %v86347
%v86355 = vadd.s32 %v86352, %v86347
%v86357 = vshll.u32 %v86352, 26
%v86358 = vshrl.u32 %v86352, 6
%v86359 = vor.u32 %v86358, %v86357
%v86360 = vxor.u32 %v86359, %v86355
%v86363 = vadd.s32 %v86360, %v86355
%v86367 = vadd.s32 %v86363, %v10
%v86369 = vshll.u32 %v86360, 6
%v86370 = vshrl.u32 %v86360, 26
%v86371 = vor.u32 %v86370, %v86369
%v86372 = vxor.u32 %v86371, %v86363
%v86375 = vadd.s32 %v86372, %v9
%v86379 = vadd.s32 3, %v86375
%v86383 = vadd.s32 %v86379, %v86367
%v86385 = vshll.u32 %v86379, 17
%v86386 = vshrl.u32 %v86379, 15
%v86387 = vor.u32 %v86386, %v86385
%v86388 = vxor.u32 %v86387, %v86383
%v86391 = vadd.s32 %v86388, %v86383
%v86393 = vshll.u32 %v86388, 29
%v86394 = vshrl.u32 %v86388, 3
%v86395 = vor.u32 %v86394, %v86393
%v86396 = vxor.u32 %v86395, %v86391
%v86399 = vadd.s32 %v86396, %v86391
%v86401 = vshll.u32 %v86396, 16
%v86402 = vshrl.u32 %v86396, 16
%v86403 = vor.u32 %v86402, %v86401
%v86404 = vxor.u32 %v86403, %v86399
%v86407 = vadd.s32 %v86404, %v86399
%v86411 = vadd.s32 %v86407, %v9
%v86413 = vshll.u32 %v86404, 24
%v86414 = vshrl.u32 %v86404, 8
%v86415 = vor.u32 %v86414, %v86413
%v86416 = vxor.u32 %v86415, %v86407
%v86419 = vadd.s32 %v86416, %v8
%v86423 = vadd.s32 4, %v86419
%v86427 = vadd.s32 %v86423, %v86411
%v86429 = vshll.u32 %v86423, 13
%v86430 = vshrl.u32 %v86423, 19
%v86431 = vor.u32 %v86430, %v86429
%v86432 = vxor.u32 %v86431, %v86427
%v86435 = vadd.s32 %v86432, %v86427
%v86437 = vshll.u32 %v86432, 15
%v86438 = vshrl.u32 %v86432, 17
%v86439 = vor.u32 %v86438, %v86437
%v86440 = vxor.u32 %v86439, %v86435
%v86443 = vadd.s32 %v86440, %v86435
%v86445 = vshll.u32 %v86440, 26
%v86446 = vshrl.u32 %v86440, 6
%v86447 = vor.u32 %v86446, %v86445
%v86448 = vxor.u32 %v86447, %v86443
%v86451 = vadd.s32 %v86448, %v86443
%v86455 = vadd.s32 %v86451, %v8
%v86457 = vshll.u32 %v86448, 6
%v86458 = vshrl.u32 %v86448, 26
%v86459 = vor.u32 %v86458, %v86457
%v86460 = vxor.u32 %v86459, %v86451
%v86463 = vadd.s32 %v86460, %v10
%v86467 = vadd.s32 5, %v86463
%v86469 = vxor.u32 %v86467, %v86455
%v86470 = vand.u32.u8 255, %v86469
%v86471 = vand.u32 65535, %v86470
%v86472 = vshrl.u32 %v86471, 1
%v86473 = vor.u32 16256, %v86472
%v86474 = vand.u32.u16 65535, %v86473
%v120208 = vadd.low.f32.bf16 -1.0, %v86474
%v86483 = vmul.f32 2.0, %v120208
%v86487 = vadd.f32 -0.99609375, %v86483
%v86491 = vmax.f32 %v86487, -0.99609375
%v86493 = vand.u32 2147483647, %v86491
%vm86496 = vcmp.eq.f32.partialorder %v86493, 1.0
%v86501 = vmul.f32 inf, %v86491
%v86503 = vxor.u32 2147483648, %v86491
%v86506 = vmul.f32 %v86503, %v86491
%v86508 = vadd.f32 1.0, %v86506
%v86509 = vlog2.pop %v86508
%v86510 = vmul.f32 0.6931472, %v86509
%v86511 = vmul.f32 -0.5, %v86506
%v86512 = vadd.f32 1.0, %v86511
%v86513 = vmul.f32 %v86512, %v86506
%v86514 = vand.u32 2147483647, %v86506
%vm86515 = vcmp.lt.f32.partialorder %v86514, 0.0004427343
%v86516 = vsel /*vm=*/%vm86515, /*on_true_vy=*/%v86513, /*on_false_vx=*/%v86510
%v86517 = vxor.u32 2147483648, %v86516
%vm86520 = vcmp.lt.f32.partialorder %v86517, 5.0
%v86525 = vsel /*vm=*/%vm86520, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v86529 = vsel /*vm=*/%vm86520, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v86533 = vsel /*vm=*/%vm86520, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v86537 = vsel /*vm=*/%vm86520, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v86541 = vsel /*vm=*/%vm86520, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v86545 = vsel /*vm=*/%vm86520, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v86549 = vsel /*vm=*/%vm86520, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v86553 = vsel /*vm=*/%vm86520, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v86557 = vsel /*vm=*/%vm86520, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v86561 = vadd.f32 -2.5, %v86517
%v86563 = vrsqrt.pop %v86517
%v86564 = vmul.f32 %v86563, %v86517
%vm86565 = vcmp.eq.f32.partialorder %v86517, inf
%v86566 = vsel /*vm=*/%vm86565, /*on_true_vy=*/%v86517, /*on_false_vx=*/%v86564
%vm86567 = vcmp.eq.f32.partialorder %v86517, 0.0
%v86568 = vand.u32 2147483648, %v86517
%v86569 = vsel /*vm=*/%vm86567, /*on_true_vy=*/%v86568, /*on_false_vx=*/%v86566
%v86572 = vadd.f32 -3.0, %v86569
%v86576 = vsel /*vm=*/%vm86520, /*on_true_vy=*/%v86561, /*on_false_vx=*/%v86572
%v86580 = vmul.f32 %v86576, %v86557
%v86584 = vadd.f32 %v86580, %v86553
%v86588 = vmul.f32 %v86584, %v86576
%v86592 = vadd.f32 %v86588, %v86549
%v86596 = vmul.f32 %v86592, %v86576
%v86600 = vadd.f32 %v86596, %v86545
%v86604 = vmul.f32 %v86600, %v86576
%v86608 = vadd.f32 %v86604, %v86541
%v86612 = vmul.f32 %v86608, %v86576
%v86616 = vadd.f32 %v86612, %v86537
%v86620 = vmul.f32 %v86616, %v86576
%v86624 = vadd.f32 %v86620, %v86533
%v86628 = vmul.f32 %v86624, %v86576
%v86632 = vadd.f32 %v86628, %v86529
%v86636 = vmul.f32 %v86632, %v86576
%v86640 = vadd.f32 %v86636, %v86525
%v86644 = vmul.f32 %v86640, %v86491
%v86648 = vsel /*vm=*/%vm86496, /*on_true_vy=*/%v86501, /*on_false_vx=*/%v86644
%v86652 = vmul.f32 1.4140625, %v86648
%v86655 = vpack.c.bf16 %v120417, %v86652
%120209 = vst [vmem:[%s280 + $0x5c] sm:$0xf] /*vst_source=*/%v86655
%v86659 = vadd.s32 %v86195, %v894
%v86669 = vadd.s32 %v86659, %v415
%vm86673 = vcmp.lt.u32.totalorder %v86669, %v86659
%vm86678 = vcmp.lt.u32.totalorder %v86659, %v894
%v86683 = vadd.s32 %v86178, %v881
%v86687 = vadd.s32 1, %v86683
%v86691 = vsel /*vm=*/%vm86678, /*on_true_vy=*/%v86687, /*on_false_vx=*/%v86683
%v86695 = vadd.s32 1, %v86691
%v86699 = vsel /*vm=*/%vm86673, /*on_true_vy=*/%v86695, /*on_false_vx=*/%v86691
%v86704 = vadd.s32 %v86699, %v10
%v86708 = vadd.s32 %v86669, %v9
%v86712 = vadd.s32 %v86708, %v86704
%v86714 = vshll.u32 %v86708, 13
%v86715 = vshrl.u32 %v86708, 19
%v86716 = vor.u32 %v86715, %v86714
%v86717 = vxor.u32 %v86716, %v86712
%v86720 = vadd.s32 %v86717, %v86712
%v86722 = vshll.u32 %v86717, 15
%v86723 = vshrl.u32 %v86717, 17
%v86724 = vor.u32 %v86723, %v86722
%v86725 = vxor.u32 %v86724, %v86720
%v86728 = vadd.s32 %v86725, %v86720
%v86730 = vshll.u32 %v86725, 26
%v86731 = vshrl.u32 %v86725, 6
%v86732 = vor.u32 %v86731, %v86730
%v86733 = vxor.u32 %v86732, %v86728
%v86736 = vadd.s32 %v86733, %v86728
%v86740 = vadd.s32 %v86736, %v9
%v86742 = vshll.u32 %v86733, 6
%v86743 = vshrl.u32 %v86733, 26
%v86744 = vor.u32 %v86743, %v86742
%v86745 = vxor.u32 %v86744, %v86736
%v86748 = vadd.s32 %v86745, %v8
%v86752 = vadd.s32 1, %v86748
%v86756 = vadd.s32 %v86752, %v86740
%v86758 = vshll.u32 %v86752, 17
%v86759 = vshrl.u32 %v86752, 15
%v86760 = vor.u32 %v86759, %v86758
%v86761 = vxor.u32 %v86760, %v86756
%v86764 = vadd.s32 %v86761, %v86756
%v86766 = vshll.u32 %v86761, 29
%v86767 = vshrl.u32 %v86761, 3
%v86768 = vor.u32 %v86767, %v86766
%v86769 = vxor.u32 %v86768, %v86764
%v86772 = vadd.s32 %v86769, %v86764
%v86774 = vshll.u32 %v86769, 16
%v86775 = vshrl.u32 %v86769, 16
%v86776 = vor.u32 %v86775, %v86774
%v86777 = vxor.u32 %v86776, %v86772
%v86780 = vadd.s32 %v86777, %v86772
%v86784 = vadd.s32 %v86780, %v8
%v86786 = vshll.u32 %v86777, 24
%v86787 = vshrl.u32 %v86777, 8
%v86788 = vor.u32 %v86787, %v86786
%v86789 = vxor.u32 %v86788, %v86780
%v86792 = vadd.s32 %v86789, %v10
%v86796 = vadd.s32 2, %v86792
%v86800 = vadd.s32 %v86796, %v86784
%v86802 = vshll.u32 %v86796, 13
%v86803 = vshrl.u32 %v86796, 19
%v86804 = vor.u32 %v86803, %v86802
%v86805 = vxor.u32 %v86804, %v86800
%v86808 = vadd.s32 %v86805, %v86800
%v86810 = vshll.u32 %v86805, 15
%v86811 = vshrl.u32 %v86805, 17
%v86812 = vor.u32 %v86811, %v86810
%v86813 = vxor.u32 %v86812, %v86808
%v86816 = vadd.s32 %v86813, %v86808
%v86818 = vshll.u32 %v86813, 26
%v86819 = vshrl.u32 %v86813, 6
%v86820 = vor.u32 %v86819, %v86818
%v86821 = vxor.u32 %v86820, %v86816
%v86824 = vadd.s32 %v86821, %v86816
%v86828 = vadd.s32 %v86824, %v10
%v86830 = vshll.u32 %v86821, 6
%v86831 = vshrl.u32 %v86821, 26
%v86832 = vor.u32 %v86831, %v86830
%v86833 = vxor.u32 %v86832, %v86824
%v86836 = vadd.s32 %v86833, %v9
%v86840 = vadd.s32 3, %v86836
%v86844 = vadd.s32 %v86840, %v86828
%v86846 = vshll.u32 %v86840, 17
%v86847 = vshrl.u32 %v86840, 15
%v86848 = vor.u32 %v86847, %v86846
%v86849 = vxor.u32 %v86848, %v86844
%v86852 = vadd.s32 %v86849, %v86844
%v86854 = vshll.u32 %v86849, 29
%v86855 = vshrl.u32 %v86849, 3
%v86856 = vor.u32 %v86855, %v86854
%v86857 = vxor.u32 %v86856, %v86852
%v86860 = vadd.s32 %v86857, %v86852
%v86862 = vshll.u32 %v86857, 16
%v86863 = vshrl.u32 %v86857, 16
%v86864 = vor.u32 %v86863, %v86862
%v86865 = vxor.u32 %v86864, %v86860
%v86868 = vadd.s32 %v86865, %v86860
%v86872 = vadd.s32 %v86868, %v9
%v86874 = vshll.u32 %v86865, 24
%v86875 = vshrl.u32 %v86865, 8
%v86876 = vor.u32 %v86875, %v86874
%v86877 = vxor.u32 %v86876, %v86868
%v86880 = vadd.s32 %v86877, %v8
%v86884 = vadd.s32 4, %v86880
%v86888 = vadd.s32 %v86884, %v86872
%v86890 = vshll.u32 %v86884, 13
%v86891 = vshrl.u32 %v86884, 19
%v86892 = vor.u32 %v86891, %v86890
%v86893 = vxor.u32 %v86892, %v86888
%v86896 = vadd.s32 %v86893, %v86888
%v86898 = vshll.u32 %v86893, 15
%v86899 = vshrl.u32 %v86893, 17
%v86900 = vor.u32 %v86899, %v86898
%v86901 = vxor.u32 %v86900, %v86896
%v86904 = vadd.s32 %v86901, %v86896
%v86906 = vshll.u32 %v86901, 26
%v86907 = vshrl.u32 %v86901, 6
%v86908 = vor.u32 %v86907, %v86906
%v86909 = vxor.u32 %v86908, %v86904
%v86912 = vadd.s32 %v86909, %v86904
%v86916 = vadd.s32 %v86912, %v8
%v86918 = vshll.u32 %v86909, 6
%v86919 = vshrl.u32 %v86909, 26
%v86920 = vor.u32 %v86919, %v86918
%v86921 = vxor.u32 %v86920, %v86912
%v86924 = vadd.s32 %v86921, %v10
%v86928 = vadd.s32 5, %v86924
%v86930 = vxor.u32 %v86928, %v86916
%v86931 = vand.u32.u8 255, %v86930
%v86932 = vand.u32 65535, %v86931
%v86933 = vshrl.u32 %v86932, 1
%v86934 = vor.u32 16256, %v86933
%v86935 = vand.u32.u16 65535, %v86934
%v120210 = vadd.low.f32.bf16 -1.0, %v86935
%v86944 = vmul.f32 2.0, %v120210
%v86948 = vadd.f32 -0.99609375, %v86944
%v86952 = vmax.f32 %v86948, -0.99609375
%v86954 = vand.u32 2147483647, %v86952
%vm86957 = vcmp.eq.f32.partialorder %v86954, 1.0
%v86962 = vmul.f32 inf, %v86952
%v86964 = vxor.u32 2147483648, %v86952
%v86967 = vmul.f32 %v86964, %v86952
%v86969 = vadd.f32 1.0, %v86967
%v86970 = vlog2.pop %v86969
%v86971 = vmul.f32 0.6931472, %v86970
%v86972 = vmul.f32 -0.5, %v86967
%v86973 = vadd.f32 1.0, %v86972
%v86974 = vmul.f32 %v86973, %v86967
%v86975 = vand.u32 2147483647, %v86967
%vm86976 = vcmp.lt.f32.partialorder %v86975, 0.0004427343
%v86977 = vsel /*vm=*/%vm86976, /*on_true_vy=*/%v86974, /*on_false_vx=*/%v86971
%v86978 = vxor.u32 2147483648, %v86977
%vm86981 = vcmp.lt.f32.partialorder %v86978, 5.0
%v86986 = vsel /*vm=*/%vm86981, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v86990 = vsel /*vm=*/%vm86981, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v86994 = vsel /*vm=*/%vm86981, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v86998 = vsel /*vm=*/%vm86981, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v87002 = vsel /*vm=*/%vm86981, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v87006 = vsel /*vm=*/%vm86981, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v87010 = vsel /*vm=*/%vm86981, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v87014 = vsel /*vm=*/%vm86981, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v87018 = vsel /*vm=*/%vm86981, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v87022 = vadd.f32 -2.5, %v86978
%v87024 = vrsqrt.pop %v86978
%v87025 = vmul.f32 %v87024, %v86978
%vm87026 = vcmp.eq.f32.partialorder %v86978, inf
%v87027 = vsel /*vm=*/%vm87026, /*on_true_vy=*/%v86978, /*on_false_vx=*/%v87025
%vm87028 = vcmp.eq.f32.partialorder %v86978, 0.0
%v87029 = vand.u32 2147483648, %v86978
%v87030 = vsel /*vm=*/%vm87028, /*on_true_vy=*/%v87029, /*on_false_vx=*/%v87027
%v87033 = vadd.f32 -3.0, %v87030
%v87037 = vsel /*vm=*/%vm86981, /*on_true_vy=*/%v87022, /*on_false_vx=*/%v87033
%v87041 = vmul.f32 %v87037, %v87018
%v87045 = vadd.f32 %v87041, %v87014
%v87049 = vmul.f32 %v87045, %v87037
%v87053 = vadd.f32 %v87049, %v87010
%v87057 = vmul.f32 %v87053, %v87037
%v87061 = vadd.f32 %v87057, %v87006
%v87065 = vmul.f32 %v87061, %v87037
%v87069 = vadd.f32 %v87065, %v87002
%v87073 = vmul.f32 %v87069, %v87037
%v87077 = vadd.f32 %v87073, %v86998
%v87081 = vmul.f32 %v87077, %v87037
%v87085 = vadd.f32 %v87081, %v86994
%v87089 = vmul.f32 %v87085, %v87037
%v87093 = vadd.f32 %v87089, %v86990
%v87097 = vmul.f32 %v87093, %v87037
%v87101 = vadd.f32 %v87097, %v86986
%v87105 = vmul.f32 %v87101, %v86952
%v87109 = vsel /*vm=*/%vm86957, /*on_true_vy=*/%v86962, /*on_false_vx=*/%v87105
%v87113 = vmul.f32 1.4140625, %v87109
%v87116 = vpack.c.bf16 %v120417, %v87113
%120211 = vst [vmem:[%s280 + $0xdc] sm:$0xf] /*vst_source=*/%v87116
%v87120 = vadd.s32 %v86195, %v1381
%v87130 = vadd.s32 %v87120, %v415
%vm87134 = vcmp.lt.u32.totalorder %v87130, %v87120
%vm87139 = vcmp.lt.u32.totalorder %v87120, %v1381
%v87144 = vadd.s32 %v86178, %v1368
%v87148 = vadd.s32 1, %v87144
%v87152 = vsel /*vm=*/%vm87139, /*on_true_vy=*/%v87148, /*on_false_vx=*/%v87144
%v87156 = vadd.s32 1, %v87152
%v87160 = vsel /*vm=*/%vm87134, /*on_true_vy=*/%v87156, /*on_false_vx=*/%v87152
%v87165 = vadd.s32 %v87160, %v10
%v87169 = vadd.s32 %v87130, %v9
%v87173 = vadd.s32 %v87169, %v87165
%v87175 = vshll.u32 %v87169, 13
%v87176 = vshrl.u32 %v87169, 19
%v87177 = vor.u32 %v87176, %v87175
%v87178 = vxor.u32 %v87177, %v87173
%v87181 = vadd.s32 %v87178, %v87173
%v87183 = vshll.u32 %v87178, 15
%v87184 = vshrl.u32 %v87178, 17
%v87185 = vor.u32 %v87184, %v87183
%v87186 = vxor.u32 %v87185, %v87181
%v87189 = vadd.s32 %v87186, %v87181
%v87191 = vshll.u32 %v87186, 26
%v87192 = vshrl.u32 %v87186, 6
%v87193 = vor.u32 %v87192, %v87191
%v87194 = vxor.u32 %v87193, %v87189
%v87197 = vadd.s32 %v87194, %v87189
%v87201 = vadd.s32 %v87197, %v9
%v87203 = vshll.u32 %v87194, 6
%v87204 = vshrl.u32 %v87194, 26
%v87205 = vor.u32 %v87204, %v87203
%v87206 = vxor.u32 %v87205, %v87197
%v87209 = vadd.s32 %v87206, %v8
%v87213 = vadd.s32 1, %v87209
%v87217 = vadd.s32 %v87213, %v87201
%v87219 = vshll.u32 %v87213, 17
%v87220 = vshrl.u32 %v87213, 15
%v87221 = vor.u32 %v87220, %v87219
%v87222 = vxor.u32 %v87221, %v87217
%v87225 = vadd.s32 %v87222, %v87217
%v87227 = vshll.u32 %v87222, 29
%v87228 = vshrl.u32 %v87222, 3
%v87229 = vor.u32 %v87228, %v87227
%v87230 = vxor.u32 %v87229, %v87225
%v87233 = vadd.s32 %v87230, %v87225
%v87235 = vshll.u32 %v87230, 16
%v87236 = vshrl.u32 %v87230, 16
%v87237 = vor.u32 %v87236, %v87235
%v87238 = vxor.u32 %v87237, %v87233
%v87241 = vadd.s32 %v87238, %v87233
%v87245 = vadd.s32 %v87241, %v8
%v87247 = vshll.u32 %v87238, 24
%v87248 = vshrl.u32 %v87238, 8
%v87249 = vor.u32 %v87248, %v87247
%v87250 = vxor.u32 %v87249, %v87241
%v87253 = vadd.s32 %v87250, %v10
%v87257 = vadd.s32 2, %v87253
%v87261 = vadd.s32 %v87257, %v87245
%v87263 = vshll.u32 %v87257, 13
%v87264 = vshrl.u32 %v87257, 19
%v87265 = vor.u32 %v87264, %v87263
%v87266 = vxor.u32 %v87265, %v87261
%v87269 = vadd.s32 %v87266, %v87261
%v87271 = vshll.u32 %v87266, 15
%v87272 = vshrl.u32 %v87266, 17
%v87273 = vor.u32 %v87272, %v87271
%v87274 = vxor.u32 %v87273, %v87269
%v87277 = vadd.s32 %v87274, %v87269
%v87279 = vshll.u32 %v87274, 26
%v87280 = vshrl.u32 %v87274, 6
%v87281 = vor.u32 %v87280, %v87279
%v87282 = vxor.u32 %v87281, %v87277
%v87285 = vadd.s32 %v87282, %v87277
%v87289 = vadd.s32 %v87285, %v10
%v87291 = vshll.u32 %v87282, 6
%v87292 = vshrl.u32 %v87282, 26
%v87293 = vor.u32 %v87292, %v87291
%v87294 = vxor.u32 %v87293, %v87285
%v87297 = vadd.s32 %v87294, %v9
%v87301 = vadd.s32 3, %v87297
%v87305 = vadd.s32 %v87301, %v87289
%v87307 = vshll.u32 %v87301, 17
%v87308 = vshrl.u32 %v87301, 15
%v87309 = vor.u32 %v87308, %v87307
%v87310 = vxor.u32 %v87309, %v87305
%v87313 = vadd.s32 %v87310, %v87305
%v87315 = vshll.u32 %v87310, 29
%v87316 = vshrl.u32 %v87310, 3
%v87317 = vor.u32 %v87316, %v87315
%v87318 = vxor.u32 %v87317, %v87313
%v87321 = vadd.s32 %v87318, %v87313
%v87323 = vshll.u32 %v87318, 16
%v87324 = vshrl.u32 %v87318, 16
%v87325 = vor.u32 %v87324, %v87323
%v87326 = vxor.u32 %v87325, %v87321
%v87329 = vadd.s32 %v87326, %v87321
%v87333 = vadd.s32 %v87329, %v9
%v87335 = vshll.u32 %v87326, 24
%v87336 = vshrl.u32 %v87326, 8
%v87337 = vor.u32 %v87336, %v87335
%v87338 = vxor.u32 %v87337, %v87329
%v87341 = vadd.s32 %v87338, %v8
%v87345 = vadd.s32 4, %v87341
%v87349 = vadd.s32 %v87345, %v87333
%v87351 = vshll.u32 %v87345, 13
%v87352 = vshrl.u32 %v87345, 19
%v87353 = vor.u32 %v87352, %v87351
%v87354 = vxor.u32 %v87353, %v87349
%v87357 = vadd.s32 %v87354, %v87349
%v87359 = vshll.u32 %v87354, 15
%v87360 = vshrl.u32 %v87354, 17
%v87361 = vor.u32 %v87360, %v87359
%v87362 = vxor.u32 %v87361, %v87357
%v87365 = vadd.s32 %v87362, %v87357
%v87367 = vshll.u32 %v87362, 26
%v87368 = vshrl.u32 %v87362, 6
%v87369 = vor.u32 %v87368, %v87367
%v87370 = vxor.u32 %v87369, %v87365
%v87373 = vadd.s32 %v87370, %v87365
%v87377 = vadd.s32 %v87373, %v8
%v87379 = vshll.u32 %v87370, 6
%v87380 = vshrl.u32 %v87370, 26
%v87381 = vor.u32 %v87380, %v87379
%v87382 = vxor.u32 %v87381, %v87373
%v87385 = vadd.s32 %v87382, %v10
%v87389 = vadd.s32 5, %v87385
%v87391 = vxor.u32 %v87389, %v87377
%v87392 = vand.u32.u8 255, %v87391
%v87393 = vand.u32 65535, %v87392
%v87394 = vshrl.u32 %v87393, 1
%v87395 = vor.u32 16256, %v87394
%v87396 = vand.u32.u16 65535, %v87395
%v120212 = vadd.low.f32.bf16 -1.0, %v87396
%v87405 = vmul.f32 2.0, %v120212
%v87409 = vadd.f32 -0.99609375, %v87405
%v87413 = vmax.f32 %v87409, -0.99609375
%v87415 = vand.u32 2147483647, %v87413
%vm87418 = vcmp.eq.f32.partialorder %v87415, 1.0
%v87423 = vmul.f32 inf, %v87413
%v87425 = vxor.u32 2147483648, %v87413
%v87428 = vmul.f32 %v87425, %v87413
%v87430 = vadd.f32 1.0, %v87428
%v87431 = vlog2.pop %v87430
%v87432 = vmul.f32 0.6931472, %v87431
%v87433 = vmul.f32 -0.5, %v87428
%v87434 = vadd.f32 1.0, %v87433
%v87435 = vmul.f32 %v87434, %v87428
%v87436 = vand.u32 2147483647, %v87428
%vm87437 = vcmp.lt.f32.partialorder %v87436, 0.0004427343
%v87438 = vsel /*vm=*/%vm87437, /*on_true_vy=*/%v87435, /*on_false_vx=*/%v87432
%v87439 = vxor.u32 2147483648, %v87438
%vm87442 = vcmp.lt.f32.partialorder %v87439, 5.0
%v87447 = vsel /*vm=*/%vm87442, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v87451 = vsel /*vm=*/%vm87442, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v87455 = vsel /*vm=*/%vm87442, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v87459 = vsel /*vm=*/%vm87442, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v87463 = vsel /*vm=*/%vm87442, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v87467 = vsel /*vm=*/%vm87442, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v87471 = vsel /*vm=*/%vm87442, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v87475 = vsel /*vm=*/%vm87442, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v87479 = vsel /*vm=*/%vm87442, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v87483 = vadd.f32 -2.5, %v87439
%v87485 = vrsqrt.pop %v87439
%v87486 = vmul.f32 %v87485, %v87439
%vm87487 = vcmp.eq.f32.partialorder %v87439, inf
%v87488 = vsel /*vm=*/%vm87487, /*on_true_vy=*/%v87439, /*on_false_vx=*/%v87486
%vm87489 = vcmp.eq.f32.partialorder %v87439, 0.0
%v87490 = vand.u32 2147483648, %v87439
%v87491 = vsel /*vm=*/%vm87489, /*on_true_vy=*/%v87490, /*on_false_vx=*/%v87488
%v87494 = vadd.f32 -3.0, %v87491
%v87498 = vsel /*vm=*/%vm87442, /*on_true_vy=*/%v87483, /*on_false_vx=*/%v87494
%v87502 = vmul.f32 %v87498, %v87479
%v87506 = vadd.f32 %v87502, %v87475
%v87510 = vmul.f32 %v87506, %v87498
%v87514 = vadd.f32 %v87510, %v87471
%v87518 = vmul.f32 %v87514, %v87498
%v87522 = vadd.f32 %v87518, %v87467
%v87526 = vmul.f32 %v87522, %v87498
%v87530 = vadd.f32 %v87526, %v87463
%v87534 = vmul.f32 %v87530, %v87498
%v87538 = vadd.f32 %v87534, %v87459
%v87542 = vmul.f32 %v87538, %v87498
%v87546 = vadd.f32 %v87542, %v87455
%v87550 = vmul.f32 %v87546, %v87498
%v87554 = vadd.f32 %v87550, %v87451
%v87558 = vmul.f32 %v87554, %v87498
%v87562 = vadd.f32 %v87558, %v87447
%v87566 = vmul.f32 %v87562, %v87413
%v87570 = vsel /*vm=*/%vm87418, /*on_true_vy=*/%v87423, /*on_false_vx=*/%v87566
%v87574 = vmul.f32 1.4140625, %v87570
%v87577 = vpack.c.bf16 %v120417, %v87574
%120213 = vst [vmem:[%s280 + $0x15c] sm:$0xf] /*vst_source=*/%v87577
%v87581 = vadd.s32 %v86195, %v1868
%v87591 = vadd.s32 %v87581, %v415
%vm87595 = vcmp.lt.u32.totalorder %v87591, %v87581
%vm87600 = vcmp.lt.u32.totalorder %v87581, %v1868
%v87605 = vadd.s32 %v86178, %v1855
%v87609 = vadd.s32 1, %v87605
%v87613 = vsel /*vm=*/%vm87600, /*on_true_vy=*/%v87609, /*on_false_vx=*/%v87605
%v87617 = vadd.s32 1, %v87613
%v87621 = vsel /*vm=*/%vm87595, /*on_true_vy=*/%v87617, /*on_false_vx=*/%v87613
%v87626 = vadd.s32 %v87621, %v10
%v87630 = vadd.s32 %v87591, %v9
%v87634 = vadd.s32 %v87630, %v87626
%v87636 = vshll.u32 %v87630, 13
%v87637 = vshrl.u32 %v87630, 19
%v87638 = vor.u32 %v87637, %v87636
%v87639 = vxor.u32 %v87638, %v87634
%v87642 = vadd.s32 %v87639, %v87634
%v87644 = vshll.u32 %v87639, 15
%v87645 = vshrl.u32 %v87639, 17
%v87646 = vor.u32 %v87645, %v87644
%v87647 = vxor.u32 %v87646, %v87642
%v87650 = vadd.s32 %v87647, %v87642
%v87652 = vshll.u32 %v87647, 26
%v87653 = vshrl.u32 %v87647, 6
%v87654 = vor.u32 %v87653, %v87652
%v87655 = vxor.u32 %v87654, %v87650
%v87658 = vadd.s32 %v87655, %v87650
%v87662 = vadd.s32 %v87658, %v9
%v87664 = vshll.u32 %v87655, 6
%v87665 = vshrl.u32 %v87655, 26
%v87666 = vor.u32 %v87665, %v87664
%v87667 = vxor.u32 %v87666, %v87658
%v87670 = vadd.s32 %v87667, %v8
%v87674 = vadd.s32 1, %v87670
%v87678 = vadd.s32 %v87674, %v87662
%v87680 = vshll.u32 %v87674, 17
%v87681 = vshrl.u32 %v87674, 15
%v87682 = vor.u32 %v87681, %v87680
%v87683 = vxor.u32 %v87682, %v87678
%v87686 = vadd.s32 %v87683, %v87678
%v87688 = vshll.u32 %v87683, 29
%v87689 = vshrl.u32 %v87683, 3
%v87690 = vor.u32 %v87689, %v87688
%v87691 = vxor.u32 %v87690, %v87686
%v87694 = vadd.s32 %v87691, %v87686
%v87696 = vshll.u32 %v87691, 16
%v87697 = vshrl.u32 %v87691, 16
%v87698 = vor.u32 %v87697, %v87696
%v87699 = vxor.u32 %v87698, %v87694
%v87702 = vadd.s32 %v87699, %v87694
%v87706 = vadd.s32 %v87702, %v8
%v87708 = vshll.u32 %v87699, 24
%v87709 = vshrl.u32 %v87699, 8
%v87710 = vor.u32 %v87709, %v87708
%v87711 = vxor.u32 %v87710, %v87702
%v87714 = vadd.s32 %v87711, %v10
%v87718 = vadd.s32 2, %v87714
%v87722 = vadd.s32 %v87718, %v87706
%v87724 = vshll.u32 %v87718, 13
%v87725 = vshrl.u32 %v87718, 19
%v87726 = vor.u32 %v87725, %v87724
%v87727 = vxor.u32 %v87726, %v87722
%v87730 = vadd.s32 %v87727, %v87722
%v87732 = vshll.u32 %v87727, 15
%v87733 = vshrl.u32 %v87727, 17
%v87734 = vor.u32 %v87733, %v87732
%v87735 = vxor.u32 %v87734, %v87730
%v87738 = vadd.s32 %v87735, %v87730
%v87740 = vshll.u32 %v87735, 26
%v87741 = vshrl.u32 %v87735, 6
%v87742 = vor.u32 %v87741, %v87740
%v87743 = vxor.u32 %v87742, %v87738
%v87746 = vadd.s32 %v87743, %v87738
%v87750 = vadd.s32 %v87746, %v10
%v87752 = vshll.u32 %v87743, 6
%v87753 = vshrl.u32 %v87743, 26
%v87754 = vor.u32 %v87753, %v87752
%v87755 = vxor.u32 %v87754, %v87746
%v87758 = vadd.s32 %v87755, %v9
%v87762 = vadd.s32 3, %v87758
%v87766 = vadd.s32 %v87762, %v87750
%v87768 = vshll.u32 %v87762, 17
%v87769 = vshrl.u32 %v87762, 15
%v87770 = vor.u32 %v87769, %v87768
%v87771 = vxor.u32 %v87770, %v87766
%v87774 = vadd.s32 %v87771, %v87766
%v87776 = vshll.u32 %v87771, 29
%v87777 = vshrl.u32 %v87771, 3
%v87778 = vor.u32 %v87777, %v87776
%v87779 = vxor.u32 %v87778, %v87774
%v87782 = vadd.s32 %v87779, %v87774
%v87784 = vshll.u32 %v87779, 16
%v87785 = vshrl.u32 %v87779, 16
%v87786 = vor.u32 %v87785, %v87784
%v87787 = vxor.u32 %v87786, %v87782
%v87790 = vadd.s32 %v87787, %v87782
%v87794 = vadd.s32 %v87790, %v9
%v87796 = vshll.u32 %v87787, 24
%v87797 = vshrl.u32 %v87787, 8
%v87798 = vor.u32 %v87797, %v87796
%v87799 = vxor.u32 %v87798, %v87790
%v87802 = vadd.s32 %v87799, %v8
%v87806 = vadd.s32 4, %v87802
%v87810 = vadd.s32 %v87806, %v87794
%v87812 = vshll.u32 %v87806, 13
%v87813 = vshrl.u32 %v87806, 19
%v87814 = vor.u32 %v87813, %v87812
%v87815 = vxor.u32 %v87814, %v87810
%v87818 = vadd.s32 %v87815, %v87810
%v87820 = vshll.u32 %v87815, 15
%v87821 = vshrl.u32 %v87815, 17
%v87822 = vor.u32 %v87821, %v87820
%v87823 = vxor.u32 %v87822, %v87818
%v87826 = vadd.s32 %v87823, %v87818
%v87828 = vshll.u32 %v87823, 26
%v87829 = vshrl.u32 %v87823, 6
%v87830 = vor.u32 %v87829, %v87828
%v87831 = vxor.u32 %v87830, %v87826
%v87834 = vadd.s32 %v87831, %v87826
%v87838 = vadd.s32 %v87834, %v8
%v87840 = vshll.u32 %v87831, 6
%v87841 = vshrl.u32 %v87831, 26
%v87842 = vor.u32 %v87841, %v87840
%v87843 = vxor.u32 %v87842, %v87834
%v87846 = vadd.s32 %v87843, %v10
%v87850 = vadd.s32 5, %v87846
%v87852 = vxor.u32 %v87850, %v87838
%v87853 = vand.u32.u8 255, %v87852
%v87854 = vand.u32 65535, %v87853
%v87855 = vshrl.u32 %v87854, 1
%v87856 = vor.u32 16256, %v87855
%v87857 = vand.u32.u16 65535, %v87856
%v120214 = vadd.low.f32.bf16 -1.0, %v87857
%v87866 = vmul.f32 2.0, %v120214
%v87870 = vadd.f32 -0.99609375, %v87866
%v87874 = vmax.f32 %v87870, -0.99609375
%v87876 = vand.u32 2147483647, %v87874
%vm87879 = vcmp.eq.f32.partialorder %v87876, 1.0
%v87884 = vmul.f32 inf, %v87874
%v87886 = vxor.u32 2147483648, %v87874
%v87889 = vmul.f32 %v87886, %v87874
%v87891 = vadd.f32 1.0, %v87889
%v87892 = vlog2.pop %v87891
%v87893 = vmul.f32 0.6931472, %v87892
%v87894 = vmul.f32 -0.5, %v87889
%v87895 = vadd.f32 1.0, %v87894
%v87896 = vmul.f32 %v87895, %v87889
%v87897 = vand.u32 2147483647, %v87889
%vm87898 = vcmp.lt.f32.partialorder %v87897, 0.0004427343
%v87899 = vsel /*vm=*/%vm87898, /*on_true_vy=*/%v87896, /*on_false_vx=*/%v87893
%v87900 = vxor.u32 2147483648, %v87899
%vm87903 = vcmp.lt.f32.partialorder %v87900, 5.0
%v87908 = vsel /*vm=*/%vm87903, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v87912 = vsel /*vm=*/%vm87903, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v87916 = vsel /*vm=*/%vm87903, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v87920 = vsel /*vm=*/%vm87903, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v87924 = vsel /*vm=*/%vm87903, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v87928 = vsel /*vm=*/%vm87903, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v87932 = vsel /*vm=*/%vm87903, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v87936 = vsel /*vm=*/%vm87903, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v87940 = vsel /*vm=*/%vm87903, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v87944 = vadd.f32 -2.5, %v87900
%v87946 = vrsqrt.pop %v87900
%v87947 = vmul.f32 %v87946, %v87900
%vm87948 = vcmp.eq.f32.partialorder %v87900, inf
%v87949 = vsel /*vm=*/%vm87948, /*on_true_vy=*/%v87900, /*on_false_vx=*/%v87947
%vm87950 = vcmp.eq.f32.partialorder %v87900, 0.0
%v87951 = vand.u32 2147483648, %v87900
%v87952 = vsel /*vm=*/%vm87950, /*on_true_vy=*/%v87951, /*on_false_vx=*/%v87949
%v87955 = vadd.f32 -3.0, %v87952
%v87959 = vsel /*vm=*/%vm87903, /*on_true_vy=*/%v87944, /*on_false_vx=*/%v87955
%v87963 = vmul.f32 %v87959, %v87940
%v87967 = vadd.f32 %v87963, %v87936
%v87971 = vmul.f32 %v87967, %v87959
%v87975 = vadd.f32 %v87971, %v87932
%v87979 = vmul.f32 %v87975, %v87959
%v87983 = vadd.f32 %v87979, %v87928
%v87987 = vmul.f32 %v87983, %v87959
%v87991 = vadd.f32 %v87987, %v87924
%v87995 = vmul.f32 %v87991, %v87959
%v87999 = vadd.f32 %v87995, %v87920
%v88003 = vmul.f32 %v87999, %v87959
%v88007 = vadd.f32 %v88003, %v87916
%v88011 = vmul.f32 %v88007, %v87959
%v88015 = vadd.f32 %v88011, %v87912
%v88019 = vmul.f32 %v88015, %v87959
%v88023 = vadd.f32 %v88019, %v87908
%v88027 = vmul.f32 %v88023, %v87874
%v88031 = vsel /*vm=*/%vm87879, /*on_true_vy=*/%v87884, /*on_false_vx=*/%v88027
%v88035 = vmul.f32 1.4140625, %v88031
%v88038 = vpack.c.bf16 %v120417, %v88035
%120215 = vst [vmem:[%s280 + $0x1dc] sm:$0xf] /*vst_source=*/%v88038
%v88042 = vadd.s32 %v86195, %v2355
%v88052 = vadd.s32 %v88042, %v415
%vm88056 = vcmp.lt.u32.totalorder %v88052, %v88042
%vm88061 = vcmp.lt.u32.totalorder %v88042, %v2355
%v88066 = vadd.s32 %v86178, %v2342
%v88070 = vadd.s32 1, %v88066
%v88074 = vsel /*vm=*/%vm88061, /*on_true_vy=*/%v88070, /*on_false_vx=*/%v88066
%v88078 = vadd.s32 1, %v88074
%v88082 = vsel /*vm=*/%vm88056, /*on_true_vy=*/%v88078, /*on_false_vx=*/%v88074
%v88087 = vadd.s32 %v88082, %v10
%v88091 = vadd.s32 %v88052, %v9
%v88095 = vadd.s32 %v88091, %v88087
%v88097 = vshll.u32 %v88091, 13
%v88098 = vshrl.u32 %v88091, 19
%v88099 = vor.u32 %v88098, %v88097
%v88100 = vxor.u32 %v88099, %v88095
%v88103 = vadd.s32 %v88100, %v88095
%v88105 = vshll.u32 %v88100, 15
%v88106 = vshrl.u32 %v88100, 17
%v88107 = vor.u32 %v88106, %v88105
%v88108 = vxor.u32 %v88107, %v88103
%v88111 = vadd.s32 %v88108, %v88103
%v88113 = vshll.u32 %v88108, 26
%v88114 = vshrl.u32 %v88108, 6
%v88115 = vor.u32 %v88114, %v88113
%v88116 = vxor.u32 %v88115, %v88111
%v88119 = vadd.s32 %v88116, %v88111
%v88123 = vadd.s32 %v88119, %v9
%v88125 = vshll.u32 %v88116, 6
%v88126 = vshrl.u32 %v88116, 26
%v88127 = vor.u32 %v88126, %v88125
%v88128 = vxor.u32 %v88127, %v88119
%v88131 = vadd.s32 %v88128, %v8
%v88135 = vadd.s32 1, %v88131
%v88139 = vadd.s32 %v88135, %v88123
%v88141 = vshll.u32 %v88135, 17
%v88142 = vshrl.u32 %v88135, 15
%v88143 = vor.u32 %v88142, %v88141
%v88144 = vxor.u32 %v88143, %v88139
%v88147 = vadd.s32 %v88144, %v88139
%v88149 = vshll.u32 %v88144, 29
%v88150 = vshrl.u32 %v88144, 3
%v88151 = vor.u32 %v88150, %v88149
%v88152 = vxor.u32 %v88151, %v88147
%v88155 = vadd.s32 %v88152, %v88147
%v88157 = vshll.u32 %v88152, 16
%v88158 = vshrl.u32 %v88152, 16
%v88159 = vor.u32 %v88158, %v88157
%v88160 = vxor.u32 %v88159, %v88155
%v88163 = vadd.s32 %v88160, %v88155
%v88167 = vadd.s32 %v88163, %v8
%v88169 = vshll.u32 %v88160, 24
%v88170 = vshrl.u32 %v88160, 8
%v88171 = vor.u32 %v88170, %v88169
%v88172 = vxor.u32 %v88171, %v88163
%v88175 = vadd.s32 %v88172, %v10
%v88179 = vadd.s32 2, %v88175
%v88183 = vadd.s32 %v88179, %v88167
%v88185 = vshll.u32 %v88179, 13
%v88186 = vshrl.u32 %v88179, 19
%v88187 = vor.u32 %v88186, %v88185
%v88188 = vxor.u32 %v88187, %v88183
%v88191 = vadd.s32 %v88188, %v88183
%v88193 = vshll.u32 %v88188, 15
%v88194 = vshrl.u32 %v88188, 17
%v88195 = vor.u32 %v88194, %v88193
%v88196 = vxor.u32 %v88195, %v88191
%v88199 = vadd.s32 %v88196, %v88191
%v88201 = vshll.u32 %v88196, 26
%v88202 = vshrl.u32 %v88196, 6
%v88203 = vor.u32 %v88202, %v88201
%v88204 = vxor.u32 %v88203, %v88199
%v88207 = vadd.s32 %v88204, %v88199
%v88211 = vadd.s32 %v88207, %v10
%v88213 = vshll.u32 %v88204, 6
%v88214 = vshrl.u32 %v88204, 26
%v88215 = vor.u32 %v88214, %v88213
%v88216 = vxor.u32 %v88215, %v88207
%v88219 = vadd.s32 %v88216, %v9
%v88223 = vadd.s32 3, %v88219
%v88227 = vadd.s32 %v88223, %v88211
%v88229 = vshll.u32 %v88223, 17
%v88230 = vshrl.u32 %v88223, 15
%v88231 = vor.u32 %v88230, %v88229
%v88232 = vxor.u32 %v88231, %v88227
%v88235 = vadd.s32 %v88232, %v88227
%v88237 = vshll.u32 %v88232, 29
%v88238 = vshrl.u32 %v88232, 3
%v88239 = vor.u32 %v88238, %v88237
%v88240 = vxor.u32 %v88239, %v88235
%v88243 = vadd.s32 %v88240, %v88235
%v88245 = vshll.u32 %v88240, 16
%v88246 = vshrl.u32 %v88240, 16
%v88247 = vor.u32 %v88246, %v88245
%v88248 = vxor.u32 %v88247, %v88243
%v88251 = vadd.s32 %v88248, %v88243
%v88255 = vadd.s32 %v88251, %v9
%v88257 = vshll.u32 %v88248, 24
%v88258 = vshrl.u32 %v88248, 8
%v88259 = vor.u32 %v88258, %v88257
%v88260 = vxor.u32 %v88259, %v88251
%v88263 = vadd.s32 %v88260, %v8
%v88267 = vadd.s32 4, %v88263
%v88271 = vadd.s32 %v88267, %v88255
%v88273 = vshll.u32 %v88267, 13
%v88274 = vshrl.u32 %v88267, 19
%v88275 = vor.u32 %v88274, %v88273
%v88276 = vxor.u32 %v88275, %v88271
%v88279 = vadd.s32 %v88276, %v88271
%v88281 = vshll.u32 %v88276, 15
%v88282 = vshrl.u32 %v88276, 17
%v88283 = vor.u32 %v88282, %v88281
%v88284 = vxor.u32 %v88283, %v88279
%v88287 = vadd.s32 %v88284, %v88279
%v88289 = vshll.u32 %v88284, 26
%v88290 = vshrl.u32 %v88284, 6
%v88291 = vor.u32 %v88290, %v88289
%v88292 = vxor.u32 %v88291, %v88287
%v88295 = vadd.s32 %v88292, %v88287
%v88299 = vadd.s32 %v88295, %v8
%v88301 = vshll.u32 %v88292, 6
%v88302 = vshrl.u32 %v88292, 26
%v88303 = vor.u32 %v88302, %v88301
%v88304 = vxor.u32 %v88303, %v88295
%v88307 = vadd.s32 %v88304, %v10
%v88311 = vadd.s32 5, %v88307
%v88313 = vxor.u32 %v88311, %v88299
%v88314 = vand.u32.u8 255, %v88313
%v88315 = vand.u32 65535, %v88314
%v88316 = vshrl.u32 %v88315, 1
%v88317 = vor.u32 16256, %v88316
%v88318 = vand.u32.u16 65535, %v88317
%v120216 = vadd.low.f32.bf16 -1.0, %v88318
%v88327 = vmul.f32 2.0, %v120216
%v88331 = vadd.f32 -0.99609375, %v88327
%v88335 = vmax.f32 %v88331, -0.99609375
%v88337 = vand.u32 2147483647, %v88335
%vm88340 = vcmp.eq.f32.partialorder %v88337, 1.0
%v88345 = vmul.f32 inf, %v88335
%v88347 = vxor.u32 2147483648, %v88335
%v88350 = vmul.f32 %v88347, %v88335
%v88352 = vadd.f32 1.0, %v88350
%v88353 = vlog2.pop %v88352
%v88354 = vmul.f32 0.6931472, %v88353
%v88355 = vmul.f32 -0.5, %v88350
%v88356 = vadd.f32 1.0, %v88355
%v88357 = vmul.f32 %v88356, %v88350
%v88358 = vand.u32 2147483647, %v88350
%vm88359 = vcmp.lt.f32.partialorder %v88358, 0.0004427343
%v88360 = vsel /*vm=*/%vm88359, /*on_true_vy=*/%v88357, /*on_false_vx=*/%v88354
%v88361 = vxor.u32 2147483648, %v88360
%vm88364 = vcmp.lt.f32.partialorder %v88361, 5.0
%v88369 = vsel /*vm=*/%vm88364, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v88373 = vsel /*vm=*/%vm88364, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v88377 = vsel /*vm=*/%vm88364, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v88381 = vsel /*vm=*/%vm88364, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v88385 = vsel /*vm=*/%vm88364, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v88389 = vsel /*vm=*/%vm88364, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v88393 = vsel /*vm=*/%vm88364, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v88397 = vsel /*vm=*/%vm88364, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v88401 = vsel /*vm=*/%vm88364, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v88405 = vadd.f32 -2.5, %v88361
%v88407 = vrsqrt.pop %v88361
%v88408 = vmul.f32 %v88407, %v88361
%vm88409 = vcmp.eq.f32.partialorder %v88361, inf
%v88410 = vsel /*vm=*/%vm88409, /*on_true_vy=*/%v88361, /*on_false_vx=*/%v88408
%vm88411 = vcmp.eq.f32.partialorder %v88361, 0.0
%v88412 = vand.u32 2147483648, %v88361
%v88413 = vsel /*vm=*/%vm88411, /*on_true_vy=*/%v88412, /*on_false_vx=*/%v88410
%v88416 = vadd.f32 -3.0, %v88413
%v88420 = vsel /*vm=*/%vm88364, /*on_true_vy=*/%v88405, /*on_false_vx=*/%v88416
%v88424 = vmul.f32 %v88420, %v88401
%v88428 = vadd.f32 %v88424, %v88397
%v88432 = vmul.f32 %v88428, %v88420
%v88436 = vadd.f32 %v88432, %v88393
%v88440 = vmul.f32 %v88436, %v88420
%v88444 = vadd.f32 %v88440, %v88389
%v88448 = vmul.f32 %v88444, %v88420
%v88452 = vadd.f32 %v88448, %v88385
%v88456 = vmul.f32 %v88452, %v88420
%v88460 = vadd.f32 %v88456, %v88381
%v88464 = vmul.f32 %v88460, %v88420
%v88468 = vadd.f32 %v88464, %v88377
%v88472 = vmul.f32 %v88468, %v88420
%v88476 = vadd.f32 %v88472, %v88373
%v88480 = vmul.f32 %v88476, %v88420
%v88484 = vadd.f32 %v88480, %v88369
%v88488 = vmul.f32 %v88484, %v88335
%v88492 = vsel /*vm=*/%vm88340, /*on_true_vy=*/%v88345, /*on_false_vx=*/%v88488
%v88496 = vmul.f32 1.4140625, %v88492
%v88499 = vpack.c.bf16 %v120417, %v88496
%120217 = vst [vmem:[%s280 + $0x25c] sm:$0xf] /*vst_source=*/%v88499
%v88503 = vadd.s32 %v86195, %v2842
%v88513 = vadd.s32 %v88503, %v415
%vm88517 = vcmp.lt.u32.totalorder %v88513, %v88503
%vm88522 = vcmp.lt.u32.totalorder %v88503, %v2842
%v88527 = vadd.s32 %v86178, %v2829
%v88531 = vadd.s32 1, %v88527
%v88535 = vsel /*vm=*/%vm88522, /*on_true_vy=*/%v88531, /*on_false_vx=*/%v88527
%v88539 = vadd.s32 1, %v88535
%v88543 = vsel /*vm=*/%vm88517, /*on_true_vy=*/%v88539, /*on_false_vx=*/%v88535
%v88548 = vadd.s32 %v88543, %v10
%v88552 = vadd.s32 %v88513, %v9
%v88556 = vadd.s32 %v88552, %v88548
%v88558 = vshll.u32 %v88552, 13
%v88559 = vshrl.u32 %v88552, 19
%v88560 = vor.u32 %v88559, %v88558
%v88561 = vxor.u32 %v88560, %v88556
%v88564 = vadd.s32 %v88561, %v88556
%v88566 = vshll.u32 %v88561, 15
%v88567 = vshrl.u32 %v88561, 17
%v88568 = vor.u32 %v88567, %v88566
%v88569 = vxor.u32 %v88568, %v88564
%v88572 = vadd.s32 %v88569, %v88564
%v88574 = vshll.u32 %v88569, 26
%v88575 = vshrl.u32 %v88569, 6
%v88576 = vor.u32 %v88575, %v88574
%v88577 = vxor.u32 %v88576, %v88572
%v88580 = vadd.s32 %v88577, %v88572
%v88584 = vadd.s32 %v88580, %v9
%v88586 = vshll.u32 %v88577, 6
%v88587 = vshrl.u32 %v88577, 26
%v88588 = vor.u32 %v88587, %v88586
%v88589 = vxor.u32 %v88588, %v88580
%v88592 = vadd.s32 %v88589, %v8
%v88596 = vadd.s32 1, %v88592
%v88600 = vadd.s32 %v88596, %v88584
%v88602 = vshll.u32 %v88596, 17
%v88603 = vshrl.u32 %v88596, 15
%v88604 = vor.u32 %v88603, %v88602
%v88605 = vxor.u32 %v88604, %v88600
%v88608 = vadd.s32 %v88605, %v88600
%v88610 = vshll.u32 %v88605, 29
%v88611 = vshrl.u32 %v88605, 3
%v88612 = vor.u32 %v88611, %v88610
%v88613 = vxor.u32 %v88612, %v88608
%v88616 = vadd.s32 %v88613, %v88608
%v88618 = vshll.u32 %v88613, 16
%v88619 = vshrl.u32 %v88613, 16
%v88620 = vor.u32 %v88619, %v88618
%v88621 = vxor.u32 %v88620, %v88616
%v88624 = vadd.s32 %v88621, %v88616
%v88628 = vadd.s32 %v88624, %v8
%v88630 = vshll.u32 %v88621, 24
%v88631 = vshrl.u32 %v88621, 8
%v88632 = vor.u32 %v88631, %v88630
%v88633 = vxor.u32 %v88632, %v88624
%v88636 = vadd.s32 %v88633, %v10
%v88640 = vadd.s32 2, %v88636
%v88644 = vadd.s32 %v88640, %v88628
%v88646 = vshll.u32 %v88640, 13
%v88647 = vshrl.u32 %v88640, 19
%v88648 = vor.u32 %v88647, %v88646
%v88649 = vxor.u32 %v88648, %v88644
%v88652 = vadd.s32 %v88649, %v88644
%v88654 = vshll.u32 %v88649, 15
%v88655 = vshrl.u32 %v88649, 17
%v88656 = vor.u32 %v88655, %v88654
%v88657 = vxor.u32 %v88656, %v88652
%v88660 = vadd.s32 %v88657, %v88652
%v88662 = vshll.u32 %v88657, 26
%v88663 = vshrl.u32 %v88657, 6
%v88664 = vor.u32 %v88663, %v88662
%v88665 = vxor.u32 %v88664, %v88660
%v88668 = vadd.s32 %v88665, %v88660
%v88672 = vadd.s32 %v88668, %v10
%v88674 = vshll.u32 %v88665, 6
%v88675 = vshrl.u32 %v88665, 26
%v88676 = vor.u32 %v88675, %v88674
%v88677 = vxor.u32 %v88676, %v88668
%v88680 = vadd.s32 %v88677, %v9
%v88684 = vadd.s32 3, %v88680
%v88688 = vadd.s32 %v88684, %v88672
%v88690 = vshll.u32 %v88684, 17
%v88691 = vshrl.u32 %v88684, 15
%v88692 = vor.u32 %v88691, %v88690
%v88693 = vxor.u32 %v88692, %v88688
%v88696 = vadd.s32 %v88693, %v88688
%v88698 = vshll.u32 %v88693, 29
%v88699 = vshrl.u32 %v88693, 3
%v88700 = vor.u32 %v88699, %v88698
%v88701 = vxor.u32 %v88700, %v88696
%v88704 = vadd.s32 %v88701, %v88696
%v88706 = vshll.u32 %v88701, 16
%v88707 = vshrl.u32 %v88701, 16
%v88708 = vor.u32 %v88707, %v88706
%v88709 = vxor.u32 %v88708, %v88704
%v88712 = vadd.s32 %v88709, %v88704
%v88716 = vadd.s32 %v88712, %v9
%v88718 = vshll.u32 %v88709, 24
%v88719 = vshrl.u32 %v88709, 8
%v88720 = vor.u32 %v88719, %v88718
%v88721 = vxor.u32 %v88720, %v88712
%v88724 = vadd.s32 %v88721, %v8
%v88728 = vadd.s32 4, %v88724
%v88732 = vadd.s32 %v88728, %v88716
%v88734 = vshll.u32 %v88728, 13
%v88735 = vshrl.u32 %v88728, 19
%v88736 = vor.u32 %v88735, %v88734
%v88737 = vxor.u32 %v88736, %v88732
%v88740 = vadd.s32 %v88737, %v88732
%v88742 = vshll.u32 %v88737, 15
%v88743 = vshrl.u32 %v88737, 17
%v88744 = vor.u32 %v88743, %v88742
%v88745 = vxor.u32 %v88744, %v88740
%v88748 = vadd.s32 %v88745, %v88740
%v88750 = vshll.u32 %v88745, 26
%v88751 = vshrl.u32 %v88745, 6
%v88752 = vor.u32 %v88751, %v88750
%v88753 = vxor.u32 %v88752, %v88748
%v88756 = vadd.s32 %v88753, %v88748
%v88760 = vadd.s32 %v88756, %v8
%v88762 = vshll.u32 %v88753, 6
%v88763 = vshrl.u32 %v88753, 26
%v88764 = vor.u32 %v88763, %v88762
%v88765 = vxor.u32 %v88764, %v88756
%v88768 = vadd.s32 %v88765, %v10
%v88772 = vadd.s32 5, %v88768
%v88774 = vxor.u32 %v88772, %v88760
%v88775 = vand.u32.u8 255, %v88774
%v88776 = vand.u32 65535, %v88775
%v88777 = vshrl.u32 %v88776, 1
%v88778 = vor.u32 16256, %v88777
%v88779 = vand.u32.u16 65535, %v88778
%v120218 = vadd.low.f32.bf16 -1.0, %v88779
%v88788 = vmul.f32 2.0, %v120218
%v88792 = vadd.f32 -0.99609375, %v88788
%v88796 = vmax.f32 %v88792, -0.99609375
%v88798 = vand.u32 2147483647, %v88796
%vm88801 = vcmp.eq.f32.partialorder %v88798, 1.0
%v88806 = vmul.f32 inf, %v88796
%v88808 = vxor.u32 2147483648, %v88796
%v88811 = vmul.f32 %v88808, %v88796
%v88813 = vadd.f32 1.0, %v88811
%v88814 = vlog2.pop %v88813
%v88815 = vmul.f32 0.6931472, %v88814
%v88816 = vmul.f32 -0.5, %v88811
%v88817 = vadd.f32 1.0, %v88816
%v88818 = vmul.f32 %v88817, %v88811
%v88819 = vand.u32 2147483647, %v88811
%vm88820 = vcmp.lt.f32.partialorder %v88819, 0.0004427343
%v88821 = vsel /*vm=*/%vm88820, /*on_true_vy=*/%v88818, /*on_false_vx=*/%v88815
%v88822 = vxor.u32 2147483648, %v88821
%vm88825 = vcmp.lt.f32.partialorder %v88822, 5.0
%v88830 = vsel /*vm=*/%vm88825, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v88834 = vsel /*vm=*/%vm88825, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v88838 = vsel /*vm=*/%vm88825, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v88842 = vsel /*vm=*/%vm88825, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v88846 = vsel /*vm=*/%vm88825, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v88850 = vsel /*vm=*/%vm88825, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v88854 = vsel /*vm=*/%vm88825, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v88858 = vsel /*vm=*/%vm88825, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v88862 = vsel /*vm=*/%vm88825, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v88866 = vadd.f32 -2.5, %v88822
%v88868 = vrsqrt.pop %v88822
%v88869 = vmul.f32 %v88868, %v88822
%vm88870 = vcmp.eq.f32.partialorder %v88822, inf
%v88871 = vsel /*vm=*/%vm88870, /*on_true_vy=*/%v88822, /*on_false_vx=*/%v88869
%vm88872 = vcmp.eq.f32.partialorder %v88822, 0.0
%v88873 = vand.u32 2147483648, %v88822
%v88874 = vsel /*vm=*/%vm88872, /*on_true_vy=*/%v88873, /*on_false_vx=*/%v88871
%v88877 = vadd.f32 -3.0, %v88874
%v88881 = vsel /*vm=*/%vm88825, /*on_true_vy=*/%v88866, /*on_false_vx=*/%v88877
%v88885 = vmul.f32 %v88881, %v88862
%v88889 = vadd.f32 %v88885, %v88858
%v88893 = vmul.f32 %v88889, %v88881
%v88897 = vadd.f32 %v88893, %v88854
%v88901 = vmul.f32 %v88897, %v88881
%v88905 = vadd.f32 %v88901, %v88850
%v88909 = vmul.f32 %v88905, %v88881
%v88913 = vadd.f32 %v88909, %v88846
%v88917 = vmul.f32 %v88913, %v88881
%v88921 = vadd.f32 %v88917, %v88842
%v88925 = vmul.f32 %v88921, %v88881
%v88929 = vadd.f32 %v88925, %v88838
%v88933 = vmul.f32 %v88929, %v88881
%v88937 = vadd.f32 %v88933, %v88834
%v88941 = vmul.f32 %v88937, %v88881
%v88945 = vadd.f32 %v88941, %v88830
%v88949 = vmul.f32 %v88945, %v88796
%v88953 = vsel /*vm=*/%vm88801, /*on_true_vy=*/%v88806, /*on_false_vx=*/%v88949
%v88957 = vmul.f32 1.4140625, %v88953
%v88960 = vpack.c.bf16 %v120417, %v88957
%120219 = vst [vmem:[%s280 + $0x2dc] sm:$0xf] /*vst_source=*/%v88960
%v88964 = vadd.s32 %v86195, %v3329
%v88974 = vadd.s32 %v88964, %v415
%vm88978 = vcmp.lt.u32.totalorder %v88974, %v88964
%vm88983 = vcmp.lt.u32.totalorder %v88964, %v3329
%v88988 = vadd.s32 %v86178, %v3316
%v88992 = vadd.s32 1, %v88988
%v88996 = vsel /*vm=*/%vm88983, /*on_true_vy=*/%v88992, /*on_false_vx=*/%v88988
%v89000 = vadd.s32 1, %v88996
%v89004 = vsel /*vm=*/%vm88978, /*on_true_vy=*/%v89000, /*on_false_vx=*/%v88996
%v89009 = vadd.s32 %v89004, %v10
%v89013 = vadd.s32 %v88974, %v9
%v89017 = vadd.s32 %v89013, %v89009
%v89019 = vshll.u32 %v89013, 13
%v89020 = vshrl.u32 %v89013, 19
%v89021 = vor.u32 %v89020, %v89019
%v89022 = vxor.u32 %v89021, %v89017
%v89025 = vadd.s32 %v89022, %v89017
%v89027 = vshll.u32 %v89022, 15
%v89028 = vshrl.u32 %v89022, 17
%v89029 = vor.u32 %v89028, %v89027
%v89030 = vxor.u32 %v89029, %v89025
%v89033 = vadd.s32 %v89030, %v89025
%v89035 = vshll.u32 %v89030, 26
%v89036 = vshrl.u32 %v89030, 6
%v89037 = vor.u32 %v89036, %v89035
%v89038 = vxor.u32 %v89037, %v89033
%v89041 = vadd.s32 %v89038, %v89033
%v89045 = vadd.s32 %v89041, %v9
%v89047 = vshll.u32 %v89038, 6
%v89048 = vshrl.u32 %v89038, 26
%v89049 = vor.u32 %v89048, %v89047
%v89050 = vxor.u32 %v89049, %v89041
%v89053 = vadd.s32 %v89050, %v8
%v89057 = vadd.s32 1, %v89053
%v89061 = vadd.s32 %v89057, %v89045
%v89063 = vshll.u32 %v89057, 17
%v89064 = vshrl.u32 %v89057, 15
%v89065 = vor.u32 %v89064, %v89063
%v89066 = vxor.u32 %v89065, %v89061
%v89069 = vadd.s32 %v89066, %v89061
%v89071 = vshll.u32 %v89066, 29
%v89072 = vshrl.u32 %v89066, 3
%v89073 = vor.u32 %v89072, %v89071
%v89074 = vxor.u32 %v89073, %v89069
%v89077 = vadd.s32 %v89074, %v89069
%v89079 = vshll.u32 %v89074, 16
%v89080 = vshrl.u32 %v89074, 16
%v89081 = vor.u32 %v89080, %v89079
%v89082 = vxor.u32 %v89081, %v89077
%v89085 = vadd.s32 %v89082, %v89077
%v89089 = vadd.s32 %v89085, %v8
%v89091 = vshll.u32 %v89082, 24
%v89092 = vshrl.u32 %v89082, 8
%v89093 = vor.u32 %v89092, %v89091
%v89094 = vxor.u32 %v89093, %v89085
%v89097 = vadd.s32 %v89094, %v10
%v89101 = vadd.s32 2, %v89097
%v89105 = vadd.s32 %v89101, %v89089
%v89107 = vshll.u32 %v89101, 13
%v89108 = vshrl.u32 %v89101, 19
%v89109 = vor.u32 %v89108, %v89107
%v89110 = vxor.u32 %v89109, %v89105
%v89113 = vadd.s32 %v89110, %v89105
%v89115 = vshll.u32 %v89110, 15
%v89116 = vshrl.u32 %v89110, 17
%v89117 = vor.u32 %v89116, %v89115
%v89118 = vxor.u32 %v89117, %v89113
%v89121 = vadd.s32 %v89118, %v89113
%v89123 = vshll.u32 %v89118, 26
%v89124 = vshrl.u32 %v89118, 6
%v89125 = vor.u32 %v89124, %v89123
%v89126 = vxor.u32 %v89125, %v89121
%v89129 = vadd.s32 %v89126, %v89121
%v89133 = vadd.s32 %v89129, %v10
%v89135 = vshll.u32 %v89126, 6
%v89136 = vshrl.u32 %v89126, 26
%v89137 = vor.u32 %v89136, %v89135
%v89138 = vxor.u32 %v89137, %v89129
%v89141 = vadd.s32 %v89138, %v9
%v89145 = vadd.s32 3, %v89141
%v89149 = vadd.s32 %v89145, %v89133
%v89151 = vshll.u32 %v89145, 17
%v89152 = vshrl.u32 %v89145, 15
%v89153 = vor.u32 %v89152, %v89151
%v89154 = vxor.u32 %v89153, %v89149
%v89157 = vadd.s32 %v89154, %v89149
%v89159 = vshll.u32 %v89154, 29
%v89160 = vshrl.u32 %v89154, 3
%v89161 = vor.u32 %v89160, %v89159
%v89162 = vxor.u32 %v89161, %v89157
%v89165 = vadd.s32 %v89162, %v89157
%v89167 = vshll.u32 %v89162, 16
%v89168 = vshrl.u32 %v89162, 16
%v89169 = vor.u32 %v89168, %v89167
%v89170 = vxor.u32 %v89169, %v89165
%v89173 = vadd.s32 %v89170, %v89165
%v89177 = vadd.s32 %v89173, %v9
%v89179 = vshll.u32 %v89170, 24
%v89180 = vshrl.u32 %v89170, 8
%v89181 = vor.u32 %v89180, %v89179
%v89182 = vxor.u32 %v89181, %v89173
%v89185 = vadd.s32 %v89182, %v8
%v89189 = vadd.s32 4, %v89185
%v89193 = vadd.s32 %v89189, %v89177
%v89195 = vshll.u32 %v89189, 13
%v89196 = vshrl.u32 %v89189, 19
%v89197 = vor.u32 %v89196, %v89195
%v89198 = vxor.u32 %v89197, %v89193
%v89201 = vadd.s32 %v89198, %v89193
%v89203 = vshll.u32 %v89198, 15
%v89204 = vshrl.u32 %v89198, 17
%v89205 = vor.u32 %v89204, %v89203
%v89206 = vxor.u32 %v89205, %v89201
%v89209 = vadd.s32 %v89206, %v89201
%v89211 = vshll.u32 %v89206, 26
%v89212 = vshrl.u32 %v89206, 6
%v89213 = vor.u32 %v89212, %v89211
%v89214 = vxor.u32 %v89213, %v89209
%v89217 = vadd.s32 %v89214, %v89209
%v89221 = vadd.s32 %v89217, %v8
%v89223 = vshll.u32 %v89214, 6
%v89224 = vshrl.u32 %v89214, 26
%v89225 = vor.u32 %v89224, %v89223
%v89226 = vxor.u32 %v89225, %v89217
%v89229 = vadd.s32 %v89226, %v10
%v89233 = vadd.s32 5, %v89229
%v89235 = vxor.u32 %v89233, %v89221
%v89236 = vand.u32.u8 255, %v89235
%v89237 = vand.u32 65535, %v89236
%v89238 = vshrl.u32 %v89237, 1
%v89239 = vor.u32 16256, %v89238
%v89240 = vand.u32.u16 65535, %v89239
%v120220 = vadd.low.f32.bf16 -1.0, %v89240
%v89249 = vmul.f32 2.0, %v120220
%v89253 = vadd.f32 -0.99609375, %v89249
%v89257 = vmax.f32 %v89253, -0.99609375
%v89259 = vand.u32 2147483647, %v89257
%vm89262 = vcmp.eq.f32.partialorder %v89259, 1.0
%v89267 = vmul.f32 inf, %v89257
%v89269 = vxor.u32 2147483648, %v89257
%v89272 = vmul.f32 %v89269, %v89257
%v89274 = vadd.f32 1.0, %v89272
%v89275 = vlog2.pop %v89274
%v89276 = vmul.f32 0.6931472, %v89275
%v89277 = vmul.f32 -0.5, %v89272
%v89278 = vadd.f32 1.0, %v89277
%v89279 = vmul.f32 %v89278, %v89272
%v89280 = vand.u32 2147483647, %v89272
%vm89281 = vcmp.lt.f32.partialorder %v89280, 0.0004427343
%v89282 = vsel /*vm=*/%vm89281, /*on_true_vy=*/%v89279, /*on_false_vx=*/%v89276
%v89283 = vxor.u32 2147483648, %v89282
%vm89286 = vcmp.lt.f32.partialorder %v89283, 5.0
%v89291 = vsel /*vm=*/%vm89286, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v89295 = vsel /*vm=*/%vm89286, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v89299 = vsel /*vm=*/%vm89286, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v89303 = vsel /*vm=*/%vm89286, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v89307 = vsel /*vm=*/%vm89286, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v89311 = vsel /*vm=*/%vm89286, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v89315 = vsel /*vm=*/%vm89286, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v89319 = vsel /*vm=*/%vm89286, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v89323 = vsel /*vm=*/%vm89286, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v89327 = vadd.f32 -2.5, %v89283
%v89329 = vrsqrt.pop %v89283
%v89330 = vmul.f32 %v89329, %v89283
%vm89331 = vcmp.eq.f32.partialorder %v89283, inf
%v89332 = vsel /*vm=*/%vm89331, /*on_true_vy=*/%v89283, /*on_false_vx=*/%v89330
%vm89333 = vcmp.eq.f32.partialorder %v89283, 0.0
%v89334 = vand.u32 2147483648, %v89283
%v89335 = vsel /*vm=*/%vm89333, /*on_true_vy=*/%v89334, /*on_false_vx=*/%v89332
%v89338 = vadd.f32 -3.0, %v89335
%v89342 = vsel /*vm=*/%vm89286, /*on_true_vy=*/%v89327, /*on_false_vx=*/%v89338
%v89346 = vmul.f32 %v89342, %v89323
%v89350 = vadd.f32 %v89346, %v89319
%v89354 = vmul.f32 %v89350, %v89342
%v89358 = vadd.f32 %v89354, %v89315
%v89362 = vmul.f32 %v89358, %v89342
%v89366 = vadd.f32 %v89362, %v89311
%v89370 = vmul.f32 %v89366, %v89342
%v89374 = vadd.f32 %v89370, %v89307
%v89378 = vmul.f32 %v89374, %v89342
%v89382 = vadd.f32 %v89378, %v89303
%v89386 = vmul.f32 %v89382, %v89342
%v89390 = vadd.f32 %v89386, %v89299
%v89394 = vmul.f32 %v89390, %v89342
%v89398 = vadd.f32 %v89394, %v89295
%v89402 = vmul.f32 %v89398, %v89342
%v89406 = vadd.f32 %v89402, %v89291
%v89410 = vmul.f32 %v89406, %v89257
%v89414 = vsel /*vm=*/%vm89262, /*on_true_vy=*/%v89267, /*on_false_vx=*/%v89410
%v89418 = vmul.f32 1.4140625, %v89414
%v89421 = vpack.c.bf16 %v120417, %v89418
%120221 = vst [vmem:[%s280 + $0x35c] sm:$0xf] /*vst_source=*/%v89421
%v89425 = vadd.s32 %v86195, %v3816
%v89435 = vadd.s32 %v89425, %v415
%vm89439 = vcmp.lt.u32.totalorder %v89435, %v89425
%vm89444 = vcmp.lt.u32.totalorder %v89425, %v3816
%v89449 = vadd.s32 %v86178, %v3803
%v89453 = vadd.s32 1, %v89449
%v89457 = vsel /*vm=*/%vm89444, /*on_true_vy=*/%v89453, /*on_false_vx=*/%v89449
%v89461 = vadd.s32 1, %v89457
%v89465 = vsel /*vm=*/%vm89439, /*on_true_vy=*/%v89461, /*on_false_vx=*/%v89457
%v89470 = vadd.s32 %v89465, %v10
%v89474 = vadd.s32 %v89435, %v9
%v89478 = vadd.s32 %v89474, %v89470
%v89480 = vshll.u32 %v89474, 13
%v89481 = vshrl.u32 %v89474, 19
%v89482 = vor.u32 %v89481, %v89480
%v89483 = vxor.u32 %v89482, %v89478
%v89486 = vadd.s32 %v89483, %v89478
%v89488 = vshll.u32 %v89483, 15
%v89489 = vshrl.u32 %v89483, 17
%v89490 = vor.u32 %v89489, %v89488
%v89491 = vxor.u32 %v89490, %v89486
%v89494 = vadd.s32 %v89491, %v89486
%v89496 = vshll.u32 %v89491, 26
%v89497 = vshrl.u32 %v89491, 6
%v89498 = vor.u32 %v89497, %v89496
%v89499 = vxor.u32 %v89498, %v89494
%v89502 = vadd.s32 %v89499, %v89494
%v89506 = vadd.s32 %v89502, %v9
%v89508 = vshll.u32 %v89499, 6
%v89509 = vshrl.u32 %v89499, 26
%v89510 = vor.u32 %v89509, %v89508
%v89511 = vxor.u32 %v89510, %v89502
%v89514 = vadd.s32 %v89511, %v8
%v89518 = vadd.s32 1, %v89514
%v89522 = vadd.s32 %v89518, %v89506
%v89524 = vshll.u32 %v89518, 17
%v89525 = vshrl.u32 %v89518, 15
%v89526 = vor.u32 %v89525, %v89524
%v89527 = vxor.u32 %v89526, %v89522
%v89530 = vadd.s32 %v89527, %v89522
%v89532 = vshll.u32 %v89527, 29
%v89533 = vshrl.u32 %v89527, 3
%v89534 = vor.u32 %v89533, %v89532
%v89535 = vxor.u32 %v89534, %v89530
%v89538 = vadd.s32 %v89535, %v89530
%v89540 = vshll.u32 %v89535, 16
%v89541 = vshrl.u32 %v89535, 16
%v89542 = vor.u32 %v89541, %v89540
%v89543 = vxor.u32 %v89542, %v89538
%v89546 = vadd.s32 %v89543, %v89538
%v89550 = vadd.s32 %v89546, %v8
%v89552 = vshll.u32 %v89543, 24
%v89553 = vshrl.u32 %v89543, 8
%v89554 = vor.u32 %v89553, %v89552
%v89555 = vxor.u32 %v89554, %v89546
%v89558 = vadd.s32 %v89555, %v10
%v89562 = vadd.s32 2, %v89558
%v89566 = vadd.s32 %v89562, %v89550
%v89568 = vshll.u32 %v89562, 13
%v89569 = vshrl.u32 %v89562, 19
%v89570 = vor.u32 %v89569, %v89568
%v89571 = vxor.u32 %v89570, %v89566
%v89574 = vadd.s32 %v89571, %v89566
%v89576 = vshll.u32 %v89571, 15
%v89577 = vshrl.u32 %v89571, 17
%v89578 = vor.u32 %v89577, %v89576
%v89579 = vxor.u32 %v89578, %v89574
%v89582 = vadd.s32 %v89579, %v89574
%v89584 = vshll.u32 %v89579, 26
%v89585 = vshrl.u32 %v89579, 6
%v89586 = vor.u32 %v89585, %v89584
%v89587 = vxor.u32 %v89586, %v89582
%v89590 = vadd.s32 %v89587, %v89582
%v89594 = vadd.s32 %v89590, %v10
%v89596 = vshll.u32 %v89587, 6
%v89597 = vshrl.u32 %v89587, 26
%v89598 = vor.u32 %v89597, %v89596
%v89599 = vxor.u32 %v89598, %v89590
%v89602 = vadd.s32 %v89599, %v9
%v89606 = vadd.s32 3, %v89602
%v89610 = vadd.s32 %v89606, %v89594
%v89612 = vshll.u32 %v89606, 17
%v89613 = vshrl.u32 %v89606, 15
%v89614 = vor.u32 %v89613, %v89612
%v89615 = vxor.u32 %v89614, %v89610
%v89618 = vadd.s32 %v89615, %v89610
%v89620 = vshll.u32 %v89615, 29
%v89621 = vshrl.u32 %v89615, 3
%v89622 = vor.u32 %v89621, %v89620
%v89623 = vxor.u32 %v89622, %v89618
%v89626 = vadd.s32 %v89623, %v89618
%v89628 = vshll.u32 %v89623, 16
%v89629 = vshrl.u32 %v89623, 16
%v89630 = vor.u32 %v89629, %v89628
%v89631 = vxor.u32 %v89630, %v89626
%v89634 = vadd.s32 %v89631, %v89626
%v89638 = vadd.s32 %v89634, %v9
%v89640 = vshll.u32 %v89631, 24
%v89641 = vshrl.u32 %v89631, 8
%v89642 = vor.u32 %v89641, %v89640
%v89643 = vxor.u32 %v89642, %v89634
%v89646 = vadd.s32 %v89643, %v8
%v89650 = vadd.s32 4, %v89646
%v89654 = vadd.s32 %v89650, %v89638
%v89656 = vshll.u32 %v89650, 13
%v89657 = vshrl.u32 %v89650, 19
%v89658 = vor.u32 %v89657, %v89656
%v89659 = vxor.u32 %v89658, %v89654
%v89662 = vadd.s32 %v89659, %v89654
%v89664 = vshll.u32 %v89659, 15
%v89665 = vshrl.u32 %v89659, 17
%v89666 = vor.u32 %v89665, %v89664
%v89667 = vxor.u32 %v89666, %v89662
%v89670 = vadd.s32 %v89667, %v89662
%v89672 = vshll.u32 %v89667, 26
%v89673 = vshrl.u32 %v89667, 6
%v89674 = vor.u32 %v89673, %v89672
%v89675 = vxor.u32 %v89674, %v89670
%v89678 = vadd.s32 %v89675, %v89670
%v89682 = vadd.s32 %v89678, %v8
%v89684 = vshll.u32 %v89675, 6
%v89685 = vshrl.u32 %v89675, 26
%v89686 = vor.u32 %v89685, %v89684
%v89687 = vxor.u32 %v89686, %v89678
%v89690 = vadd.s32 %v89687, %v10
%v89694 = vadd.s32 5, %v89690
%v89696 = vxor.u32 %v89694, %v89682
%v89697 = vand.u32.u8 255, %v89696
%v89698 = vand.u32 65535, %v89697
%v89699 = vshrl.u32 %v89698, 1
%v89700 = vor.u32 16256, %v89699
%v89701 = vand.u32.u16 65535, %v89700
%v120222 = vadd.low.f32.bf16 -1.0, %v89701
%v89710 = vmul.f32 2.0, %v120222
%v89714 = vadd.f32 -0.99609375, %v89710
%v89718 = vmax.f32 %v89714, -0.99609375
%v89720 = vand.u32 2147483647, %v89718
%vm89723 = vcmp.eq.f32.partialorder %v89720, 1.0
%v89728 = vmul.f32 inf, %v89718
%v89730 = vxor.u32 2147483648, %v89718
%v89733 = vmul.f32 %v89730, %v89718
%v89735 = vadd.f32 1.0, %v89733
%v89736 = vlog2.pop %v89735
%v89737 = vmul.f32 0.6931472, %v89736
%v89738 = vmul.f32 -0.5, %v89733
%v89739 = vadd.f32 1.0, %v89738
%v89740 = vmul.f32 %v89739, %v89733
%v89741 = vand.u32 2147483647, %v89733
%vm89742 = vcmp.lt.f32.partialorder %v89741, 0.0004427343
%v89743 = vsel /*vm=*/%vm89742, /*on_true_vy=*/%v89740, /*on_false_vx=*/%v89737
%v89744 = vxor.u32 2147483648, %v89743
%vm89747 = vcmp.lt.f32.partialorder %v89744, 5.0
%v89752 = vsel /*vm=*/%vm89747, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v89756 = vsel /*vm=*/%vm89747, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v89760 = vsel /*vm=*/%vm89747, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v89764 = vsel /*vm=*/%vm89747, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v89768 = vsel /*vm=*/%vm89747, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v89772 = vsel /*vm=*/%vm89747, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v89776 = vsel /*vm=*/%vm89747, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v89780 = vsel /*vm=*/%vm89747, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v89784 = vsel /*vm=*/%vm89747, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v89788 = vadd.f32 -2.5, %v89744
%v89790 = vrsqrt.pop %v89744
%v89791 = vmul.f32 %v89790, %v89744
%vm89792 = vcmp.eq.f32.partialorder %v89744, inf
%v89793 = vsel /*vm=*/%vm89792, /*on_true_vy=*/%v89744, /*on_false_vx=*/%v89791
%vm89794 = vcmp.eq.f32.partialorder %v89744, 0.0
%v89795 = vand.u32 2147483648, %v89744
%v89796 = vsel /*vm=*/%vm89794, /*on_true_vy=*/%v89795, /*on_false_vx=*/%v89793
%v89799 = vadd.f32 -3.0, %v89796
%v89803 = vsel /*vm=*/%vm89747, /*on_true_vy=*/%v89788, /*on_false_vx=*/%v89799
%v89807 = vmul.f32 %v89803, %v89784
%v89811 = vadd.f32 %v89807, %v89780
%v89815 = vmul.f32 %v89811, %v89803
%v89819 = vadd.f32 %v89815, %v89776
%v89823 = vmul.f32 %v89819, %v89803
%v89827 = vadd.f32 %v89823, %v89772
%v89831 = vmul.f32 %v89827, %v89803
%v89835 = vadd.f32 %v89831, %v89768
%v89839 = vmul.f32 %v89835, %v89803
%v89843 = vadd.f32 %v89839, %v89764
%v89847 = vmul.f32 %v89843, %v89803
%v89851 = vadd.f32 %v89847, %v89760
%v89855 = vmul.f32 %v89851, %v89803
%v89859 = vadd.f32 %v89855, %v89756
%v89863 = vmul.f32 %v89859, %v89803
%v89867 = vadd.f32 %v89863, %v89752
%v89871 = vmul.f32 %v89867, %v89718
%v89875 = vsel /*vm=*/%vm89723, /*on_true_vy=*/%v89728, /*on_false_vx=*/%v89871
%v89879 = vmul.f32 1.4140625, %v89875
%v89882 = vpack.c.bf16 %v120417, %v89879
%120223 = vst [vmem:[%s280 + $0x3dc] sm:$0xf] /*vst_source=*/%v89882
%v89920 = vadd.s32 %v89917, %v408
%v89930 = vadd.s32 %v89920, %v415
%vm89934 = vcmp.lt.u32.totalorder %v89930, %v89920
%vm89939 = vcmp.lt.u32.totalorder %v89920, %v408
%v89944 = vadd.s32 %v89900, %v380
%v89948 = vadd.s32 1, %v89944
%v89952 = vsel /*vm=*/%vm89939, /*on_true_vy=*/%v89948, /*on_false_vx=*/%v89944
%v89956 = vadd.s32 1, %v89952
%v89960 = vsel /*vm=*/%vm89934, /*on_true_vy=*/%v89956, /*on_false_vx=*/%v89952
%v89965 = vadd.s32 %v89960, %v10
%v89969 = vadd.s32 %v89930, %v9
%v89973 = vadd.s32 %v89969, %v89965
%v89975 = vshll.u32 %v89969, 13
%v89976 = vshrl.u32 %v89969, 19
%v89977 = vor.u32 %v89976, %v89975
%v89978 = vxor.u32 %v89977, %v89973
%v89981 = vadd.s32 %v89978, %v89973
%v89983 = vshll.u32 %v89978, 15
%v89984 = vshrl.u32 %v89978, 17
%v89985 = vor.u32 %v89984, %v89983
%v89986 = vxor.u32 %v89985, %v89981
%v89989 = vadd.s32 %v89986, %v89981
%v89991 = vshll.u32 %v89986, 26
%v89992 = vshrl.u32 %v89986, 6
%v89993 = vor.u32 %v89992, %v89991
%v89994 = vxor.u32 %v89993, %v89989
%v89997 = vadd.s32 %v89994, %v89989
%v90001 = vadd.s32 %v89997, %v9
%v90003 = vshll.u32 %v89994, 6
%v90004 = vshrl.u32 %v89994, 26
%v90005 = vor.u32 %v90004, %v90003
%v90006 = vxor.u32 %v90005, %v89997
%v90009 = vadd.s32 %v90006, %v8
%v90013 = vadd.s32 1, %v90009
%v90017 = vadd.s32 %v90013, %v90001
%v90019 = vshll.u32 %v90013, 17
%v90020 = vshrl.u32 %v90013, 15
%v90021 = vor.u32 %v90020, %v90019
%v90022 = vxor.u32 %v90021, %v90017
%v90025 = vadd.s32 %v90022, %v90017
%v90027 = vshll.u32 %v90022, 29
%v90028 = vshrl.u32 %v90022, 3
%v90029 = vor.u32 %v90028, %v90027
%v90030 = vxor.u32 %v90029, %v90025
%v90033 = vadd.s32 %v90030, %v90025
%v90035 = vshll.u32 %v90030, 16
%v90036 = vshrl.u32 %v90030, 16
%v90037 = vor.u32 %v90036, %v90035
%v90038 = vxor.u32 %v90037, %v90033
%v90041 = vadd.s32 %v90038, %v90033
%v90045 = vadd.s32 %v90041, %v8
%v90047 = vshll.u32 %v90038, 24
%v90048 = vshrl.u32 %v90038, 8
%v90049 = vor.u32 %v90048, %v90047
%v90050 = vxor.u32 %v90049, %v90041
%v90053 = vadd.s32 %v90050, %v10
%v90057 = vadd.s32 2, %v90053
%v90061 = vadd.s32 %v90057, %v90045
%v90063 = vshll.u32 %v90057, 13
%v90064 = vshrl.u32 %v90057, 19
%v90065 = vor.u32 %v90064, %v90063
%v90066 = vxor.u32 %v90065, %v90061
%v90069 = vadd.s32 %v90066, %v90061
%v90071 = vshll.u32 %v90066, 15
%v90072 = vshrl.u32 %v90066, 17
%v90073 = vor.u32 %v90072, %v90071
%v90074 = vxor.u32 %v90073, %v90069
%v90077 = vadd.s32 %v90074, %v90069
%v90079 = vshll.u32 %v90074, 26
%v90080 = vshrl.u32 %v90074, 6
%v90081 = vor.u32 %v90080, %v90079
%v90082 = vxor.u32 %v90081, %v90077
%v90085 = vadd.s32 %v90082, %v90077
%v90089 = vadd.s32 %v90085, %v10
%v90091 = vshll.u32 %v90082, 6
%v90092 = vshrl.u32 %v90082, 26
%v90093 = vor.u32 %v90092, %v90091
%v90094 = vxor.u32 %v90093, %v90085
%v90097 = vadd.s32 %v90094, %v9
%v90101 = vadd.s32 3, %v90097
%v90105 = vadd.s32 %v90101, %v90089
%v90107 = vshll.u32 %v90101, 17
%v90108 = vshrl.u32 %v90101, 15
%v90109 = vor.u32 %v90108, %v90107
%v90110 = vxor.u32 %v90109, %v90105
%v90113 = vadd.s32 %v90110, %v90105
%v90115 = vshll.u32 %v90110, 29
%v90116 = vshrl.u32 %v90110, 3
%v90117 = vor.u32 %v90116, %v90115
%v90118 = vxor.u32 %v90117, %v90113
%v90121 = vadd.s32 %v90118, %v90113
%v90123 = vshll.u32 %v90118, 16
%v90124 = vshrl.u32 %v90118, 16
%v90125 = vor.u32 %v90124, %v90123
%v90126 = vxor.u32 %v90125, %v90121
%v90129 = vadd.s32 %v90126, %v90121
%v90133 = vadd.s32 %v90129, %v9
%v90135 = vshll.u32 %v90126, 24
%v90136 = vshrl.u32 %v90126, 8
%v90137 = vor.u32 %v90136, %v90135
%v90138 = vxor.u32 %v90137, %v90129
%v90141 = vadd.s32 %v90138, %v8
%v90145 = vadd.s32 4, %v90141
%v90149 = vadd.s32 %v90145, %v90133
%v90151 = vshll.u32 %v90145, 13
%v90152 = vshrl.u32 %v90145, 19
%v90153 = vor.u32 %v90152, %v90151
%v90154 = vxor.u32 %v90153, %v90149
%v90157 = vadd.s32 %v90154, %v90149
%v90159 = vshll.u32 %v90154, 15
%v90160 = vshrl.u32 %v90154, 17
%v90161 = vor.u32 %v90160, %v90159
%v90162 = vxor.u32 %v90161, %v90157
%v90165 = vadd.s32 %v90162, %v90157
%v90167 = vshll.u32 %v90162, 26
%v90168 = vshrl.u32 %v90162, 6
%v90169 = vor.u32 %v90168, %v90167
%v90170 = vxor.u32 %v90169, %v90165
%v90173 = vadd.s32 %v90170, %v90165
%v90177 = vadd.s32 %v90173, %v8
%v90179 = vshll.u32 %v90170, 6
%v90180 = vshrl.u32 %v90170, 26
%v90181 = vor.u32 %v90180, %v90179
%v90182 = vxor.u32 %v90181, %v90173
%v90185 = vadd.s32 %v90182, %v10
%v90189 = vadd.s32 5, %v90185
%v90191 = vxor.u32 %v90189, %v90177
%v90192 = vand.u32.u8 255, %v90191
%v90193 = vand.u32 65535, %v90192
%v90194 = vshrl.u32 %v90193, 1
%v90195 = vor.u32 16256, %v90194
%v90196 = vand.u32.u16 65535, %v90195
%v120228 = vadd.low.f32.bf16 -1.0, %v90196
%v90205 = vmul.f32 2.0, %v120228
%v90209 = vadd.f32 -0.99609375, %v90205
%v90213 = vmax.f32 %v90209, -0.99609375
%v90215 = vand.u32 2147483647, %v90213
%vm90218 = vcmp.eq.f32.partialorder %v90215, 1.0
%v90223 = vmul.f32 inf, %v90213
%v90225 = vxor.u32 2147483648, %v90213
%v90228 = vmul.f32 %v90225, %v90213
%v90230 = vadd.f32 1.0, %v90228
%v90231 = vlog2.pop %v90230
%v90232 = vmul.f32 0.6931472, %v90231
%v90233 = vmul.f32 -0.5, %v90228
%v90234 = vadd.f32 1.0, %v90233
%v90235 = vmul.f32 %v90234, %v90228
%v90236 = vand.u32 2147483647, %v90228
%vm90237 = vcmp.lt.f32.partialorder %v90236, 0.0004427343
%v90238 = vsel /*vm=*/%vm90237, /*on_true_vy=*/%v90235, /*on_false_vx=*/%v90232
%v90239 = vxor.u32 2147483648, %v90238
%vm90242 = vcmp.lt.f32.partialorder %v90239, 5.0
%v90247 = vsel /*vm=*/%vm90242, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v90251 = vsel /*vm=*/%vm90242, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v90255 = vsel /*vm=*/%vm90242, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v90259 = vsel /*vm=*/%vm90242, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v90263 = vsel /*vm=*/%vm90242, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v90267 = vsel /*vm=*/%vm90242, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v90271 = vsel /*vm=*/%vm90242, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v90275 = vsel /*vm=*/%vm90242, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v90279 = vsel /*vm=*/%vm90242, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v90283 = vadd.f32 -2.5, %v90239
%v90285 = vrsqrt.pop %v90239
%v90286 = vmul.f32 %v90285, %v90239
%vm90287 = vcmp.eq.f32.partialorder %v90239, inf
%v90288 = vsel /*vm=*/%vm90287, /*on_true_vy=*/%v90239, /*on_false_vx=*/%v90286
%vm90289 = vcmp.eq.f32.partialorder %v90239, 0.0
%v90290 = vand.u32 2147483648, %v90239
%v90291 = vsel /*vm=*/%vm90289, /*on_true_vy=*/%v90290, /*on_false_vx=*/%v90288
%v90294 = vadd.f32 -3.0, %v90291
%v90298 = vsel /*vm=*/%vm90242, /*on_true_vy=*/%v90283, /*on_false_vx=*/%v90294
%v90302 = vmul.f32 %v90298, %v90279
%v90306 = vadd.f32 %v90302, %v90275
%v90310 = vmul.f32 %v90306, %v90298
%v90314 = vadd.f32 %v90310, %v90271
%v90318 = vmul.f32 %v90314, %v90298
%v90322 = vadd.f32 %v90318, %v90267
%v90326 = vmul.f32 %v90322, %v90298
%v90330 = vadd.f32 %v90326, %v90263
%v90334 = vmul.f32 %v90330, %v90298
%v90338 = vadd.f32 %v90334, %v90259
%v90342 = vmul.f32 %v90338, %v90298
%v90346 = vadd.f32 %v90342, %v90255
%v90350 = vmul.f32 %v90346, %v90298
%v90354 = vadd.f32 %v90350, %v90251
%v90358 = vmul.f32 %v90354, %v90298
%v90362 = vadd.f32 %v90358, %v90247
%v90366 = vmul.f32 %v90362, %v90213
%v90370 = vsel /*vm=*/%vm90218, /*on_true_vy=*/%v90223, /*on_false_vx=*/%v90366
%v90374 = vmul.f32 1.4140625, %v90370
%v90377 = vpack.c.bf16 %v120417, %v90374
%120229 = vst [vmem:[%s280 + $0x60] sm:$0xf] /*vst_source=*/%v90377
%v90381 = vadd.s32 %v89917, %v894
%v90391 = vadd.s32 %v90381, %v415
%vm90395 = vcmp.lt.u32.totalorder %v90391, %v90381
%vm90400 = vcmp.lt.u32.totalorder %v90381, %v894
%v90405 = vadd.s32 %v89900, %v881
%v90409 = vadd.s32 1, %v90405
%v90413 = vsel /*vm=*/%vm90400, /*on_true_vy=*/%v90409, /*on_false_vx=*/%v90405
%v90417 = vadd.s32 1, %v90413
%v90421 = vsel /*vm=*/%vm90395, /*on_true_vy=*/%v90417, /*on_false_vx=*/%v90413
%v90426 = vadd.s32 %v90421, %v10
%v90430 = vadd.s32 %v90391, %v9
%v90434 = vadd.s32 %v90430, %v90426
%v90436 = vshll.u32 %v90430, 13
%v90437 = vshrl.u32 %v90430, 19
%v90438 = vor.u32 %v90437, %v90436
%v90439 = vxor.u32 %v90438, %v90434
%v90442 = vadd.s32 %v90439, %v90434
%v90444 = vshll.u32 %v90439, 15
%v90445 = vshrl.u32 %v90439, 17
%v90446 = vor.u32 %v90445, %v90444
%v90447 = vxor.u32 %v90446, %v90442
%v90450 = vadd.s32 %v90447, %v90442
%v90452 = vshll.u32 %v90447, 26
%v90453 = vshrl.u32 %v90447, 6
%v90454 = vor.u32 %v90453, %v90452
%v90455 = vxor.u32 %v90454, %v90450
%v90458 = vadd.s32 %v90455, %v90450
%v90462 = vadd.s32 %v90458, %v9
%v90464 = vshll.u32 %v90455, 6
%v90465 = vshrl.u32 %v90455, 26
%v90466 = vor.u32 %v90465, %v90464
%v90467 = vxor.u32 %v90466, %v90458
%v90470 = vadd.s32 %v90467, %v8
%v90474 = vadd.s32 1, %v90470
%v90478 = vadd.s32 %v90474, %v90462
%v90480 = vshll.u32 %v90474, 17
%v90481 = vshrl.u32 %v90474, 15
%v90482 = vor.u32 %v90481, %v90480
%v90483 = vxor.u32 %v90482, %v90478
%v90486 = vadd.s32 %v90483, %v90478
%v90488 = vshll.u32 %v90483, 29
%v90489 = vshrl.u32 %v90483, 3
%v90490 = vor.u32 %v90489, %v90488
%v90491 = vxor.u32 %v90490, %v90486
%v90494 = vadd.s32 %v90491, %v90486
%v90496 = vshll.u32 %v90491, 16
%v90497 = vshrl.u32 %v90491, 16
%v90498 = vor.u32 %v90497, %v90496
%v90499 = vxor.u32 %v90498, %v90494
%v90502 = vadd.s32 %v90499, %v90494
%v90506 = vadd.s32 %v90502, %v8
%v90508 = vshll.u32 %v90499, 24
%v90509 = vshrl.u32 %v90499, 8
%v90510 = vor.u32 %v90509, %v90508
%v90511 = vxor.u32 %v90510, %v90502
%v90514 = vadd.s32 %v90511, %v10
%v90518 = vadd.s32 2, %v90514
%v90522 = vadd.s32 %v90518, %v90506
%v90524 = vshll.u32 %v90518, 13
%v90525 = vshrl.u32 %v90518, 19
%v90526 = vor.u32 %v90525, %v90524
%v90527 = vxor.u32 %v90526, %v90522
%v90530 = vadd.s32 %v90527, %v90522
%v90532 = vshll.u32 %v90527, 15
%v90533 = vshrl.u32 %v90527, 17
%v90534 = vor.u32 %v90533, %v90532
%v90535 = vxor.u32 %v90534, %v90530
%v90538 = vadd.s32 %v90535, %v90530
%v90540 = vshll.u32 %v90535, 26
%v90541 = vshrl.u32 %v90535, 6
%v90542 = vor.u32 %v90541, %v90540
%v90543 = vxor.u32 %v90542, %v90538
%v90546 = vadd.s32 %v90543, %v90538
%v90550 = vadd.s32 %v90546, %v10
%v90552 = vshll.u32 %v90543, 6
%v90553 = vshrl.u32 %v90543, 26
%v90554 = vor.u32 %v90553, %v90552
%v90555 = vxor.u32 %v90554, %v90546
%v90558 = vadd.s32 %v90555, %v9
%v90562 = vadd.s32 3, %v90558
%v90566 = vadd.s32 %v90562, %v90550
%v90568 = vshll.u32 %v90562, 17
%v90569 = vshrl.u32 %v90562, 15
%v90570 = vor.u32 %v90569, %v90568
%v90571 = vxor.u32 %v90570, %v90566
%v90574 = vadd.s32 %v90571, %v90566
%v90576 = vshll.u32 %v90571, 29
%v90577 = vshrl.u32 %v90571, 3
%v90578 = vor.u32 %v90577, %v90576
%v90579 = vxor.u32 %v90578, %v90574
%v90582 = vadd.s32 %v90579, %v90574
%v90584 = vshll.u32 %v90579, 16
%v90585 = vshrl.u32 %v90579, 16
%v90586 = vor.u32 %v90585, %v90584
%v90587 = vxor.u32 %v90586, %v90582
%v90590 = vadd.s32 %v90587, %v90582
%v90594 = vadd.s32 %v90590, %v9
%v90596 = vshll.u32 %v90587, 24
%v90597 = vshrl.u32 %v90587, 8
%v90598 = vor.u32 %v90597, %v90596
%v90599 = vxor.u32 %v90598, %v90590
%v90602 = vadd.s32 %v90599, %v8
%v90606 = vadd.s32 4, %v90602
%v90610 = vadd.s32 %v90606, %v90594
%v90612 = vshll.u32 %v90606, 13
%v90613 = vshrl.u32 %v90606, 19
%v90614 = vor.u32 %v90613, %v90612
%v90615 = vxor.u32 %v90614, %v90610
%v90618 = vadd.s32 %v90615, %v90610
%v90620 = vshll.u32 %v90615, 15
%v90621 = vshrl.u32 %v90615, 17
%v90622 = vor.u32 %v90621, %v90620
%v90623 = vxor.u32 %v90622, %v90618
%v90626 = vadd.s32 %v90623, %v90618
%v90628 = vshll.u32 %v90623, 26
%v90629 = vshrl.u32 %v90623, 6
%v90630 = vor.u32 %v90629, %v90628
%v90631 = vxor.u32 %v90630, %v90626
%v90634 = vadd.s32 %v90631, %v90626
%v90638 = vadd.s32 %v90634, %v8
%v90640 = vshll.u32 %v90631, 6
%v90641 = vshrl.u32 %v90631, 26
%v90642 = vor.u32 %v90641, %v90640
%v90643 = vxor.u32 %v90642, %v90634
%v90646 = vadd.s32 %v90643, %v10
%v90650 = vadd.s32 5, %v90646
%v90652 = vxor.u32 %v90650, %v90638
%v90653 = vand.u32.u8 255, %v90652
%v90654 = vand.u32 65535, %v90653
%v90655 = vshrl.u32 %v90654, 1
%v90656 = vor.u32 16256, %v90655
%v90657 = vand.u32.u16 65535, %v90656
%v120230 = vadd.low.f32.bf16 -1.0, %v90657
%v90666 = vmul.f32 2.0, %v120230
%v90670 = vadd.f32 -0.99609375, %v90666
%v90674 = vmax.f32 %v90670, -0.99609375
%v90676 = vand.u32 2147483647, %v90674
%vm90679 = vcmp.eq.f32.partialorder %v90676, 1.0
%v90684 = vmul.f32 inf, %v90674
%v90686 = vxor.u32 2147483648, %v90674
%v90689 = vmul.f32 %v90686, %v90674
%v90691 = vadd.f32 1.0, %v90689
%v90692 = vlog2.pop %v90691
%v90693 = vmul.f32 0.6931472, %v90692
%v90694 = vmul.f32 -0.5, %v90689
%v90695 = vadd.f32 1.0, %v90694
%v90696 = vmul.f32 %v90695, %v90689
%v90697 = vand.u32 2147483647, %v90689
%vm90698 = vcmp.lt.f32.partialorder %v90697, 0.0004427343
%v90699 = vsel /*vm=*/%vm90698, /*on_true_vy=*/%v90696, /*on_false_vx=*/%v90693
%v90700 = vxor.u32 2147483648, %v90699
%vm90703 = vcmp.lt.f32.partialorder %v90700, 5.0
%v90708 = vsel /*vm=*/%vm90703, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v90712 = vsel /*vm=*/%vm90703, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v90716 = vsel /*vm=*/%vm90703, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v90720 = vsel /*vm=*/%vm90703, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v90724 = vsel /*vm=*/%vm90703, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v90728 = vsel /*vm=*/%vm90703, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v90732 = vsel /*vm=*/%vm90703, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v90736 = vsel /*vm=*/%vm90703, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v90740 = vsel /*vm=*/%vm90703, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v90744 = vadd.f32 -2.5, %v90700
%v90746 = vrsqrt.pop %v90700
%v90747 = vmul.f32 %v90746, %v90700
%vm90748 = vcmp.eq.f32.partialorder %v90700, inf
%v90749 = vsel /*vm=*/%vm90748, /*on_true_vy=*/%v90700, /*on_false_vx=*/%v90747
%vm90750 = vcmp.eq.f32.partialorder %v90700, 0.0
%v90751 = vand.u32 2147483648, %v90700
%v90752 = vsel /*vm=*/%vm90750, /*on_true_vy=*/%v90751, /*on_false_vx=*/%v90749
%v90755 = vadd.f32 -3.0, %v90752
%v90759 = vsel /*vm=*/%vm90703, /*on_true_vy=*/%v90744, /*on_false_vx=*/%v90755
%v90763 = vmul.f32 %v90759, %v90740
%v90767 = vadd.f32 %v90763, %v90736
%v90771 = vmul.f32 %v90767, %v90759
%v90775 = vadd.f32 %v90771, %v90732
%v90779 = vmul.f32 %v90775, %v90759
%v90783 = vadd.f32 %v90779, %v90728
%v90787 = vmul.f32 %v90783, %v90759
%v90791 = vadd.f32 %v90787, %v90724
%v90795 = vmul.f32 %v90791, %v90759
%v90799 = vadd.f32 %v90795, %v90720
%v90803 = vmul.f32 %v90799, %v90759
%v90807 = vadd.f32 %v90803, %v90716
%v90811 = vmul.f32 %v90807, %v90759
%v90815 = vadd.f32 %v90811, %v90712
%v90819 = vmul.f32 %v90815, %v90759
%v90823 = vadd.f32 %v90819, %v90708
%v90827 = vmul.f32 %v90823, %v90674
%v90831 = vsel /*vm=*/%vm90679, /*on_true_vy=*/%v90684, /*on_false_vx=*/%v90827
%v90835 = vmul.f32 1.4140625, %v90831
%v90838 = vpack.c.bf16 %v120417, %v90835
%120231 = vst [vmem:[%s280 + $0xe0] sm:$0xf] /*vst_source=*/%v90838
%v90842 = vadd.s32 %v89917, %v1381
%v90852 = vadd.s32 %v90842, %v415
%vm90856 = vcmp.lt.u32.totalorder %v90852, %v90842
%vm90861 = vcmp.lt.u32.totalorder %v90842, %v1381
%v90866 = vadd.s32 %v89900, %v1368
%v90870 = vadd.s32 1, %v90866
%v90874 = vsel /*vm=*/%vm90861, /*on_true_vy=*/%v90870, /*on_false_vx=*/%v90866
%v90878 = vadd.s32 1, %v90874
%v90882 = vsel /*vm=*/%vm90856, /*on_true_vy=*/%v90878, /*on_false_vx=*/%v90874
%v90887 = vadd.s32 %v90882, %v10
%v90891 = vadd.s32 %v90852, %v9
%v90895 = vadd.s32 %v90891, %v90887
%v90897 = vshll.u32 %v90891, 13
%v90898 = vshrl.u32 %v90891, 19
%v90899 = vor.u32 %v90898, %v90897
%v90900 = vxor.u32 %v90899, %v90895
%v90903 = vadd.s32 %v90900, %v90895
%v90905 = vshll.u32 %v90900, 15
%v90906 = vshrl.u32 %v90900, 17
%v90907 = vor.u32 %v90906, %v90905
%v90908 = vxor.u32 %v90907, %v90903
%v90911 = vadd.s32 %v90908, %v90903
%v90913 = vshll.u32 %v90908, 26
%v90914 = vshrl.u32 %v90908, 6
%v90915 = vor.u32 %v90914, %v90913
%v90916 = vxor.u32 %v90915, %v90911
%v90919 = vadd.s32 %v90916, %v90911
%v90923 = vadd.s32 %v90919, %v9
%v90925 = vshll.u32 %v90916, 6
%v90926 = vshrl.u32 %v90916, 26
%v90927 = vor.u32 %v90926, %v90925
%v90928 = vxor.u32 %v90927, %v90919
%v90931 = vadd.s32 %v90928, %v8
%v90935 = vadd.s32 1, %v90931
%v90939 = vadd.s32 %v90935, %v90923
%v90941 = vshll.u32 %v90935, 17
%v90942 = vshrl.u32 %v90935, 15
%v90943 = vor.u32 %v90942, %v90941
%v90944 = vxor.u32 %v90943, %v90939
%v90947 = vadd.s32 %v90944, %v90939
%v90949 = vshll.u32 %v90944, 29
%v90950 = vshrl.u32 %v90944, 3
%v90951 = vor.u32 %v90950, %v90949
%v90952 = vxor.u32 %v90951, %v90947
%v90955 = vadd.s32 %v90952, %v90947
%v90957 = vshll.u32 %v90952, 16
%v90958 = vshrl.u32 %v90952, 16
%v90959 = vor.u32 %v90958, %v90957
%v90960 = vxor.u32 %v90959, %v90955
%v90963 = vadd.s32 %v90960, %v90955
%v90967 = vadd.s32 %v90963, %v8
%v90969 = vshll.u32 %v90960, 24
%v90970 = vshrl.u32 %v90960, 8
%v90971 = vor.u32 %v90970, %v90969
%v90972 = vxor.u32 %v90971, %v90963
%v90975 = vadd.s32 %v90972, %v10
%v90979 = vadd.s32 2, %v90975
%v90983 = vadd.s32 %v90979, %v90967
%v90985 = vshll.u32 %v90979, 13
%v90986 = vshrl.u32 %v90979, 19
%v90987 = vor.u32 %v90986, %v90985
%v90988 = vxor.u32 %v90987, %v90983
%v90991 = vadd.s32 %v90988, %v90983
%v90993 = vshll.u32 %v90988, 15
%v90994 = vshrl.u32 %v90988, 17
%v90995 = vor.u32 %v90994, %v90993
%v90996 = vxor.u32 %v90995, %v90991
%v90999 = vadd.s32 %v90996, %v90991
%v91001 = vshll.u32 %v90996, 26
%v91002 = vshrl.u32 %v90996, 6
%v91003 = vor.u32 %v91002, %v91001
%v91004 = vxor.u32 %v91003, %v90999
%v91007 = vadd.s32 %v91004, %v90999
%v91011 = vadd.s32 %v91007, %v10
%v91013 = vshll.u32 %v91004, 6
%v91014 = vshrl.u32 %v91004, 26
%v91015 = vor.u32 %v91014, %v91013
%v91016 = vxor.u32 %v91015, %v91007
%v91019 = vadd.s32 %v91016, %v9
%v91023 = vadd.s32 3, %v91019
%v91027 = vadd.s32 %v91023, %v91011
%v91029 = vshll.u32 %v91023, 17
%v91030 = vshrl.u32 %v91023, 15
%v91031 = vor.u32 %v91030, %v91029
%v91032 = vxor.u32 %v91031, %v91027
%v91035 = vadd.s32 %v91032, %v91027
%v91037 = vshll.u32 %v91032, 29
%v91038 = vshrl.u32 %v91032, 3
%v91039 = vor.u32 %v91038, %v91037
%v91040 = vxor.u32 %v91039, %v91035
%v91043 = vadd.s32 %v91040, %v91035
%v91045 = vshll.u32 %v91040, 16
%v91046 = vshrl.u32 %v91040, 16
%v91047 = vor.u32 %v91046, %v91045
%v91048 = vxor.u32 %v91047, %v91043
%v91051 = vadd.s32 %v91048, %v91043
%v91055 = vadd.s32 %v91051, %v9
%v91057 = vshll.u32 %v91048, 24
%v91058 = vshrl.u32 %v91048, 8
%v91059 = vor.u32 %v91058, %v91057
%v91060 = vxor.u32 %v91059, %v91051
%v91063 = vadd.s32 %v91060, %v8
%v91067 = vadd.s32 4, %v91063
%v91071 = vadd.s32 %v91067, %v91055
%v91073 = vshll.u32 %v91067, 13
%v91074 = vshrl.u32 %v91067, 19
%v91075 = vor.u32 %v91074, %v91073
%v91076 = vxor.u32 %v91075, %v91071
%v91079 = vadd.s32 %v91076, %v91071
%v91081 = vshll.u32 %v91076, 15
%v91082 = vshrl.u32 %v91076, 17
%v91083 = vor.u32 %v91082, %v91081
%v91084 = vxor.u32 %v91083, %v91079
%v91087 = vadd.s32 %v91084, %v91079
%v91089 = vshll.u32 %v91084, 26
%v91090 = vshrl.u32 %v91084, 6
%v91091 = vor.u32 %v91090, %v91089
%v91092 = vxor.u32 %v91091, %v91087
%v91095 = vadd.s32 %v91092, %v91087
%v91099 = vadd.s32 %v91095, %v8
%v91101 = vshll.u32 %v91092, 6
%v91102 = vshrl.u32 %v91092, 26
%v91103 = vor.u32 %v91102, %v91101
%v91104 = vxor.u32 %v91103, %v91095
%v91107 = vadd.s32 %v91104, %v10
%v91111 = vadd.s32 5, %v91107
%v91113 = vxor.u32 %v91111, %v91099
%v91114 = vand.u32.u8 255, %v91113
%v91115 = vand.u32 65535, %v91114
%v91116 = vshrl.u32 %v91115, 1
%v91117 = vor.u32 16256, %v91116
%v91118 = vand.u32.u16 65535, %v91117
%v120232 = vadd.low.f32.bf16 -1.0, %v91118
%v91127 = vmul.f32 2.0, %v120232
%v91131 = vadd.f32 -0.99609375, %v91127
%v91135 = vmax.f32 %v91131, -0.99609375
%v91137 = vand.u32 2147483647, %v91135
%vm91140 = vcmp.eq.f32.partialorder %v91137, 1.0
%v91145 = vmul.f32 inf, %v91135
%v91147 = vxor.u32 2147483648, %v91135
%v91150 = vmul.f32 %v91147, %v91135
%v91152 = vadd.f32 1.0, %v91150
%v91153 = vlog2.pop %v91152
%v91154 = vmul.f32 0.6931472, %v91153
%v91155 = vmul.f32 -0.5, %v91150
%v91156 = vadd.f32 1.0, %v91155
%v91157 = vmul.f32 %v91156, %v91150
%v91158 = vand.u32 2147483647, %v91150
%vm91159 = vcmp.lt.f32.partialorder %v91158, 0.0004427343
%v91160 = vsel /*vm=*/%vm91159, /*on_true_vy=*/%v91157, /*on_false_vx=*/%v91154
%v91161 = vxor.u32 2147483648, %v91160
%vm91164 = vcmp.lt.f32.partialorder %v91161, 5.0
%v91169 = vsel /*vm=*/%vm91164, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v91173 = vsel /*vm=*/%vm91164, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v91177 = vsel /*vm=*/%vm91164, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v91181 = vsel /*vm=*/%vm91164, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v91185 = vsel /*vm=*/%vm91164, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v91189 = vsel /*vm=*/%vm91164, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v91193 = vsel /*vm=*/%vm91164, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v91197 = vsel /*vm=*/%vm91164, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v91201 = vsel /*vm=*/%vm91164, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v91205 = vadd.f32 -2.5, %v91161
%v91207 = vrsqrt.pop %v91161
%v91208 = vmul.f32 %v91207, %v91161
%vm91209 = vcmp.eq.f32.partialorder %v91161, inf
%v91210 = vsel /*vm=*/%vm91209, /*on_true_vy=*/%v91161, /*on_false_vx=*/%v91208
%vm91211 = vcmp.eq.f32.partialorder %v91161, 0.0
%v91212 = vand.u32 2147483648, %v91161
%v91213 = vsel /*vm=*/%vm91211, /*on_true_vy=*/%v91212, /*on_false_vx=*/%v91210
%v91216 = vadd.f32 -3.0, %v91213
%v91220 = vsel /*vm=*/%vm91164, /*on_true_vy=*/%v91205, /*on_false_vx=*/%v91216
%v91224 = vmul.f32 %v91220, %v91201
%v91228 = vadd.f32 %v91224, %v91197
%v91232 = vmul.f32 %v91228, %v91220
%v91236 = vadd.f32 %v91232, %v91193
%v91240 = vmul.f32 %v91236, %v91220
%v91244 = vadd.f32 %v91240, %v91189
%v91248 = vmul.f32 %v91244, %v91220
%v91252 = vadd.f32 %v91248, %v91185
%v91256 = vmul.f32 %v91252, %v91220
%v91260 = vadd.f32 %v91256, %v91181
%v91264 = vmul.f32 %v91260, %v91220
%v91268 = vadd.f32 %v91264, %v91177
%v91272 = vmul.f32 %v91268, %v91220
%v91276 = vadd.f32 %v91272, %v91173
%v91280 = vmul.f32 %v91276, %v91220
%v91284 = vadd.f32 %v91280, %v91169
%v91288 = vmul.f32 %v91284, %v91135
%v91292 = vsel /*vm=*/%vm91140, /*on_true_vy=*/%v91145, /*on_false_vx=*/%v91288
%v91296 = vmul.f32 1.4140625, %v91292
%v91299 = vpack.c.bf16 %v120417, %v91296
%120233 = vst [vmem:[%s280 + $0x160] sm:$0xf] /*vst_source=*/%v91299
%v91303 = vadd.s32 %v89917, %v1868
%v91313 = vadd.s32 %v91303, %v415
%vm91317 = vcmp.lt.u32.totalorder %v91313, %v91303
%vm91322 = vcmp.lt.u32.totalorder %v91303, %v1868
%v91327 = vadd.s32 %v89900, %v1855
%v91331 = vadd.s32 1, %v91327
%v91335 = vsel /*vm=*/%vm91322, /*on_true_vy=*/%v91331, /*on_false_vx=*/%v91327
%v91339 = vadd.s32 1, %v91335
%v91343 = vsel /*vm=*/%vm91317, /*on_true_vy=*/%v91339, /*on_false_vx=*/%v91335
%v91348 = vadd.s32 %v91343, %v10
%v91352 = vadd.s32 %v91313, %v9
%v91356 = vadd.s32 %v91352, %v91348
%v91358 = vshll.u32 %v91352, 13
%v91359 = vshrl.u32 %v91352, 19
%v91360 = vor.u32 %v91359, %v91358
%v91361 = vxor.u32 %v91360, %v91356
%v91364 = vadd.s32 %v91361, %v91356
%v91366 = vshll.u32 %v91361, 15
%v91367 = vshrl.u32 %v91361, 17
%v91368 = vor.u32 %v91367, %v91366
%v91369 = vxor.u32 %v91368, %v91364
%v91372 = vadd.s32 %v91369, %v91364
%v91374 = vshll.u32 %v91369, 26
%v91375 = vshrl.u32 %v91369, 6
%v91376 = vor.u32 %v91375, %v91374
%v91377 = vxor.u32 %v91376, %v91372
%v91380 = vadd.s32 %v91377, %v91372
%v91384 = vadd.s32 %v91380, %v9
%v91386 = vshll.u32 %v91377, 6
%v91387 = vshrl.u32 %v91377, 26
%v91388 = vor.u32 %v91387, %v91386
%v91389 = vxor.u32 %v91388, %v91380
%v91392 = vadd.s32 %v91389, %v8
%v91396 = vadd.s32 1, %v91392
%v91400 = vadd.s32 %v91396, %v91384
%v91402 = vshll.u32 %v91396, 17
%v91403 = vshrl.u32 %v91396, 15
%v91404 = vor.u32 %v91403, %v91402
%v91405 = vxor.u32 %v91404, %v91400
%v91408 = vadd.s32 %v91405, %v91400
%v91410 = vshll.u32 %v91405, 29
%v91411 = vshrl.u32 %v91405, 3
%v91412 = vor.u32 %v91411, %v91410
%v91413 = vxor.u32 %v91412, %v91408
%v91416 = vadd.s32 %v91413, %v91408
%v91418 = vshll.u32 %v91413, 16
%v91419 = vshrl.u32 %v91413, 16
%v91420 = vor.u32 %v91419, %v91418
%v91421 = vxor.u32 %v91420, %v91416
%v91424 = vadd.s32 %v91421, %v91416
%v91428 = vadd.s32 %v91424, %v8
%v91430 = vshll.u32 %v91421, 24
%v91431 = vshrl.u32 %v91421, 8
%v91432 = vor.u32 %v91431, %v91430
%v91433 = vxor.u32 %v91432, %v91424
%v91436 = vadd.s32 %v91433, %v10
%v91440 = vadd.s32 2, %v91436
%v91444 = vadd.s32 %v91440, %v91428
%v91446 = vshll.u32 %v91440, 13
%v91447 = vshrl.u32 %v91440, 19
%v91448 = vor.u32 %v91447, %v91446
%v91449 = vxor.u32 %v91448, %v91444
%v91452 = vadd.s32 %v91449, %v91444
%v91454 = vshll.u32 %v91449, 15
%v91455 = vshrl.u32 %v91449, 17
%v91456 = vor.u32 %v91455, %v91454
%v91457 = vxor.u32 %v91456, %v91452
%v91460 = vadd.s32 %v91457, %v91452
%v91462 = vshll.u32 %v91457, 26
%v91463 = vshrl.u32 %v91457, 6
%v91464 = vor.u32 %v91463, %v91462
%v91465 = vxor.u32 %v91464, %v91460
%v91468 = vadd.s32 %v91465, %v91460
%v91472 = vadd.s32 %v91468, %v10
%v91474 = vshll.u32 %v91465, 6
%v91475 = vshrl.u32 %v91465, 26
%v91476 = vor.u32 %v91475, %v91474
%v91477 = vxor.u32 %v91476, %v91468
%v91480 = vadd.s32 %v91477, %v9
%v91484 = vadd.s32 3, %v91480
%v91488 = vadd.s32 %v91484, %v91472
%v91490 = vshll.u32 %v91484, 17
%v91491 = vshrl.u32 %v91484, 15
%v91492 = vor.u32 %v91491, %v91490
%v91493 = vxor.u32 %v91492, %v91488
%v91496 = vadd.s32 %v91493, %v91488
%v91498 = vshll.u32 %v91493, 29
%v91499 = vshrl.u32 %v91493, 3
%v91500 = vor.u32 %v91499, %v91498
%v91501 = vxor.u32 %v91500, %v91496
%v91504 = vadd.s32 %v91501, %v91496
%v91506 = vshll.u32 %v91501, 16
%v91507 = vshrl.u32 %v91501, 16
%v91508 = vor.u32 %v91507, %v91506
%v91509 = vxor.u32 %v91508, %v91504
%v91512 = vadd.s32 %v91509, %v91504
%v91516 = vadd.s32 %v91512, %v9
%v91518 = vshll.u32 %v91509, 24
%v91519 = vshrl.u32 %v91509, 8
%v91520 = vor.u32 %v91519, %v91518
%v91521 = vxor.u32 %v91520, %v91512
%v91524 = vadd.s32 %v91521, %v8
%v91528 = vadd.s32 4, %v91524
%v91532 = vadd.s32 %v91528, %v91516
%v91534 = vshll.u32 %v91528, 13
%v91535 = vshrl.u32 %v91528, 19
%v91536 = vor.u32 %v91535, %v91534
%v91537 = vxor.u32 %v91536, %v91532
%v91540 = vadd.s32 %v91537, %v91532
%v91542 = vshll.u32 %v91537, 15
%v91543 = vshrl.u32 %v91537, 17
%v91544 = vor.u32 %v91543, %v91542
%v91545 = vxor.u32 %v91544, %v91540
%v91548 = vadd.s32 %v91545, %v91540
%v91550 = vshll.u32 %v91545, 26
%v91551 = vshrl.u32 %v91545, 6
%v91552 = vor.u32 %v91551, %v91550
%v91553 = vxor.u32 %v91552, %v91548
%v91556 = vadd.s32 %v91553, %v91548
%v91560 = vadd.s32 %v91556, %v8
%v91562 = vshll.u32 %v91553, 6
%v91563 = vshrl.u32 %v91553, 26
%v91564 = vor.u32 %v91563, %v91562
%v91565 = vxor.u32 %v91564, %v91556
%v91568 = vadd.s32 %v91565, %v10
%v91572 = vadd.s32 5, %v91568
%v91574 = vxor.u32 %v91572, %v91560
%v91575 = vand.u32.u8 255, %v91574
%v91576 = vand.u32 65535, %v91575
%v91577 = vshrl.u32 %v91576, 1
%v91578 = vor.u32 16256, %v91577
%v91579 = vand.u32.u16 65535, %v91578
%v120234 = vadd.low.f32.bf16 -1.0, %v91579
%v91588 = vmul.f32 2.0, %v120234
%v91592 = vadd.f32 -0.99609375, %v91588
%v91596 = vmax.f32 %v91592, -0.99609375
%v91598 = vand.u32 2147483647, %v91596
%vm91601 = vcmp.eq.f32.partialorder %v91598, 1.0
%v91606 = vmul.f32 inf, %v91596
%v91608 = vxor.u32 2147483648, %v91596
%v91611 = vmul.f32 %v91608, %v91596
%v91613 = vadd.f32 1.0, %v91611
%v91614 = vlog2.pop %v91613
%v91615 = vmul.f32 0.6931472, %v91614
%v91616 = vmul.f32 -0.5, %v91611
%v91617 = vadd.f32 1.0, %v91616
%v91618 = vmul.f32 %v91617, %v91611
%v91619 = vand.u32 2147483647, %v91611
%vm91620 = vcmp.lt.f32.partialorder %v91619, 0.0004427343
%v91621 = vsel /*vm=*/%vm91620, /*on_true_vy=*/%v91618, /*on_false_vx=*/%v91615
%v91622 = vxor.u32 2147483648, %v91621
%vm91625 = vcmp.lt.f32.partialorder %v91622, 5.0
%v91630 = vsel /*vm=*/%vm91625, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v91634 = vsel /*vm=*/%vm91625, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v91638 = vsel /*vm=*/%vm91625, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v91642 = vsel /*vm=*/%vm91625, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v91646 = vsel /*vm=*/%vm91625, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v91650 = vsel /*vm=*/%vm91625, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v91654 = vsel /*vm=*/%vm91625, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v91658 = vsel /*vm=*/%vm91625, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v91662 = vsel /*vm=*/%vm91625, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v91666 = vadd.f32 -2.5, %v91622
%v91668 = vrsqrt.pop %v91622
%v91669 = vmul.f32 %v91668, %v91622
%vm91670 = vcmp.eq.f32.partialorder %v91622, inf
%v91671 = vsel /*vm=*/%vm91670, /*on_true_vy=*/%v91622, /*on_false_vx=*/%v91669
%vm91672 = vcmp.eq.f32.partialorder %v91622, 0.0
%v91673 = vand.u32 2147483648, %v91622
%v91674 = vsel /*vm=*/%vm91672, /*on_true_vy=*/%v91673, /*on_false_vx=*/%v91671
%v91677 = vadd.f32 -3.0, %v91674
%v91681 = vsel /*vm=*/%vm91625, /*on_true_vy=*/%v91666, /*on_false_vx=*/%v91677
%v91685 = vmul.f32 %v91681, %v91662
%v91689 = vadd.f32 %v91685, %v91658
%v91693 = vmul.f32 %v91689, %v91681
%v91697 = vadd.f32 %v91693, %v91654
%v91701 = vmul.f32 %v91697, %v91681
%v91705 = vadd.f32 %v91701, %v91650
%v91709 = vmul.f32 %v91705, %v91681
%v91713 = vadd.f32 %v91709, %v91646
%v91717 = vmul.f32 %v91713, %v91681
%v91721 = vadd.f32 %v91717, %v91642
%v91725 = vmul.f32 %v91721, %v91681
%v91729 = vadd.f32 %v91725, %v91638
%v91733 = vmul.f32 %v91729, %v91681
%v91737 = vadd.f32 %v91733, %v91634
%v91741 = vmul.f32 %v91737, %v91681
%v91745 = vadd.f32 %v91741, %v91630
%v91749 = vmul.f32 %v91745, %v91596
%v91753 = vsel /*vm=*/%vm91601, /*on_true_vy=*/%v91606, /*on_false_vx=*/%v91749
%v91757 = vmul.f32 1.4140625, %v91753
%v91760 = vpack.c.bf16 %v120417, %v91757
%120235 = vst [vmem:[%s280 + $0x1e0] sm:$0xf] /*vst_source=*/%v91760
%v91764 = vadd.s32 %v89917, %v2355
%v91774 = vadd.s32 %v91764, %v415
%vm91778 = vcmp.lt.u32.totalorder %v91774, %v91764
%vm91783 = vcmp.lt.u32.totalorder %v91764, %v2355
%v91788 = vadd.s32 %v89900, %v2342
%v91792 = vadd.s32 1, %v91788
%v91796 = vsel /*vm=*/%vm91783, /*on_true_vy=*/%v91792, /*on_false_vx=*/%v91788
%v91800 = vadd.s32 1, %v91796
%v91804 = vsel /*vm=*/%vm91778, /*on_true_vy=*/%v91800, /*on_false_vx=*/%v91796
%v91809 = vadd.s32 %v91804, %v10
%v91813 = vadd.s32 %v91774, %v9
%v91817 = vadd.s32 %v91813, %v91809
%v91819 = vshll.u32 %v91813, 13
%v91820 = vshrl.u32 %v91813, 19
%v91821 = vor.u32 %v91820, %v91819
%v91822 = vxor.u32 %v91821, %v91817
%v91825 = vadd.s32 %v91822, %v91817
%v91827 = vshll.u32 %v91822, 15
%v91828 = vshrl.u32 %v91822, 17
%v91829 = vor.u32 %v91828, %v91827
%v91830 = vxor.u32 %v91829, %v91825
%v91833 = vadd.s32 %v91830, %v91825
%v91835 = vshll.u32 %v91830, 26
%v91836 = vshrl.u32 %v91830, 6
%v91837 = vor.u32 %v91836, %v91835
%v91838 = vxor.u32 %v91837, %v91833
%v91841 = vadd.s32 %v91838, %v91833
%v91845 = vadd.s32 %v91841, %v9
%v91847 = vshll.u32 %v91838, 6
%v91848 = vshrl.u32 %v91838, 26
%v91849 = vor.u32 %v91848, %v91847
%v91850 = vxor.u32 %v91849, %v91841
%v91853 = vadd.s32 %v91850, %v8
%v91857 = vadd.s32 1, %v91853
%v91861 = vadd.s32 %v91857, %v91845
%v91863 = vshll.u32 %v91857, 17
%v91864 = vshrl.u32 %v91857, 15
%v91865 = vor.u32 %v91864, %v91863
%v91866 = vxor.u32 %v91865, %v91861
%v91869 = vadd.s32 %v91866, %v91861
%v91871 = vshll.u32 %v91866, 29
%v91872 = vshrl.u32 %v91866, 3
%v91873 = vor.u32 %v91872, %v91871
%v91874 = vxor.u32 %v91873, %v91869
%v91877 = vadd.s32 %v91874, %v91869
%v91879 = vshll.u32 %v91874, 16
%v91880 = vshrl.u32 %v91874, 16
%v91881 = vor.u32 %v91880, %v91879
%v91882 = vxor.u32 %v91881, %v91877
%v91885 = vadd.s32 %v91882, %v91877
%v91889 = vadd.s32 %v91885, %v8
%v91891 = vshll.u32 %v91882, 24
%v91892 = vshrl.u32 %v91882, 8
%v91893 = vor.u32 %v91892, %v91891
%v91894 = vxor.u32 %v91893, %v91885
%v91897 = vadd.s32 %v91894, %v10
%v91901 = vadd.s32 2, %v91897
%v91905 = vadd.s32 %v91901, %v91889
%v91907 = vshll.u32 %v91901, 13
%v91908 = vshrl.u32 %v91901, 19
%v91909 = vor.u32 %v91908, %v91907
%v91910 = vxor.u32 %v91909, %v91905
%v91913 = vadd.s32 %v91910, %v91905
%v91915 = vshll.u32 %v91910, 15
%v91916 = vshrl.u32 %v91910, 17
%v91917 = vor.u32 %v91916, %v91915
%v91918 = vxor.u32 %v91917, %v91913
%v91921 = vadd.s32 %v91918, %v91913
%v91923 = vshll.u32 %v91918, 26
%v91924 = vshrl.u32 %v91918, 6
%v91925 = vor.u32 %v91924, %v91923
%v91926 = vxor.u32 %v91925, %v91921
%v91929 = vadd.s32 %v91926, %v91921
%v91933 = vadd.s32 %v91929, %v10
%v91935 = vshll.u32 %v91926, 6
%v91936 = vshrl.u32 %v91926, 26
%v91937 = vor.u32 %v91936, %v91935
%v91938 = vxor.u32 %v91937, %v91929
%v91941 = vadd.s32 %v91938, %v9
%v91945 = vadd.s32 3, %v91941
%v91949 = vadd.s32 %v91945, %v91933
%v91951 = vshll.u32 %v91945, 17
%v91952 = vshrl.u32 %v91945, 15
%v91953 = vor.u32 %v91952, %v91951
%v91954 = vxor.u32 %v91953, %v91949
%v91957 = vadd.s32 %v91954, %v91949
%v91959 = vshll.u32 %v91954, 29
%v91960 = vshrl.u32 %v91954, 3
%v91961 = vor.u32 %v91960, %v91959
%v91962 = vxor.u32 %v91961, %v91957
%v91965 = vadd.s32 %v91962, %v91957
%v91967 = vshll.u32 %v91962, 16
%v91968 = vshrl.u32 %v91962, 16
%v91969 = vor.u32 %v91968, %v91967
%v91970 = vxor.u32 %v91969, %v91965
%v91973 = vadd.s32 %v91970, %v91965
%v91977 = vadd.s32 %v91973, %v9
%v91979 = vshll.u32 %v91970, 24
%v91980 = vshrl.u32 %v91970, 8
%v91981 = vor.u32 %v91980, %v91979
%v91982 = vxor.u32 %v91981, %v91973
%v91985 = vadd.s32 %v91982, %v8
%v91989 = vadd.s32 4, %v91985
%v91993 = vadd.s32 %v91989, %v91977
%v91995 = vshll.u32 %v91989, 13
%v91996 = vshrl.u32 %v91989, 19
%v91997 = vor.u32 %v91996, %v91995
%v91998 = vxor.u32 %v91997, %v91993
%v92001 = vadd.s32 %v91998, %v91993
%v92003 = vshll.u32 %v91998, 15
%v92004 = vshrl.u32 %v91998, 17
%v92005 = vor.u32 %v92004, %v92003
%v92006 = vxor.u32 %v92005, %v92001
%v92009 = vadd.s32 %v92006, %v92001
%v92011 = vshll.u32 %v92006, 26
%v92012 = vshrl.u32 %v92006, 6
%v92013 = vor.u32 %v92012, %v92011
%v92014 = vxor.u32 %v92013, %v92009
%v92017 = vadd.s32 %v92014, %v92009
%v92021 = vadd.s32 %v92017, %v8
%v92023 = vshll.u32 %v92014, 6
%v92024 = vshrl.u32 %v92014, 26
%v92025 = vor.u32 %v92024, %v92023
%v92026 = vxor.u32 %v92025, %v92017
%v92029 = vadd.s32 %v92026, %v10
%v92033 = vadd.s32 5, %v92029
%v92035 = vxor.u32 %v92033, %v92021
%v92036 = vand.u32.u8 255, %v92035
%v92037 = vand.u32 65535, %v92036
%v92038 = vshrl.u32 %v92037, 1
%v92039 = vor.u32 16256, %v92038
%v92040 = vand.u32.u16 65535, %v92039
%v120236 = vadd.low.f32.bf16 -1.0, %v92040
%v92049 = vmul.f32 2.0, %v120236
%v92053 = vadd.f32 -0.99609375, %v92049
%v92057 = vmax.f32 %v92053, -0.99609375
%v92059 = vand.u32 2147483647, %v92057
%vm92062 = vcmp.eq.f32.partialorder %v92059, 1.0
%v92067 = vmul.f32 inf, %v92057
%v92069 = vxor.u32 2147483648, %v92057
%v92072 = vmul.f32 %v92069, %v92057
%v92074 = vadd.f32 1.0, %v92072
%v92075 = vlog2.pop %v92074
%v92076 = vmul.f32 0.6931472, %v92075
%v92077 = vmul.f32 -0.5, %v92072
%v92078 = vadd.f32 1.0, %v92077
%v92079 = vmul.f32 %v92078, %v92072
%v92080 = vand.u32 2147483647, %v92072
%vm92081 = vcmp.lt.f32.partialorder %v92080, 0.0004427343
%v92082 = vsel /*vm=*/%vm92081, /*on_true_vy=*/%v92079, /*on_false_vx=*/%v92076
%v92083 = vxor.u32 2147483648, %v92082
%vm92086 = vcmp.lt.f32.partialorder %v92083, 5.0
%v92091 = vsel /*vm=*/%vm92086, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v92095 = vsel /*vm=*/%vm92086, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v92099 = vsel /*vm=*/%vm92086, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v92103 = vsel /*vm=*/%vm92086, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v92107 = vsel /*vm=*/%vm92086, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v92111 = vsel /*vm=*/%vm92086, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v92115 = vsel /*vm=*/%vm92086, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v92119 = vsel /*vm=*/%vm92086, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v92123 = vsel /*vm=*/%vm92086, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v92127 = vadd.f32 -2.5, %v92083
%v92129 = vrsqrt.pop %v92083
%v92130 = vmul.f32 %v92129, %v92083
%vm92131 = vcmp.eq.f32.partialorder %v92083, inf
%v92132 = vsel /*vm=*/%vm92131, /*on_true_vy=*/%v92083, /*on_false_vx=*/%v92130
%vm92133 = vcmp.eq.f32.partialorder %v92083, 0.0
%v92134 = vand.u32 2147483648, %v92083
%v92135 = vsel /*vm=*/%vm92133, /*on_true_vy=*/%v92134, /*on_false_vx=*/%v92132
%v92138 = vadd.f32 -3.0, %v92135
%v92142 = vsel /*vm=*/%vm92086, /*on_true_vy=*/%v92127, /*on_false_vx=*/%v92138
%v92146 = vmul.f32 %v92142, %v92123
%v92150 = vadd.f32 %v92146, %v92119
%v92154 = vmul.f32 %v92150, %v92142
%v92158 = vadd.f32 %v92154, %v92115
%v92162 = vmul.f32 %v92158, %v92142
%v92166 = vadd.f32 %v92162, %v92111
%v92170 = vmul.f32 %v92166, %v92142
%v92174 = vadd.f32 %v92170, %v92107
%v92178 = vmul.f32 %v92174, %v92142
%v92182 = vadd.f32 %v92178, %v92103
%v92186 = vmul.f32 %v92182, %v92142
%v92190 = vadd.f32 %v92186, %v92099
%v92194 = vmul.f32 %v92190, %v92142
%v92198 = vadd.f32 %v92194, %v92095
%v92202 = vmul.f32 %v92198, %v92142
%v92206 = vadd.f32 %v92202, %v92091
%v92210 = vmul.f32 %v92206, %v92057
%v92214 = vsel /*vm=*/%vm92062, /*on_true_vy=*/%v92067, /*on_false_vx=*/%v92210
%v92218 = vmul.f32 1.4140625, %v92214
%v92221 = vpack.c.bf16 %v120417, %v92218
%120237 = vst [vmem:[%s280 + $0x260] sm:$0xf] /*vst_source=*/%v92221
%v92225 = vadd.s32 %v89917, %v2842
%v92235 = vadd.s32 %v92225, %v415
%vm92239 = vcmp.lt.u32.totalorder %v92235, %v92225
%vm92244 = vcmp.lt.u32.totalorder %v92225, %v2842
%v92249 = vadd.s32 %v89900, %v2829
%v92253 = vadd.s32 1, %v92249
%v92257 = vsel /*vm=*/%vm92244, /*on_true_vy=*/%v92253, /*on_false_vx=*/%v92249
%v92261 = vadd.s32 1, %v92257
%v92265 = vsel /*vm=*/%vm92239, /*on_true_vy=*/%v92261, /*on_false_vx=*/%v92257
%v92270 = vadd.s32 %v92265, %v10
%v92274 = vadd.s32 %v92235, %v9
%v92278 = vadd.s32 %v92274, %v92270
%v92280 = vshll.u32 %v92274, 13
%v92281 = vshrl.u32 %v92274, 19
%v92282 = vor.u32 %v92281, %v92280
%v92283 = vxor.u32 %v92282, %v92278
%v92286 = vadd.s32 %v92283, %v92278
%v92288 = vshll.u32 %v92283, 15
%v92289 = vshrl.u32 %v92283, 17
%v92290 = vor.u32 %v92289, %v92288
%v92291 = vxor.u32 %v92290, %v92286
%v92294 = vadd.s32 %v92291, %v92286
%v92296 = vshll.u32 %v92291, 26
%v92297 = vshrl.u32 %v92291, 6
%v92298 = vor.u32 %v92297, %v92296
%v92299 = vxor.u32 %v92298, %v92294
%v92302 = vadd.s32 %v92299, %v92294
%v92306 = vadd.s32 %v92302, %v9
%v92308 = vshll.u32 %v92299, 6
%v92309 = vshrl.u32 %v92299, 26
%v92310 = vor.u32 %v92309, %v92308
%v92311 = vxor.u32 %v92310, %v92302
%v92314 = vadd.s32 %v92311, %v8
%v92318 = vadd.s32 1, %v92314
%v92322 = vadd.s32 %v92318, %v92306
%v92324 = vshll.u32 %v92318, 17
%v92325 = vshrl.u32 %v92318, 15
%v92326 = vor.u32 %v92325, %v92324
%v92327 = vxor.u32 %v92326, %v92322
%v92330 = vadd.s32 %v92327, %v92322
%v92332 = vshll.u32 %v92327, 29
%v92333 = vshrl.u32 %v92327, 3
%v92334 = vor.u32 %v92333, %v92332
%v92335 = vxor.u32 %v92334, %v92330
%v92338 = vadd.s32 %v92335, %v92330
%v92340 = vshll.u32 %v92335, 16
%v92341 = vshrl.u32 %v92335, 16
%v92342 = vor.u32 %v92341, %v92340
%v92343 = vxor.u32 %v92342, %v92338
%v92346 = vadd.s32 %v92343, %v92338
%v92350 = vadd.s32 %v92346, %v8
%v92352 = vshll.u32 %v92343, 24
%v92353 = vshrl.u32 %v92343, 8
%v92354 = vor.u32 %v92353, %v92352
%v92355 = vxor.u32 %v92354, %v92346
%v92358 = vadd.s32 %v92355, %v10
%v92362 = vadd.s32 2, %v92358
%v92366 = vadd.s32 %v92362, %v92350
%v92368 = vshll.u32 %v92362, 13
%v92369 = vshrl.u32 %v92362, 19
%v92370 = vor.u32 %v92369, %v92368
%v92371 = vxor.u32 %v92370, %v92366
%v92374 = vadd.s32 %v92371, %v92366
%v92376 = vshll.u32 %v92371, 15
%v92377 = vshrl.u32 %v92371, 17
%v92378 = vor.u32 %v92377, %v92376
%v92379 = vxor.u32 %v92378, %v92374
%v92382 = vadd.s32 %v92379, %v92374
%v92384 = vshll.u32 %v92379, 26
%v92385 = vshrl.u32 %v92379, 6
%v92386 = vor.u32 %v92385, %v92384
%v92387 = vxor.u32 %v92386, %v92382
%v92390 = vadd.s32 %v92387, %v92382
%v92394 = vadd.s32 %v92390, %v10
%v92396 = vshll.u32 %v92387, 6
%v92397 = vshrl.u32 %v92387, 26
%v92398 = vor.u32 %v92397, %v92396
%v92399 = vxor.u32 %v92398, %v92390
%v92402 = vadd.s32 %v92399, %v9
%v92406 = vadd.s32 3, %v92402
%v92410 = vadd.s32 %v92406, %v92394
%v92412 = vshll.u32 %v92406, 17
%v92413 = vshrl.u32 %v92406, 15
%v92414 = vor.u32 %v92413, %v92412
%v92415 = vxor.u32 %v92414, %v92410
%v92418 = vadd.s32 %v92415, %v92410
%v92420 = vshll.u32 %v92415, 29
%v92421 = vshrl.u32 %v92415, 3
%v92422 = vor.u32 %v92421, %v92420
%v92423 = vxor.u32 %v92422, %v92418
%v92426 = vadd.s32 %v92423, %v92418
%v92428 = vshll.u32 %v92423, 16
%v92429 = vshrl.u32 %v92423, 16
%v92430 = vor.u32 %v92429, %v92428
%v92431 = vxor.u32 %v92430, %v92426
%v92434 = vadd.s32 %v92431, %v92426
%v92438 = vadd.s32 %v92434, %v9
%v92440 = vshll.u32 %v92431, 24
%v92441 = vshrl.u32 %v92431, 8
%v92442 = vor.u32 %v92441, %v92440
%v92443 = vxor.u32 %v92442, %v92434
%v92446 = vadd.s32 %v92443, %v8
%v92450 = vadd.s32 4, %v92446
%v92454 = vadd.s32 %v92450, %v92438
%v92456 = vshll.u32 %v92450, 13
%v92457 = vshrl.u32 %v92450, 19
%v92458 = vor.u32 %v92457, %v92456
%v92459 = vxor.u32 %v92458, %v92454
%v92462 = vadd.s32 %v92459, %v92454
%v92464 = vshll.u32 %v92459, 15
%v92465 = vshrl.u32 %v92459, 17
%v92466 = vor.u32 %v92465, %v92464
%v92467 = vxor.u32 %v92466, %v92462
%v92470 = vadd.s32 %v92467, %v92462
%v92472 = vshll.u32 %v92467, 26
%v92473 = vshrl.u32 %v92467, 6
%v92474 = vor.u32 %v92473, %v92472
%v92475 = vxor.u32 %v92474, %v92470
%v92478 = vadd.s32 %v92475, %v92470
%v92482 = vadd.s32 %v92478, %v8
%v92484 = vshll.u32 %v92475, 6
%v92485 = vshrl.u32 %v92475, 26
%v92486 = vor.u32 %v92485, %v92484
%v92487 = vxor.u32 %v92486, %v92478
%v92490 = vadd.s32 %v92487, %v10
%v92494 = vadd.s32 5, %v92490
%v92496 = vxor.u32 %v92494, %v92482
%v92497 = vand.u32.u8 255, %v92496
%v92498 = vand.u32 65535, %v92497
%v92499 = vshrl.u32 %v92498, 1
%v92500 = vor.u32 16256, %v92499
%v92501 = vand.u32.u16 65535, %v92500
%v120238 = vadd.low.f32.bf16 -1.0, %v92501
%v92510 = vmul.f32 2.0, %v120238
%v92514 = vadd.f32 -0.99609375, %v92510
%v92518 = vmax.f32 %v92514, -0.99609375
%v92520 = vand.u32 2147483647, %v92518
%vm92523 = vcmp.eq.f32.partialorder %v92520, 1.0
%v92528 = vmul.f32 inf, %v92518
%v92530 = vxor.u32 2147483648, %v92518
%v92533 = vmul.f32 %v92530, %v92518
%v92535 = vadd.f32 1.0, %v92533
%v92536 = vlog2.pop %v92535
%v92537 = vmul.f32 0.6931472, %v92536
%v92538 = vmul.f32 -0.5, %v92533
%v92539 = vadd.f32 1.0, %v92538
%v92540 = vmul.f32 %v92539, %v92533
%v92541 = vand.u32 2147483647, %v92533
%vm92542 = vcmp.lt.f32.partialorder %v92541, 0.0004427343
%v92543 = vsel /*vm=*/%vm92542, /*on_true_vy=*/%v92540, /*on_false_vx=*/%v92537
%v92544 = vxor.u32 2147483648, %v92543
%vm92547 = vcmp.lt.f32.partialorder %v92544, 5.0
%v92552 = vsel /*vm=*/%vm92547, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v92556 = vsel /*vm=*/%vm92547, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v92560 = vsel /*vm=*/%vm92547, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v92564 = vsel /*vm=*/%vm92547, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v92568 = vsel /*vm=*/%vm92547, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v92572 = vsel /*vm=*/%vm92547, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v92576 = vsel /*vm=*/%vm92547, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v92580 = vsel /*vm=*/%vm92547, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v92584 = vsel /*vm=*/%vm92547, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v92588 = vadd.f32 -2.5, %v92544
%v92590 = vrsqrt.pop %v92544
%v92591 = vmul.f32 %v92590, %v92544
%vm92592 = vcmp.eq.f32.partialorder %v92544, inf
%v92593 = vsel /*vm=*/%vm92592, /*on_true_vy=*/%v92544, /*on_false_vx=*/%v92591
%vm92594 = vcmp.eq.f32.partialorder %v92544, 0.0
%v92595 = vand.u32 2147483648, %v92544
%v92596 = vsel /*vm=*/%vm92594, /*on_true_vy=*/%v92595, /*on_false_vx=*/%v92593
%v92599 = vadd.f32 -3.0, %v92596
%v92603 = vsel /*vm=*/%vm92547, /*on_true_vy=*/%v92588, /*on_false_vx=*/%v92599
%v92607 = vmul.f32 %v92603, %v92584
%v92611 = vadd.f32 %v92607, %v92580
%v92615 = vmul.f32 %v92611, %v92603
%v92619 = vadd.f32 %v92615, %v92576
%v92623 = vmul.f32 %v92619, %v92603
%v92627 = vadd.f32 %v92623, %v92572
%v92631 = vmul.f32 %v92627, %v92603
%v92635 = vadd.f32 %v92631, %v92568
%v92639 = vmul.f32 %v92635, %v92603
%v92643 = vadd.f32 %v92639, %v92564
%v92647 = vmul.f32 %v92643, %v92603
%v92651 = vadd.f32 %v92647, %v92560
%v92655 = vmul.f32 %v92651, %v92603
%v92659 = vadd.f32 %v92655, %v92556
%v92663 = vmul.f32 %v92659, %v92603
%v92667 = vadd.f32 %v92663, %v92552
%v92671 = vmul.f32 %v92667, %v92518
%v92675 = vsel /*vm=*/%vm92523, /*on_true_vy=*/%v92528, /*on_false_vx=*/%v92671
%v92679 = vmul.f32 1.4140625, %v92675
%v92682 = vpack.c.bf16 %v120417, %v92679
%120239 = vst [vmem:[%s280 + $0x2e0] sm:$0xf] /*vst_source=*/%v92682
%v92686 = vadd.s32 %v89917, %v3329
%v92696 = vadd.s32 %v92686, %v415
%vm92700 = vcmp.lt.u32.totalorder %v92696, %v92686
%vm92705 = vcmp.lt.u32.totalorder %v92686, %v3329
%v92710 = vadd.s32 %v89900, %v3316
%v92714 = vadd.s32 1, %v92710
%v92718 = vsel /*vm=*/%vm92705, /*on_true_vy=*/%v92714, /*on_false_vx=*/%v92710
%v92722 = vadd.s32 1, %v92718
%v92726 = vsel /*vm=*/%vm92700, /*on_true_vy=*/%v92722, /*on_false_vx=*/%v92718
%v92731 = vadd.s32 %v92726, %v10
%v92735 = vadd.s32 %v92696, %v9
%v92739 = vadd.s32 %v92735, %v92731
%v92741 = vshll.u32 %v92735, 13
%v92742 = vshrl.u32 %v92735, 19
%v92743 = vor.u32 %v92742, %v92741
%v92744 = vxor.u32 %v92743, %v92739
%v92747 = vadd.s32 %v92744, %v92739
%v92749 = vshll.u32 %v92744, 15
%v92750 = vshrl.u32 %v92744, 17
%v92751 = vor.u32 %v92750, %v92749
%v92752 = vxor.u32 %v92751, %v92747
%v92755 = vadd.s32 %v92752, %v92747
%v92757 = vshll.u32 %v92752, 26
%v92758 = vshrl.u32 %v92752, 6
%v92759 = vor.u32 %v92758, %v92757
%v92760 = vxor.u32 %v92759, %v92755
%v92763 = vadd.s32 %v92760, %v92755
%v92767 = vadd.s32 %v92763, %v9
%v92769 = vshll.u32 %v92760, 6
%v92770 = vshrl.u32 %v92760, 26
%v92771 = vor.u32 %v92770, %v92769
%v92772 = vxor.u32 %v92771, %v92763
%v92775 = vadd.s32 %v92772, %v8
%v92779 = vadd.s32 1, %v92775
%v92783 = vadd.s32 %v92779, %v92767
%v92785 = vshll.u32 %v92779, 17
%v92786 = vshrl.u32 %v92779, 15
%v92787 = vor.u32 %v92786, %v92785
%v92788 = vxor.u32 %v92787, %v92783
%v92791 = vadd.s32 %v92788, %v92783
%v92793 = vshll.u32 %v92788, 29
%v92794 = vshrl.u32 %v92788, 3
%v92795 = vor.u32 %v92794, %v92793
%v92796 = vxor.u32 %v92795, %v92791
%v92799 = vadd.s32 %v92796, %v92791
%v92801 = vshll.u32 %v92796, 16
%v92802 = vshrl.u32 %v92796, 16
%v92803 = vor.u32 %v92802, %v92801
%v92804 = vxor.u32 %v92803, %v92799
%v92807 = vadd.s32 %v92804, %v92799
%v92811 = vadd.s32 %v92807, %v8
%v92813 = vshll.u32 %v92804, 24
%v92814 = vshrl.u32 %v92804, 8
%v92815 = vor.u32 %v92814, %v92813
%v92816 = vxor.u32 %v92815, %v92807
%v92819 = vadd.s32 %v92816, %v10
%v92823 = vadd.s32 2, %v92819
%v92827 = vadd.s32 %v92823, %v92811
%v92829 = vshll.u32 %v92823, 13
%v92830 = vshrl.u32 %v92823, 19
%v92831 = vor.u32 %v92830, %v92829
%v92832 = vxor.u32 %v92831, %v92827
%v92835 = vadd.s32 %v92832, %v92827
%v92837 = vshll.u32 %v92832, 15
%v92838 = vshrl.u32 %v92832, 17
%v92839 = vor.u32 %v92838, %v92837
%v92840 = vxor.u32 %v92839, %v92835
%v92843 = vadd.s32 %v92840, %v92835
%v92845 = vshll.u32 %v92840, 26
%v92846 = vshrl.u32 %v92840, 6
%v92847 = vor.u32 %v92846, %v92845
%v92848 = vxor.u32 %v92847, %v92843
%v92851 = vadd.s32 %v92848, %v92843
%v92855 = vadd.s32 %v92851, %v10
%v92857 = vshll.u32 %v92848, 6
%v92858 = vshrl.u32 %v92848, 26
%v92859 = vor.u32 %v92858, %v92857
%v92860 = vxor.u32 %v92859, %v92851
%v92863 = vadd.s32 %v92860, %v9
%v92867 = vadd.s32 3, %v92863
%v92871 = vadd.s32 %v92867, %v92855
%v92873 = vshll.u32 %v92867, 17
%v92874 = vshrl.u32 %v92867, 15
%v92875 = vor.u32 %v92874, %v92873
%v92876 = vxor.u32 %v92875, %v92871
%v92879 = vadd.s32 %v92876, %v92871
%v92881 = vshll.u32 %v92876, 29
%v92882 = vshrl.u32 %v92876, 3
%v92883 = vor.u32 %v92882, %v92881
%v92884 = vxor.u32 %v92883, %v92879
%v92887 = vadd.s32 %v92884, %v92879
%v92889 = vshll.u32 %v92884, 16
%v92890 = vshrl.u32 %v92884, 16
%v92891 = vor.u32 %v92890, %v92889
%v92892 = vxor.u32 %v92891, %v92887
%v92895 = vadd.s32 %v92892, %v92887
%v92899 = vadd.s32 %v92895, %v9
%v92901 = vshll.u32 %v92892, 24
%v92902 = vshrl.u32 %v92892, 8
%v92903 = vor.u32 %v92902, %v92901
%v92904 = vxor.u32 %v92903, %v92895
%v92907 = vadd.s32 %v92904, %v8
%v92911 = vadd.s32 4, %v92907
%v92915 = vadd.s32 %v92911, %v92899
%v92917 = vshll.u32 %v92911, 13
%v92918 = vshrl.u32 %v92911, 19
%v92919 = vor.u32 %v92918, %v92917
%v92920 = vxor.u32 %v92919, %v92915
%v92923 = vadd.s32 %v92920, %v92915
%v92925 = vshll.u32 %v92920, 15
%v92926 = vshrl.u32 %v92920, 17
%v92927 = vor.u32 %v92926, %v92925
%v92928 = vxor.u32 %v92927, %v92923
%v92931 = vadd.s32 %v92928, %v92923
%v92933 = vshll.u32 %v92928, 26
%v92934 = vshrl.u32 %v92928, 6
%v92935 = vor.u32 %v92934, %v92933
%v92936 = vxor.u32 %v92935, %v92931
%v92939 = vadd.s32 %v92936, %v92931
%v92943 = vadd.s32 %v92939, %v8
%v92945 = vshll.u32 %v92936, 6
%v92946 = vshrl.u32 %v92936, 26
%v92947 = vor.u32 %v92946, %v92945
%v92948 = vxor.u32 %v92947, %v92939
%v92951 = vadd.s32 %v92948, %v10
%v92955 = vadd.s32 5, %v92951
%v92957 = vxor.u32 %v92955, %v92943
%v92958 = vand.u32.u8 255, %v92957
%v92959 = vand.u32 65535, %v92958
%v92960 = vshrl.u32 %v92959, 1
%v92961 = vor.u32 16256, %v92960
%v92962 = vand.u32.u16 65535, %v92961
%v120240 = vadd.low.f32.bf16 -1.0, %v92962
%v92971 = vmul.f32 2.0, %v120240
%v92975 = vadd.f32 -0.99609375, %v92971
%v92979 = vmax.f32 %v92975, -0.99609375
%v92981 = vand.u32 2147483647, %v92979
%vm92984 = vcmp.eq.f32.partialorder %v92981, 1.0
%v92989 = vmul.f32 inf, %v92979
%v92991 = vxor.u32 2147483648, %v92979
%v92994 = vmul.f32 %v92991, %v92979
%v92996 = vadd.f32 1.0, %v92994
%v92997 = vlog2.pop %v92996
%v92998 = vmul.f32 0.6931472, %v92997
%v92999 = vmul.f32 -0.5, %v92994
%v93000 = vadd.f32 1.0, %v92999
%v93001 = vmul.f32 %v93000, %v92994
%v93002 = vand.u32 2147483647, %v92994
%vm93003 = vcmp.lt.f32.partialorder %v93002, 0.0004427343
%v93004 = vsel /*vm=*/%vm93003, /*on_true_vy=*/%v93001, /*on_false_vx=*/%v92998
%v93005 = vxor.u32 2147483648, %v93004
%vm93008 = vcmp.lt.f32.partialorder %v93005, 5.0
%v93013 = vsel /*vm=*/%vm93008, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v93017 = vsel /*vm=*/%vm93008, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v93021 = vsel /*vm=*/%vm93008, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v93025 = vsel /*vm=*/%vm93008, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v93029 = vsel /*vm=*/%vm93008, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v93033 = vsel /*vm=*/%vm93008, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v93037 = vsel /*vm=*/%vm93008, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v93041 = vsel /*vm=*/%vm93008, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v93045 = vsel /*vm=*/%vm93008, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v93049 = vadd.f32 -2.5, %v93005
%v93051 = vrsqrt.pop %v93005
%v93052 = vmul.f32 %v93051, %v93005
%vm93053 = vcmp.eq.f32.partialorder %v93005, inf
%v93054 = vsel /*vm=*/%vm93053, /*on_true_vy=*/%v93005, /*on_false_vx=*/%v93052
%vm93055 = vcmp.eq.f32.partialorder %v93005, 0.0
%v93056 = vand.u32 2147483648, %v93005
%v93057 = vsel /*vm=*/%vm93055, /*on_true_vy=*/%v93056, /*on_false_vx=*/%v93054
%v93060 = vadd.f32 -3.0, %v93057
%v93064 = vsel /*vm=*/%vm93008, /*on_true_vy=*/%v93049, /*on_false_vx=*/%v93060
%v93068 = vmul.f32 %v93064, %v93045
%v93072 = vadd.f32 %v93068, %v93041
%v93076 = vmul.f32 %v93072, %v93064
%v93080 = vadd.f32 %v93076, %v93037
%v93084 = vmul.f32 %v93080, %v93064
%v93088 = vadd.f32 %v93084, %v93033
%v93092 = vmul.f32 %v93088, %v93064
%v93096 = vadd.f32 %v93092, %v93029
%v93100 = vmul.f32 %v93096, %v93064
%v93104 = vadd.f32 %v93100, %v93025
%v93108 = vmul.f32 %v93104, %v93064
%v93112 = vadd.f32 %v93108, %v93021
%v93116 = vmul.f32 %v93112, %v93064
%v93120 = vadd.f32 %v93116, %v93017
%v93124 = vmul.f32 %v93120, %v93064
%v93128 = vadd.f32 %v93124, %v93013
%v93132 = vmul.f32 %v93128, %v92979
%v93136 = vsel /*vm=*/%vm92984, /*on_true_vy=*/%v92989, /*on_false_vx=*/%v93132
%v93140 = vmul.f32 1.4140625, %v93136
%v93143 = vpack.c.bf16 %v120417, %v93140
%120241 = vst [vmem:[%s280 + $0x360] sm:$0xf] /*vst_source=*/%v93143
%v93147 = vadd.s32 %v89917, %v3816
%v93157 = vadd.s32 %v93147, %v415
%vm93161 = vcmp.lt.u32.totalorder %v93157, %v93147
%vm93166 = vcmp.lt.u32.totalorder %v93147, %v3816
%v93171 = vadd.s32 %v89900, %v3803
%v93175 = vadd.s32 1, %v93171
%v93179 = vsel /*vm=*/%vm93166, /*on_true_vy=*/%v93175, /*on_false_vx=*/%v93171
%v93183 = vadd.s32 1, %v93179
%v93187 = vsel /*vm=*/%vm93161, /*on_true_vy=*/%v93183, /*on_false_vx=*/%v93179
%v93192 = vadd.s32 %v93187, %v10
%v93196 = vadd.s32 %v93157, %v9
%v93200 = vadd.s32 %v93196, %v93192
%v93202 = vshll.u32 %v93196, 13
%v93203 = vshrl.u32 %v93196, 19
%v93204 = vor.u32 %v93203, %v93202
%v93205 = vxor.u32 %v93204, %v93200
%v93208 = vadd.s32 %v93205, %v93200
%v93210 = vshll.u32 %v93205, 15
%v93211 = vshrl.u32 %v93205, 17
%v93212 = vor.u32 %v93211, %v93210
%v93213 = vxor.u32 %v93212, %v93208
%v93216 = vadd.s32 %v93213, %v93208
%v93218 = vshll.u32 %v93213, 26
%v93219 = vshrl.u32 %v93213, 6
%v93220 = vor.u32 %v93219, %v93218
%v93221 = vxor.u32 %v93220, %v93216
%v93224 = vadd.s32 %v93221, %v93216
%v93228 = vadd.s32 %v93224, %v9
%v93230 = vshll.u32 %v93221, 6
%v93231 = vshrl.u32 %v93221, 26
%v93232 = vor.u32 %v93231, %v93230
%v93233 = vxor.u32 %v93232, %v93224
%v93236 = vadd.s32 %v93233, %v8
%v93240 = vadd.s32 1, %v93236
%v93244 = vadd.s32 %v93240, %v93228
%v93246 = vshll.u32 %v93240, 17
%v93247 = vshrl.u32 %v93240, 15
%v93248 = vor.u32 %v93247, %v93246
%v93249 = vxor.u32 %v93248, %v93244
%v93252 = vadd.s32 %v93249, %v93244
%v93254 = vshll.u32 %v93249, 29
%v93255 = vshrl.u32 %v93249, 3
%v93256 = vor.u32 %v93255, %v93254
%v93257 = vxor.u32 %v93256, %v93252
%v93260 = vadd.s32 %v93257, %v93252
%v93262 = vshll.u32 %v93257, 16
%v93263 = vshrl.u32 %v93257, 16
%v93264 = vor.u32 %v93263, %v93262
%v93265 = vxor.u32 %v93264, %v93260
%v93268 = vadd.s32 %v93265, %v93260
%v93272 = vadd.s32 %v93268, %v8
%v93274 = vshll.u32 %v93265, 24
%v93275 = vshrl.u32 %v93265, 8
%v93276 = vor.u32 %v93275, %v93274
%v93277 = vxor.u32 %v93276, %v93268
%v93280 = vadd.s32 %v93277, %v10
%v93284 = vadd.s32 2, %v93280
%v93288 = vadd.s32 %v93284, %v93272
%v93290 = vshll.u32 %v93284, 13
%v93291 = vshrl.u32 %v93284, 19
%v93292 = vor.u32 %v93291, %v93290
%v93293 = vxor.u32 %v93292, %v93288
%v93296 = vadd.s32 %v93293, %v93288
%v93298 = vshll.u32 %v93293, 15
%v93299 = vshrl.u32 %v93293, 17
%v93300 = vor.u32 %v93299, %v93298
%v93301 = vxor.u32 %v93300, %v93296
%v93304 = vadd.s32 %v93301, %v93296
%v93306 = vshll.u32 %v93301, 26
%v93307 = vshrl.u32 %v93301, 6
%v93308 = vor.u32 %v93307, %v93306
%v93309 = vxor.u32 %v93308, %v93304
%v93312 = vadd.s32 %v93309, %v93304
%v93316 = vadd.s32 %v93312, %v10
%v93318 = vshll.u32 %v93309, 6
%v93319 = vshrl.u32 %v93309, 26
%v93320 = vor.u32 %v93319, %v93318
%v93321 = vxor.u32 %v93320, %v93312
%v93324 = vadd.s32 %v93321, %v9
%v93328 = vadd.s32 3, %v93324
%v93332 = vadd.s32 %v93328, %v93316
%v93334 = vshll.u32 %v93328, 17
%v93335 = vshrl.u32 %v93328, 15
%v93336 = vor.u32 %v93335, %v93334
%v93337 = vxor.u32 %v93336, %v93332
%v93340 = vadd.s32 %v93337, %v93332
%v93342 = vshll.u32 %v93337, 29
%v93343 = vshrl.u32 %v93337, 3
%v93344 = vor.u32 %v93343, %v93342
%v93345 = vxor.u32 %v93344, %v93340
%v93348 = vadd.s32 %v93345, %v93340
%v93350 = vshll.u32 %v93345, 16
%v93351 = vshrl.u32 %v93345, 16
%v93352 = vor.u32 %v93351, %v93350
%v93353 = vxor.u32 %v93352, %v93348
%v93356 = vadd.s32 %v93353, %v93348
%v93360 = vadd.s32 %v93356, %v9
%v93362 = vshll.u32 %v93353, 24
%v93363 = vshrl.u32 %v93353, 8
%v93364 = vor.u32 %v93363, %v93362
%v93365 = vxor.u32 %v93364, %v93356
%v93368 = vadd.s32 %v93365, %v8
%v93372 = vadd.s32 4, %v93368
%v93376 = vadd.s32 %v93372, %v93360
%v93378 = vshll.u32 %v93372, 13
%v93379 = vshrl.u32 %v93372, 19
%v93380 = vor.u32 %v93379, %v93378
%v93381 = vxor.u32 %v93380, %v93376
%v93384 = vadd.s32 %v93381, %v93376
%v93386 = vshll.u32 %v93381, 15
%v93387 = vshrl.u32 %v93381, 17
%v93388 = vor.u32 %v93387, %v93386
%v93389 = vxor.u32 %v93388, %v93384
%v93392 = vadd.s32 %v93389, %v93384
%v93394 = vshll.u32 %v93389, 26
%v93395 = vshrl.u32 %v93389, 6
%v93396 = vor.u32 %v93395, %v93394
%v93397 = vxor.u32 %v93396, %v93392
%v93400 = vadd.s32 %v93397, %v93392
%v93404 = vadd.s32 %v93400, %v8
%v93406 = vshll.u32 %v93397, 6
%v93407 = vshrl.u32 %v93397, 26
%v93408 = vor.u32 %v93407, %v93406
%v93409 = vxor.u32 %v93408, %v93400
%v93412 = vadd.s32 %v93409, %v10
%v93416 = vadd.s32 5, %v93412
%v93418 = vxor.u32 %v93416, %v93404
%v93419 = vand.u32.u8 255, %v93418
%v93420 = vand.u32 65535, %v93419
%v93421 = vshrl.u32 %v93420, 1
%v93422 = vor.u32 16256, %v93421
%v93423 = vand.u32.u16 65535, %v93422
%v120242 = vadd.low.f32.bf16 -1.0, %v93423
%v93432 = vmul.f32 2.0, %v120242
%v93436 = vadd.f32 -0.99609375, %v93432
%v93440 = vmax.f32 %v93436, -0.99609375
%v93442 = vand.u32 2147483647, %v93440
%vm93445 = vcmp.eq.f32.partialorder %v93442, 1.0
%v93450 = vmul.f32 inf, %v93440
%v93452 = vxor.u32 2147483648, %v93440
%v93455 = vmul.f32 %v93452, %v93440
%v93457 = vadd.f32 1.0, %v93455
%v93458 = vlog2.pop %v93457
%v93459 = vmul.f32 0.6931472, %v93458
%v93460 = vmul.f32 -0.5, %v93455
%v93461 = vadd.f32 1.0, %v93460
%v93462 = vmul.f32 %v93461, %v93455
%v93463 = vand.u32 2147483647, %v93455
%vm93464 = vcmp.lt.f32.partialorder %v93463, 0.0004427343
%v93465 = vsel /*vm=*/%vm93464, /*on_true_vy=*/%v93462, /*on_false_vx=*/%v93459
%v93466 = vxor.u32 2147483648, %v93465
%vm93469 = vcmp.lt.f32.partialorder %v93466, 5.0
%v93474 = vsel /*vm=*/%vm93469, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v93478 = vsel /*vm=*/%vm93469, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v93482 = vsel /*vm=*/%vm93469, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v93486 = vsel /*vm=*/%vm93469, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v93490 = vsel /*vm=*/%vm93469, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v93494 = vsel /*vm=*/%vm93469, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v93498 = vsel /*vm=*/%vm93469, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v93502 = vsel /*vm=*/%vm93469, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v93506 = vsel /*vm=*/%vm93469, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v93510 = vadd.f32 -2.5, %v93466
%v93512 = vrsqrt.pop %v93466
%v93513 = vmul.f32 %v93512, %v93466
%vm93514 = vcmp.eq.f32.partialorder %v93466, inf
%v93515 = vsel /*vm=*/%vm93514, /*on_true_vy=*/%v93466, /*on_false_vx=*/%v93513
%vm93516 = vcmp.eq.f32.partialorder %v93466, 0.0
%v93517 = vand.u32 2147483648, %v93466
%v93518 = vsel /*vm=*/%vm93516, /*on_true_vy=*/%v93517, /*on_false_vx=*/%v93515
%v93521 = vadd.f32 -3.0, %v93518
%v93525 = vsel /*vm=*/%vm93469, /*on_true_vy=*/%v93510, /*on_false_vx=*/%v93521
%v93529 = vmul.f32 %v93525, %v93506
%v93533 = vadd.f32 %v93529, %v93502
%v93537 = vmul.f32 %v93533, %v93525
%v93541 = vadd.f32 %v93537, %v93498
%v93545 = vmul.f32 %v93541, %v93525
%v93549 = vadd.f32 %v93545, %v93494
%v93553 = vmul.f32 %v93549, %v93525
%v93557 = vadd.f32 %v93553, %v93490
%v93561 = vmul.f32 %v93557, %v93525
%v93565 = vadd.f32 %v93561, %v93486
%v93569 = vmul.f32 %v93565, %v93525
%v93573 = vadd.f32 %v93569, %v93482
%v93577 = vmul.f32 %v93573, %v93525
%v93581 = vadd.f32 %v93577, %v93478
%v93585 = vmul.f32 %v93581, %v93525
%v93589 = vadd.f32 %v93585, %v93474
%v93593 = vmul.f32 %v93589, %v93440
%v93597 = vsel /*vm=*/%vm93445, /*on_true_vy=*/%v93450, /*on_false_vx=*/%v93593
%v93601 = vmul.f32 1.4140625, %v93597
%v93604 = vpack.c.bf16 %v120417, %v93601
%120243 = vst [vmem:[%s280 + $0x3e0] sm:$0xf] /*vst_source=*/%v93604
%v93642 = vadd.s32 %v93639, %v408
%v93652 = vadd.s32 %v93642, %v415
%vm93656 = vcmp.lt.u32.totalorder %v93652, %v93642
%vm93661 = vcmp.lt.u32.totalorder %v93642, %v408
%v93666 = vadd.s32 %v93622, %v380
%v93670 = vadd.s32 1, %v93666
%v93674 = vsel /*vm=*/%vm93661, /*on_true_vy=*/%v93670, /*on_false_vx=*/%v93666
%v93678 = vadd.s32 1, %v93674
%v93682 = vsel /*vm=*/%vm93656, /*on_true_vy=*/%v93678, /*on_false_vx=*/%v93674
%v93687 = vadd.s32 %v93682, %v10
%v93691 = vadd.s32 %v93652, %v9
%v93695 = vadd.s32 %v93691, %v93687
%v93697 = vshll.u32 %v93691, 13
%v93698 = vshrl.u32 %v93691, 19
%v93699 = vor.u32 %v93698, %v93697
%v93700 = vxor.u32 %v93699, %v93695
%v93703 = vadd.s32 %v93700, %v93695
%v93705 = vshll.u32 %v93700, 15
%v93706 = vshrl.u32 %v93700, 17
%v93707 = vor.u32 %v93706, %v93705
%v93708 = vxor.u32 %v93707, %v93703
%v93711 = vadd.s32 %v93708, %v93703
%v93713 = vshll.u32 %v93708, 26
%v93714 = vshrl.u32 %v93708, 6
%v93715 = vor.u32 %v93714, %v93713
%v93716 = vxor.u32 %v93715, %v93711
%v93719 = vadd.s32 %v93716, %v93711
%v93723 = vadd.s32 %v93719, %v9
%v93725 = vshll.u32 %v93716, 6
%v93726 = vshrl.u32 %v93716, 26
%v93727 = vor.u32 %v93726, %v93725
%v93728 = vxor.u32 %v93727, %v93719
%v93731 = vadd.s32 %v93728, %v8
%v93735 = vadd.s32 1, %v93731
%v93739 = vadd.s32 %v93735, %v93723
%v93741 = vshll.u32 %v93735, 17
%v93742 = vshrl.u32 %v93735, 15
%v93743 = vor.u32 %v93742, %v93741
%v93744 = vxor.u32 %v93743, %v93739
%v93747 = vadd.s32 %v93744, %v93739
%v93749 = vshll.u32 %v93744, 29
%v93750 = vshrl.u32 %v93744, 3
%v93751 = vor.u32 %v93750, %v93749
%v93752 = vxor.u32 %v93751, %v93747
%v93755 = vadd.s32 %v93752, %v93747
%v93757 = vshll.u32 %v93752, 16
%v93758 = vshrl.u32 %v93752, 16
%v93759 = vor.u32 %v93758, %v93757
%v93760 = vxor.u32 %v93759, %v93755
%v93763 = vadd.s32 %v93760, %v93755
%v93767 = vadd.s32 %v93763, %v8
%v93769 = vshll.u32 %v93760, 24
%v93770 = vshrl.u32 %v93760, 8
%v93771 = vor.u32 %v93770, %v93769
%v93772 = vxor.u32 %v93771, %v93763
%v93775 = vadd.s32 %v93772, %v10
%v93779 = vadd.s32 2, %v93775
%v93783 = vadd.s32 %v93779, %v93767
%v93785 = vshll.u32 %v93779, 13
%v93786 = vshrl.u32 %v93779, 19
%v93787 = vor.u32 %v93786, %v93785
%v93788 = vxor.u32 %v93787, %v93783
%v93791 = vadd.s32 %v93788, %v93783
%v93793 = vshll.u32 %v93788, 15
%v93794 = vshrl.u32 %v93788, 17
%v93795 = vor.u32 %v93794, %v93793
%v93796 = vxor.u32 %v93795, %v93791
%v93799 = vadd.s32 %v93796, %v93791
%v93801 = vshll.u32 %v93796, 26
%v93802 = vshrl.u32 %v93796, 6
%v93803 = vor.u32 %v93802, %v93801
%v93804 = vxor.u32 %v93803, %v93799
%v93807 = vadd.s32 %v93804, %v93799
%v93811 = vadd.s32 %v93807, %v10
%v93813 = vshll.u32 %v93804, 6
%v93814 = vshrl.u32 %v93804, 26
%v93815 = vor.u32 %v93814, %v93813
%v93816 = vxor.u32 %v93815, %v93807
%v93819 = vadd.s32 %v93816, %v9
%v93823 = vadd.s32 3, %v93819
%v93827 = vadd.s32 %v93823, %v93811
%v93829 = vshll.u32 %v93823, 17
%v93830 = vshrl.u32 %v93823, 15
%v93831 = vor.u32 %v93830, %v93829
%v93832 = vxor.u32 %v93831, %v93827
%v93835 = vadd.s32 %v93832, %v93827
%v93837 = vshll.u32 %v93832, 29
%v93838 = vshrl.u32 %v93832, 3
%v93839 = vor.u32 %v93838, %v93837
%v93840 = vxor.u32 %v93839, %v93835
%v93843 = vadd.s32 %v93840, %v93835
%v93845 = vshll.u32 %v93840, 16
%v93846 = vshrl.u32 %v93840, 16
%v93847 = vor.u32 %v93846, %v93845
%v93848 = vxor.u32 %v93847, %v93843
%v93851 = vadd.s32 %v93848, %v93843
%v93855 = vadd.s32 %v93851, %v9
%v93857 = vshll.u32 %v93848, 24
%v93858 = vshrl.u32 %v93848, 8
%v93859 = vor.u32 %v93858, %v93857
%v93860 = vxor.u32 %v93859, %v93851
%v93863 = vadd.s32 %v93860, %v8
%v93867 = vadd.s32 4, %v93863
%v93871 = vadd.s32 %v93867, %v93855
%v93873 = vshll.u32 %v93867, 13
%v93874 = vshrl.u32 %v93867, 19
%v93875 = vor.u32 %v93874, %v93873
%v93876 = vxor.u32 %v93875, %v93871
%v93879 = vadd.s32 %v93876, %v93871
%v93881 = vshll.u32 %v93876, 15
%v93882 = vshrl.u32 %v93876, 17
%v93883 = vor.u32 %v93882, %v93881
%v93884 = vxor.u32 %v93883, %v93879
%v93887 = vadd.s32 %v93884, %v93879
%v93889 = vshll.u32 %v93884, 26
%v93890 = vshrl.u32 %v93884, 6
%v93891 = vor.u32 %v93890, %v93889
%v93892 = vxor.u32 %v93891, %v93887
%v93895 = vadd.s32 %v93892, %v93887
%v93899 = vadd.s32 %v93895, %v8
%v93901 = vshll.u32 %v93892, 6
%v93902 = vshrl.u32 %v93892, 26
%v93903 = vor.u32 %v93902, %v93901
%v93904 = vxor.u32 %v93903, %v93895
%v93907 = vadd.s32 %v93904, %v10
%v93911 = vadd.s32 5, %v93907
%v93913 = vxor.u32 %v93911, %v93899
%v93914 = vand.u32.u8 255, %v93913
%v93915 = vand.u32 65535, %v93914
%v93916 = vshrl.u32 %v93915, 1
%v93917 = vor.u32 16256, %v93916
%v93918 = vand.u32.u16 65535, %v93917
%v120248 = vadd.low.f32.bf16 -1.0, %v93918
%v93927 = vmul.f32 2.0, %v120248
%v93931 = vadd.f32 -0.99609375, %v93927
%v93935 = vmax.f32 %v93931, -0.99609375
%v93937 = vand.u32 2147483647, %v93935
%vm93940 = vcmp.eq.f32.partialorder %v93937, 1.0
%v93945 = vmul.f32 inf, %v93935
%v93947 = vxor.u32 2147483648, %v93935
%v93950 = vmul.f32 %v93947, %v93935
%v93952 = vadd.f32 1.0, %v93950
%v93953 = vlog2.pop %v93952
%v93954 = vmul.f32 0.6931472, %v93953
%v93955 = vmul.f32 -0.5, %v93950
%v93956 = vadd.f32 1.0, %v93955
%v93957 = vmul.f32 %v93956, %v93950
%v93958 = vand.u32 2147483647, %v93950
%vm93959 = vcmp.lt.f32.partialorder %v93958, 0.0004427343
%v93960 = vsel /*vm=*/%vm93959, /*on_true_vy=*/%v93957, /*on_false_vx=*/%v93954
%v93961 = vxor.u32 2147483648, %v93960
%vm93964 = vcmp.lt.f32.partialorder %v93961, 5.0
%v93969 = vsel /*vm=*/%vm93964, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v93973 = vsel /*vm=*/%vm93964, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v93977 = vsel /*vm=*/%vm93964, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v93981 = vsel /*vm=*/%vm93964, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v93985 = vsel /*vm=*/%vm93964, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v93989 = vsel /*vm=*/%vm93964, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v93993 = vsel /*vm=*/%vm93964, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v93997 = vsel /*vm=*/%vm93964, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v94001 = vsel /*vm=*/%vm93964, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v94005 = vadd.f32 -2.5, %v93961
%v94007 = vrsqrt.pop %v93961
%v94008 = vmul.f32 %v94007, %v93961
%vm94009 = vcmp.eq.f32.partialorder %v93961, inf
%v94010 = vsel /*vm=*/%vm94009, /*on_true_vy=*/%v93961, /*on_false_vx=*/%v94008
%vm94011 = vcmp.eq.f32.partialorder %v93961, 0.0
%v94012 = vand.u32 2147483648, %v93961
%v94013 = vsel /*vm=*/%vm94011, /*on_true_vy=*/%v94012, /*on_false_vx=*/%v94010
%v94016 = vadd.f32 -3.0, %v94013
%v94020 = vsel /*vm=*/%vm93964, /*on_true_vy=*/%v94005, /*on_false_vx=*/%v94016
%v94024 = vmul.f32 %v94020, %v94001
%v94028 = vadd.f32 %v94024, %v93997
%v94032 = vmul.f32 %v94028, %v94020
%v94036 = vadd.f32 %v94032, %v93993
%v94040 = vmul.f32 %v94036, %v94020
%v94044 = vadd.f32 %v94040, %v93989
%v94048 = vmul.f32 %v94044, %v94020
%v94052 = vadd.f32 %v94048, %v93985
%v94056 = vmul.f32 %v94052, %v94020
%v94060 = vadd.f32 %v94056, %v93981
%v94064 = vmul.f32 %v94060, %v94020
%v94068 = vadd.f32 %v94064, %v93977
%v94072 = vmul.f32 %v94068, %v94020
%v94076 = vadd.f32 %v94072, %v93973
%v94080 = vmul.f32 %v94076, %v94020
%v94084 = vadd.f32 %v94080, %v93969
%v94088 = vmul.f32 %v94084, %v93935
%v94092 = vsel /*vm=*/%vm93940, /*on_true_vy=*/%v93945, /*on_false_vx=*/%v94088
%v94096 = vmul.f32 1.4140625, %v94092
%v94099 = vpack.c.bf16 %v120417, %v94096
%120249 = vst [vmem:[%s280 + $0x64] sm:$0xf] /*vst_source=*/%v94099
%v94103 = vadd.s32 %v93639, %v894
%v94113 = vadd.s32 %v94103, %v415
%vm94117 = vcmp.lt.u32.totalorder %v94113, %v94103
%vm94122 = vcmp.lt.u32.totalorder %v94103, %v894
%v94127 = vadd.s32 %v93622, %v881
%v94131 = vadd.s32 1, %v94127
%v94135 = vsel /*vm=*/%vm94122, /*on_true_vy=*/%v94131, /*on_false_vx=*/%v94127
%v94139 = vadd.s32 1, %v94135
%v94143 = vsel /*vm=*/%vm94117, /*on_true_vy=*/%v94139, /*on_false_vx=*/%v94135
%v94148 = vadd.s32 %v94143, %v10
%v94152 = vadd.s32 %v94113, %v9
%v94156 = vadd.s32 %v94152, %v94148
%v94158 = vshll.u32 %v94152, 13
%v94159 = vshrl.u32 %v94152, 19
%v94160 = vor.u32 %v94159, %v94158
%v94161 = vxor.u32 %v94160, %v94156
%v94164 = vadd.s32 %v94161, %v94156
%v94166 = vshll.u32 %v94161, 15
%v94167 = vshrl.u32 %v94161, 17
%v94168 = vor.u32 %v94167, %v94166
%v94169 = vxor.u32 %v94168, %v94164
%v94172 = vadd.s32 %v94169, %v94164
%v94174 = vshll.u32 %v94169, 26
%v94175 = vshrl.u32 %v94169, 6
%v94176 = vor.u32 %v94175, %v94174
%v94177 = vxor.u32 %v94176, %v94172
%v94180 = vadd.s32 %v94177, %v94172
%v94184 = vadd.s32 %v94180, %v9
%v94186 = vshll.u32 %v94177, 6
%v94187 = vshrl.u32 %v94177, 26
%v94188 = vor.u32 %v94187, %v94186
%v94189 = vxor.u32 %v94188, %v94180
%v94192 = vadd.s32 %v94189, %v8
%v94196 = vadd.s32 1, %v94192
%v94200 = vadd.s32 %v94196, %v94184
%v94202 = vshll.u32 %v94196, 17
%v94203 = vshrl.u32 %v94196, 15
%v94204 = vor.u32 %v94203, %v94202
%v94205 = vxor.u32 %v94204, %v94200
%v94208 = vadd.s32 %v94205, %v94200
%v94210 = vshll.u32 %v94205, 29
%v94211 = vshrl.u32 %v94205, 3
%v94212 = vor.u32 %v94211, %v94210
%v94213 = vxor.u32 %v94212, %v94208
%v94216 = vadd.s32 %v94213, %v94208
%v94218 = vshll.u32 %v94213, 16
%v94219 = vshrl.u32 %v94213, 16
%v94220 = vor.u32 %v94219, %v94218
%v94221 = vxor.u32 %v94220, %v94216
%v94224 = vadd.s32 %v94221, %v94216
%v94228 = vadd.s32 %v94224, %v8
%v94230 = vshll.u32 %v94221, 24
%v94231 = vshrl.u32 %v94221, 8
%v94232 = vor.u32 %v94231, %v94230
%v94233 = vxor.u32 %v94232, %v94224
%v94236 = vadd.s32 %v94233, %v10
%v94240 = vadd.s32 2, %v94236
%v94244 = vadd.s32 %v94240, %v94228
%v94246 = vshll.u32 %v94240, 13
%v94247 = vshrl.u32 %v94240, 19
%v94248 = vor.u32 %v94247, %v94246
%v94249 = vxor.u32 %v94248, %v94244
%v94252 = vadd.s32 %v94249, %v94244
%v94254 = vshll.u32 %v94249, 15
%v94255 = vshrl.u32 %v94249, 17
%v94256 = vor.u32 %v94255, %v94254
%v94257 = vxor.u32 %v94256, %v94252
%v94260 = vadd.s32 %v94257, %v94252
%v94262 = vshll.u32 %v94257, 26
%v94263 = vshrl.u32 %v94257, 6
%v94264 = vor.u32 %v94263, %v94262
%v94265 = vxor.u32 %v94264, %v94260
%v94268 = vadd.s32 %v94265, %v94260
%v94272 = vadd.s32 %v94268, %v10
%v94274 = vshll.u32 %v94265, 6
%v94275 = vshrl.u32 %v94265, 26
%v94276 = vor.u32 %v94275, %v94274
%v94277 = vxor.u32 %v94276, %v94268
%v94280 = vadd.s32 %v94277, %v9
%v94284 = vadd.s32 3, %v94280
%v94288 = vadd.s32 %v94284, %v94272
%v94290 = vshll.u32 %v94284, 17
%v94291 = vshrl.u32 %v94284, 15
%v94292 = vor.u32 %v94291, %v94290
%v94293 = vxor.u32 %v94292, %v94288
%v94296 = vadd.s32 %v94293, %v94288
%v94298 = vshll.u32 %v94293, 29
%v94299 = vshrl.u32 %v94293, 3
%v94300 = vor.u32 %v94299, %v94298
%v94301 = vxor.u32 %v94300, %v94296
%v94304 = vadd.s32 %v94301, %v94296
%v94306 = vshll.u32 %v94301, 16
%v94307 = vshrl.u32 %v94301, 16
%v94308 = vor.u32 %v94307, %v94306
%v94309 = vxor.u32 %v94308, %v94304
%v94312 = vadd.s32 %v94309, %v94304
%v94316 = vadd.s32 %v94312, %v9
%v94318 = vshll.u32 %v94309, 24
%v94319 = vshrl.u32 %v94309, 8
%v94320 = vor.u32 %v94319, %v94318
%v94321 = vxor.u32 %v94320, %v94312
%v94324 = vadd.s32 %v94321, %v8
%v94328 = vadd.s32 4, %v94324
%v94332 = vadd.s32 %v94328, %v94316
%v94334 = vshll.u32 %v94328, 13
%v94335 = vshrl.u32 %v94328, 19
%v94336 = vor.u32 %v94335, %v94334
%v94337 = vxor.u32 %v94336, %v94332
%v94340 = vadd.s32 %v94337, %v94332
%v94342 = vshll.u32 %v94337, 15
%v94343 = vshrl.u32 %v94337, 17
%v94344 = vor.u32 %v94343, %v94342
%v94345 = vxor.u32 %v94344, %v94340
%v94348 = vadd.s32 %v94345, %v94340
%v94350 = vshll.u32 %v94345, 26
%v94351 = vshrl.u32 %v94345, 6
%v94352 = vor.u32 %v94351, %v94350
%v94353 = vxor.u32 %v94352, %v94348
%v94356 = vadd.s32 %v94353, %v94348
%v94360 = vadd.s32 %v94356, %v8
%v94362 = vshll.u32 %v94353, 6
%v94363 = vshrl.u32 %v94353, 26
%v94364 = vor.u32 %v94363, %v94362
%v94365 = vxor.u32 %v94364, %v94356
%v94368 = vadd.s32 %v94365, %v10
%v94372 = vadd.s32 5, %v94368
%v94374 = vxor.u32 %v94372, %v94360
%v94375 = vand.u32.u8 255, %v94374
%v94376 = vand.u32 65535, %v94375
%v94377 = vshrl.u32 %v94376, 1
%v94378 = vor.u32 16256, %v94377
%v94379 = vand.u32.u16 65535, %v94378
%v120250 = vadd.low.f32.bf16 -1.0, %v94379
%v94388 = vmul.f32 2.0, %v120250
%v94392 = vadd.f32 -0.99609375, %v94388
%v94396 = vmax.f32 %v94392, -0.99609375
%v94398 = vand.u32 2147483647, %v94396
%vm94401 = vcmp.eq.f32.partialorder %v94398, 1.0
%v94406 = vmul.f32 inf, %v94396
%v94408 = vxor.u32 2147483648, %v94396
%v94411 = vmul.f32 %v94408, %v94396
%v94413 = vadd.f32 1.0, %v94411
%v94414 = vlog2.pop %v94413
%v94415 = vmul.f32 0.6931472, %v94414
%v94416 = vmul.f32 -0.5, %v94411
%v94417 = vadd.f32 1.0, %v94416
%v94418 = vmul.f32 %v94417, %v94411
%v94419 = vand.u32 2147483647, %v94411
%vm94420 = vcmp.lt.f32.partialorder %v94419, 0.0004427343
%v94421 = vsel /*vm=*/%vm94420, /*on_true_vy=*/%v94418, /*on_false_vx=*/%v94415
%v94422 = vxor.u32 2147483648, %v94421
%vm94425 = vcmp.lt.f32.partialorder %v94422, 5.0
%v94430 = vsel /*vm=*/%vm94425, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v94434 = vsel /*vm=*/%vm94425, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v94438 = vsel /*vm=*/%vm94425, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v94442 = vsel /*vm=*/%vm94425, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v94446 = vsel /*vm=*/%vm94425, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v94450 = vsel /*vm=*/%vm94425, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v94454 = vsel /*vm=*/%vm94425, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v94458 = vsel /*vm=*/%vm94425, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v94462 = vsel /*vm=*/%vm94425, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v94466 = vadd.f32 -2.5, %v94422
%v94468 = vrsqrt.pop %v94422
%v94469 = vmul.f32 %v94468, %v94422
%vm94470 = vcmp.eq.f32.partialorder %v94422, inf
%v94471 = vsel /*vm=*/%vm94470, /*on_true_vy=*/%v94422, /*on_false_vx=*/%v94469
%vm94472 = vcmp.eq.f32.partialorder %v94422, 0.0
%v94473 = vand.u32 2147483648, %v94422
%v94474 = vsel /*vm=*/%vm94472, /*on_true_vy=*/%v94473, /*on_false_vx=*/%v94471
%v94477 = vadd.f32 -3.0, %v94474
%v94481 = vsel /*vm=*/%vm94425, /*on_true_vy=*/%v94466, /*on_false_vx=*/%v94477
%v94485 = vmul.f32 %v94481, %v94462
%v94489 = vadd.f32 %v94485, %v94458
%v94493 = vmul.f32 %v94489, %v94481
%v94497 = vadd.f32 %v94493, %v94454
%v94501 = vmul.f32 %v94497, %v94481
%v94505 = vadd.f32 %v94501, %v94450
%v94509 = vmul.f32 %v94505, %v94481
%v94513 = vadd.f32 %v94509, %v94446
%v94517 = vmul.f32 %v94513, %v94481
%v94521 = vadd.f32 %v94517, %v94442
%v94525 = vmul.f32 %v94521, %v94481
%v94529 = vadd.f32 %v94525, %v94438
%v94533 = vmul.f32 %v94529, %v94481
%v94537 = vadd.f32 %v94533, %v94434
%v94541 = vmul.f32 %v94537, %v94481
%v94545 = vadd.f32 %v94541, %v94430
%v94549 = vmul.f32 %v94545, %v94396
%v94553 = vsel /*vm=*/%vm94401, /*on_true_vy=*/%v94406, /*on_false_vx=*/%v94549
%v94557 = vmul.f32 1.4140625, %v94553
%v94560 = vpack.c.bf16 %v120417, %v94557
%120251 = vst [vmem:[%s280 + $0xe4] sm:$0xf] /*vst_source=*/%v94560
%v94564 = vadd.s32 %v93639, %v1381
%v94574 = vadd.s32 %v94564, %v415
%vm94578 = vcmp.lt.u32.totalorder %v94574, %v94564
%vm94583 = vcmp.lt.u32.totalorder %v94564, %v1381
%v94588 = vadd.s32 %v93622, %v1368
%v94592 = vadd.s32 1, %v94588
%v94596 = vsel /*vm=*/%vm94583, /*on_true_vy=*/%v94592, /*on_false_vx=*/%v94588
%v94600 = vadd.s32 1, %v94596
%v94604 = vsel /*vm=*/%vm94578, /*on_true_vy=*/%v94600, /*on_false_vx=*/%v94596
%v94609 = vadd.s32 %v94604, %v10
%v94613 = vadd.s32 %v94574, %v9
%v94617 = vadd.s32 %v94613, %v94609
%v94619 = vshll.u32 %v94613, 13
%v94620 = vshrl.u32 %v94613, 19
%v94621 = vor.u32 %v94620, %v94619
%v94622 = vxor.u32 %v94621, %v94617
%v94625 = vadd.s32 %v94622, %v94617
%v94627 = vshll.u32 %v94622, 15
%v94628 = vshrl.u32 %v94622, 17
%v94629 = vor.u32 %v94628, %v94627
%v94630 = vxor.u32 %v94629, %v94625
%v94633 = vadd.s32 %v94630, %v94625
%v94635 = vshll.u32 %v94630, 26
%v94636 = vshrl.u32 %v94630, 6
%v94637 = vor.u32 %v94636, %v94635
%v94638 = vxor.u32 %v94637, %v94633
%v94641 = vadd.s32 %v94638, %v94633
%v94645 = vadd.s32 %v94641, %v9
%v94647 = vshll.u32 %v94638, 6
%v94648 = vshrl.u32 %v94638, 26
%v94649 = vor.u32 %v94648, %v94647
%v94650 = vxor.u32 %v94649, %v94641
%v94653 = vadd.s32 %v94650, %v8
%v94657 = vadd.s32 1, %v94653
%v94661 = vadd.s32 %v94657, %v94645
%v94663 = vshll.u32 %v94657, 17
%v94664 = vshrl.u32 %v94657, 15
%v94665 = vor.u32 %v94664, %v94663
%v94666 = vxor.u32 %v94665, %v94661
%v94669 = vadd.s32 %v94666, %v94661
%v94671 = vshll.u32 %v94666, 29
%v94672 = vshrl.u32 %v94666, 3
%v94673 = vor.u32 %v94672, %v94671
%v94674 = vxor.u32 %v94673, %v94669
%v94677 = vadd.s32 %v94674, %v94669
%v94679 = vshll.u32 %v94674, 16
%v94680 = vshrl.u32 %v94674, 16
%v94681 = vor.u32 %v94680, %v94679
%v94682 = vxor.u32 %v94681, %v94677
%v94685 = vadd.s32 %v94682, %v94677
%v94689 = vadd.s32 %v94685, %v8
%v94691 = vshll.u32 %v94682, 24
%v94692 = vshrl.u32 %v94682, 8
%v94693 = vor.u32 %v94692, %v94691
%v94694 = vxor.u32 %v94693, %v94685
%v94697 = vadd.s32 %v94694, %v10
%v94701 = vadd.s32 2, %v94697
%v94705 = vadd.s32 %v94701, %v94689
%v94707 = vshll.u32 %v94701, 13
%v94708 = vshrl.u32 %v94701, 19
%v94709 = vor.u32 %v94708, %v94707
%v94710 = vxor.u32 %v94709, %v94705
%v94713 = vadd.s32 %v94710, %v94705
%v94715 = vshll.u32 %v94710, 15
%v94716 = vshrl.u32 %v94710, 17
%v94717 = vor.u32 %v94716, %v94715
%v94718 = vxor.u32 %v94717, %v94713
%v94721 = vadd.s32 %v94718, %v94713
%v94723 = vshll.u32 %v94718, 26
%v94724 = vshrl.u32 %v94718, 6
%v94725 = vor.u32 %v94724, %v94723
%v94726 = vxor.u32 %v94725, %v94721
%v94729 = vadd.s32 %v94726, %v94721
%v94733 = vadd.s32 %v94729, %v10
%v94735 = vshll.u32 %v94726, 6
%v94736 = vshrl.u32 %v94726, 26
%v94737 = vor.u32 %v94736, %v94735
%v94738 = vxor.u32 %v94737, %v94729
%v94741 = vadd.s32 %v94738, %v9
%v94745 = vadd.s32 3, %v94741
%v94749 = vadd.s32 %v94745, %v94733
%v94751 = vshll.u32 %v94745, 17
%v94752 = vshrl.u32 %v94745, 15
%v94753 = vor.u32 %v94752, %v94751
%v94754 = vxor.u32 %v94753, %v94749
%v94757 = vadd.s32 %v94754, %v94749
%v94759 = vshll.u32 %v94754, 29
%v94760 = vshrl.u32 %v94754, 3
%v94761 = vor.u32 %v94760, %v94759
%v94762 = vxor.u32 %v94761, %v94757
%v94765 = vadd.s32 %v94762, %v94757
%v94767 = vshll.u32 %v94762, 16
%v94768 = vshrl.u32 %v94762, 16
%v94769 = vor.u32 %v94768, %v94767
%v94770 = vxor.u32 %v94769, %v94765
%v94773 = vadd.s32 %v94770, %v94765
%v94777 = vadd.s32 %v94773, %v9
%v94779 = vshll.u32 %v94770, 24
%v94780 = vshrl.u32 %v94770, 8
%v94781 = vor.u32 %v94780, %v94779
%v94782 = vxor.u32 %v94781, %v94773
%v94785 = vadd.s32 %v94782, %v8
%v94789 = vadd.s32 4, %v94785
%v94793 = vadd.s32 %v94789, %v94777
%v94795 = vshll.u32 %v94789, 13
%v94796 = vshrl.u32 %v94789, 19
%v94797 = vor.u32 %v94796, %v94795
%v94798 = vxor.u32 %v94797, %v94793
%v94801 = vadd.s32 %v94798, %v94793
%v94803 = vshll.u32 %v94798, 15
%v94804 = vshrl.u32 %v94798, 17
%v94805 = vor.u32 %v94804, %v94803
%v94806 = vxor.u32 %v94805, %v94801
%v94809 = vadd.s32 %v94806, %v94801
%v94811 = vshll.u32 %v94806, 26
%v94812 = vshrl.u32 %v94806, 6
%v94813 = vor.u32 %v94812, %v94811
%v94814 = vxor.u32 %v94813, %v94809
%v94817 = vadd.s32 %v94814, %v94809
%v94821 = vadd.s32 %v94817, %v8
%v94823 = vshll.u32 %v94814, 6
%v94824 = vshrl.u32 %v94814, 26
%v94825 = vor.u32 %v94824, %v94823
%v94826 = vxor.u32 %v94825, %v94817
%v94829 = vadd.s32 %v94826, %v10
%v94833 = vadd.s32 5, %v94829
%v94835 = vxor.u32 %v94833, %v94821
%v94836 = vand.u32.u8 255, %v94835
%v94837 = vand.u32 65535, %v94836
%v94838 = vshrl.u32 %v94837, 1
%v94839 = vor.u32 16256, %v94838
%v94840 = vand.u32.u16 65535, %v94839
%v120252 = vadd.low.f32.bf16 -1.0, %v94840
%v94849 = vmul.f32 2.0, %v120252
%v94853 = vadd.f32 -0.99609375, %v94849
%v94857 = vmax.f32 %v94853, -0.99609375
%v94859 = vand.u32 2147483647, %v94857
%vm94862 = vcmp.eq.f32.partialorder %v94859, 1.0
%v94867 = vmul.f32 inf, %v94857
%v94869 = vxor.u32 2147483648, %v94857
%v94872 = vmul.f32 %v94869, %v94857
%v94874 = vadd.f32 1.0, %v94872
%v94875 = vlog2.pop %v94874
%v94876 = vmul.f32 0.6931472, %v94875
%v94877 = vmul.f32 -0.5, %v94872
%v94878 = vadd.f32 1.0, %v94877
%v94879 = vmul.f32 %v94878, %v94872
%v94880 = vand.u32 2147483647, %v94872
%vm94881 = vcmp.lt.f32.partialorder %v94880, 0.0004427343
%v94882 = vsel /*vm=*/%vm94881, /*on_true_vy=*/%v94879, /*on_false_vx=*/%v94876
%v94883 = vxor.u32 2147483648, %v94882
%vm94886 = vcmp.lt.f32.partialorder %v94883, 5.0
%v94891 = vsel /*vm=*/%vm94886, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v94895 = vsel /*vm=*/%vm94886, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v94899 = vsel /*vm=*/%vm94886, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v94903 = vsel /*vm=*/%vm94886, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v94907 = vsel /*vm=*/%vm94886, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v94911 = vsel /*vm=*/%vm94886, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v94915 = vsel /*vm=*/%vm94886, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v94919 = vsel /*vm=*/%vm94886, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v94923 = vsel /*vm=*/%vm94886, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v94927 = vadd.f32 -2.5, %v94883
%v94929 = vrsqrt.pop %v94883
%v94930 = vmul.f32 %v94929, %v94883
%vm94931 = vcmp.eq.f32.partialorder %v94883, inf
%v94932 = vsel /*vm=*/%vm94931, /*on_true_vy=*/%v94883, /*on_false_vx=*/%v94930
%vm94933 = vcmp.eq.f32.partialorder %v94883, 0.0
%v94934 = vand.u32 2147483648, %v94883
%v94935 = vsel /*vm=*/%vm94933, /*on_true_vy=*/%v94934, /*on_false_vx=*/%v94932
%v94938 = vadd.f32 -3.0, %v94935
%v94942 = vsel /*vm=*/%vm94886, /*on_true_vy=*/%v94927, /*on_false_vx=*/%v94938
%v94946 = vmul.f32 %v94942, %v94923
%v94950 = vadd.f32 %v94946, %v94919
%v94954 = vmul.f32 %v94950, %v94942
%v94958 = vadd.f32 %v94954, %v94915
%v94962 = vmul.f32 %v94958, %v94942
%v94966 = vadd.f32 %v94962, %v94911
%v94970 = vmul.f32 %v94966, %v94942
%v94974 = vadd.f32 %v94970, %v94907
%v94978 = vmul.f32 %v94974, %v94942
%v94982 = vadd.f32 %v94978, %v94903
%v94986 = vmul.f32 %v94982, %v94942
%v94990 = vadd.f32 %v94986, %v94899
%v94994 = vmul.f32 %v94990, %v94942
%v94998 = vadd.f32 %v94994, %v94895
%v95002 = vmul.f32 %v94998, %v94942
%v95006 = vadd.f32 %v95002, %v94891
%v95010 = vmul.f32 %v95006, %v94857
%v95014 = vsel /*vm=*/%vm94862, /*on_true_vy=*/%v94867, /*on_false_vx=*/%v95010
%v95018 = vmul.f32 1.4140625, %v95014
%v95021 = vpack.c.bf16 %v120417, %v95018
%120253 = vst [vmem:[%s280 + $0x164] sm:$0xf] /*vst_source=*/%v95021
%v95025 = vadd.s32 %v93639, %v1868
%v95035 = vadd.s32 %v95025, %v415
%vm95039 = vcmp.lt.u32.totalorder %v95035, %v95025
%vm95044 = vcmp.lt.u32.totalorder %v95025, %v1868
%v95049 = vadd.s32 %v93622, %v1855
%v95053 = vadd.s32 1, %v95049
%v95057 = vsel /*vm=*/%vm95044, /*on_true_vy=*/%v95053, /*on_false_vx=*/%v95049
%v95061 = vadd.s32 1, %v95057
%v95065 = vsel /*vm=*/%vm95039, /*on_true_vy=*/%v95061, /*on_false_vx=*/%v95057
%v95070 = vadd.s32 %v95065, %v10
%v95074 = vadd.s32 %v95035, %v9
%v95078 = vadd.s32 %v95074, %v95070
%v95080 = vshll.u32 %v95074, 13
%v95081 = vshrl.u32 %v95074, 19
%v95082 = vor.u32 %v95081, %v95080
%v95083 = vxor.u32 %v95082, %v95078
%v95086 = vadd.s32 %v95083, %v95078
%v95088 = vshll.u32 %v95083, 15
%v95089 = vshrl.u32 %v95083, 17
%v95090 = vor.u32 %v95089, %v95088
%v95091 = vxor.u32 %v95090, %v95086
%v95094 = vadd.s32 %v95091, %v95086
%v95096 = vshll.u32 %v95091, 26
%v95097 = vshrl.u32 %v95091, 6
%v95098 = vor.u32 %v95097, %v95096
%v95099 = vxor.u32 %v95098, %v95094
%v95102 = vadd.s32 %v95099, %v95094
%v95106 = vadd.s32 %v95102, %v9
%v95108 = vshll.u32 %v95099, 6
%v95109 = vshrl.u32 %v95099, 26
%v95110 = vor.u32 %v95109, %v95108
%v95111 = vxor.u32 %v95110, %v95102
%v95114 = vadd.s32 %v95111, %v8
%v95118 = vadd.s32 1, %v95114
%v95122 = vadd.s32 %v95118, %v95106
%v95124 = vshll.u32 %v95118, 17
%v95125 = vshrl.u32 %v95118, 15
%v95126 = vor.u32 %v95125, %v95124
%v95127 = vxor.u32 %v95126, %v95122
%v95130 = vadd.s32 %v95127, %v95122
%v95132 = vshll.u32 %v95127, 29
%v95133 = vshrl.u32 %v95127, 3
%v95134 = vor.u32 %v95133, %v95132
%v95135 = vxor.u32 %v95134, %v95130
%v95138 = vadd.s32 %v95135, %v95130
%v95140 = vshll.u32 %v95135, 16
%v95141 = vshrl.u32 %v95135, 16
%v95142 = vor.u32 %v95141, %v95140
%v95143 = vxor.u32 %v95142, %v95138
%v95146 = vadd.s32 %v95143, %v95138
%v95150 = vadd.s32 %v95146, %v8
%v95152 = vshll.u32 %v95143, 24
%v95153 = vshrl.u32 %v95143, 8
%v95154 = vor.u32 %v95153, %v95152
%v95155 = vxor.u32 %v95154, %v95146
%v95158 = vadd.s32 %v95155, %v10
%v95162 = vadd.s32 2, %v95158
%v95166 = vadd.s32 %v95162, %v95150
%v95168 = vshll.u32 %v95162, 13
%v95169 = vshrl.u32 %v95162, 19
%v95170 = vor.u32 %v95169, %v95168
%v95171 = vxor.u32 %v95170, %v95166
%v95174 = vadd.s32 %v95171, %v95166
%v95176 = vshll.u32 %v95171, 15
%v95177 = vshrl.u32 %v95171, 17
%v95178 = vor.u32 %v95177, %v95176
%v95179 = vxor.u32 %v95178, %v95174
%v95182 = vadd.s32 %v95179, %v95174
%v95184 = vshll.u32 %v95179, 26
%v95185 = vshrl.u32 %v95179, 6
%v95186 = vor.u32 %v95185, %v95184
%v95187 = vxor.u32 %v95186, %v95182
%v95190 = vadd.s32 %v95187, %v95182
%v95194 = vadd.s32 %v95190, %v10
%v95196 = vshll.u32 %v95187, 6
%v95197 = vshrl.u32 %v95187, 26
%v95198 = vor.u32 %v95197, %v95196
%v95199 = vxor.u32 %v95198, %v95190
%v95202 = vadd.s32 %v95199, %v9
%v95206 = vadd.s32 3, %v95202
%v95210 = vadd.s32 %v95206, %v95194
%v95212 = vshll.u32 %v95206, 17
%v95213 = vshrl.u32 %v95206, 15
%v95214 = vor.u32 %v95213, %v95212
%v95215 = vxor.u32 %v95214, %v95210
%v95218 = vadd.s32 %v95215, %v95210
%v95220 = vshll.u32 %v95215, 29
%v95221 = vshrl.u32 %v95215, 3
%v95222 = vor.u32 %v95221, %v95220
%v95223 = vxor.u32 %v95222, %v95218
%v95226 = vadd.s32 %v95223, %v95218
%v95228 = vshll.u32 %v95223, 16
%v95229 = vshrl.u32 %v95223, 16
%v95230 = vor.u32 %v95229, %v95228
%v95231 = vxor.u32 %v95230, %v95226
%v95234 = vadd.s32 %v95231, %v95226
%v95238 = vadd.s32 %v95234, %v9
%v95240 = vshll.u32 %v95231, 24
%v95241 = vshrl.u32 %v95231, 8
%v95242 = vor.u32 %v95241, %v95240
%v95243 = vxor.u32 %v95242, %v95234
%v95246 = vadd.s32 %v95243, %v8
%v95250 = vadd.s32 4, %v95246
%v95254 = vadd.s32 %v95250, %v95238
%v95256 = vshll.u32 %v95250, 13
%v95257 = vshrl.u32 %v95250, 19
%v95258 = vor.u32 %v95257, %v95256
%v95259 = vxor.u32 %v95258, %v95254
%v95262 = vadd.s32 %v95259, %v95254
%v95264 = vshll.u32 %v95259, 15
%v95265 = vshrl.u32 %v95259, 17
%v95266 = vor.u32 %v95265, %v95264
%v95267 = vxor.u32 %v95266, %v95262
%v95270 = vadd.s32 %v95267, %v95262
%v95272 = vshll.u32 %v95267, 26
%v95273 = vshrl.u32 %v95267, 6
%v95274 = vor.u32 %v95273, %v95272
%v95275 = vxor.u32 %v95274, %v95270
%v95278 = vadd.s32 %v95275, %v95270
%v95282 = vadd.s32 %v95278, %v8
%v95284 = vshll.u32 %v95275, 6
%v95285 = vshrl.u32 %v95275, 26
%v95286 = vor.u32 %v95285, %v95284
%v95287 = vxor.u32 %v95286, %v95278
%v95290 = vadd.s32 %v95287, %v10
%v95294 = vadd.s32 5, %v95290
%v95296 = vxor.u32 %v95294, %v95282
%v95297 = vand.u32.u8 255, %v95296
%v95298 = vand.u32 65535, %v95297
%v95299 = vshrl.u32 %v95298, 1
%v95300 = vor.u32 16256, %v95299
%v95301 = vand.u32.u16 65535, %v95300
%v120254 = vadd.low.f32.bf16 -1.0, %v95301
%v95310 = vmul.f32 2.0, %v120254
%v95314 = vadd.f32 -0.99609375, %v95310
%v95318 = vmax.f32 %v95314, -0.99609375
%v95320 = vand.u32 2147483647, %v95318
%vm95323 = vcmp.eq.f32.partialorder %v95320, 1.0
%v95328 = vmul.f32 inf, %v95318
%v95330 = vxor.u32 2147483648, %v95318
%v95333 = vmul.f32 %v95330, %v95318
%v95335 = vadd.f32 1.0, %v95333
%v95336 = vlog2.pop %v95335
%v95337 = vmul.f32 0.6931472, %v95336
%v95338 = vmul.f32 -0.5, %v95333
%v95339 = vadd.f32 1.0, %v95338
%v95340 = vmul.f32 %v95339, %v95333
%v95341 = vand.u32 2147483647, %v95333
%vm95342 = vcmp.lt.f32.partialorder %v95341, 0.0004427343
%v95343 = vsel /*vm=*/%vm95342, /*on_true_vy=*/%v95340, /*on_false_vx=*/%v95337
%v95344 = vxor.u32 2147483648, %v95343
%vm95347 = vcmp.lt.f32.partialorder %v95344, 5.0
%v95352 = vsel /*vm=*/%vm95347, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v95356 = vsel /*vm=*/%vm95347, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v95360 = vsel /*vm=*/%vm95347, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v95364 = vsel /*vm=*/%vm95347, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v95368 = vsel /*vm=*/%vm95347, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v95372 = vsel /*vm=*/%vm95347, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v95376 = vsel /*vm=*/%vm95347, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v95380 = vsel /*vm=*/%vm95347, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v95384 = vsel /*vm=*/%vm95347, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v95388 = vadd.f32 -2.5, %v95344
%v95390 = vrsqrt.pop %v95344
%v95391 = vmul.f32 %v95390, %v95344
%vm95392 = vcmp.eq.f32.partialorder %v95344, inf
%v95393 = vsel /*vm=*/%vm95392, /*on_true_vy=*/%v95344, /*on_false_vx=*/%v95391
%vm95394 = vcmp.eq.f32.partialorder %v95344, 0.0
%v95395 = vand.u32 2147483648, %v95344
%v95396 = vsel /*vm=*/%vm95394, /*on_true_vy=*/%v95395, /*on_false_vx=*/%v95393
%v95399 = vadd.f32 -3.0, %v95396
%v95403 = vsel /*vm=*/%vm95347, /*on_true_vy=*/%v95388, /*on_false_vx=*/%v95399
%v95407 = vmul.f32 %v95403, %v95384
%v95411 = vadd.f32 %v95407, %v95380
%v95415 = vmul.f32 %v95411, %v95403
%v95419 = vadd.f32 %v95415, %v95376
%v95423 = vmul.f32 %v95419, %v95403
%v95427 = vadd.f32 %v95423, %v95372
%v95431 = vmul.f32 %v95427, %v95403
%v95435 = vadd.f32 %v95431, %v95368
%v95439 = vmul.f32 %v95435, %v95403
%v95443 = vadd.f32 %v95439, %v95364
%v95447 = vmul.f32 %v95443, %v95403
%v95451 = vadd.f32 %v95447, %v95360
%v95455 = vmul.f32 %v95451, %v95403
%v95459 = vadd.f32 %v95455, %v95356
%v95463 = vmul.f32 %v95459, %v95403
%v95467 = vadd.f32 %v95463, %v95352
%v95471 = vmul.f32 %v95467, %v95318
%v95475 = vsel /*vm=*/%vm95323, /*on_true_vy=*/%v95328, /*on_false_vx=*/%v95471
%v95479 = vmul.f32 1.4140625, %v95475
%v95482 = vpack.c.bf16 %v120417, %v95479
%120255 = vst [vmem:[%s280 + $0x1e4] sm:$0xf] /*vst_source=*/%v95482
%v95486 = vadd.s32 %v93639, %v2355
%v95496 = vadd.s32 %v95486, %v415
%vm95500 = vcmp.lt.u32.totalorder %v95496, %v95486
%vm95505 = vcmp.lt.u32.totalorder %v95486, %v2355
%v95510 = vadd.s32 %v93622, %v2342
%v95514 = vadd.s32 1, %v95510
%v95518 = vsel /*vm=*/%vm95505, /*on_true_vy=*/%v95514, /*on_false_vx=*/%v95510
%v95522 = vadd.s32 1, %v95518
%v95526 = vsel /*vm=*/%vm95500, /*on_true_vy=*/%v95522, /*on_false_vx=*/%v95518
%v95531 = vadd.s32 %v95526, %v10
%v95535 = vadd.s32 %v95496, %v9
%v95539 = vadd.s32 %v95535, %v95531
%v95541 = vshll.u32 %v95535, 13
%v95542 = vshrl.u32 %v95535, 19
%v95543 = vor.u32 %v95542, %v95541
%v95544 = vxor.u32 %v95543, %v95539
%v95547 = vadd.s32 %v95544, %v95539
%v95549 = vshll.u32 %v95544, 15
%v95550 = vshrl.u32 %v95544, 17
%v95551 = vor.u32 %v95550, %v95549
%v95552 = vxor.u32 %v95551, %v95547
%v95555 = vadd.s32 %v95552, %v95547
%v95557 = vshll.u32 %v95552, 26
%v95558 = vshrl.u32 %v95552, 6
%v95559 = vor.u32 %v95558, %v95557
%v95560 = vxor.u32 %v95559, %v95555
%v95563 = vadd.s32 %v95560, %v95555
%v95567 = vadd.s32 %v95563, %v9
%v95569 = vshll.u32 %v95560, 6
%v95570 = vshrl.u32 %v95560, 26
%v95571 = vor.u32 %v95570, %v95569
%v95572 = vxor.u32 %v95571, %v95563
%v95575 = vadd.s32 %v95572, %v8
%v95579 = vadd.s32 1, %v95575
%v95583 = vadd.s32 %v95579, %v95567
%v95585 = vshll.u32 %v95579, 17
%v95586 = vshrl.u32 %v95579, 15
%v95587 = vor.u32 %v95586, %v95585
%v95588 = vxor.u32 %v95587, %v95583
%v95591 = vadd.s32 %v95588, %v95583
%v95593 = vshll.u32 %v95588, 29
%v95594 = vshrl.u32 %v95588, 3
%v95595 = vor.u32 %v95594, %v95593
%v95596 = vxor.u32 %v95595, %v95591
%v95599 = vadd.s32 %v95596, %v95591
%v95601 = vshll.u32 %v95596, 16
%v95602 = vshrl.u32 %v95596, 16
%v95603 = vor.u32 %v95602, %v95601
%v95604 = vxor.u32 %v95603, %v95599
%v95607 = vadd.s32 %v95604, %v95599
%v95611 = vadd.s32 %v95607, %v8
%v95613 = vshll.u32 %v95604, 24
%v95614 = vshrl.u32 %v95604, 8
%v95615 = vor.u32 %v95614, %v95613
%v95616 = vxor.u32 %v95615, %v95607
%v95619 = vadd.s32 %v95616, %v10
%v95623 = vadd.s32 2, %v95619
%v95627 = vadd.s32 %v95623, %v95611
%v95629 = vshll.u32 %v95623, 13
%v95630 = vshrl.u32 %v95623, 19
%v95631 = vor.u32 %v95630, %v95629
%v95632 = vxor.u32 %v95631, %v95627
%v95635 = vadd.s32 %v95632, %v95627
%v95637 = vshll.u32 %v95632, 15
%v95638 = vshrl.u32 %v95632, 17
%v95639 = vor.u32 %v95638, %v95637
%v95640 = vxor.u32 %v95639, %v95635
%v95643 = vadd.s32 %v95640, %v95635
%v95645 = vshll.u32 %v95640, 26
%v95646 = vshrl.u32 %v95640, 6
%v95647 = vor.u32 %v95646, %v95645
%v95648 = vxor.u32 %v95647, %v95643
%v95651 = vadd.s32 %v95648, %v95643
%v95655 = vadd.s32 %v95651, %v10
%v95657 = vshll.u32 %v95648, 6
%v95658 = vshrl.u32 %v95648, 26
%v95659 = vor.u32 %v95658, %v95657
%v95660 = vxor.u32 %v95659, %v95651
%v95663 = vadd.s32 %v95660, %v9
%v95667 = vadd.s32 3, %v95663
%v95671 = vadd.s32 %v95667, %v95655
%v95673 = vshll.u32 %v95667, 17
%v95674 = vshrl.u32 %v95667, 15
%v95675 = vor.u32 %v95674, %v95673
%v95676 = vxor.u32 %v95675, %v95671
%v95679 = vadd.s32 %v95676, %v95671
%v95681 = vshll.u32 %v95676, 29
%v95682 = vshrl.u32 %v95676, 3
%v95683 = vor.u32 %v95682, %v95681
%v95684 = vxor.u32 %v95683, %v95679
%v95687 = vadd.s32 %v95684, %v95679
%v95689 = vshll.u32 %v95684, 16
%v95690 = vshrl.u32 %v95684, 16
%v95691 = vor.u32 %v95690, %v95689
%v95692 = vxor.u32 %v95691, %v95687
%v95695 = vadd.s32 %v95692, %v95687
%v95699 = vadd.s32 %v95695, %v9
%v95701 = vshll.u32 %v95692, 24
%v95702 = vshrl.u32 %v95692, 8
%v95703 = vor.u32 %v95702, %v95701
%v95704 = vxor.u32 %v95703, %v95695
%v95707 = vadd.s32 %v95704, %v8
%v95711 = vadd.s32 4, %v95707
%v95715 = vadd.s32 %v95711, %v95699
%v95717 = vshll.u32 %v95711, 13
%v95718 = vshrl.u32 %v95711, 19
%v95719 = vor.u32 %v95718, %v95717
%v95720 = vxor.u32 %v95719, %v95715
%v95723 = vadd.s32 %v95720, %v95715
%v95725 = vshll.u32 %v95720, 15
%v95726 = vshrl.u32 %v95720, 17
%v95727 = vor.u32 %v95726, %v95725
%v95728 = vxor.u32 %v95727, %v95723
%v95731 = vadd.s32 %v95728, %v95723
%v95733 = vshll.u32 %v95728, 26
%v95734 = vshrl.u32 %v95728, 6
%v95735 = vor.u32 %v95734, %v95733
%v95736 = vxor.u32 %v95735, %v95731
%v95739 = vadd.s32 %v95736, %v95731
%v95743 = vadd.s32 %v95739, %v8
%v95745 = vshll.u32 %v95736, 6
%v95746 = vshrl.u32 %v95736, 26
%v95747 = vor.u32 %v95746, %v95745
%v95748 = vxor.u32 %v95747, %v95739
%v95751 = vadd.s32 %v95748, %v10
%v95755 = vadd.s32 5, %v95751
%v95757 = vxor.u32 %v95755, %v95743
%v95758 = vand.u32.u8 255, %v95757
%v95759 = vand.u32 65535, %v95758
%v95760 = vshrl.u32 %v95759, 1
%v95761 = vor.u32 16256, %v95760
%v95762 = vand.u32.u16 65535, %v95761
%v120256 = vadd.low.f32.bf16 -1.0, %v95762
%v95771 = vmul.f32 2.0, %v120256
%v95775 = vadd.f32 -0.99609375, %v95771
%v95779 = vmax.f32 %v95775, -0.99609375
%v95781 = vand.u32 2147483647, %v95779
%vm95784 = vcmp.eq.f32.partialorder %v95781, 1.0
%v95789 = vmul.f32 inf, %v95779
%v95791 = vxor.u32 2147483648, %v95779
%v95794 = vmul.f32 %v95791, %v95779
%v95796 = vadd.f32 1.0, %v95794
%v95797 = vlog2.pop %v95796
%v95798 = vmul.f32 0.6931472, %v95797
%v95799 = vmul.f32 -0.5, %v95794
%v95800 = vadd.f32 1.0, %v95799
%v95801 = vmul.f32 %v95800, %v95794
%v95802 = vand.u32 2147483647, %v95794
%vm95803 = vcmp.lt.f32.partialorder %v95802, 0.0004427343
%v95804 = vsel /*vm=*/%vm95803, /*on_true_vy=*/%v95801, /*on_false_vx=*/%v95798
%v95805 = vxor.u32 2147483648, %v95804
%vm95808 = vcmp.lt.f32.partialorder %v95805, 5.0
%v95813 = vsel /*vm=*/%vm95808, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v95817 = vsel /*vm=*/%vm95808, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v95821 = vsel /*vm=*/%vm95808, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v95825 = vsel /*vm=*/%vm95808, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v95829 = vsel /*vm=*/%vm95808, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v95833 = vsel /*vm=*/%vm95808, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v95837 = vsel /*vm=*/%vm95808, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v95841 = vsel /*vm=*/%vm95808, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v95845 = vsel /*vm=*/%vm95808, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v95849 = vadd.f32 -2.5, %v95805
%v95851 = vrsqrt.pop %v95805
%v95852 = vmul.f32 %v95851, %v95805
%vm95853 = vcmp.eq.f32.partialorder %v95805, inf
%v95854 = vsel /*vm=*/%vm95853, /*on_true_vy=*/%v95805, /*on_false_vx=*/%v95852
%vm95855 = vcmp.eq.f32.partialorder %v95805, 0.0
%v95856 = vand.u32 2147483648, %v95805
%v95857 = vsel /*vm=*/%vm95855, /*on_true_vy=*/%v95856, /*on_false_vx=*/%v95854
%v95860 = vadd.f32 -3.0, %v95857
%v95864 = vsel /*vm=*/%vm95808, /*on_true_vy=*/%v95849, /*on_false_vx=*/%v95860
%v95868 = vmul.f32 %v95864, %v95845
%v95872 = vadd.f32 %v95868, %v95841
%v95876 = vmul.f32 %v95872, %v95864
%v95880 = vadd.f32 %v95876, %v95837
%v95884 = vmul.f32 %v95880, %v95864
%v95888 = vadd.f32 %v95884, %v95833
%v95892 = vmul.f32 %v95888, %v95864
%v95896 = vadd.f32 %v95892, %v95829
%v95900 = vmul.f32 %v95896, %v95864
%v95904 = vadd.f32 %v95900, %v95825
%v95908 = vmul.f32 %v95904, %v95864
%v95912 = vadd.f32 %v95908, %v95821
%v95916 = vmul.f32 %v95912, %v95864
%v95920 = vadd.f32 %v95916, %v95817
%v95924 = vmul.f32 %v95920, %v95864
%v95928 = vadd.f32 %v95924, %v95813
%v95932 = vmul.f32 %v95928, %v95779
%v95936 = vsel /*vm=*/%vm95784, /*on_true_vy=*/%v95789, /*on_false_vx=*/%v95932
%v95940 = vmul.f32 1.4140625, %v95936
%v95943 = vpack.c.bf16 %v120417, %v95940
%120257 = vst [vmem:[%s280 + $0x264] sm:$0xf] /*vst_source=*/%v95943
%v95947 = vadd.s32 %v93639, %v2842
%v95957 = vadd.s32 %v95947, %v415
%vm95961 = vcmp.lt.u32.totalorder %v95957, %v95947
%vm95966 = vcmp.lt.u32.totalorder %v95947, %v2842
%v95971 = vadd.s32 %v93622, %v2829
%v95975 = vadd.s32 1, %v95971
%v95979 = vsel /*vm=*/%vm95966, /*on_true_vy=*/%v95975, /*on_false_vx=*/%v95971
%v95983 = vadd.s32 1, %v95979
%v95987 = vsel /*vm=*/%vm95961, /*on_true_vy=*/%v95983, /*on_false_vx=*/%v95979
%v95992 = vadd.s32 %v95987, %v10
%v95996 = vadd.s32 %v95957, %v9
%v96000 = vadd.s32 %v95996, %v95992
%v96002 = vshll.u32 %v95996, 13
%v96003 = vshrl.u32 %v95996, 19
%v96004 = vor.u32 %v96003, %v96002
%v96005 = vxor.u32 %v96004, %v96000
%v96008 = vadd.s32 %v96005, %v96000
%v96010 = vshll.u32 %v96005, 15
%v96011 = vshrl.u32 %v96005, 17
%v96012 = vor.u32 %v96011, %v96010
%v96013 = vxor.u32 %v96012, %v96008
%v96016 = vadd.s32 %v96013, %v96008
%v96018 = vshll.u32 %v96013, 26
%v96019 = vshrl.u32 %v96013, 6
%v96020 = vor.u32 %v96019, %v96018
%v96021 = vxor.u32 %v96020, %v96016
%v96024 = vadd.s32 %v96021, %v96016
%v96028 = vadd.s32 %v96024, %v9
%v96030 = vshll.u32 %v96021, 6
%v96031 = vshrl.u32 %v96021, 26
%v96032 = vor.u32 %v96031, %v96030
%v96033 = vxor.u32 %v96032, %v96024
%v96036 = vadd.s32 %v96033, %v8
%v96040 = vadd.s32 1, %v96036
%v96044 = vadd.s32 %v96040, %v96028
%v96046 = vshll.u32 %v96040, 17
%v96047 = vshrl.u32 %v96040, 15
%v96048 = vor.u32 %v96047, %v96046
%v96049 = vxor.u32 %v96048, %v96044
%v96052 = vadd.s32 %v96049, %v96044
%v96054 = vshll.u32 %v96049, 29
%v96055 = vshrl.u32 %v96049, 3
%v96056 = vor.u32 %v96055, %v96054
%v96057 = vxor.u32 %v96056, %v96052
%v96060 = vadd.s32 %v96057, %v96052
%v96062 = vshll.u32 %v96057, 16
%v96063 = vshrl.u32 %v96057, 16
%v96064 = vor.u32 %v96063, %v96062
%v96065 = vxor.u32 %v96064, %v96060
%v96068 = vadd.s32 %v96065, %v96060
%v96072 = vadd.s32 %v96068, %v8
%v96074 = vshll.u32 %v96065, 24
%v96075 = vshrl.u32 %v96065, 8
%v96076 = vor.u32 %v96075, %v96074
%v96077 = vxor.u32 %v96076, %v96068
%v96080 = vadd.s32 %v96077, %v10
%v96084 = vadd.s32 2, %v96080
%v96088 = vadd.s32 %v96084, %v96072
%v96090 = vshll.u32 %v96084, 13
%v96091 = vshrl.u32 %v96084, 19
%v96092 = vor.u32 %v96091, %v96090
%v96093 = vxor.u32 %v96092, %v96088
%v96096 = vadd.s32 %v96093, %v96088
%v96098 = vshll.u32 %v96093, 15
%v96099 = vshrl.u32 %v96093, 17
%v96100 = vor.u32 %v96099, %v96098
%v96101 = vxor.u32 %v96100, %v96096
%v96104 = vadd.s32 %v96101, %v96096
%v96106 = vshll.u32 %v96101, 26
%v96107 = vshrl.u32 %v96101, 6
%v96108 = vor.u32 %v96107, %v96106
%v96109 = vxor.u32 %v96108, %v96104
%v96112 = vadd.s32 %v96109, %v96104
%v96116 = vadd.s32 %v96112, %v10
%v96118 = vshll.u32 %v96109, 6
%v96119 = vshrl.u32 %v96109, 26
%v96120 = vor.u32 %v96119, %v96118
%v96121 = vxor.u32 %v96120, %v96112
%v96124 = vadd.s32 %v96121, %v9
%v96128 = vadd.s32 3, %v96124
%v96132 = vadd.s32 %v96128, %v96116
%v96134 = vshll.u32 %v96128, 17
%v96135 = vshrl.u32 %v96128, 15
%v96136 = vor.u32 %v96135, %v96134
%v96137 = vxor.u32 %v96136, %v96132
%v96140 = vadd.s32 %v96137, %v96132
%v96142 = vshll.u32 %v96137, 29
%v96143 = vshrl.u32 %v96137, 3
%v96144 = vor.u32 %v96143, %v96142
%v96145 = vxor.u32 %v96144, %v96140
%v96148 = vadd.s32 %v96145, %v96140
%v96150 = vshll.u32 %v96145, 16
%v96151 = vshrl.u32 %v96145, 16
%v96152 = vor.u32 %v96151, %v96150
%v96153 = vxor.u32 %v96152, %v96148
%v96156 = vadd.s32 %v96153, %v96148
%v96160 = vadd.s32 %v96156, %v9
%v96162 = vshll.u32 %v96153, 24
%v96163 = vshrl.u32 %v96153, 8
%v96164 = vor.u32 %v96163, %v96162
%v96165 = vxor.u32 %v96164, %v96156
%v96168 = vadd.s32 %v96165, %v8
%v96172 = vadd.s32 4, %v96168
%v96176 = vadd.s32 %v96172, %v96160
%v96178 = vshll.u32 %v96172, 13
%v96179 = vshrl.u32 %v96172, 19
%v96180 = vor.u32 %v96179, %v96178
%v96181 = vxor.u32 %v96180, %v96176
%v96184 = vadd.s32 %v96181, %v96176
%v96186 = vshll.u32 %v96181, 15
%v96187 = vshrl.u32 %v96181, 17
%v96188 = vor.u32 %v96187, %v96186
%v96189 = vxor.u32 %v96188, %v96184
%v96192 = vadd.s32 %v96189, %v96184
%v96194 = vshll.u32 %v96189, 26
%v96195 = vshrl.u32 %v96189, 6
%v96196 = vor.u32 %v96195, %v96194
%v96197 = vxor.u32 %v96196, %v96192
%v96200 = vadd.s32 %v96197, %v96192
%v96204 = vadd.s32 %v96200, %v8
%v96206 = vshll.u32 %v96197, 6
%v96207 = vshrl.u32 %v96197, 26
%v96208 = vor.u32 %v96207, %v96206
%v96209 = vxor.u32 %v96208, %v96200
%v96212 = vadd.s32 %v96209, %v10
%v96216 = vadd.s32 5, %v96212
%v96218 = vxor.u32 %v96216, %v96204
%v96219 = vand.u32.u8 255, %v96218
%v96220 = vand.u32 65535, %v96219
%v96221 = vshrl.u32 %v96220, 1
%v96222 = vor.u32 16256, %v96221
%v96223 = vand.u32.u16 65535, %v96222
%v120258 = vadd.low.f32.bf16 -1.0, %v96223
%v96232 = vmul.f32 2.0, %v120258
%v96236 = vadd.f32 -0.99609375, %v96232
%v96240 = vmax.f32 %v96236, -0.99609375
%v96242 = vand.u32 2147483647, %v96240
%vm96245 = vcmp.eq.f32.partialorder %v96242, 1.0
%v96250 = vmul.f32 inf, %v96240
%v96252 = vxor.u32 2147483648, %v96240
%v96255 = vmul.f32 %v96252, %v96240
%v96257 = vadd.f32 1.0, %v96255
%v96258 = vlog2.pop %v96257
%v96259 = vmul.f32 0.6931472, %v96258
%v96260 = vmul.f32 -0.5, %v96255
%v96261 = vadd.f32 1.0, %v96260
%v96262 = vmul.f32 %v96261, %v96255
%v96263 = vand.u32 2147483647, %v96255
%vm96264 = vcmp.lt.f32.partialorder %v96263, 0.0004427343
%v96265 = vsel /*vm=*/%vm96264, /*on_true_vy=*/%v96262, /*on_false_vx=*/%v96259
%v96266 = vxor.u32 2147483648, %v96265
%vm96269 = vcmp.lt.f32.partialorder %v96266, 5.0
%v96274 = vsel /*vm=*/%vm96269, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v96278 = vsel /*vm=*/%vm96269, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v96282 = vsel /*vm=*/%vm96269, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v96286 = vsel /*vm=*/%vm96269, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v96290 = vsel /*vm=*/%vm96269, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v96294 = vsel /*vm=*/%vm96269, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v96298 = vsel /*vm=*/%vm96269, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v96302 = vsel /*vm=*/%vm96269, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v96306 = vsel /*vm=*/%vm96269, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v96310 = vadd.f32 -2.5, %v96266
%v96312 = vrsqrt.pop %v96266
%v96313 = vmul.f32 %v96312, %v96266
%vm96314 = vcmp.eq.f32.partialorder %v96266, inf
%v96315 = vsel /*vm=*/%vm96314, /*on_true_vy=*/%v96266, /*on_false_vx=*/%v96313
%vm96316 = vcmp.eq.f32.partialorder %v96266, 0.0
%v96317 = vand.u32 2147483648, %v96266
%v96318 = vsel /*vm=*/%vm96316, /*on_true_vy=*/%v96317, /*on_false_vx=*/%v96315
%v96321 = vadd.f32 -3.0, %v96318
%v96325 = vsel /*vm=*/%vm96269, /*on_true_vy=*/%v96310, /*on_false_vx=*/%v96321
%v96329 = vmul.f32 %v96325, %v96306
%v96333 = vadd.f32 %v96329, %v96302
%v96337 = vmul.f32 %v96333, %v96325
%v96341 = vadd.f32 %v96337, %v96298
%v96345 = vmul.f32 %v96341, %v96325
%v96349 = vadd.f32 %v96345, %v96294
%v96353 = vmul.f32 %v96349, %v96325
%v96357 = vadd.f32 %v96353, %v96290
%v96361 = vmul.f32 %v96357, %v96325
%v96365 = vadd.f32 %v96361, %v96286
%v96369 = vmul.f32 %v96365, %v96325
%v96373 = vadd.f32 %v96369, %v96282
%v96377 = vmul.f32 %v96373, %v96325
%v96381 = vadd.f32 %v96377, %v96278
%v96385 = vmul.f32 %v96381, %v96325
%v96389 = vadd.f32 %v96385, %v96274
%v96393 = vmul.f32 %v96389, %v96240
%v96397 = vsel /*vm=*/%vm96245, /*on_true_vy=*/%v96250, /*on_false_vx=*/%v96393
%v96401 = vmul.f32 1.4140625, %v96397
%v96404 = vpack.c.bf16 %v120417, %v96401
%120259 = vst [vmem:[%s280 + $0x2e4] sm:$0xf] /*vst_source=*/%v96404
%v96408 = vadd.s32 %v93639, %v3329
%v96418 = vadd.s32 %v96408, %v415
%vm96422 = vcmp.lt.u32.totalorder %v96418, %v96408
%vm96427 = vcmp.lt.u32.totalorder %v96408, %v3329
%v96432 = vadd.s32 %v93622, %v3316
%v96436 = vadd.s32 1, %v96432
%v96440 = vsel /*vm=*/%vm96427, /*on_true_vy=*/%v96436, /*on_false_vx=*/%v96432
%v96444 = vadd.s32 1, %v96440
%v96448 = vsel /*vm=*/%vm96422, /*on_true_vy=*/%v96444, /*on_false_vx=*/%v96440
%v96453 = vadd.s32 %v96448, %v10
%v96457 = vadd.s32 %v96418, %v9
%v96461 = vadd.s32 %v96457, %v96453
%v96463 = vshll.u32 %v96457, 13
%v96464 = vshrl.u32 %v96457, 19
%v96465 = vor.u32 %v96464, %v96463
%v96466 = vxor.u32 %v96465, %v96461
%v96469 = vadd.s32 %v96466, %v96461
%v96471 = vshll.u32 %v96466, 15
%v96472 = vshrl.u32 %v96466, 17
%v96473 = vor.u32 %v96472, %v96471
%v96474 = vxor.u32 %v96473, %v96469
%v96477 = vadd.s32 %v96474, %v96469
%v96479 = vshll.u32 %v96474, 26
%v96480 = vshrl.u32 %v96474, 6
%v96481 = vor.u32 %v96480, %v96479
%v96482 = vxor.u32 %v96481, %v96477
%v96485 = vadd.s32 %v96482, %v96477
%v96489 = vadd.s32 %v96485, %v9
%v96491 = vshll.u32 %v96482, 6
%v96492 = vshrl.u32 %v96482, 26
%v96493 = vor.u32 %v96492, %v96491
%v96494 = vxor.u32 %v96493, %v96485
%v96497 = vadd.s32 %v96494, %v8
%v96501 = vadd.s32 1, %v96497
%v96505 = vadd.s32 %v96501, %v96489
%v96507 = vshll.u32 %v96501, 17
%v96508 = vshrl.u32 %v96501, 15
%v96509 = vor.u32 %v96508, %v96507
%v96510 = vxor.u32 %v96509, %v96505
%v96513 = vadd.s32 %v96510, %v96505
%v96515 = vshll.u32 %v96510, 29
%v96516 = vshrl.u32 %v96510, 3
%v96517 = vor.u32 %v96516, %v96515
%v96518 = vxor.u32 %v96517, %v96513
%v96521 = vadd.s32 %v96518, %v96513
%v96523 = vshll.u32 %v96518, 16
%v96524 = vshrl.u32 %v96518, 16
%v96525 = vor.u32 %v96524, %v96523
%v96526 = vxor.u32 %v96525, %v96521
%v96529 = vadd.s32 %v96526, %v96521
%v96533 = vadd.s32 %v96529, %v8
%v96535 = vshll.u32 %v96526, 24
%v96536 = vshrl.u32 %v96526, 8
%v96537 = vor.u32 %v96536, %v96535
%v96538 = vxor.u32 %v96537, %v96529
%v96541 = vadd.s32 %v96538, %v10
%v96545 = vadd.s32 2, %v96541
%v96549 = vadd.s32 %v96545, %v96533
%v96551 = vshll.u32 %v96545, 13
%v96552 = vshrl.u32 %v96545, 19
%v96553 = vor.u32 %v96552, %v96551
%v96554 = vxor.u32 %v96553, %v96549
%v96557 = vadd.s32 %v96554, %v96549
%v96559 = vshll.u32 %v96554, 15
%v96560 = vshrl.u32 %v96554, 17
%v96561 = vor.u32 %v96560, %v96559
%v96562 = vxor.u32 %v96561, %v96557
%v96565 = vadd.s32 %v96562, %v96557
%v96567 = vshll.u32 %v96562, 26
%v96568 = vshrl.u32 %v96562, 6
%v96569 = vor.u32 %v96568, %v96567
%v96570 = vxor.u32 %v96569, %v96565
%v96573 = vadd.s32 %v96570, %v96565
%v96577 = vadd.s32 %v96573, %v10
%v96579 = vshll.u32 %v96570, 6
%v96580 = vshrl.u32 %v96570, 26
%v96581 = vor.u32 %v96580, %v96579
%v96582 = vxor.u32 %v96581, %v96573
%v96585 = vadd.s32 %v96582, %v9
%v96589 = vadd.s32 3, %v96585
%v96593 = vadd.s32 %v96589, %v96577
%v96595 = vshll.u32 %v96589, 17
%v96596 = vshrl.u32 %v96589, 15
%v96597 = vor.u32 %v96596, %v96595
%v96598 = vxor.u32 %v96597, %v96593
%v96601 = vadd.s32 %v96598, %v96593
%v96603 = vshll.u32 %v96598, 29
%v96604 = vshrl.u32 %v96598, 3
%v96605 = vor.u32 %v96604, %v96603
%v96606 = vxor.u32 %v96605, %v96601
%v96609 = vadd.s32 %v96606, %v96601
%v96611 = vshll.u32 %v96606, 16
%v96612 = vshrl.u32 %v96606, 16
%v96613 = vor.u32 %v96612, %v96611
%v96614 = vxor.u32 %v96613, %v96609
%v96617 = vadd.s32 %v96614, %v96609
%v96621 = vadd.s32 %v96617, %v9
%v96623 = vshll.u32 %v96614, 24
%v96624 = vshrl.u32 %v96614, 8
%v96625 = vor.u32 %v96624, %v96623
%v96626 = vxor.u32 %v96625, %v96617
%v96629 = vadd.s32 %v96626, %v8
%v96633 = vadd.s32 4, %v96629
%v96637 = vadd.s32 %v96633, %v96621
%v96639 = vshll.u32 %v96633, 13
%v96640 = vshrl.u32 %v96633, 19
%v96641 = vor.u32 %v96640, %v96639
%v96642 = vxor.u32 %v96641, %v96637
%v96645 = vadd.s32 %v96642, %v96637
%v96647 = vshll.u32 %v96642, 15
%v96648 = vshrl.u32 %v96642, 17
%v96649 = vor.u32 %v96648, %v96647
%v96650 = vxor.u32 %v96649, %v96645
%v96653 = vadd.s32 %v96650, %v96645
%v96655 = vshll.u32 %v96650, 26
%v96656 = vshrl.u32 %v96650, 6
%v96657 = vor.u32 %v96656, %v96655
%v96658 = vxor.u32 %v96657, %v96653
%v96661 = vadd.s32 %v96658, %v96653
%v96665 = vadd.s32 %v96661, %v8
%v96667 = vshll.u32 %v96658, 6
%v96668 = vshrl.u32 %v96658, 26
%v96669 = vor.u32 %v96668, %v96667
%v96670 = vxor.u32 %v96669, %v96661
%v96673 = vadd.s32 %v96670, %v10
%v96677 = vadd.s32 5, %v96673
%v96679 = vxor.u32 %v96677, %v96665
%v96680 = vand.u32.u8 255, %v96679
%v96681 = vand.u32 65535, %v96680
%v96682 = vshrl.u32 %v96681, 1
%v96683 = vor.u32 16256, %v96682
%v96684 = vand.u32.u16 65535, %v96683
%v120260 = vadd.low.f32.bf16 -1.0, %v96684
%v96693 = vmul.f32 2.0, %v120260
%v96697 = vadd.f32 -0.99609375, %v96693
%v96701 = vmax.f32 %v96697, -0.99609375
%v96703 = vand.u32 2147483647, %v96701
%vm96706 = vcmp.eq.f32.partialorder %v96703, 1.0
%v96711 = vmul.f32 inf, %v96701
%v96713 = vxor.u32 2147483648, %v96701
%v96716 = vmul.f32 %v96713, %v96701
%v96718 = vadd.f32 1.0, %v96716
%v96719 = vlog2.pop %v96718
%v96720 = vmul.f32 0.6931472, %v96719
%v96721 = vmul.f32 -0.5, %v96716
%v96722 = vadd.f32 1.0, %v96721
%v96723 = vmul.f32 %v96722, %v96716
%v96724 = vand.u32 2147483647, %v96716
%vm96725 = vcmp.lt.f32.partialorder %v96724, 0.0004427343
%v96726 = vsel /*vm=*/%vm96725, /*on_true_vy=*/%v96723, /*on_false_vx=*/%v96720
%v96727 = vxor.u32 2147483648, %v96726
%vm96730 = vcmp.lt.f32.partialorder %v96727, 5.0
%v96735 = vsel /*vm=*/%vm96730, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v96739 = vsel /*vm=*/%vm96730, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v96743 = vsel /*vm=*/%vm96730, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v96747 = vsel /*vm=*/%vm96730, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v96751 = vsel /*vm=*/%vm96730, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v96755 = vsel /*vm=*/%vm96730, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v96759 = vsel /*vm=*/%vm96730, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v96763 = vsel /*vm=*/%vm96730, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v96767 = vsel /*vm=*/%vm96730, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v96771 = vadd.f32 -2.5, %v96727
%v96773 = vrsqrt.pop %v96727
%v96774 = vmul.f32 %v96773, %v96727
%vm96775 = vcmp.eq.f32.partialorder %v96727, inf
%v96776 = vsel /*vm=*/%vm96775, /*on_true_vy=*/%v96727, /*on_false_vx=*/%v96774
%vm96777 = vcmp.eq.f32.partialorder %v96727, 0.0
%v96778 = vand.u32 2147483648, %v96727
%v96779 = vsel /*vm=*/%vm96777, /*on_true_vy=*/%v96778, /*on_false_vx=*/%v96776
%v96782 = vadd.f32 -3.0, %v96779
%v96786 = vsel /*vm=*/%vm96730, /*on_true_vy=*/%v96771, /*on_false_vx=*/%v96782
%v96790 = vmul.f32 %v96786, %v96767
%v96794 = vadd.f32 %v96790, %v96763
%v96798 = vmul.f32 %v96794, %v96786
%v96802 = vadd.f32 %v96798, %v96759
%v96806 = vmul.f32 %v96802, %v96786
%v96810 = vadd.f32 %v96806, %v96755
%v96814 = vmul.f32 %v96810, %v96786
%v96818 = vadd.f32 %v96814, %v96751
%v96822 = vmul.f32 %v96818, %v96786
%v96826 = vadd.f32 %v96822, %v96747
%v96830 = vmul.f32 %v96826, %v96786
%v96834 = vadd.f32 %v96830, %v96743
%v96838 = vmul.f32 %v96834, %v96786
%v96842 = vadd.f32 %v96838, %v96739
%v96846 = vmul.f32 %v96842, %v96786
%v96850 = vadd.f32 %v96846, %v96735
%v96854 = vmul.f32 %v96850, %v96701
%v96858 = vsel /*vm=*/%vm96706, /*on_true_vy=*/%v96711, /*on_false_vx=*/%v96854
%v96862 = vmul.f32 1.4140625, %v96858
%v96865 = vpack.c.bf16 %v120417, %v96862
%120261 = vst [vmem:[%s280 + $0x364] sm:$0xf] /*vst_source=*/%v96865
%v96869 = vadd.s32 %v93639, %v3816
%v96879 = vadd.s32 %v96869, %v415
%vm96883 = vcmp.lt.u32.totalorder %v96879, %v96869
%vm96888 = vcmp.lt.u32.totalorder %v96869, %v3816
%v96893 = vadd.s32 %v93622, %v3803
%v96897 = vadd.s32 1, %v96893
%v96901 = vsel /*vm=*/%vm96888, /*on_true_vy=*/%v96897, /*on_false_vx=*/%v96893
%v96905 = vadd.s32 1, %v96901
%v96909 = vsel /*vm=*/%vm96883, /*on_true_vy=*/%v96905, /*on_false_vx=*/%v96901
%v96914 = vadd.s32 %v96909, %v10
%v96918 = vadd.s32 %v96879, %v9
%v96922 = vadd.s32 %v96918, %v96914
%v96924 = vshll.u32 %v96918, 13
%v96925 = vshrl.u32 %v96918, 19
%v96926 = vor.u32 %v96925, %v96924
%v96927 = vxor.u32 %v96926, %v96922
%v96930 = vadd.s32 %v96927, %v96922
%v96932 = vshll.u32 %v96927, 15
%v96933 = vshrl.u32 %v96927, 17
%v96934 = vor.u32 %v96933, %v96932
%v96935 = vxor.u32 %v96934, %v96930
%v96938 = vadd.s32 %v96935, %v96930
%v96940 = vshll.u32 %v96935, 26
%v96941 = vshrl.u32 %v96935, 6
%v96942 = vor.u32 %v96941, %v96940
%v96943 = vxor.u32 %v96942, %v96938
%v96946 = vadd.s32 %v96943, %v96938
%v96950 = vadd.s32 %v96946, %v9
%v96952 = vshll.u32 %v96943, 6
%v96953 = vshrl.u32 %v96943, 26
%v96954 = vor.u32 %v96953, %v96952
%v96955 = vxor.u32 %v96954, %v96946
%v96958 = vadd.s32 %v96955, %v8
%v96962 = vadd.s32 1, %v96958
%v96966 = vadd.s32 %v96962, %v96950
%v96968 = vshll.u32 %v96962, 17
%v96969 = vshrl.u32 %v96962, 15
%v96970 = vor.u32 %v96969, %v96968
%v96971 = vxor.u32 %v96970, %v96966
%v96974 = vadd.s32 %v96971, %v96966
%v96976 = vshll.u32 %v96971, 29
%v96977 = vshrl.u32 %v96971, 3
%v96978 = vor.u32 %v96977, %v96976
%v96979 = vxor.u32 %v96978, %v96974
%v96982 = vadd.s32 %v96979, %v96974
%v96984 = vshll.u32 %v96979, 16
%v96985 = vshrl.u32 %v96979, 16
%v96986 = vor.u32 %v96985, %v96984
%v96987 = vxor.u32 %v96986, %v96982
%v96990 = vadd.s32 %v96987, %v96982
%v96994 = vadd.s32 %v96990, %v8
%v96996 = vshll.u32 %v96987, 24
%v96997 = vshrl.u32 %v96987, 8
%v96998 = vor.u32 %v96997, %v96996
%v96999 = vxor.u32 %v96998, %v96990
%v97002 = vadd.s32 %v96999, %v10
%v97006 = vadd.s32 2, %v97002
%v97010 = vadd.s32 %v97006, %v96994
%v97012 = vshll.u32 %v97006, 13
%v97013 = vshrl.u32 %v97006, 19
%v97014 = vor.u32 %v97013, %v97012
%v97015 = vxor.u32 %v97014, %v97010
%v97018 = vadd.s32 %v97015, %v97010
%v97020 = vshll.u32 %v97015, 15
%v97021 = vshrl.u32 %v97015, 17
%v97022 = vor.u32 %v97021, %v97020
%v97023 = vxor.u32 %v97022, %v97018
%v97026 = vadd.s32 %v97023, %v97018
%v97028 = vshll.u32 %v97023, 26
%v97029 = vshrl.u32 %v97023, 6
%v97030 = vor.u32 %v97029, %v97028
%v97031 = vxor.u32 %v97030, %v97026
%v97034 = vadd.s32 %v97031, %v97026
%v97038 = vadd.s32 %v97034, %v10
%v97040 = vshll.u32 %v97031, 6
%v97041 = vshrl.u32 %v97031, 26
%v97042 = vor.u32 %v97041, %v97040
%v97043 = vxor.u32 %v97042, %v97034
%v97046 = vadd.s32 %v97043, %v9
%v97050 = vadd.s32 3, %v97046
%v97054 = vadd.s32 %v97050, %v97038
%v97056 = vshll.u32 %v97050, 17
%v97057 = vshrl.u32 %v97050, 15
%v97058 = vor.u32 %v97057, %v97056
%v97059 = vxor.u32 %v97058, %v97054
%v97062 = vadd.s32 %v97059, %v97054
%v97064 = vshll.u32 %v97059, 29
%v97065 = vshrl.u32 %v97059, 3
%v97066 = vor.u32 %v97065, %v97064
%v97067 = vxor.u32 %v97066, %v97062
%v97070 = vadd.s32 %v97067, %v97062
%v97072 = vshll.u32 %v97067, 16
%v97073 = vshrl.u32 %v97067, 16
%v97074 = vor.u32 %v97073, %v97072
%v97075 = vxor.u32 %v97074, %v97070
%v97078 = vadd.s32 %v97075, %v97070
%v97082 = vadd.s32 %v97078, %v9
%v97084 = vshll.u32 %v97075, 24
%v97085 = vshrl.u32 %v97075, 8
%v97086 = vor.u32 %v97085, %v97084
%v97087 = vxor.u32 %v97086, %v97078
%v97090 = vadd.s32 %v97087, %v8
%v97094 = vadd.s32 4, %v97090
%v97098 = vadd.s32 %v97094, %v97082
%v97100 = vshll.u32 %v97094, 13
%v97101 = vshrl.u32 %v97094, 19
%v97102 = vor.u32 %v97101, %v97100
%v97103 = vxor.u32 %v97102, %v97098
%v97106 = vadd.s32 %v97103, %v97098
%v97108 = vshll.u32 %v97103, 15
%v97109 = vshrl.u32 %v97103, 17
%v97110 = vor.u32 %v97109, %v97108
%v97111 = vxor.u32 %v97110, %v97106
%v97114 = vadd.s32 %v97111, %v97106
%v97116 = vshll.u32 %v97111, 26
%v97117 = vshrl.u32 %v97111, 6
%v97118 = vor.u32 %v97117, %v97116
%v97119 = vxor.u32 %v97118, %v97114
%v97122 = vadd.s32 %v97119, %v97114
%v97126 = vadd.s32 %v97122, %v8
%v97128 = vshll.u32 %v97119, 6
%v97129 = vshrl.u32 %v97119, 26
%v97130 = vor.u32 %v97129, %v97128
%v97131 = vxor.u32 %v97130, %v97122
%v97134 = vadd.s32 %v97131, %v10
%v97138 = vadd.s32 5, %v97134
%v97140 = vxor.u32 %v97138, %v97126
%v97141 = vand.u32.u8 255, %v97140
%v97142 = vand.u32 65535, %v97141
%v97143 = vshrl.u32 %v97142, 1
%v97144 = vor.u32 16256, %v97143
%v97145 = vand.u32.u16 65535, %v97144
%v120262 = vadd.low.f32.bf16 -1.0, %v97145
%v97154 = vmul.f32 2.0, %v120262
%v97158 = vadd.f32 -0.99609375, %v97154
%v97162 = vmax.f32 %v97158, -0.99609375
%v97164 = vand.u32 2147483647, %v97162
%vm97167 = vcmp.eq.f32.partialorder %v97164, 1.0
%v97172 = vmul.f32 inf, %v97162
%v97174 = vxor.u32 2147483648, %v97162
%v97177 = vmul.f32 %v97174, %v97162
%v97179 = vadd.f32 1.0, %v97177
%v97180 = vlog2.pop %v97179
%v97181 = vmul.f32 0.6931472, %v97180
%v97182 = vmul.f32 -0.5, %v97177
%v97183 = vadd.f32 1.0, %v97182
%v97184 = vmul.f32 %v97183, %v97177
%v97185 = vand.u32 2147483647, %v97177
%vm97186 = vcmp.lt.f32.partialorder %v97185, 0.0004427343
%v97187 = vsel /*vm=*/%vm97186, /*on_true_vy=*/%v97184, /*on_false_vx=*/%v97181
%v97188 = vxor.u32 2147483648, %v97187
%vm97191 = vcmp.lt.f32.partialorder %v97188, 5.0
%v97196 = vsel /*vm=*/%vm97191, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v97200 = vsel /*vm=*/%vm97191, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v97204 = vsel /*vm=*/%vm97191, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v97208 = vsel /*vm=*/%vm97191, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v97212 = vsel /*vm=*/%vm97191, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v97216 = vsel /*vm=*/%vm97191, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v97220 = vsel /*vm=*/%vm97191, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v97224 = vsel /*vm=*/%vm97191, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v97228 = vsel /*vm=*/%vm97191, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v97232 = vadd.f32 -2.5, %v97188
%v97234 = vrsqrt.pop %v97188
%v97235 = vmul.f32 %v97234, %v97188
%vm97236 = vcmp.eq.f32.partialorder %v97188, inf
%v97237 = vsel /*vm=*/%vm97236, /*on_true_vy=*/%v97188, /*on_false_vx=*/%v97235
%vm97238 = vcmp.eq.f32.partialorder %v97188, 0.0
%v97239 = vand.u32 2147483648, %v97188
%v97240 = vsel /*vm=*/%vm97238, /*on_true_vy=*/%v97239, /*on_false_vx=*/%v97237
%v97243 = vadd.f32 -3.0, %v97240
%v97247 = vsel /*vm=*/%vm97191, /*on_true_vy=*/%v97232, /*on_false_vx=*/%v97243
%v97251 = vmul.f32 %v97247, %v97228
%v97255 = vadd.f32 %v97251, %v97224
%v97259 = vmul.f32 %v97255, %v97247
%v97263 = vadd.f32 %v97259, %v97220
%v97267 = vmul.f32 %v97263, %v97247
%v97271 = vadd.f32 %v97267, %v97216
%v97275 = vmul.f32 %v97271, %v97247
%v97279 = vadd.f32 %v97275, %v97212
%v97283 = vmul.f32 %v97279, %v97247
%v97287 = vadd.f32 %v97283, %v97208
%v97291 = vmul.f32 %v97287, %v97247
%v97295 = vadd.f32 %v97291, %v97204
%v97299 = vmul.f32 %v97295, %v97247
%v97303 = vadd.f32 %v97299, %v97200
%v97307 = vmul.f32 %v97303, %v97247
%v97311 = vadd.f32 %v97307, %v97196
%v97315 = vmul.f32 %v97311, %v97162
%v97319 = vsel /*vm=*/%vm97167, /*on_true_vy=*/%v97172, /*on_false_vx=*/%v97315
%v97323 = vmul.f32 1.4140625, %v97319
%v97326 = vpack.c.bf16 %v120417, %v97323
%120263 = vst [vmem:[%s280 + $0x3e4] sm:$0xf] /*vst_source=*/%v97326
%v97364 = vadd.s32 %v97361, %v408
%v97374 = vadd.s32 %v97364, %v415
%vm97378 = vcmp.lt.u32.totalorder %v97374, %v97364
%vm97383 = vcmp.lt.u32.totalorder %v97364, %v408
%v97388 = vadd.s32 %v97344, %v380
%v97392 = vadd.s32 1, %v97388
%v97396 = vsel /*vm=*/%vm97383, /*on_true_vy=*/%v97392, /*on_false_vx=*/%v97388
%v97400 = vadd.s32 1, %v97396
%v97404 = vsel /*vm=*/%vm97378, /*on_true_vy=*/%v97400, /*on_false_vx=*/%v97396
%v97409 = vadd.s32 %v97404, %v10
%v97413 = vadd.s32 %v97374, %v9
%v97417 = vadd.s32 %v97413, %v97409
%v97419 = vshll.u32 %v97413, 13
%v97420 = vshrl.u32 %v97413, 19
%v97421 = vor.u32 %v97420, %v97419
%v97422 = vxor.u32 %v97421, %v97417
%v97425 = vadd.s32 %v97422, %v97417
%v97427 = vshll.u32 %v97422, 15
%v97428 = vshrl.u32 %v97422, 17
%v97429 = vor.u32 %v97428, %v97427
%v97430 = vxor.u32 %v97429, %v97425
%v97433 = vadd.s32 %v97430, %v97425
%v97435 = vshll.u32 %v97430, 26
%v97436 = vshrl.u32 %v97430, 6
%v97437 = vor.u32 %v97436, %v97435
%v97438 = vxor.u32 %v97437, %v97433
%v97441 = vadd.s32 %v97438, %v97433
%v97445 = vadd.s32 %v97441, %v9
%v97447 = vshll.u32 %v97438, 6
%v97448 = vshrl.u32 %v97438, 26
%v97449 = vor.u32 %v97448, %v97447
%v97450 = vxor.u32 %v97449, %v97441
%v97453 = vadd.s32 %v97450, %v8
%v97457 = vadd.s32 1, %v97453
%v97461 = vadd.s32 %v97457, %v97445
%v97463 = vshll.u32 %v97457, 17
%v97464 = vshrl.u32 %v97457, 15
%v97465 = vor.u32 %v97464, %v97463
%v97466 = vxor.u32 %v97465, %v97461
%v97469 = vadd.s32 %v97466, %v97461
%v97471 = vshll.u32 %v97466, 29
%v97472 = vshrl.u32 %v97466, 3
%v97473 = vor.u32 %v97472, %v97471
%v97474 = vxor.u32 %v97473, %v97469
%v97477 = vadd.s32 %v97474, %v97469
%v97479 = vshll.u32 %v97474, 16
%v97480 = vshrl.u32 %v97474, 16
%v97481 = vor.u32 %v97480, %v97479
%v97482 = vxor.u32 %v97481, %v97477
%v97485 = vadd.s32 %v97482, %v97477
%v97489 = vadd.s32 %v97485, %v8
%v97491 = vshll.u32 %v97482, 24
%v97492 = vshrl.u32 %v97482, 8
%v97493 = vor.u32 %v97492, %v97491
%v97494 = vxor.u32 %v97493, %v97485
%v97497 = vadd.s32 %v97494, %v10
%v97501 = vadd.s32 2, %v97497
%v97505 = vadd.s32 %v97501, %v97489
%v97507 = vshll.u32 %v97501, 13
%v97508 = vshrl.u32 %v97501, 19
%v97509 = vor.u32 %v97508, %v97507
%v97510 = vxor.u32 %v97509, %v97505
%v97513 = vadd.s32 %v97510, %v97505
%v97515 = vshll.u32 %v97510, 15
%v97516 = vshrl.u32 %v97510, 17
%v97517 = vor.u32 %v97516, %v97515
%v97518 = vxor.u32 %v97517, %v97513
%v97521 = vadd.s32 %v97518, %v97513
%v97523 = vshll.u32 %v97518, 26
%v97524 = vshrl.u32 %v97518, 6
%v97525 = vor.u32 %v97524, %v97523
%v97526 = vxor.u32 %v97525, %v97521
%v97529 = vadd.s32 %v97526, %v97521
%v97533 = vadd.s32 %v97529, %v10
%v97535 = vshll.u32 %v97526, 6
%v97536 = vshrl.u32 %v97526, 26
%v97537 = vor.u32 %v97536, %v97535
%v97538 = vxor.u32 %v97537, %v97529
%v97541 = vadd.s32 %v97538, %v9
%v97545 = vadd.s32 3, %v97541
%v97549 = vadd.s32 %v97545, %v97533
%v97551 = vshll.u32 %v97545, 17
%v97552 = vshrl.u32 %v97545, 15
%v97553 = vor.u32 %v97552, %v97551
%v97554 = vxor.u32 %v97553, %v97549
%v97557 = vadd.s32 %v97554, %v97549
%v97559 = vshll.u32 %v97554, 29
%v97560 = vshrl.u32 %v97554, 3
%v97561 = vor.u32 %v97560, %v97559
%v97562 = vxor.u32 %v97561, %v97557
%v97565 = vadd.s32 %v97562, %v97557
%v97567 = vshll.u32 %v97562, 16
%v97568 = vshrl.u32 %v97562, 16
%v97569 = vor.u32 %v97568, %v97567
%v97570 = vxor.u32 %v97569, %v97565
%v97573 = vadd.s32 %v97570, %v97565
%v97577 = vadd.s32 %v97573, %v9
%v97579 = vshll.u32 %v97570, 24
%v97580 = vshrl.u32 %v97570, 8
%v97581 = vor.u32 %v97580, %v97579
%v97582 = vxor.u32 %v97581, %v97573
%v97585 = vadd.s32 %v97582, %v8
%v97589 = vadd.s32 4, %v97585
%v97593 = vadd.s32 %v97589, %v97577
%v97595 = vshll.u32 %v97589, 13
%v97596 = vshrl.u32 %v97589, 19
%v97597 = vor.u32 %v97596, %v97595
%v97598 = vxor.u32 %v97597, %v97593
%v97601 = vadd.s32 %v97598, %v97593
%v97603 = vshll.u32 %v97598, 15
%v97604 = vshrl.u32 %v97598, 17
%v97605 = vor.u32 %v97604, %v97603
%v97606 = vxor.u32 %v97605, %v97601
%v97609 = vadd.s32 %v97606, %v97601
%v97611 = vshll.u32 %v97606, 26
%v97612 = vshrl.u32 %v97606, 6
%v97613 = vor.u32 %v97612, %v97611
%v97614 = vxor.u32 %v97613, %v97609
%v97617 = vadd.s32 %v97614, %v97609
%v97621 = vadd.s32 %v97617, %v8
%v97623 = vshll.u32 %v97614, 6
%v97624 = vshrl.u32 %v97614, 26
%v97625 = vor.u32 %v97624, %v97623
%v97626 = vxor.u32 %v97625, %v97617
%v97629 = vadd.s32 %v97626, %v10
%v97633 = vadd.s32 5, %v97629
%v97635 = vxor.u32 %v97633, %v97621
%v97636 = vand.u32.u8 255, %v97635
%v97637 = vand.u32 65535, %v97636
%v97638 = vshrl.u32 %v97637, 1
%v97639 = vor.u32 16256, %v97638
%v97640 = vand.u32.u16 65535, %v97639
%v120268 = vadd.low.f32.bf16 -1.0, %v97640
%v97649 = vmul.f32 2.0, %v120268
%v97653 = vadd.f32 -0.99609375, %v97649
%v97657 = vmax.f32 %v97653, -0.99609375
%v97659 = vand.u32 2147483647, %v97657
%vm97662 = vcmp.eq.f32.partialorder %v97659, 1.0
%v97667 = vmul.f32 inf, %v97657
%v97669 = vxor.u32 2147483648, %v97657
%v97672 = vmul.f32 %v97669, %v97657
%v97674 = vadd.f32 1.0, %v97672
%v97675 = vlog2.pop %v97674
%v97676 = vmul.f32 0.6931472, %v97675
%v97677 = vmul.f32 -0.5, %v97672
%v97678 = vadd.f32 1.0, %v97677
%v97679 = vmul.f32 %v97678, %v97672
%v97680 = vand.u32 2147483647, %v97672
%vm97681 = vcmp.lt.f32.partialorder %v97680, 0.0004427343
%v97682 = vsel /*vm=*/%vm97681, /*on_true_vy=*/%v97679, /*on_false_vx=*/%v97676
%v97683 = vxor.u32 2147483648, %v97682
%vm97686 = vcmp.lt.f32.partialorder %v97683, 5.0
%v97691 = vsel /*vm=*/%vm97686, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v97695 = vsel /*vm=*/%vm97686, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v97699 = vsel /*vm=*/%vm97686, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v97703 = vsel /*vm=*/%vm97686, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v97707 = vsel /*vm=*/%vm97686, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v97711 = vsel /*vm=*/%vm97686, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v97715 = vsel /*vm=*/%vm97686, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v97719 = vsel /*vm=*/%vm97686, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v97723 = vsel /*vm=*/%vm97686, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v97727 = vadd.f32 -2.5, %v97683
%v97729 = vrsqrt.pop %v97683
%v97730 = vmul.f32 %v97729, %v97683
%vm97731 = vcmp.eq.f32.partialorder %v97683, inf
%v97732 = vsel /*vm=*/%vm97731, /*on_true_vy=*/%v97683, /*on_false_vx=*/%v97730
%vm97733 = vcmp.eq.f32.partialorder %v97683, 0.0
%v97734 = vand.u32 2147483648, %v97683
%v97735 = vsel /*vm=*/%vm97733, /*on_true_vy=*/%v97734, /*on_false_vx=*/%v97732
%v97738 = vadd.f32 -3.0, %v97735
%v97742 = vsel /*vm=*/%vm97686, /*on_true_vy=*/%v97727, /*on_false_vx=*/%v97738
%v97746 = vmul.f32 %v97742, %v97723
%v97750 = vadd.f32 %v97746, %v97719
%v97754 = vmul.f32 %v97750, %v97742
%v97758 = vadd.f32 %v97754, %v97715
%v97762 = vmul.f32 %v97758, %v97742
%v97766 = vadd.f32 %v97762, %v97711
%v97770 = vmul.f32 %v97766, %v97742
%v97774 = vadd.f32 %v97770, %v97707
%v97778 = vmul.f32 %v97774, %v97742
%v97782 = vadd.f32 %v97778, %v97703
%v97786 = vmul.f32 %v97782, %v97742
%v97790 = vadd.f32 %v97786, %v97699
%v97794 = vmul.f32 %v97790, %v97742
%v97798 = vadd.f32 %v97794, %v97695
%v97802 = vmul.f32 %v97798, %v97742
%v97806 = vadd.f32 %v97802, %v97691
%v97810 = vmul.f32 %v97806, %v97657
%v97814 = vsel /*vm=*/%vm97662, /*on_true_vy=*/%v97667, /*on_false_vx=*/%v97810
%v97818 = vmul.f32 1.4140625, %v97814
%v97821 = vpack.c.bf16 %v120417, %v97818
%120269 = vst [vmem:[%s280 + $0x68] sm:$0xf] /*vst_source=*/%v97821
%v97825 = vadd.s32 %v97361, %v894
%v97835 = vadd.s32 %v97825, %v415
%vm97839 = vcmp.lt.u32.totalorder %v97835, %v97825
%vm97844 = vcmp.lt.u32.totalorder %v97825, %v894
%v97849 = vadd.s32 %v97344, %v881
%v97853 = vadd.s32 1, %v97849
%v97857 = vsel /*vm=*/%vm97844, /*on_true_vy=*/%v97853, /*on_false_vx=*/%v97849
%v97861 = vadd.s32 1, %v97857
%v97865 = vsel /*vm=*/%vm97839, /*on_true_vy=*/%v97861, /*on_false_vx=*/%v97857
%v97870 = vadd.s32 %v97865, %v10
%v97874 = vadd.s32 %v97835, %v9
%v97878 = vadd.s32 %v97874, %v97870
%v97880 = vshll.u32 %v97874, 13
%v97881 = vshrl.u32 %v97874, 19
%v97882 = vor.u32 %v97881, %v97880
%v97883 = vxor.u32 %v97882, %v97878
%v97886 = vadd.s32 %v97883, %v97878
%v97888 = vshll.u32 %v97883, 15
%v97889 = vshrl.u32 %v97883, 17
%v97890 = vor.u32 %v97889, %v97888
%v97891 = vxor.u32 %v97890, %v97886
%v97894 = vadd.s32 %v97891, %v97886
%v97896 = vshll.u32 %v97891, 26
%v97897 = vshrl.u32 %v97891, 6
%v97898 = vor.u32 %v97897, %v97896
%v97899 = vxor.u32 %v97898, %v97894
%v97902 = vadd.s32 %v97899, %v97894
%v97906 = vadd.s32 %v97902, %v9
%v97908 = vshll.u32 %v97899, 6
%v97909 = vshrl.u32 %v97899, 26
%v97910 = vor.u32 %v97909, %v97908
%v97911 = vxor.u32 %v97910, %v97902
%v97914 = vadd.s32 %v97911, %v8
%v97918 = vadd.s32 1, %v97914
%v97922 = vadd.s32 %v97918, %v97906
%v97924 = vshll.u32 %v97918, 17
%v97925 = vshrl.u32 %v97918, 15
%v97926 = vor.u32 %v97925, %v97924
%v97927 = vxor.u32 %v97926, %v97922
%v97930 = vadd.s32 %v97927, %v97922
%v97932 = vshll.u32 %v97927, 29
%v97933 = vshrl.u32 %v97927, 3
%v97934 = vor.u32 %v97933, %v97932
%v97935 = vxor.u32 %v97934, %v97930
%v97938 = vadd.s32 %v97935, %v97930
%v97940 = vshll.u32 %v97935, 16
%v97941 = vshrl.u32 %v97935, 16
%v97942 = vor.u32 %v97941, %v97940
%v97943 = vxor.u32 %v97942, %v97938
%v97946 = vadd.s32 %v97943, %v97938
%v97950 = vadd.s32 %v97946, %v8
%v97952 = vshll.u32 %v97943, 24
%v97953 = vshrl.u32 %v97943, 8
%v97954 = vor.u32 %v97953, %v97952
%v97955 = vxor.u32 %v97954, %v97946
%v97958 = vadd.s32 %v97955, %v10
%v97962 = vadd.s32 2, %v97958
%v97966 = vadd.s32 %v97962, %v97950
%v97968 = vshll.u32 %v97962, 13
%v97969 = vshrl.u32 %v97962, 19
%v97970 = vor.u32 %v97969, %v97968
%v97971 = vxor.u32 %v97970, %v97966
%v97974 = vadd.s32 %v97971, %v97966
%v97976 = vshll.u32 %v97971, 15
%v97977 = vshrl.u32 %v97971, 17
%v97978 = vor.u32 %v97977, %v97976
%v97979 = vxor.u32 %v97978, %v97974
%v97982 = vadd.s32 %v97979, %v97974
%v97984 = vshll.u32 %v97979, 26
%v97985 = vshrl.u32 %v97979, 6
%v97986 = vor.u32 %v97985, %v97984
%v97987 = vxor.u32 %v97986, %v97982
%v97990 = vadd.s32 %v97987, %v97982
%v97994 = vadd.s32 %v97990, %v10
%v97996 = vshll.u32 %v97987, 6
%v97997 = vshrl.u32 %v97987, 26
%v97998 = vor.u32 %v97997, %v97996
%v97999 = vxor.u32 %v97998, %v97990
%v98002 = vadd.s32 %v97999, %v9
%v98006 = vadd.s32 3, %v98002
%v98010 = vadd.s32 %v98006, %v97994
%v98012 = vshll.u32 %v98006, 17
%v98013 = vshrl.u32 %v98006, 15
%v98014 = vor.u32 %v98013, %v98012
%v98015 = vxor.u32 %v98014, %v98010
%v98018 = vadd.s32 %v98015, %v98010
%v98020 = vshll.u32 %v98015, 29
%v98021 = vshrl.u32 %v98015, 3
%v98022 = vor.u32 %v98021, %v98020
%v98023 = vxor.u32 %v98022, %v98018
%v98026 = vadd.s32 %v98023, %v98018
%v98028 = vshll.u32 %v98023, 16
%v98029 = vshrl.u32 %v98023, 16
%v98030 = vor.u32 %v98029, %v98028
%v98031 = vxor.u32 %v98030, %v98026
%v98034 = vadd.s32 %v98031, %v98026
%v98038 = vadd.s32 %v98034, %v9
%v98040 = vshll.u32 %v98031, 24
%v98041 = vshrl.u32 %v98031, 8
%v98042 = vor.u32 %v98041, %v98040
%v98043 = vxor.u32 %v98042, %v98034
%v98046 = vadd.s32 %v98043, %v8
%v98050 = vadd.s32 4, %v98046
%v98054 = vadd.s32 %v98050, %v98038
%v98056 = vshll.u32 %v98050, 13
%v98057 = vshrl.u32 %v98050, 19
%v98058 = vor.u32 %v98057, %v98056
%v98059 = vxor.u32 %v98058, %v98054
%v98062 = vadd.s32 %v98059, %v98054
%v98064 = vshll.u32 %v98059, 15
%v98065 = vshrl.u32 %v98059, 17
%v98066 = vor.u32 %v98065, %v98064
%v98067 = vxor.u32 %v98066, %v98062
%v98070 = vadd.s32 %v98067, %v98062
%v98072 = vshll.u32 %v98067, 26
%v98073 = vshrl.u32 %v98067, 6
%v98074 = vor.u32 %v98073, %v98072
%v98075 = vxor.u32 %v98074, %v98070
%v98078 = vadd.s32 %v98075, %v98070
%v98082 = vadd.s32 %v98078, %v8
%v98084 = vshll.u32 %v98075, 6
%v98085 = vshrl.u32 %v98075, 26
%v98086 = vor.u32 %v98085, %v98084
%v98087 = vxor.u32 %v98086, %v98078
%v98090 = vadd.s32 %v98087, %v10
%v98094 = vadd.s32 5, %v98090
%v98096 = vxor.u32 %v98094, %v98082
%v98097 = vand.u32.u8 255, %v98096
%v98098 = vand.u32 65535, %v98097
%v98099 = vshrl.u32 %v98098, 1
%v98100 = vor.u32 16256, %v98099
%v98101 = vand.u32.u16 65535, %v98100
%v120270 = vadd.low.f32.bf16 -1.0, %v98101
%v98110 = vmul.f32 2.0, %v120270
%v98114 = vadd.f32 -0.99609375, %v98110
%v98118 = vmax.f32 %v98114, -0.99609375
%v98120 = vand.u32 2147483647, %v98118
%vm98123 = vcmp.eq.f32.partialorder %v98120, 1.0
%v98128 = vmul.f32 inf, %v98118
%v98130 = vxor.u32 2147483648, %v98118
%v98133 = vmul.f32 %v98130, %v98118
%v98135 = vadd.f32 1.0, %v98133
%v98136 = vlog2.pop %v98135
%v98137 = vmul.f32 0.6931472, %v98136
%v98138 = vmul.f32 -0.5, %v98133
%v98139 = vadd.f32 1.0, %v98138
%v98140 = vmul.f32 %v98139, %v98133
%v98141 = vand.u32 2147483647, %v98133
%vm98142 = vcmp.lt.f32.partialorder %v98141, 0.0004427343
%v98143 = vsel /*vm=*/%vm98142, /*on_true_vy=*/%v98140, /*on_false_vx=*/%v98137
%v98144 = vxor.u32 2147483648, %v98143
%vm98147 = vcmp.lt.f32.partialorder %v98144, 5.0
%v98152 = vsel /*vm=*/%vm98147, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v98156 = vsel /*vm=*/%vm98147, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v98160 = vsel /*vm=*/%vm98147, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v98164 = vsel /*vm=*/%vm98147, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v98168 = vsel /*vm=*/%vm98147, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v98172 = vsel /*vm=*/%vm98147, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v98176 = vsel /*vm=*/%vm98147, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v98180 = vsel /*vm=*/%vm98147, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v98184 = vsel /*vm=*/%vm98147, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v98188 = vadd.f32 -2.5, %v98144
%v98190 = vrsqrt.pop %v98144
%v98191 = vmul.f32 %v98190, %v98144
%vm98192 = vcmp.eq.f32.partialorder %v98144, inf
%v98193 = vsel /*vm=*/%vm98192, /*on_true_vy=*/%v98144, /*on_false_vx=*/%v98191
%vm98194 = vcmp.eq.f32.partialorder %v98144, 0.0
%v98195 = vand.u32 2147483648, %v98144
%v98196 = vsel /*vm=*/%vm98194, /*on_true_vy=*/%v98195, /*on_false_vx=*/%v98193
%v98199 = vadd.f32 -3.0, %v98196
%v98203 = vsel /*vm=*/%vm98147, /*on_true_vy=*/%v98188, /*on_false_vx=*/%v98199
%v98207 = vmul.f32 %v98203, %v98184
%v98211 = vadd.f32 %v98207, %v98180
%v98215 = vmul.f32 %v98211, %v98203
%v98219 = vadd.f32 %v98215, %v98176
%v98223 = vmul.f32 %v98219, %v98203
%v98227 = vadd.f32 %v98223, %v98172
%v98231 = vmul.f32 %v98227, %v98203
%v98235 = vadd.f32 %v98231, %v98168
%v98239 = vmul.f32 %v98235, %v98203
%v98243 = vadd.f32 %v98239, %v98164
%v98247 = vmul.f32 %v98243, %v98203
%v98251 = vadd.f32 %v98247, %v98160
%v98255 = vmul.f32 %v98251, %v98203
%v98259 = vadd.f32 %v98255, %v98156
%v98263 = vmul.f32 %v98259, %v98203
%v98267 = vadd.f32 %v98263, %v98152
%v98271 = vmul.f32 %v98267, %v98118
%v98275 = vsel /*vm=*/%vm98123, /*on_true_vy=*/%v98128, /*on_false_vx=*/%v98271
%v98279 = vmul.f32 1.4140625, %v98275
%v98282 = vpack.c.bf16 %v120417, %v98279
%120271 = vst [vmem:[%s280 + $0xe8] sm:$0xf] /*vst_source=*/%v98282
%v98286 = vadd.s32 %v97361, %v1381
%v98296 = vadd.s32 %v98286, %v415
%vm98300 = vcmp.lt.u32.totalorder %v98296, %v98286
%vm98305 = vcmp.lt.u32.totalorder %v98286, %v1381
%v98310 = vadd.s32 %v97344, %v1368
%v98314 = vadd.s32 1, %v98310
%v98318 = vsel /*vm=*/%vm98305, /*on_true_vy=*/%v98314, /*on_false_vx=*/%v98310
%v98322 = vadd.s32 1, %v98318
%v98326 = vsel /*vm=*/%vm98300, /*on_true_vy=*/%v98322, /*on_false_vx=*/%v98318
%v98331 = vadd.s32 %v98326, %v10
%v98335 = vadd.s32 %v98296, %v9
%v98339 = vadd.s32 %v98335, %v98331
%v98341 = vshll.u32 %v98335, 13
%v98342 = vshrl.u32 %v98335, 19
%v98343 = vor.u32 %v98342, %v98341
%v98344 = vxor.u32 %v98343, %v98339
%v98347 = vadd.s32 %v98344, %v98339
%v98349 = vshll.u32 %v98344, 15
%v98350 = vshrl.u32 %v98344, 17
%v98351 = vor.u32 %v98350, %v98349
%v98352 = vxor.u32 %v98351, %v98347
%v98355 = vadd.s32 %v98352, %v98347
%v98357 = vshll.u32 %v98352, 26
%v98358 = vshrl.u32 %v98352, 6
%v98359 = vor.u32 %v98358, %v98357
%v98360 = vxor.u32 %v98359, %v98355
%v98363 = vadd.s32 %v98360, %v98355
%v98367 = vadd.s32 %v98363, %v9
%v98369 = vshll.u32 %v98360, 6
%v98370 = vshrl.u32 %v98360, 26
%v98371 = vor.u32 %v98370, %v98369
%v98372 = vxor.u32 %v98371, %v98363
%v98375 = vadd.s32 %v98372, %v8
%v98379 = vadd.s32 1, %v98375
%v98383 = vadd.s32 %v98379, %v98367
%v98385 = vshll.u32 %v98379, 17
%v98386 = vshrl.u32 %v98379, 15
%v98387 = vor.u32 %v98386, %v98385
%v98388 = vxor.u32 %v98387, %v98383
%v98391 = vadd.s32 %v98388, %v98383
%v98393 = vshll.u32 %v98388, 29
%v98394 = vshrl.u32 %v98388, 3
%v98395 = vor.u32 %v98394, %v98393
%v98396 = vxor.u32 %v98395, %v98391
%v98399 = vadd.s32 %v98396, %v98391
%v98401 = vshll.u32 %v98396, 16
%v98402 = vshrl.u32 %v98396, 16
%v98403 = vor.u32 %v98402, %v98401
%v98404 = vxor.u32 %v98403, %v98399
%v98407 = vadd.s32 %v98404, %v98399
%v98411 = vadd.s32 %v98407, %v8
%v98413 = vshll.u32 %v98404, 24
%v98414 = vshrl.u32 %v98404, 8
%v98415 = vor.u32 %v98414, %v98413
%v98416 = vxor.u32 %v98415, %v98407
%v98419 = vadd.s32 %v98416, %v10
%v98423 = vadd.s32 2, %v98419
%v98427 = vadd.s32 %v98423, %v98411
%v98429 = vshll.u32 %v98423, 13
%v98430 = vshrl.u32 %v98423, 19
%v98431 = vor.u32 %v98430, %v98429
%v98432 = vxor.u32 %v98431, %v98427
%v98435 = vadd.s32 %v98432, %v98427
%v98437 = vshll.u32 %v98432, 15
%v98438 = vshrl.u32 %v98432, 17
%v98439 = vor.u32 %v98438, %v98437
%v98440 = vxor.u32 %v98439, %v98435
%v98443 = vadd.s32 %v98440, %v98435
%v98445 = vshll.u32 %v98440, 26
%v98446 = vshrl.u32 %v98440, 6
%v98447 = vor.u32 %v98446, %v98445
%v98448 = vxor.u32 %v98447, %v98443
%v98451 = vadd.s32 %v98448, %v98443
%v98455 = vadd.s32 %v98451, %v10
%v98457 = vshll.u32 %v98448, 6
%v98458 = vshrl.u32 %v98448, 26
%v98459 = vor.u32 %v98458, %v98457
%v98460 = vxor.u32 %v98459, %v98451
%v98463 = vadd.s32 %v98460, %v9
%v98467 = vadd.s32 3, %v98463
%v98471 = vadd.s32 %v98467, %v98455
%v98473 = vshll.u32 %v98467, 17
%v98474 = vshrl.u32 %v98467, 15
%v98475 = vor.u32 %v98474, %v98473
%v98476 = vxor.u32 %v98475, %v98471
%v98479 = vadd.s32 %v98476, %v98471
%v98481 = vshll.u32 %v98476, 29
%v98482 = vshrl.u32 %v98476, 3
%v98483 = vor.u32 %v98482, %v98481
%v98484 = vxor.u32 %v98483, %v98479
%v98487 = vadd.s32 %v98484, %v98479
%v98489 = vshll.u32 %v98484, 16
%v98490 = vshrl.u32 %v98484, 16
%v98491 = vor.u32 %v98490, %v98489
%v98492 = vxor.u32 %v98491, %v98487
%v98495 = vadd.s32 %v98492, %v98487
%v98499 = vadd.s32 %v98495, %v9
%v98501 = vshll.u32 %v98492, 24
%v98502 = vshrl.u32 %v98492, 8
%v98503 = vor.u32 %v98502, %v98501
%v98504 = vxor.u32 %v98503, %v98495
%v98507 = vadd.s32 %v98504, %v8
%v98511 = vadd.s32 4, %v98507
%v98515 = vadd.s32 %v98511, %v98499
%v98517 = vshll.u32 %v98511, 13
%v98518 = vshrl.u32 %v98511, 19
%v98519 = vor.u32 %v98518, %v98517
%v98520 = vxor.u32 %v98519, %v98515
%v98523 = vadd.s32 %v98520, %v98515
%v98525 = vshll.u32 %v98520, 15
%v98526 = vshrl.u32 %v98520, 17
%v98527 = vor.u32 %v98526, %v98525
%v98528 = vxor.u32 %v98527, %v98523
%v98531 = vadd.s32 %v98528, %v98523
%v98533 = vshll.u32 %v98528, 26
%v98534 = vshrl.u32 %v98528, 6
%v98535 = vor.u32 %v98534, %v98533
%v98536 = vxor.u32 %v98535, %v98531
%v98539 = vadd.s32 %v98536, %v98531
%v98543 = vadd.s32 %v98539, %v8
%v98545 = vshll.u32 %v98536, 6
%v98546 = vshrl.u32 %v98536, 26
%v98547 = vor.u32 %v98546, %v98545
%v98548 = vxor.u32 %v98547, %v98539
%v98551 = vadd.s32 %v98548, %v10
%v98555 = vadd.s32 5, %v98551
%v98557 = vxor.u32 %v98555, %v98543
%v98558 = vand.u32.u8 255, %v98557
%v98559 = vand.u32 65535, %v98558
%v98560 = vshrl.u32 %v98559, 1
%v98561 = vor.u32 16256, %v98560
%v98562 = vand.u32.u16 65535, %v98561
%v120272 = vadd.low.f32.bf16 -1.0, %v98562
%v98571 = vmul.f32 2.0, %v120272
%v98575 = vadd.f32 -0.99609375, %v98571
%v98579 = vmax.f32 %v98575, -0.99609375
%v98581 = vand.u32 2147483647, %v98579
%vm98584 = vcmp.eq.f32.partialorder %v98581, 1.0
%v98589 = vmul.f32 inf, %v98579
%v98591 = vxor.u32 2147483648, %v98579
%v98594 = vmul.f32 %v98591, %v98579
%v98596 = vadd.f32 1.0, %v98594
%v98597 = vlog2.pop %v98596
%v98598 = vmul.f32 0.6931472, %v98597
%v98599 = vmul.f32 -0.5, %v98594
%v98600 = vadd.f32 1.0, %v98599
%v98601 = vmul.f32 %v98600, %v98594
%v98602 = vand.u32 2147483647, %v98594
%vm98603 = vcmp.lt.f32.partialorder %v98602, 0.0004427343
%v98604 = vsel /*vm=*/%vm98603, /*on_true_vy=*/%v98601, /*on_false_vx=*/%v98598
%v98605 = vxor.u32 2147483648, %v98604
%vm98608 = vcmp.lt.f32.partialorder %v98605, 5.0
%v98613 = vsel /*vm=*/%vm98608, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v98617 = vsel /*vm=*/%vm98608, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v98621 = vsel /*vm=*/%vm98608, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v98625 = vsel /*vm=*/%vm98608, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v98629 = vsel /*vm=*/%vm98608, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v98633 = vsel /*vm=*/%vm98608, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v98637 = vsel /*vm=*/%vm98608, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v98641 = vsel /*vm=*/%vm98608, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v98645 = vsel /*vm=*/%vm98608, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v98649 = vadd.f32 -2.5, %v98605
%v98651 = vrsqrt.pop %v98605
%v98652 = vmul.f32 %v98651, %v98605
%vm98653 = vcmp.eq.f32.partialorder %v98605, inf
%v98654 = vsel /*vm=*/%vm98653, /*on_true_vy=*/%v98605, /*on_false_vx=*/%v98652
%vm98655 = vcmp.eq.f32.partialorder %v98605, 0.0
%v98656 = vand.u32 2147483648, %v98605
%v98657 = vsel /*vm=*/%vm98655, /*on_true_vy=*/%v98656, /*on_false_vx=*/%v98654
%v98660 = vadd.f32 -3.0, %v98657
%v98664 = vsel /*vm=*/%vm98608, /*on_true_vy=*/%v98649, /*on_false_vx=*/%v98660
%v98668 = vmul.f32 %v98664, %v98645
%v98672 = vadd.f32 %v98668, %v98641
%v98676 = vmul.f32 %v98672, %v98664
%v98680 = vadd.f32 %v98676, %v98637
%v98684 = vmul.f32 %v98680, %v98664
%v98688 = vadd.f32 %v98684, %v98633
%v98692 = vmul.f32 %v98688, %v98664
%v98696 = vadd.f32 %v98692, %v98629
%v98700 = vmul.f32 %v98696, %v98664
%v98704 = vadd.f32 %v98700, %v98625
%v98708 = vmul.f32 %v98704, %v98664
%v98712 = vadd.f32 %v98708, %v98621
%v98716 = vmul.f32 %v98712, %v98664
%v98720 = vadd.f32 %v98716, %v98617
%v98724 = vmul.f32 %v98720, %v98664
%v98728 = vadd.f32 %v98724, %v98613
%v98732 = vmul.f32 %v98728, %v98579
%v98736 = vsel /*vm=*/%vm98584, /*on_true_vy=*/%v98589, /*on_false_vx=*/%v98732
%v98740 = vmul.f32 1.4140625, %v98736
%v98743 = vpack.c.bf16 %v120417, %v98740
%120273 = vst [vmem:[%s280 + $0x168] sm:$0xf] /*vst_source=*/%v98743
%v98747 = vadd.s32 %v97361, %v1868
%v98757 = vadd.s32 %v98747, %v415
%vm98761 = vcmp.lt.u32.totalorder %v98757, %v98747
%vm98766 = vcmp.lt.u32.totalorder %v98747, %v1868
%v98771 = vadd.s32 %v97344, %v1855
%v98775 = vadd.s32 1, %v98771
%v98779 = vsel /*vm=*/%vm98766, /*on_true_vy=*/%v98775, /*on_false_vx=*/%v98771
%v98783 = vadd.s32 1, %v98779
%v98787 = vsel /*vm=*/%vm98761, /*on_true_vy=*/%v98783, /*on_false_vx=*/%v98779
%v98792 = vadd.s32 %v98787, %v10
%v98796 = vadd.s32 %v98757, %v9
%v98800 = vadd.s32 %v98796, %v98792
%v98802 = vshll.u32 %v98796, 13
%v98803 = vshrl.u32 %v98796, 19
%v98804 = vor.u32 %v98803, %v98802
%v98805 = vxor.u32 %v98804, %v98800
%v98808 = vadd.s32 %v98805, %v98800
%v98810 = vshll.u32 %v98805, 15
%v98811 = vshrl.u32 %v98805, 17
%v98812 = vor.u32 %v98811, %v98810
%v98813 = vxor.u32 %v98812, %v98808
%v98816 = vadd.s32 %v98813, %v98808
%v98818 = vshll.u32 %v98813, 26
%v98819 = vshrl.u32 %v98813, 6
%v98820 = vor.u32 %v98819, %v98818
%v98821 = vxor.u32 %v98820, %v98816
%v98824 = vadd.s32 %v98821, %v98816
%v98828 = vadd.s32 %v98824, %v9
%v98830 = vshll.u32 %v98821, 6
%v98831 = vshrl.u32 %v98821, 26
%v98832 = vor.u32 %v98831, %v98830
%v98833 = vxor.u32 %v98832, %v98824
%v98836 = vadd.s32 %v98833, %v8
%v98840 = vadd.s32 1, %v98836
%v98844 = vadd.s32 %v98840, %v98828
%v98846 = vshll.u32 %v98840, 17
%v98847 = vshrl.u32 %v98840, 15
%v98848 = vor.u32 %v98847, %v98846
%v98849 = vxor.u32 %v98848, %v98844
%v98852 = vadd.s32 %v98849, %v98844
%v98854 = vshll.u32 %v98849, 29
%v98855 = vshrl.u32 %v98849, 3
%v98856 = vor.u32 %v98855, %v98854
%v98857 = vxor.u32 %v98856, %v98852
%v98860 = vadd.s32 %v98857, %v98852
%v98862 = vshll.u32 %v98857, 16
%v98863 = vshrl.u32 %v98857, 16
%v98864 = vor.u32 %v98863, %v98862
%v98865 = vxor.u32 %v98864, %v98860
%v98868 = vadd.s32 %v98865, %v98860
%v98872 = vadd.s32 %v98868, %v8
%v98874 = vshll.u32 %v98865, 24
%v98875 = vshrl.u32 %v98865, 8
%v98876 = vor.u32 %v98875, %v98874
%v98877 = vxor.u32 %v98876, %v98868
%v98880 = vadd.s32 %v98877, %v10
%v98884 = vadd.s32 2, %v98880
%v98888 = vadd.s32 %v98884, %v98872
%v98890 = vshll.u32 %v98884, 13
%v98891 = vshrl.u32 %v98884, 19
%v98892 = vor.u32 %v98891, %v98890
%v98893 = vxor.u32 %v98892, %v98888
%v98896 = vadd.s32 %v98893, %v98888
%v98898 = vshll.u32 %v98893, 15
%v98899 = vshrl.u32 %v98893, 17
%v98900 = vor.u32 %v98899, %v98898
%v98901 = vxor.u32 %v98900, %v98896
%v98904 = vadd.s32 %v98901, %v98896
%v98906 = vshll.u32 %v98901, 26
%v98907 = vshrl.u32 %v98901, 6
%v98908 = vor.u32 %v98907, %v98906
%v98909 = vxor.u32 %v98908, %v98904
%v98912 = vadd.s32 %v98909, %v98904
%v98916 = vadd.s32 %v98912, %v10
%v98918 = vshll.u32 %v98909, 6
%v98919 = vshrl.u32 %v98909, 26
%v98920 = vor.u32 %v98919, %v98918
%v98921 = vxor.u32 %v98920, %v98912
%v98924 = vadd.s32 %v98921, %v9
%v98928 = vadd.s32 3, %v98924
%v98932 = vadd.s32 %v98928, %v98916
%v98934 = vshll.u32 %v98928, 17
%v98935 = vshrl.u32 %v98928, 15
%v98936 = vor.u32 %v98935, %v98934
%v98937 = vxor.u32 %v98936, %v98932
%v98940 = vadd.s32 %v98937, %v98932
%v98942 = vshll.u32 %v98937, 29
%v98943 = vshrl.u32 %v98937, 3
%v98944 = vor.u32 %v98943, %v98942
%v98945 = vxor.u32 %v98944, %v98940
%v98948 = vadd.s32 %v98945, %v98940
%v98950 = vshll.u32 %v98945, 16
%v98951 = vshrl.u32 %v98945, 16
%v98952 = vor.u32 %v98951, %v98950
%v98953 = vxor.u32 %v98952, %v98948
%v98956 = vadd.s32 %v98953, %v98948
%v98960 = vadd.s32 %v98956, %v9
%v98962 = vshll.u32 %v98953, 24
%v98963 = vshrl.u32 %v98953, 8
%v98964 = vor.u32 %v98963, %v98962
%v98965 = vxor.u32 %v98964, %v98956
%v98968 = vadd.s32 %v98965, %v8
%v98972 = vadd.s32 4, %v98968
%v98976 = vadd.s32 %v98972, %v98960
%v98978 = vshll.u32 %v98972, 13
%v98979 = vshrl.u32 %v98972, 19
%v98980 = vor.u32 %v98979, %v98978
%v98981 = vxor.u32 %v98980, %v98976
%v98984 = vadd.s32 %v98981, %v98976
%v98986 = vshll.u32 %v98981, 15
%v98987 = vshrl.u32 %v98981, 17
%v98988 = vor.u32 %v98987, %v98986
%v98989 = vxor.u32 %v98988, %v98984
%v98992 = vadd.s32 %v98989, %v98984
%v98994 = vshll.u32 %v98989, 26
%v98995 = vshrl.u32 %v98989, 6
%v98996 = vor.u32 %v98995, %v98994
%v98997 = vxor.u32 %v98996, %v98992
%v99000 = vadd.s32 %v98997, %v98992
%v99004 = vadd.s32 %v99000, %v8
%v99006 = vshll.u32 %v98997, 6
%v99007 = vshrl.u32 %v98997, 26
%v99008 = vor.u32 %v99007, %v99006
%v99009 = vxor.u32 %v99008, %v99000
%v99012 = vadd.s32 %v99009, %v10
%v99016 = vadd.s32 5, %v99012
%v99018 = vxor.u32 %v99016, %v99004
%v99019 = vand.u32.u8 255, %v99018
%v99020 = vand.u32 65535, %v99019
%v99021 = vshrl.u32 %v99020, 1
%v99022 = vor.u32 16256, %v99021
%v99023 = vand.u32.u16 65535, %v99022
%v120274 = vadd.low.f32.bf16 -1.0, %v99023
%v99032 = vmul.f32 2.0, %v120274
%v99036 = vadd.f32 -0.99609375, %v99032
%v99040 = vmax.f32 %v99036, -0.99609375
%v99042 = vand.u32 2147483647, %v99040
%vm99045 = vcmp.eq.f32.partialorder %v99042, 1.0
%v99050 = vmul.f32 inf, %v99040
%v99052 = vxor.u32 2147483648, %v99040
%v99055 = vmul.f32 %v99052, %v99040
%v99057 = vadd.f32 1.0, %v99055
%v99058 = vlog2.pop %v99057
%v99059 = vmul.f32 0.6931472, %v99058
%v99060 = vmul.f32 -0.5, %v99055
%v99061 = vadd.f32 1.0, %v99060
%v99062 = vmul.f32 %v99061, %v99055
%v99063 = vand.u32 2147483647, %v99055
%vm99064 = vcmp.lt.f32.partialorder %v99063, 0.0004427343
%v99065 = vsel /*vm=*/%vm99064, /*on_true_vy=*/%v99062, /*on_false_vx=*/%v99059
%v99066 = vxor.u32 2147483648, %v99065
%vm99069 = vcmp.lt.f32.partialorder %v99066, 5.0
%v99074 = vsel /*vm=*/%vm99069, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v99078 = vsel /*vm=*/%vm99069, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v99082 = vsel /*vm=*/%vm99069, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v99086 = vsel /*vm=*/%vm99069, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v99090 = vsel /*vm=*/%vm99069, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v99094 = vsel /*vm=*/%vm99069, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v99098 = vsel /*vm=*/%vm99069, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v99102 = vsel /*vm=*/%vm99069, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v99106 = vsel /*vm=*/%vm99069, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v99110 = vadd.f32 -2.5, %v99066
%v99112 = vrsqrt.pop %v99066
%v99113 = vmul.f32 %v99112, %v99066
%vm99114 = vcmp.eq.f32.partialorder %v99066, inf
%v99115 = vsel /*vm=*/%vm99114, /*on_true_vy=*/%v99066, /*on_false_vx=*/%v99113
%vm99116 = vcmp.eq.f32.partialorder %v99066, 0.0
%v99117 = vand.u32 2147483648, %v99066
%v99118 = vsel /*vm=*/%vm99116, /*on_true_vy=*/%v99117, /*on_false_vx=*/%v99115
%v99121 = vadd.f32 -3.0, %v99118
%v99125 = vsel /*vm=*/%vm99069, /*on_true_vy=*/%v99110, /*on_false_vx=*/%v99121
%v99129 = vmul.f32 %v99125, %v99106
%v99133 = vadd.f32 %v99129, %v99102
%v99137 = vmul.f32 %v99133, %v99125
%v99141 = vadd.f32 %v99137, %v99098
%v99145 = vmul.f32 %v99141, %v99125
%v99149 = vadd.f32 %v99145, %v99094
%v99153 = vmul.f32 %v99149, %v99125
%v99157 = vadd.f32 %v99153, %v99090
%v99161 = vmul.f32 %v99157, %v99125
%v99165 = vadd.f32 %v99161, %v99086
%v99169 = vmul.f32 %v99165, %v99125
%v99173 = vadd.f32 %v99169, %v99082
%v99177 = vmul.f32 %v99173, %v99125
%v99181 = vadd.f32 %v99177, %v99078
%v99185 = vmul.f32 %v99181, %v99125
%v99189 = vadd.f32 %v99185, %v99074
%v99193 = vmul.f32 %v99189, %v99040
%v99197 = vsel /*vm=*/%vm99045, /*on_true_vy=*/%v99050, /*on_false_vx=*/%v99193
%v99201 = vmul.f32 1.4140625, %v99197
%v99204 = vpack.c.bf16 %v120417, %v99201
%120275 = vst [vmem:[%s280 + $0x1e8] sm:$0xf] /*vst_source=*/%v99204
%v99208 = vadd.s32 %v97361, %v2355
%v99218 = vadd.s32 %v99208, %v415
%vm99222 = vcmp.lt.u32.totalorder %v99218, %v99208
%vm99227 = vcmp.lt.u32.totalorder %v99208, %v2355
%v99232 = vadd.s32 %v97344, %v2342
%v99236 = vadd.s32 1, %v99232
%v99240 = vsel /*vm=*/%vm99227, /*on_true_vy=*/%v99236, /*on_false_vx=*/%v99232
%v99244 = vadd.s32 1, %v99240
%v99248 = vsel /*vm=*/%vm99222, /*on_true_vy=*/%v99244, /*on_false_vx=*/%v99240
%v99253 = vadd.s32 %v99248, %v10
%v99257 = vadd.s32 %v99218, %v9
%v99261 = vadd.s32 %v99257, %v99253
%v99263 = vshll.u32 %v99257, 13
%v99264 = vshrl.u32 %v99257, 19
%v99265 = vor.u32 %v99264, %v99263
%v99266 = vxor.u32 %v99265, %v99261
%v99269 = vadd.s32 %v99266, %v99261
%v99271 = vshll.u32 %v99266, 15
%v99272 = vshrl.u32 %v99266, 17
%v99273 = vor.u32 %v99272, %v99271
%v99274 = vxor.u32 %v99273, %v99269
%v99277 = vadd.s32 %v99274, %v99269
%v99279 = vshll.u32 %v99274, 26
%v99280 = vshrl.u32 %v99274, 6
%v99281 = vor.u32 %v99280, %v99279
%v99282 = vxor.u32 %v99281, %v99277
%v99285 = vadd.s32 %v99282, %v99277
%v99289 = vadd.s32 %v99285, %v9
%v99291 = vshll.u32 %v99282, 6
%v99292 = vshrl.u32 %v99282, 26
%v99293 = vor.u32 %v99292, %v99291
%v99294 = vxor.u32 %v99293, %v99285
%v99297 = vadd.s32 %v99294, %v8
%v99301 = vadd.s32 1, %v99297
%v99305 = vadd.s32 %v99301, %v99289
%v99307 = vshll.u32 %v99301, 17
%v99308 = vshrl.u32 %v99301, 15
%v99309 = vor.u32 %v99308, %v99307
%v99310 = vxor.u32 %v99309, %v99305
%v99313 = vadd.s32 %v99310, %v99305
%v99315 = vshll.u32 %v99310, 29
%v99316 = vshrl.u32 %v99310, 3
%v99317 = vor.u32 %v99316, %v99315
%v99318 = vxor.u32 %v99317, %v99313
%v99321 = vadd.s32 %v99318, %v99313
%v99323 = vshll.u32 %v99318, 16
%v99324 = vshrl.u32 %v99318, 16
%v99325 = vor.u32 %v99324, %v99323
%v99326 = vxor.u32 %v99325, %v99321
%v99329 = vadd.s32 %v99326, %v99321
%v99333 = vadd.s32 %v99329, %v8
%v99335 = vshll.u32 %v99326, 24
%v99336 = vshrl.u32 %v99326, 8
%v99337 = vor.u32 %v99336, %v99335
%v99338 = vxor.u32 %v99337, %v99329
%v99341 = vadd.s32 %v99338, %v10
%v99345 = vadd.s32 2, %v99341
%v99349 = vadd.s32 %v99345, %v99333
%v99351 = vshll.u32 %v99345, 13
%v99352 = vshrl.u32 %v99345, 19
%v99353 = vor.u32 %v99352, %v99351
%v99354 = vxor.u32 %v99353, %v99349
%v99357 = vadd.s32 %v99354, %v99349
%v99359 = vshll.u32 %v99354, 15
%v99360 = vshrl.u32 %v99354, 17
%v99361 = vor.u32 %v99360, %v99359
%v99362 = vxor.u32 %v99361, %v99357
%v99365 = vadd.s32 %v99362, %v99357
%v99367 = vshll.u32 %v99362, 26
%v99368 = vshrl.u32 %v99362, 6
%v99369 = vor.u32 %v99368, %v99367
%v99370 = vxor.u32 %v99369, %v99365
%v99373 = vadd.s32 %v99370, %v99365
%v99377 = vadd.s32 %v99373, %v10
%v99379 = vshll.u32 %v99370, 6
%v99380 = vshrl.u32 %v99370, 26
%v99381 = vor.u32 %v99380, %v99379
%v99382 = vxor.u32 %v99381, %v99373
%v99385 = vadd.s32 %v99382, %v9
%v99389 = vadd.s32 3, %v99385
%v99393 = vadd.s32 %v99389, %v99377
%v99395 = vshll.u32 %v99389, 17
%v99396 = vshrl.u32 %v99389, 15
%v99397 = vor.u32 %v99396, %v99395
%v99398 = vxor.u32 %v99397, %v99393
%v99401 = vadd.s32 %v99398, %v99393
%v99403 = vshll.u32 %v99398, 29
%v99404 = vshrl.u32 %v99398, 3
%v99405 = vor.u32 %v99404, %v99403
%v99406 = vxor.u32 %v99405, %v99401
%v99409 = vadd.s32 %v99406, %v99401
%v99411 = vshll.u32 %v99406, 16
%v99412 = vshrl.u32 %v99406, 16
%v99413 = vor.u32 %v99412, %v99411
%v99414 = vxor.u32 %v99413, %v99409
%v99417 = vadd.s32 %v99414, %v99409
%v99421 = vadd.s32 %v99417, %v9
%v99423 = vshll.u32 %v99414, 24
%v99424 = vshrl.u32 %v99414, 8
%v99425 = vor.u32 %v99424, %v99423
%v99426 = vxor.u32 %v99425, %v99417
%v99429 = vadd.s32 %v99426, %v8
%v99433 = vadd.s32 4, %v99429
%v99437 = vadd.s32 %v99433, %v99421
%v99439 = vshll.u32 %v99433, 13
%v99440 = vshrl.u32 %v99433, 19
%v99441 = vor.u32 %v99440, %v99439
%v99442 = vxor.u32 %v99441, %v99437
%v99445 = vadd.s32 %v99442, %v99437
%v99447 = vshll.u32 %v99442, 15
%v99448 = vshrl.u32 %v99442, 17
%v99449 = vor.u32 %v99448, %v99447
%v99450 = vxor.u32 %v99449, %v99445
%v99453 = vadd.s32 %v99450, %v99445
%v99455 = vshll.u32 %v99450, 26
%v99456 = vshrl.u32 %v99450, 6
%v99457 = vor.u32 %v99456, %v99455
%v99458 = vxor.u32 %v99457, %v99453
%v99461 = vadd.s32 %v99458, %v99453
%v99465 = vadd.s32 %v99461, %v8
%v99467 = vshll.u32 %v99458, 6
%v99468 = vshrl.u32 %v99458, 26
%v99469 = vor.u32 %v99468, %v99467
%v99470 = vxor.u32 %v99469, %v99461
%v99473 = vadd.s32 %v99470, %v10
%v99477 = vadd.s32 5, %v99473
%v99479 = vxor.u32 %v99477, %v99465
%v99480 = vand.u32.u8 255, %v99479
%v99481 = vand.u32 65535, %v99480
%v99482 = vshrl.u32 %v99481, 1
%v99483 = vor.u32 16256, %v99482
%v99484 = vand.u32.u16 65535, %v99483
%v120276 = vadd.low.f32.bf16 -1.0, %v99484
%v99493 = vmul.f32 2.0, %v120276
%v99497 = vadd.f32 -0.99609375, %v99493
%v99501 = vmax.f32 %v99497, -0.99609375
%v99503 = vand.u32 2147483647, %v99501
%vm99506 = vcmp.eq.f32.partialorder %v99503, 1.0
%v99511 = vmul.f32 inf, %v99501
%v99513 = vxor.u32 2147483648, %v99501
%v99516 = vmul.f32 %v99513, %v99501
%v99518 = vadd.f32 1.0, %v99516
%v99519 = vlog2.pop %v99518
%v99520 = vmul.f32 0.6931472, %v99519
%v99521 = vmul.f32 -0.5, %v99516
%v99522 = vadd.f32 1.0, %v99521
%v99523 = vmul.f32 %v99522, %v99516
%v99524 = vand.u32 2147483647, %v99516
%vm99525 = vcmp.lt.f32.partialorder %v99524, 0.0004427343
%v99526 = vsel /*vm=*/%vm99525, /*on_true_vy=*/%v99523, /*on_false_vx=*/%v99520
%v99527 = vxor.u32 2147483648, %v99526
%vm99530 = vcmp.lt.f32.partialorder %v99527, 5.0
%v99535 = vsel /*vm=*/%vm99530, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v99539 = vsel /*vm=*/%vm99530, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v99543 = vsel /*vm=*/%vm99530, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v99547 = vsel /*vm=*/%vm99530, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v99551 = vsel /*vm=*/%vm99530, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v99555 = vsel /*vm=*/%vm99530, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v99559 = vsel /*vm=*/%vm99530, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v99563 = vsel /*vm=*/%vm99530, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v99567 = vsel /*vm=*/%vm99530, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v99571 = vadd.f32 -2.5, %v99527
%v99573 = vrsqrt.pop %v99527
%v99574 = vmul.f32 %v99573, %v99527
%vm99575 = vcmp.eq.f32.partialorder %v99527, inf
%v99576 = vsel /*vm=*/%vm99575, /*on_true_vy=*/%v99527, /*on_false_vx=*/%v99574
%vm99577 = vcmp.eq.f32.partialorder %v99527, 0.0
%v99578 = vand.u32 2147483648, %v99527
%v99579 = vsel /*vm=*/%vm99577, /*on_true_vy=*/%v99578, /*on_false_vx=*/%v99576
%v99582 = vadd.f32 -3.0, %v99579
%v99586 = vsel /*vm=*/%vm99530, /*on_true_vy=*/%v99571, /*on_false_vx=*/%v99582
%v99590 = vmul.f32 %v99586, %v99567
%v99594 = vadd.f32 %v99590, %v99563
%v99598 = vmul.f32 %v99594, %v99586
%v99602 = vadd.f32 %v99598, %v99559
%v99606 = vmul.f32 %v99602, %v99586
%v99610 = vadd.f32 %v99606, %v99555
%v99614 = vmul.f32 %v99610, %v99586
%v99618 = vadd.f32 %v99614, %v99551
%v99622 = vmul.f32 %v99618, %v99586
%v99626 = vadd.f32 %v99622, %v99547
%v99630 = vmul.f32 %v99626, %v99586
%v99634 = vadd.f32 %v99630, %v99543
%v99638 = vmul.f32 %v99634, %v99586
%v99642 = vadd.f32 %v99638, %v99539
%v99646 = vmul.f32 %v99642, %v99586
%v99650 = vadd.f32 %v99646, %v99535
%v99654 = vmul.f32 %v99650, %v99501
%v99658 = vsel /*vm=*/%vm99506, /*on_true_vy=*/%v99511, /*on_false_vx=*/%v99654
%v99662 = vmul.f32 1.4140625, %v99658
%v99665 = vpack.c.bf16 %v120417, %v99662
%120277 = vst [vmem:[%s280 + $0x268] sm:$0xf] /*vst_source=*/%v99665
%v99669 = vadd.s32 %v97361, %v2842
%v99679 = vadd.s32 %v99669, %v415
%vm99683 = vcmp.lt.u32.totalorder %v99679, %v99669
%vm99688 = vcmp.lt.u32.totalorder %v99669, %v2842
%v99693 = vadd.s32 %v97344, %v2829
%v99697 = vadd.s32 1, %v99693
%v99701 = vsel /*vm=*/%vm99688, /*on_true_vy=*/%v99697, /*on_false_vx=*/%v99693
%v99705 = vadd.s32 1, %v99701
%v99709 = vsel /*vm=*/%vm99683, /*on_true_vy=*/%v99705, /*on_false_vx=*/%v99701
%v99714 = vadd.s32 %v99709, %v10
%v99718 = vadd.s32 %v99679, %v9
%v99722 = vadd.s32 %v99718, %v99714
%v99724 = vshll.u32 %v99718, 13
%v99725 = vshrl.u32 %v99718, 19
%v99726 = vor.u32 %v99725, %v99724
%v99727 = vxor.u32 %v99726, %v99722
%v99730 = vadd.s32 %v99727, %v99722
%v99732 = vshll.u32 %v99727, 15
%v99733 = vshrl.u32 %v99727, 17
%v99734 = vor.u32 %v99733, %v99732
%v99735 = vxor.u32 %v99734, %v99730
%v99738 = vadd.s32 %v99735, %v99730
%v99740 = vshll.u32 %v99735, 26
%v99741 = vshrl.u32 %v99735, 6
%v99742 = vor.u32 %v99741, %v99740
%v99743 = vxor.u32 %v99742, %v99738
%v99746 = vadd.s32 %v99743, %v99738
%v99750 = vadd.s32 %v99746, %v9
%v99752 = vshll.u32 %v99743, 6
%v99753 = vshrl.u32 %v99743, 26
%v99754 = vor.u32 %v99753, %v99752
%v99755 = vxor.u32 %v99754, %v99746
%v99758 = vadd.s32 %v99755, %v8
%v99762 = vadd.s32 1, %v99758
%v99766 = vadd.s32 %v99762, %v99750
%v99768 = vshll.u32 %v99762, 17
%v99769 = vshrl.u32 %v99762, 15
%v99770 = vor.u32 %v99769, %v99768
%v99771 = vxor.u32 %v99770, %v99766
%v99774 = vadd.s32 %v99771, %v99766
%v99776 = vshll.u32 %v99771, 29
%v99777 = vshrl.u32 %v99771, 3
%v99778 = vor.u32 %v99777, %v99776
%v99779 = vxor.u32 %v99778, %v99774
%v99782 = vadd.s32 %v99779, %v99774
%v99784 = vshll.u32 %v99779, 16
%v99785 = vshrl.u32 %v99779, 16
%v99786 = vor.u32 %v99785, %v99784
%v99787 = vxor.u32 %v99786, %v99782
%v99790 = vadd.s32 %v99787, %v99782
%v99794 = vadd.s32 %v99790, %v8
%v99796 = vshll.u32 %v99787, 24
%v99797 = vshrl.u32 %v99787, 8
%v99798 = vor.u32 %v99797, %v99796
%v99799 = vxor.u32 %v99798, %v99790
%v99802 = vadd.s32 %v99799, %v10
%v99806 = vadd.s32 2, %v99802
%v99810 = vadd.s32 %v99806, %v99794
%v99812 = vshll.u32 %v99806, 13
%v99813 = vshrl.u32 %v99806, 19
%v99814 = vor.u32 %v99813, %v99812
%v99815 = vxor.u32 %v99814, %v99810
%v99818 = vadd.s32 %v99815, %v99810
%v99820 = vshll.u32 %v99815, 15
%v99821 = vshrl.u32 %v99815, 17
%v99822 = vor.u32 %v99821, %v99820
%v99823 = vxor.u32 %v99822, %v99818
%v99826 = vadd.s32 %v99823, %v99818
%v99828 = vshll.u32 %v99823, 26
%v99829 = vshrl.u32 %v99823, 6
%v99830 = vor.u32 %v99829, %v99828
%v99831 = vxor.u32 %v99830, %v99826
%v99834 = vadd.s32 %v99831, %v99826
%v99838 = vadd.s32 %v99834, %v10
%v99840 = vshll.u32 %v99831, 6
%v99841 = vshrl.u32 %v99831, 26
%v99842 = vor.u32 %v99841, %v99840
%v99843 = vxor.u32 %v99842, %v99834
%v99846 = vadd.s32 %v99843, %v9
%v99850 = vadd.s32 3, %v99846
%v99854 = vadd.s32 %v99850, %v99838
%v99856 = vshll.u32 %v99850, 17
%v99857 = vshrl.u32 %v99850, 15
%v99858 = vor.u32 %v99857, %v99856
%v99859 = vxor.u32 %v99858, %v99854
%v99862 = vadd.s32 %v99859, %v99854
%v99864 = vshll.u32 %v99859, 29
%v99865 = vshrl.u32 %v99859, 3
%v99866 = vor.u32 %v99865, %v99864
%v99867 = vxor.u32 %v99866, %v99862
%v99870 = vadd.s32 %v99867, %v99862
%v99872 = vshll.u32 %v99867, 16
%v99873 = vshrl.u32 %v99867, 16
%v99874 = vor.u32 %v99873, %v99872
%v99875 = vxor.u32 %v99874, %v99870
%v99878 = vadd.s32 %v99875, %v99870
%v99882 = vadd.s32 %v99878, %v9
%v99884 = vshll.u32 %v99875, 24
%v99885 = vshrl.u32 %v99875, 8
%v99886 = vor.u32 %v99885, %v99884
%v99887 = vxor.u32 %v99886, %v99878
%v99890 = vadd.s32 %v99887, %v8
%v99894 = vadd.s32 4, %v99890
%v99898 = vadd.s32 %v99894, %v99882
%v99900 = vshll.u32 %v99894, 13
%v99901 = vshrl.u32 %v99894, 19
%v99902 = vor.u32 %v99901, %v99900
%v99903 = vxor.u32 %v99902, %v99898
%v99906 = vadd.s32 %v99903, %v99898
%v99908 = vshll.u32 %v99903, 15
%v99909 = vshrl.u32 %v99903, 17
%v99910 = vor.u32 %v99909, %v99908
%v99911 = vxor.u32 %v99910, %v99906
%v99914 = vadd.s32 %v99911, %v99906
%v99916 = vshll.u32 %v99911, 26
%v99917 = vshrl.u32 %v99911, 6
%v99918 = vor.u32 %v99917, %v99916
%v99919 = vxor.u32 %v99918, %v99914
%v99922 = vadd.s32 %v99919, %v99914
%v99926 = vadd.s32 %v99922, %v8
%v99928 = vshll.u32 %v99919, 6
%v99929 = vshrl.u32 %v99919, 26
%v99930 = vor.u32 %v99929, %v99928
%v99931 = vxor.u32 %v99930, %v99922
%v99934 = vadd.s32 %v99931, %v10
%v99938 = vadd.s32 5, %v99934
%v99940 = vxor.u32 %v99938, %v99926
%v99941 = vand.u32.u8 255, %v99940
%v99942 = vand.u32 65535, %v99941
%v99943 = vshrl.u32 %v99942, 1
%v99944 = vor.u32 16256, %v99943
%v99945 = vand.u32.u16 65535, %v99944
%v120278 = vadd.low.f32.bf16 -1.0, %v99945
%v99954 = vmul.f32 2.0, %v120278
%v99958 = vadd.f32 -0.99609375, %v99954
%v99962 = vmax.f32 %v99958, -0.99609375
%v99964 = vand.u32 2147483647, %v99962
%vm99967 = vcmp.eq.f32.partialorder %v99964, 1.0
%v99972 = vmul.f32 inf, %v99962
%v99974 = vxor.u32 2147483648, %v99962
%v99977 = vmul.f32 %v99974, %v99962
%v99979 = vadd.f32 1.0, %v99977
%v99980 = vlog2.pop %v99979
%v99981 = vmul.f32 0.6931472, %v99980
%v99982 = vmul.f32 -0.5, %v99977
%v99983 = vadd.f32 1.0, %v99982
%v99984 = vmul.f32 %v99983, %v99977
%v99985 = vand.u32 2147483647, %v99977
%vm99986 = vcmp.lt.f32.partialorder %v99985, 0.0004427343
%v99987 = vsel /*vm=*/%vm99986, /*on_true_vy=*/%v99984, /*on_false_vx=*/%v99981
%v99988 = vxor.u32 2147483648, %v99987
%vm99991 = vcmp.lt.f32.partialorder %v99988, 5.0
%v99996 = vsel /*vm=*/%vm99991, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v100000 = vsel /*vm=*/%vm99991, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v100004 = vsel /*vm=*/%vm99991, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v100008 = vsel /*vm=*/%vm99991, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v100012 = vsel /*vm=*/%vm99991, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v100016 = vsel /*vm=*/%vm99991, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v100020 = vsel /*vm=*/%vm99991, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v100024 = vsel /*vm=*/%vm99991, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v100028 = vsel /*vm=*/%vm99991, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v100032 = vadd.f32 -2.5, %v99988
%v100034 = vrsqrt.pop %v99988
%v100035 = vmul.f32 %v100034, %v99988
%vm100036 = vcmp.eq.f32.partialorder %v99988, inf
%v100037 = vsel /*vm=*/%vm100036, /*on_true_vy=*/%v99988, /*on_false_vx=*/%v100035
%vm100038 = vcmp.eq.f32.partialorder %v99988, 0.0
%v100039 = vand.u32 2147483648, %v99988
%v100040 = vsel /*vm=*/%vm100038, /*on_true_vy=*/%v100039, /*on_false_vx=*/%v100037
%v100043 = vadd.f32 -3.0, %v100040
%v100047 = vsel /*vm=*/%vm99991, /*on_true_vy=*/%v100032, /*on_false_vx=*/%v100043
%v100051 = vmul.f32 %v100047, %v100028
%v100055 = vadd.f32 %v100051, %v100024
%v100059 = vmul.f32 %v100055, %v100047
%v100063 = vadd.f32 %v100059, %v100020
%v100067 = vmul.f32 %v100063, %v100047
%v100071 = vadd.f32 %v100067, %v100016
%v100075 = vmul.f32 %v100071, %v100047
%v100079 = vadd.f32 %v100075, %v100012
%v100083 = vmul.f32 %v100079, %v100047
%v100087 = vadd.f32 %v100083, %v100008
%v100091 = vmul.f32 %v100087, %v100047
%v100095 = vadd.f32 %v100091, %v100004
%v100099 = vmul.f32 %v100095, %v100047
%v100103 = vadd.f32 %v100099, %v100000
%v100107 = vmul.f32 %v100103, %v100047
%v100111 = vadd.f32 %v100107, %v99996
%v100115 = vmul.f32 %v100111, %v99962
%v100119 = vsel /*vm=*/%vm99967, /*on_true_vy=*/%v99972, /*on_false_vx=*/%v100115
%v100123 = vmul.f32 1.4140625, %v100119
%v100126 = vpack.c.bf16 %v120417, %v100123
%120279 = vst [vmem:[%s280 + $0x2e8] sm:$0xf] /*vst_source=*/%v100126
%v100130 = vadd.s32 %v97361, %v3329
%v100140 = vadd.s32 %v100130, %v415
%vm100144 = vcmp.lt.u32.totalorder %v100140, %v100130
%vm100149 = vcmp.lt.u32.totalorder %v100130, %v3329
%v100154 = vadd.s32 %v97344, %v3316
%v100158 = vadd.s32 1, %v100154
%v100162 = vsel /*vm=*/%vm100149, /*on_true_vy=*/%v100158, /*on_false_vx=*/%v100154
%v100166 = vadd.s32 1, %v100162
%v100170 = vsel /*vm=*/%vm100144, /*on_true_vy=*/%v100166, /*on_false_vx=*/%v100162
%v100175 = vadd.s32 %v100170, %v10
%v100179 = vadd.s32 %v100140, %v9
%v100183 = vadd.s32 %v100179, %v100175
%v100185 = vshll.u32 %v100179, 13
%v100186 = vshrl.u32 %v100179, 19
%v100187 = vor.u32 %v100186, %v100185
%v100188 = vxor.u32 %v100187, %v100183
%v100191 = vadd.s32 %v100188, %v100183
%v100193 = vshll.u32 %v100188, 15
%v100194 = vshrl.u32 %v100188, 17
%v100195 = vor.u32 %v100194, %v100193
%v100196 = vxor.u32 %v100195, %v100191
%v100199 = vadd.s32 %v100196, %v100191
%v100201 = vshll.u32 %v100196, 26
%v100202 = vshrl.u32 %v100196, 6
%v100203 = vor.u32 %v100202, %v100201
%v100204 = vxor.u32 %v100203, %v100199
%v100207 = vadd.s32 %v100204, %v100199
%v100211 = vadd.s32 %v100207, %v9
%v100213 = vshll.u32 %v100204, 6
%v100214 = vshrl.u32 %v100204, 26
%v100215 = vor.u32 %v100214, %v100213
%v100216 = vxor.u32 %v100215, %v100207
%v100219 = vadd.s32 %v100216, %v8
%v100223 = vadd.s32 1, %v100219
%v100227 = vadd.s32 %v100223, %v100211
%v100229 = vshll.u32 %v100223, 17
%v100230 = vshrl.u32 %v100223, 15
%v100231 = vor.u32 %v100230, %v100229
%v100232 = vxor.u32 %v100231, %v100227
%v100235 = vadd.s32 %v100232, %v100227
%v100237 = vshll.u32 %v100232, 29
%v100238 = vshrl.u32 %v100232, 3
%v100239 = vor.u32 %v100238, %v100237
%v100240 = vxor.u32 %v100239, %v100235
%v100243 = vadd.s32 %v100240, %v100235
%v100245 = vshll.u32 %v100240, 16
%v100246 = vshrl.u32 %v100240, 16
%v100247 = vor.u32 %v100246, %v100245
%v100248 = vxor.u32 %v100247, %v100243
%v100251 = vadd.s32 %v100248, %v100243
%v100255 = vadd.s32 %v100251, %v8
%v100257 = vshll.u32 %v100248, 24
%v100258 = vshrl.u32 %v100248, 8
%v100259 = vor.u32 %v100258, %v100257
%v100260 = vxor.u32 %v100259, %v100251
%v100263 = vadd.s32 %v100260, %v10
%v100267 = vadd.s32 2, %v100263
%v100271 = vadd.s32 %v100267, %v100255
%v100273 = vshll.u32 %v100267, 13
%v100274 = vshrl.u32 %v100267, 19
%v100275 = vor.u32 %v100274, %v100273
%v100276 = vxor.u32 %v100275, %v100271
%v100279 = vadd.s32 %v100276, %v100271
%v100281 = vshll.u32 %v100276, 15
%v100282 = vshrl.u32 %v100276, 17
%v100283 = vor.u32 %v100282, %v100281
%v100284 = vxor.u32 %v100283, %v100279
%v100287 = vadd.s32 %v100284, %v100279
%v100289 = vshll.u32 %v100284, 26
%v100290 = vshrl.u32 %v100284, 6
%v100291 = vor.u32 %v100290, %v100289
%v100292 = vxor.u32 %v100291, %v100287
%v100295 = vadd.s32 %v100292, %v100287
%v100299 = vadd.s32 %v100295, %v10
%v100301 = vshll.u32 %v100292, 6
%v100302 = vshrl.u32 %v100292, 26
%v100303 = vor.u32 %v100302, %v100301
%v100304 = vxor.u32 %v100303, %v100295
%v100307 = vadd.s32 %v100304, %v9
%v100311 = vadd.s32 3, %v100307
%v100315 = vadd.s32 %v100311, %v100299
%v100317 = vshll.u32 %v100311, 17
%v100318 = vshrl.u32 %v100311, 15
%v100319 = vor.u32 %v100318, %v100317
%v100320 = vxor.u32 %v100319, %v100315
%v100323 = vadd.s32 %v100320, %v100315
%v100325 = vshll.u32 %v100320, 29
%v100326 = vshrl.u32 %v100320, 3
%v100327 = vor.u32 %v100326, %v100325
%v100328 = vxor.u32 %v100327, %v100323
%v100331 = vadd.s32 %v100328, %v100323
%v100333 = vshll.u32 %v100328, 16
%v100334 = vshrl.u32 %v100328, 16
%v100335 = vor.u32 %v100334, %v100333
%v100336 = vxor.u32 %v100335, %v100331
%v100339 = vadd.s32 %v100336, %v100331
%v100343 = vadd.s32 %v100339, %v9
%v100345 = vshll.u32 %v100336, 24
%v100346 = vshrl.u32 %v100336, 8
%v100347 = vor.u32 %v100346, %v100345
%v100348 = vxor.u32 %v100347, %v100339
%v100351 = vadd.s32 %v100348, %v8
%v100355 = vadd.s32 4, %v100351
%v100359 = vadd.s32 %v100355, %v100343
%v100361 = vshll.u32 %v100355, 13
%v100362 = vshrl.u32 %v100355, 19
%v100363 = vor.u32 %v100362, %v100361
%v100364 = vxor.u32 %v100363, %v100359
%v100367 = vadd.s32 %v100364, %v100359
%v100369 = vshll.u32 %v100364, 15
%v100370 = vshrl.u32 %v100364, 17
%v100371 = vor.u32 %v100370, %v100369
%v100372 = vxor.u32 %v100371, %v100367
%v100375 = vadd.s32 %v100372, %v100367
%v100377 = vshll.u32 %v100372, 26
%v100378 = vshrl.u32 %v100372, 6
%v100379 = vor.u32 %v100378, %v100377
%v100380 = vxor.u32 %v100379, %v100375
%v100383 = vadd.s32 %v100380, %v100375
%v100387 = vadd.s32 %v100383, %v8
%v100389 = vshll.u32 %v100380, 6
%v100390 = vshrl.u32 %v100380, 26
%v100391 = vor.u32 %v100390, %v100389
%v100392 = vxor.u32 %v100391, %v100383
%v100395 = vadd.s32 %v100392, %v10
%v100399 = vadd.s32 5, %v100395
%v100401 = vxor.u32 %v100399, %v100387
%v100402 = vand.u32.u8 255, %v100401
%v100403 = vand.u32 65535, %v100402
%v100404 = vshrl.u32 %v100403, 1
%v100405 = vor.u32 16256, %v100404
%v100406 = vand.u32.u16 65535, %v100405
%v120280 = vadd.low.f32.bf16 -1.0, %v100406
%v100415 = vmul.f32 2.0, %v120280
%v100419 = vadd.f32 -0.99609375, %v100415
%v100423 = vmax.f32 %v100419, -0.99609375
%v100425 = vand.u32 2147483647, %v100423
%vm100428 = vcmp.eq.f32.partialorder %v100425, 1.0
%v100433 = vmul.f32 inf, %v100423
%v100435 = vxor.u32 2147483648, %v100423
%v100438 = vmul.f32 %v100435, %v100423
%v100440 = vadd.f32 1.0, %v100438
%v100441 = vlog2.pop %v100440
%v100442 = vmul.f32 0.6931472, %v100441
%v100443 = vmul.f32 -0.5, %v100438
%v100444 = vadd.f32 1.0, %v100443
%v100445 = vmul.f32 %v100444, %v100438
%v100446 = vand.u32 2147483647, %v100438
%vm100447 = vcmp.lt.f32.partialorder %v100446, 0.0004427343
%v100448 = vsel /*vm=*/%vm100447, /*on_true_vy=*/%v100445, /*on_false_vx=*/%v100442
%v100449 = vxor.u32 2147483648, %v100448
%vm100452 = vcmp.lt.f32.partialorder %v100449, 5.0
%v100457 = vsel /*vm=*/%vm100452, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v100461 = vsel /*vm=*/%vm100452, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v100465 = vsel /*vm=*/%vm100452, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v100469 = vsel /*vm=*/%vm100452, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v100473 = vsel /*vm=*/%vm100452, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v100477 = vsel /*vm=*/%vm100452, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v100481 = vsel /*vm=*/%vm100452, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v100485 = vsel /*vm=*/%vm100452, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v100489 = vsel /*vm=*/%vm100452, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v100493 = vadd.f32 -2.5, %v100449
%v100495 = vrsqrt.pop %v100449
%v100496 = vmul.f32 %v100495, %v100449
%vm100497 = vcmp.eq.f32.partialorder %v100449, inf
%v100498 = vsel /*vm=*/%vm100497, /*on_true_vy=*/%v100449, /*on_false_vx=*/%v100496
%vm100499 = vcmp.eq.f32.partialorder %v100449, 0.0
%v100500 = vand.u32 2147483648, %v100449
%v100501 = vsel /*vm=*/%vm100499, /*on_true_vy=*/%v100500, /*on_false_vx=*/%v100498
%v100504 = vadd.f32 -3.0, %v100501
%v100508 = vsel /*vm=*/%vm100452, /*on_true_vy=*/%v100493, /*on_false_vx=*/%v100504
%v100512 = vmul.f32 %v100508, %v100489
%v100516 = vadd.f32 %v100512, %v100485
%v100520 = vmul.f32 %v100516, %v100508
%v100524 = vadd.f32 %v100520, %v100481
%v100528 = vmul.f32 %v100524, %v100508
%v100532 = vadd.f32 %v100528, %v100477
%v100536 = vmul.f32 %v100532, %v100508
%v100540 = vadd.f32 %v100536, %v100473
%v100544 = vmul.f32 %v100540, %v100508
%v100548 = vadd.f32 %v100544, %v100469
%v100552 = vmul.f32 %v100548, %v100508
%v100556 = vadd.f32 %v100552, %v100465
%v100560 = vmul.f32 %v100556, %v100508
%v100564 = vadd.f32 %v100560, %v100461
%v100568 = vmul.f32 %v100564, %v100508
%v100572 = vadd.f32 %v100568, %v100457
%v100576 = vmul.f32 %v100572, %v100423
%v100580 = vsel /*vm=*/%vm100428, /*on_true_vy=*/%v100433, /*on_false_vx=*/%v100576
%v100584 = vmul.f32 1.4140625, %v100580
%v100587 = vpack.c.bf16 %v120417, %v100584
%120281 = vst [vmem:[%s280 + $0x368] sm:$0xf] /*vst_source=*/%v100587
%v100591 = vadd.s32 %v97361, %v3816
%v100601 = vadd.s32 %v100591, %v415
%vm100605 = vcmp.lt.u32.totalorder %v100601, %v100591
%vm100610 = vcmp.lt.u32.totalorder %v100591, %v3816
%v100615 = vadd.s32 %v97344, %v3803
%v100619 = vadd.s32 1, %v100615
%v100623 = vsel /*vm=*/%vm100610, /*on_true_vy=*/%v100619, /*on_false_vx=*/%v100615
%v100627 = vadd.s32 1, %v100623
%v100631 = vsel /*vm=*/%vm100605, /*on_true_vy=*/%v100627, /*on_false_vx=*/%v100623
%v100636 = vadd.s32 %v100631, %v10
%v100640 = vadd.s32 %v100601, %v9
%v100644 = vadd.s32 %v100640, %v100636
%v100646 = vshll.u32 %v100640, 13
%v100647 = vshrl.u32 %v100640, 19
%v100648 = vor.u32 %v100647, %v100646
%v100649 = vxor.u32 %v100648, %v100644
%v100652 = vadd.s32 %v100649, %v100644
%v100654 = vshll.u32 %v100649, 15
%v100655 = vshrl.u32 %v100649, 17
%v100656 = vor.u32 %v100655, %v100654
%v100657 = vxor.u32 %v100656, %v100652
%v100660 = vadd.s32 %v100657, %v100652
%v100662 = vshll.u32 %v100657, 26
%v100663 = vshrl.u32 %v100657, 6
%v100664 = vor.u32 %v100663, %v100662
%v100665 = vxor.u32 %v100664, %v100660
%v100668 = vadd.s32 %v100665, %v100660
%v100672 = vadd.s32 %v100668, %v9
%v100674 = vshll.u32 %v100665, 6
%v100675 = vshrl.u32 %v100665, 26
%v100676 = vor.u32 %v100675, %v100674
%v100677 = vxor.u32 %v100676, %v100668
%v100680 = vadd.s32 %v100677, %v8
%v100684 = vadd.s32 1, %v100680
%v100688 = vadd.s32 %v100684, %v100672
%v100690 = vshll.u32 %v100684, 17
%v100691 = vshrl.u32 %v100684, 15
%v100692 = vor.u32 %v100691, %v100690
%v100693 = vxor.u32 %v100692, %v100688
%v100696 = vadd.s32 %v100693, %v100688
%v100698 = vshll.u32 %v100693, 29
%v100699 = vshrl.u32 %v100693, 3
%v100700 = vor.u32 %v100699, %v100698
%v100701 = vxor.u32 %v100700, %v100696
%v100704 = vadd.s32 %v100701, %v100696
%v100706 = vshll.u32 %v100701, 16
%v100707 = vshrl.u32 %v100701, 16
%v100708 = vor.u32 %v100707, %v100706
%v100709 = vxor.u32 %v100708, %v100704
%v100712 = vadd.s32 %v100709, %v100704
%v100716 = vadd.s32 %v100712, %v8
%v100718 = vshll.u32 %v100709, 24
%v100719 = vshrl.u32 %v100709, 8
%v100720 = vor.u32 %v100719, %v100718
%v100721 = vxor.u32 %v100720, %v100712
%v100724 = vadd.s32 %v100721, %v10
%v100728 = vadd.s32 2, %v100724
%v100732 = vadd.s32 %v100728, %v100716
%v100734 = vshll.u32 %v100728, 13
%v100735 = vshrl.u32 %v100728, 19
%v100736 = vor.u32 %v100735, %v100734
%v100737 = vxor.u32 %v100736, %v100732
%v100740 = vadd.s32 %v100737, %v100732
%v100742 = vshll.u32 %v100737, 15
%v100743 = vshrl.u32 %v100737, 17
%v100744 = vor.u32 %v100743, %v100742
%v100745 = vxor.u32 %v100744, %v100740
%v100748 = vadd.s32 %v100745, %v100740
%v100750 = vshll.u32 %v100745, 26
%v100751 = vshrl.u32 %v100745, 6
%v100752 = vor.u32 %v100751, %v100750
%v100753 = vxor.u32 %v100752, %v100748
%v100756 = vadd.s32 %v100753, %v100748
%v100760 = vadd.s32 %v100756, %v10
%v100762 = vshll.u32 %v100753, 6
%v100763 = vshrl.u32 %v100753, 26
%v100764 = vor.u32 %v100763, %v100762
%v100765 = vxor.u32 %v100764, %v100756
%v100768 = vadd.s32 %v100765, %v9
%v100772 = vadd.s32 3, %v100768
%v100776 = vadd.s32 %v100772, %v100760
%v100778 = vshll.u32 %v100772, 17
%v100779 = vshrl.u32 %v100772, 15
%v100780 = vor.u32 %v100779, %v100778
%v100781 = vxor.u32 %v100780, %v100776
%v100784 = vadd.s32 %v100781, %v100776
%v100786 = vshll.u32 %v100781, 29
%v100787 = vshrl.u32 %v100781, 3
%v100788 = vor.u32 %v100787, %v100786
%v100789 = vxor.u32 %v100788, %v100784
%v100792 = vadd.s32 %v100789, %v100784
%v100794 = vshll.u32 %v100789, 16
%v100795 = vshrl.u32 %v100789, 16
%v100796 = vor.u32 %v100795, %v100794
%v100797 = vxor.u32 %v100796, %v100792
%v100800 = vadd.s32 %v100797, %v100792
%v100804 = vadd.s32 %v100800, %v9
%v100806 = vshll.u32 %v100797, 24
%v100807 = vshrl.u32 %v100797, 8
%v100808 = vor.u32 %v100807, %v100806
%v100809 = vxor.u32 %v100808, %v100800
%v100812 = vadd.s32 %v100809, %v8
%v100816 = vadd.s32 4, %v100812
%v100820 = vadd.s32 %v100816, %v100804
%v100822 = vshll.u32 %v100816, 13
%v100823 = vshrl.u32 %v100816, 19
%v100824 = vor.u32 %v100823, %v100822
%v100825 = vxor.u32 %v100824, %v100820
%v100828 = vadd.s32 %v100825, %v100820
%v100830 = vshll.u32 %v100825, 15
%v100831 = vshrl.u32 %v100825, 17
%v100832 = vor.u32 %v100831, %v100830
%v100833 = vxor.u32 %v100832, %v100828
%v100836 = vadd.s32 %v100833, %v100828
%v100838 = vshll.u32 %v100833, 26
%v100839 = vshrl.u32 %v100833, 6
%v100840 = vor.u32 %v100839, %v100838
%v100841 = vxor.u32 %v100840, %v100836
%v100844 = vadd.s32 %v100841, %v100836
%v100848 = vadd.s32 %v100844, %v8
%v100850 = vshll.u32 %v100841, 6
%v100851 = vshrl.u32 %v100841, 26
%v100852 = vor.u32 %v100851, %v100850
%v100853 = vxor.u32 %v100852, %v100844
%v100856 = vadd.s32 %v100853, %v10
%v100860 = vadd.s32 5, %v100856
%v100862 = vxor.u32 %v100860, %v100848
%v100863 = vand.u32.u8 255, %v100862
%v100864 = vand.u32 65535, %v100863
%v100865 = vshrl.u32 %v100864, 1
%v100866 = vor.u32 16256, %v100865
%v100867 = vand.u32.u16 65535, %v100866
%v120282 = vadd.low.f32.bf16 -1.0, %v100867
%v100876 = vmul.f32 2.0, %v120282
%v100880 = vadd.f32 -0.99609375, %v100876
%v100884 = vmax.f32 %v100880, -0.99609375
%v100886 = vand.u32 2147483647, %v100884
%vm100889 = vcmp.eq.f32.partialorder %v100886, 1.0
%v100894 = vmul.f32 inf, %v100884
%v100896 = vxor.u32 2147483648, %v100884
%v100899 = vmul.f32 %v100896, %v100884
%v100901 = vadd.f32 1.0, %v100899
%v100902 = vlog2.pop %v100901
%v100903 = vmul.f32 0.6931472, %v100902
%v100904 = vmul.f32 -0.5, %v100899
%v100905 = vadd.f32 1.0, %v100904
%v100906 = vmul.f32 %v100905, %v100899
%v100907 = vand.u32 2147483647, %v100899
%vm100908 = vcmp.lt.f32.partialorder %v100907, 0.0004427343
%v100909 = vsel /*vm=*/%vm100908, /*on_true_vy=*/%v100906, /*on_false_vx=*/%v100903
%v100910 = vxor.u32 2147483648, %v100909
%vm100913 = vcmp.lt.f32.partialorder %v100910, 5.0
%v100918 = vsel /*vm=*/%vm100913, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v100922 = vsel /*vm=*/%vm100913, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v100926 = vsel /*vm=*/%vm100913, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v100930 = vsel /*vm=*/%vm100913, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v100934 = vsel /*vm=*/%vm100913, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v100938 = vsel /*vm=*/%vm100913, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v100942 = vsel /*vm=*/%vm100913, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v100946 = vsel /*vm=*/%vm100913, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v100950 = vsel /*vm=*/%vm100913, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v100954 = vadd.f32 -2.5, %v100910
%v100956 = vrsqrt.pop %v100910
%v100957 = vmul.f32 %v100956, %v100910
%vm100958 = vcmp.eq.f32.partialorder %v100910, inf
%v100959 = vsel /*vm=*/%vm100958, /*on_true_vy=*/%v100910, /*on_false_vx=*/%v100957
%vm100960 = vcmp.eq.f32.partialorder %v100910, 0.0
%v100961 = vand.u32 2147483648, %v100910
%v100962 = vsel /*vm=*/%vm100960, /*on_true_vy=*/%v100961, /*on_false_vx=*/%v100959
%v100965 = vadd.f32 -3.0, %v100962
%v100969 = vsel /*vm=*/%vm100913, /*on_true_vy=*/%v100954, /*on_false_vx=*/%v100965
%v100973 = vmul.f32 %v100969, %v100950
%v100977 = vadd.f32 %v100973, %v100946
%v100981 = vmul.f32 %v100977, %v100969
%v100985 = vadd.f32 %v100981, %v100942
%v100989 = vmul.f32 %v100985, %v100969
%v100993 = vadd.f32 %v100989, %v100938
%v100997 = vmul.f32 %v100993, %v100969
%v101001 = vadd.f32 %v100997, %v100934
%v101005 = vmul.f32 %v101001, %v100969
%v101009 = vadd.f32 %v101005, %v100930
%v101013 = vmul.f32 %v101009, %v100969
%v101017 = vadd.f32 %v101013, %v100926
%v101021 = vmul.f32 %v101017, %v100969
%v101025 = vadd.f32 %v101021, %v100922
%v101029 = vmul.f32 %v101025, %v100969
%v101033 = vadd.f32 %v101029, %v100918
%v101037 = vmul.f32 %v101033, %v100884
%v101041 = vsel /*vm=*/%vm100889, /*on_true_vy=*/%v100894, /*on_false_vx=*/%v101037
%v101045 = vmul.f32 1.4140625, %v101041
%v101048 = vpack.c.bf16 %v120417, %v101045
%120283 = vst [vmem:[%s280 + $0x3e8] sm:$0xf] /*vst_source=*/%v101048
%v101086 = vadd.s32 %v101083, %v408
%v101096 = vadd.s32 %v101086, %v415
%vm101100 = vcmp.lt.u32.totalorder %v101096, %v101086
%vm101105 = vcmp.lt.u32.totalorder %v101086, %v408
%v101110 = vadd.s32 %v101066, %v380
%v101114 = vadd.s32 1, %v101110
%v101118 = vsel /*vm=*/%vm101105, /*on_true_vy=*/%v101114, /*on_false_vx=*/%v101110
%v101122 = vadd.s32 1, %v101118
%v101126 = vsel /*vm=*/%vm101100, /*on_true_vy=*/%v101122, /*on_false_vx=*/%v101118
%v101131 = vadd.s32 %v101126, %v10
%v101135 = vadd.s32 %v101096, %v9
%v101139 = vadd.s32 %v101135, %v101131
%v101141 = vshll.u32 %v101135, 13
%v101142 = vshrl.u32 %v101135, 19
%v101143 = vor.u32 %v101142, %v101141
%v101144 = vxor.u32 %v101143, %v101139
%v101147 = vadd.s32 %v101144, %v101139
%v101149 = vshll.u32 %v101144, 15
%v101150 = vshrl.u32 %v101144, 17
%v101151 = vor.u32 %v101150, %v101149
%v101152 = vxor.u32 %v101151, %v101147
%v101155 = vadd.s32 %v101152, %v101147
%v101157 = vshll.u32 %v101152, 26
%v101158 = vshrl.u32 %v101152, 6
%v101159 = vor.u32 %v101158, %v101157
%v101160 = vxor.u32 %v101159, %v101155
%v101163 = vadd.s32 %v101160, %v101155
%v101167 = vadd.s32 %v101163, %v9
%v101169 = vshll.u32 %v101160, 6
%v101170 = vshrl.u32 %v101160, 26
%v101171 = vor.u32 %v101170, %v101169
%v101172 = vxor.u32 %v101171, %v101163
%v101175 = vadd.s32 %v101172, %v8
%v101179 = vadd.s32 1, %v101175
%v101183 = vadd.s32 %v101179, %v101167
%v101185 = vshll.u32 %v101179, 17
%v101186 = vshrl.u32 %v101179, 15
%v101187 = vor.u32 %v101186, %v101185
%v101188 = vxor.u32 %v101187, %v101183
%v101191 = vadd.s32 %v101188, %v101183
%v101193 = vshll.u32 %v101188, 29
%v101194 = vshrl.u32 %v101188, 3
%v101195 = vor.u32 %v101194, %v101193
%v101196 = vxor.u32 %v101195, %v101191
%v101199 = vadd.s32 %v101196, %v101191
%v101201 = vshll.u32 %v101196, 16
%v101202 = vshrl.u32 %v101196, 16
%v101203 = vor.u32 %v101202, %v101201
%v101204 = vxor.u32 %v101203, %v101199
%v101207 = vadd.s32 %v101204, %v101199
%v101211 = vadd.s32 %v101207, %v8
%v101213 = vshll.u32 %v101204, 24
%v101214 = vshrl.u32 %v101204, 8
%v101215 = vor.u32 %v101214, %v101213
%v101216 = vxor.u32 %v101215, %v101207
%v101219 = vadd.s32 %v101216, %v10
%v101223 = vadd.s32 2, %v101219
%v101227 = vadd.s32 %v101223, %v101211
%v101229 = vshll.u32 %v101223, 13
%v101230 = vshrl.u32 %v101223, 19
%v101231 = vor.u32 %v101230, %v101229
%v101232 = vxor.u32 %v101231, %v101227
%v101235 = vadd.s32 %v101232, %v101227
%v101237 = vshll.u32 %v101232, 15
%v101238 = vshrl.u32 %v101232, 17
%v101239 = vor.u32 %v101238, %v101237
%v101240 = vxor.u32 %v101239, %v101235
%v101243 = vadd.s32 %v101240, %v101235
%v101245 = vshll.u32 %v101240, 26
%v101246 = vshrl.u32 %v101240, 6
%v101247 = vor.u32 %v101246, %v101245
%v101248 = vxor.u32 %v101247, %v101243
%v101251 = vadd.s32 %v101248, %v101243
%v101255 = vadd.s32 %v101251, %v10
%v101257 = vshll.u32 %v101248, 6
%v101258 = vshrl.u32 %v101248, 26
%v101259 = vor.u32 %v101258, %v101257
%v101260 = vxor.u32 %v101259, %v101251
%v101263 = vadd.s32 %v101260, %v9
%v101267 = vadd.s32 3, %v101263
%v101271 = vadd.s32 %v101267, %v101255
%v101273 = vshll.u32 %v101267, 17
%v101274 = vshrl.u32 %v101267, 15
%v101275 = vor.u32 %v101274, %v101273
%v101276 = vxor.u32 %v101275, %v101271
%v101279 = vadd.s32 %v101276, %v101271
%v101281 = vshll.u32 %v101276, 29
%v101282 = vshrl.u32 %v101276, 3
%v101283 = vor.u32 %v101282, %v101281
%v101284 = vxor.u32 %v101283, %v101279
%v101287 = vadd.s32 %v101284, %v101279
%v101289 = vshll.u32 %v101284, 16
%v101290 = vshrl.u32 %v101284, 16
%v101291 = vor.u32 %v101290, %v101289
%v101292 = vxor.u32 %v101291, %v101287
%v101295 = vadd.s32 %v101292, %v101287
%v101299 = vadd.s32 %v101295, %v9
%v101301 = vshll.u32 %v101292, 24
%v101302 = vshrl.u32 %v101292, 8
%v101303 = vor.u32 %v101302, %v101301
%v101304 = vxor.u32 %v101303, %v101295
%v101307 = vadd.s32 %v101304, %v8
%v101311 = vadd.s32 4, %v101307
%v101315 = vadd.s32 %v101311, %v101299
%v101317 = vshll.u32 %v101311, 13
%v101318 = vshrl.u32 %v101311, 19
%v101319 = vor.u32 %v101318, %v101317
%v101320 = vxor.u32 %v101319, %v101315
%v101323 = vadd.s32 %v101320, %v101315
%v101325 = vshll.u32 %v101320, 15
%v101326 = vshrl.u32 %v101320, 17
%v101327 = vor.u32 %v101326, %v101325
%v101328 = vxor.u32 %v101327, %v101323
%v101331 = vadd.s32 %v101328, %v101323
%v101333 = vshll.u32 %v101328, 26
%v101334 = vshrl.u32 %v101328, 6
%v101335 = vor.u32 %v101334, %v101333
%v101336 = vxor.u32 %v101335, %v101331
%v101339 = vadd.s32 %v101336, %v101331
%v101343 = vadd.s32 %v101339, %v8
%v101345 = vshll.u32 %v101336, 6
%v101346 = vshrl.u32 %v101336, 26
%v101347 = vor.u32 %v101346, %v101345
%v101348 = vxor.u32 %v101347, %v101339
%v101351 = vadd.s32 %v101348, %v10
%v101355 = vadd.s32 5, %v101351
%v101357 = vxor.u32 %v101355, %v101343
%v101358 = vand.u32.u8 255, %v101357
%v101359 = vand.u32 65535, %v101358
%v101360 = vshrl.u32 %v101359, 1
%v101361 = vor.u32 16256, %v101360
%v101362 = vand.u32.u16 65535, %v101361
%v120288 = vadd.low.f32.bf16 -1.0, %v101362
%v101371 = vmul.f32 2.0, %v120288
%v101375 = vadd.f32 -0.99609375, %v101371
%v101379 = vmax.f32 %v101375, -0.99609375
%v101381 = vand.u32 2147483647, %v101379
%vm101384 = vcmp.eq.f32.partialorder %v101381, 1.0
%v101389 = vmul.f32 inf, %v101379
%v101391 = vxor.u32 2147483648, %v101379
%v101394 = vmul.f32 %v101391, %v101379
%v101396 = vadd.f32 1.0, %v101394
%v101397 = vlog2.pop %v101396
%v101398 = vmul.f32 0.6931472, %v101397
%v101399 = vmul.f32 -0.5, %v101394
%v101400 = vadd.f32 1.0, %v101399
%v101401 = vmul.f32 %v101400, %v101394
%v101402 = vand.u32 2147483647, %v101394
%vm101403 = vcmp.lt.f32.partialorder %v101402, 0.0004427343
%v101404 = vsel /*vm=*/%vm101403, /*on_true_vy=*/%v101401, /*on_false_vx=*/%v101398
%v101405 = vxor.u32 2147483648, %v101404
%vm101408 = vcmp.lt.f32.partialorder %v101405, 5.0
%v101413 = vsel /*vm=*/%vm101408, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v101417 = vsel /*vm=*/%vm101408, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v101421 = vsel /*vm=*/%vm101408, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v101425 = vsel /*vm=*/%vm101408, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v101429 = vsel /*vm=*/%vm101408, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v101433 = vsel /*vm=*/%vm101408, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v101437 = vsel /*vm=*/%vm101408, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v101441 = vsel /*vm=*/%vm101408, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v101445 = vsel /*vm=*/%vm101408, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v101449 = vadd.f32 -2.5, %v101405
%v101451 = vrsqrt.pop %v101405
%v101452 = vmul.f32 %v101451, %v101405
%vm101453 = vcmp.eq.f32.partialorder %v101405, inf
%v101454 = vsel /*vm=*/%vm101453, /*on_true_vy=*/%v101405, /*on_false_vx=*/%v101452
%vm101455 = vcmp.eq.f32.partialorder %v101405, 0.0
%v101456 = vand.u32 2147483648, %v101405
%v101457 = vsel /*vm=*/%vm101455, /*on_true_vy=*/%v101456, /*on_false_vx=*/%v101454
%v101460 = vadd.f32 -3.0, %v101457
%v101464 = vsel /*vm=*/%vm101408, /*on_true_vy=*/%v101449, /*on_false_vx=*/%v101460
%v101468 = vmul.f32 %v101464, %v101445
%v101472 = vadd.f32 %v101468, %v101441
%v101476 = vmul.f32 %v101472, %v101464
%v101480 = vadd.f32 %v101476, %v101437
%v101484 = vmul.f32 %v101480, %v101464
%v101488 = vadd.f32 %v101484, %v101433
%v101492 = vmul.f32 %v101488, %v101464
%v101496 = vadd.f32 %v101492, %v101429
%v101500 = vmul.f32 %v101496, %v101464
%v101504 = vadd.f32 %v101500, %v101425
%v101508 = vmul.f32 %v101504, %v101464
%v101512 = vadd.f32 %v101508, %v101421
%v101516 = vmul.f32 %v101512, %v101464
%v101520 = vadd.f32 %v101516, %v101417
%v101524 = vmul.f32 %v101520, %v101464
%v101528 = vadd.f32 %v101524, %v101413
%v101532 = vmul.f32 %v101528, %v101379
%v101536 = vsel /*vm=*/%vm101384, /*on_true_vy=*/%v101389, /*on_false_vx=*/%v101532
%v101540 = vmul.f32 1.4140625, %v101536
%v101543 = vpack.c.bf16 %v120417, %v101540
%120289 = vst [vmem:[%s280 + $0x6c] sm:$0xf] /*vst_source=*/%v101543
%v101547 = vadd.s32 %v101083, %v894
%v101557 = vadd.s32 %v101547, %v415
%vm101561 = vcmp.lt.u32.totalorder %v101557, %v101547
%vm101566 = vcmp.lt.u32.totalorder %v101547, %v894
%v101571 = vadd.s32 %v101066, %v881
%v101575 = vadd.s32 1, %v101571
%v101579 = vsel /*vm=*/%vm101566, /*on_true_vy=*/%v101575, /*on_false_vx=*/%v101571
%v101583 = vadd.s32 1, %v101579
%v101587 = vsel /*vm=*/%vm101561, /*on_true_vy=*/%v101583, /*on_false_vx=*/%v101579
%v101592 = vadd.s32 %v101587, %v10
%v101596 = vadd.s32 %v101557, %v9
%v101600 = vadd.s32 %v101596, %v101592
%v101602 = vshll.u32 %v101596, 13
%v101603 = vshrl.u32 %v101596, 19
%v101604 = vor.u32 %v101603, %v101602
%v101605 = vxor.u32 %v101604, %v101600
%v101608 = vadd.s32 %v101605, %v101600
%v101610 = vshll.u32 %v101605, 15
%v101611 = vshrl.u32 %v101605, 17
%v101612 = vor.u32 %v101611, %v101610
%v101613 = vxor.u32 %v101612, %v101608
%v101616 = vadd.s32 %v101613, %v101608
%v101618 = vshll.u32 %v101613, 26
%v101619 = vshrl.u32 %v101613, 6
%v101620 = vor.u32 %v101619, %v101618
%v101621 = vxor.u32 %v101620, %v101616
%v101624 = vadd.s32 %v101621, %v101616
%v101628 = vadd.s32 %v101624, %v9
%v101630 = vshll.u32 %v101621, 6
%v101631 = vshrl.u32 %v101621, 26
%v101632 = vor.u32 %v101631, %v101630
%v101633 = vxor.u32 %v101632, %v101624
%v101636 = vadd.s32 %v101633, %v8
%v101640 = vadd.s32 1, %v101636
%v101644 = vadd.s32 %v101640, %v101628
%v101646 = vshll.u32 %v101640, 17
%v101647 = vshrl.u32 %v101640, 15
%v101648 = vor.u32 %v101647, %v101646
%v101649 = vxor.u32 %v101648, %v101644
%v101652 = vadd.s32 %v101649, %v101644
%v101654 = vshll.u32 %v101649, 29
%v101655 = vshrl.u32 %v101649, 3
%v101656 = vor.u32 %v101655, %v101654
%v101657 = vxor.u32 %v101656, %v101652
%v101660 = vadd.s32 %v101657, %v101652
%v101662 = vshll.u32 %v101657, 16
%v101663 = vshrl.u32 %v101657, 16
%v101664 = vor.u32 %v101663, %v101662
%v101665 = vxor.u32 %v101664, %v101660
%v101668 = vadd.s32 %v101665, %v101660
%v101672 = vadd.s32 %v101668, %v8
%v101674 = vshll.u32 %v101665, 24
%v101675 = vshrl.u32 %v101665, 8
%v101676 = vor.u32 %v101675, %v101674
%v101677 = vxor.u32 %v101676, %v101668
%v101680 = vadd.s32 %v101677, %v10
%v101684 = vadd.s32 2, %v101680
%v101688 = vadd.s32 %v101684, %v101672
%v101690 = vshll.u32 %v101684, 13
%v101691 = vshrl.u32 %v101684, 19
%v101692 = vor.u32 %v101691, %v101690
%v101693 = vxor.u32 %v101692, %v101688
%v101696 = vadd.s32 %v101693, %v101688
%v101698 = vshll.u32 %v101693, 15
%v101699 = vshrl.u32 %v101693, 17
%v101700 = vor.u32 %v101699, %v101698
%v101701 = vxor.u32 %v101700, %v101696
%v101704 = vadd.s32 %v101701, %v101696
%v101706 = vshll.u32 %v101701, 26
%v101707 = vshrl.u32 %v101701, 6
%v101708 = vor.u32 %v101707, %v101706
%v101709 = vxor.u32 %v101708, %v101704
%v101712 = vadd.s32 %v101709, %v101704
%v101716 = vadd.s32 %v101712, %v10
%v101718 = vshll.u32 %v101709, 6
%v101719 = vshrl.u32 %v101709, 26
%v101720 = vor.u32 %v101719, %v101718
%v101721 = vxor.u32 %v101720, %v101712
%v101724 = vadd.s32 %v101721, %v9
%v101728 = vadd.s32 3, %v101724
%v101732 = vadd.s32 %v101728, %v101716
%v101734 = vshll.u32 %v101728, 17
%v101735 = vshrl.u32 %v101728, 15
%v101736 = vor.u32 %v101735, %v101734
%v101737 = vxor.u32 %v101736, %v101732
%v101740 = vadd.s32 %v101737, %v101732
%v101742 = vshll.u32 %v101737, 29
%v101743 = vshrl.u32 %v101737, 3
%v101744 = vor.u32 %v101743, %v101742
%v101745 = vxor.u32 %v101744, %v101740
%v101748 = vadd.s32 %v101745, %v101740
%v101750 = vshll.u32 %v101745, 16
%v101751 = vshrl.u32 %v101745, 16
%v101752 = vor.u32 %v101751, %v101750
%v101753 = vxor.u32 %v101752, %v101748
%v101756 = vadd.s32 %v101753, %v101748
%v101760 = vadd.s32 %v101756, %v9
%v101762 = vshll.u32 %v101753, 24
%v101763 = vshrl.u32 %v101753, 8
%v101764 = vor.u32 %v101763, %v101762
%v101765 = vxor.u32 %v101764, %v101756
%v101768 = vadd.s32 %v101765, %v8
%v101772 = vadd.s32 4, %v101768
%v101776 = vadd.s32 %v101772, %v101760
%v101778 = vshll.u32 %v101772, 13
%v101779 = vshrl.u32 %v101772, 19
%v101780 = vor.u32 %v101779, %v101778
%v101781 = vxor.u32 %v101780, %v101776
%v101784 = vadd.s32 %v101781, %v101776
%v101786 = vshll.u32 %v101781, 15
%v101787 = vshrl.u32 %v101781, 17
%v101788 = vor.u32 %v101787, %v101786
%v101789 = vxor.u32 %v101788, %v101784
%v101792 = vadd.s32 %v101789, %v101784
%v101794 = vshll.u32 %v101789, 26
%v101795 = vshrl.u32 %v101789, 6
%v101796 = vor.u32 %v101795, %v101794
%v101797 = vxor.u32 %v101796, %v101792
%v101800 = vadd.s32 %v101797, %v101792
%v101804 = vadd.s32 %v101800, %v8
%v101806 = vshll.u32 %v101797, 6
%v101807 = vshrl.u32 %v101797, 26
%v101808 = vor.u32 %v101807, %v101806
%v101809 = vxor.u32 %v101808, %v101800
%v101812 = vadd.s32 %v101809, %v10
%v101816 = vadd.s32 5, %v101812
%v101818 = vxor.u32 %v101816, %v101804
%v101819 = vand.u32.u8 255, %v101818
%v101820 = vand.u32 65535, %v101819
%v101821 = vshrl.u32 %v101820, 1
%v101822 = vor.u32 16256, %v101821
%v101823 = vand.u32.u16 65535, %v101822
%v120290 = vadd.low.f32.bf16 -1.0, %v101823
%v101832 = vmul.f32 2.0, %v120290
%v101836 = vadd.f32 -0.99609375, %v101832
%v101840 = vmax.f32 %v101836, -0.99609375
%v101842 = vand.u32 2147483647, %v101840
%vm101845 = vcmp.eq.f32.partialorder %v101842, 1.0
%v101850 = vmul.f32 inf, %v101840
%v101852 = vxor.u32 2147483648, %v101840
%v101855 = vmul.f32 %v101852, %v101840
%v101857 = vadd.f32 1.0, %v101855
%v101858 = vlog2.pop %v101857
%v101859 = vmul.f32 0.6931472, %v101858
%v101860 = vmul.f32 -0.5, %v101855
%v101861 = vadd.f32 1.0, %v101860
%v101862 = vmul.f32 %v101861, %v101855
%v101863 = vand.u32 2147483647, %v101855
%vm101864 = vcmp.lt.f32.partialorder %v101863, 0.0004427343
%v101865 = vsel /*vm=*/%vm101864, /*on_true_vy=*/%v101862, /*on_false_vx=*/%v101859
%v101866 = vxor.u32 2147483648, %v101865
%vm101869 = vcmp.lt.f32.partialorder %v101866, 5.0
%v101874 = vsel /*vm=*/%vm101869, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v101878 = vsel /*vm=*/%vm101869, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v101882 = vsel /*vm=*/%vm101869, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v101886 = vsel /*vm=*/%vm101869, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v101890 = vsel /*vm=*/%vm101869, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v101894 = vsel /*vm=*/%vm101869, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v101898 = vsel /*vm=*/%vm101869, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v101902 = vsel /*vm=*/%vm101869, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v101906 = vsel /*vm=*/%vm101869, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v101910 = vadd.f32 -2.5, %v101866
%v101912 = vrsqrt.pop %v101866
%v101913 = vmul.f32 %v101912, %v101866
%vm101914 = vcmp.eq.f32.partialorder %v101866, inf
%v101915 = vsel /*vm=*/%vm101914, /*on_true_vy=*/%v101866, /*on_false_vx=*/%v101913
%vm101916 = vcmp.eq.f32.partialorder %v101866, 0.0
%v101917 = vand.u32 2147483648, %v101866
%v101918 = vsel /*vm=*/%vm101916, /*on_true_vy=*/%v101917, /*on_false_vx=*/%v101915
%v101921 = vadd.f32 -3.0, %v101918
%v101925 = vsel /*vm=*/%vm101869, /*on_true_vy=*/%v101910, /*on_false_vx=*/%v101921
%v101929 = vmul.f32 %v101925, %v101906
%v101933 = vadd.f32 %v101929, %v101902
%v101937 = vmul.f32 %v101933, %v101925
%v101941 = vadd.f32 %v101937, %v101898
%v101945 = vmul.f32 %v101941, %v101925
%v101949 = vadd.f32 %v101945, %v101894
%v101953 = vmul.f32 %v101949, %v101925
%v101957 = vadd.f32 %v101953, %v101890
%v101961 = vmul.f32 %v101957, %v101925
%v101965 = vadd.f32 %v101961, %v101886
%v101969 = vmul.f32 %v101965, %v101925
%v101973 = vadd.f32 %v101969, %v101882
%v101977 = vmul.f32 %v101973, %v101925
%v101981 = vadd.f32 %v101977, %v101878
%v101985 = vmul.f32 %v101981, %v101925
%v101989 = vadd.f32 %v101985, %v101874
%v101993 = vmul.f32 %v101989, %v101840
%v101997 = vsel /*vm=*/%vm101845, /*on_true_vy=*/%v101850, /*on_false_vx=*/%v101993
%v102001 = vmul.f32 1.4140625, %v101997
%v102004 = vpack.c.bf16 %v120417, %v102001
%120291 = vst [vmem:[%s280 + $0xec] sm:$0xf] /*vst_source=*/%v102004
%v102008 = vadd.s32 %v101083, %v1381
%v102018 = vadd.s32 %v102008, %v415
%vm102022 = vcmp.lt.u32.totalorder %v102018, %v102008
%vm102027 = vcmp.lt.u32.totalorder %v102008, %v1381
%v102032 = vadd.s32 %v101066, %v1368
%v102036 = vadd.s32 1, %v102032
%v102040 = vsel /*vm=*/%vm102027, /*on_true_vy=*/%v102036, /*on_false_vx=*/%v102032
%v102044 = vadd.s32 1, %v102040
%v102048 = vsel /*vm=*/%vm102022, /*on_true_vy=*/%v102044, /*on_false_vx=*/%v102040
%v102053 = vadd.s32 %v102048, %v10
%v102057 = vadd.s32 %v102018, %v9
%v102061 = vadd.s32 %v102057, %v102053
%v102063 = vshll.u32 %v102057, 13
%v102064 = vshrl.u32 %v102057, 19
%v102065 = vor.u32 %v102064, %v102063
%v102066 = vxor.u32 %v102065, %v102061
%v102069 = vadd.s32 %v102066, %v102061
%v102071 = vshll.u32 %v102066, 15
%v102072 = vshrl.u32 %v102066, 17
%v102073 = vor.u32 %v102072, %v102071
%v102074 = vxor.u32 %v102073, %v102069
%v102077 = vadd.s32 %v102074, %v102069
%v102079 = vshll.u32 %v102074, 26
%v102080 = vshrl.u32 %v102074, 6
%v102081 = vor.u32 %v102080, %v102079
%v102082 = vxor.u32 %v102081, %v102077
%v102085 = vadd.s32 %v102082, %v102077
%v102089 = vadd.s32 %v102085, %v9
%v102091 = vshll.u32 %v102082, 6
%v102092 = vshrl.u32 %v102082, 26
%v102093 = vor.u32 %v102092, %v102091
%v102094 = vxor.u32 %v102093, %v102085
%v102097 = vadd.s32 %v102094, %v8
%v102101 = vadd.s32 1, %v102097
%v102105 = vadd.s32 %v102101, %v102089
%v102107 = vshll.u32 %v102101, 17
%v102108 = vshrl.u32 %v102101, 15
%v102109 = vor.u32 %v102108, %v102107
%v102110 = vxor.u32 %v102109, %v102105
%v102113 = vadd.s32 %v102110, %v102105
%v102115 = vshll.u32 %v102110, 29
%v102116 = vshrl.u32 %v102110, 3
%v102117 = vor.u32 %v102116, %v102115
%v102118 = vxor.u32 %v102117, %v102113
%v102121 = vadd.s32 %v102118, %v102113
%v102123 = vshll.u32 %v102118, 16
%v102124 = vshrl.u32 %v102118, 16
%v102125 = vor.u32 %v102124, %v102123
%v102126 = vxor.u32 %v102125, %v102121
%v102129 = vadd.s32 %v102126, %v102121
%v102133 = vadd.s32 %v102129, %v8
%v102135 = vshll.u32 %v102126, 24
%v102136 = vshrl.u32 %v102126, 8
%v102137 = vor.u32 %v102136, %v102135
%v102138 = vxor.u32 %v102137, %v102129
%v102141 = vadd.s32 %v102138, %v10
%v102145 = vadd.s32 2, %v102141
%v102149 = vadd.s32 %v102145, %v102133
%v102151 = vshll.u32 %v102145, 13
%v102152 = vshrl.u32 %v102145, 19
%v102153 = vor.u32 %v102152, %v102151
%v102154 = vxor.u32 %v102153, %v102149
%v102157 = vadd.s32 %v102154, %v102149
%v102159 = vshll.u32 %v102154, 15
%v102160 = vshrl.u32 %v102154, 17
%v102161 = vor.u32 %v102160, %v102159
%v102162 = vxor.u32 %v102161, %v102157
%v102165 = vadd.s32 %v102162, %v102157
%v102167 = vshll.u32 %v102162, 26
%v102168 = vshrl.u32 %v102162, 6
%v102169 = vor.u32 %v102168, %v102167
%v102170 = vxor.u32 %v102169, %v102165
%v102173 = vadd.s32 %v102170, %v102165
%v102177 = vadd.s32 %v102173, %v10
%v102179 = vshll.u32 %v102170, 6
%v102180 = vshrl.u32 %v102170, 26
%v102181 = vor.u32 %v102180, %v102179
%v102182 = vxor.u32 %v102181, %v102173
%v102185 = vadd.s32 %v102182, %v9
%v102189 = vadd.s32 3, %v102185
%v102193 = vadd.s32 %v102189, %v102177
%v102195 = vshll.u32 %v102189, 17
%v102196 = vshrl.u32 %v102189, 15
%v102197 = vor.u32 %v102196, %v102195
%v102198 = vxor.u32 %v102197, %v102193
%v102201 = vadd.s32 %v102198, %v102193
%v102203 = vshll.u32 %v102198, 29
%v102204 = vshrl.u32 %v102198, 3
%v102205 = vor.u32 %v102204, %v102203
%v102206 = vxor.u32 %v102205, %v102201
%v102209 = vadd.s32 %v102206, %v102201
%v102211 = vshll.u32 %v102206, 16
%v102212 = vshrl.u32 %v102206, 16
%v102213 = vor.u32 %v102212, %v102211
%v102214 = vxor.u32 %v102213, %v102209
%v102217 = vadd.s32 %v102214, %v102209
%v102221 = vadd.s32 %v102217, %v9
%v102223 = vshll.u32 %v102214, 24
%v102224 = vshrl.u32 %v102214, 8
%v102225 = vor.u32 %v102224, %v102223
%v102226 = vxor.u32 %v102225, %v102217
%v102229 = vadd.s32 %v102226, %v8
%v102233 = vadd.s32 4, %v102229
%v102237 = vadd.s32 %v102233, %v102221
%v102239 = vshll.u32 %v102233, 13
%v102240 = vshrl.u32 %v102233, 19
%v102241 = vor.u32 %v102240, %v102239
%v102242 = vxor.u32 %v102241, %v102237
%v102245 = vadd.s32 %v102242, %v102237
%v102247 = vshll.u32 %v102242, 15
%v102248 = vshrl.u32 %v102242, 17
%v102249 = vor.u32 %v102248, %v102247
%v102250 = vxor.u32 %v102249, %v102245
%v102253 = vadd.s32 %v102250, %v102245
%v102255 = vshll.u32 %v102250, 26
%v102256 = vshrl.u32 %v102250, 6
%v102257 = vor.u32 %v102256, %v102255
%v102258 = vxor.u32 %v102257, %v102253
%v102261 = vadd.s32 %v102258, %v102253
%v102265 = vadd.s32 %v102261, %v8
%v102267 = vshll.u32 %v102258, 6
%v102268 = vshrl.u32 %v102258, 26
%v102269 = vor.u32 %v102268, %v102267
%v102270 = vxor.u32 %v102269, %v102261
%v102273 = vadd.s32 %v102270, %v10
%v102277 = vadd.s32 5, %v102273
%v102279 = vxor.u32 %v102277, %v102265
%v102280 = vand.u32.u8 255, %v102279
%v102281 = vand.u32 65535, %v102280
%v102282 = vshrl.u32 %v102281, 1
%v102283 = vor.u32 16256, %v102282
%v102284 = vand.u32.u16 65535, %v102283
%v120292 = vadd.low.f32.bf16 -1.0, %v102284
%v102293 = vmul.f32 2.0, %v120292
%v102297 = vadd.f32 -0.99609375, %v102293
%v102301 = vmax.f32 %v102297, -0.99609375
%v102303 = vand.u32 2147483647, %v102301
%vm102306 = vcmp.eq.f32.partialorder %v102303, 1.0
%v102311 = vmul.f32 inf, %v102301
%v102313 = vxor.u32 2147483648, %v102301
%v102316 = vmul.f32 %v102313, %v102301
%v102318 = vadd.f32 1.0, %v102316
%v102319 = vlog2.pop %v102318
%v102320 = vmul.f32 0.6931472, %v102319
%v102321 = vmul.f32 -0.5, %v102316
%v102322 = vadd.f32 1.0, %v102321
%v102323 = vmul.f32 %v102322, %v102316
%v102324 = vand.u32 2147483647, %v102316
%vm102325 = vcmp.lt.f32.partialorder %v102324, 0.0004427343
%v102326 = vsel /*vm=*/%vm102325, /*on_true_vy=*/%v102323, /*on_false_vx=*/%v102320
%v102327 = vxor.u32 2147483648, %v102326
%vm102330 = vcmp.lt.f32.partialorder %v102327, 5.0
%v102335 = vsel /*vm=*/%vm102330, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v102339 = vsel /*vm=*/%vm102330, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v102343 = vsel /*vm=*/%vm102330, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v102347 = vsel /*vm=*/%vm102330, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v102351 = vsel /*vm=*/%vm102330, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v102355 = vsel /*vm=*/%vm102330, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v102359 = vsel /*vm=*/%vm102330, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v102363 = vsel /*vm=*/%vm102330, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v102367 = vsel /*vm=*/%vm102330, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v102371 = vadd.f32 -2.5, %v102327
%v102373 = vrsqrt.pop %v102327
%v102374 = vmul.f32 %v102373, %v102327
%vm102375 = vcmp.eq.f32.partialorder %v102327, inf
%v102376 = vsel /*vm=*/%vm102375, /*on_true_vy=*/%v102327, /*on_false_vx=*/%v102374
%vm102377 = vcmp.eq.f32.partialorder %v102327, 0.0
%v102378 = vand.u32 2147483648, %v102327
%v102379 = vsel /*vm=*/%vm102377, /*on_true_vy=*/%v102378, /*on_false_vx=*/%v102376
%v102382 = vadd.f32 -3.0, %v102379
%v102386 = vsel /*vm=*/%vm102330, /*on_true_vy=*/%v102371, /*on_false_vx=*/%v102382
%v102390 = vmul.f32 %v102386, %v102367
%v102394 = vadd.f32 %v102390, %v102363
%v102398 = vmul.f32 %v102394, %v102386
%v102402 = vadd.f32 %v102398, %v102359
%v102406 = vmul.f32 %v102402, %v102386
%v102410 = vadd.f32 %v102406, %v102355
%v102414 = vmul.f32 %v102410, %v102386
%v102418 = vadd.f32 %v102414, %v102351
%v102422 = vmul.f32 %v102418, %v102386
%v102426 = vadd.f32 %v102422, %v102347
%v102430 = vmul.f32 %v102426, %v102386
%v102434 = vadd.f32 %v102430, %v102343
%v102438 = vmul.f32 %v102434, %v102386
%v102442 = vadd.f32 %v102438, %v102339
%v102446 = vmul.f32 %v102442, %v102386
%v102450 = vadd.f32 %v102446, %v102335
%v102454 = vmul.f32 %v102450, %v102301
%v102458 = vsel /*vm=*/%vm102306, /*on_true_vy=*/%v102311, /*on_false_vx=*/%v102454
%v102462 = vmul.f32 1.4140625, %v102458
%v102465 = vpack.c.bf16 %v120417, %v102462
%120293 = vst [vmem:[%s280 + $0x16c] sm:$0xf] /*vst_source=*/%v102465
%v102469 = vadd.s32 %v101083, %v1868
%v102479 = vadd.s32 %v102469, %v415
%vm102483 = vcmp.lt.u32.totalorder %v102479, %v102469
%vm102488 = vcmp.lt.u32.totalorder %v102469, %v1868
%v102493 = vadd.s32 %v101066, %v1855
%v102497 = vadd.s32 1, %v102493
%v102501 = vsel /*vm=*/%vm102488, /*on_true_vy=*/%v102497, /*on_false_vx=*/%v102493
%v102505 = vadd.s32 1, %v102501
%v102509 = vsel /*vm=*/%vm102483, /*on_true_vy=*/%v102505, /*on_false_vx=*/%v102501
%v102514 = vadd.s32 %v102509, %v10
%v102518 = vadd.s32 %v102479, %v9
%v102522 = vadd.s32 %v102518, %v102514
%v102524 = vshll.u32 %v102518, 13
%v102525 = vshrl.u32 %v102518, 19
%v102526 = vor.u32 %v102525, %v102524
%v102527 = vxor.u32 %v102526, %v102522
%v102530 = vadd.s32 %v102527, %v102522
%v102532 = vshll.u32 %v102527, 15
%v102533 = vshrl.u32 %v102527, 17
%v102534 = vor.u32 %v102533, %v102532
%v102535 = vxor.u32 %v102534, %v102530
%v102538 = vadd.s32 %v102535, %v102530
%v102540 = vshll.u32 %v102535, 26
%v102541 = vshrl.u32 %v102535, 6
%v102542 = vor.u32 %v102541, %v102540
%v102543 = vxor.u32 %v102542, %v102538
%v102546 = vadd.s32 %v102543, %v102538
%v102550 = vadd.s32 %v102546, %v9
%v102552 = vshll.u32 %v102543, 6
%v102553 = vshrl.u32 %v102543, 26
%v102554 = vor.u32 %v102553, %v102552
%v102555 = vxor.u32 %v102554, %v102546
%v102558 = vadd.s32 %v102555, %v8
%v102562 = vadd.s32 1, %v102558
%v102566 = vadd.s32 %v102562, %v102550
%v102568 = vshll.u32 %v102562, 17
%v102569 = vshrl.u32 %v102562, 15
%v102570 = vor.u32 %v102569, %v102568
%v102571 = vxor.u32 %v102570, %v102566
%v102574 = vadd.s32 %v102571, %v102566
%v102576 = vshll.u32 %v102571, 29
%v102577 = vshrl.u32 %v102571, 3
%v102578 = vor.u32 %v102577, %v102576
%v102579 = vxor.u32 %v102578, %v102574
%v102582 = vadd.s32 %v102579, %v102574
%v102584 = vshll.u32 %v102579, 16
%v102585 = vshrl.u32 %v102579, 16
%v102586 = vor.u32 %v102585, %v102584
%v102587 = vxor.u32 %v102586, %v102582
%v102590 = vadd.s32 %v102587, %v102582
%v102594 = vadd.s32 %v102590, %v8
%v102596 = vshll.u32 %v102587, 24
%v102597 = vshrl.u32 %v102587, 8
%v102598 = vor.u32 %v102597, %v102596
%v102599 = vxor.u32 %v102598, %v102590
%v102602 = vadd.s32 %v102599, %v10
%v102606 = vadd.s32 2, %v102602
%v102610 = vadd.s32 %v102606, %v102594
%v102612 = vshll.u32 %v102606, 13
%v102613 = vshrl.u32 %v102606, 19
%v102614 = vor.u32 %v102613, %v102612
%v102615 = vxor.u32 %v102614, %v102610
%v102618 = vadd.s32 %v102615, %v102610
%v102620 = vshll.u32 %v102615, 15
%v102621 = vshrl.u32 %v102615, 17
%v102622 = vor.u32 %v102621, %v102620
%v102623 = vxor.u32 %v102622, %v102618
%v102626 = vadd.s32 %v102623, %v102618
%v102628 = vshll.u32 %v102623, 26
%v102629 = vshrl.u32 %v102623, 6
%v102630 = vor.u32 %v102629, %v102628
%v102631 = vxor.u32 %v102630, %v102626
%v102634 = vadd.s32 %v102631, %v102626
%v102638 = vadd.s32 %v102634, %v10
%v102640 = vshll.u32 %v102631, 6
%v102641 = vshrl.u32 %v102631, 26
%v102642 = vor.u32 %v102641, %v102640
%v102643 = vxor.u32 %v102642, %v102634
%v102646 = vadd.s32 %v102643, %v9
%v102650 = vadd.s32 3, %v102646
%v102654 = vadd.s32 %v102650, %v102638
%v102656 = vshll.u32 %v102650, 17
%v102657 = vshrl.u32 %v102650, 15
%v102658 = vor.u32 %v102657, %v102656
%v102659 = vxor.u32 %v102658, %v102654
%v102662 = vadd.s32 %v102659, %v102654
%v102664 = vshll.u32 %v102659, 29
%v102665 = vshrl.u32 %v102659, 3
%v102666 = vor.u32 %v102665, %v102664
%v102667 = vxor.u32 %v102666, %v102662
%v102670 = vadd.s32 %v102667, %v102662
%v102672 = vshll.u32 %v102667, 16
%v102673 = vshrl.u32 %v102667, 16
%v102674 = vor.u32 %v102673, %v102672
%v102675 = vxor.u32 %v102674, %v102670
%v102678 = vadd.s32 %v102675, %v102670
%v102682 = vadd.s32 %v102678, %v9
%v102684 = vshll.u32 %v102675, 24
%v102685 = vshrl.u32 %v102675, 8
%v102686 = vor.u32 %v102685, %v102684
%v102687 = vxor.u32 %v102686, %v102678
%v102690 = vadd.s32 %v102687, %v8
%v102694 = vadd.s32 4, %v102690
%v102698 = vadd.s32 %v102694, %v102682
%v102700 = vshll.u32 %v102694, 13
%v102701 = vshrl.u32 %v102694, 19
%v102702 = vor.u32 %v102701, %v102700
%v102703 = vxor.u32 %v102702, %v102698
%v102706 = vadd.s32 %v102703, %v102698
%v102708 = vshll.u32 %v102703, 15
%v102709 = vshrl.u32 %v102703, 17
%v102710 = vor.u32 %v102709, %v102708
%v102711 = vxor.u32 %v102710, %v102706
%v102714 = vadd.s32 %v102711, %v102706
%v102716 = vshll.u32 %v102711, 26
%v102717 = vshrl.u32 %v102711, 6
%v102718 = vor.u32 %v102717, %v102716
%v102719 = vxor.u32 %v102718, %v102714
%v102722 = vadd.s32 %v102719, %v102714
%v102726 = vadd.s32 %v102722, %v8
%v102728 = vshll.u32 %v102719, 6
%v102729 = vshrl.u32 %v102719, 26
%v102730 = vor.u32 %v102729, %v102728
%v102731 = vxor.u32 %v102730, %v102722
%v102734 = vadd.s32 %v102731, %v10
%v102738 = vadd.s32 5, %v102734
%v102740 = vxor.u32 %v102738, %v102726
%v102741 = vand.u32.u8 255, %v102740
%v102742 = vand.u32 65535, %v102741
%v102743 = vshrl.u32 %v102742, 1
%v102744 = vor.u32 16256, %v102743
%v102745 = vand.u32.u16 65535, %v102744
%v120294 = vadd.low.f32.bf16 -1.0, %v102745
%v102754 = vmul.f32 2.0, %v120294
%v102758 = vadd.f32 -0.99609375, %v102754
%v102762 = vmax.f32 %v102758, -0.99609375
%v102764 = vand.u32 2147483647, %v102762
%vm102767 = vcmp.eq.f32.partialorder %v102764, 1.0
%v102772 = vmul.f32 inf, %v102762
%v102774 = vxor.u32 2147483648, %v102762
%v102777 = vmul.f32 %v102774, %v102762
%v102779 = vadd.f32 1.0, %v102777
%v102780 = vlog2.pop %v102779
%v102781 = vmul.f32 0.6931472, %v102780
%v102782 = vmul.f32 -0.5, %v102777
%v102783 = vadd.f32 1.0, %v102782
%v102784 = vmul.f32 %v102783, %v102777
%v102785 = vand.u32 2147483647, %v102777
%vm102786 = vcmp.lt.f32.partialorder %v102785, 0.0004427343
%v102787 = vsel /*vm=*/%vm102786, /*on_true_vy=*/%v102784, /*on_false_vx=*/%v102781
%v102788 = vxor.u32 2147483648, %v102787
%vm102791 = vcmp.lt.f32.partialorder %v102788, 5.0
%v102796 = vsel /*vm=*/%vm102791, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v102800 = vsel /*vm=*/%vm102791, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v102804 = vsel /*vm=*/%vm102791, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v102808 = vsel /*vm=*/%vm102791, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v102812 = vsel /*vm=*/%vm102791, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v102816 = vsel /*vm=*/%vm102791, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v102820 = vsel /*vm=*/%vm102791, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v102824 = vsel /*vm=*/%vm102791, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v102828 = vsel /*vm=*/%vm102791, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v102832 = vadd.f32 -2.5, %v102788
%v102834 = vrsqrt.pop %v102788
%v102835 = vmul.f32 %v102834, %v102788
%vm102836 = vcmp.eq.f32.partialorder %v102788, inf
%v102837 = vsel /*vm=*/%vm102836, /*on_true_vy=*/%v102788, /*on_false_vx=*/%v102835
%vm102838 = vcmp.eq.f32.partialorder %v102788, 0.0
%v102839 = vand.u32 2147483648, %v102788
%v102840 = vsel /*vm=*/%vm102838, /*on_true_vy=*/%v102839, /*on_false_vx=*/%v102837
%v102843 = vadd.f32 -3.0, %v102840
%v102847 = vsel /*vm=*/%vm102791, /*on_true_vy=*/%v102832, /*on_false_vx=*/%v102843
%v102851 = vmul.f32 %v102847, %v102828
%v102855 = vadd.f32 %v102851, %v102824
%v102859 = vmul.f32 %v102855, %v102847
%v102863 = vadd.f32 %v102859, %v102820
%v102867 = vmul.f32 %v102863, %v102847
%v102871 = vadd.f32 %v102867, %v102816
%v102875 = vmul.f32 %v102871, %v102847
%v102879 = vadd.f32 %v102875, %v102812
%v102883 = vmul.f32 %v102879, %v102847
%v102887 = vadd.f32 %v102883, %v102808
%v102891 = vmul.f32 %v102887, %v102847
%v102895 = vadd.f32 %v102891, %v102804
%v102899 = vmul.f32 %v102895, %v102847
%v102903 = vadd.f32 %v102899, %v102800
%v102907 = vmul.f32 %v102903, %v102847
%v102911 = vadd.f32 %v102907, %v102796
%v102915 = vmul.f32 %v102911, %v102762
%v102919 = vsel /*vm=*/%vm102767, /*on_true_vy=*/%v102772, /*on_false_vx=*/%v102915
%v102923 = vmul.f32 1.4140625, %v102919
%v102926 = vpack.c.bf16 %v120417, %v102923
%120295 = vst [vmem:[%s280 + $0x1ec] sm:$0xf] /*vst_source=*/%v102926
%v102930 = vadd.s32 %v101083, %v2355
%v102940 = vadd.s32 %v102930, %v415
%vm102944 = vcmp.lt.u32.totalorder %v102940, %v102930
%vm102949 = vcmp.lt.u32.totalorder %v102930, %v2355
%v102954 = vadd.s32 %v101066, %v2342
%v102958 = vadd.s32 1, %v102954
%v102962 = vsel /*vm=*/%vm102949, /*on_true_vy=*/%v102958, /*on_false_vx=*/%v102954
%v102966 = vadd.s32 1, %v102962
%v102970 = vsel /*vm=*/%vm102944, /*on_true_vy=*/%v102966, /*on_false_vx=*/%v102962
%v102975 = vadd.s32 %v102970, %v10
%v102979 = vadd.s32 %v102940, %v9
%v102983 = vadd.s32 %v102979, %v102975
%v102985 = vshll.u32 %v102979, 13
%v102986 = vshrl.u32 %v102979, 19
%v102987 = vor.u32 %v102986, %v102985
%v102988 = vxor.u32 %v102987, %v102983
%v102991 = vadd.s32 %v102988, %v102983
%v102993 = vshll.u32 %v102988, 15
%v102994 = vshrl.u32 %v102988, 17
%v102995 = vor.u32 %v102994, %v102993
%v102996 = vxor.u32 %v102995, %v102991
%v102999 = vadd.s32 %v102996, %v102991
%v103001 = vshll.u32 %v102996, 26
%v103002 = vshrl.u32 %v102996, 6
%v103003 = vor.u32 %v103002, %v103001
%v103004 = vxor.u32 %v103003, %v102999
%v103007 = vadd.s32 %v103004, %v102999
%v103011 = vadd.s32 %v103007, %v9
%v103013 = vshll.u32 %v103004, 6
%v103014 = vshrl.u32 %v103004, 26
%v103015 = vor.u32 %v103014, %v103013
%v103016 = vxor.u32 %v103015, %v103007
%v103019 = vadd.s32 %v103016, %v8
%v103023 = vadd.s32 1, %v103019
%v103027 = vadd.s32 %v103023, %v103011
%v103029 = vshll.u32 %v103023, 17
%v103030 = vshrl.u32 %v103023, 15
%v103031 = vor.u32 %v103030, %v103029
%v103032 = vxor.u32 %v103031, %v103027
%v103035 = vadd.s32 %v103032, %v103027
%v103037 = vshll.u32 %v103032, 29
%v103038 = vshrl.u32 %v103032, 3
%v103039 = vor.u32 %v103038, %v103037
%v103040 = vxor.u32 %v103039, %v103035
%v103043 = vadd.s32 %v103040, %v103035
%v103045 = vshll.u32 %v103040, 16
%v103046 = vshrl.u32 %v103040, 16
%v103047 = vor.u32 %v103046, %v103045
%v103048 = vxor.u32 %v103047, %v103043
%v103051 = vadd.s32 %v103048, %v103043
%v103055 = vadd.s32 %v103051, %v8
%v103057 = vshll.u32 %v103048, 24
%v103058 = vshrl.u32 %v103048, 8
%v103059 = vor.u32 %v103058, %v103057
%v103060 = vxor.u32 %v103059, %v103051
%v103063 = vadd.s32 %v103060, %v10
%v103067 = vadd.s32 2, %v103063
%v103071 = vadd.s32 %v103067, %v103055
%v103073 = vshll.u32 %v103067, 13
%v103074 = vshrl.u32 %v103067, 19
%v103075 = vor.u32 %v103074, %v103073
%v103076 = vxor.u32 %v103075, %v103071
%v103079 = vadd.s32 %v103076, %v103071
%v103081 = vshll.u32 %v103076, 15
%v103082 = vshrl.u32 %v103076, 17
%v103083 = vor.u32 %v103082, %v103081
%v103084 = vxor.u32 %v103083, %v103079
%v103087 = vadd.s32 %v103084, %v103079
%v103089 = vshll.u32 %v103084, 26
%v103090 = vshrl.u32 %v103084, 6
%v103091 = vor.u32 %v103090, %v103089
%v103092 = vxor.u32 %v103091, %v103087
%v103095 = vadd.s32 %v103092, %v103087
%v103099 = vadd.s32 %v103095, %v10
%v103101 = vshll.u32 %v103092, 6
%v103102 = vshrl.u32 %v103092, 26
%v103103 = vor.u32 %v103102, %v103101
%v103104 = vxor.u32 %v103103, %v103095
%v103107 = vadd.s32 %v103104, %v9
%v103111 = vadd.s32 3, %v103107
%v103115 = vadd.s32 %v103111, %v103099
%v103117 = vshll.u32 %v103111, 17
%v103118 = vshrl.u32 %v103111, 15
%v103119 = vor.u32 %v103118, %v103117
%v103120 = vxor.u32 %v103119, %v103115
%v103123 = vadd.s32 %v103120, %v103115
%v103125 = vshll.u32 %v103120, 29
%v103126 = vshrl.u32 %v103120, 3
%v103127 = vor.u32 %v103126, %v103125
%v103128 = vxor.u32 %v103127, %v103123
%v103131 = vadd.s32 %v103128, %v103123
%v103133 = vshll.u32 %v103128, 16
%v103134 = vshrl.u32 %v103128, 16
%v103135 = vor.u32 %v103134, %v103133
%v103136 = vxor.u32 %v103135, %v103131
%v103139 = vadd.s32 %v103136, %v103131
%v103143 = vadd.s32 %v103139, %v9
%v103145 = vshll.u32 %v103136, 24
%v103146 = vshrl.u32 %v103136, 8
%v103147 = vor.u32 %v103146, %v103145
%v103148 = vxor.u32 %v103147, %v103139
%v103151 = vadd.s32 %v103148, %v8
%v103155 = vadd.s32 4, %v103151
%v103159 = vadd.s32 %v103155, %v103143
%v103161 = vshll.u32 %v103155, 13
%v103162 = vshrl.u32 %v103155, 19
%v103163 = vor.u32 %v103162, %v103161
%v103164 = vxor.u32 %v103163, %v103159
%v103167 = vadd.s32 %v103164, %v103159
%v103169 = vshll.u32 %v103164, 15
%v103170 = vshrl.u32 %v103164, 17
%v103171 = vor.u32 %v103170, %v103169
%v103172 = vxor.u32 %v103171, %v103167
%v103175 = vadd.s32 %v103172, %v103167
%v103177 = vshll.u32 %v103172, 26
%v103178 = vshrl.u32 %v103172, 6
%v103179 = vor.u32 %v103178, %v103177
%v103180 = vxor.u32 %v103179, %v103175
%v103183 = vadd.s32 %v103180, %v103175
%v103187 = vadd.s32 %v103183, %v8
%v103189 = vshll.u32 %v103180, 6
%v103190 = vshrl.u32 %v103180, 26
%v103191 = vor.u32 %v103190, %v103189
%v103192 = vxor.u32 %v103191, %v103183
%v103195 = vadd.s32 %v103192, %v10
%v103199 = vadd.s32 5, %v103195
%v103201 = vxor.u32 %v103199, %v103187
%v103202 = vand.u32.u8 255, %v103201
%v103203 = vand.u32 65535, %v103202
%v103204 = vshrl.u32 %v103203, 1
%v103205 = vor.u32 16256, %v103204
%v103206 = vand.u32.u16 65535, %v103205
%v120296 = vadd.low.f32.bf16 -1.0, %v103206
%v103215 = vmul.f32 2.0, %v120296
%v103219 = vadd.f32 -0.99609375, %v103215
%v103223 = vmax.f32 %v103219, -0.99609375
%v103225 = vand.u32 2147483647, %v103223
%vm103228 = vcmp.eq.f32.partialorder %v103225, 1.0
%v103233 = vmul.f32 inf, %v103223
%v103235 = vxor.u32 2147483648, %v103223
%v103238 = vmul.f32 %v103235, %v103223
%v103240 = vadd.f32 1.0, %v103238
%v103241 = vlog2.pop %v103240
%v103242 = vmul.f32 0.6931472, %v103241
%v103243 = vmul.f32 -0.5, %v103238
%v103244 = vadd.f32 1.0, %v103243
%v103245 = vmul.f32 %v103244, %v103238
%v103246 = vand.u32 2147483647, %v103238
%vm103247 = vcmp.lt.f32.partialorder %v103246, 0.0004427343
%v103248 = vsel /*vm=*/%vm103247, /*on_true_vy=*/%v103245, /*on_false_vx=*/%v103242
%v103249 = vxor.u32 2147483648, %v103248
%vm103252 = vcmp.lt.f32.partialorder %v103249, 5.0
%v103257 = vsel /*vm=*/%vm103252, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v103261 = vsel /*vm=*/%vm103252, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v103265 = vsel /*vm=*/%vm103252, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v103269 = vsel /*vm=*/%vm103252, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v103273 = vsel /*vm=*/%vm103252, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v103277 = vsel /*vm=*/%vm103252, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v103281 = vsel /*vm=*/%vm103252, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v103285 = vsel /*vm=*/%vm103252, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v103289 = vsel /*vm=*/%vm103252, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v103293 = vadd.f32 -2.5, %v103249
%v103295 = vrsqrt.pop %v103249
%v103296 = vmul.f32 %v103295, %v103249
%vm103297 = vcmp.eq.f32.partialorder %v103249, inf
%v103298 = vsel /*vm=*/%vm103297, /*on_true_vy=*/%v103249, /*on_false_vx=*/%v103296
%vm103299 = vcmp.eq.f32.partialorder %v103249, 0.0
%v103300 = vand.u32 2147483648, %v103249
%v103301 = vsel /*vm=*/%vm103299, /*on_true_vy=*/%v103300, /*on_false_vx=*/%v103298
%v103304 = vadd.f32 -3.0, %v103301
%v103308 = vsel /*vm=*/%vm103252, /*on_true_vy=*/%v103293, /*on_false_vx=*/%v103304
%v103312 = vmul.f32 %v103308, %v103289
%v103316 = vadd.f32 %v103312, %v103285
%v103320 = vmul.f32 %v103316, %v103308
%v103324 = vadd.f32 %v103320, %v103281
%v103328 = vmul.f32 %v103324, %v103308
%v103332 = vadd.f32 %v103328, %v103277
%v103336 = vmul.f32 %v103332, %v103308
%v103340 = vadd.f32 %v103336, %v103273
%v103344 = vmul.f32 %v103340, %v103308
%v103348 = vadd.f32 %v103344, %v103269
%v103352 = vmul.f32 %v103348, %v103308
%v103356 = vadd.f32 %v103352, %v103265
%v103360 = vmul.f32 %v103356, %v103308
%v103364 = vadd.f32 %v103360, %v103261
%v103368 = vmul.f32 %v103364, %v103308
%v103372 = vadd.f32 %v103368, %v103257
%v103376 = vmul.f32 %v103372, %v103223
%v103380 = vsel /*vm=*/%vm103228, /*on_true_vy=*/%v103233, /*on_false_vx=*/%v103376
%v103384 = vmul.f32 1.4140625, %v103380
%v103387 = vpack.c.bf16 %v120417, %v103384
%120297 = vst [vmem:[%s280 + $0x26c] sm:$0xf] /*vst_source=*/%v103387
%v103391 = vadd.s32 %v101083, %v2842
%v103401 = vadd.s32 %v103391, %v415
%vm103405 = vcmp.lt.u32.totalorder %v103401, %v103391
%vm103410 = vcmp.lt.u32.totalorder %v103391, %v2842
%v103415 = vadd.s32 %v101066, %v2829
%v103419 = vadd.s32 1, %v103415
%v103423 = vsel /*vm=*/%vm103410, /*on_true_vy=*/%v103419, /*on_false_vx=*/%v103415
%v103427 = vadd.s32 1, %v103423
%v103431 = vsel /*vm=*/%vm103405, /*on_true_vy=*/%v103427, /*on_false_vx=*/%v103423
%v103436 = vadd.s32 %v103431, %v10
%v103440 = vadd.s32 %v103401, %v9
%v103444 = vadd.s32 %v103440, %v103436
%v103446 = vshll.u32 %v103440, 13
%v103447 = vshrl.u32 %v103440, 19
%v103448 = vor.u32 %v103447, %v103446
%v103449 = vxor.u32 %v103448, %v103444
%v103452 = vadd.s32 %v103449, %v103444
%v103454 = vshll.u32 %v103449, 15
%v103455 = vshrl.u32 %v103449, 17
%v103456 = vor.u32 %v103455, %v103454
%v103457 = vxor.u32 %v103456, %v103452
%v103460 = vadd.s32 %v103457, %v103452
%v103462 = vshll.u32 %v103457, 26
%v103463 = vshrl.u32 %v103457, 6
%v103464 = vor.u32 %v103463, %v103462
%v103465 = vxor.u32 %v103464, %v103460
%v103468 = vadd.s32 %v103465, %v103460
%v103472 = vadd.s32 %v103468, %v9
%v103474 = vshll.u32 %v103465, 6
%v103475 = vshrl.u32 %v103465, 26
%v103476 = vor.u32 %v103475, %v103474
%v103477 = vxor.u32 %v103476, %v103468
%v103480 = vadd.s32 %v103477, %v8
%v103484 = vadd.s32 1, %v103480
%v103488 = vadd.s32 %v103484, %v103472
%v103490 = vshll.u32 %v103484, 17
%v103491 = vshrl.u32 %v103484, 15
%v103492 = vor.u32 %v103491, %v103490
%v103493 = vxor.u32 %v103492, %v103488
%v103496 = vadd.s32 %v103493, %v103488
%v103498 = vshll.u32 %v103493, 29
%v103499 = vshrl.u32 %v103493, 3
%v103500 = vor.u32 %v103499, %v103498
%v103501 = vxor.u32 %v103500, %v103496
%v103504 = vadd.s32 %v103501, %v103496
%v103506 = vshll.u32 %v103501, 16
%v103507 = vshrl.u32 %v103501, 16
%v103508 = vor.u32 %v103507, %v103506
%v103509 = vxor.u32 %v103508, %v103504
%v103512 = vadd.s32 %v103509, %v103504
%v103516 = vadd.s32 %v103512, %v8
%v103518 = vshll.u32 %v103509, 24
%v103519 = vshrl.u32 %v103509, 8
%v103520 = vor.u32 %v103519, %v103518
%v103521 = vxor.u32 %v103520, %v103512
%v103524 = vadd.s32 %v103521, %v10
%v103528 = vadd.s32 2, %v103524
%v103532 = vadd.s32 %v103528, %v103516
%v103534 = vshll.u32 %v103528, 13
%v103535 = vshrl.u32 %v103528, 19
%v103536 = vor.u32 %v103535, %v103534
%v103537 = vxor.u32 %v103536, %v103532
%v103540 = vadd.s32 %v103537, %v103532
%v103542 = vshll.u32 %v103537, 15
%v103543 = vshrl.u32 %v103537, 17
%v103544 = vor.u32 %v103543, %v103542
%v103545 = vxor.u32 %v103544, %v103540
%v103548 = vadd.s32 %v103545, %v103540
%v103550 = vshll.u32 %v103545, 26
%v103551 = vshrl.u32 %v103545, 6
%v103552 = vor.u32 %v103551, %v103550
%v103553 = vxor.u32 %v103552, %v103548
%v103556 = vadd.s32 %v103553, %v103548
%v103560 = vadd.s32 %v103556, %v10
%v103562 = vshll.u32 %v103553, 6
%v103563 = vshrl.u32 %v103553, 26
%v103564 = vor.u32 %v103563, %v103562
%v103565 = vxor.u32 %v103564, %v103556
%v103568 = vadd.s32 %v103565, %v9
%v103572 = vadd.s32 3, %v103568
%v103576 = vadd.s32 %v103572, %v103560
%v103578 = vshll.u32 %v103572, 17
%v103579 = vshrl.u32 %v103572, 15
%v103580 = vor.u32 %v103579, %v103578
%v103581 = vxor.u32 %v103580, %v103576
%v103584 = vadd.s32 %v103581, %v103576
%v103586 = vshll.u32 %v103581, 29
%v103587 = vshrl.u32 %v103581, 3
%v103588 = vor.u32 %v103587, %v103586
%v103589 = vxor.u32 %v103588, %v103584
%v103592 = vadd.s32 %v103589, %v103584
%v103594 = vshll.u32 %v103589, 16
%v103595 = vshrl.u32 %v103589, 16
%v103596 = vor.u32 %v103595, %v103594
%v103597 = vxor.u32 %v103596, %v103592
%v103600 = vadd.s32 %v103597, %v103592
%v103604 = vadd.s32 %v103600, %v9
%v103606 = vshll.u32 %v103597, 24
%v103607 = vshrl.u32 %v103597, 8
%v103608 = vor.u32 %v103607, %v103606
%v103609 = vxor.u32 %v103608, %v103600
%v103612 = vadd.s32 %v103609, %v8
%v103616 = vadd.s32 4, %v103612
%v103620 = vadd.s32 %v103616, %v103604
%v103622 = vshll.u32 %v103616, 13
%v103623 = vshrl.u32 %v103616, 19
%v103624 = vor.u32 %v103623, %v103622
%v103625 = vxor.u32 %v103624, %v103620
%v103628 = vadd.s32 %v103625, %v103620
%v103630 = vshll.u32 %v103625, 15
%v103631 = vshrl.u32 %v103625, 17
%v103632 = vor.u32 %v103631, %v103630
%v103633 = vxor.u32 %v103632, %v103628
%v103636 = vadd.s32 %v103633, %v103628
%v103638 = vshll.u32 %v103633, 26
%v103639 = vshrl.u32 %v103633, 6
%v103640 = vor.u32 %v103639, %v103638
%v103641 = vxor.u32 %v103640, %v103636
%v103644 = vadd.s32 %v103641, %v103636
%v103648 = vadd.s32 %v103644, %v8
%v103650 = vshll.u32 %v103641, 6
%v103651 = vshrl.u32 %v103641, 26
%v103652 = vor.u32 %v103651, %v103650
%v103653 = vxor.u32 %v103652, %v103644
%v103656 = vadd.s32 %v103653, %v10
%v103660 = vadd.s32 5, %v103656
%v103662 = vxor.u32 %v103660, %v103648
%v103663 = vand.u32.u8 255, %v103662
%v103664 = vand.u32 65535, %v103663
%v103665 = vshrl.u32 %v103664, 1
%v103666 = vor.u32 16256, %v103665
%v103667 = vand.u32.u16 65535, %v103666
%v120298 = vadd.low.f32.bf16 -1.0, %v103667
%v103676 = vmul.f32 2.0, %v120298
%v103680 = vadd.f32 -0.99609375, %v103676
%v103684 = vmax.f32 %v103680, -0.99609375
%v103686 = vand.u32 2147483647, %v103684
%vm103689 = vcmp.eq.f32.partialorder %v103686, 1.0
%v103694 = vmul.f32 inf, %v103684
%v103696 = vxor.u32 2147483648, %v103684
%v103699 = vmul.f32 %v103696, %v103684
%v103701 = vadd.f32 1.0, %v103699
%v103702 = vlog2.pop %v103701
%v103703 = vmul.f32 0.6931472, %v103702
%v103704 = vmul.f32 -0.5, %v103699
%v103705 = vadd.f32 1.0, %v103704
%v103706 = vmul.f32 %v103705, %v103699
%v103707 = vand.u32 2147483647, %v103699
%vm103708 = vcmp.lt.f32.partialorder %v103707, 0.0004427343
%v103709 = vsel /*vm=*/%vm103708, /*on_true_vy=*/%v103706, /*on_false_vx=*/%v103703
%v103710 = vxor.u32 2147483648, %v103709
%vm103713 = vcmp.lt.f32.partialorder %v103710, 5.0
%v103718 = vsel /*vm=*/%vm103713, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v103722 = vsel /*vm=*/%vm103713, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v103726 = vsel /*vm=*/%vm103713, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v103730 = vsel /*vm=*/%vm103713, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v103734 = vsel /*vm=*/%vm103713, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v103738 = vsel /*vm=*/%vm103713, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v103742 = vsel /*vm=*/%vm103713, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v103746 = vsel /*vm=*/%vm103713, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v103750 = vsel /*vm=*/%vm103713, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v103754 = vadd.f32 -2.5, %v103710
%v103756 = vrsqrt.pop %v103710
%v103757 = vmul.f32 %v103756, %v103710
%vm103758 = vcmp.eq.f32.partialorder %v103710, inf
%v103759 = vsel /*vm=*/%vm103758, /*on_true_vy=*/%v103710, /*on_false_vx=*/%v103757
%vm103760 = vcmp.eq.f32.partialorder %v103710, 0.0
%v103761 = vand.u32 2147483648, %v103710
%v103762 = vsel /*vm=*/%vm103760, /*on_true_vy=*/%v103761, /*on_false_vx=*/%v103759
%v103765 = vadd.f32 -3.0, %v103762
%v103769 = vsel /*vm=*/%vm103713, /*on_true_vy=*/%v103754, /*on_false_vx=*/%v103765
%v103773 = vmul.f32 %v103769, %v103750
%v103777 = vadd.f32 %v103773, %v103746
%v103781 = vmul.f32 %v103777, %v103769
%v103785 = vadd.f32 %v103781, %v103742
%v103789 = vmul.f32 %v103785, %v103769
%v103793 = vadd.f32 %v103789, %v103738
%v103797 = vmul.f32 %v103793, %v103769
%v103801 = vadd.f32 %v103797, %v103734
%v103805 = vmul.f32 %v103801, %v103769
%v103809 = vadd.f32 %v103805, %v103730
%v103813 = vmul.f32 %v103809, %v103769
%v103817 = vadd.f32 %v103813, %v103726
%v103821 = vmul.f32 %v103817, %v103769
%v103825 = vadd.f32 %v103821, %v103722
%v103829 = vmul.f32 %v103825, %v103769
%v103833 = vadd.f32 %v103829, %v103718
%v103837 = vmul.f32 %v103833, %v103684
%v103841 = vsel /*vm=*/%vm103689, /*on_true_vy=*/%v103694, /*on_false_vx=*/%v103837
%v103845 = vmul.f32 1.4140625, %v103841
%v103848 = vpack.c.bf16 %v120417, %v103845
%120299 = vst [vmem:[%s280 + $0x2ec] sm:$0xf] /*vst_source=*/%v103848
%v103852 = vadd.s32 %v101083, %v3329
%v103862 = vadd.s32 %v103852, %v415
%vm103866 = vcmp.lt.u32.totalorder %v103862, %v103852
%vm103871 = vcmp.lt.u32.totalorder %v103852, %v3329
%v103876 = vadd.s32 %v101066, %v3316
%v103880 = vadd.s32 1, %v103876
%v103884 = vsel /*vm=*/%vm103871, /*on_true_vy=*/%v103880, /*on_false_vx=*/%v103876
%v103888 = vadd.s32 1, %v103884
%v103892 = vsel /*vm=*/%vm103866, /*on_true_vy=*/%v103888, /*on_false_vx=*/%v103884
%v103897 = vadd.s32 %v103892, %v10
%v103901 = vadd.s32 %v103862, %v9
%v103905 = vadd.s32 %v103901, %v103897
%v103907 = vshll.u32 %v103901, 13
%v103908 = vshrl.u32 %v103901, 19
%v103909 = vor.u32 %v103908, %v103907
%v103910 = vxor.u32 %v103909, %v103905
%v103913 = vadd.s32 %v103910, %v103905
%v103915 = vshll.u32 %v103910, 15
%v103916 = vshrl.u32 %v103910, 17
%v103917 = vor.u32 %v103916, %v103915
%v103918 = vxor.u32 %v103917, %v103913
%v103921 = vadd.s32 %v103918, %v103913
%v103923 = vshll.u32 %v103918, 26
%v103924 = vshrl.u32 %v103918, 6
%v103925 = vor.u32 %v103924, %v103923
%v103926 = vxor.u32 %v103925, %v103921
%v103929 = vadd.s32 %v103926, %v103921
%v103933 = vadd.s32 %v103929, %v9
%v103935 = vshll.u32 %v103926, 6
%v103936 = vshrl.u32 %v103926, 26
%v103937 = vor.u32 %v103936, %v103935
%v103938 = vxor.u32 %v103937, %v103929
%v103941 = vadd.s32 %v103938, %v8
%v103945 = vadd.s32 1, %v103941
%v103949 = vadd.s32 %v103945, %v103933
%v103951 = vshll.u32 %v103945, 17
%v103952 = vshrl.u32 %v103945, 15
%v103953 = vor.u32 %v103952, %v103951
%v103954 = vxor.u32 %v103953, %v103949
%v103957 = vadd.s32 %v103954, %v103949
%v103959 = vshll.u32 %v103954, 29
%v103960 = vshrl.u32 %v103954, 3
%v103961 = vor.u32 %v103960, %v103959
%v103962 = vxor.u32 %v103961, %v103957
%v103965 = vadd.s32 %v103962, %v103957
%v103967 = vshll.u32 %v103962, 16
%v103968 = vshrl.u32 %v103962, 16
%v103969 = vor.u32 %v103968, %v103967
%v103970 = vxor.u32 %v103969, %v103965
%v103973 = vadd.s32 %v103970, %v103965
%v103977 = vadd.s32 %v103973, %v8
%v103979 = vshll.u32 %v103970, 24
%v103980 = vshrl.u32 %v103970, 8
%v103981 = vor.u32 %v103980, %v103979
%v103982 = vxor.u32 %v103981, %v103973
%v103985 = vadd.s32 %v103982, %v10
%v103989 = vadd.s32 2, %v103985
%v103993 = vadd.s32 %v103989, %v103977
%v103995 = vshll.u32 %v103989, 13
%v103996 = vshrl.u32 %v103989, 19
%v103997 = vor.u32 %v103996, %v103995
%v103998 = vxor.u32 %v103997, %v103993
%v104001 = vadd.s32 %v103998, %v103993
%v104003 = vshll.u32 %v103998, 15
%v104004 = vshrl.u32 %v103998, 17
%v104005 = vor.u32 %v104004, %v104003
%v104006 = vxor.u32 %v104005, %v104001
%v104009 = vadd.s32 %v104006, %v104001
%v104011 = vshll.u32 %v104006, 26
%v104012 = vshrl.u32 %v104006, 6
%v104013 = vor.u32 %v104012, %v104011
%v104014 = vxor.u32 %v104013, %v104009
%v104017 = vadd.s32 %v104014, %v104009
%v104021 = vadd.s32 %v104017, %v10
%v104023 = vshll.u32 %v104014, 6
%v104024 = vshrl.u32 %v104014, 26
%v104025 = vor.u32 %v104024, %v104023
%v104026 = vxor.u32 %v104025, %v104017
%v104029 = vadd.s32 %v104026, %v9
%v104033 = vadd.s32 3, %v104029
%v104037 = vadd.s32 %v104033, %v104021
%v104039 = vshll.u32 %v104033, 17
%v104040 = vshrl.u32 %v104033, 15
%v104041 = vor.u32 %v104040, %v104039
%v104042 = vxor.u32 %v104041, %v104037
%v104045 = vadd.s32 %v104042, %v104037
%v104047 = vshll.u32 %v104042, 29
%v104048 = vshrl.u32 %v104042, 3
%v104049 = vor.u32 %v104048, %v104047
%v104050 = vxor.u32 %v104049, %v104045
%v104053 = vadd.s32 %v104050, %v104045
%v104055 = vshll.u32 %v104050, 16
%v104056 = vshrl.u32 %v104050, 16
%v104057 = vor.u32 %v104056, %v104055
%v104058 = vxor.u32 %v104057, %v104053
%v104061 = vadd.s32 %v104058, %v104053
%v104065 = vadd.s32 %v104061, %v9
%v104067 = vshll.u32 %v104058, 24
%v104068 = vshrl.u32 %v104058, 8
%v104069 = vor.u32 %v104068, %v104067
%v104070 = vxor.u32 %v104069, %v104061
%v104073 = vadd.s32 %v104070, %v8
%v104077 = vadd.s32 4, %v104073
%v104081 = vadd.s32 %v104077, %v104065
%v104083 = vshll.u32 %v104077, 13
%v104084 = vshrl.u32 %v104077, 19
%v104085 = vor.u32 %v104084, %v104083
%v104086 = vxor.u32 %v104085, %v104081
%v104089 = vadd.s32 %v104086, %v104081
%v104091 = vshll.u32 %v104086, 15
%v104092 = vshrl.u32 %v104086, 17
%v104093 = vor.u32 %v104092, %v104091
%v104094 = vxor.u32 %v104093, %v104089
%v104097 = vadd.s32 %v104094, %v104089
%v104099 = vshll.u32 %v104094, 26
%v104100 = vshrl.u32 %v104094, 6
%v104101 = vor.u32 %v104100, %v104099
%v104102 = vxor.u32 %v104101, %v104097
%v104105 = vadd.s32 %v104102, %v104097
%v104109 = vadd.s32 %v104105, %v8
%v104111 = vshll.u32 %v104102, 6
%v104112 = vshrl.u32 %v104102, 26
%v104113 = vor.u32 %v104112, %v104111
%v104114 = vxor.u32 %v104113, %v104105
%v104117 = vadd.s32 %v104114, %v10
%v104121 = vadd.s32 5, %v104117
%v104123 = vxor.u32 %v104121, %v104109
%v104124 = vand.u32.u8 255, %v104123
%v104125 = vand.u32 65535, %v104124
%v104126 = vshrl.u32 %v104125, 1
%v104127 = vor.u32 16256, %v104126
%v104128 = vand.u32.u16 65535, %v104127
%v120300 = vadd.low.f32.bf16 -1.0, %v104128
%v104137 = vmul.f32 2.0, %v120300
%v104141 = vadd.f32 -0.99609375, %v104137
%v104145 = vmax.f32 %v104141, -0.99609375
%v104147 = vand.u32 2147483647, %v104145
%vm104150 = vcmp.eq.f32.partialorder %v104147, 1.0
%v104155 = vmul.f32 inf, %v104145
%v104157 = vxor.u32 2147483648, %v104145
%v104160 = vmul.f32 %v104157, %v104145
%v104162 = vadd.f32 1.0, %v104160
%v104163 = vlog2.pop %v104162
%v104164 = vmul.f32 0.6931472, %v104163
%v104165 = vmul.f32 -0.5, %v104160
%v104166 = vadd.f32 1.0, %v104165
%v104167 = vmul.f32 %v104166, %v104160
%v104168 = vand.u32 2147483647, %v104160
%vm104169 = vcmp.lt.f32.partialorder %v104168, 0.0004427343
%v104170 = vsel /*vm=*/%vm104169, /*on_true_vy=*/%v104167, /*on_false_vx=*/%v104164
%v104171 = vxor.u32 2147483648, %v104170
%vm104174 = vcmp.lt.f32.partialorder %v104171, 5.0
%v104179 = vsel /*vm=*/%vm104174, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v104183 = vsel /*vm=*/%vm104174, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v104187 = vsel /*vm=*/%vm104174, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v104191 = vsel /*vm=*/%vm104174, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v104195 = vsel /*vm=*/%vm104174, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v104199 = vsel /*vm=*/%vm104174, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v104203 = vsel /*vm=*/%vm104174, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v104207 = vsel /*vm=*/%vm104174, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v104211 = vsel /*vm=*/%vm104174, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v104215 = vadd.f32 -2.5, %v104171
%v104217 = vrsqrt.pop %v104171
%v104218 = vmul.f32 %v104217, %v104171
%vm104219 = vcmp.eq.f32.partialorder %v104171, inf
%v104220 = vsel /*vm=*/%vm104219, /*on_true_vy=*/%v104171, /*on_false_vx=*/%v104218
%vm104221 = vcmp.eq.f32.partialorder %v104171, 0.0
%v104222 = vand.u32 2147483648, %v104171
%v104223 = vsel /*vm=*/%vm104221, /*on_true_vy=*/%v104222, /*on_false_vx=*/%v104220
%v104226 = vadd.f32 -3.0, %v104223
%v104230 = vsel /*vm=*/%vm104174, /*on_true_vy=*/%v104215, /*on_false_vx=*/%v104226
%v104234 = vmul.f32 %v104230, %v104211
%v104238 = vadd.f32 %v104234, %v104207
%v104242 = vmul.f32 %v104238, %v104230
%v104246 = vadd.f32 %v104242, %v104203
%v104250 = vmul.f32 %v104246, %v104230
%v104254 = vadd.f32 %v104250, %v104199
%v104258 = vmul.f32 %v104254, %v104230
%v104262 = vadd.f32 %v104258, %v104195
%v104266 = vmul.f32 %v104262, %v104230
%v104270 = vadd.f32 %v104266, %v104191
%v104274 = vmul.f32 %v104270, %v104230
%v104278 = vadd.f32 %v104274, %v104187
%v104282 = vmul.f32 %v104278, %v104230
%v104286 = vadd.f32 %v104282, %v104183
%v104290 = vmul.f32 %v104286, %v104230
%v104294 = vadd.f32 %v104290, %v104179
%v104298 = vmul.f32 %v104294, %v104145
%v104302 = vsel /*vm=*/%vm104150, /*on_true_vy=*/%v104155, /*on_false_vx=*/%v104298
%v104306 = vmul.f32 1.4140625, %v104302
%v104309 = vpack.c.bf16 %v120417, %v104306
%120301 = vst [vmem:[%s280 + $0x36c] sm:$0xf] /*vst_source=*/%v104309
%v104313 = vadd.s32 %v101083, %v3816
%v104323 = vadd.s32 %v104313, %v415
%vm104327 = vcmp.lt.u32.totalorder %v104323, %v104313
%vm104332 = vcmp.lt.u32.totalorder %v104313, %v3816
%v104337 = vadd.s32 %v101066, %v3803
%v104341 = vadd.s32 1, %v104337
%v104345 = vsel /*vm=*/%vm104332, /*on_true_vy=*/%v104341, /*on_false_vx=*/%v104337
%v104349 = vadd.s32 1, %v104345
%v104353 = vsel /*vm=*/%vm104327, /*on_true_vy=*/%v104349, /*on_false_vx=*/%v104345
%v104358 = vadd.s32 %v104353, %v10
%v104362 = vadd.s32 %v104323, %v9
%v104366 = vadd.s32 %v104362, %v104358
%v104368 = vshll.u32 %v104362, 13
%v104369 = vshrl.u32 %v104362, 19
%v104370 = vor.u32 %v104369, %v104368
%v104371 = vxor.u32 %v104370, %v104366
%v104374 = vadd.s32 %v104371, %v104366
%v104376 = vshll.u32 %v104371, 15
%v104377 = vshrl.u32 %v104371, 17
%v104378 = vor.u32 %v104377, %v104376
%v104379 = vxor.u32 %v104378, %v104374
%v104382 = vadd.s32 %v104379, %v104374
%v104384 = vshll.u32 %v104379, 26
%v104385 = vshrl.u32 %v104379, 6
%v104386 = vor.u32 %v104385, %v104384
%v104387 = vxor.u32 %v104386, %v104382
%v104390 = vadd.s32 %v104387, %v104382
%v104394 = vadd.s32 %v104390, %v9
%v104396 = vshll.u32 %v104387, 6
%v104397 = vshrl.u32 %v104387, 26
%v104398 = vor.u32 %v104397, %v104396
%v104399 = vxor.u32 %v104398, %v104390
%v104402 = vadd.s32 %v104399, %v8
%v104406 = vadd.s32 1, %v104402
%v104410 = vadd.s32 %v104406, %v104394
%v104412 = vshll.u32 %v104406, 17
%v104413 = vshrl.u32 %v104406, 15
%v104414 = vor.u32 %v104413, %v104412
%v104415 = vxor.u32 %v104414, %v104410
%v104418 = vadd.s32 %v104415, %v104410
%v104420 = vshll.u32 %v104415, 29
%v104421 = vshrl.u32 %v104415, 3
%v104422 = vor.u32 %v104421, %v104420
%v104423 = vxor.u32 %v104422, %v104418
%v104426 = vadd.s32 %v104423, %v104418
%v104428 = vshll.u32 %v104423, 16
%v104429 = vshrl.u32 %v104423, 16
%v104430 = vor.u32 %v104429, %v104428
%v104431 = vxor.u32 %v104430, %v104426
%v104434 = vadd.s32 %v104431, %v104426
%v104438 = vadd.s32 %v104434, %v8
%v104440 = vshll.u32 %v104431, 24
%v104441 = vshrl.u32 %v104431, 8
%v104442 = vor.u32 %v104441, %v104440
%v104443 = vxor.u32 %v104442, %v104434
%v104446 = vadd.s32 %v104443, %v10
%v104450 = vadd.s32 2, %v104446
%v104454 = vadd.s32 %v104450, %v104438
%v104456 = vshll.u32 %v104450, 13
%v104457 = vshrl.u32 %v104450, 19
%v104458 = vor.u32 %v104457, %v104456
%v104459 = vxor.u32 %v104458, %v104454
%v104462 = vadd.s32 %v104459, %v104454
%v104464 = vshll.u32 %v104459, 15
%v104465 = vshrl.u32 %v104459, 17
%v104466 = vor.u32 %v104465, %v104464
%v104467 = vxor.u32 %v104466, %v104462
%v104470 = vadd.s32 %v104467, %v104462
%v104472 = vshll.u32 %v104467, 26
%v104473 = vshrl.u32 %v104467, 6
%v104474 = vor.u32 %v104473, %v104472
%v104475 = vxor.u32 %v104474, %v104470
%v104478 = vadd.s32 %v104475, %v104470
%v104482 = vadd.s32 %v104478, %v10
%v104484 = vshll.u32 %v104475, 6
%v104485 = vshrl.u32 %v104475, 26
%v104486 = vor.u32 %v104485, %v104484
%v104487 = vxor.u32 %v104486, %v104478
%v104490 = vadd.s32 %v104487, %v9
%v104494 = vadd.s32 3, %v104490
%v104498 = vadd.s32 %v104494, %v104482
%v104500 = vshll.u32 %v104494, 17
%v104501 = vshrl.u32 %v104494, 15
%v104502 = vor.u32 %v104501, %v104500
%v104503 = vxor.u32 %v104502, %v104498
%v104506 = vadd.s32 %v104503, %v104498
%v104508 = vshll.u32 %v104503, 29
%v104509 = vshrl.u32 %v104503, 3
%v104510 = vor.u32 %v104509, %v104508
%v104511 = vxor.u32 %v104510, %v104506
%v104514 = vadd.s32 %v104511, %v104506
%v104516 = vshll.u32 %v104511, 16
%v104517 = vshrl.u32 %v104511, 16
%v104518 = vor.u32 %v104517, %v104516
%v104519 = vxor.u32 %v104518, %v104514
%v104522 = vadd.s32 %v104519, %v104514
%v104526 = vadd.s32 %v104522, %v9
%v104528 = vshll.u32 %v104519, 24
%v104529 = vshrl.u32 %v104519, 8
%v104530 = vor.u32 %v104529, %v104528
%v104531 = vxor.u32 %v104530, %v104522
%v104534 = vadd.s32 %v104531, %v8
%v104538 = vadd.s32 4, %v104534
%v104542 = vadd.s32 %v104538, %v104526
%v104544 = vshll.u32 %v104538, 13
%v104545 = vshrl.u32 %v104538, 19
%v104546 = vor.u32 %v104545, %v104544
%v104547 = vxor.u32 %v104546, %v104542
%v104550 = vadd.s32 %v104547, %v104542
%v104552 = vshll.u32 %v104547, 15
%v104553 = vshrl.u32 %v104547, 17
%v104554 = vor.u32 %v104553, %v104552
%v104555 = vxor.u32 %v104554, %v104550
%v104558 = vadd.s32 %v104555, %v104550
%v104560 = vshll.u32 %v104555, 26
%v104561 = vshrl.u32 %v104555, 6
%v104562 = vor.u32 %v104561, %v104560
%v104563 = vxor.u32 %v104562, %v104558
%v104566 = vadd.s32 %v104563, %v104558
%v104570 = vadd.s32 %v104566, %v8
%v104572 = vshll.u32 %v104563, 6
%v104573 = vshrl.u32 %v104563, 26
%v104574 = vor.u32 %v104573, %v104572
%v104575 = vxor.u32 %v104574, %v104566
%v104578 = vadd.s32 %v104575, %v10
%v104582 = vadd.s32 5, %v104578
%v104584 = vxor.u32 %v104582, %v104570
%v104585 = vand.u32.u8 255, %v104584
%v104586 = vand.u32 65535, %v104585
%v104587 = vshrl.u32 %v104586, 1
%v104588 = vor.u32 16256, %v104587
%v104589 = vand.u32.u16 65535, %v104588
%v120302 = vadd.low.f32.bf16 -1.0, %v104589
%v104598 = vmul.f32 2.0, %v120302
%v104602 = vadd.f32 -0.99609375, %v104598
%v104606 = vmax.f32 %v104602, -0.99609375
%v104608 = vand.u32 2147483647, %v104606
%vm104611 = vcmp.eq.f32.partialorder %v104608, 1.0
%v104616 = vmul.f32 inf, %v104606
%v104618 = vxor.u32 2147483648, %v104606
%v104621 = vmul.f32 %v104618, %v104606
%v104623 = vadd.f32 1.0, %v104621
%v104624 = vlog2.pop %v104623
%v104625 = vmul.f32 0.6931472, %v104624
%v104626 = vmul.f32 -0.5, %v104621
%v104627 = vadd.f32 1.0, %v104626
%v104628 = vmul.f32 %v104627, %v104621
%v104629 = vand.u32 2147483647, %v104621
%vm104630 = vcmp.lt.f32.partialorder %v104629, 0.0004427343
%v104631 = vsel /*vm=*/%vm104630, /*on_true_vy=*/%v104628, /*on_false_vx=*/%v104625
%v104632 = vxor.u32 2147483648, %v104631
%vm104635 = vcmp.lt.f32.partialorder %v104632, 5.0
%v104640 = vsel /*vm=*/%vm104635, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v104644 = vsel /*vm=*/%vm104635, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v104648 = vsel /*vm=*/%vm104635, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v104652 = vsel /*vm=*/%vm104635, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v104656 = vsel /*vm=*/%vm104635, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v104660 = vsel /*vm=*/%vm104635, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v104664 = vsel /*vm=*/%vm104635, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v104668 = vsel /*vm=*/%vm104635, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v104672 = vsel /*vm=*/%vm104635, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v104676 = vadd.f32 -2.5, %v104632
%v104678 = vrsqrt.pop %v104632
%v104679 = vmul.f32 %v104678, %v104632
%vm104680 = vcmp.eq.f32.partialorder %v104632, inf
%v104681 = vsel /*vm=*/%vm104680, /*on_true_vy=*/%v104632, /*on_false_vx=*/%v104679
%vm104682 = vcmp.eq.f32.partialorder %v104632, 0.0
%v104683 = vand.u32 2147483648, %v104632
%v104684 = vsel /*vm=*/%vm104682, /*on_true_vy=*/%v104683, /*on_false_vx=*/%v104681
%v104687 = vadd.f32 -3.0, %v104684
%v104691 = vsel /*vm=*/%vm104635, /*on_true_vy=*/%v104676, /*on_false_vx=*/%v104687
%v104695 = vmul.f32 %v104691, %v104672
%v104699 = vadd.f32 %v104695, %v104668
%v104703 = vmul.f32 %v104699, %v104691
%v104707 = vadd.f32 %v104703, %v104664
%v104711 = vmul.f32 %v104707, %v104691
%v104715 = vadd.f32 %v104711, %v104660
%v104719 = vmul.f32 %v104715, %v104691
%v104723 = vadd.f32 %v104719, %v104656
%v104727 = vmul.f32 %v104723, %v104691
%v104731 = vadd.f32 %v104727, %v104652
%v104735 = vmul.f32 %v104731, %v104691
%v104739 = vadd.f32 %v104735, %v104648
%v104743 = vmul.f32 %v104739, %v104691
%v104747 = vadd.f32 %v104743, %v104644
%v104751 = vmul.f32 %v104747, %v104691
%v104755 = vadd.f32 %v104751, %v104640
%v104759 = vmul.f32 %v104755, %v104606
%v104763 = vsel /*vm=*/%vm104611, /*on_true_vy=*/%v104616, /*on_false_vx=*/%v104759
%v104767 = vmul.f32 1.4140625, %v104763
%v104770 = vpack.c.bf16 %v120417, %v104767
%120303 = vst [vmem:[%s280 + $0x3ec] sm:$0xf] /*vst_source=*/%v104770
%v104808 = vadd.s32 %v104805, %v408
%v104818 = vadd.s32 %v104808, %v415
%vm104822 = vcmp.lt.u32.totalorder %v104818, %v104808
%vm104827 = vcmp.lt.u32.totalorder %v104808, %v408
%v104832 = vadd.s32 %v104788, %v380
%v104836 = vadd.s32 1, %v104832
%v104840 = vsel /*vm=*/%vm104827, /*on_true_vy=*/%v104836, /*on_false_vx=*/%v104832
%v104844 = vadd.s32 1, %v104840
%v104848 = vsel /*vm=*/%vm104822, /*on_true_vy=*/%v104844, /*on_false_vx=*/%v104840
%v104853 = vadd.s32 %v104848, %v10
%v104857 = vadd.s32 %v104818, %v9
%v104861 = vadd.s32 %v104857, %v104853
%v104863 = vshll.u32 %v104857, 13
%v104864 = vshrl.u32 %v104857, 19
%v104865 = vor.u32 %v104864, %v104863
%v104866 = vxor.u32 %v104865, %v104861
%v104869 = vadd.s32 %v104866, %v104861
%v104871 = vshll.u32 %v104866, 15
%v104872 = vshrl.u32 %v104866, 17
%v104873 = vor.u32 %v104872, %v104871
%v104874 = vxor.u32 %v104873, %v104869
%v104877 = vadd.s32 %v104874, %v104869
%v104879 = vshll.u32 %v104874, 26
%v104880 = vshrl.u32 %v104874, 6
%v104881 = vor.u32 %v104880, %v104879
%v104882 = vxor.u32 %v104881, %v104877
%v104885 = vadd.s32 %v104882, %v104877
%v104889 = vadd.s32 %v104885, %v9
%v104891 = vshll.u32 %v104882, 6
%v104892 = vshrl.u32 %v104882, 26
%v104893 = vor.u32 %v104892, %v104891
%v104894 = vxor.u32 %v104893, %v104885
%v104897 = vadd.s32 %v104894, %v8
%v104901 = vadd.s32 1, %v104897
%v104905 = vadd.s32 %v104901, %v104889
%v104907 = vshll.u32 %v104901, 17
%v104908 = vshrl.u32 %v104901, 15
%v104909 = vor.u32 %v104908, %v104907
%v104910 = vxor.u32 %v104909, %v104905
%v104913 = vadd.s32 %v104910, %v104905
%v104915 = vshll.u32 %v104910, 29
%v104916 = vshrl.u32 %v104910, 3
%v104917 = vor.u32 %v104916, %v104915
%v104918 = vxor.u32 %v104917, %v104913
%v104921 = vadd.s32 %v104918, %v104913
%v104923 = vshll.u32 %v104918, 16
%v104924 = vshrl.u32 %v104918, 16
%v104925 = vor.u32 %v104924, %v104923
%v104926 = vxor.u32 %v104925, %v104921
%v104929 = vadd.s32 %v104926, %v104921
%v104933 = vadd.s32 %v104929, %v8
%v104935 = vshll.u32 %v104926, 24
%v104936 = vshrl.u32 %v104926, 8
%v104937 = vor.u32 %v104936, %v104935
%v104938 = vxor.u32 %v104937, %v104929
%v104941 = vadd.s32 %v104938, %v10
%v104945 = vadd.s32 2, %v104941
%v104949 = vadd.s32 %v104945, %v104933
%v104951 = vshll.u32 %v104945, 13
%v104952 = vshrl.u32 %v104945, 19
%v104953 = vor.u32 %v104952, %v104951
%v104954 = vxor.u32 %v104953, %v104949
%v104957 = vadd.s32 %v104954, %v104949
%v104959 = vshll.u32 %v104954, 15
%v104960 = vshrl.u32 %v104954, 17
%v104961 = vor.u32 %v104960, %v104959
%v104962 = vxor.u32 %v104961, %v104957
%v104965 = vadd.s32 %v104962, %v104957
%v104967 = vshll.u32 %v104962, 26
%v104968 = vshrl.u32 %v104962, 6
%v104969 = vor.u32 %v104968, %v104967
%v104970 = vxor.u32 %v104969, %v104965
%v104973 = vadd.s32 %v104970, %v104965
%v104977 = vadd.s32 %v104973, %v10
%v104979 = vshll.u32 %v104970, 6
%v104980 = vshrl.u32 %v104970, 26
%v104981 = vor.u32 %v104980, %v104979
%v104982 = vxor.u32 %v104981, %v104973
%v104985 = vadd.s32 %v104982, %v9
%v104989 = vadd.s32 3, %v104985
%v104993 = vadd.s32 %v104989, %v104977
%v104995 = vshll.u32 %v104989, 17
%v104996 = vshrl.u32 %v104989, 15
%v104997 = vor.u32 %v104996, %v104995
%v104998 = vxor.u32 %v104997, %v104993
%v105001 = vadd.s32 %v104998, %v104993
%v105003 = vshll.u32 %v104998, 29
%v105004 = vshrl.u32 %v104998, 3
%v105005 = vor.u32 %v105004, %v105003
%v105006 = vxor.u32 %v105005, %v105001
%v105009 = vadd.s32 %v105006, %v105001
%v105011 = vshll.u32 %v105006, 16
%v105012 = vshrl.u32 %v105006, 16
%v105013 = vor.u32 %v105012, %v105011
%v105014 = vxor.u32 %v105013, %v105009
%v105017 = vadd.s32 %v105014, %v105009
%v105021 = vadd.s32 %v105017, %v9
%v105023 = vshll.u32 %v105014, 24
%v105024 = vshrl.u32 %v105014, 8
%v105025 = vor.u32 %v105024, %v105023
%v105026 = vxor.u32 %v105025, %v105017
%v105029 = vadd.s32 %v105026, %v8
%v105033 = vadd.s32 4, %v105029
%v105037 = vadd.s32 %v105033, %v105021
%v105039 = vshll.u32 %v105033, 13
%v105040 = vshrl.u32 %v105033, 19
%v105041 = vor.u32 %v105040, %v105039
%v105042 = vxor.u32 %v105041, %v105037
%v105045 = vadd.s32 %v105042, %v105037
%v105047 = vshll.u32 %v105042, 15
%v105048 = vshrl.u32 %v105042, 17
%v105049 = vor.u32 %v105048, %v105047
%v105050 = vxor.u32 %v105049, %v105045
%v105053 = vadd.s32 %v105050, %v105045
%v105055 = vshll.u32 %v105050, 26
%v105056 = vshrl.u32 %v105050, 6
%v105057 = vor.u32 %v105056, %v105055
%v105058 = vxor.u32 %v105057, %v105053
%v105061 = vadd.s32 %v105058, %v105053
%v105065 = vadd.s32 %v105061, %v8
%v105067 = vshll.u32 %v105058, 6
%v105068 = vshrl.u32 %v105058, 26
%v105069 = vor.u32 %v105068, %v105067
%v105070 = vxor.u32 %v105069, %v105061
%v105073 = vadd.s32 %v105070, %v10
%v105077 = vadd.s32 5, %v105073
%v105079 = vxor.u32 %v105077, %v105065
%v105080 = vand.u32.u8 255, %v105079
%v105081 = vand.u32 65535, %v105080
%v105082 = vshrl.u32 %v105081, 1
%v105083 = vor.u32 16256, %v105082
%v105084 = vand.u32.u16 65535, %v105083
%v120308 = vadd.low.f32.bf16 -1.0, %v105084
%v105093 = vmul.f32 2.0, %v120308
%v105097 = vadd.f32 -0.99609375, %v105093
%v105101 = vmax.f32 %v105097, -0.99609375
%v105103 = vand.u32 2147483647, %v105101
%vm105106 = vcmp.eq.f32.partialorder %v105103, 1.0
%v105111 = vmul.f32 inf, %v105101
%v105113 = vxor.u32 2147483648, %v105101
%v105116 = vmul.f32 %v105113, %v105101
%v105118 = vadd.f32 1.0, %v105116
%v105119 = vlog2.pop %v105118
%v105120 = vmul.f32 0.6931472, %v105119
%v105121 = vmul.f32 -0.5, %v105116
%v105122 = vadd.f32 1.0, %v105121
%v105123 = vmul.f32 %v105122, %v105116
%v105124 = vand.u32 2147483647, %v105116
%vm105125 = vcmp.lt.f32.partialorder %v105124, 0.0004427343
%v105126 = vsel /*vm=*/%vm105125, /*on_true_vy=*/%v105123, /*on_false_vx=*/%v105120
%v105127 = vxor.u32 2147483648, %v105126
%vm105130 = vcmp.lt.f32.partialorder %v105127, 5.0
%v105135 = vsel /*vm=*/%vm105130, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v105139 = vsel /*vm=*/%vm105130, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v105143 = vsel /*vm=*/%vm105130, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v105147 = vsel /*vm=*/%vm105130, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v105151 = vsel /*vm=*/%vm105130, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v105155 = vsel /*vm=*/%vm105130, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v105159 = vsel /*vm=*/%vm105130, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v105163 = vsel /*vm=*/%vm105130, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v105167 = vsel /*vm=*/%vm105130, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v105171 = vadd.f32 -2.5, %v105127
%v105173 = vrsqrt.pop %v105127
%v105174 = vmul.f32 %v105173, %v105127
%vm105175 = vcmp.eq.f32.partialorder %v105127, inf
%v105176 = vsel /*vm=*/%vm105175, /*on_true_vy=*/%v105127, /*on_false_vx=*/%v105174
%vm105177 = vcmp.eq.f32.partialorder %v105127, 0.0
%v105178 = vand.u32 2147483648, %v105127
%v105179 = vsel /*vm=*/%vm105177, /*on_true_vy=*/%v105178, /*on_false_vx=*/%v105176
%v105182 = vadd.f32 -3.0, %v105179
%v105186 = vsel /*vm=*/%vm105130, /*on_true_vy=*/%v105171, /*on_false_vx=*/%v105182
%v105190 = vmul.f32 %v105186, %v105167
%v105194 = vadd.f32 %v105190, %v105163
%v105198 = vmul.f32 %v105194, %v105186
%v105202 = vadd.f32 %v105198, %v105159
%v105206 = vmul.f32 %v105202, %v105186
%v105210 = vadd.f32 %v105206, %v105155
%v105214 = vmul.f32 %v105210, %v105186
%v105218 = vadd.f32 %v105214, %v105151
%v105222 = vmul.f32 %v105218, %v105186
%v105226 = vadd.f32 %v105222, %v105147
%v105230 = vmul.f32 %v105226, %v105186
%v105234 = vadd.f32 %v105230, %v105143
%v105238 = vmul.f32 %v105234, %v105186
%v105242 = vadd.f32 %v105238, %v105139
%v105246 = vmul.f32 %v105242, %v105186
%v105250 = vadd.f32 %v105246, %v105135
%v105254 = vmul.f32 %v105250, %v105101
%v105258 = vsel /*vm=*/%vm105106, /*on_true_vy=*/%v105111, /*on_false_vx=*/%v105254
%v105262 = vmul.f32 1.4140625, %v105258
%v105265 = vpack.c.bf16 %v120417, %v105262
%120309 = vst [vmem:[%s280 + $0x70] sm:$0xf] /*vst_source=*/%v105265
%v105269 = vadd.s32 %v104805, %v894
%v105279 = vadd.s32 %v105269, %v415
%vm105283 = vcmp.lt.u32.totalorder %v105279, %v105269
%vm105288 = vcmp.lt.u32.totalorder %v105269, %v894
%v105293 = vadd.s32 %v104788, %v881
%v105297 = vadd.s32 1, %v105293
%v105301 = vsel /*vm=*/%vm105288, /*on_true_vy=*/%v105297, /*on_false_vx=*/%v105293
%v105305 = vadd.s32 1, %v105301
%v105309 = vsel /*vm=*/%vm105283, /*on_true_vy=*/%v105305, /*on_false_vx=*/%v105301
%v105314 = vadd.s32 %v105309, %v10
%v105318 = vadd.s32 %v105279, %v9
%v105322 = vadd.s32 %v105318, %v105314
%v105324 = vshll.u32 %v105318, 13
%v105325 = vshrl.u32 %v105318, 19
%v105326 = vor.u32 %v105325, %v105324
%v105327 = vxor.u32 %v105326, %v105322
%v105330 = vadd.s32 %v105327, %v105322
%v105332 = vshll.u32 %v105327, 15
%v105333 = vshrl.u32 %v105327, 17
%v105334 = vor.u32 %v105333, %v105332
%v105335 = vxor.u32 %v105334, %v105330
%v105338 = vadd.s32 %v105335, %v105330
%v105340 = vshll.u32 %v105335, 26
%v105341 = vshrl.u32 %v105335, 6
%v105342 = vor.u32 %v105341, %v105340
%v105343 = vxor.u32 %v105342, %v105338
%v105346 = vadd.s32 %v105343, %v105338
%v105350 = vadd.s32 %v105346, %v9
%v105352 = vshll.u32 %v105343, 6
%v105353 = vshrl.u32 %v105343, 26
%v105354 = vor.u32 %v105353, %v105352
%v105355 = vxor.u32 %v105354, %v105346
%v105358 = vadd.s32 %v105355, %v8
%v105362 = vadd.s32 1, %v105358
%v105366 = vadd.s32 %v105362, %v105350
%v105368 = vshll.u32 %v105362, 17
%v105369 = vshrl.u32 %v105362, 15
%v105370 = vor.u32 %v105369, %v105368
%v105371 = vxor.u32 %v105370, %v105366
%v105374 = vadd.s32 %v105371, %v105366
%v105376 = vshll.u32 %v105371, 29
%v105377 = vshrl.u32 %v105371, 3
%v105378 = vor.u32 %v105377, %v105376
%v105379 = vxor.u32 %v105378, %v105374
%v105382 = vadd.s32 %v105379, %v105374
%v105384 = vshll.u32 %v105379, 16
%v105385 = vshrl.u32 %v105379, 16
%v105386 = vor.u32 %v105385, %v105384
%v105387 = vxor.u32 %v105386, %v105382
%v105390 = vadd.s32 %v105387, %v105382
%v105394 = vadd.s32 %v105390, %v8
%v105396 = vshll.u32 %v105387, 24
%v105397 = vshrl.u32 %v105387, 8
%v105398 = vor.u32 %v105397, %v105396
%v105399 = vxor.u32 %v105398, %v105390
%v105402 = vadd.s32 %v105399, %v10
%v105406 = vadd.s32 2, %v105402
%v105410 = vadd.s32 %v105406, %v105394
%v105412 = vshll.u32 %v105406, 13
%v105413 = vshrl.u32 %v105406, 19
%v105414 = vor.u32 %v105413, %v105412
%v105415 = vxor.u32 %v105414, %v105410
%v105418 = vadd.s32 %v105415, %v105410
%v105420 = vshll.u32 %v105415, 15
%v105421 = vshrl.u32 %v105415, 17
%v105422 = vor.u32 %v105421, %v105420
%v105423 = vxor.u32 %v105422, %v105418
%v105426 = vadd.s32 %v105423, %v105418
%v105428 = vshll.u32 %v105423, 26
%v105429 = vshrl.u32 %v105423, 6
%v105430 = vor.u32 %v105429, %v105428
%v105431 = vxor.u32 %v105430, %v105426
%v105434 = vadd.s32 %v105431, %v105426
%v105438 = vadd.s32 %v105434, %v10
%v105440 = vshll.u32 %v105431, 6
%v105441 = vshrl.u32 %v105431, 26
%v105442 = vor.u32 %v105441, %v105440
%v105443 = vxor.u32 %v105442, %v105434
%v105446 = vadd.s32 %v105443, %v9
%v105450 = vadd.s32 3, %v105446
%v105454 = vadd.s32 %v105450, %v105438
%v105456 = vshll.u32 %v105450, 17
%v105457 = vshrl.u32 %v105450, 15
%v105458 = vor.u32 %v105457, %v105456
%v105459 = vxor.u32 %v105458, %v105454
%v105462 = vadd.s32 %v105459, %v105454
%v105464 = vshll.u32 %v105459, 29
%v105465 = vshrl.u32 %v105459, 3
%v105466 = vor.u32 %v105465, %v105464
%v105467 = vxor.u32 %v105466, %v105462
%v105470 = vadd.s32 %v105467, %v105462
%v105472 = vshll.u32 %v105467, 16
%v105473 = vshrl.u32 %v105467, 16
%v105474 = vor.u32 %v105473, %v105472
%v105475 = vxor.u32 %v105474, %v105470
%v105478 = vadd.s32 %v105475, %v105470
%v105482 = vadd.s32 %v105478, %v9
%v105484 = vshll.u32 %v105475, 24
%v105485 = vshrl.u32 %v105475, 8
%v105486 = vor.u32 %v105485, %v105484
%v105487 = vxor.u32 %v105486, %v105478
%v105490 = vadd.s32 %v105487, %v8
%v105494 = vadd.s32 4, %v105490
%v105498 = vadd.s32 %v105494, %v105482
%v105500 = vshll.u32 %v105494, 13
%v105501 = vshrl.u32 %v105494, 19
%v105502 = vor.u32 %v105501, %v105500
%v105503 = vxor.u32 %v105502, %v105498
%v105506 = vadd.s32 %v105503, %v105498
%v105508 = vshll.u32 %v105503, 15
%v105509 = vshrl.u32 %v105503, 17
%v105510 = vor.u32 %v105509, %v105508
%v105511 = vxor.u32 %v105510, %v105506
%v105514 = vadd.s32 %v105511, %v105506
%v105516 = vshll.u32 %v105511, 26
%v105517 = vshrl.u32 %v105511, 6
%v105518 = vor.u32 %v105517, %v105516
%v105519 = vxor.u32 %v105518, %v105514
%v105522 = vadd.s32 %v105519, %v105514
%v105526 = vadd.s32 %v105522, %v8
%v105528 = vshll.u32 %v105519, 6
%v105529 = vshrl.u32 %v105519, 26
%v105530 = vor.u32 %v105529, %v105528
%v105531 = vxor.u32 %v105530, %v105522
%v105534 = vadd.s32 %v105531, %v10
%v105538 = vadd.s32 5, %v105534
%v105540 = vxor.u32 %v105538, %v105526
%v105541 = vand.u32.u8 255, %v105540
%v105542 = vand.u32 65535, %v105541
%v105543 = vshrl.u32 %v105542, 1
%v105544 = vor.u32 16256, %v105543
%v105545 = vand.u32.u16 65535, %v105544
%v120310 = vadd.low.f32.bf16 -1.0, %v105545
%v105554 = vmul.f32 2.0, %v120310
%v105558 = vadd.f32 -0.99609375, %v105554
%v105562 = vmax.f32 %v105558, -0.99609375
%v105564 = vand.u32 2147483647, %v105562
%vm105567 = vcmp.eq.f32.partialorder %v105564, 1.0
%v105572 = vmul.f32 inf, %v105562
%v105574 = vxor.u32 2147483648, %v105562
%v105577 = vmul.f32 %v105574, %v105562
%v105579 = vadd.f32 1.0, %v105577
%v105580 = vlog2.pop %v105579
%v105581 = vmul.f32 0.6931472, %v105580
%v105582 = vmul.f32 -0.5, %v105577
%v105583 = vadd.f32 1.0, %v105582
%v105584 = vmul.f32 %v105583, %v105577
%v105585 = vand.u32 2147483647, %v105577
%vm105586 = vcmp.lt.f32.partialorder %v105585, 0.0004427343
%v105587 = vsel /*vm=*/%vm105586, /*on_true_vy=*/%v105584, /*on_false_vx=*/%v105581
%v105588 = vxor.u32 2147483648, %v105587
%vm105591 = vcmp.lt.f32.partialorder %v105588, 5.0
%v105596 = vsel /*vm=*/%vm105591, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v105600 = vsel /*vm=*/%vm105591, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v105604 = vsel /*vm=*/%vm105591, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v105608 = vsel /*vm=*/%vm105591, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v105612 = vsel /*vm=*/%vm105591, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v105616 = vsel /*vm=*/%vm105591, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v105620 = vsel /*vm=*/%vm105591, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v105624 = vsel /*vm=*/%vm105591, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v105628 = vsel /*vm=*/%vm105591, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v105632 = vadd.f32 -2.5, %v105588
%v105634 = vrsqrt.pop %v105588
%v105635 = vmul.f32 %v105634, %v105588
%vm105636 = vcmp.eq.f32.partialorder %v105588, inf
%v105637 = vsel /*vm=*/%vm105636, /*on_true_vy=*/%v105588, /*on_false_vx=*/%v105635
%vm105638 = vcmp.eq.f32.partialorder %v105588, 0.0
%v105639 = vand.u32 2147483648, %v105588
%v105640 = vsel /*vm=*/%vm105638, /*on_true_vy=*/%v105639, /*on_false_vx=*/%v105637
%v105643 = vadd.f32 -3.0, %v105640
%v105647 = vsel /*vm=*/%vm105591, /*on_true_vy=*/%v105632, /*on_false_vx=*/%v105643
%v105651 = vmul.f32 %v105647, %v105628
%v105655 = vadd.f32 %v105651, %v105624
%v105659 = vmul.f32 %v105655, %v105647
%v105663 = vadd.f32 %v105659, %v105620
%v105667 = vmul.f32 %v105663, %v105647
%v105671 = vadd.f32 %v105667, %v105616
%v105675 = vmul.f32 %v105671, %v105647
%v105679 = vadd.f32 %v105675, %v105612
%v105683 = vmul.f32 %v105679, %v105647
%v105687 = vadd.f32 %v105683, %v105608
%v105691 = vmul.f32 %v105687, %v105647
%v105695 = vadd.f32 %v105691, %v105604
%v105699 = vmul.f32 %v105695, %v105647
%v105703 = vadd.f32 %v105699, %v105600
%v105707 = vmul.f32 %v105703, %v105647
%v105711 = vadd.f32 %v105707, %v105596
%v105715 = vmul.f32 %v105711, %v105562
%v105719 = vsel /*vm=*/%vm105567, /*on_true_vy=*/%v105572, /*on_false_vx=*/%v105715
%v105723 = vmul.f32 1.4140625, %v105719
%v105726 = vpack.c.bf16 %v120417, %v105723
%120311 = vst [vmem:[%s280 + $0xf0] sm:$0xf] /*vst_source=*/%v105726
%v105730 = vadd.s32 %v104805, %v1381
%v105740 = vadd.s32 %v105730, %v415
%vm105744 = vcmp.lt.u32.totalorder %v105740, %v105730
%vm105749 = vcmp.lt.u32.totalorder %v105730, %v1381
%v105754 = vadd.s32 %v104788, %v1368
%v105758 = vadd.s32 1, %v105754
%v105762 = vsel /*vm=*/%vm105749, /*on_true_vy=*/%v105758, /*on_false_vx=*/%v105754
%v105766 = vadd.s32 1, %v105762
%v105770 = vsel /*vm=*/%vm105744, /*on_true_vy=*/%v105766, /*on_false_vx=*/%v105762
%v105775 = vadd.s32 %v105770, %v10
%v105779 = vadd.s32 %v105740, %v9
%v105783 = vadd.s32 %v105779, %v105775
%v105785 = vshll.u32 %v105779, 13
%v105786 = vshrl.u32 %v105779, 19
%v105787 = vor.u32 %v105786, %v105785
%v105788 = vxor.u32 %v105787, %v105783
%v105791 = vadd.s32 %v105788, %v105783
%v105793 = vshll.u32 %v105788, 15
%v105794 = vshrl.u32 %v105788, 17
%v105795 = vor.u32 %v105794, %v105793
%v105796 = vxor.u32 %v105795, %v105791
%v105799 = vadd.s32 %v105796, %v105791
%v105801 = vshll.u32 %v105796, 26
%v105802 = vshrl.u32 %v105796, 6
%v105803 = vor.u32 %v105802, %v105801
%v105804 = vxor.u32 %v105803, %v105799
%v105807 = vadd.s32 %v105804, %v105799
%v105811 = vadd.s32 %v105807, %v9
%v105813 = vshll.u32 %v105804, 6
%v105814 = vshrl.u32 %v105804, 26
%v105815 = vor.u32 %v105814, %v105813
%v105816 = vxor.u32 %v105815, %v105807
%v105819 = vadd.s32 %v105816, %v8
%v105823 = vadd.s32 1, %v105819
%v105827 = vadd.s32 %v105823, %v105811
%v105829 = vshll.u32 %v105823, 17
%v105830 = vshrl.u32 %v105823, 15
%v105831 = vor.u32 %v105830, %v105829
%v105832 = vxor.u32 %v105831, %v105827
%v105835 = vadd.s32 %v105832, %v105827
%v105837 = vshll.u32 %v105832, 29
%v105838 = vshrl.u32 %v105832, 3
%v105839 = vor.u32 %v105838, %v105837
%v105840 = vxor.u32 %v105839, %v105835
%v105843 = vadd.s32 %v105840, %v105835
%v105845 = vshll.u32 %v105840, 16
%v105846 = vshrl.u32 %v105840, 16
%v105847 = vor.u32 %v105846, %v105845
%v105848 = vxor.u32 %v105847, %v105843
%v105851 = vadd.s32 %v105848, %v105843
%v105855 = vadd.s32 %v105851, %v8
%v105857 = vshll.u32 %v105848, 24
%v105858 = vshrl.u32 %v105848, 8
%v105859 = vor.u32 %v105858, %v105857
%v105860 = vxor.u32 %v105859, %v105851
%v105863 = vadd.s32 %v105860, %v10
%v105867 = vadd.s32 2, %v105863
%v105871 = vadd.s32 %v105867, %v105855
%v105873 = vshll.u32 %v105867, 13
%v105874 = vshrl.u32 %v105867, 19
%v105875 = vor.u32 %v105874, %v105873
%v105876 = vxor.u32 %v105875, %v105871
%v105879 = vadd.s32 %v105876, %v105871
%v105881 = vshll.u32 %v105876, 15
%v105882 = vshrl.u32 %v105876, 17
%v105883 = vor.u32 %v105882, %v105881
%v105884 = vxor.u32 %v105883, %v105879
%v105887 = vadd.s32 %v105884, %v105879
%v105889 = vshll.u32 %v105884, 26
%v105890 = vshrl.u32 %v105884, 6
%v105891 = vor.u32 %v105890, %v105889
%v105892 = vxor.u32 %v105891, %v105887
%v105895 = vadd.s32 %v105892, %v105887
%v105899 = vadd.s32 %v105895, %v10
%v105901 = vshll.u32 %v105892, 6
%v105902 = vshrl.u32 %v105892, 26
%v105903 = vor.u32 %v105902, %v105901
%v105904 = vxor.u32 %v105903, %v105895
%v105907 = vadd.s32 %v105904, %v9
%v105911 = vadd.s32 3, %v105907
%v105915 = vadd.s32 %v105911, %v105899
%v105917 = vshll.u32 %v105911, 17
%v105918 = vshrl.u32 %v105911, 15
%v105919 = vor.u32 %v105918, %v105917
%v105920 = vxor.u32 %v105919, %v105915
%v105923 = vadd.s32 %v105920, %v105915
%v105925 = vshll.u32 %v105920, 29
%v105926 = vshrl.u32 %v105920, 3
%v105927 = vor.u32 %v105926, %v105925
%v105928 = vxor.u32 %v105927, %v105923
%v105931 = vadd.s32 %v105928, %v105923
%v105933 = vshll.u32 %v105928, 16
%v105934 = vshrl.u32 %v105928, 16
%v105935 = vor.u32 %v105934, %v105933
%v105936 = vxor.u32 %v105935, %v105931
%v105939 = vadd.s32 %v105936, %v105931
%v105943 = vadd.s32 %v105939, %v9
%v105945 = vshll.u32 %v105936, 24
%v105946 = vshrl.u32 %v105936, 8
%v105947 = vor.u32 %v105946, %v105945
%v105948 = vxor.u32 %v105947, %v105939
%v105951 = vadd.s32 %v105948, %v8
%v105955 = vadd.s32 4, %v105951
%v105959 = vadd.s32 %v105955, %v105943
%v105961 = vshll.u32 %v105955, 13
%v105962 = vshrl.u32 %v105955, 19
%v105963 = vor.u32 %v105962, %v105961
%v105964 = vxor.u32 %v105963, %v105959
%v105967 = vadd.s32 %v105964, %v105959
%v105969 = vshll.u32 %v105964, 15
%v105970 = vshrl.u32 %v105964, 17
%v105971 = vor.u32 %v105970, %v105969
%v105972 = vxor.u32 %v105971, %v105967
%v105975 = vadd.s32 %v105972, %v105967
%v105977 = vshll.u32 %v105972, 26
%v105978 = vshrl.u32 %v105972, 6
%v105979 = vor.u32 %v105978, %v105977
%v105980 = vxor.u32 %v105979, %v105975
%v105983 = vadd.s32 %v105980, %v105975
%v105987 = vadd.s32 %v105983, %v8
%v105989 = vshll.u32 %v105980, 6
%v105990 = vshrl.u32 %v105980, 26
%v105991 = vor.u32 %v105990, %v105989
%v105992 = vxor.u32 %v105991, %v105983
%v105995 = vadd.s32 %v105992, %v10
%v105999 = vadd.s32 5, %v105995
%v106001 = vxor.u32 %v105999, %v105987
%v106002 = vand.u32.u8 255, %v106001
%v106003 = vand.u32 65535, %v106002
%v106004 = vshrl.u32 %v106003, 1
%v106005 = vor.u32 16256, %v106004
%v106006 = vand.u32.u16 65535, %v106005
%v120312 = vadd.low.f32.bf16 -1.0, %v106006
%v106015 = vmul.f32 2.0, %v120312
%v106019 = vadd.f32 -0.99609375, %v106015
%v106023 = vmax.f32 %v106019, -0.99609375
%v106025 = vand.u32 2147483647, %v106023
%vm106028 = vcmp.eq.f32.partialorder %v106025, 1.0
%v106033 = vmul.f32 inf, %v106023
%v106035 = vxor.u32 2147483648, %v106023
%v106038 = vmul.f32 %v106035, %v106023
%v106040 = vadd.f32 1.0, %v106038
%v106041 = vlog2.pop %v106040
%v106042 = vmul.f32 0.6931472, %v106041
%v106043 = vmul.f32 -0.5, %v106038
%v106044 = vadd.f32 1.0, %v106043
%v106045 = vmul.f32 %v106044, %v106038
%v106046 = vand.u32 2147483647, %v106038
%vm106047 = vcmp.lt.f32.partialorder %v106046, 0.0004427343
%v106048 = vsel /*vm=*/%vm106047, /*on_true_vy=*/%v106045, /*on_false_vx=*/%v106042
%v106049 = vxor.u32 2147483648, %v106048
%vm106052 = vcmp.lt.f32.partialorder %v106049, 5.0
%v106057 = vsel /*vm=*/%vm106052, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v106061 = vsel /*vm=*/%vm106052, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v106065 = vsel /*vm=*/%vm106052, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v106069 = vsel /*vm=*/%vm106052, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v106073 = vsel /*vm=*/%vm106052, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v106077 = vsel /*vm=*/%vm106052, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v106081 = vsel /*vm=*/%vm106052, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v106085 = vsel /*vm=*/%vm106052, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v106089 = vsel /*vm=*/%vm106052, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v106093 = vadd.f32 -2.5, %v106049
%v106095 = vrsqrt.pop %v106049
%v106096 = vmul.f32 %v106095, %v106049
%vm106097 = vcmp.eq.f32.partialorder %v106049, inf
%v106098 = vsel /*vm=*/%vm106097, /*on_true_vy=*/%v106049, /*on_false_vx=*/%v106096
%vm106099 = vcmp.eq.f32.partialorder %v106049, 0.0
%v106100 = vand.u32 2147483648, %v106049
%v106101 = vsel /*vm=*/%vm106099, /*on_true_vy=*/%v106100, /*on_false_vx=*/%v106098
%v106104 = vadd.f32 -3.0, %v106101
%v106108 = vsel /*vm=*/%vm106052, /*on_true_vy=*/%v106093, /*on_false_vx=*/%v106104
%v106112 = vmul.f32 %v106108, %v106089
%v106116 = vadd.f32 %v106112, %v106085
%v106120 = vmul.f32 %v106116, %v106108
%v106124 = vadd.f32 %v106120, %v106081
%v106128 = vmul.f32 %v106124, %v106108
%v106132 = vadd.f32 %v106128, %v106077
%v106136 = vmul.f32 %v106132, %v106108
%v106140 = vadd.f32 %v106136, %v106073
%v106144 = vmul.f32 %v106140, %v106108
%v106148 = vadd.f32 %v106144, %v106069
%v106152 = vmul.f32 %v106148, %v106108
%v106156 = vadd.f32 %v106152, %v106065
%v106160 = vmul.f32 %v106156, %v106108
%v106164 = vadd.f32 %v106160, %v106061
%v106168 = vmul.f32 %v106164, %v106108
%v106172 = vadd.f32 %v106168, %v106057
%v106176 = vmul.f32 %v106172, %v106023
%v106180 = vsel /*vm=*/%vm106028, /*on_true_vy=*/%v106033, /*on_false_vx=*/%v106176
%v106184 = vmul.f32 1.4140625, %v106180
%v106187 = vpack.c.bf16 %v120417, %v106184
%120313 = vst [vmem:[%s280 + $0x170] sm:$0xf] /*vst_source=*/%v106187
%v106191 = vadd.s32 %v104805, %v1868
%v106201 = vadd.s32 %v106191, %v415
%vm106205 = vcmp.lt.u32.totalorder %v106201, %v106191
%vm106210 = vcmp.lt.u32.totalorder %v106191, %v1868
%v106215 = vadd.s32 %v104788, %v1855
%v106219 = vadd.s32 1, %v106215
%v106223 = vsel /*vm=*/%vm106210, /*on_true_vy=*/%v106219, /*on_false_vx=*/%v106215
%v106227 = vadd.s32 1, %v106223
%v106231 = vsel /*vm=*/%vm106205, /*on_true_vy=*/%v106227, /*on_false_vx=*/%v106223
%v106236 = vadd.s32 %v106231, %v10
%v106240 = vadd.s32 %v106201, %v9
%v106244 = vadd.s32 %v106240, %v106236
%v106246 = vshll.u32 %v106240, 13
%v106247 = vshrl.u32 %v106240, 19
%v106248 = vor.u32 %v106247, %v106246
%v106249 = vxor.u32 %v106248, %v106244
%v106252 = vadd.s32 %v106249, %v106244
%v106254 = vshll.u32 %v106249, 15
%v106255 = vshrl.u32 %v106249, 17
%v106256 = vor.u32 %v106255, %v106254
%v106257 = vxor.u32 %v106256, %v106252
%v106260 = vadd.s32 %v106257, %v106252
%v106262 = vshll.u32 %v106257, 26
%v106263 = vshrl.u32 %v106257, 6
%v106264 = vor.u32 %v106263, %v106262
%v106265 = vxor.u32 %v106264, %v106260
%v106268 = vadd.s32 %v106265, %v106260
%v106272 = vadd.s32 %v106268, %v9
%v106274 = vshll.u32 %v106265, 6
%v106275 = vshrl.u32 %v106265, 26
%v106276 = vor.u32 %v106275, %v106274
%v106277 = vxor.u32 %v106276, %v106268
%v106280 = vadd.s32 %v106277, %v8
%v106284 = vadd.s32 1, %v106280
%v106288 = vadd.s32 %v106284, %v106272
%v106290 = vshll.u32 %v106284, 17
%v106291 = vshrl.u32 %v106284, 15
%v106292 = vor.u32 %v106291, %v106290
%v106293 = vxor.u32 %v106292, %v106288
%v106296 = vadd.s32 %v106293, %v106288
%v106298 = vshll.u32 %v106293, 29
%v106299 = vshrl.u32 %v106293, 3
%v106300 = vor.u32 %v106299, %v106298
%v106301 = vxor.u32 %v106300, %v106296
%v106304 = vadd.s32 %v106301, %v106296
%v106306 = vshll.u32 %v106301, 16
%v106307 = vshrl.u32 %v106301, 16
%v106308 = vor.u32 %v106307, %v106306
%v106309 = vxor.u32 %v106308, %v106304
%v106312 = vadd.s32 %v106309, %v106304
%v106316 = vadd.s32 %v106312, %v8
%v106318 = vshll.u32 %v106309, 24
%v106319 = vshrl.u32 %v106309, 8
%v106320 = vor.u32 %v106319, %v106318
%v106321 = vxor.u32 %v106320, %v106312
%v106324 = vadd.s32 %v106321, %v10
%v106328 = vadd.s32 2, %v106324
%v106332 = vadd.s32 %v106328, %v106316
%v106334 = vshll.u32 %v106328, 13
%v106335 = vshrl.u32 %v106328, 19
%v106336 = vor.u32 %v106335, %v106334
%v106337 = vxor.u32 %v106336, %v106332
%v106340 = vadd.s32 %v106337, %v106332
%v106342 = vshll.u32 %v106337, 15
%v106343 = vshrl.u32 %v106337, 17
%v106344 = vor.u32 %v106343, %v106342
%v106345 = vxor.u32 %v106344, %v106340
%v106348 = vadd.s32 %v106345, %v106340
%v106350 = vshll.u32 %v106345, 26
%v106351 = vshrl.u32 %v106345, 6
%v106352 = vor.u32 %v106351, %v106350
%v106353 = vxor.u32 %v106352, %v106348
%v106356 = vadd.s32 %v106353, %v106348
%v106360 = vadd.s32 %v106356, %v10
%v106362 = vshll.u32 %v106353, 6
%v106363 = vshrl.u32 %v106353, 26
%v106364 = vor.u32 %v106363, %v106362
%v106365 = vxor.u32 %v106364, %v106356
%v106368 = vadd.s32 %v106365, %v9
%v106372 = vadd.s32 3, %v106368
%v106376 = vadd.s32 %v106372, %v106360
%v106378 = vshll.u32 %v106372, 17
%v106379 = vshrl.u32 %v106372, 15
%v106380 = vor.u32 %v106379, %v106378
%v106381 = vxor.u32 %v106380, %v106376
%v106384 = vadd.s32 %v106381, %v106376
%v106386 = vshll.u32 %v106381, 29
%v106387 = vshrl.u32 %v106381, 3
%v106388 = vor.u32 %v106387, %v106386
%v106389 = vxor.u32 %v106388, %v106384
%v106392 = vadd.s32 %v106389, %v106384
%v106394 = vshll.u32 %v106389, 16
%v106395 = vshrl.u32 %v106389, 16
%v106396 = vor.u32 %v106395, %v106394
%v106397 = vxor.u32 %v106396, %v106392
%v106400 = vadd.s32 %v106397, %v106392
%v106404 = vadd.s32 %v106400, %v9
%v106406 = vshll.u32 %v106397, 24
%v106407 = vshrl.u32 %v106397, 8
%v106408 = vor.u32 %v106407, %v106406
%v106409 = vxor.u32 %v106408, %v106400
%v106412 = vadd.s32 %v106409, %v8
%v106416 = vadd.s32 4, %v106412
%v106420 = vadd.s32 %v106416, %v106404
%v106422 = vshll.u32 %v106416, 13
%v106423 = vshrl.u32 %v106416, 19
%v106424 = vor.u32 %v106423, %v106422
%v106425 = vxor.u32 %v106424, %v106420
%v106428 = vadd.s32 %v106425, %v106420
%v106430 = vshll.u32 %v106425, 15
%v106431 = vshrl.u32 %v106425, 17
%v106432 = vor.u32 %v106431, %v106430
%v106433 = vxor.u32 %v106432, %v106428
%v106436 = vadd.s32 %v106433, %v106428
%v106438 = vshll.u32 %v106433, 26
%v106439 = vshrl.u32 %v106433, 6
%v106440 = vor.u32 %v106439, %v106438
%v106441 = vxor.u32 %v106440, %v106436
%v106444 = vadd.s32 %v106441, %v106436
%v106448 = vadd.s32 %v106444, %v8
%v106450 = vshll.u32 %v106441, 6
%v106451 = vshrl.u32 %v106441, 26
%v106452 = vor.u32 %v106451, %v106450
%v106453 = vxor.u32 %v106452, %v106444
%v106456 = vadd.s32 %v106453, %v10
%v106460 = vadd.s32 5, %v106456
%v106462 = vxor.u32 %v106460, %v106448
%v106463 = vand.u32.u8 255, %v106462
%v106464 = vand.u32 65535, %v106463
%v106465 = vshrl.u32 %v106464, 1
%v106466 = vor.u32 16256, %v106465
%v106467 = vand.u32.u16 65535, %v106466
%v120314 = vadd.low.f32.bf16 -1.0, %v106467
%v106476 = vmul.f32 2.0, %v120314
%v106480 = vadd.f32 -0.99609375, %v106476
%v106484 = vmax.f32 %v106480, -0.99609375
%v106486 = vand.u32 2147483647, %v106484
%vm106489 = vcmp.eq.f32.partialorder %v106486, 1.0
%v106494 = vmul.f32 inf, %v106484
%v106496 = vxor.u32 2147483648, %v106484
%v106499 = vmul.f32 %v106496, %v106484
%v106501 = vadd.f32 1.0, %v106499
%v106502 = vlog2.pop %v106501
%v106503 = vmul.f32 0.6931472, %v106502
%v106504 = vmul.f32 -0.5, %v106499
%v106505 = vadd.f32 1.0, %v106504
%v106506 = vmul.f32 %v106505, %v106499
%v106507 = vand.u32 2147483647, %v106499
%vm106508 = vcmp.lt.f32.partialorder %v106507, 0.0004427343
%v106509 = vsel /*vm=*/%vm106508, /*on_true_vy=*/%v106506, /*on_false_vx=*/%v106503
%v106510 = vxor.u32 2147483648, %v106509
%vm106513 = vcmp.lt.f32.partialorder %v106510, 5.0
%v106518 = vsel /*vm=*/%vm106513, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v106522 = vsel /*vm=*/%vm106513, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v106526 = vsel /*vm=*/%vm106513, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v106530 = vsel /*vm=*/%vm106513, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v106534 = vsel /*vm=*/%vm106513, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v106538 = vsel /*vm=*/%vm106513, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v106542 = vsel /*vm=*/%vm106513, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v106546 = vsel /*vm=*/%vm106513, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v106550 = vsel /*vm=*/%vm106513, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v106554 = vadd.f32 -2.5, %v106510
%v106556 = vrsqrt.pop %v106510
%v106557 = vmul.f32 %v106556, %v106510
%vm106558 = vcmp.eq.f32.partialorder %v106510, inf
%v106559 = vsel /*vm=*/%vm106558, /*on_true_vy=*/%v106510, /*on_false_vx=*/%v106557
%vm106560 = vcmp.eq.f32.partialorder %v106510, 0.0
%v106561 = vand.u32 2147483648, %v106510
%v106562 = vsel /*vm=*/%vm106560, /*on_true_vy=*/%v106561, /*on_false_vx=*/%v106559
%v106565 = vadd.f32 -3.0, %v106562
%v106569 = vsel /*vm=*/%vm106513, /*on_true_vy=*/%v106554, /*on_false_vx=*/%v106565
%v106573 = vmul.f32 %v106569, %v106550
%v106577 = vadd.f32 %v106573, %v106546
%v106581 = vmul.f32 %v106577, %v106569
%v106585 = vadd.f32 %v106581, %v106542
%v106589 = vmul.f32 %v106585, %v106569
%v106593 = vadd.f32 %v106589, %v106538
%v106597 = vmul.f32 %v106593, %v106569
%v106601 = vadd.f32 %v106597, %v106534
%v106605 = vmul.f32 %v106601, %v106569
%v106609 = vadd.f32 %v106605, %v106530
%v106613 = vmul.f32 %v106609, %v106569
%v106617 = vadd.f32 %v106613, %v106526
%v106621 = vmul.f32 %v106617, %v106569
%v106625 = vadd.f32 %v106621, %v106522
%v106629 = vmul.f32 %v106625, %v106569
%v106633 = vadd.f32 %v106629, %v106518
%v106637 = vmul.f32 %v106633, %v106484
%v106641 = vsel /*vm=*/%vm106489, /*on_true_vy=*/%v106494, /*on_false_vx=*/%v106637
%v106645 = vmul.f32 1.4140625, %v106641
%v106648 = vpack.c.bf16 %v120417, %v106645
%120315 = vst [vmem:[%s280 + $0x1f0] sm:$0xf] /*vst_source=*/%v106648
%v106652 = vadd.s32 %v104805, %v2355
%v106662 = vadd.s32 %v106652, %v415
%vm106666 = vcmp.lt.u32.totalorder %v106662, %v106652
%vm106671 = vcmp.lt.u32.totalorder %v106652, %v2355
%v106676 = vadd.s32 %v104788, %v2342
%v106680 = vadd.s32 1, %v106676
%v106684 = vsel /*vm=*/%vm106671, /*on_true_vy=*/%v106680, /*on_false_vx=*/%v106676
%v106688 = vadd.s32 1, %v106684
%v106692 = vsel /*vm=*/%vm106666, /*on_true_vy=*/%v106688, /*on_false_vx=*/%v106684
%v106697 = vadd.s32 %v106692, %v10
%v106701 = vadd.s32 %v106662, %v9
%v106705 = vadd.s32 %v106701, %v106697
%v106707 = vshll.u32 %v106701, 13
%v106708 = vshrl.u32 %v106701, 19
%v106709 = vor.u32 %v106708, %v106707
%v106710 = vxor.u32 %v106709, %v106705
%v106713 = vadd.s32 %v106710, %v106705
%v106715 = vshll.u32 %v106710, 15
%v106716 = vshrl.u32 %v106710, 17
%v106717 = vor.u32 %v106716, %v106715
%v106718 = vxor.u32 %v106717, %v106713
%v106721 = vadd.s32 %v106718, %v106713
%v106723 = vshll.u32 %v106718, 26
%v106724 = vshrl.u32 %v106718, 6
%v106725 = vor.u32 %v106724, %v106723
%v106726 = vxor.u32 %v106725, %v106721
%v106729 = vadd.s32 %v106726, %v106721
%v106733 = vadd.s32 %v106729, %v9
%v106735 = vshll.u32 %v106726, 6
%v106736 = vshrl.u32 %v106726, 26
%v106737 = vor.u32 %v106736, %v106735
%v106738 = vxor.u32 %v106737, %v106729
%v106741 = vadd.s32 %v106738, %v8
%v106745 = vadd.s32 1, %v106741
%v106749 = vadd.s32 %v106745, %v106733
%v106751 = vshll.u32 %v106745, 17
%v106752 = vshrl.u32 %v106745, 15
%v106753 = vor.u32 %v106752, %v106751
%v106754 = vxor.u32 %v106753, %v106749
%v106757 = vadd.s32 %v106754, %v106749
%v106759 = vshll.u32 %v106754, 29
%v106760 = vshrl.u32 %v106754, 3
%v106761 = vor.u32 %v106760, %v106759
%v106762 = vxor.u32 %v106761, %v106757
%v106765 = vadd.s32 %v106762, %v106757
%v106767 = vshll.u32 %v106762, 16
%v106768 = vshrl.u32 %v106762, 16
%v106769 = vor.u32 %v106768, %v106767
%v106770 = vxor.u32 %v106769, %v106765
%v106773 = vadd.s32 %v106770, %v106765
%v106777 = vadd.s32 %v106773, %v8
%v106779 = vshll.u32 %v106770, 24
%v106780 = vshrl.u32 %v106770, 8
%v106781 = vor.u32 %v106780, %v106779
%v106782 = vxor.u32 %v106781, %v106773
%v106785 = vadd.s32 %v106782, %v10
%v106789 = vadd.s32 2, %v106785
%v106793 = vadd.s32 %v106789, %v106777
%v106795 = vshll.u32 %v106789, 13
%v106796 = vshrl.u32 %v106789, 19
%v106797 = vor.u32 %v106796, %v106795
%v106798 = vxor.u32 %v106797, %v106793
%v106801 = vadd.s32 %v106798, %v106793
%v106803 = vshll.u32 %v106798, 15
%v106804 = vshrl.u32 %v106798, 17
%v106805 = vor.u32 %v106804, %v106803
%v106806 = vxor.u32 %v106805, %v106801
%v106809 = vadd.s32 %v106806, %v106801
%v106811 = vshll.u32 %v106806, 26
%v106812 = vshrl.u32 %v106806, 6
%v106813 = vor.u32 %v106812, %v106811
%v106814 = vxor.u32 %v106813, %v106809
%v106817 = vadd.s32 %v106814, %v106809
%v106821 = vadd.s32 %v106817, %v10
%v106823 = vshll.u32 %v106814, 6
%v106824 = vshrl.u32 %v106814, 26
%v106825 = vor.u32 %v106824, %v106823
%v106826 = vxor.u32 %v106825, %v106817
%v106829 = vadd.s32 %v106826, %v9
%v106833 = vadd.s32 3, %v106829
%v106837 = vadd.s32 %v106833, %v106821
%v106839 = vshll.u32 %v106833, 17
%v106840 = vshrl.u32 %v106833, 15
%v106841 = vor.u32 %v106840, %v106839
%v106842 = vxor.u32 %v106841, %v106837
%v106845 = vadd.s32 %v106842, %v106837
%v106847 = vshll.u32 %v106842, 29
%v106848 = vshrl.u32 %v106842, 3
%v106849 = vor.u32 %v106848, %v106847
%v106850 = vxor.u32 %v106849, %v106845
%v106853 = vadd.s32 %v106850, %v106845
%v106855 = vshll.u32 %v106850, 16
%v106856 = vshrl.u32 %v106850, 16
%v106857 = vor.u32 %v106856, %v106855
%v106858 = vxor.u32 %v106857, %v106853
%v106861 = vadd.s32 %v106858, %v106853
%v106865 = vadd.s32 %v106861, %v9
%v106867 = vshll.u32 %v106858, 24
%v106868 = vshrl.u32 %v106858, 8
%v106869 = vor.u32 %v106868, %v106867
%v106870 = vxor.u32 %v106869, %v106861
%v106873 = vadd.s32 %v106870, %v8
%v106877 = vadd.s32 4, %v106873
%v106881 = vadd.s32 %v106877, %v106865
%v106883 = vshll.u32 %v106877, 13
%v106884 = vshrl.u32 %v106877, 19
%v106885 = vor.u32 %v106884, %v106883
%v106886 = vxor.u32 %v106885, %v106881
%v106889 = vadd.s32 %v106886, %v106881
%v106891 = vshll.u32 %v106886, 15
%v106892 = vshrl.u32 %v106886, 17
%v106893 = vor.u32 %v106892, %v106891
%v106894 = vxor.u32 %v106893, %v106889
%v106897 = vadd.s32 %v106894, %v106889
%v106899 = vshll.u32 %v106894, 26
%v106900 = vshrl.u32 %v106894, 6
%v106901 = vor.u32 %v106900, %v106899
%v106902 = vxor.u32 %v106901, %v106897
%v106905 = vadd.s32 %v106902, %v106897
%v106909 = vadd.s32 %v106905, %v8
%v106911 = vshll.u32 %v106902, 6
%v106912 = vshrl.u32 %v106902, 26
%v106913 = vor.u32 %v106912, %v106911
%v106914 = vxor.u32 %v106913, %v106905
%v106917 = vadd.s32 %v106914, %v10
%v106921 = vadd.s32 5, %v106917
%v106923 = vxor.u32 %v106921, %v106909
%v106924 = vand.u32.u8 255, %v106923
%v106925 = vand.u32 65535, %v106924
%v106926 = vshrl.u32 %v106925, 1
%v106927 = vor.u32 16256, %v106926
%v106928 = vand.u32.u16 65535, %v106927
%v120316 = vadd.low.f32.bf16 -1.0, %v106928
%v106937 = vmul.f32 2.0, %v120316
%v106941 = vadd.f32 -0.99609375, %v106937
%v106945 = vmax.f32 %v106941, -0.99609375
%v106947 = vand.u32 2147483647, %v106945
%vm106950 = vcmp.eq.f32.partialorder %v106947, 1.0
%v106955 = vmul.f32 inf, %v106945
%v106957 = vxor.u32 2147483648, %v106945
%v106960 = vmul.f32 %v106957, %v106945
%v106962 = vadd.f32 1.0, %v106960
%v106963 = vlog2.pop %v106962
%v106964 = vmul.f32 0.6931472, %v106963
%v106965 = vmul.f32 -0.5, %v106960
%v106966 = vadd.f32 1.0, %v106965
%v106967 = vmul.f32 %v106966, %v106960
%v106968 = vand.u32 2147483647, %v106960
%vm106969 = vcmp.lt.f32.partialorder %v106968, 0.0004427343
%v106970 = vsel /*vm=*/%vm106969, /*on_true_vy=*/%v106967, /*on_false_vx=*/%v106964
%v106971 = vxor.u32 2147483648, %v106970
%vm106974 = vcmp.lt.f32.partialorder %v106971, 5.0
%v106979 = vsel /*vm=*/%vm106974, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v106983 = vsel /*vm=*/%vm106974, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v106987 = vsel /*vm=*/%vm106974, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v106991 = vsel /*vm=*/%vm106974, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v106995 = vsel /*vm=*/%vm106974, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v106999 = vsel /*vm=*/%vm106974, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v107003 = vsel /*vm=*/%vm106974, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v107007 = vsel /*vm=*/%vm106974, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v107011 = vsel /*vm=*/%vm106974, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v107015 = vadd.f32 -2.5, %v106971
%v107017 = vrsqrt.pop %v106971
%v107018 = vmul.f32 %v107017, %v106971
%vm107019 = vcmp.eq.f32.partialorder %v106971, inf
%v107020 = vsel /*vm=*/%vm107019, /*on_true_vy=*/%v106971, /*on_false_vx=*/%v107018
%vm107021 = vcmp.eq.f32.partialorder %v106971, 0.0
%v107022 = vand.u32 2147483648, %v106971
%v107023 = vsel /*vm=*/%vm107021, /*on_true_vy=*/%v107022, /*on_false_vx=*/%v107020
%v107026 = vadd.f32 -3.0, %v107023
%v107030 = vsel /*vm=*/%vm106974, /*on_true_vy=*/%v107015, /*on_false_vx=*/%v107026
%v107034 = vmul.f32 %v107030, %v107011
%v107038 = vadd.f32 %v107034, %v107007
%v107042 = vmul.f32 %v107038, %v107030
%v107046 = vadd.f32 %v107042, %v107003
%v107050 = vmul.f32 %v107046, %v107030
%v107054 = vadd.f32 %v107050, %v106999
%v107058 = vmul.f32 %v107054, %v107030
%v107062 = vadd.f32 %v107058, %v106995
%v107066 = vmul.f32 %v107062, %v107030
%v107070 = vadd.f32 %v107066, %v106991
%v107074 = vmul.f32 %v107070, %v107030
%v107078 = vadd.f32 %v107074, %v106987
%v107082 = vmul.f32 %v107078, %v107030
%v107086 = vadd.f32 %v107082, %v106983
%v107090 = vmul.f32 %v107086, %v107030
%v107094 = vadd.f32 %v107090, %v106979
%v107098 = vmul.f32 %v107094, %v106945
%v107102 = vsel /*vm=*/%vm106950, /*on_true_vy=*/%v106955, /*on_false_vx=*/%v107098
%v107106 = vmul.f32 1.4140625, %v107102
%v107109 = vpack.c.bf16 %v120417, %v107106
%120317 = vst [vmem:[%s280 + $0x270] sm:$0xf] /*vst_source=*/%v107109
%v107113 = vadd.s32 %v104805, %v2842
%v107123 = vadd.s32 %v107113, %v415
%vm107127 = vcmp.lt.u32.totalorder %v107123, %v107113
%vm107132 = vcmp.lt.u32.totalorder %v107113, %v2842
%v107137 = vadd.s32 %v104788, %v2829
%v107141 = vadd.s32 1, %v107137
%v107145 = vsel /*vm=*/%vm107132, /*on_true_vy=*/%v107141, /*on_false_vx=*/%v107137
%v107149 = vadd.s32 1, %v107145
%v107153 = vsel /*vm=*/%vm107127, /*on_true_vy=*/%v107149, /*on_false_vx=*/%v107145
%v107158 = vadd.s32 %v107153, %v10
%v107162 = vadd.s32 %v107123, %v9
%v107166 = vadd.s32 %v107162, %v107158
%v107168 = vshll.u32 %v107162, 13
%v107169 = vshrl.u32 %v107162, 19
%v107170 = vor.u32 %v107169, %v107168
%v107171 = vxor.u32 %v107170, %v107166
%v107174 = vadd.s32 %v107171, %v107166
%v107176 = vshll.u32 %v107171, 15
%v107177 = vshrl.u32 %v107171, 17
%v107178 = vor.u32 %v107177, %v107176
%v107179 = vxor.u32 %v107178, %v107174
%v107182 = vadd.s32 %v107179, %v107174
%v107184 = vshll.u32 %v107179, 26
%v107185 = vshrl.u32 %v107179, 6
%v107186 = vor.u32 %v107185, %v107184
%v107187 = vxor.u32 %v107186, %v107182
%v107190 = vadd.s32 %v107187, %v107182
%v107194 = vadd.s32 %v107190, %v9
%v107196 = vshll.u32 %v107187, 6
%v107197 = vshrl.u32 %v107187, 26
%v107198 = vor.u32 %v107197, %v107196
%v107199 = vxor.u32 %v107198, %v107190
%v107202 = vadd.s32 %v107199, %v8
%v107206 = vadd.s32 1, %v107202
%v107210 = vadd.s32 %v107206, %v107194
%v107212 = vshll.u32 %v107206, 17
%v107213 = vshrl.u32 %v107206, 15
%v107214 = vor.u32 %v107213, %v107212
%v107215 = vxor.u32 %v107214, %v107210
%v107218 = vadd.s32 %v107215, %v107210
%v107220 = vshll.u32 %v107215, 29
%v107221 = vshrl.u32 %v107215, 3
%v107222 = vor.u32 %v107221, %v107220
%v107223 = vxor.u32 %v107222, %v107218
%v107226 = vadd.s32 %v107223, %v107218
%v107228 = vshll.u32 %v107223, 16
%v107229 = vshrl.u32 %v107223, 16
%v107230 = vor.u32 %v107229, %v107228
%v107231 = vxor.u32 %v107230, %v107226
%v107234 = vadd.s32 %v107231, %v107226
%v107238 = vadd.s32 %v107234, %v8
%v107240 = vshll.u32 %v107231, 24
%v107241 = vshrl.u32 %v107231, 8
%v107242 = vor.u32 %v107241, %v107240
%v107243 = vxor.u32 %v107242, %v107234
%v107246 = vadd.s32 %v107243, %v10
%v107250 = vadd.s32 2, %v107246
%v107254 = vadd.s32 %v107250, %v107238
%v107256 = vshll.u32 %v107250, 13
%v107257 = vshrl.u32 %v107250, 19
%v107258 = vor.u32 %v107257, %v107256
%v107259 = vxor.u32 %v107258, %v107254
%v107262 = vadd.s32 %v107259, %v107254
%v107264 = vshll.u32 %v107259, 15
%v107265 = vshrl.u32 %v107259, 17
%v107266 = vor.u32 %v107265, %v107264
%v107267 = vxor.u32 %v107266, %v107262
%v107270 = vadd.s32 %v107267, %v107262
%v107272 = vshll.u32 %v107267, 26
%v107273 = vshrl.u32 %v107267, 6
%v107274 = vor.u32 %v107273, %v107272
%v107275 = vxor.u32 %v107274, %v107270
%v107278 = vadd.s32 %v107275, %v107270
%v107282 = vadd.s32 %v107278, %v10
%v107284 = vshll.u32 %v107275, 6
%v107285 = vshrl.u32 %v107275, 26
%v107286 = vor.u32 %v107285, %v107284
%v107287 = vxor.u32 %v107286, %v107278
%v107290 = vadd.s32 %v107287, %v9
%v107294 = vadd.s32 3, %v107290
%v107298 = vadd.s32 %v107294, %v107282
%v107300 = vshll.u32 %v107294, 17
%v107301 = vshrl.u32 %v107294, 15
%v107302 = vor.u32 %v107301, %v107300
%v107303 = vxor.u32 %v107302, %v107298
%v107306 = vadd.s32 %v107303, %v107298
%v107308 = vshll.u32 %v107303, 29
%v107309 = vshrl.u32 %v107303, 3
%v107310 = vor.u32 %v107309, %v107308
%v107311 = vxor.u32 %v107310, %v107306
%v107314 = vadd.s32 %v107311, %v107306
%v107316 = vshll.u32 %v107311, 16
%v107317 = vshrl.u32 %v107311, 16
%v107318 = vor.u32 %v107317, %v107316
%v107319 = vxor.u32 %v107318, %v107314
%v107322 = vadd.s32 %v107319, %v107314
%v107326 = vadd.s32 %v107322, %v9
%v107328 = vshll.u32 %v107319, 24
%v107329 = vshrl.u32 %v107319, 8
%v107330 = vor.u32 %v107329, %v107328
%v107331 = vxor.u32 %v107330, %v107322
%v107334 = vadd.s32 %v107331, %v8
%v107338 = vadd.s32 4, %v107334
%v107342 = vadd.s32 %v107338, %v107326
%v107344 = vshll.u32 %v107338, 13
%v107345 = vshrl.u32 %v107338, 19
%v107346 = vor.u32 %v107345, %v107344
%v107347 = vxor.u32 %v107346, %v107342
%v107350 = vadd.s32 %v107347, %v107342
%v107352 = vshll.u32 %v107347, 15
%v107353 = vshrl.u32 %v107347, 17
%v107354 = vor.u32 %v107353, %v107352
%v107355 = vxor.u32 %v107354, %v107350
%v107358 = vadd.s32 %v107355, %v107350
%v107360 = vshll.u32 %v107355, 26
%v107361 = vshrl.u32 %v107355, 6
%v107362 = vor.u32 %v107361, %v107360
%v107363 = vxor.u32 %v107362, %v107358
%v107366 = vadd.s32 %v107363, %v107358
%v107370 = vadd.s32 %v107366, %v8
%v107372 = vshll.u32 %v107363, 6
%v107373 = vshrl.u32 %v107363, 26
%v107374 = vor.u32 %v107373, %v107372
%v107375 = vxor.u32 %v107374, %v107366
%v107378 = vadd.s32 %v107375, %v10
%v107382 = vadd.s32 5, %v107378
%v107384 = vxor.u32 %v107382, %v107370
%v107385 = vand.u32.u8 255, %v107384
%v107386 = vand.u32 65535, %v107385
%v107387 = vshrl.u32 %v107386, 1
%v107388 = vor.u32 16256, %v107387
%v107389 = vand.u32.u16 65535, %v107388
%v120318 = vadd.low.f32.bf16 -1.0, %v107389
%v107398 = vmul.f32 2.0, %v120318
%v107402 = vadd.f32 -0.99609375, %v107398
%v107406 = vmax.f32 %v107402, -0.99609375
%v107408 = vand.u32 2147483647, %v107406
%vm107411 = vcmp.eq.f32.partialorder %v107408, 1.0
%v107416 = vmul.f32 inf, %v107406
%v107418 = vxor.u32 2147483648, %v107406
%v107421 = vmul.f32 %v107418, %v107406
%v107423 = vadd.f32 1.0, %v107421
%v107424 = vlog2.pop %v107423
%v107425 = vmul.f32 0.6931472, %v107424
%v107426 = vmul.f32 -0.5, %v107421
%v107427 = vadd.f32 1.0, %v107426
%v107428 = vmul.f32 %v107427, %v107421
%v107429 = vand.u32 2147483647, %v107421
%vm107430 = vcmp.lt.f32.partialorder %v107429, 0.0004427343
%v107431 = vsel /*vm=*/%vm107430, /*on_true_vy=*/%v107428, /*on_false_vx=*/%v107425
%v107432 = vxor.u32 2147483648, %v107431
%vm107435 = vcmp.lt.f32.partialorder %v107432, 5.0
%v107440 = vsel /*vm=*/%vm107435, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v107444 = vsel /*vm=*/%vm107435, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v107448 = vsel /*vm=*/%vm107435, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v107452 = vsel /*vm=*/%vm107435, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v107456 = vsel /*vm=*/%vm107435, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v107460 = vsel /*vm=*/%vm107435, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v107464 = vsel /*vm=*/%vm107435, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v107468 = vsel /*vm=*/%vm107435, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v107472 = vsel /*vm=*/%vm107435, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v107476 = vadd.f32 -2.5, %v107432
%v107478 = vrsqrt.pop %v107432
%v107479 = vmul.f32 %v107478, %v107432
%vm107480 = vcmp.eq.f32.partialorder %v107432, inf
%v107481 = vsel /*vm=*/%vm107480, /*on_true_vy=*/%v107432, /*on_false_vx=*/%v107479
%vm107482 = vcmp.eq.f32.partialorder %v107432, 0.0
%v107483 = vand.u32 2147483648, %v107432
%v107484 = vsel /*vm=*/%vm107482, /*on_true_vy=*/%v107483, /*on_false_vx=*/%v107481
%v107487 = vadd.f32 -3.0, %v107484
%v107491 = vsel /*vm=*/%vm107435, /*on_true_vy=*/%v107476, /*on_false_vx=*/%v107487
%v107495 = vmul.f32 %v107491, %v107472
%v107499 = vadd.f32 %v107495, %v107468
%v107503 = vmul.f32 %v107499, %v107491
%v107507 = vadd.f32 %v107503, %v107464
%v107511 = vmul.f32 %v107507, %v107491
%v107515 = vadd.f32 %v107511, %v107460
%v107519 = vmul.f32 %v107515, %v107491
%v107523 = vadd.f32 %v107519, %v107456
%v107527 = vmul.f32 %v107523, %v107491
%v107531 = vadd.f32 %v107527, %v107452
%v107535 = vmul.f32 %v107531, %v107491
%v107539 = vadd.f32 %v107535, %v107448
%v107543 = vmul.f32 %v107539, %v107491
%v107547 = vadd.f32 %v107543, %v107444
%v107551 = vmul.f32 %v107547, %v107491
%v107555 = vadd.f32 %v107551, %v107440
%v107559 = vmul.f32 %v107555, %v107406
%v107563 = vsel /*vm=*/%vm107411, /*on_true_vy=*/%v107416, /*on_false_vx=*/%v107559
%v107567 = vmul.f32 1.4140625, %v107563
%v107570 = vpack.c.bf16 %v120417, %v107567
%120319 = vst [vmem:[%s280 + $0x2f0] sm:$0xf] /*vst_source=*/%v107570
%v107574 = vadd.s32 %v104805, %v3329
%v107584 = vadd.s32 %v107574, %v415
%vm107588 = vcmp.lt.u32.totalorder %v107584, %v107574
%vm107593 = vcmp.lt.u32.totalorder %v107574, %v3329
%v107598 = vadd.s32 %v104788, %v3316
%v107602 = vadd.s32 1, %v107598
%v107606 = vsel /*vm=*/%vm107593, /*on_true_vy=*/%v107602, /*on_false_vx=*/%v107598
%v107610 = vadd.s32 1, %v107606
%v107614 = vsel /*vm=*/%vm107588, /*on_true_vy=*/%v107610, /*on_false_vx=*/%v107606
%v107619 = vadd.s32 %v107614, %v10
%v107623 = vadd.s32 %v107584, %v9
%v107627 = vadd.s32 %v107623, %v107619
%v107629 = vshll.u32 %v107623, 13
%v107630 = vshrl.u32 %v107623, 19
%v107631 = vor.u32 %v107630, %v107629
%v107632 = vxor.u32 %v107631, %v107627
%v107635 = vadd.s32 %v107632, %v107627
%v107637 = vshll.u32 %v107632, 15
%v107638 = vshrl.u32 %v107632, 17
%v107639 = vor.u32 %v107638, %v107637
%v107640 = vxor.u32 %v107639, %v107635
%v107643 = vadd.s32 %v107640, %v107635
%v107645 = vshll.u32 %v107640, 26
%v107646 = vshrl.u32 %v107640, 6
%v107647 = vor.u32 %v107646, %v107645
%v107648 = vxor.u32 %v107647, %v107643
%v107651 = vadd.s32 %v107648, %v107643
%v107655 = vadd.s32 %v107651, %v9
%v107657 = vshll.u32 %v107648, 6
%v107658 = vshrl.u32 %v107648, 26
%v107659 = vor.u32 %v107658, %v107657
%v107660 = vxor.u32 %v107659, %v107651
%v107663 = vadd.s32 %v107660, %v8
%v107667 = vadd.s32 1, %v107663
%v107671 = vadd.s32 %v107667, %v107655
%v107673 = vshll.u32 %v107667, 17
%v107674 = vshrl.u32 %v107667, 15
%v107675 = vor.u32 %v107674, %v107673
%v107676 = vxor.u32 %v107675, %v107671
%v107679 = vadd.s32 %v107676, %v107671
%v107681 = vshll.u32 %v107676, 29
%v107682 = vshrl.u32 %v107676, 3
%v107683 = vor.u32 %v107682, %v107681
%v107684 = vxor.u32 %v107683, %v107679
%v107687 = vadd.s32 %v107684, %v107679
%v107689 = vshll.u32 %v107684, 16
%v107690 = vshrl.u32 %v107684, 16
%v107691 = vor.u32 %v107690, %v107689
%v107692 = vxor.u32 %v107691, %v107687
%v107695 = vadd.s32 %v107692, %v107687
%v107699 = vadd.s32 %v107695, %v8
%v107701 = vshll.u32 %v107692, 24
%v107702 = vshrl.u32 %v107692, 8
%v107703 = vor.u32 %v107702, %v107701
%v107704 = vxor.u32 %v107703, %v107695
%v107707 = vadd.s32 %v107704, %v10
%v107711 = vadd.s32 2, %v107707
%v107715 = vadd.s32 %v107711, %v107699
%v107717 = vshll.u32 %v107711, 13
%v107718 = vshrl.u32 %v107711, 19
%v107719 = vor.u32 %v107718, %v107717
%v107720 = vxor.u32 %v107719, %v107715
%v107723 = vadd.s32 %v107720, %v107715
%v107725 = vshll.u32 %v107720, 15
%v107726 = vshrl.u32 %v107720, 17
%v107727 = vor.u32 %v107726, %v107725
%v107728 = vxor.u32 %v107727, %v107723
%v107731 = vadd.s32 %v107728, %v107723
%v107733 = vshll.u32 %v107728, 26
%v107734 = vshrl.u32 %v107728, 6
%v107735 = vor.u32 %v107734, %v107733
%v107736 = vxor.u32 %v107735, %v107731
%v107739 = vadd.s32 %v107736, %v107731
%v107743 = vadd.s32 %v107739, %v10
%v107745 = vshll.u32 %v107736, 6
%v107746 = vshrl.u32 %v107736, 26
%v107747 = vor.u32 %v107746, %v107745
%v107748 = vxor.u32 %v107747, %v107739
%v107751 = vadd.s32 %v107748, %v9
%v107755 = vadd.s32 3, %v107751
%v107759 = vadd.s32 %v107755, %v107743
%v107761 = vshll.u32 %v107755, 17
%v107762 = vshrl.u32 %v107755, 15
%v107763 = vor.u32 %v107762, %v107761
%v107764 = vxor.u32 %v107763, %v107759
%v107767 = vadd.s32 %v107764, %v107759
%v107769 = vshll.u32 %v107764, 29
%v107770 = vshrl.u32 %v107764, 3
%v107771 = vor.u32 %v107770, %v107769
%v107772 = vxor.u32 %v107771, %v107767
%v107775 = vadd.s32 %v107772, %v107767
%v107777 = vshll.u32 %v107772, 16
%v107778 = vshrl.u32 %v107772, 16
%v107779 = vor.u32 %v107778, %v107777
%v107780 = vxor.u32 %v107779, %v107775
%v107783 = vadd.s32 %v107780, %v107775
%v107787 = vadd.s32 %v107783, %v9
%v107789 = vshll.u32 %v107780, 24
%v107790 = vshrl.u32 %v107780, 8
%v107791 = vor.u32 %v107790, %v107789
%v107792 = vxor.u32 %v107791, %v107783
%v107795 = vadd.s32 %v107792, %v8
%v107799 = vadd.s32 4, %v107795
%v107803 = vadd.s32 %v107799, %v107787
%v107805 = vshll.u32 %v107799, 13
%v107806 = vshrl.u32 %v107799, 19
%v107807 = vor.u32 %v107806, %v107805
%v107808 = vxor.u32 %v107807, %v107803
%v107811 = vadd.s32 %v107808, %v107803
%v107813 = vshll.u32 %v107808, 15
%v107814 = vshrl.u32 %v107808, 17
%v107815 = vor.u32 %v107814, %v107813
%v107816 = vxor.u32 %v107815, %v107811
%v107819 = vadd.s32 %v107816, %v107811
%v107821 = vshll.u32 %v107816, 26
%v107822 = vshrl.u32 %v107816, 6
%v107823 = vor.u32 %v107822, %v107821
%v107824 = vxor.u32 %v107823, %v107819
%v107827 = vadd.s32 %v107824, %v107819
%v107831 = vadd.s32 %v107827, %v8
%v107833 = vshll.u32 %v107824, 6
%v107834 = vshrl.u32 %v107824, 26
%v107835 = vor.u32 %v107834, %v107833
%v107836 = vxor.u32 %v107835, %v107827
%v107839 = vadd.s32 %v107836, %v10
%v107843 = vadd.s32 5, %v107839
%v107845 = vxor.u32 %v107843, %v107831
%v107846 = vand.u32.u8 255, %v107845
%v107847 = vand.u32 65535, %v107846
%v107848 = vshrl.u32 %v107847, 1
%v107849 = vor.u32 16256, %v107848
%v107850 = vand.u32.u16 65535, %v107849
%v120320 = vadd.low.f32.bf16 -1.0, %v107850
%v107859 = vmul.f32 2.0, %v120320
%v107863 = vadd.f32 -0.99609375, %v107859
%v107867 = vmax.f32 %v107863, -0.99609375
%v107869 = vand.u32 2147483647, %v107867
%vm107872 = vcmp.eq.f32.partialorder %v107869, 1.0
%v107877 = vmul.f32 inf, %v107867
%v107879 = vxor.u32 2147483648, %v107867
%v107882 = vmul.f32 %v107879, %v107867
%v107884 = vadd.f32 1.0, %v107882
%v107885 = vlog2.pop %v107884
%v107886 = vmul.f32 0.6931472, %v107885
%v107887 = vmul.f32 -0.5, %v107882
%v107888 = vadd.f32 1.0, %v107887
%v107889 = vmul.f32 %v107888, %v107882
%v107890 = vand.u32 2147483647, %v107882
%vm107891 = vcmp.lt.f32.partialorder %v107890, 0.0004427343
%v107892 = vsel /*vm=*/%vm107891, /*on_true_vy=*/%v107889, /*on_false_vx=*/%v107886
%v107893 = vxor.u32 2147483648, %v107892
%vm107896 = vcmp.lt.f32.partialorder %v107893, 5.0
%v107901 = vsel /*vm=*/%vm107896, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v107905 = vsel /*vm=*/%vm107896, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v107909 = vsel /*vm=*/%vm107896, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v107913 = vsel /*vm=*/%vm107896, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v107917 = vsel /*vm=*/%vm107896, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v107921 = vsel /*vm=*/%vm107896, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v107925 = vsel /*vm=*/%vm107896, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v107929 = vsel /*vm=*/%vm107896, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v107933 = vsel /*vm=*/%vm107896, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v107937 = vadd.f32 -2.5, %v107893
%v107939 = vrsqrt.pop %v107893
%v107940 = vmul.f32 %v107939, %v107893
%vm107941 = vcmp.eq.f32.partialorder %v107893, inf
%v107942 = vsel /*vm=*/%vm107941, /*on_true_vy=*/%v107893, /*on_false_vx=*/%v107940
%vm107943 = vcmp.eq.f32.partialorder %v107893, 0.0
%v107944 = vand.u32 2147483648, %v107893
%v107945 = vsel /*vm=*/%vm107943, /*on_true_vy=*/%v107944, /*on_false_vx=*/%v107942
%v107948 = vadd.f32 -3.0, %v107945
%v107952 = vsel /*vm=*/%vm107896, /*on_true_vy=*/%v107937, /*on_false_vx=*/%v107948
%v107956 = vmul.f32 %v107952, %v107933
%v107960 = vadd.f32 %v107956, %v107929
%v107964 = vmul.f32 %v107960, %v107952
%v107968 = vadd.f32 %v107964, %v107925
%v107972 = vmul.f32 %v107968, %v107952
%v107976 = vadd.f32 %v107972, %v107921
%v107980 = vmul.f32 %v107976, %v107952
%v107984 = vadd.f32 %v107980, %v107917
%v107988 = vmul.f32 %v107984, %v107952
%v107992 = vadd.f32 %v107988, %v107913
%v107996 = vmul.f32 %v107992, %v107952
%v108000 = vadd.f32 %v107996, %v107909
%v108004 = vmul.f32 %v108000, %v107952
%v108008 = vadd.f32 %v108004, %v107905
%v108012 = vmul.f32 %v108008, %v107952
%v108016 = vadd.f32 %v108012, %v107901
%v108020 = vmul.f32 %v108016, %v107867
%v108024 = vsel /*vm=*/%vm107872, /*on_true_vy=*/%v107877, /*on_false_vx=*/%v108020
%v108028 = vmul.f32 1.4140625, %v108024
%v108031 = vpack.c.bf16 %v120417, %v108028
%120321 = vst [vmem:[%s280 + $0x370] sm:$0xf] /*vst_source=*/%v108031
%v108035 = vadd.s32 %v104805, %v3816
%v108045 = vadd.s32 %v108035, %v415
%vm108049 = vcmp.lt.u32.totalorder %v108045, %v108035
%vm108054 = vcmp.lt.u32.totalorder %v108035, %v3816
%v108059 = vadd.s32 %v104788, %v3803
%v108063 = vadd.s32 1, %v108059
%v108067 = vsel /*vm=*/%vm108054, /*on_true_vy=*/%v108063, /*on_false_vx=*/%v108059
%v108071 = vadd.s32 1, %v108067
%v108075 = vsel /*vm=*/%vm108049, /*on_true_vy=*/%v108071, /*on_false_vx=*/%v108067
%v108080 = vadd.s32 %v108075, %v10
%v108084 = vadd.s32 %v108045, %v9
%v108088 = vadd.s32 %v108084, %v108080
%v108090 = vshll.u32 %v108084, 13
%v108091 = vshrl.u32 %v108084, 19
%v108092 = vor.u32 %v108091, %v108090
%v108093 = vxor.u32 %v108092, %v108088
%v108096 = vadd.s32 %v108093, %v108088
%v108098 = vshll.u32 %v108093, 15
%v108099 = vshrl.u32 %v108093, 17
%v108100 = vor.u32 %v108099, %v108098
%v108101 = vxor.u32 %v108100, %v108096
%v108104 = vadd.s32 %v108101, %v108096
%v108106 = vshll.u32 %v108101, 26
%v108107 = vshrl.u32 %v108101, 6
%v108108 = vor.u32 %v108107, %v108106
%v108109 = vxor.u32 %v108108, %v108104
%v108112 = vadd.s32 %v108109, %v108104
%v108116 = vadd.s32 %v108112, %v9
%v108118 = vshll.u32 %v108109, 6
%v108119 = vshrl.u32 %v108109, 26
%v108120 = vor.u32 %v108119, %v108118
%v108121 = vxor.u32 %v108120, %v108112
%v108124 = vadd.s32 %v108121, %v8
%v108128 = vadd.s32 1, %v108124
%v108132 = vadd.s32 %v108128, %v108116
%v108134 = vshll.u32 %v108128, 17
%v108135 = vshrl.u32 %v108128, 15
%v108136 = vor.u32 %v108135, %v108134
%v108137 = vxor.u32 %v108136, %v108132
%v108140 = vadd.s32 %v108137, %v108132
%v108142 = vshll.u32 %v108137, 29
%v108143 = vshrl.u32 %v108137, 3
%v108144 = vor.u32 %v108143, %v108142
%v108145 = vxor.u32 %v108144, %v108140
%v108148 = vadd.s32 %v108145, %v108140
%v108150 = vshll.u32 %v108145, 16
%v108151 = vshrl.u32 %v108145, 16
%v108152 = vor.u32 %v108151, %v108150
%v108153 = vxor.u32 %v108152, %v108148
%v108156 = vadd.s32 %v108153, %v108148
%v108160 = vadd.s32 %v108156, %v8
%v108162 = vshll.u32 %v108153, 24
%v108163 = vshrl.u32 %v108153, 8
%v108164 = vor.u32 %v108163, %v108162
%v108165 = vxor.u32 %v108164, %v108156
%v108168 = vadd.s32 %v108165, %v10
%v108172 = vadd.s32 2, %v108168
%v108176 = vadd.s32 %v108172, %v108160
%v108178 = vshll.u32 %v108172, 13
%v108179 = vshrl.u32 %v108172, 19
%v108180 = vor.u32 %v108179, %v108178
%v108181 = vxor.u32 %v108180, %v108176
%v108184 = vadd.s32 %v108181, %v108176
%v108186 = vshll.u32 %v108181, 15
%v108187 = vshrl.u32 %v108181, 17
%v108188 = vor.u32 %v108187, %v108186
%v108189 = vxor.u32 %v108188, %v108184
%v108192 = vadd.s32 %v108189, %v108184
%v108194 = vshll.u32 %v108189, 26
%v108195 = vshrl.u32 %v108189, 6
%v108196 = vor.u32 %v108195, %v108194
%v108197 = vxor.u32 %v108196, %v108192
%v108200 = vadd.s32 %v108197, %v108192
%v108204 = vadd.s32 %v108200, %v10
%v108206 = vshll.u32 %v108197, 6
%v108207 = vshrl.u32 %v108197, 26
%v108208 = vor.u32 %v108207, %v108206
%v108209 = vxor.u32 %v108208, %v108200
%v108212 = vadd.s32 %v108209, %v9
%v108216 = vadd.s32 3, %v108212
%v108220 = vadd.s32 %v108216, %v108204
%v108222 = vshll.u32 %v108216, 17
%v108223 = vshrl.u32 %v108216, 15
%v108224 = vor.u32 %v108223, %v108222
%v108225 = vxor.u32 %v108224, %v108220
%v108228 = vadd.s32 %v108225, %v108220
%v108230 = vshll.u32 %v108225, 29
%v108231 = vshrl.u32 %v108225, 3
%v108232 = vor.u32 %v108231, %v108230
%v108233 = vxor.u32 %v108232, %v108228
%v108236 = vadd.s32 %v108233, %v108228
%v108238 = vshll.u32 %v108233, 16
%v108239 = vshrl.u32 %v108233, 16
%v108240 = vor.u32 %v108239, %v108238
%v108241 = vxor.u32 %v108240, %v108236
%v108244 = vadd.s32 %v108241, %v108236
%v108248 = vadd.s32 %v108244, %v9
%v108250 = vshll.u32 %v108241, 24
%v108251 = vshrl.u32 %v108241, 8
%v108252 = vor.u32 %v108251, %v108250
%v108253 = vxor.u32 %v108252, %v108244
%v108256 = vadd.s32 %v108253, %v8
%v108260 = vadd.s32 4, %v108256
%v108264 = vadd.s32 %v108260, %v108248
%v108266 = vshll.u32 %v108260, 13
%v108267 = vshrl.u32 %v108260, 19
%v108268 = vor.u32 %v108267, %v108266
%v108269 = vxor.u32 %v108268, %v108264
%v108272 = vadd.s32 %v108269, %v108264
%v108274 = vshll.u32 %v108269, 15
%v108275 = vshrl.u32 %v108269, 17
%v108276 = vor.u32 %v108275, %v108274
%v108277 = vxor.u32 %v108276, %v108272
%v108280 = vadd.s32 %v108277, %v108272
%v108282 = vshll.u32 %v108277, 26
%v108283 = vshrl.u32 %v108277, 6
%v108284 = vor.u32 %v108283, %v108282
%v108285 = vxor.u32 %v108284, %v108280
%v108288 = vadd.s32 %v108285, %v108280
%v108292 = vadd.s32 %v108288, %v8
%v108294 = vshll.u32 %v108285, 6
%v108295 = vshrl.u32 %v108285, 26
%v108296 = vor.u32 %v108295, %v108294
%v108297 = vxor.u32 %v108296, %v108288
%v108300 = vadd.s32 %v108297, %v10
%v108304 = vadd.s32 5, %v108300
%v108306 = vxor.u32 %v108304, %v108292
%v108307 = vand.u32.u8 255, %v108306
%v108308 = vand.u32 65535, %v108307
%v108309 = vshrl.u32 %v108308, 1
%v108310 = vor.u32 16256, %v108309
%v108311 = vand.u32.u16 65535, %v108310
%v120322 = vadd.low.f32.bf16 -1.0, %v108311
%v108320 = vmul.f32 2.0, %v120322
%v108324 = vadd.f32 -0.99609375, %v108320
%v108328 = vmax.f32 %v108324, -0.99609375
%v108330 = vand.u32 2147483647, %v108328
%vm108333 = vcmp.eq.f32.partialorder %v108330, 1.0
%v108338 = vmul.f32 inf, %v108328
%v108340 = vxor.u32 2147483648, %v108328
%v108343 = vmul.f32 %v108340, %v108328
%v108345 = vadd.f32 1.0, %v108343
%v108346 = vlog2.pop %v108345
%v108347 = vmul.f32 0.6931472, %v108346
%v108348 = vmul.f32 -0.5, %v108343
%v108349 = vadd.f32 1.0, %v108348
%v108350 = vmul.f32 %v108349, %v108343
%v108351 = vand.u32 2147483647, %v108343
%vm108352 = vcmp.lt.f32.partialorder %v108351, 0.0004427343
%v108353 = vsel /*vm=*/%vm108352, /*on_true_vy=*/%v108350, /*on_false_vx=*/%v108347
%v108354 = vxor.u32 2147483648, %v108353
%vm108357 = vcmp.lt.f32.partialorder %v108354, 5.0
%v108362 = vsel /*vm=*/%vm108357, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v108366 = vsel /*vm=*/%vm108357, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v108370 = vsel /*vm=*/%vm108357, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v108374 = vsel /*vm=*/%vm108357, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v108378 = vsel /*vm=*/%vm108357, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v108382 = vsel /*vm=*/%vm108357, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v108386 = vsel /*vm=*/%vm108357, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v108390 = vsel /*vm=*/%vm108357, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v108394 = vsel /*vm=*/%vm108357, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v108398 = vadd.f32 -2.5, %v108354
%v108400 = vrsqrt.pop %v108354
%v108401 = vmul.f32 %v108400, %v108354
%vm108402 = vcmp.eq.f32.partialorder %v108354, inf
%v108403 = vsel /*vm=*/%vm108402, /*on_true_vy=*/%v108354, /*on_false_vx=*/%v108401
%vm108404 = vcmp.eq.f32.partialorder %v108354, 0.0
%v108405 = vand.u32 2147483648, %v108354
%v108406 = vsel /*vm=*/%vm108404, /*on_true_vy=*/%v108405, /*on_false_vx=*/%v108403
%v108409 = vadd.f32 -3.0, %v108406
%v108413 = vsel /*vm=*/%vm108357, /*on_true_vy=*/%v108398, /*on_false_vx=*/%v108409
%v108417 = vmul.f32 %v108413, %v108394
%v108421 = vadd.f32 %v108417, %v108390
%v108425 = vmul.f32 %v108421, %v108413
%v108429 = vadd.f32 %v108425, %v108386
%v108433 = vmul.f32 %v108429, %v108413
%v108437 = vadd.f32 %v108433, %v108382
%v108441 = vmul.f32 %v108437, %v108413
%v108445 = vadd.f32 %v108441, %v108378
%v108449 = vmul.f32 %v108445, %v108413
%v108453 = vadd.f32 %v108449, %v108374
%v108457 = vmul.f32 %v108453, %v108413
%v108461 = vadd.f32 %v108457, %v108370
%v108465 = vmul.f32 %v108461, %v108413
%v108469 = vadd.f32 %v108465, %v108366
%v108473 = vmul.f32 %v108469, %v108413
%v108477 = vadd.f32 %v108473, %v108362
%v108481 = vmul.f32 %v108477, %v108328
%v108485 = vsel /*vm=*/%vm108333, /*on_true_vy=*/%v108338, /*on_false_vx=*/%v108481
%v108489 = vmul.f32 1.4140625, %v108485
%v108492 = vpack.c.bf16 %v120417, %v108489
%120323 = vst [vmem:[%s280 + $0x3f0] sm:$0xf] /*vst_source=*/%v108492
%v108530 = vadd.s32 %v108527, %v408
%v108540 = vadd.s32 %v108530, %v415
%vm108544 = vcmp.lt.u32.totalorder %v108540, %v108530
%vm108549 = vcmp.lt.u32.totalorder %v108530, %v408
%v108554 = vadd.s32 %v108510, %v380
%v108558 = vadd.s32 1, %v108554
%v108562 = vsel /*vm=*/%vm108549, /*on_true_vy=*/%v108558, /*on_false_vx=*/%v108554
%v108566 = vadd.s32 1, %v108562
%v108570 = vsel /*vm=*/%vm108544, /*on_true_vy=*/%v108566, /*on_false_vx=*/%v108562
%v108575 = vadd.s32 %v108570, %v10
%v108579 = vadd.s32 %v108540, %v9
%v108583 = vadd.s32 %v108579, %v108575
%v108585 = vshll.u32 %v108579, 13
%v108586 = vshrl.u32 %v108579, 19
%v108587 = vor.u32 %v108586, %v108585
%v108588 = vxor.u32 %v108587, %v108583
%v108591 = vadd.s32 %v108588, %v108583
%v108593 = vshll.u32 %v108588, 15
%v108594 = vshrl.u32 %v108588, 17
%v108595 = vor.u32 %v108594, %v108593
%v108596 = vxor.u32 %v108595, %v108591
%v108599 = vadd.s32 %v108596, %v108591
%v108601 = vshll.u32 %v108596, 26
%v108602 = vshrl.u32 %v108596, 6
%v108603 = vor.u32 %v108602, %v108601
%v108604 = vxor.u32 %v108603, %v108599
%v108607 = vadd.s32 %v108604, %v108599
%v108611 = vadd.s32 %v108607, %v9
%v108613 = vshll.u32 %v108604, 6
%v108614 = vshrl.u32 %v108604, 26
%v108615 = vor.u32 %v108614, %v108613
%v108616 = vxor.u32 %v108615, %v108607
%v108619 = vadd.s32 %v108616, %v8
%v108623 = vadd.s32 1, %v108619
%v108627 = vadd.s32 %v108623, %v108611
%v108629 = vshll.u32 %v108623, 17
%v108630 = vshrl.u32 %v108623, 15
%v108631 = vor.u32 %v108630, %v108629
%v108632 = vxor.u32 %v108631, %v108627
%v108635 = vadd.s32 %v108632, %v108627
%v108637 = vshll.u32 %v108632, 29
%v108638 = vshrl.u32 %v108632, 3
%v108639 = vor.u32 %v108638, %v108637
%v108640 = vxor.u32 %v108639, %v108635
%v108643 = vadd.s32 %v108640, %v108635
%v108645 = vshll.u32 %v108640, 16
%v108646 = vshrl.u32 %v108640, 16
%v108647 = vor.u32 %v108646, %v108645
%v108648 = vxor.u32 %v108647, %v108643
%v108651 = vadd.s32 %v108648, %v108643
%v108655 = vadd.s32 %v108651, %v8
%v108657 = vshll.u32 %v108648, 24
%v108658 = vshrl.u32 %v108648, 8
%v108659 = vor.u32 %v108658, %v108657
%v108660 = vxor.u32 %v108659, %v108651
%v108663 = vadd.s32 %v108660, %v10
%v108667 = vadd.s32 2, %v108663
%v108671 = vadd.s32 %v108667, %v108655
%v108673 = vshll.u32 %v108667, 13
%v108674 = vshrl.u32 %v108667, 19
%v108675 = vor.u32 %v108674, %v108673
%v108676 = vxor.u32 %v108675, %v108671
%v108679 = vadd.s32 %v108676, %v108671
%v108681 = vshll.u32 %v108676, 15
%v108682 = vshrl.u32 %v108676, 17
%v108683 = vor.u32 %v108682, %v108681
%v108684 = vxor.u32 %v108683, %v108679
%v108687 = vadd.s32 %v108684, %v108679
%v108689 = vshll.u32 %v108684, 26
%v108690 = vshrl.u32 %v108684, 6
%v108691 = vor.u32 %v108690, %v108689
%v108692 = vxor.u32 %v108691, %v108687
%v108695 = vadd.s32 %v108692, %v108687
%v108699 = vadd.s32 %v108695, %v10
%v108701 = vshll.u32 %v108692, 6
%v108702 = vshrl.u32 %v108692, 26
%v108703 = vor.u32 %v108702, %v108701
%v108704 = vxor.u32 %v108703, %v108695
%v108707 = vadd.s32 %v108704, %v9
%v108711 = vadd.s32 3, %v108707
%v108715 = vadd.s32 %v108711, %v108699
%v108717 = vshll.u32 %v108711, 17
%v108718 = vshrl.u32 %v108711, 15
%v108719 = vor.u32 %v108718, %v108717
%v108720 = vxor.u32 %v108719, %v108715
%v108723 = vadd.s32 %v108720, %v108715
%v108725 = vshll.u32 %v108720, 29
%v108726 = vshrl.u32 %v108720, 3
%v108727 = vor.u32 %v108726, %v108725
%v108728 = vxor.u32 %v108727, %v108723
%v108731 = vadd.s32 %v108728, %v108723
%v108733 = vshll.u32 %v108728, 16
%v108734 = vshrl.u32 %v108728, 16
%v108735 = vor.u32 %v108734, %v108733
%v108736 = vxor.u32 %v108735, %v108731
%v108739 = vadd.s32 %v108736, %v108731
%v108743 = vadd.s32 %v108739, %v9
%v108745 = vshll.u32 %v108736, 24
%v108746 = vshrl.u32 %v108736, 8
%v108747 = vor.u32 %v108746, %v108745
%v108748 = vxor.u32 %v108747, %v108739
%v108751 = vadd.s32 %v108748, %v8
%v108755 = vadd.s32 4, %v108751
%v108759 = vadd.s32 %v108755, %v108743
%v108761 = vshll.u32 %v108755, 13
%v108762 = vshrl.u32 %v108755, 19
%v108763 = vor.u32 %v108762, %v108761
%v108764 = vxor.u32 %v108763, %v108759
%v108767 = vadd.s32 %v108764, %v108759
%v108769 = vshll.u32 %v108764, 15
%v108770 = vshrl.u32 %v108764, 17
%v108771 = vor.u32 %v108770, %v108769
%v108772 = vxor.u32 %v108771, %v108767
%v108775 = vadd.s32 %v108772, %v108767
%v108777 = vshll.u32 %v108772, 26
%v108778 = vshrl.u32 %v108772, 6
%v108779 = vor.u32 %v108778, %v108777
%v108780 = vxor.u32 %v108779, %v108775
%v108783 = vadd.s32 %v108780, %v108775
%v108787 = vadd.s32 %v108783, %v8
%v108789 = vshll.u32 %v108780, 6
%v108790 = vshrl.u32 %v108780, 26
%v108791 = vor.u32 %v108790, %v108789
%v108792 = vxor.u32 %v108791, %v108783
%v108795 = vadd.s32 %v108792, %v10
%v108799 = vadd.s32 5, %v108795
%v108801 = vxor.u32 %v108799, %v108787
%v108802 = vand.u32.u8 255, %v108801
%v108803 = vand.u32 65535, %v108802
%v108804 = vshrl.u32 %v108803, 1
%v108805 = vor.u32 16256, %v108804
%v108806 = vand.u32.u16 65535, %v108805
%v120328 = vadd.low.f32.bf16 -1.0, %v108806
%v108815 = vmul.f32 2.0, %v120328
%v108819 = vadd.f32 -0.99609375, %v108815
%v108823 = vmax.f32 %v108819, -0.99609375
%v108825 = vand.u32 2147483647, %v108823
%vm108828 = vcmp.eq.f32.partialorder %v108825, 1.0
%v108833 = vmul.f32 inf, %v108823
%v108835 = vxor.u32 2147483648, %v108823
%v108838 = vmul.f32 %v108835, %v108823
%v108840 = vadd.f32 1.0, %v108838
%v108841 = vlog2.pop %v108840
%v108842 = vmul.f32 0.6931472, %v108841
%v108843 = vmul.f32 -0.5, %v108838
%v108844 = vadd.f32 1.0, %v108843
%v108845 = vmul.f32 %v108844, %v108838
%v108846 = vand.u32 2147483647, %v108838
%vm108847 = vcmp.lt.f32.partialorder %v108846, 0.0004427343
%v108848 = vsel /*vm=*/%vm108847, /*on_true_vy=*/%v108845, /*on_false_vx=*/%v108842
%v108849 = vxor.u32 2147483648, %v108848
%vm108852 = vcmp.lt.f32.partialorder %v108849, 5.0
%v108857 = vsel /*vm=*/%vm108852, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v108861 = vsel /*vm=*/%vm108852, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v108865 = vsel /*vm=*/%vm108852, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v108869 = vsel /*vm=*/%vm108852, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v108873 = vsel /*vm=*/%vm108852, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v108877 = vsel /*vm=*/%vm108852, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v108881 = vsel /*vm=*/%vm108852, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v108885 = vsel /*vm=*/%vm108852, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v108889 = vsel /*vm=*/%vm108852, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v108893 = vadd.f32 -2.5, %v108849
%v108895 = vrsqrt.pop %v108849
%v108896 = vmul.f32 %v108895, %v108849
%vm108897 = vcmp.eq.f32.partialorder %v108849, inf
%v108898 = vsel /*vm=*/%vm108897, /*on_true_vy=*/%v108849, /*on_false_vx=*/%v108896
%vm108899 = vcmp.eq.f32.partialorder %v108849, 0.0
%v108900 = vand.u32 2147483648, %v108849
%v108901 = vsel /*vm=*/%vm108899, /*on_true_vy=*/%v108900, /*on_false_vx=*/%v108898
%v108904 = vadd.f32 -3.0, %v108901
%v108908 = vsel /*vm=*/%vm108852, /*on_true_vy=*/%v108893, /*on_false_vx=*/%v108904
%v108912 = vmul.f32 %v108908, %v108889
%v108916 = vadd.f32 %v108912, %v108885
%v108920 = vmul.f32 %v108916, %v108908
%v108924 = vadd.f32 %v108920, %v108881
%v108928 = vmul.f32 %v108924, %v108908
%v108932 = vadd.f32 %v108928, %v108877
%v108936 = vmul.f32 %v108932, %v108908
%v108940 = vadd.f32 %v108936, %v108873
%v108944 = vmul.f32 %v108940, %v108908
%v108948 = vadd.f32 %v108944, %v108869
%v108952 = vmul.f32 %v108948, %v108908
%v108956 = vadd.f32 %v108952, %v108865
%v108960 = vmul.f32 %v108956, %v108908
%v108964 = vadd.f32 %v108960, %v108861
%v108968 = vmul.f32 %v108964, %v108908
%v108972 = vadd.f32 %v108968, %v108857
%v108976 = vmul.f32 %v108972, %v108823
%v108980 = vsel /*vm=*/%vm108828, /*on_true_vy=*/%v108833, /*on_false_vx=*/%v108976
%v108984 = vmul.f32 1.4140625, %v108980
%v108987 = vpack.c.bf16 %v120417, %v108984
%120329 = vst [vmem:[%s280 + $0x74] sm:$0xf] /*vst_source=*/%v108987
%v108991 = vadd.s32 %v108527, %v894
%v109001 = vadd.s32 %v108991, %v415
%vm109005 = vcmp.lt.u32.totalorder %v109001, %v108991
%vm109010 = vcmp.lt.u32.totalorder %v108991, %v894
%v109015 = vadd.s32 %v108510, %v881
%v109019 = vadd.s32 1, %v109015
%v109023 = vsel /*vm=*/%vm109010, /*on_true_vy=*/%v109019, /*on_false_vx=*/%v109015
%v109027 = vadd.s32 1, %v109023
%v109031 = vsel /*vm=*/%vm109005, /*on_true_vy=*/%v109027, /*on_false_vx=*/%v109023
%v109036 = vadd.s32 %v109031, %v10
%v109040 = vadd.s32 %v109001, %v9
%v109044 = vadd.s32 %v109040, %v109036
%v109046 = vshll.u32 %v109040, 13
%v109047 = vshrl.u32 %v109040, 19
%v109048 = vor.u32 %v109047, %v109046
%v109049 = vxor.u32 %v109048, %v109044
%v109052 = vadd.s32 %v109049, %v109044
%v109054 = vshll.u32 %v109049, 15
%v109055 = vshrl.u32 %v109049, 17
%v109056 = vor.u32 %v109055, %v109054
%v109057 = vxor.u32 %v109056, %v109052
%v109060 = vadd.s32 %v109057, %v109052
%v109062 = vshll.u32 %v109057, 26
%v109063 = vshrl.u32 %v109057, 6
%v109064 = vor.u32 %v109063, %v109062
%v109065 = vxor.u32 %v109064, %v109060
%v109068 = vadd.s32 %v109065, %v109060
%v109072 = vadd.s32 %v109068, %v9
%v109074 = vshll.u32 %v109065, 6
%v109075 = vshrl.u32 %v109065, 26
%v109076 = vor.u32 %v109075, %v109074
%v109077 = vxor.u32 %v109076, %v109068
%v109080 = vadd.s32 %v109077, %v8
%v109084 = vadd.s32 1, %v109080
%v109088 = vadd.s32 %v109084, %v109072
%v109090 = vshll.u32 %v109084, 17
%v109091 = vshrl.u32 %v109084, 15
%v109092 = vor.u32 %v109091, %v109090
%v109093 = vxor.u32 %v109092, %v109088
%v109096 = vadd.s32 %v109093, %v109088
%v109098 = vshll.u32 %v109093, 29
%v109099 = vshrl.u32 %v109093, 3
%v109100 = vor.u32 %v109099, %v109098
%v109101 = vxor.u32 %v109100, %v109096
%v109104 = vadd.s32 %v109101, %v109096
%v109106 = vshll.u32 %v109101, 16
%v109107 = vshrl.u32 %v109101, 16
%v109108 = vor.u32 %v109107, %v109106
%v109109 = vxor.u32 %v109108, %v109104
%v109112 = vadd.s32 %v109109, %v109104
%v109116 = vadd.s32 %v109112, %v8
%v109118 = vshll.u32 %v109109, 24
%v109119 = vshrl.u32 %v109109, 8
%v109120 = vor.u32 %v109119, %v109118
%v109121 = vxor.u32 %v109120, %v109112
%v109124 = vadd.s32 %v109121, %v10
%v109128 = vadd.s32 2, %v109124
%v109132 = vadd.s32 %v109128, %v109116
%v109134 = vshll.u32 %v109128, 13
%v109135 = vshrl.u32 %v109128, 19
%v109136 = vor.u32 %v109135, %v109134
%v109137 = vxor.u32 %v109136, %v109132
%v109140 = vadd.s32 %v109137, %v109132
%v109142 = vshll.u32 %v109137, 15
%v109143 = vshrl.u32 %v109137, 17
%v109144 = vor.u32 %v109143, %v109142
%v109145 = vxor.u32 %v109144, %v109140
%v109148 = vadd.s32 %v109145, %v109140
%v109150 = vshll.u32 %v109145, 26
%v109151 = vshrl.u32 %v109145, 6
%v109152 = vor.u32 %v109151, %v109150
%v109153 = vxor.u32 %v109152, %v109148
%v109156 = vadd.s32 %v109153, %v109148
%v109160 = vadd.s32 %v109156, %v10
%v109162 = vshll.u32 %v109153, 6
%v109163 = vshrl.u32 %v109153, 26
%v109164 = vor.u32 %v109163, %v109162
%v109165 = vxor.u32 %v109164, %v109156
%v109168 = vadd.s32 %v109165, %v9
%v109172 = vadd.s32 3, %v109168
%v109176 = vadd.s32 %v109172, %v109160
%v109178 = vshll.u32 %v109172, 17
%v109179 = vshrl.u32 %v109172, 15
%v109180 = vor.u32 %v109179, %v109178
%v109181 = vxor.u32 %v109180, %v109176
%v109184 = vadd.s32 %v109181, %v109176
%v109186 = vshll.u32 %v109181, 29
%v109187 = vshrl.u32 %v109181, 3
%v109188 = vor.u32 %v109187, %v109186
%v109189 = vxor.u32 %v109188, %v109184
%v109192 = vadd.s32 %v109189, %v109184
%v109194 = vshll.u32 %v109189, 16
%v109195 = vshrl.u32 %v109189, 16
%v109196 = vor.u32 %v109195, %v109194
%v109197 = vxor.u32 %v109196, %v109192
%v109200 = vadd.s32 %v109197, %v109192
%v109204 = vadd.s32 %v109200, %v9
%v109206 = vshll.u32 %v109197, 24
%v109207 = vshrl.u32 %v109197, 8
%v109208 = vor.u32 %v109207, %v109206
%v109209 = vxor.u32 %v109208, %v109200
%v109212 = vadd.s32 %v109209, %v8
%v109216 = vadd.s32 4, %v109212
%v109220 = vadd.s32 %v109216, %v109204
%v109222 = vshll.u32 %v109216, 13
%v109223 = vshrl.u32 %v109216, 19
%v109224 = vor.u32 %v109223, %v109222
%v109225 = vxor.u32 %v109224, %v109220
%v109228 = vadd.s32 %v109225, %v109220
%v109230 = vshll.u32 %v109225, 15
%v109231 = vshrl.u32 %v109225, 17
%v109232 = vor.u32 %v109231, %v109230
%v109233 = vxor.u32 %v109232, %v109228
%v109236 = vadd.s32 %v109233, %v109228
%v109238 = vshll.u32 %v109233, 26
%v109239 = vshrl.u32 %v109233, 6
%v109240 = vor.u32 %v109239, %v109238
%v109241 = vxor.u32 %v109240, %v109236
%v109244 = vadd.s32 %v109241, %v109236
%v109248 = vadd.s32 %v109244, %v8
%v109250 = vshll.u32 %v109241, 6
%v109251 = vshrl.u32 %v109241, 26
%v109252 = vor.u32 %v109251, %v109250
%v109253 = vxor.u32 %v109252, %v109244
%v109256 = vadd.s32 %v109253, %v10
%v109260 = vadd.s32 5, %v109256
%v109262 = vxor.u32 %v109260, %v109248
%v109263 = vand.u32.u8 255, %v109262
%v109264 = vand.u32 65535, %v109263
%v109265 = vshrl.u32 %v109264, 1
%v109266 = vor.u32 16256, %v109265
%v109267 = vand.u32.u16 65535, %v109266
%v120330 = vadd.low.f32.bf16 -1.0, %v109267
%v109276 = vmul.f32 2.0, %v120330
%v109280 = vadd.f32 -0.99609375, %v109276
%v109284 = vmax.f32 %v109280, -0.99609375
%v109286 = vand.u32 2147483647, %v109284
%vm109289 = vcmp.eq.f32.partialorder %v109286, 1.0
%v109294 = vmul.f32 inf, %v109284
%v109296 = vxor.u32 2147483648, %v109284
%v109299 = vmul.f32 %v109296, %v109284
%v109301 = vadd.f32 1.0, %v109299
%v109302 = vlog2.pop %v109301
%v109303 = vmul.f32 0.6931472, %v109302
%v109304 = vmul.f32 -0.5, %v109299
%v109305 = vadd.f32 1.0, %v109304
%v109306 = vmul.f32 %v109305, %v109299
%v109307 = vand.u32 2147483647, %v109299
%vm109308 = vcmp.lt.f32.partialorder %v109307, 0.0004427343
%v109309 = vsel /*vm=*/%vm109308, /*on_true_vy=*/%v109306, /*on_false_vx=*/%v109303
%v109310 = vxor.u32 2147483648, %v109309
%vm109313 = vcmp.lt.f32.partialorder %v109310, 5.0
%v109318 = vsel /*vm=*/%vm109313, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v109322 = vsel /*vm=*/%vm109313, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v109326 = vsel /*vm=*/%vm109313, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v109330 = vsel /*vm=*/%vm109313, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v109334 = vsel /*vm=*/%vm109313, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v109338 = vsel /*vm=*/%vm109313, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v109342 = vsel /*vm=*/%vm109313, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v109346 = vsel /*vm=*/%vm109313, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v109350 = vsel /*vm=*/%vm109313, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v109354 = vadd.f32 -2.5, %v109310
%v109356 = vrsqrt.pop %v109310
%v109357 = vmul.f32 %v109356, %v109310
%vm109358 = vcmp.eq.f32.partialorder %v109310, inf
%v109359 = vsel /*vm=*/%vm109358, /*on_true_vy=*/%v109310, /*on_false_vx=*/%v109357
%vm109360 = vcmp.eq.f32.partialorder %v109310, 0.0
%v109361 = vand.u32 2147483648, %v109310
%v109362 = vsel /*vm=*/%vm109360, /*on_true_vy=*/%v109361, /*on_false_vx=*/%v109359
%v109365 = vadd.f32 -3.0, %v109362
%v109369 = vsel /*vm=*/%vm109313, /*on_true_vy=*/%v109354, /*on_false_vx=*/%v109365
%v109373 = vmul.f32 %v109369, %v109350
%v109377 = vadd.f32 %v109373, %v109346
%v109381 = vmul.f32 %v109377, %v109369
%v109385 = vadd.f32 %v109381, %v109342
%v109389 = vmul.f32 %v109385, %v109369
%v109393 = vadd.f32 %v109389, %v109338
%v109397 = vmul.f32 %v109393, %v109369
%v109401 = vadd.f32 %v109397, %v109334
%v109405 = vmul.f32 %v109401, %v109369
%v109409 = vadd.f32 %v109405, %v109330
%v109413 = vmul.f32 %v109409, %v109369
%v109417 = vadd.f32 %v109413, %v109326
%v109421 = vmul.f32 %v109417, %v109369
%v109425 = vadd.f32 %v109421, %v109322
%v109429 = vmul.f32 %v109425, %v109369
%v109433 = vadd.f32 %v109429, %v109318
%v109437 = vmul.f32 %v109433, %v109284
%v109441 = vsel /*vm=*/%vm109289, /*on_true_vy=*/%v109294, /*on_false_vx=*/%v109437
%v109445 = vmul.f32 1.4140625, %v109441
%v109448 = vpack.c.bf16 %v120417, %v109445
%120331 = vst [vmem:[%s280 + $0xf4] sm:$0xf] /*vst_source=*/%v109448
%v109452 = vadd.s32 %v108527, %v1381
%v109462 = vadd.s32 %v109452, %v415
%vm109466 = vcmp.lt.u32.totalorder %v109462, %v109452
%vm109471 = vcmp.lt.u32.totalorder %v109452, %v1381
%v109476 = vadd.s32 %v108510, %v1368
%v109480 = vadd.s32 1, %v109476
%v109484 = vsel /*vm=*/%vm109471, /*on_true_vy=*/%v109480, /*on_false_vx=*/%v109476
%v109488 = vadd.s32 1, %v109484
%v109492 = vsel /*vm=*/%vm109466, /*on_true_vy=*/%v109488, /*on_false_vx=*/%v109484
%v109497 = vadd.s32 %v109492, %v10
%v109501 = vadd.s32 %v109462, %v9
%v109505 = vadd.s32 %v109501, %v109497
%v109507 = vshll.u32 %v109501, 13
%v109508 = vshrl.u32 %v109501, 19
%v109509 = vor.u32 %v109508, %v109507
%v109510 = vxor.u32 %v109509, %v109505
%v109513 = vadd.s32 %v109510, %v109505
%v109515 = vshll.u32 %v109510, 15
%v109516 = vshrl.u32 %v109510, 17
%v109517 = vor.u32 %v109516, %v109515
%v109518 = vxor.u32 %v109517, %v109513
%v109521 = vadd.s32 %v109518, %v109513
%v109523 = vshll.u32 %v109518, 26
%v109524 = vshrl.u32 %v109518, 6
%v109525 = vor.u32 %v109524, %v109523
%v109526 = vxor.u32 %v109525, %v109521
%v109529 = vadd.s32 %v109526, %v109521
%v109533 = vadd.s32 %v109529, %v9
%v109535 = vshll.u32 %v109526, 6
%v109536 = vshrl.u32 %v109526, 26
%v109537 = vor.u32 %v109536, %v109535
%v109538 = vxor.u32 %v109537, %v109529
%v109541 = vadd.s32 %v109538, %v8
%v109545 = vadd.s32 1, %v109541
%v109549 = vadd.s32 %v109545, %v109533
%v109551 = vshll.u32 %v109545, 17
%v109552 = vshrl.u32 %v109545, 15
%v109553 = vor.u32 %v109552, %v109551
%v109554 = vxor.u32 %v109553, %v109549
%v109557 = vadd.s32 %v109554, %v109549
%v109559 = vshll.u32 %v109554, 29
%v109560 = vshrl.u32 %v109554, 3
%v109561 = vor.u32 %v109560, %v109559
%v109562 = vxor.u32 %v109561, %v109557
%v109565 = vadd.s32 %v109562, %v109557
%v109567 = vshll.u32 %v109562, 16
%v109568 = vshrl.u32 %v109562, 16
%v109569 = vor.u32 %v109568, %v109567
%v109570 = vxor.u32 %v109569, %v109565
%v109573 = vadd.s32 %v109570, %v109565
%v109577 = vadd.s32 %v109573, %v8
%v109579 = vshll.u32 %v109570, 24
%v109580 = vshrl.u32 %v109570, 8
%v109581 = vor.u32 %v109580, %v109579
%v109582 = vxor.u32 %v109581, %v109573
%v109585 = vadd.s32 %v109582, %v10
%v109589 = vadd.s32 2, %v109585
%v109593 = vadd.s32 %v109589, %v109577
%v109595 = vshll.u32 %v109589, 13
%v109596 = vshrl.u32 %v109589, 19
%v109597 = vor.u32 %v109596, %v109595
%v109598 = vxor.u32 %v109597, %v109593
%v109601 = vadd.s32 %v109598, %v109593
%v109603 = vshll.u32 %v109598, 15
%v109604 = vshrl.u32 %v109598, 17
%v109605 = vor.u32 %v109604, %v109603
%v109606 = vxor.u32 %v109605, %v109601
%v109609 = vadd.s32 %v109606, %v109601
%v109611 = vshll.u32 %v109606, 26
%v109612 = vshrl.u32 %v109606, 6
%v109613 = vor.u32 %v109612, %v109611
%v109614 = vxor.u32 %v109613, %v109609
%v109617 = vadd.s32 %v109614, %v109609
%v109621 = vadd.s32 %v109617, %v10
%v109623 = vshll.u32 %v109614, 6
%v109624 = vshrl.u32 %v109614, 26
%v109625 = vor.u32 %v109624, %v109623
%v109626 = vxor.u32 %v109625, %v109617
%v109629 = vadd.s32 %v109626, %v9
%v109633 = vadd.s32 3, %v109629
%v109637 = vadd.s32 %v109633, %v109621
%v109639 = vshll.u32 %v109633, 17
%v109640 = vshrl.u32 %v109633, 15
%v109641 = vor.u32 %v109640, %v109639
%v109642 = vxor.u32 %v109641, %v109637
%v109645 = vadd.s32 %v109642, %v109637
%v109647 = vshll.u32 %v109642, 29
%v109648 = vshrl.u32 %v109642, 3
%v109649 = vor.u32 %v109648, %v109647
%v109650 = vxor.u32 %v109649, %v109645
%v109653 = vadd.s32 %v109650, %v109645
%v109655 = vshll.u32 %v109650, 16
%v109656 = vshrl.u32 %v109650, 16
%v109657 = vor.u32 %v109656, %v109655
%v109658 = vxor.u32 %v109657, %v109653
%v109661 = vadd.s32 %v109658, %v109653
%v109665 = vadd.s32 %v109661, %v9
%v109667 = vshll.u32 %v109658, 24
%v109668 = vshrl.u32 %v109658, 8
%v109669 = vor.u32 %v109668, %v109667
%v109670 = vxor.u32 %v109669, %v109661
%v109673 = vadd.s32 %v109670, %v8
%v109677 = vadd.s32 4, %v109673
%v109681 = vadd.s32 %v109677, %v109665
%v109683 = vshll.u32 %v109677, 13
%v109684 = vshrl.u32 %v109677, 19
%v109685 = vor.u32 %v109684, %v109683
%v109686 = vxor.u32 %v109685, %v109681
%v109689 = vadd.s32 %v109686, %v109681
%v109691 = vshll.u32 %v109686, 15
%v109692 = vshrl.u32 %v109686, 17
%v109693 = vor.u32 %v109692, %v109691
%v109694 = vxor.u32 %v109693, %v109689
%v109697 = vadd.s32 %v109694, %v109689
%v109699 = vshll.u32 %v109694, 26
%v109700 = vshrl.u32 %v109694, 6
%v109701 = vor.u32 %v109700, %v109699
%v109702 = vxor.u32 %v109701, %v109697
%v109705 = vadd.s32 %v109702, %v109697
%v109709 = vadd.s32 %v109705, %v8
%v109711 = vshll.u32 %v109702, 6
%v109712 = vshrl.u32 %v109702, 26
%v109713 = vor.u32 %v109712, %v109711
%v109714 = vxor.u32 %v109713, %v109705
%v109717 = vadd.s32 %v109714, %v10
%v109721 = vadd.s32 5, %v109717
%v109723 = vxor.u32 %v109721, %v109709
%v109724 = vand.u32.u8 255, %v109723
%v109725 = vand.u32 65535, %v109724
%v109726 = vshrl.u32 %v109725, 1
%v109727 = vor.u32 16256, %v109726
%v109728 = vand.u32.u16 65535, %v109727
%v120332 = vadd.low.f32.bf16 -1.0, %v109728
%v109737 = vmul.f32 2.0, %v120332
%v109741 = vadd.f32 -0.99609375, %v109737
%v109745 = vmax.f32 %v109741, -0.99609375
%v109747 = vand.u32 2147483647, %v109745
%vm109750 = vcmp.eq.f32.partialorder %v109747, 1.0
%v109755 = vmul.f32 inf, %v109745
%v109757 = vxor.u32 2147483648, %v109745
%v109760 = vmul.f32 %v109757, %v109745
%v109762 = vadd.f32 1.0, %v109760
%v109763 = vlog2.pop %v109762
%v109764 = vmul.f32 0.6931472, %v109763
%v109765 = vmul.f32 -0.5, %v109760
%v109766 = vadd.f32 1.0, %v109765
%v109767 = vmul.f32 %v109766, %v109760
%v109768 = vand.u32 2147483647, %v109760
%vm109769 = vcmp.lt.f32.partialorder %v109768, 0.0004427343
%v109770 = vsel /*vm=*/%vm109769, /*on_true_vy=*/%v109767, /*on_false_vx=*/%v109764
%v109771 = vxor.u32 2147483648, %v109770
%vm109774 = vcmp.lt.f32.partialorder %v109771, 5.0
%v109779 = vsel /*vm=*/%vm109774, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v109783 = vsel /*vm=*/%vm109774, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v109787 = vsel /*vm=*/%vm109774, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v109791 = vsel /*vm=*/%vm109774, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v109795 = vsel /*vm=*/%vm109774, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v109799 = vsel /*vm=*/%vm109774, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v109803 = vsel /*vm=*/%vm109774, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v109807 = vsel /*vm=*/%vm109774, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v109811 = vsel /*vm=*/%vm109774, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v109815 = vadd.f32 -2.5, %v109771
%v109817 = vrsqrt.pop %v109771
%v109818 = vmul.f32 %v109817, %v109771
%vm109819 = vcmp.eq.f32.partialorder %v109771, inf
%v109820 = vsel /*vm=*/%vm109819, /*on_true_vy=*/%v109771, /*on_false_vx=*/%v109818
%vm109821 = vcmp.eq.f32.partialorder %v109771, 0.0
%v109822 = vand.u32 2147483648, %v109771
%v109823 = vsel /*vm=*/%vm109821, /*on_true_vy=*/%v109822, /*on_false_vx=*/%v109820
%v109826 = vadd.f32 -3.0, %v109823
%v109830 = vsel /*vm=*/%vm109774, /*on_true_vy=*/%v109815, /*on_false_vx=*/%v109826
%v109834 = vmul.f32 %v109830, %v109811
%v109838 = vadd.f32 %v109834, %v109807
%v109842 = vmul.f32 %v109838, %v109830
%v109846 = vadd.f32 %v109842, %v109803
%v109850 = vmul.f32 %v109846, %v109830
%v109854 = vadd.f32 %v109850, %v109799
%v109858 = vmul.f32 %v109854, %v109830
%v109862 = vadd.f32 %v109858, %v109795
%v109866 = vmul.f32 %v109862, %v109830
%v109870 = vadd.f32 %v109866, %v109791
%v109874 = vmul.f32 %v109870, %v109830
%v109878 = vadd.f32 %v109874, %v109787
%v109882 = vmul.f32 %v109878, %v109830
%v109886 = vadd.f32 %v109882, %v109783
%v109890 = vmul.f32 %v109886, %v109830
%v109894 = vadd.f32 %v109890, %v109779
%v109898 = vmul.f32 %v109894, %v109745
%v109902 = vsel /*vm=*/%vm109750, /*on_true_vy=*/%v109755, /*on_false_vx=*/%v109898
%v109906 = vmul.f32 1.4140625, %v109902
%v109909 = vpack.c.bf16 %v120417, %v109906
%120333 = vst [vmem:[%s280 + $0x174] sm:$0xf] /*vst_source=*/%v109909
%v109913 = vadd.s32 %v108527, %v1868
%v109923 = vadd.s32 %v109913, %v415
%vm109927 = vcmp.lt.u32.totalorder %v109923, %v109913
%vm109932 = vcmp.lt.u32.totalorder %v109913, %v1868
%v109937 = vadd.s32 %v108510, %v1855
%v109941 = vadd.s32 1, %v109937
%v109945 = vsel /*vm=*/%vm109932, /*on_true_vy=*/%v109941, /*on_false_vx=*/%v109937
%v109949 = vadd.s32 1, %v109945
%v109953 = vsel /*vm=*/%vm109927, /*on_true_vy=*/%v109949, /*on_false_vx=*/%v109945
%v109958 = vadd.s32 %v109953, %v10
%v109962 = vadd.s32 %v109923, %v9
%v109966 = vadd.s32 %v109962, %v109958
%v109968 = vshll.u32 %v109962, 13
%v109969 = vshrl.u32 %v109962, 19
%v109970 = vor.u32 %v109969, %v109968
%v109971 = vxor.u32 %v109970, %v109966
%v109974 = vadd.s32 %v109971, %v109966
%v109976 = vshll.u32 %v109971, 15
%v109977 = vshrl.u32 %v109971, 17
%v109978 = vor.u32 %v109977, %v109976
%v109979 = vxor.u32 %v109978, %v109974
%v109982 = vadd.s32 %v109979, %v109974
%v109984 = vshll.u32 %v109979, 26
%v109985 = vshrl.u32 %v109979, 6
%v109986 = vor.u32 %v109985, %v109984
%v109987 = vxor.u32 %v109986, %v109982
%v109990 = vadd.s32 %v109987, %v109982
%v109994 = vadd.s32 %v109990, %v9
%v109996 = vshll.u32 %v109987, 6
%v109997 = vshrl.u32 %v109987, 26
%v109998 = vor.u32 %v109997, %v109996
%v109999 = vxor.u32 %v109998, %v109990
%v110002 = vadd.s32 %v109999, %v8
%v110006 = vadd.s32 1, %v110002
%v110010 = vadd.s32 %v110006, %v109994
%v110012 = vshll.u32 %v110006, 17
%v110013 = vshrl.u32 %v110006, 15
%v110014 = vor.u32 %v110013, %v110012
%v110015 = vxor.u32 %v110014, %v110010
%v110018 = vadd.s32 %v110015, %v110010
%v110020 = vshll.u32 %v110015, 29
%v110021 = vshrl.u32 %v110015, 3
%v110022 = vor.u32 %v110021, %v110020
%v110023 = vxor.u32 %v110022, %v110018
%v110026 = vadd.s32 %v110023, %v110018
%v110028 = vshll.u32 %v110023, 16
%v110029 = vshrl.u32 %v110023, 16
%v110030 = vor.u32 %v110029, %v110028
%v110031 = vxor.u32 %v110030, %v110026
%v110034 = vadd.s32 %v110031, %v110026
%v110038 = vadd.s32 %v110034, %v8
%v110040 = vshll.u32 %v110031, 24
%v110041 = vshrl.u32 %v110031, 8
%v110042 = vor.u32 %v110041, %v110040
%v110043 = vxor.u32 %v110042, %v110034
%v110046 = vadd.s32 %v110043, %v10
%v110050 = vadd.s32 2, %v110046
%v110054 = vadd.s32 %v110050, %v110038
%v110056 = vshll.u32 %v110050, 13
%v110057 = vshrl.u32 %v110050, 19
%v110058 = vor.u32 %v110057, %v110056
%v110059 = vxor.u32 %v110058, %v110054
%v110062 = vadd.s32 %v110059, %v110054
%v110064 = vshll.u32 %v110059, 15
%v110065 = vshrl.u32 %v110059, 17
%v110066 = vor.u32 %v110065, %v110064
%v110067 = vxor.u32 %v110066, %v110062
%v110070 = vadd.s32 %v110067, %v110062
%v110072 = vshll.u32 %v110067, 26
%v110073 = vshrl.u32 %v110067, 6
%v110074 = vor.u32 %v110073, %v110072
%v110075 = vxor.u32 %v110074, %v110070
%v110078 = vadd.s32 %v110075, %v110070
%v110082 = vadd.s32 %v110078, %v10
%v110084 = vshll.u32 %v110075, 6
%v110085 = vshrl.u32 %v110075, 26
%v110086 = vor.u32 %v110085, %v110084
%v110087 = vxor.u32 %v110086, %v110078
%v110090 = vadd.s32 %v110087, %v9
%v110094 = vadd.s32 3, %v110090
%v110098 = vadd.s32 %v110094, %v110082
%v110100 = vshll.u32 %v110094, 17
%v110101 = vshrl.u32 %v110094, 15
%v110102 = vor.u32 %v110101, %v110100
%v110103 = vxor.u32 %v110102, %v110098
%v110106 = vadd.s32 %v110103, %v110098
%v110108 = vshll.u32 %v110103, 29
%v110109 = vshrl.u32 %v110103, 3
%v110110 = vor.u32 %v110109, %v110108
%v110111 = vxor.u32 %v110110, %v110106
%v110114 = vadd.s32 %v110111, %v110106
%v110116 = vshll.u32 %v110111, 16
%v110117 = vshrl.u32 %v110111, 16
%v110118 = vor.u32 %v110117, %v110116
%v110119 = vxor.u32 %v110118, %v110114
%v110122 = vadd.s32 %v110119, %v110114
%v110126 = vadd.s32 %v110122, %v9
%v110128 = vshll.u32 %v110119, 24
%v110129 = vshrl.u32 %v110119, 8
%v110130 = vor.u32 %v110129, %v110128
%v110131 = vxor.u32 %v110130, %v110122
%v110134 = vadd.s32 %v110131, %v8
%v110138 = vadd.s32 4, %v110134
%v110142 = vadd.s32 %v110138, %v110126
%v110144 = vshll.u32 %v110138, 13
%v110145 = vshrl.u32 %v110138, 19
%v110146 = vor.u32 %v110145, %v110144
%v110147 = vxor.u32 %v110146, %v110142
%v110150 = vadd.s32 %v110147, %v110142
%v110152 = vshll.u32 %v110147, 15
%v110153 = vshrl.u32 %v110147, 17
%v110154 = vor.u32 %v110153, %v110152
%v110155 = vxor.u32 %v110154, %v110150
%v110158 = vadd.s32 %v110155, %v110150
%v110160 = vshll.u32 %v110155, 26
%v110161 = vshrl.u32 %v110155, 6
%v110162 = vor.u32 %v110161, %v110160
%v110163 = vxor.u32 %v110162, %v110158
%v110166 = vadd.s32 %v110163, %v110158
%v110170 = vadd.s32 %v110166, %v8
%v110172 = vshll.u32 %v110163, 6
%v110173 = vshrl.u32 %v110163, 26
%v110174 = vor.u32 %v110173, %v110172
%v110175 = vxor.u32 %v110174, %v110166
%v110178 = vadd.s32 %v110175, %v10
%v110182 = vadd.s32 5, %v110178
%v110184 = vxor.u32 %v110182, %v110170
%v110185 = vand.u32.u8 255, %v110184
%v110186 = vand.u32 65535, %v110185
%v110187 = vshrl.u32 %v110186, 1
%v110188 = vor.u32 16256, %v110187
%v110189 = vand.u32.u16 65535, %v110188
%v120334 = vadd.low.f32.bf16 -1.0, %v110189
%v110198 = vmul.f32 2.0, %v120334
%v110202 = vadd.f32 -0.99609375, %v110198
%v110206 = vmax.f32 %v110202, -0.99609375
%v110208 = vand.u32 2147483647, %v110206
%vm110211 = vcmp.eq.f32.partialorder %v110208, 1.0
%v110216 = vmul.f32 inf, %v110206
%v110218 = vxor.u32 2147483648, %v110206
%v110221 = vmul.f32 %v110218, %v110206
%v110223 = vadd.f32 1.0, %v110221
%v110224 = vlog2.pop %v110223
%v110225 = vmul.f32 0.6931472, %v110224
%v110226 = vmul.f32 -0.5, %v110221
%v110227 = vadd.f32 1.0, %v110226
%v110228 = vmul.f32 %v110227, %v110221
%v110229 = vand.u32 2147483647, %v110221
%vm110230 = vcmp.lt.f32.partialorder %v110229, 0.0004427343
%v110231 = vsel /*vm=*/%vm110230, /*on_true_vy=*/%v110228, /*on_false_vx=*/%v110225
%v110232 = vxor.u32 2147483648, %v110231
%vm110235 = vcmp.lt.f32.partialorder %v110232, 5.0
%v110240 = vsel /*vm=*/%vm110235, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v110244 = vsel /*vm=*/%vm110235, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v110248 = vsel /*vm=*/%vm110235, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v110252 = vsel /*vm=*/%vm110235, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v110256 = vsel /*vm=*/%vm110235, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v110260 = vsel /*vm=*/%vm110235, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v110264 = vsel /*vm=*/%vm110235, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v110268 = vsel /*vm=*/%vm110235, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v110272 = vsel /*vm=*/%vm110235, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v110276 = vadd.f32 -2.5, %v110232
%v110278 = vrsqrt.pop %v110232
%v110279 = vmul.f32 %v110278, %v110232
%vm110280 = vcmp.eq.f32.partialorder %v110232, inf
%v110281 = vsel /*vm=*/%vm110280, /*on_true_vy=*/%v110232, /*on_false_vx=*/%v110279
%vm110282 = vcmp.eq.f32.partialorder %v110232, 0.0
%v110283 = vand.u32 2147483648, %v110232
%v110284 = vsel /*vm=*/%vm110282, /*on_true_vy=*/%v110283, /*on_false_vx=*/%v110281
%v110287 = vadd.f32 -3.0, %v110284
%v110291 = vsel /*vm=*/%vm110235, /*on_true_vy=*/%v110276, /*on_false_vx=*/%v110287
%v110295 = vmul.f32 %v110291, %v110272
%v110299 = vadd.f32 %v110295, %v110268
%v110303 = vmul.f32 %v110299, %v110291
%v110307 = vadd.f32 %v110303, %v110264
%v110311 = vmul.f32 %v110307, %v110291
%v110315 = vadd.f32 %v110311, %v110260
%v110319 = vmul.f32 %v110315, %v110291
%v110323 = vadd.f32 %v110319, %v110256
%v110327 = vmul.f32 %v110323, %v110291
%v110331 = vadd.f32 %v110327, %v110252
%v110335 = vmul.f32 %v110331, %v110291
%v110339 = vadd.f32 %v110335, %v110248
%v110343 = vmul.f32 %v110339, %v110291
%v110347 = vadd.f32 %v110343, %v110244
%v110351 = vmul.f32 %v110347, %v110291
%v110355 = vadd.f32 %v110351, %v110240
%v110359 = vmul.f32 %v110355, %v110206
%v110363 = vsel /*vm=*/%vm110211, /*on_true_vy=*/%v110216, /*on_false_vx=*/%v110359
%v110367 = vmul.f32 1.4140625, %v110363
%v110370 = vpack.c.bf16 %v120417, %v110367
%120335 = vst [vmem:[%s280 + $0x1f4] sm:$0xf] /*vst_source=*/%v110370
%v110374 = vadd.s32 %v108527, %v2355
%v110384 = vadd.s32 %v110374, %v415
%vm110388 = vcmp.lt.u32.totalorder %v110384, %v110374
%vm110393 = vcmp.lt.u32.totalorder %v110374, %v2355
%v110398 = vadd.s32 %v108510, %v2342
%v110402 = vadd.s32 1, %v110398
%v110406 = vsel /*vm=*/%vm110393, /*on_true_vy=*/%v110402, /*on_false_vx=*/%v110398
%v110410 = vadd.s32 1, %v110406
%v110414 = vsel /*vm=*/%vm110388, /*on_true_vy=*/%v110410, /*on_false_vx=*/%v110406
%v110419 = vadd.s32 %v110414, %v10
%v110423 = vadd.s32 %v110384, %v9
%v110427 = vadd.s32 %v110423, %v110419
%v110429 = vshll.u32 %v110423, 13
%v110430 = vshrl.u32 %v110423, 19
%v110431 = vor.u32 %v110430, %v110429
%v110432 = vxor.u32 %v110431, %v110427
%v110435 = vadd.s32 %v110432, %v110427
%v110437 = vshll.u32 %v110432, 15
%v110438 = vshrl.u32 %v110432, 17
%v110439 = vor.u32 %v110438, %v110437
%v110440 = vxor.u32 %v110439, %v110435
%v110443 = vadd.s32 %v110440, %v110435
%v110445 = vshll.u32 %v110440, 26
%v110446 = vshrl.u32 %v110440, 6
%v110447 = vor.u32 %v110446, %v110445
%v110448 = vxor.u32 %v110447, %v110443
%v110451 = vadd.s32 %v110448, %v110443
%v110455 = vadd.s32 %v110451, %v9
%v110457 = vshll.u32 %v110448, 6
%v110458 = vshrl.u32 %v110448, 26
%v110459 = vor.u32 %v110458, %v110457
%v110460 = vxor.u32 %v110459, %v110451
%v110463 = vadd.s32 %v110460, %v8
%v110467 = vadd.s32 1, %v110463
%v110471 = vadd.s32 %v110467, %v110455
%v110473 = vshll.u32 %v110467, 17
%v110474 = vshrl.u32 %v110467, 15
%v110475 = vor.u32 %v110474, %v110473
%v110476 = vxor.u32 %v110475, %v110471
%v110479 = vadd.s32 %v110476, %v110471
%v110481 = vshll.u32 %v110476, 29
%v110482 = vshrl.u32 %v110476, 3
%v110483 = vor.u32 %v110482, %v110481
%v110484 = vxor.u32 %v110483, %v110479
%v110487 = vadd.s32 %v110484, %v110479
%v110489 = vshll.u32 %v110484, 16
%v110490 = vshrl.u32 %v110484, 16
%v110491 = vor.u32 %v110490, %v110489
%v110492 = vxor.u32 %v110491, %v110487
%v110495 = vadd.s32 %v110492, %v110487
%v110499 = vadd.s32 %v110495, %v8
%v110501 = vshll.u32 %v110492, 24
%v110502 = vshrl.u32 %v110492, 8
%v110503 = vor.u32 %v110502, %v110501
%v110504 = vxor.u32 %v110503, %v110495
%v110507 = vadd.s32 %v110504, %v10
%v110511 = vadd.s32 2, %v110507
%v110515 = vadd.s32 %v110511, %v110499
%v110517 = vshll.u32 %v110511, 13
%v110518 = vshrl.u32 %v110511, 19
%v110519 = vor.u32 %v110518, %v110517
%v110520 = vxor.u32 %v110519, %v110515
%v110523 = vadd.s32 %v110520, %v110515
%v110525 = vshll.u32 %v110520, 15
%v110526 = vshrl.u32 %v110520, 17
%v110527 = vor.u32 %v110526, %v110525
%v110528 = vxor.u32 %v110527, %v110523
%v110531 = vadd.s32 %v110528, %v110523
%v110533 = vshll.u32 %v110528, 26
%v110534 = vshrl.u32 %v110528, 6
%v110535 = vor.u32 %v110534, %v110533
%v110536 = vxor.u32 %v110535, %v110531
%v110539 = vadd.s32 %v110536, %v110531
%v110543 = vadd.s32 %v110539, %v10
%v110545 = vshll.u32 %v110536, 6
%v110546 = vshrl.u32 %v110536, 26
%v110547 = vor.u32 %v110546, %v110545
%v110548 = vxor.u32 %v110547, %v110539
%v110551 = vadd.s32 %v110548, %v9
%v110555 = vadd.s32 3, %v110551
%v110559 = vadd.s32 %v110555, %v110543
%v110561 = vshll.u32 %v110555, 17
%v110562 = vshrl.u32 %v110555, 15
%v110563 = vor.u32 %v110562, %v110561
%v110564 = vxor.u32 %v110563, %v110559
%v110567 = vadd.s32 %v110564, %v110559
%v110569 = vshll.u32 %v110564, 29
%v110570 = vshrl.u32 %v110564, 3
%v110571 = vor.u32 %v110570, %v110569
%v110572 = vxor.u32 %v110571, %v110567
%v110575 = vadd.s32 %v110572, %v110567
%v110577 = vshll.u32 %v110572, 16
%v110578 = vshrl.u32 %v110572, 16
%v110579 = vor.u32 %v110578, %v110577
%v110580 = vxor.u32 %v110579, %v110575
%v110583 = vadd.s32 %v110580, %v110575
%v110587 = vadd.s32 %v110583, %v9
%v110589 = vshll.u32 %v110580, 24
%v110590 = vshrl.u32 %v110580, 8
%v110591 = vor.u32 %v110590, %v110589
%v110592 = vxor.u32 %v110591, %v110583
%v110595 = vadd.s32 %v110592, %v8
%v110599 = vadd.s32 4, %v110595
%v110603 = vadd.s32 %v110599, %v110587
%v110605 = vshll.u32 %v110599, 13
%v110606 = vshrl.u32 %v110599, 19
%v110607 = vor.u32 %v110606, %v110605
%v110608 = vxor.u32 %v110607, %v110603
%v110611 = vadd.s32 %v110608, %v110603
%v110613 = vshll.u32 %v110608, 15
%v110614 = vshrl.u32 %v110608, 17
%v110615 = vor.u32 %v110614, %v110613
%v110616 = vxor.u32 %v110615, %v110611
%v110619 = vadd.s32 %v110616, %v110611
%v110621 = vshll.u32 %v110616, 26
%v110622 = vshrl.u32 %v110616, 6
%v110623 = vor.u32 %v110622, %v110621
%v110624 = vxor.u32 %v110623, %v110619
%v110627 = vadd.s32 %v110624, %v110619
%v110631 = vadd.s32 %v110627, %v8
%v110633 = vshll.u32 %v110624, 6
%v110634 = vshrl.u32 %v110624, 26
%v110635 = vor.u32 %v110634, %v110633
%v110636 = vxor.u32 %v110635, %v110627
%v110639 = vadd.s32 %v110636, %v10
%v110643 = vadd.s32 5, %v110639
%v110645 = vxor.u32 %v110643, %v110631
%v110646 = vand.u32.u8 255, %v110645
%v110647 = vand.u32 65535, %v110646
%v110648 = vshrl.u32 %v110647, 1
%v110649 = vor.u32 16256, %v110648
%v110650 = vand.u32.u16 65535, %v110649
%v120336 = vadd.low.f32.bf16 -1.0, %v110650
%v110659 = vmul.f32 2.0, %v120336
%v110663 = vadd.f32 -0.99609375, %v110659
%v110667 = vmax.f32 %v110663, -0.99609375
%v110669 = vand.u32 2147483647, %v110667
%vm110672 = vcmp.eq.f32.partialorder %v110669, 1.0
%v110677 = vmul.f32 inf, %v110667
%v110679 = vxor.u32 2147483648, %v110667
%v110682 = vmul.f32 %v110679, %v110667
%v110684 = vadd.f32 1.0, %v110682
%v110685 = vlog2.pop %v110684
%v110686 = vmul.f32 0.6931472, %v110685
%v110687 = vmul.f32 -0.5, %v110682
%v110688 = vadd.f32 1.0, %v110687
%v110689 = vmul.f32 %v110688, %v110682
%v110690 = vand.u32 2147483647, %v110682
%vm110691 = vcmp.lt.f32.partialorder %v110690, 0.0004427343
%v110692 = vsel /*vm=*/%vm110691, /*on_true_vy=*/%v110689, /*on_false_vx=*/%v110686
%v110693 = vxor.u32 2147483648, %v110692
%vm110696 = vcmp.lt.f32.partialorder %v110693, 5.0
%v110701 = vsel /*vm=*/%vm110696, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v110705 = vsel /*vm=*/%vm110696, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v110709 = vsel /*vm=*/%vm110696, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v110713 = vsel /*vm=*/%vm110696, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v110717 = vsel /*vm=*/%vm110696, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v110721 = vsel /*vm=*/%vm110696, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v110725 = vsel /*vm=*/%vm110696, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v110729 = vsel /*vm=*/%vm110696, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v110733 = vsel /*vm=*/%vm110696, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v110737 = vadd.f32 -2.5, %v110693
%v110739 = vrsqrt.pop %v110693
%v110740 = vmul.f32 %v110739, %v110693
%vm110741 = vcmp.eq.f32.partialorder %v110693, inf
%v110742 = vsel /*vm=*/%vm110741, /*on_true_vy=*/%v110693, /*on_false_vx=*/%v110740
%vm110743 = vcmp.eq.f32.partialorder %v110693, 0.0
%v110744 = vand.u32 2147483648, %v110693
%v110745 = vsel /*vm=*/%vm110743, /*on_true_vy=*/%v110744, /*on_false_vx=*/%v110742
%v110748 = vadd.f32 -3.0, %v110745
%v110752 = vsel /*vm=*/%vm110696, /*on_true_vy=*/%v110737, /*on_false_vx=*/%v110748
%v110756 = vmul.f32 %v110752, %v110733
%v110760 = vadd.f32 %v110756, %v110729
%v110764 = vmul.f32 %v110760, %v110752
%v110768 = vadd.f32 %v110764, %v110725
%v110772 = vmul.f32 %v110768, %v110752
%v110776 = vadd.f32 %v110772, %v110721
%v110780 = vmul.f32 %v110776, %v110752
%v110784 = vadd.f32 %v110780, %v110717
%v110788 = vmul.f32 %v110784, %v110752
%v110792 = vadd.f32 %v110788, %v110713
%v110796 = vmul.f32 %v110792, %v110752
%v110800 = vadd.f32 %v110796, %v110709
%v110804 = vmul.f32 %v110800, %v110752
%v110808 = vadd.f32 %v110804, %v110705
%v110812 = vmul.f32 %v110808, %v110752
%v110816 = vadd.f32 %v110812, %v110701
%v110820 = vmul.f32 %v110816, %v110667
%v110824 = vsel /*vm=*/%vm110672, /*on_true_vy=*/%v110677, /*on_false_vx=*/%v110820
%v110828 = vmul.f32 1.4140625, %v110824
%v110831 = vpack.c.bf16 %v120417, %v110828
%120337 = vst [vmem:[%s280 + $0x274] sm:$0xf] /*vst_source=*/%v110831
%v110835 = vadd.s32 %v108527, %v2842
%v110845 = vadd.s32 %v110835, %v415
%vm110849 = vcmp.lt.u32.totalorder %v110845, %v110835
%vm110854 = vcmp.lt.u32.totalorder %v110835, %v2842
%v110859 = vadd.s32 %v108510, %v2829
%v110863 = vadd.s32 1, %v110859
%v110867 = vsel /*vm=*/%vm110854, /*on_true_vy=*/%v110863, /*on_false_vx=*/%v110859
%v110871 = vadd.s32 1, %v110867
%v110875 = vsel /*vm=*/%vm110849, /*on_true_vy=*/%v110871, /*on_false_vx=*/%v110867
%v110880 = vadd.s32 %v110875, %v10
%v110884 = vadd.s32 %v110845, %v9
%v110888 = vadd.s32 %v110884, %v110880
%v110890 = vshll.u32 %v110884, 13
%v110891 = vshrl.u32 %v110884, 19
%v110892 = vor.u32 %v110891, %v110890
%v110893 = vxor.u32 %v110892, %v110888
%v110896 = vadd.s32 %v110893, %v110888
%v110898 = vshll.u32 %v110893, 15
%v110899 = vshrl.u32 %v110893, 17
%v110900 = vor.u32 %v110899, %v110898
%v110901 = vxor.u32 %v110900, %v110896
%v110904 = vadd.s32 %v110901, %v110896
%v110906 = vshll.u32 %v110901, 26
%v110907 = vshrl.u32 %v110901, 6
%v110908 = vor.u32 %v110907, %v110906
%v110909 = vxor.u32 %v110908, %v110904
%v110912 = vadd.s32 %v110909, %v110904
%v110916 = vadd.s32 %v110912, %v9
%v110918 = vshll.u32 %v110909, 6
%v110919 = vshrl.u32 %v110909, 26
%v110920 = vor.u32 %v110919, %v110918
%v110921 = vxor.u32 %v110920, %v110912
%v110924 = vadd.s32 %v110921, %v8
%v110928 = vadd.s32 1, %v110924
%v110932 = vadd.s32 %v110928, %v110916
%v110934 = vshll.u32 %v110928, 17
%v110935 = vshrl.u32 %v110928, 15
%v110936 = vor.u32 %v110935, %v110934
%v110937 = vxor.u32 %v110936, %v110932
%v110940 = vadd.s32 %v110937, %v110932
%v110942 = vshll.u32 %v110937, 29
%v110943 = vshrl.u32 %v110937, 3
%v110944 = vor.u32 %v110943, %v110942
%v110945 = vxor.u32 %v110944, %v110940
%v110948 = vadd.s32 %v110945, %v110940
%v110950 = vshll.u32 %v110945, 16
%v110951 = vshrl.u32 %v110945, 16
%v110952 = vor.u32 %v110951, %v110950
%v110953 = vxor.u32 %v110952, %v110948
%v110956 = vadd.s32 %v110953, %v110948
%v110960 = vadd.s32 %v110956, %v8
%v110962 = vshll.u32 %v110953, 24
%v110963 = vshrl.u32 %v110953, 8
%v110964 = vor.u32 %v110963, %v110962
%v110965 = vxor.u32 %v110964, %v110956
%v110968 = vadd.s32 %v110965, %v10
%v110972 = vadd.s32 2, %v110968
%v110976 = vadd.s32 %v110972, %v110960
%v110978 = vshll.u32 %v110972, 13
%v110979 = vshrl.u32 %v110972, 19
%v110980 = vor.u32 %v110979, %v110978
%v110981 = vxor.u32 %v110980, %v110976
%v110984 = vadd.s32 %v110981, %v110976
%v110986 = vshll.u32 %v110981, 15
%v110987 = vshrl.u32 %v110981, 17
%v110988 = vor.u32 %v110987, %v110986
%v110989 = vxor.u32 %v110988, %v110984
%v110992 = vadd.s32 %v110989, %v110984
%v110994 = vshll.u32 %v110989, 26
%v110995 = vshrl.u32 %v110989, 6
%v110996 = vor.u32 %v110995, %v110994
%v110997 = vxor.u32 %v110996, %v110992
%v111000 = vadd.s32 %v110997, %v110992
%v111004 = vadd.s32 %v111000, %v10
%v111006 = vshll.u32 %v110997, 6
%v111007 = vshrl.u32 %v110997, 26
%v111008 = vor.u32 %v111007, %v111006
%v111009 = vxor.u32 %v111008, %v111000
%v111012 = vadd.s32 %v111009, %v9
%v111016 = vadd.s32 3, %v111012
%v111020 = vadd.s32 %v111016, %v111004
%v111022 = vshll.u32 %v111016, 17
%v111023 = vshrl.u32 %v111016, 15
%v111024 = vor.u32 %v111023, %v111022
%v111025 = vxor.u32 %v111024, %v111020
%v111028 = vadd.s32 %v111025, %v111020
%v111030 = vshll.u32 %v111025, 29
%v111031 = vshrl.u32 %v111025, 3
%v111032 = vor.u32 %v111031, %v111030
%v111033 = vxor.u32 %v111032, %v111028
%v111036 = vadd.s32 %v111033, %v111028
%v111038 = vshll.u32 %v111033, 16
%v111039 = vshrl.u32 %v111033, 16
%v111040 = vor.u32 %v111039, %v111038
%v111041 = vxor.u32 %v111040, %v111036
%v111044 = vadd.s32 %v111041, %v111036
%v111048 = vadd.s32 %v111044, %v9
%v111050 = vshll.u32 %v111041, 24
%v111051 = vshrl.u32 %v111041, 8
%v111052 = vor.u32 %v111051, %v111050
%v111053 = vxor.u32 %v111052, %v111044
%v111056 = vadd.s32 %v111053, %v8
%v111060 = vadd.s32 4, %v111056
%v111064 = vadd.s32 %v111060, %v111048
%v111066 = vshll.u32 %v111060, 13
%v111067 = vshrl.u32 %v111060, 19
%v111068 = vor.u32 %v111067, %v111066
%v111069 = vxor.u32 %v111068, %v111064
%v111072 = vadd.s32 %v111069, %v111064
%v111074 = vshll.u32 %v111069, 15
%v111075 = vshrl.u32 %v111069, 17
%v111076 = vor.u32 %v111075, %v111074
%v111077 = vxor.u32 %v111076, %v111072
%v111080 = vadd.s32 %v111077, %v111072
%v111082 = vshll.u32 %v111077, 26
%v111083 = vshrl.u32 %v111077, 6
%v111084 = vor.u32 %v111083, %v111082
%v111085 = vxor.u32 %v111084, %v111080
%v111088 = vadd.s32 %v111085, %v111080
%v111092 = vadd.s32 %v111088, %v8
%v111094 = vshll.u32 %v111085, 6
%v111095 = vshrl.u32 %v111085, 26
%v111096 = vor.u32 %v111095, %v111094
%v111097 = vxor.u32 %v111096, %v111088
%v111100 = vadd.s32 %v111097, %v10
%v111104 = vadd.s32 5, %v111100
%v111106 = vxor.u32 %v111104, %v111092
%v111107 = vand.u32.u8 255, %v111106
%v111108 = vand.u32 65535, %v111107
%v111109 = vshrl.u32 %v111108, 1
%v111110 = vor.u32 16256, %v111109
%v111111 = vand.u32.u16 65535, %v111110
%v120338 = vadd.low.f32.bf16 -1.0, %v111111
%v111120 = vmul.f32 2.0, %v120338
%v111124 = vadd.f32 -0.99609375, %v111120
%v111128 = vmax.f32 %v111124, -0.99609375
%v111130 = vand.u32 2147483647, %v111128
%vm111133 = vcmp.eq.f32.partialorder %v111130, 1.0
%v111138 = vmul.f32 inf, %v111128
%v111140 = vxor.u32 2147483648, %v111128
%v111143 = vmul.f32 %v111140, %v111128
%v111145 = vadd.f32 1.0, %v111143
%v111146 = vlog2.pop %v111145
%v111147 = vmul.f32 0.6931472, %v111146
%v111148 = vmul.f32 -0.5, %v111143
%v111149 = vadd.f32 1.0, %v111148
%v111150 = vmul.f32 %v111149, %v111143
%v111151 = vand.u32 2147483647, %v111143
%vm111152 = vcmp.lt.f32.partialorder %v111151, 0.0004427343
%v111153 = vsel /*vm=*/%vm111152, /*on_true_vy=*/%v111150, /*on_false_vx=*/%v111147
%v111154 = vxor.u32 2147483648, %v111153
%vm111157 = vcmp.lt.f32.partialorder %v111154, 5.0
%v111162 = vsel /*vm=*/%vm111157, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v111166 = vsel /*vm=*/%vm111157, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v111170 = vsel /*vm=*/%vm111157, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v111174 = vsel /*vm=*/%vm111157, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v111178 = vsel /*vm=*/%vm111157, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v111182 = vsel /*vm=*/%vm111157, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v111186 = vsel /*vm=*/%vm111157, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v111190 = vsel /*vm=*/%vm111157, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v111194 = vsel /*vm=*/%vm111157, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v111198 = vadd.f32 -2.5, %v111154
%v111200 = vrsqrt.pop %v111154
%v111201 = vmul.f32 %v111200, %v111154
%vm111202 = vcmp.eq.f32.partialorder %v111154, inf
%v111203 = vsel /*vm=*/%vm111202, /*on_true_vy=*/%v111154, /*on_false_vx=*/%v111201
%vm111204 = vcmp.eq.f32.partialorder %v111154, 0.0
%v111205 = vand.u32 2147483648, %v111154
%v111206 = vsel /*vm=*/%vm111204, /*on_true_vy=*/%v111205, /*on_false_vx=*/%v111203
%v111209 = vadd.f32 -3.0, %v111206
%v111213 = vsel /*vm=*/%vm111157, /*on_true_vy=*/%v111198, /*on_false_vx=*/%v111209
%v111217 = vmul.f32 %v111213, %v111194
%v111221 = vadd.f32 %v111217, %v111190
%v111225 = vmul.f32 %v111221, %v111213
%v111229 = vadd.f32 %v111225, %v111186
%v111233 = vmul.f32 %v111229, %v111213
%v111237 = vadd.f32 %v111233, %v111182
%v111241 = vmul.f32 %v111237, %v111213
%v111245 = vadd.f32 %v111241, %v111178
%v111249 = vmul.f32 %v111245, %v111213
%v111253 = vadd.f32 %v111249, %v111174
%v111257 = vmul.f32 %v111253, %v111213
%v111261 = vadd.f32 %v111257, %v111170
%v111265 = vmul.f32 %v111261, %v111213
%v111269 = vadd.f32 %v111265, %v111166
%v111273 = vmul.f32 %v111269, %v111213
%v111277 = vadd.f32 %v111273, %v111162
%v111281 = vmul.f32 %v111277, %v111128
%v111285 = vsel /*vm=*/%vm111133, /*on_true_vy=*/%v111138, /*on_false_vx=*/%v111281
%v111289 = vmul.f32 1.4140625, %v111285
%v111292 = vpack.c.bf16 %v120417, %v111289
%120339 = vst [vmem:[%s280 + $0x2f4] sm:$0xf] /*vst_source=*/%v111292
%v111296 = vadd.s32 %v108527, %v3329
%v111306 = vadd.s32 %v111296, %v415
%vm111310 = vcmp.lt.u32.totalorder %v111306, %v111296
%vm111315 = vcmp.lt.u32.totalorder %v111296, %v3329
%v111320 = vadd.s32 %v108510, %v3316
%v111324 = vadd.s32 1, %v111320
%v111328 = vsel /*vm=*/%vm111315, /*on_true_vy=*/%v111324, /*on_false_vx=*/%v111320
%v111332 = vadd.s32 1, %v111328
%v111336 = vsel /*vm=*/%vm111310, /*on_true_vy=*/%v111332, /*on_false_vx=*/%v111328
%v111341 = vadd.s32 %v111336, %v10
%v111345 = vadd.s32 %v111306, %v9
%v111349 = vadd.s32 %v111345, %v111341
%v111351 = vshll.u32 %v111345, 13
%v111352 = vshrl.u32 %v111345, 19
%v111353 = vor.u32 %v111352, %v111351
%v111354 = vxor.u32 %v111353, %v111349
%v111357 = vadd.s32 %v111354, %v111349
%v111359 = vshll.u32 %v111354, 15
%v111360 = vshrl.u32 %v111354, 17
%v111361 = vor.u32 %v111360, %v111359
%v111362 = vxor.u32 %v111361, %v111357
%v111365 = vadd.s32 %v111362, %v111357
%v111367 = vshll.u32 %v111362, 26
%v111368 = vshrl.u32 %v111362, 6
%v111369 = vor.u32 %v111368, %v111367
%v111370 = vxor.u32 %v111369, %v111365
%v111373 = vadd.s32 %v111370, %v111365
%v111377 = vadd.s32 %v111373, %v9
%v111379 = vshll.u32 %v111370, 6
%v111380 = vshrl.u32 %v111370, 26
%v111381 = vor.u32 %v111380, %v111379
%v111382 = vxor.u32 %v111381, %v111373
%v111385 = vadd.s32 %v111382, %v8
%v111389 = vadd.s32 1, %v111385
%v111393 = vadd.s32 %v111389, %v111377
%v111395 = vshll.u32 %v111389, 17
%v111396 = vshrl.u32 %v111389, 15
%v111397 = vor.u32 %v111396, %v111395
%v111398 = vxor.u32 %v111397, %v111393
%v111401 = vadd.s32 %v111398, %v111393
%v111403 = vshll.u32 %v111398, 29
%v111404 = vshrl.u32 %v111398, 3
%v111405 = vor.u32 %v111404, %v111403
%v111406 = vxor.u32 %v111405, %v111401
%v111409 = vadd.s32 %v111406, %v111401
%v111411 = vshll.u32 %v111406, 16
%v111412 = vshrl.u32 %v111406, 16
%v111413 = vor.u32 %v111412, %v111411
%v111414 = vxor.u32 %v111413, %v111409
%v111417 = vadd.s32 %v111414, %v111409
%v111421 = vadd.s32 %v111417, %v8
%v111423 = vshll.u32 %v111414, 24
%v111424 = vshrl.u32 %v111414, 8
%v111425 = vor.u32 %v111424, %v111423
%v111426 = vxor.u32 %v111425, %v111417
%v111429 = vadd.s32 %v111426, %v10
%v111433 = vadd.s32 2, %v111429
%v111437 = vadd.s32 %v111433, %v111421
%v111439 = vshll.u32 %v111433, 13
%v111440 = vshrl.u32 %v111433, 19
%v111441 = vor.u32 %v111440, %v111439
%v111442 = vxor.u32 %v111441, %v111437
%v111445 = vadd.s32 %v111442, %v111437
%v111447 = vshll.u32 %v111442, 15
%v111448 = vshrl.u32 %v111442, 17
%v111449 = vor.u32 %v111448, %v111447
%v111450 = vxor.u32 %v111449, %v111445
%v111453 = vadd.s32 %v111450, %v111445
%v111455 = vshll.u32 %v111450, 26
%v111456 = vshrl.u32 %v111450, 6
%v111457 = vor.u32 %v111456, %v111455
%v111458 = vxor.u32 %v111457, %v111453
%v111461 = vadd.s32 %v111458, %v111453
%v111465 = vadd.s32 %v111461, %v10
%v111467 = vshll.u32 %v111458, 6
%v111468 = vshrl.u32 %v111458, 26
%v111469 = vor.u32 %v111468, %v111467
%v111470 = vxor.u32 %v111469, %v111461
%v111473 = vadd.s32 %v111470, %v9
%v111477 = vadd.s32 3, %v111473
%v111481 = vadd.s32 %v111477, %v111465
%v111483 = vshll.u32 %v111477, 17
%v111484 = vshrl.u32 %v111477, 15
%v111485 = vor.u32 %v111484, %v111483
%v111486 = vxor.u32 %v111485, %v111481
%v111489 = vadd.s32 %v111486, %v111481
%v111491 = vshll.u32 %v111486, 29
%v111492 = vshrl.u32 %v111486, 3
%v111493 = vor.u32 %v111492, %v111491
%v111494 = vxor.u32 %v111493, %v111489
%v111497 = vadd.s32 %v111494, %v111489
%v111499 = vshll.u32 %v111494, 16
%v111500 = vshrl.u32 %v111494, 16
%v111501 = vor.u32 %v111500, %v111499
%v111502 = vxor.u32 %v111501, %v111497
%v111505 = vadd.s32 %v111502, %v111497
%v111509 = vadd.s32 %v111505, %v9
%v111511 = vshll.u32 %v111502, 24
%v111512 = vshrl.u32 %v111502, 8
%v111513 = vor.u32 %v111512, %v111511
%v111514 = vxor.u32 %v111513, %v111505
%v111517 = vadd.s32 %v111514, %v8
%v111521 = vadd.s32 4, %v111517
%v111525 = vadd.s32 %v111521, %v111509
%v111527 = vshll.u32 %v111521, 13
%v111528 = vshrl.u32 %v111521, 19
%v111529 = vor.u32 %v111528, %v111527
%v111530 = vxor.u32 %v111529, %v111525
%v111533 = vadd.s32 %v111530, %v111525
%v111535 = vshll.u32 %v111530, 15
%v111536 = vshrl.u32 %v111530, 17
%v111537 = vor.u32 %v111536, %v111535
%v111538 = vxor.u32 %v111537, %v111533
%v111541 = vadd.s32 %v111538, %v111533
%v111543 = vshll.u32 %v111538, 26
%v111544 = vshrl.u32 %v111538, 6
%v111545 = vor.u32 %v111544, %v111543
%v111546 = vxor.u32 %v111545, %v111541
%v111549 = vadd.s32 %v111546, %v111541
%v111553 = vadd.s32 %v111549, %v8
%v111555 = vshll.u32 %v111546, 6
%v111556 = vshrl.u32 %v111546, 26
%v111557 = vor.u32 %v111556, %v111555
%v111558 = vxor.u32 %v111557, %v111549
%v111561 = vadd.s32 %v111558, %v10
%v111565 = vadd.s32 5, %v111561
%v111567 = vxor.u32 %v111565, %v111553
%v111568 = vand.u32.u8 255, %v111567
%v111569 = vand.u32 65535, %v111568
%v111570 = vshrl.u32 %v111569, 1
%v111571 = vor.u32 16256, %v111570
%v111572 = vand.u32.u16 65535, %v111571
%v120340 = vadd.low.f32.bf16 -1.0, %v111572
%v111581 = vmul.f32 2.0, %v120340
%v111585 = vadd.f32 -0.99609375, %v111581
%v111589 = vmax.f32 %v111585, -0.99609375
%v111591 = vand.u32 2147483647, %v111589
%vm111594 = vcmp.eq.f32.partialorder %v111591, 1.0
%v111599 = vmul.f32 inf, %v111589
%v111601 = vxor.u32 2147483648, %v111589
%v111604 = vmul.f32 %v111601, %v111589
%v111606 = vadd.f32 1.0, %v111604
%v111607 = vlog2.pop %v111606
%v111608 = vmul.f32 0.6931472, %v111607
%v111609 = vmul.f32 -0.5, %v111604
%v111610 = vadd.f32 1.0, %v111609
%v111611 = vmul.f32 %v111610, %v111604
%v111612 = vand.u32 2147483647, %v111604
%vm111613 = vcmp.lt.f32.partialorder %v111612, 0.0004427343
%v111614 = vsel /*vm=*/%vm111613, /*on_true_vy=*/%v111611, /*on_false_vx=*/%v111608
%v111615 = vxor.u32 2147483648, %v111614
%vm111618 = vcmp.lt.f32.partialorder %v111615, 5.0
%v111623 = vsel /*vm=*/%vm111618, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v111627 = vsel /*vm=*/%vm111618, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v111631 = vsel /*vm=*/%vm111618, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v111635 = vsel /*vm=*/%vm111618, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v111639 = vsel /*vm=*/%vm111618, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v111643 = vsel /*vm=*/%vm111618, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v111647 = vsel /*vm=*/%vm111618, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v111651 = vsel /*vm=*/%vm111618, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v111655 = vsel /*vm=*/%vm111618, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v111659 = vadd.f32 -2.5, %v111615
%v111661 = vrsqrt.pop %v111615
%v111662 = vmul.f32 %v111661, %v111615
%vm111663 = vcmp.eq.f32.partialorder %v111615, inf
%v111664 = vsel /*vm=*/%vm111663, /*on_true_vy=*/%v111615, /*on_false_vx=*/%v111662
%vm111665 = vcmp.eq.f32.partialorder %v111615, 0.0
%v111666 = vand.u32 2147483648, %v111615
%v111667 = vsel /*vm=*/%vm111665, /*on_true_vy=*/%v111666, /*on_false_vx=*/%v111664
%v111670 = vadd.f32 -3.0, %v111667
%v111674 = vsel /*vm=*/%vm111618, /*on_true_vy=*/%v111659, /*on_false_vx=*/%v111670
%v111678 = vmul.f32 %v111674, %v111655
%v111682 = vadd.f32 %v111678, %v111651
%v111686 = vmul.f32 %v111682, %v111674
%v111690 = vadd.f32 %v111686, %v111647
%v111694 = vmul.f32 %v111690, %v111674
%v111698 = vadd.f32 %v111694, %v111643
%v111702 = vmul.f32 %v111698, %v111674
%v111706 = vadd.f32 %v111702, %v111639
%v111710 = vmul.f32 %v111706, %v111674
%v111714 = vadd.f32 %v111710, %v111635
%v111718 = vmul.f32 %v111714, %v111674
%v111722 = vadd.f32 %v111718, %v111631
%v111726 = vmul.f32 %v111722, %v111674
%v111730 = vadd.f32 %v111726, %v111627
%v111734 = vmul.f32 %v111730, %v111674
%v111738 = vadd.f32 %v111734, %v111623
%v111742 = vmul.f32 %v111738, %v111589
%v111746 = vsel /*vm=*/%vm111594, /*on_true_vy=*/%v111599, /*on_false_vx=*/%v111742
%v111750 = vmul.f32 1.4140625, %v111746
%v111753 = vpack.c.bf16 %v120417, %v111750
%120341 = vst [vmem:[%s280 + $0x374] sm:$0xf] /*vst_source=*/%v111753
%v111757 = vadd.s32 %v108527, %v3816
%v111767 = vadd.s32 %v111757, %v415
%vm111771 = vcmp.lt.u32.totalorder %v111767, %v111757
%vm111776 = vcmp.lt.u32.totalorder %v111757, %v3816
%v111781 = vadd.s32 %v108510, %v3803
%v111785 = vadd.s32 1, %v111781
%v111789 = vsel /*vm=*/%vm111776, /*on_true_vy=*/%v111785, /*on_false_vx=*/%v111781
%v111793 = vadd.s32 1, %v111789
%v111797 = vsel /*vm=*/%vm111771, /*on_true_vy=*/%v111793, /*on_false_vx=*/%v111789
%v111802 = vadd.s32 %v111797, %v10
%v111806 = vadd.s32 %v111767, %v9
%v111810 = vadd.s32 %v111806, %v111802
%v111812 = vshll.u32 %v111806, 13
%v111813 = vshrl.u32 %v111806, 19
%v111814 = vor.u32 %v111813, %v111812
%v111815 = vxor.u32 %v111814, %v111810
%v111818 = vadd.s32 %v111815, %v111810
%v111820 = vshll.u32 %v111815, 15
%v111821 = vshrl.u32 %v111815, 17
%v111822 = vor.u32 %v111821, %v111820
%v111823 = vxor.u32 %v111822, %v111818
%v111826 = vadd.s32 %v111823, %v111818
%v111828 = vshll.u32 %v111823, 26
%v111829 = vshrl.u32 %v111823, 6
%v111830 = vor.u32 %v111829, %v111828
%v111831 = vxor.u32 %v111830, %v111826
%v111834 = vadd.s32 %v111831, %v111826
%v111838 = vadd.s32 %v111834, %v9
%v111840 = vshll.u32 %v111831, 6
%v111841 = vshrl.u32 %v111831, 26
%v111842 = vor.u32 %v111841, %v111840
%v111843 = vxor.u32 %v111842, %v111834
%v111846 = vadd.s32 %v111843, %v8
%v111850 = vadd.s32 1, %v111846
%v111854 = vadd.s32 %v111850, %v111838
%v111856 = vshll.u32 %v111850, 17
%v111857 = vshrl.u32 %v111850, 15
%v111858 = vor.u32 %v111857, %v111856
%v111859 = vxor.u32 %v111858, %v111854
%v111862 = vadd.s32 %v111859, %v111854
%v111864 = vshll.u32 %v111859, 29
%v111865 = vshrl.u32 %v111859, 3
%v111866 = vor.u32 %v111865, %v111864
%v111867 = vxor.u32 %v111866, %v111862
%v111870 = vadd.s32 %v111867, %v111862
%v111872 = vshll.u32 %v111867, 16
%v111873 = vshrl.u32 %v111867, 16
%v111874 = vor.u32 %v111873, %v111872
%v111875 = vxor.u32 %v111874, %v111870
%v111878 = vadd.s32 %v111875, %v111870
%v111882 = vadd.s32 %v111878, %v8
%v111884 = vshll.u32 %v111875, 24
%v111885 = vshrl.u32 %v111875, 8
%v111886 = vor.u32 %v111885, %v111884
%v111887 = vxor.u32 %v111886, %v111878
%v111890 = vadd.s32 %v111887, %v10
%v111894 = vadd.s32 2, %v111890
%v111898 = vadd.s32 %v111894, %v111882
%v111900 = vshll.u32 %v111894, 13
%v111901 = vshrl.u32 %v111894, 19
%v111902 = vor.u32 %v111901, %v111900
%v111903 = vxor.u32 %v111902, %v111898
%v111906 = vadd.s32 %v111903, %v111898
%v111908 = vshll.u32 %v111903, 15
%v111909 = vshrl.u32 %v111903, 17
%v111910 = vor.u32 %v111909, %v111908
%v111911 = vxor.u32 %v111910, %v111906
%v111914 = vadd.s32 %v111911, %v111906
%v111916 = vshll.u32 %v111911, 26
%v111917 = vshrl.u32 %v111911, 6
%v111918 = vor.u32 %v111917, %v111916
%v111919 = vxor.u32 %v111918, %v111914
%v111922 = vadd.s32 %v111919, %v111914
%v111926 = vadd.s32 %v111922, %v10
%v111928 = vshll.u32 %v111919, 6
%v111929 = vshrl.u32 %v111919, 26
%v111930 = vor.u32 %v111929, %v111928
%v111931 = vxor.u32 %v111930, %v111922
%v111934 = vadd.s32 %v111931, %v9
%v111938 = vadd.s32 3, %v111934
%v111942 = vadd.s32 %v111938, %v111926
%v111944 = vshll.u32 %v111938, 17
%v111945 = vshrl.u32 %v111938, 15
%v111946 = vor.u32 %v111945, %v111944
%v111947 = vxor.u32 %v111946, %v111942
%v111950 = vadd.s32 %v111947, %v111942
%v111952 = vshll.u32 %v111947, 29
%v111953 = vshrl.u32 %v111947, 3
%v111954 = vor.u32 %v111953, %v111952
%v111955 = vxor.u32 %v111954, %v111950
%v111958 = vadd.s32 %v111955, %v111950
%v111960 = vshll.u32 %v111955, 16
%v111961 = vshrl.u32 %v111955, 16
%v111962 = vor.u32 %v111961, %v111960
%v111963 = vxor.u32 %v111962, %v111958
%v111966 = vadd.s32 %v111963, %v111958
%v111970 = vadd.s32 %v111966, %v9
%v111972 = vshll.u32 %v111963, 24
%v111973 = vshrl.u32 %v111963, 8
%v111974 = vor.u32 %v111973, %v111972
%v111975 = vxor.u32 %v111974, %v111966
%v111978 = vadd.s32 %v111975, %v8
%v111982 = vadd.s32 4, %v111978
%v111986 = vadd.s32 %v111982, %v111970
%v111988 = vshll.u32 %v111982, 13
%v111989 = vshrl.u32 %v111982, 19
%v111990 = vor.u32 %v111989, %v111988
%v111991 = vxor.u32 %v111990, %v111986
%v111994 = vadd.s32 %v111991, %v111986
%v111996 = vshll.u32 %v111991, 15
%v111997 = vshrl.u32 %v111991, 17
%v111998 = vor.u32 %v111997, %v111996
%v111999 = vxor.u32 %v111998, %v111994
%v112002 = vadd.s32 %v111999, %v111994
%v112004 = vshll.u32 %v111999, 26
%v112005 = vshrl.u32 %v111999, 6
%v112006 = vor.u32 %v112005, %v112004
%v112007 = vxor.u32 %v112006, %v112002
%v112010 = vadd.s32 %v112007, %v112002
%v112014 = vadd.s32 %v112010, %v8
%v112016 = vshll.u32 %v112007, 6
%v112017 = vshrl.u32 %v112007, 26
%v112018 = vor.u32 %v112017, %v112016
%v112019 = vxor.u32 %v112018, %v112010
%v112022 = vadd.s32 %v112019, %v10
%v112026 = vadd.s32 5, %v112022
%v112028 = vxor.u32 %v112026, %v112014
%v112029 = vand.u32.u8 255, %v112028
%v112030 = vand.u32 65535, %v112029
%v112031 = vshrl.u32 %v112030, 1
%v112032 = vor.u32 16256, %v112031
%v112033 = vand.u32.u16 65535, %v112032
%v120342 = vadd.low.f32.bf16 -1.0, %v112033
%v112042 = vmul.f32 2.0, %v120342
%v112046 = vadd.f32 -0.99609375, %v112042
%v112050 = vmax.f32 %v112046, -0.99609375
%v112052 = vand.u32 2147483647, %v112050
%vm112055 = vcmp.eq.f32.partialorder %v112052, 1.0
%v112060 = vmul.f32 inf, %v112050
%v112062 = vxor.u32 2147483648, %v112050
%v112065 = vmul.f32 %v112062, %v112050
%v112067 = vadd.f32 1.0, %v112065
%v112068 = vlog2.pop %v112067
%v112069 = vmul.f32 0.6931472, %v112068
%v112070 = vmul.f32 -0.5, %v112065
%v112071 = vadd.f32 1.0, %v112070
%v112072 = vmul.f32 %v112071, %v112065
%v112073 = vand.u32 2147483647, %v112065
%vm112074 = vcmp.lt.f32.partialorder %v112073, 0.0004427343
%v112075 = vsel /*vm=*/%vm112074, /*on_true_vy=*/%v112072, /*on_false_vx=*/%v112069
%v112076 = vxor.u32 2147483648, %v112075
%vm112079 = vcmp.lt.f32.partialorder %v112076, 5.0
%v112084 = vsel /*vm=*/%vm112079, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v112088 = vsel /*vm=*/%vm112079, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v112092 = vsel /*vm=*/%vm112079, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v112096 = vsel /*vm=*/%vm112079, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v112100 = vsel /*vm=*/%vm112079, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v112104 = vsel /*vm=*/%vm112079, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v112108 = vsel /*vm=*/%vm112079, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v112112 = vsel /*vm=*/%vm112079, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v112116 = vsel /*vm=*/%vm112079, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v112120 = vadd.f32 -2.5, %v112076
%v112122 = vrsqrt.pop %v112076
%v112123 = vmul.f32 %v112122, %v112076
%vm112124 = vcmp.eq.f32.partialorder %v112076, inf
%v112125 = vsel /*vm=*/%vm112124, /*on_true_vy=*/%v112076, /*on_false_vx=*/%v112123
%vm112126 = vcmp.eq.f32.partialorder %v112076, 0.0
%v112127 = vand.u32 2147483648, %v112076
%v112128 = vsel /*vm=*/%vm112126, /*on_true_vy=*/%v112127, /*on_false_vx=*/%v112125
%v112131 = vadd.f32 -3.0, %v112128
%v112135 = vsel /*vm=*/%vm112079, /*on_true_vy=*/%v112120, /*on_false_vx=*/%v112131
%v112139 = vmul.f32 %v112135, %v112116
%v112143 = vadd.f32 %v112139, %v112112
%v112147 = vmul.f32 %v112143, %v112135
%v112151 = vadd.f32 %v112147, %v112108
%v112155 = vmul.f32 %v112151, %v112135
%v112159 = vadd.f32 %v112155, %v112104
%v112163 = vmul.f32 %v112159, %v112135
%v112167 = vadd.f32 %v112163, %v112100
%v112171 = vmul.f32 %v112167, %v112135
%v112175 = vadd.f32 %v112171, %v112096
%v112179 = vmul.f32 %v112175, %v112135
%v112183 = vadd.f32 %v112179, %v112092
%v112187 = vmul.f32 %v112183, %v112135
%v112191 = vadd.f32 %v112187, %v112088
%v112195 = vmul.f32 %v112191, %v112135
%v112199 = vadd.f32 %v112195, %v112084
%v112203 = vmul.f32 %v112199, %v112050
%v112207 = vsel /*vm=*/%vm112055, /*on_true_vy=*/%v112060, /*on_false_vx=*/%v112203
%v112211 = vmul.f32 1.4140625, %v112207
%v112214 = vpack.c.bf16 %v120417, %v112211
%120343 = vst [vmem:[%s280 + $0x3f4] sm:$0xf] /*vst_source=*/%v112214
%v112252 = vadd.s32 %v112249, %v408
%v112262 = vadd.s32 %v112252, %v415
%vm112266 = vcmp.lt.u32.totalorder %v112262, %v112252
%vm112271 = vcmp.lt.u32.totalorder %v112252, %v408
%v112276 = vadd.s32 %v112232, %v380
%v112280 = vadd.s32 1, %v112276
%v112284 = vsel /*vm=*/%vm112271, /*on_true_vy=*/%v112280, /*on_false_vx=*/%v112276
%v112288 = vadd.s32 1, %v112284
%v112292 = vsel /*vm=*/%vm112266, /*on_true_vy=*/%v112288, /*on_false_vx=*/%v112284
%v112297 = vadd.s32 %v112292, %v10
%v112301 = vadd.s32 %v112262, %v9
%v112305 = vadd.s32 %v112301, %v112297
%v112307 = vshll.u32 %v112301, 13
%v112308 = vshrl.u32 %v112301, 19
%v112309 = vor.u32 %v112308, %v112307
%v112310 = vxor.u32 %v112309, %v112305
%v112313 = vadd.s32 %v112310, %v112305
%v112315 = vshll.u32 %v112310, 15
%v112316 = vshrl.u32 %v112310, 17
%v112317 = vor.u32 %v112316, %v112315
%v112318 = vxor.u32 %v112317, %v112313
%v112321 = vadd.s32 %v112318, %v112313
%v112323 = vshll.u32 %v112318, 26
%v112324 = vshrl.u32 %v112318, 6
%v112325 = vor.u32 %v112324, %v112323
%v112326 = vxor.u32 %v112325, %v112321
%v112329 = vadd.s32 %v112326, %v112321
%v112333 = vadd.s32 %v112329, %v9
%v112335 = vshll.u32 %v112326, 6
%v112336 = vshrl.u32 %v112326, 26
%v112337 = vor.u32 %v112336, %v112335
%v112338 = vxor.u32 %v112337, %v112329
%v112341 = vadd.s32 %v112338, %v8
%v112345 = vadd.s32 1, %v112341
%v112349 = vadd.s32 %v112345, %v112333
%v112351 = vshll.u32 %v112345, 17
%v112352 = vshrl.u32 %v112345, 15
%v112353 = vor.u32 %v112352, %v112351
%v112354 = vxor.u32 %v112353, %v112349
%v112357 = vadd.s32 %v112354, %v112349
%v112359 = vshll.u32 %v112354, 29
%v112360 = vshrl.u32 %v112354, 3
%v112361 = vor.u32 %v112360, %v112359
%v112362 = vxor.u32 %v112361, %v112357
%v112365 = vadd.s32 %v112362, %v112357
%v112367 = vshll.u32 %v112362, 16
%v112368 = vshrl.u32 %v112362, 16
%v112369 = vor.u32 %v112368, %v112367
%v112370 = vxor.u32 %v112369, %v112365
%v112373 = vadd.s32 %v112370, %v112365
%v112377 = vadd.s32 %v112373, %v8
%v112379 = vshll.u32 %v112370, 24
%v112380 = vshrl.u32 %v112370, 8
%v112381 = vor.u32 %v112380, %v112379
%v112382 = vxor.u32 %v112381, %v112373
%v112385 = vadd.s32 %v112382, %v10
%v112389 = vadd.s32 2, %v112385
%v112393 = vadd.s32 %v112389, %v112377
%v112395 = vshll.u32 %v112389, 13
%v112396 = vshrl.u32 %v112389, 19
%v112397 = vor.u32 %v112396, %v112395
%v112398 = vxor.u32 %v112397, %v112393
%v112401 = vadd.s32 %v112398, %v112393
%v112403 = vshll.u32 %v112398, 15
%v112404 = vshrl.u32 %v112398, 17
%v112405 = vor.u32 %v112404, %v112403
%v112406 = vxor.u32 %v112405, %v112401
%v112409 = vadd.s32 %v112406, %v112401
%v112411 = vshll.u32 %v112406, 26
%v112412 = vshrl.u32 %v112406, 6
%v112413 = vor.u32 %v112412, %v112411
%v112414 = vxor.u32 %v112413, %v112409
%v112417 = vadd.s32 %v112414, %v112409
%v112421 = vadd.s32 %v112417, %v10
%v112423 = vshll.u32 %v112414, 6
%v112424 = vshrl.u32 %v112414, 26
%v112425 = vor.u32 %v112424, %v112423
%v112426 = vxor.u32 %v112425, %v112417
%v112429 = vadd.s32 %v112426, %v9
%v112433 = vadd.s32 3, %v112429
%v112437 = vadd.s32 %v112433, %v112421
%v112439 = vshll.u32 %v112433, 17
%v112440 = vshrl.u32 %v112433, 15
%v112441 = vor.u32 %v112440, %v112439
%v112442 = vxor.u32 %v112441, %v112437
%v112445 = vadd.s32 %v112442, %v112437
%v112447 = vshll.u32 %v112442, 29
%v112448 = vshrl.u32 %v112442, 3
%v112449 = vor.u32 %v112448, %v112447
%v112450 = vxor.u32 %v112449, %v112445
%v112453 = vadd.s32 %v112450, %v112445
%v112455 = vshll.u32 %v112450, 16
%v112456 = vshrl.u32 %v112450, 16
%v112457 = vor.u32 %v112456, %v112455
%v112458 = vxor.u32 %v112457, %v112453
%v112461 = vadd.s32 %v112458, %v112453
%v112465 = vadd.s32 %v112461, %v9
%v112467 = vshll.u32 %v112458, 24
%v112468 = vshrl.u32 %v112458, 8
%v112469 = vor.u32 %v112468, %v112467
%v112470 = vxor.u32 %v112469, %v112461
%v112473 = vadd.s32 %v112470, %v8
%v112477 = vadd.s32 4, %v112473
%v112481 = vadd.s32 %v112477, %v112465
%v112483 = vshll.u32 %v112477, 13
%v112484 = vshrl.u32 %v112477, 19
%v112485 = vor.u32 %v112484, %v112483
%v112486 = vxor.u32 %v112485, %v112481
%v112489 = vadd.s32 %v112486, %v112481
%v112491 = vshll.u32 %v112486, 15
%v112492 = vshrl.u32 %v112486, 17
%v112493 = vor.u32 %v112492, %v112491
%v112494 = vxor.u32 %v112493, %v112489
%v112497 = vadd.s32 %v112494, %v112489
%v112499 = vshll.u32 %v112494, 26
%v112500 = vshrl.u32 %v112494, 6
%v112501 = vor.u32 %v112500, %v112499
%v112502 = vxor.u32 %v112501, %v112497
%v112505 = vadd.s32 %v112502, %v112497
%v112509 = vadd.s32 %v112505, %v8
%v112511 = vshll.u32 %v112502, 6
%v112512 = vshrl.u32 %v112502, 26
%v112513 = vor.u32 %v112512, %v112511
%v112514 = vxor.u32 %v112513, %v112505
%v112517 = vadd.s32 %v112514, %v10
%v112521 = vadd.s32 5, %v112517
%v112523 = vxor.u32 %v112521, %v112509
%v112524 = vand.u32.u8 255, %v112523
%v112525 = vand.u32 65535, %v112524
%v112526 = vshrl.u32 %v112525, 1
%v112527 = vor.u32 16256, %v112526
%v112528 = vand.u32.u16 65535, %v112527
%v120348 = vadd.low.f32.bf16 -1.0, %v112528
%v112537 = vmul.f32 2.0, %v120348
%v112541 = vadd.f32 -0.99609375, %v112537
%v112545 = vmax.f32 %v112541, -0.99609375
%v112547 = vand.u32 2147483647, %v112545
%vm112550 = vcmp.eq.f32.partialorder %v112547, 1.0
%v112555 = vmul.f32 inf, %v112545
%v112557 = vxor.u32 2147483648, %v112545
%v112560 = vmul.f32 %v112557, %v112545
%v112562 = vadd.f32 1.0, %v112560
%v112563 = vlog2.pop %v112562
%v112564 = vmul.f32 0.6931472, %v112563
%v112565 = vmul.f32 -0.5, %v112560
%v112566 = vadd.f32 1.0, %v112565
%v112567 = vmul.f32 %v112566, %v112560
%v112568 = vand.u32 2147483647, %v112560
%vm112569 = vcmp.lt.f32.partialorder %v112568, 0.0004427343
%v112570 = vsel /*vm=*/%vm112569, /*on_true_vy=*/%v112567, /*on_false_vx=*/%v112564
%v112571 = vxor.u32 2147483648, %v112570
%vm112574 = vcmp.lt.f32.partialorder %v112571, 5.0
%v112579 = vsel /*vm=*/%vm112574, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v112583 = vsel /*vm=*/%vm112574, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v112587 = vsel /*vm=*/%vm112574, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v112591 = vsel /*vm=*/%vm112574, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v112595 = vsel /*vm=*/%vm112574, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v112599 = vsel /*vm=*/%vm112574, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v112603 = vsel /*vm=*/%vm112574, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v112607 = vsel /*vm=*/%vm112574, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v112611 = vsel /*vm=*/%vm112574, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v112615 = vadd.f32 -2.5, %v112571
%v112617 = vrsqrt.pop %v112571
%v112618 = vmul.f32 %v112617, %v112571
%vm112619 = vcmp.eq.f32.partialorder %v112571, inf
%v112620 = vsel /*vm=*/%vm112619, /*on_true_vy=*/%v112571, /*on_false_vx=*/%v112618
%vm112621 = vcmp.eq.f32.partialorder %v112571, 0.0
%v112622 = vand.u32 2147483648, %v112571
%v112623 = vsel /*vm=*/%vm112621, /*on_true_vy=*/%v112622, /*on_false_vx=*/%v112620
%v112626 = vadd.f32 -3.0, %v112623
%v112630 = vsel /*vm=*/%vm112574, /*on_true_vy=*/%v112615, /*on_false_vx=*/%v112626
%v112634 = vmul.f32 %v112630, %v112611
%v112638 = vadd.f32 %v112634, %v112607
%v112642 = vmul.f32 %v112638, %v112630
%v112646 = vadd.f32 %v112642, %v112603
%v112650 = vmul.f32 %v112646, %v112630
%v112654 = vadd.f32 %v112650, %v112599
%v112658 = vmul.f32 %v112654, %v112630
%v112662 = vadd.f32 %v112658, %v112595
%v112666 = vmul.f32 %v112662, %v112630
%v112670 = vadd.f32 %v112666, %v112591
%v112674 = vmul.f32 %v112670, %v112630
%v112678 = vadd.f32 %v112674, %v112587
%v112682 = vmul.f32 %v112678, %v112630
%v112686 = vadd.f32 %v112682, %v112583
%v112690 = vmul.f32 %v112686, %v112630
%v112694 = vadd.f32 %v112690, %v112579
%v112698 = vmul.f32 %v112694, %v112545
%v112702 = vsel /*vm=*/%vm112550, /*on_true_vy=*/%v112555, /*on_false_vx=*/%v112698
%v112706 = vmul.f32 1.4140625, %v112702
%v112709 = vpack.c.bf16 %v120417, %v112706
%120349 = vst [vmem:[%s280 + $0x78] sm:$0xf] /*vst_source=*/%v112709
%v112713 = vadd.s32 %v112249, %v894
%v112723 = vadd.s32 %v112713, %v415
%vm112727 = vcmp.lt.u32.totalorder %v112723, %v112713
%vm112732 = vcmp.lt.u32.totalorder %v112713, %v894
%v112737 = vadd.s32 %v112232, %v881
%v112741 = vadd.s32 1, %v112737
%v112745 = vsel /*vm=*/%vm112732, /*on_true_vy=*/%v112741, /*on_false_vx=*/%v112737
%v112749 = vadd.s32 1, %v112745
%v112753 = vsel /*vm=*/%vm112727, /*on_true_vy=*/%v112749, /*on_false_vx=*/%v112745
%v112758 = vadd.s32 %v112753, %v10
%v112762 = vadd.s32 %v112723, %v9
%v112766 = vadd.s32 %v112762, %v112758
%v112768 = vshll.u32 %v112762, 13
%v112769 = vshrl.u32 %v112762, 19
%v112770 = vor.u32 %v112769, %v112768
%v112771 = vxor.u32 %v112770, %v112766
%v112774 = vadd.s32 %v112771, %v112766
%v112776 = vshll.u32 %v112771, 15
%v112777 = vshrl.u32 %v112771, 17
%v112778 = vor.u32 %v112777, %v112776
%v112779 = vxor.u32 %v112778, %v112774
%v112782 = vadd.s32 %v112779, %v112774
%v112784 = vshll.u32 %v112779, 26
%v112785 = vshrl.u32 %v112779, 6
%v112786 = vor.u32 %v112785, %v112784
%v112787 = vxor.u32 %v112786, %v112782
%v112790 = vadd.s32 %v112787, %v112782
%v112794 = vadd.s32 %v112790, %v9
%v112796 = vshll.u32 %v112787, 6
%v112797 = vshrl.u32 %v112787, 26
%v112798 = vor.u32 %v112797, %v112796
%v112799 = vxor.u32 %v112798, %v112790
%v112802 = vadd.s32 %v112799, %v8
%v112806 = vadd.s32 1, %v112802
%v112810 = vadd.s32 %v112806, %v112794
%v112812 = vshll.u32 %v112806, 17
%v112813 = vshrl.u32 %v112806, 15
%v112814 = vor.u32 %v112813, %v112812
%v112815 = vxor.u32 %v112814, %v112810
%v112818 = vadd.s32 %v112815, %v112810
%v112820 = vshll.u32 %v112815, 29
%v112821 = vshrl.u32 %v112815, 3
%v112822 = vor.u32 %v112821, %v112820
%v112823 = vxor.u32 %v112822, %v112818
%v112826 = vadd.s32 %v112823, %v112818
%v112828 = vshll.u32 %v112823, 16
%v112829 = vshrl.u32 %v112823, 16
%v112830 = vor.u32 %v112829, %v112828
%v112831 = vxor.u32 %v112830, %v112826
%v112834 = vadd.s32 %v112831, %v112826
%v112838 = vadd.s32 %v112834, %v8
%v112840 = vshll.u32 %v112831, 24
%v112841 = vshrl.u32 %v112831, 8
%v112842 = vor.u32 %v112841, %v112840
%v112843 = vxor.u32 %v112842, %v112834
%v112846 = vadd.s32 %v112843, %v10
%v112850 = vadd.s32 2, %v112846
%v112854 = vadd.s32 %v112850, %v112838
%v112856 = vshll.u32 %v112850, 13
%v112857 = vshrl.u32 %v112850, 19
%v112858 = vor.u32 %v112857, %v112856
%v112859 = vxor.u32 %v112858, %v112854
%v112862 = vadd.s32 %v112859, %v112854
%v112864 = vshll.u32 %v112859, 15
%v112865 = vshrl.u32 %v112859, 17
%v112866 = vor.u32 %v112865, %v112864
%v112867 = vxor.u32 %v112866, %v112862
%v112870 = vadd.s32 %v112867, %v112862
%v112872 = vshll.u32 %v112867, 26
%v112873 = vshrl.u32 %v112867, 6
%v112874 = vor.u32 %v112873, %v112872
%v112875 = vxor.u32 %v112874, %v112870
%v112878 = vadd.s32 %v112875, %v112870
%v112882 = vadd.s32 %v112878, %v10
%v112884 = vshll.u32 %v112875, 6
%v112885 = vshrl.u32 %v112875, 26
%v112886 = vor.u32 %v112885, %v112884
%v112887 = vxor.u32 %v112886, %v112878
%v112890 = vadd.s32 %v112887, %v9
%v112894 = vadd.s32 3, %v112890
%v112898 = vadd.s32 %v112894, %v112882
%v112900 = vshll.u32 %v112894, 17
%v112901 = vshrl.u32 %v112894, 15
%v112902 = vor.u32 %v112901, %v112900
%v112903 = vxor.u32 %v112902, %v112898
%v112906 = vadd.s32 %v112903, %v112898
%v112908 = vshll.u32 %v112903, 29
%v112909 = vshrl.u32 %v112903, 3
%v112910 = vor.u32 %v112909, %v112908
%v112911 = vxor.u32 %v112910, %v112906
%v112914 = vadd.s32 %v112911, %v112906
%v112916 = vshll.u32 %v112911, 16
%v112917 = vshrl.u32 %v112911, 16
%v112918 = vor.u32 %v112917, %v112916
%v112919 = vxor.u32 %v112918, %v112914
%v112922 = vadd.s32 %v112919, %v112914
%v112926 = vadd.s32 %v112922, %v9
%v112928 = vshll.u32 %v112919, 24
%v112929 = vshrl.u32 %v112919, 8
%v112930 = vor.u32 %v112929, %v112928
%v112931 = vxor.u32 %v112930, %v112922
%v112934 = vadd.s32 %v112931, %v8
%v112938 = vadd.s32 4, %v112934
%v112942 = vadd.s32 %v112938, %v112926
%v112944 = vshll.u32 %v112938, 13
%v112945 = vshrl.u32 %v112938, 19
%v112946 = vor.u32 %v112945, %v112944
%v112947 = vxor.u32 %v112946, %v112942
%v112950 = vadd.s32 %v112947, %v112942
%v112952 = vshll.u32 %v112947, 15
%v112953 = vshrl.u32 %v112947, 17
%v112954 = vor.u32 %v112953, %v112952
%v112955 = vxor.u32 %v112954, %v112950
%v112958 = vadd.s32 %v112955, %v112950
%v112960 = vshll.u32 %v112955, 26
%v112961 = vshrl.u32 %v112955, 6
%v112962 = vor.u32 %v112961, %v112960
%v112963 = vxor.u32 %v112962, %v112958
%v112966 = vadd.s32 %v112963, %v112958
%v112970 = vadd.s32 %v112966, %v8
%v112972 = vshll.u32 %v112963, 6
%v112973 = vshrl.u32 %v112963, 26
%v112974 = vor.u32 %v112973, %v112972
%v112975 = vxor.u32 %v112974, %v112966
%v112978 = vadd.s32 %v112975, %v10
%v112982 = vadd.s32 5, %v112978
%v112984 = vxor.u32 %v112982, %v112970
%v112985 = vand.u32.u8 255, %v112984
%v112986 = vand.u32 65535, %v112985
%v112987 = vshrl.u32 %v112986, 1
%v112988 = vor.u32 16256, %v112987
%v112989 = vand.u32.u16 65535, %v112988
%v120350 = vadd.low.f32.bf16 -1.0, %v112989
%v112998 = vmul.f32 2.0, %v120350
%v113002 = vadd.f32 -0.99609375, %v112998
%v113006 = vmax.f32 %v113002, -0.99609375
%v113008 = vand.u32 2147483647, %v113006
%vm113011 = vcmp.eq.f32.partialorder %v113008, 1.0
%v113016 = vmul.f32 inf, %v113006
%v113018 = vxor.u32 2147483648, %v113006
%v113021 = vmul.f32 %v113018, %v113006
%v113023 = vadd.f32 1.0, %v113021
%v113024 = vlog2.pop %v113023
%v113025 = vmul.f32 0.6931472, %v113024
%v113026 = vmul.f32 -0.5, %v113021
%v113027 = vadd.f32 1.0, %v113026
%v113028 = vmul.f32 %v113027, %v113021
%v113029 = vand.u32 2147483647, %v113021
%vm113030 = vcmp.lt.f32.partialorder %v113029, 0.0004427343
%v113031 = vsel /*vm=*/%vm113030, /*on_true_vy=*/%v113028, /*on_false_vx=*/%v113025
%v113032 = vxor.u32 2147483648, %v113031
%vm113035 = vcmp.lt.f32.partialorder %v113032, 5.0
%v113040 = vsel /*vm=*/%vm113035, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v113044 = vsel /*vm=*/%vm113035, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v113048 = vsel /*vm=*/%vm113035, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v113052 = vsel /*vm=*/%vm113035, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v113056 = vsel /*vm=*/%vm113035, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v113060 = vsel /*vm=*/%vm113035, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v113064 = vsel /*vm=*/%vm113035, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v113068 = vsel /*vm=*/%vm113035, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v113072 = vsel /*vm=*/%vm113035, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v113076 = vadd.f32 -2.5, %v113032
%v113078 = vrsqrt.pop %v113032
%v113079 = vmul.f32 %v113078, %v113032
%vm113080 = vcmp.eq.f32.partialorder %v113032, inf
%v113081 = vsel /*vm=*/%vm113080, /*on_true_vy=*/%v113032, /*on_false_vx=*/%v113079
%vm113082 = vcmp.eq.f32.partialorder %v113032, 0.0
%v113083 = vand.u32 2147483648, %v113032
%v113084 = vsel /*vm=*/%vm113082, /*on_true_vy=*/%v113083, /*on_false_vx=*/%v113081
%v113087 = vadd.f32 -3.0, %v113084
%v113091 = vsel /*vm=*/%vm113035, /*on_true_vy=*/%v113076, /*on_false_vx=*/%v113087
%v113095 = vmul.f32 %v113091, %v113072
%v113099 = vadd.f32 %v113095, %v113068
%v113103 = vmul.f32 %v113099, %v113091
%v113107 = vadd.f32 %v113103, %v113064
%v113111 = vmul.f32 %v113107, %v113091
%v113115 = vadd.f32 %v113111, %v113060
%v113119 = vmul.f32 %v113115, %v113091
%v113123 = vadd.f32 %v113119, %v113056
%v113127 = vmul.f32 %v113123, %v113091
%v113131 = vadd.f32 %v113127, %v113052
%v113135 = vmul.f32 %v113131, %v113091
%v113139 = vadd.f32 %v113135, %v113048
%v113143 = vmul.f32 %v113139, %v113091
%v113147 = vadd.f32 %v113143, %v113044
%v113151 = vmul.f32 %v113147, %v113091
%v113155 = vadd.f32 %v113151, %v113040
%v113159 = vmul.f32 %v113155, %v113006
%v113163 = vsel /*vm=*/%vm113011, /*on_true_vy=*/%v113016, /*on_false_vx=*/%v113159
%v113167 = vmul.f32 1.4140625, %v113163
%v113170 = vpack.c.bf16 %v120417, %v113167
%120351 = vst [vmem:[%s280 + $0xf8] sm:$0xf] /*vst_source=*/%v113170
%v113174 = vadd.s32 %v112249, %v1381
%v113184 = vadd.s32 %v113174, %v415
%vm113188 = vcmp.lt.u32.totalorder %v113184, %v113174
%vm113193 = vcmp.lt.u32.totalorder %v113174, %v1381
%v113198 = vadd.s32 %v112232, %v1368
%v113202 = vadd.s32 1, %v113198
%v113206 = vsel /*vm=*/%vm113193, /*on_true_vy=*/%v113202, /*on_false_vx=*/%v113198
%v113210 = vadd.s32 1, %v113206
%v113214 = vsel /*vm=*/%vm113188, /*on_true_vy=*/%v113210, /*on_false_vx=*/%v113206
%v113219 = vadd.s32 %v113214, %v10
%v113223 = vadd.s32 %v113184, %v9
%v113227 = vadd.s32 %v113223, %v113219
%v113229 = vshll.u32 %v113223, 13
%v113230 = vshrl.u32 %v113223, 19
%v113231 = vor.u32 %v113230, %v113229
%v113232 = vxor.u32 %v113231, %v113227
%v113235 = vadd.s32 %v113232, %v113227
%v113237 = vshll.u32 %v113232, 15
%v113238 = vshrl.u32 %v113232, 17
%v113239 = vor.u32 %v113238, %v113237
%v113240 = vxor.u32 %v113239, %v113235
%v113243 = vadd.s32 %v113240, %v113235
%v113245 = vshll.u32 %v113240, 26
%v113246 = vshrl.u32 %v113240, 6
%v113247 = vor.u32 %v113246, %v113245
%v113248 = vxor.u32 %v113247, %v113243
%v113251 = vadd.s32 %v113248, %v113243
%v113255 = vadd.s32 %v113251, %v9
%v113257 = vshll.u32 %v113248, 6
%v113258 = vshrl.u32 %v113248, 26
%v113259 = vor.u32 %v113258, %v113257
%v113260 = vxor.u32 %v113259, %v113251
%v113263 = vadd.s32 %v113260, %v8
%v113267 = vadd.s32 1, %v113263
%v113271 = vadd.s32 %v113267, %v113255
%v113273 = vshll.u32 %v113267, 17
%v113274 = vshrl.u32 %v113267, 15
%v113275 = vor.u32 %v113274, %v113273
%v113276 = vxor.u32 %v113275, %v113271
%v113279 = vadd.s32 %v113276, %v113271
%v113281 = vshll.u32 %v113276, 29
%v113282 = vshrl.u32 %v113276, 3
%v113283 = vor.u32 %v113282, %v113281
%v113284 = vxor.u32 %v113283, %v113279
%v113287 = vadd.s32 %v113284, %v113279
%v113289 = vshll.u32 %v113284, 16
%v113290 = vshrl.u32 %v113284, 16
%v113291 = vor.u32 %v113290, %v113289
%v113292 = vxor.u32 %v113291, %v113287
%v113295 = vadd.s32 %v113292, %v113287
%v113299 = vadd.s32 %v113295, %v8
%v113301 = vshll.u32 %v113292, 24
%v113302 = vshrl.u32 %v113292, 8
%v113303 = vor.u32 %v113302, %v113301
%v113304 = vxor.u32 %v113303, %v113295
%v113307 = vadd.s32 %v113304, %v10
%v113311 = vadd.s32 2, %v113307
%v113315 = vadd.s32 %v113311, %v113299
%v113317 = vshll.u32 %v113311, 13
%v113318 = vshrl.u32 %v113311, 19
%v113319 = vor.u32 %v113318, %v113317
%v113320 = vxor.u32 %v113319, %v113315
%v113323 = vadd.s32 %v113320, %v113315
%v113325 = vshll.u32 %v113320, 15
%v113326 = vshrl.u32 %v113320, 17
%v113327 = vor.u32 %v113326, %v113325
%v113328 = vxor.u32 %v113327, %v113323
%v113331 = vadd.s32 %v113328, %v113323
%v113333 = vshll.u32 %v113328, 26
%v113334 = vshrl.u32 %v113328, 6
%v113335 = vor.u32 %v113334, %v113333
%v113336 = vxor.u32 %v113335, %v113331
%v113339 = vadd.s32 %v113336, %v113331
%v113343 = vadd.s32 %v113339, %v10
%v113345 = vshll.u32 %v113336, 6
%v113346 = vshrl.u32 %v113336, 26
%v113347 = vor.u32 %v113346, %v113345
%v113348 = vxor.u32 %v113347, %v113339
%v113351 = vadd.s32 %v113348, %v9
%v113355 = vadd.s32 3, %v113351
%v113359 = vadd.s32 %v113355, %v113343
%v113361 = vshll.u32 %v113355, 17
%v113362 = vshrl.u32 %v113355, 15
%v113363 = vor.u32 %v113362, %v113361
%v113364 = vxor.u32 %v113363, %v113359
%v113367 = vadd.s32 %v113364, %v113359
%v113369 = vshll.u32 %v113364, 29
%v113370 = vshrl.u32 %v113364, 3
%v113371 = vor.u32 %v113370, %v113369
%v113372 = vxor.u32 %v113371, %v113367
%v113375 = vadd.s32 %v113372, %v113367
%v113377 = vshll.u32 %v113372, 16
%v113378 = vshrl.u32 %v113372, 16
%v113379 = vor.u32 %v113378, %v113377
%v113380 = vxor.u32 %v113379, %v113375
%v113383 = vadd.s32 %v113380, %v113375
%v113387 = vadd.s32 %v113383, %v9
%v113389 = vshll.u32 %v113380, 24
%v113390 = vshrl.u32 %v113380, 8
%v113391 = vor.u32 %v113390, %v113389
%v113392 = vxor.u32 %v113391, %v113383
%v113395 = vadd.s32 %v113392, %v8
%v113399 = vadd.s32 4, %v113395
%v113403 = vadd.s32 %v113399, %v113387
%v113405 = vshll.u32 %v113399, 13
%v113406 = vshrl.u32 %v113399, 19
%v113407 = vor.u32 %v113406, %v113405
%v113408 = vxor.u32 %v113407, %v113403
%v113411 = vadd.s32 %v113408, %v113403
%v113413 = vshll.u32 %v113408, 15
%v113414 = vshrl.u32 %v113408, 17
%v113415 = vor.u32 %v113414, %v113413
%v113416 = vxor.u32 %v113415, %v113411
%v113419 = vadd.s32 %v113416, %v113411
%v113421 = vshll.u32 %v113416, 26
%v113422 = vshrl.u32 %v113416, 6
%v113423 = vor.u32 %v113422, %v113421
%v113424 = vxor.u32 %v113423, %v113419
%v113427 = vadd.s32 %v113424, %v113419
%v113431 = vadd.s32 %v113427, %v8
%v113433 = vshll.u32 %v113424, 6
%v113434 = vshrl.u32 %v113424, 26
%v113435 = vor.u32 %v113434, %v113433
%v113436 = vxor.u32 %v113435, %v113427
%v113439 = vadd.s32 %v113436, %v10
%v113443 = vadd.s32 5, %v113439
%v113445 = vxor.u32 %v113443, %v113431
%v113446 = vand.u32.u8 255, %v113445
%v113447 = vand.u32 65535, %v113446
%v113448 = vshrl.u32 %v113447, 1
%v113449 = vor.u32 16256, %v113448
%v113450 = vand.u32.u16 65535, %v113449
%v120352 = vadd.low.f32.bf16 -1.0, %v113450
%v113459 = vmul.f32 2.0, %v120352
%v113463 = vadd.f32 -0.99609375, %v113459
%v113467 = vmax.f32 %v113463, -0.99609375
%v113469 = vand.u32 2147483647, %v113467
%vm113472 = vcmp.eq.f32.partialorder %v113469, 1.0
%v113477 = vmul.f32 inf, %v113467
%v113479 = vxor.u32 2147483648, %v113467
%v113482 = vmul.f32 %v113479, %v113467
%v113484 = vadd.f32 1.0, %v113482
%v113485 = vlog2.pop %v113484
%v113486 = vmul.f32 0.6931472, %v113485
%v113487 = vmul.f32 -0.5, %v113482
%v113488 = vadd.f32 1.0, %v113487
%v113489 = vmul.f32 %v113488, %v113482
%v113490 = vand.u32 2147483647, %v113482
%vm113491 = vcmp.lt.f32.partialorder %v113490, 0.0004427343
%v113492 = vsel /*vm=*/%vm113491, /*on_true_vy=*/%v113489, /*on_false_vx=*/%v113486
%v113493 = vxor.u32 2147483648, %v113492
%vm113496 = vcmp.lt.f32.partialorder %v113493, 5.0
%v113501 = vsel /*vm=*/%vm113496, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v113505 = vsel /*vm=*/%vm113496, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v113509 = vsel /*vm=*/%vm113496, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v113513 = vsel /*vm=*/%vm113496, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v113517 = vsel /*vm=*/%vm113496, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v113521 = vsel /*vm=*/%vm113496, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v113525 = vsel /*vm=*/%vm113496, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v113529 = vsel /*vm=*/%vm113496, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v113533 = vsel /*vm=*/%vm113496, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v113537 = vadd.f32 -2.5, %v113493
%v113539 = vrsqrt.pop %v113493
%v113540 = vmul.f32 %v113539, %v113493
%vm113541 = vcmp.eq.f32.partialorder %v113493, inf
%v113542 = vsel /*vm=*/%vm113541, /*on_true_vy=*/%v113493, /*on_false_vx=*/%v113540
%vm113543 = vcmp.eq.f32.partialorder %v113493, 0.0
%v113544 = vand.u32 2147483648, %v113493
%v113545 = vsel /*vm=*/%vm113543, /*on_true_vy=*/%v113544, /*on_false_vx=*/%v113542
%v113548 = vadd.f32 -3.0, %v113545
%v113552 = vsel /*vm=*/%vm113496, /*on_true_vy=*/%v113537, /*on_false_vx=*/%v113548
%v113556 = vmul.f32 %v113552, %v113533
%v113560 = vadd.f32 %v113556, %v113529
%v113564 = vmul.f32 %v113560, %v113552
%v113568 = vadd.f32 %v113564, %v113525
%v113572 = vmul.f32 %v113568, %v113552
%v113576 = vadd.f32 %v113572, %v113521
%v113580 = vmul.f32 %v113576, %v113552
%v113584 = vadd.f32 %v113580, %v113517
%v113588 = vmul.f32 %v113584, %v113552
%v113592 = vadd.f32 %v113588, %v113513
%v113596 = vmul.f32 %v113592, %v113552
%v113600 = vadd.f32 %v113596, %v113509
%v113604 = vmul.f32 %v113600, %v113552
%v113608 = vadd.f32 %v113604, %v113505
%v113612 = vmul.f32 %v113608, %v113552
%v113616 = vadd.f32 %v113612, %v113501
%v113620 = vmul.f32 %v113616, %v113467
%v113624 = vsel /*vm=*/%vm113472, /*on_true_vy=*/%v113477, /*on_false_vx=*/%v113620
%v113628 = vmul.f32 1.4140625, %v113624
%v113631 = vpack.c.bf16 %v120417, %v113628
%120353 = vst [vmem:[%s280 + $0x178] sm:$0xf] /*vst_source=*/%v113631
%v113635 = vadd.s32 %v112249, %v1868
%v113645 = vadd.s32 %v113635, %v415
%vm113649 = vcmp.lt.u32.totalorder %v113645, %v113635
%vm113654 = vcmp.lt.u32.totalorder %v113635, %v1868
%v113659 = vadd.s32 %v112232, %v1855
%v113663 = vadd.s32 1, %v113659
%v113667 = vsel /*vm=*/%vm113654, /*on_true_vy=*/%v113663, /*on_false_vx=*/%v113659
%v113671 = vadd.s32 1, %v113667
%v113675 = vsel /*vm=*/%vm113649, /*on_true_vy=*/%v113671, /*on_false_vx=*/%v113667
%v113680 = vadd.s32 %v113675, %v10
%v113684 = vadd.s32 %v113645, %v9
%v113688 = vadd.s32 %v113684, %v113680
%v113690 = vshll.u32 %v113684, 13
%v113691 = vshrl.u32 %v113684, 19
%v113692 = vor.u32 %v113691, %v113690
%v113693 = vxor.u32 %v113692, %v113688
%v113696 = vadd.s32 %v113693, %v113688
%v113698 = vshll.u32 %v113693, 15
%v113699 = vshrl.u32 %v113693, 17
%v113700 = vor.u32 %v113699, %v113698
%v113701 = vxor.u32 %v113700, %v113696
%v113704 = vadd.s32 %v113701, %v113696
%v113706 = vshll.u32 %v113701, 26
%v113707 = vshrl.u32 %v113701, 6
%v113708 = vor.u32 %v113707, %v113706
%v113709 = vxor.u32 %v113708, %v113704
%v113712 = vadd.s32 %v113709, %v113704
%v113716 = vadd.s32 %v113712, %v9
%v113718 = vshll.u32 %v113709, 6
%v113719 = vshrl.u32 %v113709, 26
%v113720 = vor.u32 %v113719, %v113718
%v113721 = vxor.u32 %v113720, %v113712
%v113724 = vadd.s32 %v113721, %v8
%v113728 = vadd.s32 1, %v113724
%v113732 = vadd.s32 %v113728, %v113716
%v113734 = vshll.u32 %v113728, 17
%v113735 = vshrl.u32 %v113728, 15
%v113736 = vor.u32 %v113735, %v113734
%v113737 = vxor.u32 %v113736, %v113732
%v113740 = vadd.s32 %v113737, %v113732
%v113742 = vshll.u32 %v113737, 29
%v113743 = vshrl.u32 %v113737, 3
%v113744 = vor.u32 %v113743, %v113742
%v113745 = vxor.u32 %v113744, %v113740
%v113748 = vadd.s32 %v113745, %v113740
%v113750 = vshll.u32 %v113745, 16
%v113751 = vshrl.u32 %v113745, 16
%v113752 = vor.u32 %v113751, %v113750
%v113753 = vxor.u32 %v113752, %v113748
%v113756 = vadd.s32 %v113753, %v113748
%v113760 = vadd.s32 %v113756, %v8
%v113762 = vshll.u32 %v113753, 24
%v113763 = vshrl.u32 %v113753, 8
%v113764 = vor.u32 %v113763, %v113762
%v113765 = vxor.u32 %v113764, %v113756
%v113768 = vadd.s32 %v113765, %v10
%v113772 = vadd.s32 2, %v113768
%v113776 = vadd.s32 %v113772, %v113760
%v113778 = vshll.u32 %v113772, 13
%v113779 = vshrl.u32 %v113772, 19
%v113780 = vor.u32 %v113779, %v113778
%v113781 = vxor.u32 %v113780, %v113776
%v113784 = vadd.s32 %v113781, %v113776
%v113786 = vshll.u32 %v113781, 15
%v113787 = vshrl.u32 %v113781, 17
%v113788 = vor.u32 %v113787, %v113786
%v113789 = vxor.u32 %v113788, %v113784
%v113792 = vadd.s32 %v113789, %v113784
%v113794 = vshll.u32 %v113789, 26
%v113795 = vshrl.u32 %v113789, 6
%v113796 = vor.u32 %v113795, %v113794
%v113797 = vxor.u32 %v113796, %v113792
%v113800 = vadd.s32 %v113797, %v113792
%v113804 = vadd.s32 %v113800, %v10
%v113806 = vshll.u32 %v113797, 6
%v113807 = vshrl.u32 %v113797, 26
%v113808 = vor.u32 %v113807, %v113806
%v113809 = vxor.u32 %v113808, %v113800
%v113812 = vadd.s32 %v113809, %v9
%v113816 = vadd.s32 3, %v113812
%v113820 = vadd.s32 %v113816, %v113804
%v113822 = vshll.u32 %v113816, 17
%v113823 = vshrl.u32 %v113816, 15
%v113824 = vor.u32 %v113823, %v113822
%v113825 = vxor.u32 %v113824, %v113820
%v113828 = vadd.s32 %v113825, %v113820
%v113830 = vshll.u32 %v113825, 29
%v113831 = vshrl.u32 %v113825, 3
%v113832 = vor.u32 %v113831, %v113830
%v113833 = vxor.u32 %v113832, %v113828
%v113836 = vadd.s32 %v113833, %v113828
%v113838 = vshll.u32 %v113833, 16
%v113839 = vshrl.u32 %v113833, 16
%v113840 = vor.u32 %v113839, %v113838
%v113841 = vxor.u32 %v113840, %v113836
%v113844 = vadd.s32 %v113841, %v113836
%v113848 = vadd.s32 %v113844, %v9
%v113850 = vshll.u32 %v113841, 24
%v113851 = vshrl.u32 %v113841, 8
%v113852 = vor.u32 %v113851, %v113850
%v113853 = vxor.u32 %v113852, %v113844
%v113856 = vadd.s32 %v113853, %v8
%v113860 = vadd.s32 4, %v113856
%v113864 = vadd.s32 %v113860, %v113848
%v113866 = vshll.u32 %v113860, 13
%v113867 = vshrl.u32 %v113860, 19
%v113868 = vor.u32 %v113867, %v113866
%v113869 = vxor.u32 %v113868, %v113864
%v113872 = vadd.s32 %v113869, %v113864
%v113874 = vshll.u32 %v113869, 15
%v113875 = vshrl.u32 %v113869, 17
%v113876 = vor.u32 %v113875, %v113874
%v113877 = vxor.u32 %v113876, %v113872
%v113880 = vadd.s32 %v113877, %v113872
%v113882 = vshll.u32 %v113877, 26
%v113883 = vshrl.u32 %v113877, 6
%v113884 = vor.u32 %v113883, %v113882
%v113885 = vxor.u32 %v113884, %v113880
%v113888 = vadd.s32 %v113885, %v113880
%v113892 = vadd.s32 %v113888, %v8
%v113894 = vshll.u32 %v113885, 6
%v113895 = vshrl.u32 %v113885, 26
%v113896 = vor.u32 %v113895, %v113894
%v113897 = vxor.u32 %v113896, %v113888
%v113900 = vadd.s32 %v113897, %v10
%v113904 = vadd.s32 5, %v113900
%v113906 = vxor.u32 %v113904, %v113892
%v113907 = vand.u32.u8 255, %v113906
%v113908 = vand.u32 65535, %v113907
%v113909 = vshrl.u32 %v113908, 1
%v113910 = vor.u32 16256, %v113909
%v113911 = vand.u32.u16 65535, %v113910
%v120354 = vadd.low.f32.bf16 -1.0, %v113911
%v113920 = vmul.f32 2.0, %v120354
%v113924 = vadd.f32 -0.99609375, %v113920
%v113928 = vmax.f32 %v113924, -0.99609375
%v113930 = vand.u32 2147483647, %v113928
%vm113933 = vcmp.eq.f32.partialorder %v113930, 1.0
%v113938 = vmul.f32 inf, %v113928
%v113940 = vxor.u32 2147483648, %v113928
%v113943 = vmul.f32 %v113940, %v113928
%v113945 = vadd.f32 1.0, %v113943
%v113946 = vlog2.pop %v113945
%v113947 = vmul.f32 0.6931472, %v113946
%v113948 = vmul.f32 -0.5, %v113943
%v113949 = vadd.f32 1.0, %v113948
%v113950 = vmul.f32 %v113949, %v113943
%v113951 = vand.u32 2147483647, %v113943
%vm113952 = vcmp.lt.f32.partialorder %v113951, 0.0004427343
%v113953 = vsel /*vm=*/%vm113952, /*on_true_vy=*/%v113950, /*on_false_vx=*/%v113947
%v113954 = vxor.u32 2147483648, %v113953
%vm113957 = vcmp.lt.f32.partialorder %v113954, 5.0
%v113962 = vsel /*vm=*/%vm113957, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v113966 = vsel /*vm=*/%vm113957, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v113970 = vsel /*vm=*/%vm113957, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v113974 = vsel /*vm=*/%vm113957, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v113978 = vsel /*vm=*/%vm113957, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v113982 = vsel /*vm=*/%vm113957, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v113986 = vsel /*vm=*/%vm113957, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v113990 = vsel /*vm=*/%vm113957, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v113994 = vsel /*vm=*/%vm113957, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v113998 = vadd.f32 -2.5, %v113954
%v114000 = vrsqrt.pop %v113954
%v114001 = vmul.f32 %v114000, %v113954
%vm114002 = vcmp.eq.f32.partialorder %v113954, inf
%v114003 = vsel /*vm=*/%vm114002, /*on_true_vy=*/%v113954, /*on_false_vx=*/%v114001
%vm114004 = vcmp.eq.f32.partialorder %v113954, 0.0
%v114005 = vand.u32 2147483648, %v113954
%v114006 = vsel /*vm=*/%vm114004, /*on_true_vy=*/%v114005, /*on_false_vx=*/%v114003
%v114009 = vadd.f32 -3.0, %v114006
%v114013 = vsel /*vm=*/%vm113957, /*on_true_vy=*/%v113998, /*on_false_vx=*/%v114009
%v114017 = vmul.f32 %v114013, %v113994
%v114021 = vadd.f32 %v114017, %v113990
%v114025 = vmul.f32 %v114021, %v114013
%v114029 = vadd.f32 %v114025, %v113986
%v114033 = vmul.f32 %v114029, %v114013
%v114037 = vadd.f32 %v114033, %v113982
%v114041 = vmul.f32 %v114037, %v114013
%v114045 = vadd.f32 %v114041, %v113978
%v114049 = vmul.f32 %v114045, %v114013
%v114053 = vadd.f32 %v114049, %v113974
%v114057 = vmul.f32 %v114053, %v114013
%v114061 = vadd.f32 %v114057, %v113970
%v114065 = vmul.f32 %v114061, %v114013
%v114069 = vadd.f32 %v114065, %v113966
%v114073 = vmul.f32 %v114069, %v114013
%v114077 = vadd.f32 %v114073, %v113962
%v114081 = vmul.f32 %v114077, %v113928
%v114085 = vsel /*vm=*/%vm113933, /*on_true_vy=*/%v113938, /*on_false_vx=*/%v114081
%v114089 = vmul.f32 1.4140625, %v114085
%v114092 = vpack.c.bf16 %v120417, %v114089
%120355 = vst [vmem:[%s280 + $0x1f8] sm:$0xf] /*vst_source=*/%v114092
%v114096 = vadd.s32 %v112249, %v2355
%v114106 = vadd.s32 %v114096, %v415
%vm114110 = vcmp.lt.u32.totalorder %v114106, %v114096
%vm114115 = vcmp.lt.u32.totalorder %v114096, %v2355
%v114120 = vadd.s32 %v112232, %v2342
%v114124 = vadd.s32 1, %v114120
%v114128 = vsel /*vm=*/%vm114115, /*on_true_vy=*/%v114124, /*on_false_vx=*/%v114120
%v114132 = vadd.s32 1, %v114128
%v114136 = vsel /*vm=*/%vm114110, /*on_true_vy=*/%v114132, /*on_false_vx=*/%v114128
%v114141 = vadd.s32 %v114136, %v10
%v114145 = vadd.s32 %v114106, %v9
%v114149 = vadd.s32 %v114145, %v114141
%v114151 = vshll.u32 %v114145, 13
%v114152 = vshrl.u32 %v114145, 19
%v114153 = vor.u32 %v114152, %v114151
%v114154 = vxor.u32 %v114153, %v114149
%v114157 = vadd.s32 %v114154, %v114149
%v114159 = vshll.u32 %v114154, 15
%v114160 = vshrl.u32 %v114154, 17
%v114161 = vor.u32 %v114160, %v114159
%v114162 = vxor.u32 %v114161, %v114157
%v114165 = vadd.s32 %v114162, %v114157
%v114167 = vshll.u32 %v114162, 26
%v114168 = vshrl.u32 %v114162, 6
%v114169 = vor.u32 %v114168, %v114167
%v114170 = vxor.u32 %v114169, %v114165
%v114173 = vadd.s32 %v114170, %v114165
%v114177 = vadd.s32 %v114173, %v9
%v114179 = vshll.u32 %v114170, 6
%v114180 = vshrl.u32 %v114170, 26
%v114181 = vor.u32 %v114180, %v114179
%v114182 = vxor.u32 %v114181, %v114173
%v114185 = vadd.s32 %v114182, %v8
%v114189 = vadd.s32 1, %v114185
%v114193 = vadd.s32 %v114189, %v114177
%v114195 = vshll.u32 %v114189, 17
%v114196 = vshrl.u32 %v114189, 15
%v114197 = vor.u32 %v114196, %v114195
%v114198 = vxor.u32 %v114197, %v114193
%v114201 = vadd.s32 %v114198, %v114193
%v114203 = vshll.u32 %v114198, 29
%v114204 = vshrl.u32 %v114198, 3
%v114205 = vor.u32 %v114204, %v114203
%v114206 = vxor.u32 %v114205, %v114201
%v114209 = vadd.s32 %v114206, %v114201
%v114211 = vshll.u32 %v114206, 16
%v114212 = vshrl.u32 %v114206, 16
%v114213 = vor.u32 %v114212, %v114211
%v114214 = vxor.u32 %v114213, %v114209
%v114217 = vadd.s32 %v114214, %v114209
%v114221 = vadd.s32 %v114217, %v8
%v114223 = vshll.u32 %v114214, 24
%v114224 = vshrl.u32 %v114214, 8
%v114225 = vor.u32 %v114224, %v114223
%v114226 = vxor.u32 %v114225, %v114217
%v114229 = vadd.s32 %v114226, %v10
%v114233 = vadd.s32 2, %v114229
%v114237 = vadd.s32 %v114233, %v114221
%v114239 = vshll.u32 %v114233, 13
%v114240 = vshrl.u32 %v114233, 19
%v114241 = vor.u32 %v114240, %v114239
%v114242 = vxor.u32 %v114241, %v114237
%v114245 = vadd.s32 %v114242, %v114237
%v114247 = vshll.u32 %v114242, 15
%v114248 = vshrl.u32 %v114242, 17
%v114249 = vor.u32 %v114248, %v114247
%v114250 = vxor.u32 %v114249, %v114245
%v114253 = vadd.s32 %v114250, %v114245
%v114255 = vshll.u32 %v114250, 26
%v114256 = vshrl.u32 %v114250, 6
%v114257 = vor.u32 %v114256, %v114255
%v114258 = vxor.u32 %v114257, %v114253
%v114261 = vadd.s32 %v114258, %v114253
%v114265 = vadd.s32 %v114261, %v10
%v114267 = vshll.u32 %v114258, 6
%v114268 = vshrl.u32 %v114258, 26
%v114269 = vor.u32 %v114268, %v114267
%v114270 = vxor.u32 %v114269, %v114261
%v114273 = vadd.s32 %v114270, %v9
%v114277 = vadd.s32 3, %v114273
%v114281 = vadd.s32 %v114277, %v114265
%v114283 = vshll.u32 %v114277, 17
%v114284 = vshrl.u32 %v114277, 15
%v114285 = vor.u32 %v114284, %v114283
%v114286 = vxor.u32 %v114285, %v114281
%v114289 = vadd.s32 %v114286, %v114281
%v114291 = vshll.u32 %v114286, 29
%v114292 = vshrl.u32 %v114286, 3
%v114293 = vor.u32 %v114292, %v114291
%v114294 = vxor.u32 %v114293, %v114289
%v114297 = vadd.s32 %v114294, %v114289
%v114299 = vshll.u32 %v114294, 16
%v114300 = vshrl.u32 %v114294, 16
%v114301 = vor.u32 %v114300, %v114299
%v114302 = vxor.u32 %v114301, %v114297
%v114305 = vadd.s32 %v114302, %v114297
%v114309 = vadd.s32 %v114305, %v9
%v114311 = vshll.u32 %v114302, 24
%v114312 = vshrl.u32 %v114302, 8
%v114313 = vor.u32 %v114312, %v114311
%v114314 = vxor.u32 %v114313, %v114305
%v114317 = vadd.s32 %v114314, %v8
%v114321 = vadd.s32 4, %v114317
%v114325 = vadd.s32 %v114321, %v114309
%v114327 = vshll.u32 %v114321, 13
%v114328 = vshrl.u32 %v114321, 19
%v114329 = vor.u32 %v114328, %v114327
%v114330 = vxor.u32 %v114329, %v114325
%v114333 = vadd.s32 %v114330, %v114325
%v114335 = vshll.u32 %v114330, 15
%v114336 = vshrl.u32 %v114330, 17
%v114337 = vor.u32 %v114336, %v114335
%v114338 = vxor.u32 %v114337, %v114333
%v114341 = vadd.s32 %v114338, %v114333
%v114343 = vshll.u32 %v114338, 26
%v114344 = vshrl.u32 %v114338, 6
%v114345 = vor.u32 %v114344, %v114343
%v114346 = vxor.u32 %v114345, %v114341
%v114349 = vadd.s32 %v114346, %v114341
%v114353 = vadd.s32 %v114349, %v8
%v114355 = vshll.u32 %v114346, 6
%v114356 = vshrl.u32 %v114346, 26
%v114357 = vor.u32 %v114356, %v114355
%v114358 = vxor.u32 %v114357, %v114349
%v114361 = vadd.s32 %v114358, %v10
%v114365 = vadd.s32 5, %v114361
%v114367 = vxor.u32 %v114365, %v114353
%v114368 = vand.u32.u8 255, %v114367
%v114369 = vand.u32 65535, %v114368
%v114370 = vshrl.u32 %v114369, 1
%v114371 = vor.u32 16256, %v114370
%v114372 = vand.u32.u16 65535, %v114371
%v120356 = vadd.low.f32.bf16 -1.0, %v114372
%v114381 = vmul.f32 2.0, %v120356
%v114385 = vadd.f32 -0.99609375, %v114381
%v114389 = vmax.f32 %v114385, -0.99609375
%v114391 = vand.u32 2147483647, %v114389
%vm114394 = vcmp.eq.f32.partialorder %v114391, 1.0
%v114399 = vmul.f32 inf, %v114389
%v114401 = vxor.u32 2147483648, %v114389
%v114404 = vmul.f32 %v114401, %v114389
%v114406 = vadd.f32 1.0, %v114404
%v114407 = vlog2.pop %v114406
%v114408 = vmul.f32 0.6931472, %v114407
%v114409 = vmul.f32 -0.5, %v114404
%v114410 = vadd.f32 1.0, %v114409
%v114411 = vmul.f32 %v114410, %v114404
%v114412 = vand.u32 2147483647, %v114404
%vm114413 = vcmp.lt.f32.partialorder %v114412, 0.0004427343
%v114414 = vsel /*vm=*/%vm114413, /*on_true_vy=*/%v114411, /*on_false_vx=*/%v114408
%v114415 = vxor.u32 2147483648, %v114414
%vm114418 = vcmp.lt.f32.partialorder %v114415, 5.0
%v114423 = vsel /*vm=*/%vm114418, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v114427 = vsel /*vm=*/%vm114418, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v114431 = vsel /*vm=*/%vm114418, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v114435 = vsel /*vm=*/%vm114418, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v114439 = vsel /*vm=*/%vm114418, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v114443 = vsel /*vm=*/%vm114418, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v114447 = vsel /*vm=*/%vm114418, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v114451 = vsel /*vm=*/%vm114418, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v114455 = vsel /*vm=*/%vm114418, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v114459 = vadd.f32 -2.5, %v114415
%v114461 = vrsqrt.pop %v114415
%v114462 = vmul.f32 %v114461, %v114415
%vm114463 = vcmp.eq.f32.partialorder %v114415, inf
%v114464 = vsel /*vm=*/%vm114463, /*on_true_vy=*/%v114415, /*on_false_vx=*/%v114462
%vm114465 = vcmp.eq.f32.partialorder %v114415, 0.0
%v114466 = vand.u32 2147483648, %v114415
%v114467 = vsel /*vm=*/%vm114465, /*on_true_vy=*/%v114466, /*on_false_vx=*/%v114464
%v114470 = vadd.f32 -3.0, %v114467
%v114474 = vsel /*vm=*/%vm114418, /*on_true_vy=*/%v114459, /*on_false_vx=*/%v114470
%v114478 = vmul.f32 %v114474, %v114455
%v114482 = vadd.f32 %v114478, %v114451
%v114486 = vmul.f32 %v114482, %v114474
%v114490 = vadd.f32 %v114486, %v114447
%v114494 = vmul.f32 %v114490, %v114474
%v114498 = vadd.f32 %v114494, %v114443
%v114502 = vmul.f32 %v114498, %v114474
%v114506 = vadd.f32 %v114502, %v114439
%v114510 = vmul.f32 %v114506, %v114474
%v114514 = vadd.f32 %v114510, %v114435
%v114518 = vmul.f32 %v114514, %v114474
%v114522 = vadd.f32 %v114518, %v114431
%v114526 = vmul.f32 %v114522, %v114474
%v114530 = vadd.f32 %v114526, %v114427
%v114534 = vmul.f32 %v114530, %v114474
%v114538 = vadd.f32 %v114534, %v114423
%v114542 = vmul.f32 %v114538, %v114389
%v114546 = vsel /*vm=*/%vm114394, /*on_true_vy=*/%v114399, /*on_false_vx=*/%v114542
%v114550 = vmul.f32 1.4140625, %v114546
%v114553 = vpack.c.bf16 %v120417, %v114550
%120357 = vst [vmem:[%s280 + $0x278] sm:$0xf] /*vst_source=*/%v114553
%v114557 = vadd.s32 %v112249, %v2842
%v114567 = vadd.s32 %v114557, %v415
%vm114571 = vcmp.lt.u32.totalorder %v114567, %v114557
%vm114576 = vcmp.lt.u32.totalorder %v114557, %v2842
%v114581 = vadd.s32 %v112232, %v2829
%v114585 = vadd.s32 1, %v114581
%v114589 = vsel /*vm=*/%vm114576, /*on_true_vy=*/%v114585, /*on_false_vx=*/%v114581
%v114593 = vadd.s32 1, %v114589
%v114597 = vsel /*vm=*/%vm114571, /*on_true_vy=*/%v114593, /*on_false_vx=*/%v114589
%v114602 = vadd.s32 %v114597, %v10
%v114606 = vadd.s32 %v114567, %v9
%v114610 = vadd.s32 %v114606, %v114602
%v114612 = vshll.u32 %v114606, 13
%v114613 = vshrl.u32 %v114606, 19
%v114614 = vor.u32 %v114613, %v114612
%v114615 = vxor.u32 %v114614, %v114610
%v114618 = vadd.s32 %v114615, %v114610
%v114620 = vshll.u32 %v114615, 15
%v114621 = vshrl.u32 %v114615, 17
%v114622 = vor.u32 %v114621, %v114620
%v114623 = vxor.u32 %v114622, %v114618
%v114626 = vadd.s32 %v114623, %v114618
%v114628 = vshll.u32 %v114623, 26
%v114629 = vshrl.u32 %v114623, 6
%v114630 = vor.u32 %v114629, %v114628
%v114631 = vxor.u32 %v114630, %v114626
%v114634 = vadd.s32 %v114631, %v114626
%v114638 = vadd.s32 %v114634, %v9
%v114640 = vshll.u32 %v114631, 6
%v114641 = vshrl.u32 %v114631, 26
%v114642 = vor.u32 %v114641, %v114640
%v114643 = vxor.u32 %v114642, %v114634
%v114646 = vadd.s32 %v114643, %v8
%v114650 = vadd.s32 1, %v114646
%v114654 = vadd.s32 %v114650, %v114638
%v114656 = vshll.u32 %v114650, 17
%v114657 = vshrl.u32 %v114650, 15
%v114658 = vor.u32 %v114657, %v114656
%v114659 = vxor.u32 %v114658, %v114654
%v114662 = vadd.s32 %v114659, %v114654
%v114664 = vshll.u32 %v114659, 29
%v114665 = vshrl.u32 %v114659, 3
%v114666 = vor.u32 %v114665, %v114664
%v114667 = vxor.u32 %v114666, %v114662
%v114670 = vadd.s32 %v114667, %v114662
%v114672 = vshll.u32 %v114667, 16
%v114673 = vshrl.u32 %v114667, 16
%v114674 = vor.u32 %v114673, %v114672
%v114675 = vxor.u32 %v114674, %v114670
%v114678 = vadd.s32 %v114675, %v114670
%v114682 = vadd.s32 %v114678, %v8
%v114684 = vshll.u32 %v114675, 24
%v114685 = vshrl.u32 %v114675, 8
%v114686 = vor.u32 %v114685, %v114684
%v114687 = vxor.u32 %v114686, %v114678
%v114690 = vadd.s32 %v114687, %v10
%v114694 = vadd.s32 2, %v114690
%v114698 = vadd.s32 %v114694, %v114682
%v114700 = vshll.u32 %v114694, 13
%v114701 = vshrl.u32 %v114694, 19
%v114702 = vor.u32 %v114701, %v114700
%v114703 = vxor.u32 %v114702, %v114698
%v114706 = vadd.s32 %v114703, %v114698
%v114708 = vshll.u32 %v114703, 15
%v114709 = vshrl.u32 %v114703, 17
%v114710 = vor.u32 %v114709, %v114708
%v114711 = vxor.u32 %v114710, %v114706
%v114714 = vadd.s32 %v114711, %v114706
%v114716 = vshll.u32 %v114711, 26
%v114717 = vshrl.u32 %v114711, 6
%v114718 = vor.u32 %v114717, %v114716
%v114719 = vxor.u32 %v114718, %v114714
%v114722 = vadd.s32 %v114719, %v114714
%v114726 = vadd.s32 %v114722, %v10
%v114728 = vshll.u32 %v114719, 6
%v114729 = vshrl.u32 %v114719, 26
%v114730 = vor.u32 %v114729, %v114728
%v114731 = vxor.u32 %v114730, %v114722
%v114734 = vadd.s32 %v114731, %v9
%v114738 = vadd.s32 3, %v114734
%v114742 = vadd.s32 %v114738, %v114726
%v114744 = vshll.u32 %v114738, 17
%v114745 = vshrl.u32 %v114738, 15
%v114746 = vor.u32 %v114745, %v114744
%v114747 = vxor.u32 %v114746, %v114742
%v114750 = vadd.s32 %v114747, %v114742
%v114752 = vshll.u32 %v114747, 29
%v114753 = vshrl.u32 %v114747, 3
%v114754 = vor.u32 %v114753, %v114752
%v114755 = vxor.u32 %v114754, %v114750
%v114758 = vadd.s32 %v114755, %v114750
%v114760 = vshll.u32 %v114755, 16
%v114761 = vshrl.u32 %v114755, 16
%v114762 = vor.u32 %v114761, %v114760
%v114763 = vxor.u32 %v114762, %v114758
%v114766 = vadd.s32 %v114763, %v114758
%v114770 = vadd.s32 %v114766, %v9
%v114772 = vshll.u32 %v114763, 24
%v114773 = vshrl.u32 %v114763, 8
%v114774 = vor.u32 %v114773, %v114772
%v114775 = vxor.u32 %v114774, %v114766
%v114778 = vadd.s32 %v114775, %v8
%v114782 = vadd.s32 4, %v114778
%v114786 = vadd.s32 %v114782, %v114770
%v114788 = vshll.u32 %v114782, 13
%v114789 = vshrl.u32 %v114782, 19
%v114790 = vor.u32 %v114789, %v114788
%v114791 = vxor.u32 %v114790, %v114786
%v114794 = vadd.s32 %v114791, %v114786
%v114796 = vshll.u32 %v114791, 15
%v114797 = vshrl.u32 %v114791, 17
%v114798 = vor.u32 %v114797, %v114796
%v114799 = vxor.u32 %v114798, %v114794
%v114802 = vadd.s32 %v114799, %v114794
%v114804 = vshll.u32 %v114799, 26
%v114805 = vshrl.u32 %v114799, 6
%v114806 = vor.u32 %v114805, %v114804
%v114807 = vxor.u32 %v114806, %v114802
%v114810 = vadd.s32 %v114807, %v114802
%v114814 = vadd.s32 %v114810, %v8
%v114816 = vshll.u32 %v114807, 6
%v114817 = vshrl.u32 %v114807, 26
%v114818 = vor.u32 %v114817, %v114816
%v114819 = vxor.u32 %v114818, %v114810
%v114822 = vadd.s32 %v114819, %v10
%v114826 = vadd.s32 5, %v114822
%v114828 = vxor.u32 %v114826, %v114814
%v114829 = vand.u32.u8 255, %v114828
%v114830 = vand.u32 65535, %v114829
%v114831 = vshrl.u32 %v114830, 1
%v114832 = vor.u32 16256, %v114831
%v114833 = vand.u32.u16 65535, %v114832
%v120358 = vadd.low.f32.bf16 -1.0, %v114833
%v114842 = vmul.f32 2.0, %v120358
%v114846 = vadd.f32 -0.99609375, %v114842
%v114850 = vmax.f32 %v114846, -0.99609375
%v114852 = vand.u32 2147483647, %v114850
%vm114855 = vcmp.eq.f32.partialorder %v114852, 1.0
%v114860 = vmul.f32 inf, %v114850
%v114862 = vxor.u32 2147483648, %v114850
%v114865 = vmul.f32 %v114862, %v114850
%v114867 = vadd.f32 1.0, %v114865
%v114868 = vlog2.pop %v114867
%v114869 = vmul.f32 0.6931472, %v114868
%v114870 = vmul.f32 -0.5, %v114865
%v114871 = vadd.f32 1.0, %v114870
%v114872 = vmul.f32 %v114871, %v114865
%v114873 = vand.u32 2147483647, %v114865
%vm114874 = vcmp.lt.f32.partialorder %v114873, 0.0004427343
%v114875 = vsel /*vm=*/%vm114874, /*on_true_vy=*/%v114872, /*on_false_vx=*/%v114869
%v114876 = vxor.u32 2147483648, %v114875
%vm114879 = vcmp.lt.f32.partialorder %v114876, 5.0
%v114884 = vsel /*vm=*/%vm114879, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v114888 = vsel /*vm=*/%vm114879, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v114892 = vsel /*vm=*/%vm114879, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v114896 = vsel /*vm=*/%vm114879, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v114900 = vsel /*vm=*/%vm114879, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v114904 = vsel /*vm=*/%vm114879, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v114908 = vsel /*vm=*/%vm114879, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v114912 = vsel /*vm=*/%vm114879, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v114916 = vsel /*vm=*/%vm114879, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v114920 = vadd.f32 -2.5, %v114876
%v114922 = vrsqrt.pop %v114876
%v114923 = vmul.f32 %v114922, %v114876
%vm114924 = vcmp.eq.f32.partialorder %v114876, inf
%v114925 = vsel /*vm=*/%vm114924, /*on_true_vy=*/%v114876, /*on_false_vx=*/%v114923
%vm114926 = vcmp.eq.f32.partialorder %v114876, 0.0
%v114927 = vand.u32 2147483648, %v114876
%v114928 = vsel /*vm=*/%vm114926, /*on_true_vy=*/%v114927, /*on_false_vx=*/%v114925
%v114931 = vadd.f32 -3.0, %v114928
%v114935 = vsel /*vm=*/%vm114879, /*on_true_vy=*/%v114920, /*on_false_vx=*/%v114931
%v114939 = vmul.f32 %v114935, %v114916
%v114943 = vadd.f32 %v114939, %v114912
%v114947 = vmul.f32 %v114943, %v114935
%v114951 = vadd.f32 %v114947, %v114908
%v114955 = vmul.f32 %v114951, %v114935
%v114959 = vadd.f32 %v114955, %v114904
%v114963 = vmul.f32 %v114959, %v114935
%v114967 = vadd.f32 %v114963, %v114900
%v114971 = vmul.f32 %v114967, %v114935
%v114975 = vadd.f32 %v114971, %v114896
%v114979 = vmul.f32 %v114975, %v114935
%v114983 = vadd.f32 %v114979, %v114892
%v114987 = vmul.f32 %v114983, %v114935
%v114991 = vadd.f32 %v114987, %v114888
%v114995 = vmul.f32 %v114991, %v114935
%v114999 = vadd.f32 %v114995, %v114884
%v115003 = vmul.f32 %v114999, %v114850
%v115007 = vsel /*vm=*/%vm114855, /*on_true_vy=*/%v114860, /*on_false_vx=*/%v115003
%v115011 = vmul.f32 1.4140625, %v115007
%v115014 = vpack.c.bf16 %v120417, %v115011
%120359 = vst [vmem:[%s280 + $0x2f8] sm:$0xf] /*vst_source=*/%v115014
%v115018 = vadd.s32 %v112249, %v3329
%v115028 = vadd.s32 %v115018, %v415
%vm115032 = vcmp.lt.u32.totalorder %v115028, %v115018
%vm115037 = vcmp.lt.u32.totalorder %v115018, %v3329
%v115042 = vadd.s32 %v112232, %v3316
%v115046 = vadd.s32 1, %v115042
%v115050 = vsel /*vm=*/%vm115037, /*on_true_vy=*/%v115046, /*on_false_vx=*/%v115042
%v115054 = vadd.s32 1, %v115050
%v115058 = vsel /*vm=*/%vm115032, /*on_true_vy=*/%v115054, /*on_false_vx=*/%v115050
%v115063 = vadd.s32 %v115058, %v10
%v115067 = vadd.s32 %v115028, %v9
%v115071 = vadd.s32 %v115067, %v115063
%v115073 = vshll.u32 %v115067, 13
%v115074 = vshrl.u32 %v115067, 19
%v115075 = vor.u32 %v115074, %v115073
%v115076 = vxor.u32 %v115075, %v115071
%v115079 = vadd.s32 %v115076, %v115071
%v115081 = vshll.u32 %v115076, 15
%v115082 = vshrl.u32 %v115076, 17
%v115083 = vor.u32 %v115082, %v115081
%v115084 = vxor.u32 %v115083, %v115079
%v115087 = vadd.s32 %v115084, %v115079
%v115089 = vshll.u32 %v115084, 26
%v115090 = vshrl.u32 %v115084, 6
%v115091 = vor.u32 %v115090, %v115089
%v115092 = vxor.u32 %v115091, %v115087
%v115095 = vadd.s32 %v115092, %v115087
%v115099 = vadd.s32 %v115095, %v9
%v115101 = vshll.u32 %v115092, 6
%v115102 = vshrl.u32 %v115092, 26
%v115103 = vor.u32 %v115102, %v115101
%v115104 = vxor.u32 %v115103, %v115095
%v115107 = vadd.s32 %v115104, %v8
%v115111 = vadd.s32 1, %v115107
%v115115 = vadd.s32 %v115111, %v115099
%v115117 = vshll.u32 %v115111, 17
%v115118 = vshrl.u32 %v115111, 15
%v115119 = vor.u32 %v115118, %v115117
%v115120 = vxor.u32 %v115119, %v115115
%v115123 = vadd.s32 %v115120, %v115115
%v115125 = vshll.u32 %v115120, 29
%v115126 = vshrl.u32 %v115120, 3
%v115127 = vor.u32 %v115126, %v115125
%v115128 = vxor.u32 %v115127, %v115123
%v115131 = vadd.s32 %v115128, %v115123
%v115133 = vshll.u32 %v115128, 16
%v115134 = vshrl.u32 %v115128, 16
%v115135 = vor.u32 %v115134, %v115133
%v115136 = vxor.u32 %v115135, %v115131
%v115139 = vadd.s32 %v115136, %v115131
%v115143 = vadd.s32 %v115139, %v8
%v115145 = vshll.u32 %v115136, 24
%v115146 = vshrl.u32 %v115136, 8
%v115147 = vor.u32 %v115146, %v115145
%v115148 = vxor.u32 %v115147, %v115139
%v115151 = vadd.s32 %v115148, %v10
%v115155 = vadd.s32 2, %v115151
%v115159 = vadd.s32 %v115155, %v115143
%v115161 = vshll.u32 %v115155, 13
%v115162 = vshrl.u32 %v115155, 19
%v115163 = vor.u32 %v115162, %v115161
%v115164 = vxor.u32 %v115163, %v115159
%v115167 = vadd.s32 %v115164, %v115159
%v115169 = vshll.u32 %v115164, 15
%v115170 = vshrl.u32 %v115164, 17
%v115171 = vor.u32 %v115170, %v115169
%v115172 = vxor.u32 %v115171, %v115167
%v115175 = vadd.s32 %v115172, %v115167
%v115177 = vshll.u32 %v115172, 26
%v115178 = vshrl.u32 %v115172, 6
%v115179 = vor.u32 %v115178, %v115177
%v115180 = vxor.u32 %v115179, %v115175
%v115183 = vadd.s32 %v115180, %v115175
%v115187 = vadd.s32 %v115183, %v10
%v115189 = vshll.u32 %v115180, 6
%v115190 = vshrl.u32 %v115180, 26
%v115191 = vor.u32 %v115190, %v115189
%v115192 = vxor.u32 %v115191, %v115183
%v115195 = vadd.s32 %v115192, %v9
%v115199 = vadd.s32 3, %v115195
%v115203 = vadd.s32 %v115199, %v115187
%v115205 = vshll.u32 %v115199, 17
%v115206 = vshrl.u32 %v115199, 15
%v115207 = vor.u32 %v115206, %v115205
%v115208 = vxor.u32 %v115207, %v115203
%v115211 = vadd.s32 %v115208, %v115203
%v115213 = vshll.u32 %v115208, 29
%v115214 = vshrl.u32 %v115208, 3
%v115215 = vor.u32 %v115214, %v115213
%v115216 = vxor.u32 %v115215, %v115211
%v115219 = vadd.s32 %v115216, %v115211
%v115221 = vshll.u32 %v115216, 16
%v115222 = vshrl.u32 %v115216, 16
%v115223 = vor.u32 %v115222, %v115221
%v115224 = vxor.u32 %v115223, %v115219
%v115227 = vadd.s32 %v115224, %v115219
%v115231 = vadd.s32 %v115227, %v9
%v115233 = vshll.u32 %v115224, 24
%v115234 = vshrl.u32 %v115224, 8
%v115235 = vor.u32 %v115234, %v115233
%v115236 = vxor.u32 %v115235, %v115227
%v115239 = vadd.s32 %v115236, %v8
%v115243 = vadd.s32 4, %v115239
%v115247 = vadd.s32 %v115243, %v115231
%v115249 = vshll.u32 %v115243, 13
%v115250 = vshrl.u32 %v115243, 19
%v115251 = vor.u32 %v115250, %v115249
%v115252 = vxor.u32 %v115251, %v115247
%v115255 = vadd.s32 %v115252, %v115247
%v115257 = vshll.u32 %v115252, 15
%v115258 = vshrl.u32 %v115252, 17
%v115259 = vor.u32 %v115258, %v115257
%v115260 = vxor.u32 %v115259, %v115255
%v115263 = vadd.s32 %v115260, %v115255
%v115265 = vshll.u32 %v115260, 26
%v115266 = vshrl.u32 %v115260, 6
%v115267 = vor.u32 %v115266, %v115265
%v115268 = vxor.u32 %v115267, %v115263
%v115271 = vadd.s32 %v115268, %v115263
%v115275 = vadd.s32 %v115271, %v8
%v115277 = vshll.u32 %v115268, 6
%v115278 = vshrl.u32 %v115268, 26
%v115279 = vor.u32 %v115278, %v115277
%v115280 = vxor.u32 %v115279, %v115271
%v115283 = vadd.s32 %v115280, %v10
%v115287 = vadd.s32 5, %v115283
%v115289 = vxor.u32 %v115287, %v115275
%v115290 = vand.u32.u8 255, %v115289
%v115291 = vand.u32 65535, %v115290
%v115292 = vshrl.u32 %v115291, 1
%v115293 = vor.u32 16256, %v115292
%v115294 = vand.u32.u16 65535, %v115293
%v120360 = vadd.low.f32.bf16 -1.0, %v115294
%v115303 = vmul.f32 2.0, %v120360
%v115307 = vadd.f32 -0.99609375, %v115303
%v115311 = vmax.f32 %v115307, -0.99609375
%v115313 = vand.u32 2147483647, %v115311
%vm115316 = vcmp.eq.f32.partialorder %v115313, 1.0
%v115321 = vmul.f32 inf, %v115311
%v115323 = vxor.u32 2147483648, %v115311
%v115326 = vmul.f32 %v115323, %v115311
%v115328 = vadd.f32 1.0, %v115326
%v115329 = vlog2.pop %v115328
%v115330 = vmul.f32 0.6931472, %v115329
%v115331 = vmul.f32 -0.5, %v115326
%v115332 = vadd.f32 1.0, %v115331
%v115333 = vmul.f32 %v115332, %v115326
%v115334 = vand.u32 2147483647, %v115326
%vm115335 = vcmp.lt.f32.partialorder %v115334, 0.0004427343
%v115336 = vsel /*vm=*/%vm115335, /*on_true_vy=*/%v115333, /*on_false_vx=*/%v115330
%v115337 = vxor.u32 2147483648, %v115336
%vm115340 = vcmp.lt.f32.partialorder %v115337, 5.0
%v115345 = vsel /*vm=*/%vm115340, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v115349 = vsel /*vm=*/%vm115340, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v115353 = vsel /*vm=*/%vm115340, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v115357 = vsel /*vm=*/%vm115340, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v115361 = vsel /*vm=*/%vm115340, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v115365 = vsel /*vm=*/%vm115340, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v115369 = vsel /*vm=*/%vm115340, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v115373 = vsel /*vm=*/%vm115340, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v115377 = vsel /*vm=*/%vm115340, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v115381 = vadd.f32 -2.5, %v115337
%v115383 = vrsqrt.pop %v115337
%v115384 = vmul.f32 %v115383, %v115337
%vm115385 = vcmp.eq.f32.partialorder %v115337, inf
%v115386 = vsel /*vm=*/%vm115385, /*on_true_vy=*/%v115337, /*on_false_vx=*/%v115384
%vm115387 = vcmp.eq.f32.partialorder %v115337, 0.0
%v115388 = vand.u32 2147483648, %v115337
%v115389 = vsel /*vm=*/%vm115387, /*on_true_vy=*/%v115388, /*on_false_vx=*/%v115386
%v115392 = vadd.f32 -3.0, %v115389
%v115396 = vsel /*vm=*/%vm115340, /*on_true_vy=*/%v115381, /*on_false_vx=*/%v115392
%v115400 = vmul.f32 %v115396, %v115377
%v115404 = vadd.f32 %v115400, %v115373
%v115408 = vmul.f32 %v115404, %v115396
%v115412 = vadd.f32 %v115408, %v115369
%v115416 = vmul.f32 %v115412, %v115396
%v115420 = vadd.f32 %v115416, %v115365
%v115424 = vmul.f32 %v115420, %v115396
%v115428 = vadd.f32 %v115424, %v115361
%v115432 = vmul.f32 %v115428, %v115396
%v115436 = vadd.f32 %v115432, %v115357
%v115440 = vmul.f32 %v115436, %v115396
%v115444 = vadd.f32 %v115440, %v115353
%v115448 = vmul.f32 %v115444, %v115396
%v115452 = vadd.f32 %v115448, %v115349
%v115456 = vmul.f32 %v115452, %v115396
%v115460 = vadd.f32 %v115456, %v115345
%v115464 = vmul.f32 %v115460, %v115311
%v115468 = vsel /*vm=*/%vm115316, /*on_true_vy=*/%v115321, /*on_false_vx=*/%v115464
%v115472 = vmul.f32 1.4140625, %v115468
%v115475 = vpack.c.bf16 %v120417, %v115472
%120361 = vst [vmem:[%s280 + $0x378] sm:$0xf] /*vst_source=*/%v115475
%v115479 = vadd.s32 %v112249, %v3816
%v115489 = vadd.s32 %v115479, %v415
%vm115493 = vcmp.lt.u32.totalorder %v115489, %v115479
%vm115498 = vcmp.lt.u32.totalorder %v115479, %v3816
%v115503 = vadd.s32 %v112232, %v3803
%v115507 = vadd.s32 1, %v115503
%v115511 = vsel /*vm=*/%vm115498, /*on_true_vy=*/%v115507, /*on_false_vx=*/%v115503
%v115515 = vadd.s32 1, %v115511
%v115519 = vsel /*vm=*/%vm115493, /*on_true_vy=*/%v115515, /*on_false_vx=*/%v115511
%v115524 = vadd.s32 %v115519, %v10
%v115528 = vadd.s32 %v115489, %v9
%v115532 = vadd.s32 %v115528, %v115524
%v115534 = vshll.u32 %v115528, 13
%v115535 = vshrl.u32 %v115528, 19
%v115536 = vor.u32 %v115535, %v115534
%v115537 = vxor.u32 %v115536, %v115532
%v115540 = vadd.s32 %v115537, %v115532
%v115542 = vshll.u32 %v115537, 15
%v115543 = vshrl.u32 %v115537, 17
%v115544 = vor.u32 %v115543, %v115542
%v115545 = vxor.u32 %v115544, %v115540
%v115548 = vadd.s32 %v115545, %v115540
%v115550 = vshll.u32 %v115545, 26
%v115551 = vshrl.u32 %v115545, 6
%v115552 = vor.u32 %v115551, %v115550
%v115553 = vxor.u32 %v115552, %v115548
%v115556 = vadd.s32 %v115553, %v115548
%v115560 = vadd.s32 %v115556, %v9
%v115562 = vshll.u32 %v115553, 6
%v115563 = vshrl.u32 %v115553, 26
%v115564 = vor.u32 %v115563, %v115562
%v115565 = vxor.u32 %v115564, %v115556
%v115568 = vadd.s32 %v115565, %v8
%v115572 = vadd.s32 1, %v115568
%v115576 = vadd.s32 %v115572, %v115560
%v115578 = vshll.u32 %v115572, 17
%v115579 = vshrl.u32 %v115572, 15
%v115580 = vor.u32 %v115579, %v115578
%v115581 = vxor.u32 %v115580, %v115576
%v115584 = vadd.s32 %v115581, %v115576
%v115586 = vshll.u32 %v115581, 29
%v115587 = vshrl.u32 %v115581, 3
%v115588 = vor.u32 %v115587, %v115586
%v115589 = vxor.u32 %v115588, %v115584
%v115592 = vadd.s32 %v115589, %v115584
%v115594 = vshll.u32 %v115589, 16
%v115595 = vshrl.u32 %v115589, 16
%v115596 = vor.u32 %v115595, %v115594
%v115597 = vxor.u32 %v115596, %v115592
%v115600 = vadd.s32 %v115597, %v115592
%v115604 = vadd.s32 %v115600, %v8
%v115606 = vshll.u32 %v115597, 24
%v115607 = vshrl.u32 %v115597, 8
%v115608 = vor.u32 %v115607, %v115606
%v115609 = vxor.u32 %v115608, %v115600
%v115612 = vadd.s32 %v115609, %v10
%v115616 = vadd.s32 2, %v115612
%v115620 = vadd.s32 %v115616, %v115604
%v115622 = vshll.u32 %v115616, 13
%v115623 = vshrl.u32 %v115616, 19
%v115624 = vor.u32 %v115623, %v115622
%v115625 = vxor.u32 %v115624, %v115620
%v115628 = vadd.s32 %v115625, %v115620
%v115630 = vshll.u32 %v115625, 15
%v115631 = vshrl.u32 %v115625, 17
%v115632 = vor.u32 %v115631, %v115630
%v115633 = vxor.u32 %v115632, %v115628
%v115636 = vadd.s32 %v115633, %v115628
%v115638 = vshll.u32 %v115633, 26
%v115639 = vshrl.u32 %v115633, 6
%v115640 = vor.u32 %v115639, %v115638
%v115641 = vxor.u32 %v115640, %v115636
%v115644 = vadd.s32 %v115641, %v115636
%v115648 = vadd.s32 %v115644, %v10
%v115650 = vshll.u32 %v115641, 6
%v115651 = vshrl.u32 %v115641, 26
%v115652 = vor.u32 %v115651, %v115650
%v115653 = vxor.u32 %v115652, %v115644
%v115656 = vadd.s32 %v115653, %v9
%v115660 = vadd.s32 3, %v115656
%v115664 = vadd.s32 %v115660, %v115648
%v115666 = vshll.u32 %v115660, 17
%v115667 = vshrl.u32 %v115660, 15
%v115668 = vor.u32 %v115667, %v115666
%v115669 = vxor.u32 %v115668, %v115664
%v115672 = vadd.s32 %v115669, %v115664
%v115674 = vshll.u32 %v115669, 29
%v115675 = vshrl.u32 %v115669, 3
%v115676 = vor.u32 %v115675, %v115674
%v115677 = vxor.u32 %v115676, %v115672
%v115680 = vadd.s32 %v115677, %v115672
%v115682 = vshll.u32 %v115677, 16
%v115683 = vshrl.u32 %v115677, 16
%v115684 = vor.u32 %v115683, %v115682
%v115685 = vxor.u32 %v115684, %v115680
%v115688 = vadd.s32 %v115685, %v115680
%v115692 = vadd.s32 %v115688, %v9
%v115694 = vshll.u32 %v115685, 24
%v115695 = vshrl.u32 %v115685, 8
%v115696 = vor.u32 %v115695, %v115694
%v115697 = vxor.u32 %v115696, %v115688
%v115700 = vadd.s32 %v115697, %v8
%v115704 = vadd.s32 4, %v115700
%v115708 = vadd.s32 %v115704, %v115692
%v115710 = vshll.u32 %v115704, 13
%v115711 = vshrl.u32 %v115704, 19
%v115712 = vor.u32 %v115711, %v115710
%v115713 = vxor.u32 %v115712, %v115708
%v115716 = vadd.s32 %v115713, %v115708
%v115718 = vshll.u32 %v115713, 15
%v115719 = vshrl.u32 %v115713, 17
%v115720 = vor.u32 %v115719, %v115718
%v115721 = vxor.u32 %v115720, %v115716
%v115724 = vadd.s32 %v115721, %v115716
%v115726 = vshll.u32 %v115721, 26
%v115727 = vshrl.u32 %v115721, 6
%v115728 = vor.u32 %v115727, %v115726
%v115729 = vxor.u32 %v115728, %v115724
%v115732 = vadd.s32 %v115729, %v115724
%v115736 = vadd.s32 %v115732, %v8
%v115738 = vshll.u32 %v115729, 6
%v115739 = vshrl.u32 %v115729, 26
%v115740 = vor.u32 %v115739, %v115738
%v115741 = vxor.u32 %v115740, %v115732
%v115744 = vadd.s32 %v115741, %v10
%v115748 = vadd.s32 5, %v115744
%v115750 = vxor.u32 %v115748, %v115736
%v115751 = vand.u32.u8 255, %v115750
%v115752 = vand.u32 65535, %v115751
%v115753 = vshrl.u32 %v115752, 1
%v115754 = vor.u32 16256, %v115753
%v115755 = vand.u32.u16 65535, %v115754
%v120362 = vadd.low.f32.bf16 -1.0, %v115755
%v115764 = vmul.f32 2.0, %v120362
%v115768 = vadd.f32 -0.99609375, %v115764
%v115772 = vmax.f32 %v115768, -0.99609375
%v115774 = vand.u32 2147483647, %v115772
%vm115777 = vcmp.eq.f32.partialorder %v115774, 1.0
%v115782 = vmul.f32 inf, %v115772
%v115784 = vxor.u32 2147483648, %v115772
%v115787 = vmul.f32 %v115784, %v115772
%v115789 = vadd.f32 1.0, %v115787
%v115790 = vlog2.pop %v115789
%v115791 = vmul.f32 0.6931472, %v115790
%v115792 = vmul.f32 -0.5, %v115787
%v115793 = vadd.f32 1.0, %v115792
%v115794 = vmul.f32 %v115793, %v115787
%v115795 = vand.u32 2147483647, %v115787
%vm115796 = vcmp.lt.f32.partialorder %v115795, 0.0004427343
%v115797 = vsel /*vm=*/%vm115796, /*on_true_vy=*/%v115794, /*on_false_vx=*/%v115791
%v115798 = vxor.u32 2147483648, %v115797
%vm115801 = vcmp.lt.f32.partialorder %v115798, 5.0
%v115806 = vsel /*vm=*/%vm115801, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v115810 = vsel /*vm=*/%vm115801, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v115814 = vsel /*vm=*/%vm115801, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v115818 = vsel /*vm=*/%vm115801, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v115822 = vsel /*vm=*/%vm115801, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v115826 = vsel /*vm=*/%vm115801, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v115830 = vsel /*vm=*/%vm115801, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v115834 = vsel /*vm=*/%vm115801, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v115838 = vsel /*vm=*/%vm115801, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v115842 = vadd.f32 -2.5, %v115798
%v115844 = vrsqrt.pop %v115798
%v115845 = vmul.f32 %v115844, %v115798
%vm115846 = vcmp.eq.f32.partialorder %v115798, inf
%v115847 = vsel /*vm=*/%vm115846, /*on_true_vy=*/%v115798, /*on_false_vx=*/%v115845
%vm115848 = vcmp.eq.f32.partialorder %v115798, 0.0
%v115849 = vand.u32 2147483648, %v115798
%v115850 = vsel /*vm=*/%vm115848, /*on_true_vy=*/%v115849, /*on_false_vx=*/%v115847
%v115853 = vadd.f32 -3.0, %v115850
%v115857 = vsel /*vm=*/%vm115801, /*on_true_vy=*/%v115842, /*on_false_vx=*/%v115853
%v115861 = vmul.f32 %v115857, %v115838
%v115865 = vadd.f32 %v115861, %v115834
%v115869 = vmul.f32 %v115865, %v115857
%v115873 = vadd.f32 %v115869, %v115830
%v115877 = vmul.f32 %v115873, %v115857
%v115881 = vadd.f32 %v115877, %v115826
%v115885 = vmul.f32 %v115881, %v115857
%v115889 = vadd.f32 %v115885, %v115822
%v115893 = vmul.f32 %v115889, %v115857
%v115897 = vadd.f32 %v115893, %v115818
%v115901 = vmul.f32 %v115897, %v115857
%v115905 = vadd.f32 %v115901, %v115814
%v115909 = vmul.f32 %v115905, %v115857
%v115913 = vadd.f32 %v115909, %v115810
%v115917 = vmul.f32 %v115913, %v115857
%v115921 = vadd.f32 %v115917, %v115806
%v115925 = vmul.f32 %v115921, %v115772
%v115929 = vsel /*vm=*/%vm115777, /*on_true_vy=*/%v115782, /*on_false_vx=*/%v115925
%v115933 = vmul.f32 1.4140625, %v115929
%v115936 = vpack.c.bf16 %v120417, %v115933
%120363 = vst [vmem:[%s280 + $0x3f8] sm:$0xf] /*vst_source=*/%v115936
%v115974 = vadd.s32 %v115971, %v408
%v115984 = vadd.s32 %v115974, %v415
%vm115988 = vcmp.lt.u32.totalorder %v115984, %v115974
%vm115993 = vcmp.lt.u32.totalorder %v115974, %v408
%v115998 = vadd.s32 %v115954, %v380
%v116002 = vadd.s32 1, %v115998
%v116006 = vsel /*vm=*/%vm115993, /*on_true_vy=*/%v116002, /*on_false_vx=*/%v115998
%v116010 = vadd.s32 1, %v116006
%v116014 = vsel /*vm=*/%vm115988, /*on_true_vy=*/%v116010, /*on_false_vx=*/%v116006
%v116019 = vadd.s32 %v116014, %v10
%v116023 = vadd.s32 %v115984, %v9
%v116027 = vadd.s32 %v116023, %v116019
%v116029 = vshll.u32 %v116023, 13
%v116030 = vshrl.u32 %v116023, 19
%v116031 = vor.u32 %v116030, %v116029
%v116032 = vxor.u32 %v116031, %v116027
%v116035 = vadd.s32 %v116032, %v116027
%v116037 = vshll.u32 %v116032, 15
%v116038 = vshrl.u32 %v116032, 17
%v116039 = vor.u32 %v116038, %v116037
%v116040 = vxor.u32 %v116039, %v116035
%v116043 = vadd.s32 %v116040, %v116035
%v116045 = vshll.u32 %v116040, 26
%v116046 = vshrl.u32 %v116040, 6
%v116047 = vor.u32 %v116046, %v116045
%v116048 = vxor.u32 %v116047, %v116043
%v116051 = vadd.s32 %v116048, %v116043
%v116055 = vadd.s32 %v116051, %v9
%v116057 = vshll.u32 %v116048, 6
%v116058 = vshrl.u32 %v116048, 26
%v116059 = vor.u32 %v116058, %v116057
%v116060 = vxor.u32 %v116059, %v116051
%v116063 = vadd.s32 %v116060, %v8
%v116067 = vadd.s32 1, %v116063
%v116071 = vadd.s32 %v116067, %v116055
%v116073 = vshll.u32 %v116067, 17
%v116074 = vshrl.u32 %v116067, 15
%v116075 = vor.u32 %v116074, %v116073
%v116076 = vxor.u32 %v116075, %v116071
%v116079 = vadd.s32 %v116076, %v116071
%v116081 = vshll.u32 %v116076, 29
%v116082 = vshrl.u32 %v116076, 3
%v116083 = vor.u32 %v116082, %v116081
%v116084 = vxor.u32 %v116083, %v116079
%v116087 = vadd.s32 %v116084, %v116079
%v116089 = vshll.u32 %v116084, 16
%v116090 = vshrl.u32 %v116084, 16
%v116091 = vor.u32 %v116090, %v116089
%v116092 = vxor.u32 %v116091, %v116087
%v116095 = vadd.s32 %v116092, %v116087
%v116099 = vadd.s32 %v116095, %v8
%v116101 = vshll.u32 %v116092, 24
%v116102 = vshrl.u32 %v116092, 8
%v116103 = vor.u32 %v116102, %v116101
%v116104 = vxor.u32 %v116103, %v116095
%v116107 = vadd.s32 %v116104, %v10
%v116111 = vadd.s32 2, %v116107
%v116115 = vadd.s32 %v116111, %v116099
%v116117 = vshll.u32 %v116111, 13
%v116118 = vshrl.u32 %v116111, 19
%v116119 = vor.u32 %v116118, %v116117
%v116120 = vxor.u32 %v116119, %v116115
%v116123 = vadd.s32 %v116120, %v116115
%v116125 = vshll.u32 %v116120, 15
%v116126 = vshrl.u32 %v116120, 17
%v116127 = vor.u32 %v116126, %v116125
%v116128 = vxor.u32 %v116127, %v116123
%v116131 = vadd.s32 %v116128, %v116123
%v116133 = vshll.u32 %v116128, 26
%v116134 = vshrl.u32 %v116128, 6
%v116135 = vor.u32 %v116134, %v116133
%v116136 = vxor.u32 %v116135, %v116131
%v116139 = vadd.s32 %v116136, %v116131
%v116143 = vadd.s32 %v116139, %v10
%v116145 = vshll.u32 %v116136, 6
%v116146 = vshrl.u32 %v116136, 26
%v116147 = vor.u32 %v116146, %v116145
%v116148 = vxor.u32 %v116147, %v116139
%v116151 = vadd.s32 %v116148, %v9
%v116155 = vadd.s32 3, %v116151
%v116159 = vadd.s32 %v116155, %v116143
%v116161 = vshll.u32 %v116155, 17
%v116162 = vshrl.u32 %v116155, 15
%v116163 = vor.u32 %v116162, %v116161
%v116164 = vxor.u32 %v116163, %v116159
%v116167 = vadd.s32 %v116164, %v116159
%v116169 = vshll.u32 %v116164, 29
%v116170 = vshrl.u32 %v116164, 3
%v116171 = vor.u32 %v116170, %v116169
%v116172 = vxor.u32 %v116171, %v116167
%v116175 = vadd.s32 %v116172, %v116167
%v116177 = vshll.u32 %v116172, 16
%v116178 = vshrl.u32 %v116172, 16
%v116179 = vor.u32 %v116178, %v116177
%v116180 = vxor.u32 %v116179, %v116175
%v116183 = vadd.s32 %v116180, %v116175
%v116187 = vadd.s32 %v116183, %v9
%v116189 = vshll.u32 %v116180, 24
%v116190 = vshrl.u32 %v116180, 8
%v116191 = vor.u32 %v116190, %v116189
%v116192 = vxor.u32 %v116191, %v116183
%v116195 = vadd.s32 %v116192, %v8
%v116199 = vadd.s32 4, %v116195
%v116203 = vadd.s32 %v116199, %v116187
%v116205 = vshll.u32 %v116199, 13
%v116206 = vshrl.u32 %v116199, 19
%v116207 = vor.u32 %v116206, %v116205
%v116208 = vxor.u32 %v116207, %v116203
%v116211 = vadd.s32 %v116208, %v116203
%v116213 = vshll.u32 %v116208, 15
%v116214 = vshrl.u32 %v116208, 17
%v116215 = vor.u32 %v116214, %v116213
%v116216 = vxor.u32 %v116215, %v116211
%v116219 = vadd.s32 %v116216, %v116211
%v116221 = vshll.u32 %v116216, 26
%v116222 = vshrl.u32 %v116216, 6
%v116223 = vor.u32 %v116222, %v116221
%v116224 = vxor.u32 %v116223, %v116219
%v116227 = vadd.s32 %v116224, %v116219
%v116231 = vadd.s32 %v116227, %v8
%v116233 = vshll.u32 %v116224, 6
%v116234 = vshrl.u32 %v116224, 26
%v116235 = vor.u32 %v116234, %v116233
%v116236 = vxor.u32 %v116235, %v116227
%v116239 = vadd.s32 %v116236, %v10
%v116243 = vadd.s32 5, %v116239
%v116245 = vxor.u32 %v116243, %v116231
%v116246 = vand.u32.u8 255, %v116245
%v116247 = vand.u32 65535, %v116246
%v116248 = vshrl.u32 %v116247, 1
%v116249 = vor.u32 16256, %v116248
%v116250 = vand.u32.u16 65535, %v116249
%v120368 = vadd.low.f32.bf16 -1.0, %v116250
%v116259 = vmul.f32 2.0, %v120368
%v116263 = vadd.f32 -0.99609375, %v116259
%v116267 = vmax.f32 %v116263, -0.99609375
%v116269 = vand.u32 2147483647, %v116267
%vm116272 = vcmp.eq.f32.partialorder %v116269, 1.0
%v116277 = vmul.f32 inf, %v116267
%v116279 = vxor.u32 2147483648, %v116267
%v116282 = vmul.f32 %v116279, %v116267
%v116284 = vadd.f32 1.0, %v116282
%v116285 = vlog2.pop %v116284
%v116286 = vmul.f32 0.6931472, %v116285
%v116287 = vmul.f32 -0.5, %v116282
%v116288 = vadd.f32 1.0, %v116287
%v116289 = vmul.f32 %v116288, %v116282
%v116290 = vand.u32 2147483647, %v116282
%vm116291 = vcmp.lt.f32.partialorder %v116290, 0.0004427343
%v116292 = vsel /*vm=*/%vm116291, /*on_true_vy=*/%v116289, /*on_false_vx=*/%v116286
%v116293 = vxor.u32 2147483648, %v116292
%vm116296 = vcmp.lt.f32.partialorder %v116293, 5.0
%v116301 = vsel /*vm=*/%vm116296, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v116305 = vsel /*vm=*/%vm116296, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v116309 = vsel /*vm=*/%vm116296, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v116313 = vsel /*vm=*/%vm116296, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v116317 = vsel /*vm=*/%vm116296, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v116321 = vsel /*vm=*/%vm116296, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v116325 = vsel /*vm=*/%vm116296, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v116329 = vsel /*vm=*/%vm116296, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v116333 = vsel /*vm=*/%vm116296, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v116337 = vadd.f32 -2.5, %v116293
%v116339 = vrsqrt.pop %v116293
%v116340 = vmul.f32 %v116339, %v116293
%vm116341 = vcmp.eq.f32.partialorder %v116293, inf
%v116342 = vsel /*vm=*/%vm116341, /*on_true_vy=*/%v116293, /*on_false_vx=*/%v116340
%vm116343 = vcmp.eq.f32.partialorder %v116293, 0.0
%v116344 = vand.u32 2147483648, %v116293
%v116345 = vsel /*vm=*/%vm116343, /*on_true_vy=*/%v116344, /*on_false_vx=*/%v116342
%v116348 = vadd.f32 -3.0, %v116345
%v116352 = vsel /*vm=*/%vm116296, /*on_true_vy=*/%v116337, /*on_false_vx=*/%v116348
%v116356 = vmul.f32 %v116352, %v116333
%v116360 = vadd.f32 %v116356, %v116329
%v116364 = vmul.f32 %v116360, %v116352
%v116368 = vadd.f32 %v116364, %v116325
%v116372 = vmul.f32 %v116368, %v116352
%v116376 = vadd.f32 %v116372, %v116321
%v116380 = vmul.f32 %v116376, %v116352
%v116384 = vadd.f32 %v116380, %v116317
%v116388 = vmul.f32 %v116384, %v116352
%v116392 = vadd.f32 %v116388, %v116313
%v116396 = vmul.f32 %v116392, %v116352
%v116400 = vadd.f32 %v116396, %v116309
%v116404 = vmul.f32 %v116400, %v116352
%v116408 = vadd.f32 %v116404, %v116305
%v116412 = vmul.f32 %v116408, %v116352
%v116416 = vadd.f32 %v116412, %v116301
%v116420 = vmul.f32 %v116416, %v116267
%v116424 = vsel /*vm=*/%vm116272, /*on_true_vy=*/%v116277, /*on_false_vx=*/%v116420
%v116428 = vmul.f32 1.4140625, %v116424
%v116431 = vpack.c.bf16 %v120417, %v116428
%120369 = vst [vmem:[%s280 + $0x7c] sm:$0xf] /*vst_source=*/%v116431
%v116435 = vadd.s32 %v115971, %v894
%v116445 = vadd.s32 %v116435, %v415
%vm116449 = vcmp.lt.u32.totalorder %v116445, %v116435
%vm116454 = vcmp.lt.u32.totalorder %v116435, %v894
%v116459 = vadd.s32 %v115954, %v881
%v116463 = vadd.s32 1, %v116459
%v116467 = vsel /*vm=*/%vm116454, /*on_true_vy=*/%v116463, /*on_false_vx=*/%v116459
%v116471 = vadd.s32 1, %v116467
%v116475 = vsel /*vm=*/%vm116449, /*on_true_vy=*/%v116471, /*on_false_vx=*/%v116467
%v116480 = vadd.s32 %v116475, %v10
%v116484 = vadd.s32 %v116445, %v9
%v116488 = vadd.s32 %v116484, %v116480
%v116490 = vshll.u32 %v116484, 13
%v116491 = vshrl.u32 %v116484, 19
%v116492 = vor.u32 %v116491, %v116490
%v116493 = vxor.u32 %v116492, %v116488
%v116496 = vadd.s32 %v116493, %v116488
%v116498 = vshll.u32 %v116493, 15
%v116499 = vshrl.u32 %v116493, 17
%v116500 = vor.u32 %v116499, %v116498
%v116501 = vxor.u32 %v116500, %v116496
%v116504 = vadd.s32 %v116501, %v116496
%v116506 = vshll.u32 %v116501, 26
%v116507 = vshrl.u32 %v116501, 6
%v116508 = vor.u32 %v116507, %v116506
%v116509 = vxor.u32 %v116508, %v116504
%v116512 = vadd.s32 %v116509, %v116504
%v116516 = vadd.s32 %v116512, %v9
%v116518 = vshll.u32 %v116509, 6
%v116519 = vshrl.u32 %v116509, 26
%v116520 = vor.u32 %v116519, %v116518
%v116521 = vxor.u32 %v116520, %v116512
%v116524 = vadd.s32 %v116521, %v8
%v116528 = vadd.s32 1, %v116524
%v116532 = vadd.s32 %v116528, %v116516
%v116534 = vshll.u32 %v116528, 17
%v116535 = vshrl.u32 %v116528, 15
%v116536 = vor.u32 %v116535, %v116534
%v116537 = vxor.u32 %v116536, %v116532
%v116540 = vadd.s32 %v116537, %v116532
%v116542 = vshll.u32 %v116537, 29
%v116543 = vshrl.u32 %v116537, 3
%v116544 = vor.u32 %v116543, %v116542
%v116545 = vxor.u32 %v116544, %v116540
%v116548 = vadd.s32 %v116545, %v116540
%v116550 = vshll.u32 %v116545, 16
%v116551 = vshrl.u32 %v116545, 16
%v116552 = vor.u32 %v116551, %v116550
%v116553 = vxor.u32 %v116552, %v116548
%v116556 = vadd.s32 %v116553, %v116548
%v116560 = vadd.s32 %v116556, %v8
%v116562 = vshll.u32 %v116553, 24
%v116563 = vshrl.u32 %v116553, 8
%v116564 = vor.u32 %v116563, %v116562
%v116565 = vxor.u32 %v116564, %v116556
%v116568 = vadd.s32 %v116565, %v10
%v116572 = vadd.s32 2, %v116568
%v116576 = vadd.s32 %v116572, %v116560
%v116578 = vshll.u32 %v116572, 13
%v116579 = vshrl.u32 %v116572, 19
%v116580 = vor.u32 %v116579, %v116578
%v116581 = vxor.u32 %v116580, %v116576
%v116584 = vadd.s32 %v116581, %v116576
%v116586 = vshll.u32 %v116581, 15
%v116587 = vshrl.u32 %v116581, 17
%v116588 = vor.u32 %v116587, %v116586
%v116589 = vxor.u32 %v116588, %v116584
%v116592 = vadd.s32 %v116589, %v116584
%v116594 = vshll.u32 %v116589, 26
%v116595 = vshrl.u32 %v116589, 6
%v116596 = vor.u32 %v116595, %v116594
%v116597 = vxor.u32 %v116596, %v116592
%v116600 = vadd.s32 %v116597, %v116592
%v116604 = vadd.s32 %v116600, %v10
%v116606 = vshll.u32 %v116597, 6
%v116607 = vshrl.u32 %v116597, 26
%v116608 = vor.u32 %v116607, %v116606
%v116609 = vxor.u32 %v116608, %v116600
%v116612 = vadd.s32 %v116609, %v9
%v116616 = vadd.s32 3, %v116612
%v116620 = vadd.s32 %v116616, %v116604
%v116622 = vshll.u32 %v116616, 17
%v116623 = vshrl.u32 %v116616, 15
%v116624 = vor.u32 %v116623, %v116622
%v116625 = vxor.u32 %v116624, %v116620
%v116628 = vadd.s32 %v116625, %v116620
%v116630 = vshll.u32 %v116625, 29
%v116631 = vshrl.u32 %v116625, 3
%v116632 = vor.u32 %v116631, %v116630
%v116633 = vxor.u32 %v116632, %v116628
%v116636 = vadd.s32 %v116633, %v116628
%v116638 = vshll.u32 %v116633, 16
%v116639 = vshrl.u32 %v116633, 16
%v116640 = vor.u32 %v116639, %v116638
%v116641 = vxor.u32 %v116640, %v116636
%v116644 = vadd.s32 %v116641, %v116636
%v116648 = vadd.s32 %v116644, %v9
%v116650 = vshll.u32 %v116641, 24
%v116651 = vshrl.u32 %v116641, 8
%v116652 = vor.u32 %v116651, %v116650
%v116653 = vxor.u32 %v116652, %v116644
%v116656 = vadd.s32 %v116653, %v8
%v116660 = vadd.s32 4, %v116656
%v116664 = vadd.s32 %v116660, %v116648
%v116666 = vshll.u32 %v116660, 13
%v116667 = vshrl.u32 %v116660, 19
%v116668 = vor.u32 %v116667, %v116666
%v116669 = vxor.u32 %v116668, %v116664
%v116672 = vadd.s32 %v116669, %v116664
%v116674 = vshll.u32 %v116669, 15
%v116675 = vshrl.u32 %v116669, 17
%v116676 = vor.u32 %v116675, %v116674
%v116677 = vxor.u32 %v116676, %v116672
%v116680 = vadd.s32 %v116677, %v116672
%v116682 = vshll.u32 %v116677, 26
%v116683 = vshrl.u32 %v116677, 6
%v116684 = vor.u32 %v116683, %v116682
%v116685 = vxor.u32 %v116684, %v116680
%v116688 = vadd.s32 %v116685, %v116680
%v116692 = vadd.s32 %v116688, %v8
%v116694 = vshll.u32 %v116685, 6
%v116695 = vshrl.u32 %v116685, 26
%v116696 = vor.u32 %v116695, %v116694
%v116697 = vxor.u32 %v116696, %v116688
%v116700 = vadd.s32 %v116697, %v10
%v116704 = vadd.s32 5, %v116700
%v116706 = vxor.u32 %v116704, %v116692
%v116707 = vand.u32.u8 255, %v116706
%v116708 = vand.u32 65535, %v116707
%v116709 = vshrl.u32 %v116708, 1
%v116710 = vor.u32 16256, %v116709
%v116711 = vand.u32.u16 65535, %v116710
%v120370 = vadd.low.f32.bf16 -1.0, %v116711
%v116720 = vmul.f32 2.0, %v120370
%v116724 = vadd.f32 -0.99609375, %v116720
%v116728 = vmax.f32 %v116724, -0.99609375
%v116730 = vand.u32 2147483647, %v116728
%vm116733 = vcmp.eq.f32.partialorder %v116730, 1.0
%v116738 = vmul.f32 inf, %v116728
%v116740 = vxor.u32 2147483648, %v116728
%v116743 = vmul.f32 %v116740, %v116728
%v116745 = vadd.f32 1.0, %v116743
%v116746 = vlog2.pop %v116745
%v116747 = vmul.f32 0.6931472, %v116746
%v116748 = vmul.f32 -0.5, %v116743
%v116749 = vadd.f32 1.0, %v116748
%v116750 = vmul.f32 %v116749, %v116743
%v116751 = vand.u32 2147483647, %v116743
%vm116752 = vcmp.lt.f32.partialorder %v116751, 0.0004427343
%v116753 = vsel /*vm=*/%vm116752, /*on_true_vy=*/%v116750, /*on_false_vx=*/%v116747
%v116754 = vxor.u32 2147483648, %v116753
%vm116757 = vcmp.lt.f32.partialorder %v116754, 5.0
%v116762 = vsel /*vm=*/%vm116757, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v116766 = vsel /*vm=*/%vm116757, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v116770 = vsel /*vm=*/%vm116757, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v116774 = vsel /*vm=*/%vm116757, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v116778 = vsel /*vm=*/%vm116757, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v116782 = vsel /*vm=*/%vm116757, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v116786 = vsel /*vm=*/%vm116757, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v116790 = vsel /*vm=*/%vm116757, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v116794 = vsel /*vm=*/%vm116757, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v116798 = vadd.f32 -2.5, %v116754
%v116800 = vrsqrt.pop %v116754
%v116801 = vmul.f32 %v116800, %v116754
%vm116802 = vcmp.eq.f32.partialorder %v116754, inf
%v116803 = vsel /*vm=*/%vm116802, /*on_true_vy=*/%v116754, /*on_false_vx=*/%v116801
%vm116804 = vcmp.eq.f32.partialorder %v116754, 0.0
%v116805 = vand.u32 2147483648, %v116754
%v116806 = vsel /*vm=*/%vm116804, /*on_true_vy=*/%v116805, /*on_false_vx=*/%v116803
%v116809 = vadd.f32 -3.0, %v116806
%v116813 = vsel /*vm=*/%vm116757, /*on_true_vy=*/%v116798, /*on_false_vx=*/%v116809
%v116817 = vmul.f32 %v116813, %v116794
%v116821 = vadd.f32 %v116817, %v116790
%v116825 = vmul.f32 %v116821, %v116813
%v116829 = vadd.f32 %v116825, %v116786
%v116833 = vmul.f32 %v116829, %v116813
%v116837 = vadd.f32 %v116833, %v116782
%v116841 = vmul.f32 %v116837, %v116813
%v116845 = vadd.f32 %v116841, %v116778
%v116849 = vmul.f32 %v116845, %v116813
%v116853 = vadd.f32 %v116849, %v116774
%v116857 = vmul.f32 %v116853, %v116813
%v116861 = vadd.f32 %v116857, %v116770
%v116865 = vmul.f32 %v116861, %v116813
%v116869 = vadd.f32 %v116865, %v116766
%v116873 = vmul.f32 %v116869, %v116813
%v116877 = vadd.f32 %v116873, %v116762
%v116881 = vmul.f32 %v116877, %v116728
%v116885 = vsel /*vm=*/%vm116733, /*on_true_vy=*/%v116738, /*on_false_vx=*/%v116881
%v116889 = vmul.f32 1.4140625, %v116885
%v116892 = vpack.c.bf16 %v120417, %v116889
%120371 = vst [vmem:[%s280 + $0xfc] sm:$0xf] /*vst_source=*/%v116892
%v116896 = vadd.s32 %v115971, %v1381
%v116906 = vadd.s32 %v116896, %v415
%vm116910 = vcmp.lt.u32.totalorder %v116906, %v116896
%vm116915 = vcmp.lt.u32.totalorder %v116896, %v1381
%v116920 = vadd.s32 %v115954, %v1368
%v116924 = vadd.s32 1, %v116920
%v116928 = vsel /*vm=*/%vm116915, /*on_true_vy=*/%v116924, /*on_false_vx=*/%v116920
%v116932 = vadd.s32 1, %v116928
%v116936 = vsel /*vm=*/%vm116910, /*on_true_vy=*/%v116932, /*on_false_vx=*/%v116928
%v116941 = vadd.s32 %v116936, %v10
%v116945 = vadd.s32 %v116906, %v9
%v116949 = vadd.s32 %v116945, %v116941
%v116951 = vshll.u32 %v116945, 13
%v116952 = vshrl.u32 %v116945, 19
%v116953 = vor.u32 %v116952, %v116951
%v116954 = vxor.u32 %v116953, %v116949
%v116957 = vadd.s32 %v116954, %v116949
%v116959 = vshll.u32 %v116954, 15
%v116960 = vshrl.u32 %v116954, 17
%v116961 = vor.u32 %v116960, %v116959
%v116962 = vxor.u32 %v116961, %v116957
%v116965 = vadd.s32 %v116962, %v116957
%v116967 = vshll.u32 %v116962, 26
%v116968 = vshrl.u32 %v116962, 6
%v116969 = vor.u32 %v116968, %v116967
%v116970 = vxor.u32 %v116969, %v116965
%v116973 = vadd.s32 %v116970, %v116965
%v116977 = vadd.s32 %v116973, %v9
%v116979 = vshll.u32 %v116970, 6
%v116980 = vshrl.u32 %v116970, 26
%v116981 = vor.u32 %v116980, %v116979
%v116982 = vxor.u32 %v116981, %v116973
%v116985 = vadd.s32 %v116982, %v8
%v116989 = vadd.s32 1, %v116985
%v116993 = vadd.s32 %v116989, %v116977
%v116995 = vshll.u32 %v116989, 17
%v116996 = vshrl.u32 %v116989, 15
%v116997 = vor.u32 %v116996, %v116995
%v116998 = vxor.u32 %v116997, %v116993
%v117001 = vadd.s32 %v116998, %v116993
%v117003 = vshll.u32 %v116998, 29
%v117004 = vshrl.u32 %v116998, 3
%v117005 = vor.u32 %v117004, %v117003
%v117006 = vxor.u32 %v117005, %v117001
%v117009 = vadd.s32 %v117006, %v117001
%v117011 = vshll.u32 %v117006, 16
%v117012 = vshrl.u32 %v117006, 16
%v117013 = vor.u32 %v117012, %v117011
%v117014 = vxor.u32 %v117013, %v117009
%v117017 = vadd.s32 %v117014, %v117009
%v117021 = vadd.s32 %v117017, %v8
%v117023 = vshll.u32 %v117014, 24
%v117024 = vshrl.u32 %v117014, 8
%v117025 = vor.u32 %v117024, %v117023
%v117026 = vxor.u32 %v117025, %v117017
%v117029 = vadd.s32 %v117026, %v10
%v117033 = vadd.s32 2, %v117029
%v117037 = vadd.s32 %v117033, %v117021
%v117039 = vshll.u32 %v117033, 13
%v117040 = vshrl.u32 %v117033, 19
%v117041 = vor.u32 %v117040, %v117039
%v117042 = vxor.u32 %v117041, %v117037
%v117045 = vadd.s32 %v117042, %v117037
%v117047 = vshll.u32 %v117042, 15
%v117048 = vshrl.u32 %v117042, 17
%v117049 = vor.u32 %v117048, %v117047
%v117050 = vxor.u32 %v117049, %v117045
%v117053 = vadd.s32 %v117050, %v117045
%v117055 = vshll.u32 %v117050, 26
%v117056 = vshrl.u32 %v117050, 6
%v117057 = vor.u32 %v117056, %v117055
%v117058 = vxor.u32 %v117057, %v117053
%v117061 = vadd.s32 %v117058, %v117053
%v117065 = vadd.s32 %v117061, %v10
%v117067 = vshll.u32 %v117058, 6
%v117068 = vshrl.u32 %v117058, 26
%v117069 = vor.u32 %v117068, %v117067
%v117070 = vxor.u32 %v117069, %v117061
%v117073 = vadd.s32 %v117070, %v9
%v117077 = vadd.s32 3, %v117073
%v117081 = vadd.s32 %v117077, %v117065
%v117083 = vshll.u32 %v117077, 17
%v117084 = vshrl.u32 %v117077, 15
%v117085 = vor.u32 %v117084, %v117083
%v117086 = vxor.u32 %v117085, %v117081
%v117089 = vadd.s32 %v117086, %v117081
%v117091 = vshll.u32 %v117086, 29
%v117092 = vshrl.u32 %v117086, 3
%v117093 = vor.u32 %v117092, %v117091
%v117094 = vxor.u32 %v117093, %v117089
%v117097 = vadd.s32 %v117094, %v117089
%v117099 = vshll.u32 %v117094, 16
%v117100 = vshrl.u32 %v117094, 16
%v117101 = vor.u32 %v117100, %v117099
%v117102 = vxor.u32 %v117101, %v117097
%v117105 = vadd.s32 %v117102, %v117097
%v117109 = vadd.s32 %v117105, %v9
%v117111 = vshll.u32 %v117102, 24
%v117112 = vshrl.u32 %v117102, 8
%v117113 = vor.u32 %v117112, %v117111
%v117114 = vxor.u32 %v117113, %v117105
%v117117 = vadd.s32 %v117114, %v8
%v117121 = vadd.s32 4, %v117117
%v117125 = vadd.s32 %v117121, %v117109
%v117127 = vshll.u32 %v117121, 13
%v117128 = vshrl.u32 %v117121, 19
%v117129 = vor.u32 %v117128, %v117127
%v117130 = vxor.u32 %v117129, %v117125
%v117133 = vadd.s32 %v117130, %v117125
%v117135 = vshll.u32 %v117130, 15
%v117136 = vshrl.u32 %v117130, 17
%v117137 = vor.u32 %v117136, %v117135
%v117138 = vxor.u32 %v117137, %v117133
%v117141 = vadd.s32 %v117138, %v117133
%v117143 = vshll.u32 %v117138, 26
%v117144 = vshrl.u32 %v117138, 6
%v117145 = vor.u32 %v117144, %v117143
%v117146 = vxor.u32 %v117145, %v117141
%v117149 = vadd.s32 %v117146, %v117141
%v117153 = vadd.s32 %v117149, %v8
%v117155 = vshll.u32 %v117146, 6
%v117156 = vshrl.u32 %v117146, 26
%v117157 = vor.u32 %v117156, %v117155
%v117158 = vxor.u32 %v117157, %v117149
%v117161 = vadd.s32 %v117158, %v10
%v117165 = vadd.s32 5, %v117161
%v117167 = vxor.u32 %v117165, %v117153
%v117168 = vand.u32.u8 255, %v117167
%v117169 = vand.u32 65535, %v117168
%v117170 = vshrl.u32 %v117169, 1
%v117171 = vor.u32 16256, %v117170
%v117172 = vand.u32.u16 65535, %v117171
%v120372 = vadd.low.f32.bf16 -1.0, %v117172
%v117181 = vmul.f32 2.0, %v120372
%v117185 = vadd.f32 -0.99609375, %v117181
%v117189 = vmax.f32 %v117185, -0.99609375
%v117191 = vand.u32 2147483647, %v117189
%vm117194 = vcmp.eq.f32.partialorder %v117191, 1.0
%v117199 = vmul.f32 inf, %v117189
%v117201 = vxor.u32 2147483648, %v117189
%v117204 = vmul.f32 %v117201, %v117189
%v117206 = vadd.f32 1.0, %v117204
%v117207 = vlog2.pop %v117206
%v117208 = vmul.f32 0.6931472, %v117207
%v117209 = vmul.f32 -0.5, %v117204
%v117210 = vadd.f32 1.0, %v117209
%v117211 = vmul.f32 %v117210, %v117204
%v117212 = vand.u32 2147483647, %v117204
%vm117213 = vcmp.lt.f32.partialorder %v117212, 0.0004427343
%v117214 = vsel /*vm=*/%vm117213, /*on_true_vy=*/%v117211, /*on_false_vx=*/%v117208
%v117215 = vxor.u32 2147483648, %v117214
%vm117218 = vcmp.lt.f32.partialorder %v117215, 5.0
%v117223 = vsel /*vm=*/%vm117218, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v117227 = vsel /*vm=*/%vm117218, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v117231 = vsel /*vm=*/%vm117218, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v117235 = vsel /*vm=*/%vm117218, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v117239 = vsel /*vm=*/%vm117218, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v117243 = vsel /*vm=*/%vm117218, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v117247 = vsel /*vm=*/%vm117218, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v117251 = vsel /*vm=*/%vm117218, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v117255 = vsel /*vm=*/%vm117218, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v117259 = vadd.f32 -2.5, %v117215
%v117261 = vrsqrt.pop %v117215
%v117262 = vmul.f32 %v117261, %v117215
%vm117263 = vcmp.eq.f32.partialorder %v117215, inf
%v117264 = vsel /*vm=*/%vm117263, /*on_true_vy=*/%v117215, /*on_false_vx=*/%v117262
%vm117265 = vcmp.eq.f32.partialorder %v117215, 0.0
%v117266 = vand.u32 2147483648, %v117215
%v117267 = vsel /*vm=*/%vm117265, /*on_true_vy=*/%v117266, /*on_false_vx=*/%v117264
%v117270 = vadd.f32 -3.0, %v117267
%v117274 = vsel /*vm=*/%vm117218, /*on_true_vy=*/%v117259, /*on_false_vx=*/%v117270
%v117278 = vmul.f32 %v117274, %v117255
%v117282 = vadd.f32 %v117278, %v117251
%v117286 = vmul.f32 %v117282, %v117274
%v117290 = vadd.f32 %v117286, %v117247
%v117294 = vmul.f32 %v117290, %v117274
%v117298 = vadd.f32 %v117294, %v117243
%v117302 = vmul.f32 %v117298, %v117274
%v117306 = vadd.f32 %v117302, %v117239
%v117310 = vmul.f32 %v117306, %v117274
%v117314 = vadd.f32 %v117310, %v117235
%v117318 = vmul.f32 %v117314, %v117274
%v117322 = vadd.f32 %v117318, %v117231
%v117326 = vmul.f32 %v117322, %v117274
%v117330 = vadd.f32 %v117326, %v117227
%v117334 = vmul.f32 %v117330, %v117274
%v117338 = vadd.f32 %v117334, %v117223
%v117342 = vmul.f32 %v117338, %v117189
%v117346 = vsel /*vm=*/%vm117194, /*on_true_vy=*/%v117199, /*on_false_vx=*/%v117342
%v117350 = vmul.f32 1.4140625, %v117346
%v117353 = vpack.c.bf16 %v120417, %v117350
%120373 = vst [vmem:[%s280 + $0x17c] sm:$0xf] /*vst_source=*/%v117353
%v117357 = vadd.s32 %v115971, %v1868
%v117367 = vadd.s32 %v117357, %v415
%vm117371 = vcmp.lt.u32.totalorder %v117367, %v117357
%vm117376 = vcmp.lt.u32.totalorder %v117357, %v1868
%v117381 = vadd.s32 %v115954, %v1855
%v117385 = vadd.s32 1, %v117381
%v117389 = vsel /*vm=*/%vm117376, /*on_true_vy=*/%v117385, /*on_false_vx=*/%v117381
%v117393 = vadd.s32 1, %v117389
%v117397 = vsel /*vm=*/%vm117371, /*on_true_vy=*/%v117393, /*on_false_vx=*/%v117389
%v117402 = vadd.s32 %v117397, %v10
%v117406 = vadd.s32 %v117367, %v9
%v117410 = vadd.s32 %v117406, %v117402
%v117412 = vshll.u32 %v117406, 13
%v117413 = vshrl.u32 %v117406, 19
%v117414 = vor.u32 %v117413, %v117412
%v117415 = vxor.u32 %v117414, %v117410
%v117418 = vadd.s32 %v117415, %v117410
%v117420 = vshll.u32 %v117415, 15
%v117421 = vshrl.u32 %v117415, 17
%v117422 = vor.u32 %v117421, %v117420
%v117423 = vxor.u32 %v117422, %v117418
%v117426 = vadd.s32 %v117423, %v117418
%v117428 = vshll.u32 %v117423, 26
%v117429 = vshrl.u32 %v117423, 6
%v117430 = vor.u32 %v117429, %v117428
%v117431 = vxor.u32 %v117430, %v117426
%v117434 = vadd.s32 %v117431, %v117426
%v117438 = vadd.s32 %v117434, %v9
%v117440 = vshll.u32 %v117431, 6
%v117441 = vshrl.u32 %v117431, 26
%v117442 = vor.u32 %v117441, %v117440
%v117443 = vxor.u32 %v117442, %v117434
%v117446 = vadd.s32 %v117443, %v8
%v117450 = vadd.s32 1, %v117446
%v117454 = vadd.s32 %v117450, %v117438
%v117456 = vshll.u32 %v117450, 17
%v117457 = vshrl.u32 %v117450, 15
%v117458 = vor.u32 %v117457, %v117456
%v117459 = vxor.u32 %v117458, %v117454
%v117462 = vadd.s32 %v117459, %v117454
%v117464 = vshll.u32 %v117459, 29
%v117465 = vshrl.u32 %v117459, 3
%v117466 = vor.u32 %v117465, %v117464
%v117467 = vxor.u32 %v117466, %v117462
%v117470 = vadd.s32 %v117467, %v117462
%v117472 = vshll.u32 %v117467, 16
%v117473 = vshrl.u32 %v117467, 16
%v117474 = vor.u32 %v117473, %v117472
%v117475 = vxor.u32 %v117474, %v117470
%v117478 = vadd.s32 %v117475, %v117470
%v117482 = vadd.s32 %v117478, %v8
%v117484 = vshll.u32 %v117475, 24
%v117485 = vshrl.u32 %v117475, 8
%v117486 = vor.u32 %v117485, %v117484
%v117487 = vxor.u32 %v117486, %v117478
%v117490 = vadd.s32 %v117487, %v10
%v117494 = vadd.s32 2, %v117490
%v117498 = vadd.s32 %v117494, %v117482
%v117500 = vshll.u32 %v117494, 13
%v117501 = vshrl.u32 %v117494, 19
%v117502 = vor.u32 %v117501, %v117500
%v117503 = vxor.u32 %v117502, %v117498
%v117506 = vadd.s32 %v117503, %v117498
%v117508 = vshll.u32 %v117503, 15
%v117509 = vshrl.u32 %v117503, 17
%v117510 = vor.u32 %v117509, %v117508
%v117511 = vxor.u32 %v117510, %v117506
%v117514 = vadd.s32 %v117511, %v117506
%v117516 = vshll.u32 %v117511, 26
%v117517 = vshrl.u32 %v117511, 6
%v117518 = vor.u32 %v117517, %v117516
%v117519 = vxor.u32 %v117518, %v117514
%v117522 = vadd.s32 %v117519, %v117514
%v117526 = vadd.s32 %v117522, %v10
%v117528 = vshll.u32 %v117519, 6
%v117529 = vshrl.u32 %v117519, 26
%v117530 = vor.u32 %v117529, %v117528
%v117531 = vxor.u32 %v117530, %v117522
%v117534 = vadd.s32 %v117531, %v9
%v117538 = vadd.s32 3, %v117534
%v117542 = vadd.s32 %v117538, %v117526
%v117544 = vshll.u32 %v117538, 17
%v117545 = vshrl.u32 %v117538, 15
%v117546 = vor.u32 %v117545, %v117544
%v117547 = vxor.u32 %v117546, %v117542
%v117550 = vadd.s32 %v117547, %v117542
%v117552 = vshll.u32 %v117547, 29
%v117553 = vshrl.u32 %v117547, 3
%v117554 = vor.u32 %v117553, %v117552
%v117555 = vxor.u32 %v117554, %v117550
%v117558 = vadd.s32 %v117555, %v117550
%v117560 = vshll.u32 %v117555, 16
%v117561 = vshrl.u32 %v117555, 16
%v117562 = vor.u32 %v117561, %v117560
%v117563 = vxor.u32 %v117562, %v117558
%v117566 = vadd.s32 %v117563, %v117558
%v117570 = vadd.s32 %v117566, %v9
%v117572 = vshll.u32 %v117563, 24
%v117573 = vshrl.u32 %v117563, 8
%v117574 = vor.u32 %v117573, %v117572
%v117575 = vxor.u32 %v117574, %v117566
%v117578 = vadd.s32 %v117575, %v8
%v117582 = vadd.s32 4, %v117578
%v117586 = vadd.s32 %v117582, %v117570
%v117588 = vshll.u32 %v117582, 13
%v117589 = vshrl.u32 %v117582, 19
%v117590 = vor.u32 %v117589, %v117588
%v117591 = vxor.u32 %v117590, %v117586
%v117594 = vadd.s32 %v117591, %v117586
%v117596 = vshll.u32 %v117591, 15
%v117597 = vshrl.u32 %v117591, 17
%v117598 = vor.u32 %v117597, %v117596
%v117599 = vxor.u32 %v117598, %v117594
%v117602 = vadd.s32 %v117599, %v117594
%v117604 = vshll.u32 %v117599, 26
%v117605 = vshrl.u32 %v117599, 6
%v117606 = vor.u32 %v117605, %v117604
%v117607 = vxor.u32 %v117606, %v117602
%v117610 = vadd.s32 %v117607, %v117602
%v117614 = vadd.s32 %v117610, %v8
%v117616 = vshll.u32 %v117607, 6
%v117617 = vshrl.u32 %v117607, 26
%v117618 = vor.u32 %v117617, %v117616
%v117619 = vxor.u32 %v117618, %v117610
%v117622 = vadd.s32 %v117619, %v10
%v117626 = vadd.s32 5, %v117622
%v117628 = vxor.u32 %v117626, %v117614
%v117629 = vand.u32.u8 255, %v117628
%v117630 = vand.u32 65535, %v117629
%v117631 = vshrl.u32 %v117630, 1
%v117632 = vor.u32 16256, %v117631
%v117633 = vand.u32.u16 65535, %v117632
%v120374 = vadd.low.f32.bf16 -1.0, %v117633
%v117642 = vmul.f32 2.0, %v120374
%v117646 = vadd.f32 -0.99609375, %v117642
%v117650 = vmax.f32 %v117646, -0.99609375
%v117652 = vand.u32 2147483647, %v117650
%vm117655 = vcmp.eq.f32.partialorder %v117652, 1.0
%v117660 = vmul.f32 inf, %v117650
%v117662 = vxor.u32 2147483648, %v117650
%v117665 = vmul.f32 %v117662, %v117650
%v117667 = vadd.f32 1.0, %v117665
%v117668 = vlog2.pop %v117667
%v117669 = vmul.f32 0.6931472, %v117668
%v117670 = vmul.f32 -0.5, %v117665
%v117671 = vadd.f32 1.0, %v117670
%v117672 = vmul.f32 %v117671, %v117665
%v117673 = vand.u32 2147483647, %v117665
%vm117674 = vcmp.lt.f32.partialorder %v117673, 0.0004427343
%v117675 = vsel /*vm=*/%vm117674, /*on_true_vy=*/%v117672, /*on_false_vx=*/%v117669
%v117676 = vxor.u32 2147483648, %v117675
%vm117679 = vcmp.lt.f32.partialorder %v117676, 5.0
%v117684 = vsel /*vm=*/%vm117679, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v117688 = vsel /*vm=*/%vm117679, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v117692 = vsel /*vm=*/%vm117679, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v117696 = vsel /*vm=*/%vm117679, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v117700 = vsel /*vm=*/%vm117679, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v117704 = vsel /*vm=*/%vm117679, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v117708 = vsel /*vm=*/%vm117679, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v117712 = vsel /*vm=*/%vm117679, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v117716 = vsel /*vm=*/%vm117679, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v117720 = vadd.f32 -2.5, %v117676
%v117722 = vrsqrt.pop %v117676
%v117723 = vmul.f32 %v117722, %v117676
%vm117724 = vcmp.eq.f32.partialorder %v117676, inf
%v117725 = vsel /*vm=*/%vm117724, /*on_true_vy=*/%v117676, /*on_false_vx=*/%v117723
%vm117726 = vcmp.eq.f32.partialorder %v117676, 0.0
%v117727 = vand.u32 2147483648, %v117676
%v117728 = vsel /*vm=*/%vm117726, /*on_true_vy=*/%v117727, /*on_false_vx=*/%v117725
%v117731 = vadd.f32 -3.0, %v117728
%v117735 = vsel /*vm=*/%vm117679, /*on_true_vy=*/%v117720, /*on_false_vx=*/%v117731
%v117739 = vmul.f32 %v117735, %v117716
%v117743 = vadd.f32 %v117739, %v117712
%v117747 = vmul.f32 %v117743, %v117735
%v117751 = vadd.f32 %v117747, %v117708
%v117755 = vmul.f32 %v117751, %v117735
%v117759 = vadd.f32 %v117755, %v117704
%v117763 = vmul.f32 %v117759, %v117735
%v117767 = vadd.f32 %v117763, %v117700
%v117771 = vmul.f32 %v117767, %v117735
%v117775 = vadd.f32 %v117771, %v117696
%v117779 = vmul.f32 %v117775, %v117735
%v117783 = vadd.f32 %v117779, %v117692
%v117787 = vmul.f32 %v117783, %v117735
%v117791 = vadd.f32 %v117787, %v117688
%v117795 = vmul.f32 %v117791, %v117735
%v117799 = vadd.f32 %v117795, %v117684
%v117803 = vmul.f32 %v117799, %v117650
%v117807 = vsel /*vm=*/%vm117655, /*on_true_vy=*/%v117660, /*on_false_vx=*/%v117803
%v117811 = vmul.f32 1.4140625, %v117807
%v117814 = vpack.c.bf16 %v120417, %v117811
%120375 = vst [vmem:[%s280 + $0x1fc] sm:$0xf] /*vst_source=*/%v117814
%v117818 = vadd.s32 %v115971, %v2355
%v117828 = vadd.s32 %v117818, %v415
%vm117832 = vcmp.lt.u32.totalorder %v117828, %v117818
%vm117837 = vcmp.lt.u32.totalorder %v117818, %v2355
%v117842 = vadd.s32 %v115954, %v2342
%v117846 = vadd.s32 1, %v117842
%v117850 = vsel /*vm=*/%vm117837, /*on_true_vy=*/%v117846, /*on_false_vx=*/%v117842
%v117854 = vadd.s32 1, %v117850
%v117858 = vsel /*vm=*/%vm117832, /*on_true_vy=*/%v117854, /*on_false_vx=*/%v117850
%v117863 = vadd.s32 %v117858, %v10
%v117867 = vadd.s32 %v117828, %v9
%v117871 = vadd.s32 %v117867, %v117863
%v117873 = vshll.u32 %v117867, 13
%v117874 = vshrl.u32 %v117867, 19
%v117875 = vor.u32 %v117874, %v117873
%v117876 = vxor.u32 %v117875, %v117871
%v117879 = vadd.s32 %v117876, %v117871
%v117881 = vshll.u32 %v117876, 15
%v117882 = vshrl.u32 %v117876, 17
%v117883 = vor.u32 %v117882, %v117881
%v117884 = vxor.u32 %v117883, %v117879
%v117887 = vadd.s32 %v117884, %v117879
%v117889 = vshll.u32 %v117884, 26
%v117890 = vshrl.u32 %v117884, 6
%v117891 = vor.u32 %v117890, %v117889
%v117892 = vxor.u32 %v117891, %v117887
%v117895 = vadd.s32 %v117892, %v117887
%v117899 = vadd.s32 %v117895, %v9
%v117901 = vshll.u32 %v117892, 6
%v117902 = vshrl.u32 %v117892, 26
%v117903 = vor.u32 %v117902, %v117901
%v117904 = vxor.u32 %v117903, %v117895
%v117907 = vadd.s32 %v117904, %v8
%v117911 = vadd.s32 1, %v117907
%v117915 = vadd.s32 %v117911, %v117899
%v117917 = vshll.u32 %v117911, 17
%v117918 = vshrl.u32 %v117911, 15
%v117919 = vor.u32 %v117918, %v117917
%v117920 = vxor.u32 %v117919, %v117915
%v117923 = vadd.s32 %v117920, %v117915
%v117925 = vshll.u32 %v117920, 29
%v117926 = vshrl.u32 %v117920, 3
%v117927 = vor.u32 %v117926, %v117925
%v117928 = vxor.u32 %v117927, %v117923
%v117931 = vadd.s32 %v117928, %v117923
%v117933 = vshll.u32 %v117928, 16
%v117934 = vshrl.u32 %v117928, 16
%v117935 = vor.u32 %v117934, %v117933
%v117936 = vxor.u32 %v117935, %v117931
%v117939 = vadd.s32 %v117936, %v117931
%v117943 = vadd.s32 %v117939, %v8
%v117945 = vshll.u32 %v117936, 24
%v117946 = vshrl.u32 %v117936, 8
%v117947 = vor.u32 %v117946, %v117945
%v117948 = vxor.u32 %v117947, %v117939
%v117951 = vadd.s32 %v117948, %v10
%v117955 = vadd.s32 2, %v117951
%v117959 = vadd.s32 %v117955, %v117943
%v117961 = vshll.u32 %v117955, 13
%v117962 = vshrl.u32 %v117955, 19
%v117963 = vor.u32 %v117962, %v117961
%v117964 = vxor.u32 %v117963, %v117959
%v117967 = vadd.s32 %v117964, %v117959
%v117969 = vshll.u32 %v117964, 15
%v117970 = vshrl.u32 %v117964, 17
%v117971 = vor.u32 %v117970, %v117969
%v117972 = vxor.u32 %v117971, %v117967
%v117975 = vadd.s32 %v117972, %v117967
%v117977 = vshll.u32 %v117972, 26
%v117978 = vshrl.u32 %v117972, 6
%v117979 = vor.u32 %v117978, %v117977
%v117980 = vxor.u32 %v117979, %v117975
%v117983 = vadd.s32 %v117980, %v117975
%v117987 = vadd.s32 %v117983, %v10
%v117989 = vshll.u32 %v117980, 6
%v117990 = vshrl.u32 %v117980, 26
%v117991 = vor.u32 %v117990, %v117989
%v117992 = vxor.u32 %v117991, %v117983
%v117995 = vadd.s32 %v117992, %v9
%v117999 = vadd.s32 3, %v117995
%v118003 = vadd.s32 %v117999, %v117987
%v118005 = vshll.u32 %v117999, 17
%v118006 = vshrl.u32 %v117999, 15
%v118007 = vor.u32 %v118006, %v118005
%v118008 = vxor.u32 %v118007, %v118003
%v118011 = vadd.s32 %v118008, %v118003
%v118013 = vshll.u32 %v118008, 29
%v118014 = vshrl.u32 %v118008, 3
%v118015 = vor.u32 %v118014, %v118013
%v118016 = vxor.u32 %v118015, %v118011
%v118019 = vadd.s32 %v118016, %v118011
%v118021 = vshll.u32 %v118016, 16
%v118022 = vshrl.u32 %v118016, 16
%v118023 = vor.u32 %v118022, %v118021
%v118024 = vxor.u32 %v118023, %v118019
%v118027 = vadd.s32 %v118024, %v118019
%v118031 = vadd.s32 %v118027, %v9
%v118033 = vshll.u32 %v118024, 24
%v118034 = vshrl.u32 %v118024, 8
%v118035 = vor.u32 %v118034, %v118033
%v118036 = vxor.u32 %v118035, %v118027
%v118039 = vadd.s32 %v118036, %v8
%v118043 = vadd.s32 4, %v118039
%v118047 = vadd.s32 %v118043, %v118031
%v118049 = vshll.u32 %v118043, 13
%v118050 = vshrl.u32 %v118043, 19
%v118051 = vor.u32 %v118050, %v118049
%v118052 = vxor.u32 %v118051, %v118047
%v118055 = vadd.s32 %v118052, %v118047
%v118057 = vshll.u32 %v118052, 15
%v118058 = vshrl.u32 %v118052, 17
%v118059 = vor.u32 %v118058, %v118057
%v118060 = vxor.u32 %v118059, %v118055
%v118063 = vadd.s32 %v118060, %v118055
%v118065 = vshll.u32 %v118060, 26
%v118066 = vshrl.u32 %v118060, 6
%v118067 = vor.u32 %v118066, %v118065
%v118068 = vxor.u32 %v118067, %v118063
%v118071 = vadd.s32 %v118068, %v118063
%v118075 = vadd.s32 %v118071, %v8
%v118077 = vshll.u32 %v118068, 6
%v118078 = vshrl.u32 %v118068, 26
%v118079 = vor.u32 %v118078, %v118077
%v118080 = vxor.u32 %v118079, %v118071
%v118083 = vadd.s32 %v118080, %v10
%v118087 = vadd.s32 5, %v118083
%v118089 = vxor.u32 %v118087, %v118075
%v118090 = vand.u32.u8 255, %v118089
%v118091 = vand.u32 65535, %v118090
%v118092 = vshrl.u32 %v118091, 1
%v118093 = vor.u32 16256, %v118092
%v118094 = vand.u32.u16 65535, %v118093
%v120376 = vadd.low.f32.bf16 -1.0, %v118094
%v118103 = vmul.f32 2.0, %v120376
%v118107 = vadd.f32 -0.99609375, %v118103
%v118111 = vmax.f32 %v118107, -0.99609375
%v118113 = vand.u32 2147483647, %v118111
%vm118116 = vcmp.eq.f32.partialorder %v118113, 1.0
%v118121 = vmul.f32 inf, %v118111
%v118123 = vxor.u32 2147483648, %v118111
%v118126 = vmul.f32 %v118123, %v118111
%v118128 = vadd.f32 1.0, %v118126
%v118129 = vlog2.pop %v118128
%v118130 = vmul.f32 0.6931472, %v118129
%v118131 = vmul.f32 -0.5, %v118126
%v118132 = vadd.f32 1.0, %v118131
%v118133 = vmul.f32 %v118132, %v118126
%v118134 = vand.u32 2147483647, %v118126
%vm118135 = vcmp.lt.f32.partialorder %v118134, 0.0004427343
%v118136 = vsel /*vm=*/%vm118135, /*on_true_vy=*/%v118133, /*on_false_vx=*/%v118130
%v118137 = vxor.u32 2147483648, %v118136
%vm118140 = vcmp.lt.f32.partialorder %v118137, 5.0
%v118145 = vsel /*vm=*/%vm118140, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v118149 = vsel /*vm=*/%vm118140, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v118153 = vsel /*vm=*/%vm118140, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v118157 = vsel /*vm=*/%vm118140, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v118161 = vsel /*vm=*/%vm118140, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v118165 = vsel /*vm=*/%vm118140, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v118169 = vsel /*vm=*/%vm118140, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v118173 = vsel /*vm=*/%vm118140, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v118177 = vsel /*vm=*/%vm118140, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v118181 = vadd.f32 -2.5, %v118137
%v118183 = vrsqrt.pop %v118137
%v118184 = vmul.f32 %v118183, %v118137
%vm118185 = vcmp.eq.f32.partialorder %v118137, inf
%v118186 = vsel /*vm=*/%vm118185, /*on_true_vy=*/%v118137, /*on_false_vx=*/%v118184
%vm118187 = vcmp.eq.f32.partialorder %v118137, 0.0
%v118188 = vand.u32 2147483648, %v118137
%v118189 = vsel /*vm=*/%vm118187, /*on_true_vy=*/%v118188, /*on_false_vx=*/%v118186
%v118192 = vadd.f32 -3.0, %v118189
%v118196 = vsel /*vm=*/%vm118140, /*on_true_vy=*/%v118181, /*on_false_vx=*/%v118192
%v118200 = vmul.f32 %v118196, %v118177
%v118204 = vadd.f32 %v118200, %v118173
%v118208 = vmul.f32 %v118204, %v118196
%v118212 = vadd.f32 %v118208, %v118169
%v118216 = vmul.f32 %v118212, %v118196
%v118220 = vadd.f32 %v118216, %v118165
%v118224 = vmul.f32 %v118220, %v118196
%v118228 = vadd.f32 %v118224, %v118161
%v118232 = vmul.f32 %v118228, %v118196
%v118236 = vadd.f32 %v118232, %v118157
%v118240 = vmul.f32 %v118236, %v118196
%v118244 = vadd.f32 %v118240, %v118153
%v118248 = vmul.f32 %v118244, %v118196
%v118252 = vadd.f32 %v118248, %v118149
%v118256 = vmul.f32 %v118252, %v118196
%v118260 = vadd.f32 %v118256, %v118145
%v118264 = vmul.f32 %v118260, %v118111
%v118268 = vsel /*vm=*/%vm118116, /*on_true_vy=*/%v118121, /*on_false_vx=*/%v118264
%v118272 = vmul.f32 1.4140625, %v118268
%v118275 = vpack.c.bf16 %v120417, %v118272
%120377 = vst [vmem:[%s280 + $0x27c] sm:$0xf] /*vst_source=*/%v118275
%v118279 = vadd.s32 %v115971, %v2842
%v118289 = vadd.s32 %v118279, %v415
%vm118293 = vcmp.lt.u32.totalorder %v118289, %v118279
%vm118298 = vcmp.lt.u32.totalorder %v118279, %v2842
%v118303 = vadd.s32 %v115954, %v2829
%v118307 = vadd.s32 1, %v118303
%v118311 = vsel /*vm=*/%vm118298, /*on_true_vy=*/%v118307, /*on_false_vx=*/%v118303
%v118315 = vadd.s32 1, %v118311
%v118319 = vsel /*vm=*/%vm118293, /*on_true_vy=*/%v118315, /*on_false_vx=*/%v118311
%v118324 = vadd.s32 %v118319, %v10
%v118328 = vadd.s32 %v118289, %v9
%v118332 = vadd.s32 %v118328, %v118324
%v118334 = vshll.u32 %v118328, 13
%v118335 = vshrl.u32 %v118328, 19
%v118336 = vor.u32 %v118335, %v118334
%v118337 = vxor.u32 %v118336, %v118332
%v118340 = vadd.s32 %v118337, %v118332
%v118342 = vshll.u32 %v118337, 15
%v118343 = vshrl.u32 %v118337, 17
%v118344 = vor.u32 %v118343, %v118342
%v118345 = vxor.u32 %v118344, %v118340
%v118348 = vadd.s32 %v118345, %v118340
%v118350 = vshll.u32 %v118345, 26
%v118351 = vshrl.u32 %v118345, 6
%v118352 = vor.u32 %v118351, %v118350
%v118353 = vxor.u32 %v118352, %v118348
%v118356 = vadd.s32 %v118353, %v118348
%v118360 = vadd.s32 %v118356, %v9
%v118362 = vshll.u32 %v118353, 6
%v118363 = vshrl.u32 %v118353, 26
%v118364 = vor.u32 %v118363, %v118362
%v118365 = vxor.u32 %v118364, %v118356
%v118368 = vadd.s32 %v118365, %v8
%v118372 = vadd.s32 1, %v118368
%v118376 = vadd.s32 %v118372, %v118360
%v118378 = vshll.u32 %v118372, 17
%v118379 = vshrl.u32 %v118372, 15
%v118380 = vor.u32 %v118379, %v118378
%v118381 = vxor.u32 %v118380, %v118376
%v118384 = vadd.s32 %v118381, %v118376
%v118386 = vshll.u32 %v118381, 29
%v118387 = vshrl.u32 %v118381, 3
%v118388 = vor.u32 %v118387, %v118386
%v118389 = vxor.u32 %v118388, %v118384
%v118392 = vadd.s32 %v118389, %v118384
%v118394 = vshll.u32 %v118389, 16
%v118395 = vshrl.u32 %v118389, 16
%v118396 = vor.u32 %v118395, %v118394
%v118397 = vxor.u32 %v118396, %v118392
%v118400 = vadd.s32 %v118397, %v118392
%v118404 = vadd.s32 %v118400, %v8
%v118406 = vshll.u32 %v118397, 24
%v118407 = vshrl.u32 %v118397, 8
%v118408 = vor.u32 %v118407, %v118406
%v118409 = vxor.u32 %v118408, %v118400
%v118412 = vadd.s32 %v118409, %v10
%v118416 = vadd.s32 2, %v118412
%v118420 = vadd.s32 %v118416, %v118404
%v118422 = vshll.u32 %v118416, 13
%v118423 = vshrl.u32 %v118416, 19
%v118424 = vor.u32 %v118423, %v118422
%v118425 = vxor.u32 %v118424, %v118420
%v118428 = vadd.s32 %v118425, %v118420
%v118430 = vshll.u32 %v118425, 15
%v118431 = vshrl.u32 %v118425, 17
%v118432 = vor.u32 %v118431, %v118430
%v118433 = vxor.u32 %v118432, %v118428
%v118436 = vadd.s32 %v118433, %v118428
%v118438 = vshll.u32 %v118433, 26
%v118439 = vshrl.u32 %v118433, 6
%v118440 = vor.u32 %v118439, %v118438
%v118441 = vxor.u32 %v118440, %v118436
%v118444 = vadd.s32 %v118441, %v118436
%v118448 = vadd.s32 %v118444, %v10
%v118450 = vshll.u32 %v118441, 6
%v118451 = vshrl.u32 %v118441, 26
%v118452 = vor.u32 %v118451, %v118450
%v118453 = vxor.u32 %v118452, %v118444
%v118456 = vadd.s32 %v118453, %v9
%v118460 = vadd.s32 3, %v118456
%v118464 = vadd.s32 %v118460, %v118448
%v118466 = vshll.u32 %v118460, 17
%v118467 = vshrl.u32 %v118460, 15
%v118468 = vor.u32 %v118467, %v118466
%v118469 = vxor.u32 %v118468, %v118464
%v118472 = vadd.s32 %v118469, %v118464
%v118474 = vshll.u32 %v118469, 29
%v118475 = vshrl.u32 %v118469, 3
%v118476 = vor.u32 %v118475, %v118474
%v118477 = vxor.u32 %v118476, %v118472
%v118480 = vadd.s32 %v118477, %v118472
%v118482 = vshll.u32 %v118477, 16
%v118483 = vshrl.u32 %v118477, 16
%v118484 = vor.u32 %v118483, %v118482
%v118485 = vxor.u32 %v118484, %v118480
%v118488 = vadd.s32 %v118485, %v118480
%v118492 = vadd.s32 %v118488, %v9
%v118494 = vshll.u32 %v118485, 24
%v118495 = vshrl.u32 %v118485, 8
%v118496 = vor.u32 %v118495, %v118494
%v118497 = vxor.u32 %v118496, %v118488
%v118500 = vadd.s32 %v118497, %v8
%v118504 = vadd.s32 4, %v118500
%v118508 = vadd.s32 %v118504, %v118492
%v118510 = vshll.u32 %v118504, 13
%v118511 = vshrl.u32 %v118504, 19
%v118512 = vor.u32 %v118511, %v118510
%v118513 = vxor.u32 %v118512, %v118508
%v118516 = vadd.s32 %v118513, %v118508
%v118518 = vshll.u32 %v118513, 15
%v118519 = vshrl.u32 %v118513, 17
%v118520 = vor.u32 %v118519, %v118518
%v118521 = vxor.u32 %v118520, %v118516
%v118524 = vadd.s32 %v118521, %v118516
%v118526 = vshll.u32 %v118521, 26
%v118527 = vshrl.u32 %v118521, 6
%v118528 = vor.u32 %v118527, %v118526
%v118529 = vxor.u32 %v118528, %v118524
%v118532 = vadd.s32 %v118529, %v118524
%v118536 = vadd.s32 %v118532, %v8
%v118538 = vshll.u32 %v118529, 6
%v118539 = vshrl.u32 %v118529, 26
%v118540 = vor.u32 %v118539, %v118538
%v118541 = vxor.u32 %v118540, %v118532
%v118544 = vadd.s32 %v118541, %v10
%v118548 = vadd.s32 5, %v118544
%v118550 = vxor.u32 %v118548, %v118536
%v118551 = vand.u32.u8 255, %v118550
%v118552 = vand.u32 65535, %v118551
%v118553 = vshrl.u32 %v118552, 1
%v118554 = vor.u32 16256, %v118553
%v118555 = vand.u32.u16 65535, %v118554
%v120378 = vadd.low.f32.bf16 -1.0, %v118555
%v118564 = vmul.f32 2.0, %v120378
%v118568 = vadd.f32 -0.99609375, %v118564
%v118572 = vmax.f32 %v118568, -0.99609375
%v118574 = vand.u32 2147483647, %v118572
%vm118577 = vcmp.eq.f32.partialorder %v118574, 1.0
%v118582 = vmul.f32 inf, %v118572
%v118584 = vxor.u32 2147483648, %v118572
%v118587 = vmul.f32 %v118584, %v118572
%v118589 = vadd.f32 1.0, %v118587
%v118590 = vlog2.pop %v118589
%v118591 = vmul.f32 0.6931472, %v118590
%v118592 = vmul.f32 -0.5, %v118587
%v118593 = vadd.f32 1.0, %v118592
%v118594 = vmul.f32 %v118593, %v118587
%v118595 = vand.u32 2147483647, %v118587
%vm118596 = vcmp.lt.f32.partialorder %v118595, 0.0004427343
%v118597 = vsel /*vm=*/%vm118596, /*on_true_vy=*/%v118594, /*on_false_vx=*/%v118591
%v118598 = vxor.u32 2147483648, %v118597
%vm118601 = vcmp.lt.f32.partialorder %v118598, 5.0
%v118606 = vsel /*vm=*/%vm118601, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v118610 = vsel /*vm=*/%vm118601, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v118614 = vsel /*vm=*/%vm118601, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v118618 = vsel /*vm=*/%vm118601, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v118622 = vsel /*vm=*/%vm118601, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v118626 = vsel /*vm=*/%vm118601, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v118630 = vsel /*vm=*/%vm118601, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v118634 = vsel /*vm=*/%vm118601, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v118638 = vsel /*vm=*/%vm118601, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v118642 = vadd.f32 -2.5, %v118598
%v118644 = vrsqrt.pop %v118598
%v118645 = vmul.f32 %v118644, %v118598
%vm118646 = vcmp.eq.f32.partialorder %v118598, inf
%v118647 = vsel /*vm=*/%vm118646, /*on_true_vy=*/%v118598, /*on_false_vx=*/%v118645
%vm118648 = vcmp.eq.f32.partialorder %v118598, 0.0
%v118649 = vand.u32 2147483648, %v118598
%v118650 = vsel /*vm=*/%vm118648, /*on_true_vy=*/%v118649, /*on_false_vx=*/%v118647
%v118653 = vadd.f32 -3.0, %v118650
%v118657 = vsel /*vm=*/%vm118601, /*on_true_vy=*/%v118642, /*on_false_vx=*/%v118653
%v118661 = vmul.f32 %v118657, %v118638
%v118665 = vadd.f32 %v118661, %v118634
%v118669 = vmul.f32 %v118665, %v118657
%v118673 = vadd.f32 %v118669, %v118630
%v118677 = vmul.f32 %v118673, %v118657
%v118681 = vadd.f32 %v118677, %v118626
%v118685 = vmul.f32 %v118681, %v118657
%v118689 = vadd.f32 %v118685, %v118622
%v118693 = vmul.f32 %v118689, %v118657
%v118697 = vadd.f32 %v118693, %v118618
%v118701 = vmul.f32 %v118697, %v118657
%v118705 = vadd.f32 %v118701, %v118614
%v118709 = vmul.f32 %v118705, %v118657
%v118713 = vadd.f32 %v118709, %v118610
%v118717 = vmul.f32 %v118713, %v118657
%v118721 = vadd.f32 %v118717, %v118606
%v118725 = vmul.f32 %v118721, %v118572
%v118729 = vsel /*vm=*/%vm118577, /*on_true_vy=*/%v118582, /*on_false_vx=*/%v118725
%v118733 = vmul.f32 1.4140625, %v118729
%v118736 = vpack.c.bf16 %v120417, %v118733
%120379 = vst [vmem:[%s280 + $0x2fc] sm:$0xf] /*vst_source=*/%v118736
%v118740 = vadd.s32 %v115971, %v3329
%v118750 = vadd.s32 %v118740, %v415
%vm118754 = vcmp.lt.u32.totalorder %v118750, %v118740
%vm118759 = vcmp.lt.u32.totalorder %v118740, %v3329
%v118764 = vadd.s32 %v115954, %v3316
%v118768 = vadd.s32 1, %v118764
%v118772 = vsel /*vm=*/%vm118759, /*on_true_vy=*/%v118768, /*on_false_vx=*/%v118764
%v118776 = vadd.s32 1, %v118772
%v118780 = vsel /*vm=*/%vm118754, /*on_true_vy=*/%v118776, /*on_false_vx=*/%v118772
%v118785 = vadd.s32 %v118780, %v10
%v118789 = vadd.s32 %v118750, %v9
%v118793 = vadd.s32 %v118789, %v118785
%v118795 = vshll.u32 %v118789, 13
%v118796 = vshrl.u32 %v118789, 19
%v118797 = vor.u32 %v118796, %v118795
%v118798 = vxor.u32 %v118797, %v118793
%v118801 = vadd.s32 %v118798, %v118793
%v118803 = vshll.u32 %v118798, 15
%v118804 = vshrl.u32 %v118798, 17
%v118805 = vor.u32 %v118804, %v118803
%v118806 = vxor.u32 %v118805, %v118801
%v118809 = vadd.s32 %v118806, %v118801
%v118811 = vshll.u32 %v118806, 26
%v118812 = vshrl.u32 %v118806, 6
%v118813 = vor.u32 %v118812, %v118811
%v118814 = vxor.u32 %v118813, %v118809
%v118817 = vadd.s32 %v118814, %v118809
%v118821 = vadd.s32 %v118817, %v9
%v118823 = vshll.u32 %v118814, 6
%v118824 = vshrl.u32 %v118814, 26
%v118825 = vor.u32 %v118824, %v118823
%v118826 = vxor.u32 %v118825, %v118817
%v118829 = vadd.s32 %v118826, %v8
%v118833 = vadd.s32 1, %v118829
%v118837 = vadd.s32 %v118833, %v118821
%v118839 = vshll.u32 %v118833, 17
%v118840 = vshrl.u32 %v118833, 15
%v118841 = vor.u32 %v118840, %v118839
%v118842 = vxor.u32 %v118841, %v118837
%v118845 = vadd.s32 %v118842, %v118837
%v118847 = vshll.u32 %v118842, 29
%v118848 = vshrl.u32 %v118842, 3
%v118849 = vor.u32 %v118848, %v118847
%v118850 = vxor.u32 %v118849, %v118845
%v118853 = vadd.s32 %v118850, %v118845
%v118855 = vshll.u32 %v118850, 16
%v118856 = vshrl.u32 %v118850, 16
%v118857 = vor.u32 %v118856, %v118855
%v118858 = vxor.u32 %v118857, %v118853
%v118861 = vadd.s32 %v118858, %v118853
%v118865 = vadd.s32 %v118861, %v8
%v118867 = vshll.u32 %v118858, 24
%v118868 = vshrl.u32 %v118858, 8
%v118869 = vor.u32 %v118868, %v118867
%v118870 = vxor.u32 %v118869, %v118861
%v118873 = vadd.s32 %v118870, %v10
%v118877 = vadd.s32 2, %v118873
%v118881 = vadd.s32 %v118877, %v118865
%v118883 = vshll.u32 %v118877, 13
%v118884 = vshrl.u32 %v118877, 19
%v118885 = vor.u32 %v118884, %v118883
%v118886 = vxor.u32 %v118885, %v118881
%v118889 = vadd.s32 %v118886, %v118881
%v118891 = vshll.u32 %v118886, 15
%v118892 = vshrl.u32 %v118886, 17
%v118893 = vor.u32 %v118892, %v118891
%v118894 = vxor.u32 %v118893, %v118889
%v118897 = vadd.s32 %v118894, %v118889
%v118899 = vshll.u32 %v118894, 26
%v118900 = vshrl.u32 %v118894, 6
%v118901 = vor.u32 %v118900, %v118899
%v118902 = vxor.u32 %v118901, %v118897
%v118905 = vadd.s32 %v118902, %v118897
%v118909 = vadd.s32 %v118905, %v10
%v118911 = vshll.u32 %v118902, 6
%v118912 = vshrl.u32 %v118902, 26
%v118913 = vor.u32 %v118912, %v118911
%v118914 = vxor.u32 %v118913, %v118905
%v118917 = vadd.s32 %v118914, %v9
%v118921 = vadd.s32 3, %v118917
%v118925 = vadd.s32 %v118921, %v118909
%v118927 = vshll.u32 %v118921, 17
%v118928 = vshrl.u32 %v118921, 15
%v118929 = vor.u32 %v118928, %v118927
%v118930 = vxor.u32 %v118929, %v118925
%v118933 = vadd.s32 %v118930, %v118925
%v118935 = vshll.u32 %v118930, 29
%v118936 = vshrl.u32 %v118930, 3
%v118937 = vor.u32 %v118936, %v118935
%v118938 = vxor.u32 %v118937, %v118933
%v118941 = vadd.s32 %v118938, %v118933
%v118943 = vshll.u32 %v118938, 16
%v118944 = vshrl.u32 %v118938, 16
%v118945 = vor.u32 %v118944, %v118943
%v118946 = vxor.u32 %v118945, %v118941
%v118949 = vadd.s32 %v118946, %v118941
%v118953 = vadd.s32 %v118949, %v9
%v118955 = vshll.u32 %v118946, 24
%v118956 = vshrl.u32 %v118946, 8
%v118957 = vor.u32 %v118956, %v118955
%v118958 = vxor.u32 %v118957, %v118949
%v118961 = vadd.s32 %v118958, %v8
%v118965 = vadd.s32 4, %v118961
%v118969 = vadd.s32 %v118965, %v118953
%v118971 = vshll.u32 %v118965, 13
%v118972 = vshrl.u32 %v118965, 19
%v118973 = vor.u32 %v118972, %v118971
%v118974 = vxor.u32 %v118973, %v118969
%v118977 = vadd.s32 %v118974, %v118969
%v118979 = vshll.u32 %v118974, 15
%v118980 = vshrl.u32 %v118974, 17
%v118981 = vor.u32 %v118980, %v118979
%v118982 = vxor.u32 %v118981, %v118977
%v118985 = vadd.s32 %v118982, %v118977
%v118987 = vshll.u32 %v118982, 26
%v118988 = vshrl.u32 %v118982, 6
%v118989 = vor.u32 %v118988, %v118987
%v118990 = vxor.u32 %v118989, %v118985
%v118993 = vadd.s32 %v118990, %v118985
%v118997 = vadd.s32 %v118993, %v8
%v118999 = vshll.u32 %v118990, 6
%v119000 = vshrl.u32 %v118990, 26
%v119001 = vor.u32 %v119000, %v118999
%v119002 = vxor.u32 %v119001, %v118993
%v119005 = vadd.s32 %v119002, %v10
%v119009 = vadd.s32 5, %v119005
%v119011 = vxor.u32 %v119009, %v118997
%v119012 = vand.u32.u8 255, %v119011
%v119013 = vand.u32 65535, %v119012
%v119014 = vshrl.u32 %v119013, 1
%v119015 = vor.u32 16256, %v119014
%v119016 = vand.u32.u16 65535, %v119015
%v120380 = vadd.low.f32.bf16 -1.0, %v119016
%v119025 = vmul.f32 2.0, %v120380
%v119029 = vadd.f32 -0.99609375, %v119025
%v119033 = vmax.f32 %v119029, -0.99609375
%v119035 = vand.u32 2147483647, %v119033
%vm119038 = vcmp.eq.f32.partialorder %v119035, 1.0
%v119043 = vmul.f32 inf, %v119033
%v119045 = vxor.u32 2147483648, %v119033
%v119048 = vmul.f32 %v119045, %v119033
%v119050 = vadd.f32 1.0, %v119048
%v119051 = vlog2.pop %v119050
%v119052 = vmul.f32 0.6931472, %v119051
%v119053 = vmul.f32 -0.5, %v119048
%v119054 = vadd.f32 1.0, %v119053
%v119055 = vmul.f32 %v119054, %v119048
%v119056 = vand.u32 2147483647, %v119048
%vm119057 = vcmp.lt.f32.partialorder %v119056, 0.0004427343
%v119058 = vsel /*vm=*/%vm119057, /*on_true_vy=*/%v119055, /*on_false_vx=*/%v119052
%v119059 = vxor.u32 2147483648, %v119058
%vm119062 = vcmp.lt.f32.partialorder %v119059, 5.0
%v119067 = vsel /*vm=*/%vm119062, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v119071 = vsel /*vm=*/%vm119062, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v119075 = vsel /*vm=*/%vm119062, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v119079 = vsel /*vm=*/%vm119062, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v119083 = vsel /*vm=*/%vm119062, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v119087 = vsel /*vm=*/%vm119062, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v119091 = vsel /*vm=*/%vm119062, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v119095 = vsel /*vm=*/%vm119062, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v119099 = vsel /*vm=*/%vm119062, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v119103 = vadd.f32 -2.5, %v119059
%v119105 = vrsqrt.pop %v119059
%v119106 = vmul.f32 %v119105, %v119059
%vm119107 = vcmp.eq.f32.partialorder %v119059, inf
%v119108 = vsel /*vm=*/%vm119107, /*on_true_vy=*/%v119059, /*on_false_vx=*/%v119106
%vm119109 = vcmp.eq.f32.partialorder %v119059, 0.0
%v119110 = vand.u32 2147483648, %v119059
%v119111 = vsel /*vm=*/%vm119109, /*on_true_vy=*/%v119110, /*on_false_vx=*/%v119108
%v119114 = vadd.f32 -3.0, %v119111
%v119118 = vsel /*vm=*/%vm119062, /*on_true_vy=*/%v119103, /*on_false_vx=*/%v119114
%v119122 = vmul.f32 %v119118, %v119099
%v119126 = vadd.f32 %v119122, %v119095
%v119130 = vmul.f32 %v119126, %v119118
%v119134 = vadd.f32 %v119130, %v119091
%v119138 = vmul.f32 %v119134, %v119118
%v119142 = vadd.f32 %v119138, %v119087
%v119146 = vmul.f32 %v119142, %v119118
%v119150 = vadd.f32 %v119146, %v119083
%v119154 = vmul.f32 %v119150, %v119118
%v119158 = vadd.f32 %v119154, %v119079
%v119162 = vmul.f32 %v119158, %v119118
%v119166 = vadd.f32 %v119162, %v119075
%v119170 = vmul.f32 %v119166, %v119118
%v119174 = vadd.f32 %v119170, %v119071
%v119178 = vmul.f32 %v119174, %v119118
%v119182 = vadd.f32 %v119178, %v119067
%v119186 = vmul.f32 %v119182, %v119033
%v119190 = vsel /*vm=*/%vm119038, /*on_true_vy=*/%v119043, /*on_false_vx=*/%v119186
%v119194 = vmul.f32 1.4140625, %v119190
%v119197 = vpack.c.bf16 %v120417, %v119194
%120381 = vst [vmem:[%s280 + $0x37c] sm:$0xf] /*vst_source=*/%v119197
%v119201 = vadd.s32 %v115971, %v3816
%v119211 = vadd.s32 %v119201, %v415
%vm119215 = vcmp.lt.u32.totalorder %v119211, %v119201
%vm119220 = vcmp.lt.u32.totalorder %v119201, %v3816
%v119225 = vadd.s32 %v115954, %v3803
%v119229 = vadd.s32 1, %v119225
%v119233 = vsel /*vm=*/%vm119220, /*on_true_vy=*/%v119229, /*on_false_vx=*/%v119225
%v119237 = vadd.s32 1, %v119233
%v119241 = vsel /*vm=*/%vm119215, /*on_true_vy=*/%v119237, /*on_false_vx=*/%v119233
%v119246 = vadd.s32 %v119241, %v10
%v119250 = vadd.s32 %v119211, %v9
%v119254 = vadd.s32 %v119250, %v119246
%v119256 = vshll.u32 %v119250, 13
%v119257 = vshrl.u32 %v119250, 19
%v119258 = vor.u32 %v119257, %v119256
%v119259 = vxor.u32 %v119258, %v119254
%v119262 = vadd.s32 %v119259, %v119254
%v119264 = vshll.u32 %v119259, 15
%v119265 = vshrl.u32 %v119259, 17
%v119266 = vor.u32 %v119265, %v119264
%v119267 = vxor.u32 %v119266, %v119262
%v119270 = vadd.s32 %v119267, %v119262
%v119272 = vshll.u32 %v119267, 26
%v119273 = vshrl.u32 %v119267, 6
%v119274 = vor.u32 %v119273, %v119272
%v119275 = vxor.u32 %v119274, %v119270
%v119278 = vadd.s32 %v119275, %v119270
%v119282 = vadd.s32 %v119278, %v9
%v119284 = vshll.u32 %v119275, 6
%v119285 = vshrl.u32 %v119275, 26
%v119286 = vor.u32 %v119285, %v119284
%v119287 = vxor.u32 %v119286, %v119278
%v119290 = vadd.s32 %v119287, %v8
%v119294 = vadd.s32 1, %v119290
%v119298 = vadd.s32 %v119294, %v119282
%v119300 = vshll.u32 %v119294, 17
%v119301 = vshrl.u32 %v119294, 15
%v119302 = vor.u32 %v119301, %v119300
%v119303 = vxor.u32 %v119302, %v119298
%v119306 = vadd.s32 %v119303, %v119298
%v119308 = vshll.u32 %v119303, 29
%v119309 = vshrl.u32 %v119303, 3
%v119310 = vor.u32 %v119309, %v119308
%v119311 = vxor.u32 %v119310, %v119306
%v119314 = vadd.s32 %v119311, %v119306
%v119316 = vshll.u32 %v119311, 16
%v119317 = vshrl.u32 %v119311, 16
%v119318 = vor.u32 %v119317, %v119316
%v119319 = vxor.u32 %v119318, %v119314
%v119322 = vadd.s32 %v119319, %v119314
%v119326 = vadd.s32 %v119322, %v8
%v119328 = vshll.u32 %v119319, 24
%v119329 = vshrl.u32 %v119319, 8
%v119330 = vor.u32 %v119329, %v119328
%v119331 = vxor.u32 %v119330, %v119322
%v119334 = vadd.s32 %v119331, %v10
%v119338 = vadd.s32 2, %v119334
%v119342 = vadd.s32 %v119338, %v119326
%v119344 = vshll.u32 %v119338, 13
%v119345 = vshrl.u32 %v119338, 19
%v119346 = vor.u32 %v119345, %v119344
%v119347 = vxor.u32 %v119346, %v119342
%v119350 = vadd.s32 %v119347, %v119342
%v119352 = vshll.u32 %v119347, 15
%v119353 = vshrl.u32 %v119347, 17
%v119354 = vor.u32 %v119353, %v119352
%v119355 = vxor.u32 %v119354, %v119350
%v119358 = vadd.s32 %v119355, %v119350
%v119360 = vshll.u32 %v119355, 26
%v119361 = vshrl.u32 %v119355, 6
%v119362 = vor.u32 %v119361, %v119360
%v119363 = vxor.u32 %v119362, %v119358
%v119366 = vadd.s32 %v119363, %v119358
%v119370 = vadd.s32 %v119366, %v10
%v119372 = vshll.u32 %v119363, 6
%v119373 = vshrl.u32 %v119363, 26
%v119374 = vor.u32 %v119373, %v119372
%v119375 = vxor.u32 %v119374, %v119366
%v119378 = vadd.s32 %v119375, %v9
%v119382 = vadd.s32 3, %v119378
%v119386 = vadd.s32 %v119382, %v119370
%v119388 = vshll.u32 %v119382, 17
%v119389 = vshrl.u32 %v119382, 15
%v119390 = vor.u32 %v119389, %v119388
%v119391 = vxor.u32 %v119390, %v119386
%v119394 = vadd.s32 %v119391, %v119386
%v119396 = vshll.u32 %v119391, 29
%v119397 = vshrl.u32 %v119391, 3
%v119398 = vor.u32 %v119397, %v119396
%v119399 = vxor.u32 %v119398, %v119394
%v119402 = vadd.s32 %v119399, %v119394
%v119404 = vshll.u32 %v119399, 16
%v119405 = vshrl.u32 %v119399, 16
%v119406 = vor.u32 %v119405, %v119404
%v119407 = vxor.u32 %v119406, %v119402
%v119410 = vadd.s32 %v119407, %v119402
%v119414 = vadd.s32 %v119410, %v9
%v119416 = vshll.u32 %v119407, 24
%v119417 = vshrl.u32 %v119407, 8
%v119418 = vor.u32 %v119417, %v119416
%v119419 = vxor.u32 %v119418, %v119410
%v119422 = vadd.s32 %v119419, %v8
%v119426 = vadd.s32 4, %v119422
%v119430 = vadd.s32 %v119426, %v119414
%v119432 = vshll.u32 %v119426, 13
%v119433 = vshrl.u32 %v119426, 19
%v119434 = vor.u32 %v119433, %v119432
%v119435 = vxor.u32 %v119434, %v119430
%v119438 = vadd.s32 %v119435, %v119430
%v119440 = vshll.u32 %v119435, 15
%v119441 = vshrl.u32 %v119435, 17
%v119442 = vor.u32 %v119441, %v119440
%v119443 = vxor.u32 %v119442, %v119438
%v119446 = vadd.s32 %v119443, %v119438
%v119448 = vshll.u32 %v119443, 26
%v119449 = vshrl.u32 %v119443, 6
%v119450 = vor.u32 %v119449, %v119448
%v119451 = vxor.u32 %v119450, %v119446
%v119454 = vadd.s32 %v119451, %v119446
%v119458 = vadd.s32 %v119454, %v8
%v119460 = vshll.u32 %v119451, 6
%v119461 = vshrl.u32 %v119451, 26
%v119462 = vor.u32 %v119461, %v119460
%v119463 = vxor.u32 %v119462, %v119454
%v119466 = vadd.s32 %v119463, %v10
%v119470 = vadd.s32 5, %v119466
%v119472 = vxor.u32 %v119470, %v119458
%v119473 = vand.u32.u8 255, %v119472
%v119474 = vand.u32 65535, %v119473
%v119475 = vshrl.u32 %v119474, 1
%v119476 = vor.u32 16256, %v119475
%v119477 = vand.u32.u16 65535, %v119476
%v120382 = vadd.low.f32.bf16 -1.0, %v119477
%v119486 = vmul.f32 2.0, %v120382
%v119490 = vadd.f32 -0.99609375, %v119486
%v119494 = vmax.f32 %v119490, -0.99609375
%v119496 = vand.u32 2147483647, %v119494
%vm119499 = vcmp.eq.f32.partialorder %v119496, 1.0
%v119504 = vmul.f32 inf, %v119494
%v119506 = vxor.u32 2147483648, %v119494
%v119509 = vmul.f32 %v119506, %v119494
%v119511 = vadd.f32 1.0, %v119509
%v119512 = vlog2.pop %v119511
%v119513 = vmul.f32 0.6931472, %v119512
%v119514 = vmul.f32 -0.5, %v119509
%v119515 = vadd.f32 1.0, %v119514
%v119516 = vmul.f32 %v119515, %v119509
%v119517 = vand.u32 2147483647, %v119509
%vm119518 = vcmp.lt.f32.partialorder %v119517, 0.0004427343
%v119519 = vsel /*vm=*/%vm119518, /*on_true_vy=*/%v119516, /*on_false_vx=*/%v119513
%v119520 = vxor.u32 2147483648, %v119519
%vm119523 = vcmp.lt.f32.partialorder %v119520, 5.0
%v119528 = vsel /*vm=*/%vm119523, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v120408
%v119532 = vsel /*vm=*/%vm119523, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v120409
%v119536 = vsel /*vm=*/%vm119523, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v120410
%v119540 = vsel /*vm=*/%vm119523, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v120411
%v119544 = vsel /*vm=*/%vm119523, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v120412
%v119548 = vsel /*vm=*/%vm119523, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v120413
%v119552 = vsel /*vm=*/%vm119523, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v120414
%v119556 = vsel /*vm=*/%vm119523, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v120415
%v119560 = vsel /*vm=*/%vm119523, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v120416
%v119564 = vadd.f32 -2.5, %v119520
%v119566 = vrsqrt.pop %v119520
%v119567 = vmul.f32 %v119566, %v119520
%vm119568 = vcmp.eq.f32.partialorder %v119520, inf
%v119569 = vsel /*vm=*/%vm119568, /*on_true_vy=*/%v119520, /*on_false_vx=*/%v119567
%vm119570 = vcmp.eq.f32.partialorder %v119520, 0.0
%v119571 = vand.u32 2147483648, %v119520
%v119572 = vsel /*vm=*/%vm119570, /*on_true_vy=*/%v119571, /*on_false_vx=*/%v119569
%v119575 = vadd.f32 -3.0, %v119572
%v119579 = vsel /*vm=*/%vm119523, /*on_true_vy=*/%v119564, /*on_false_vx=*/%v119575
%v119583 = vmul.f32 %v119579, %v119560
%v119587 = vadd.f32 %v119583, %v119556
%v119591 = vmul.f32 %v119587, %v119579
%v119595 = vadd.f32 %v119591, %v119552
%v119599 = vmul.f32 %v119595, %v119579
%v119603 = vadd.f32 %v119599, %v119548
%v119607 = vmul.f32 %v119603, %v119579
%v119611 = vadd.f32 %v119607, %v119544
%v119615 = vmul.f32 %v119611, %v119579
%v119619 = vadd.f32 %v119615, %v119540
%v119623 = vmul.f32 %v119619, %v119579
%v119627 = vadd.f32 %v119623, %v119536
%v119631 = vmul.f32 %v119627, %v119579
%v119635 = vadd.f32 %v119631, %v119532
%v119639 = vmul.f32 %v119635, %v119579
%v119643 = vadd.f32 %v119639, %v119528
%v119647 = vmul.f32 %v119643, %v119494
%v119651 = vsel /*vm=*/%vm119499, /*on_true_vy=*/%v119504, /*on_false_vx=*/%v119647
%v119655 = vmul.f32 1.4140625, %v119651
%v119658 = vpack.c.bf16 %v120417, %v119655
%120383 = vst [vmem:[%s280 + $0x3fc] sm:$0xf] /*vst_source=*/%v119658
%s119661 = scalar_lea.sflag [#allocation2], %s278
%s120392 = sshll.u32 %s120407, 11
%s119674 = scalar_lea.hbm %s7, %s120392
%s119675 = sshll.u32 %s280, 4
%s119676 = int_to_ptr.vmem [resolvable:$true] %s119675
%s120418 = smov 2048 /* materialized constant */
%s120419 = smov 16384 /* materialized constant */
%s120420 = smov 128 /* materialized constant */
%119681 = dma.vmem_to_hbm [thread:$0]  /*vmem=*/%s119676, /*size_in_granules=*/16384, /*hbm=*/%s119674, /*dst_syncflagno=*/%s119661, /*src_stride=*/%s120418, /*dst_stride=*/%s120419, /*steps_per_stride=*/%s120420 /* 
base_bounds: (8, 256, 1)
dynamic_base_bounds: (8, 256, 1)
window_bounds: (8, 32, 1)
iteration_bounds: (1, 8, 1)
strides: (8, 32, 1)
pad_low: (0, 0, 0)
pad_high: (0, 0, 0)
element_size_in_bytes: 2048 */

%p120388 = scmp.lt.s32.totalorder %s120399, 2
%p120395 = scmp.ge.s32.totalorder %s120399, 2
%s120389 = sadd.s32 4294967294, %s120399
%s119687 = sand.u32 1, %s120389 /* smod.u32 w/div 2 */
%s119688 = scalar_lea.sflag [#allocation2], %s119687

%120394 = dma.done (%p120395), %s119688, 16384 /* pipeline-emitter-dma-wait */

%s19 = sadd.s32 1, %s120399

%p16 = scmp.ge.s32.totalorder %s19, 10 /* loop exit test */
%s120397 = smov %s19 /* copy for cssa */
%s120401 = smov %s37 /* copy for cssa */
%s120405 = smov %s120403 /* copy for cssa :: phi copy :: iteration index, stage = 0 iter bound = 1 */
%s120422 = smov %s120405 /* copy for cssa :: phi copy :: iteration index, stage = 0 iter bound = 1 */
%s120426 = smov %s120401 /* copy for cssa */
%s120430 = smov %s120397 /* copy for cssa */

%18 = sbr.rel (!%p16) target = $region50

%119693 = vsyncpa [#allocation2], 1

%119695 = vsyncpa [#allocation2 + $0x1], 1
