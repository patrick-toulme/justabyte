
%s0 = inlined_call_operand.vmem [shape: u32[2048], index: 0, kind: output, shape index: {0}] /* operand 0 */
%s1 = inlined_call_operand.vmem [shape: u32[2048], index: 1, kind: output, shape index: {1}] /* operand 1 */
%v3 = vlaneseq
%v6 = vand.u32 65535, %v3
%v9 = vmul.u32 128, %v6
%v11 = vand.u32 65535, %v9
%v15 = vshrl.u32 %v3, 16
%v18 = vmul.u32 128, %v15
%v20 = vshrl.u32 %v9, 16
%v23 = vadd.s32 %v20, %v18
%v25 = vand.u32 65535, %v23
%v26 = vshll.u32 %v25, 16
%v27 = vor.u32 %v26, %v11
%v28 = vshrl.u32 %v23, 16
%v29 = vshrl.u32 %v25, 16
%v32 = vadd.s32 %v29, %v28
%35 = vst [vmem:[%s0] sm:$0xff] /*vst_source=*/%v27
%36 = vst [vmem:[%s1] sm:$0xff] /*vst_source=*/%v32
%v39 = vadd.s32 1024, %v3
%v40 = vand.u32 65535, %v39
%v43 = vmul.u32 128, %v40
%v45 = vand.u32 65535, %v43
%v49 = vshrl.u32 %v39, 16
%v52 = vmul.u32 128, %v49
%v54 = vshrl.u32 %v43, 16
%v57 = vadd.s32 %v54, %v52
%v59 = vand.u32 65535, %v57
%v60 = vshll.u32 %v59, 16
%v61 = vor.u32 %v60, %v45
%v62 = vshrl.u32 %v57, 16
%v63 = vshrl.u32 %v59, 16
%v66 = vadd.s32 %v63, %v62
%73 = vst [vmem:[%s0 + $0x8] sm:$0xff] /*vst_source=*/%v61
%74 = vst [vmem:[%s1 + $0x8] sm:$0xff] /*vst_source=*/%v66
