// Slow dumping with stack traces? Try building with --dynamic_mode=fully
$region0: #{fusion.21}
  #allocation3 [shape = 's32[1]{0}', space=sflag, size = 0x4, tag = 'scoped memory for fusion.21'] (stack0)
  %s0 = inlined_call_operand.<no memory space> [shape: u32[], index: 0, kind: input, shape index: {}] /* operand 0 */ (stack1)
  %s1 = inlined_call_operand.<no memory space> [shape: u32[], index: 1, kind: input, shape index: {}] /* operand 1 */ (stack1)
  %s2 = inlined_call_operand.<no memory space> [shape: u32[], index: 2, kind: input, shape index: {}] /* operand 2 */ (stack1)
  %s3 = inlined_call_operand.vmem [shape: u32[2048], index: 3, kind: input, shape index: {}] /* operand 3 */ (stack1)
  %s4 = inlined_call_operand.vmem [shape: u32[8], index: 4, kind: input, shape index: {}] /* operand 4 */ (stack1)
  %s5 = inlined_call_operand.vmem [shape: u32[2048], index: 5, kind: input, shape index: {}] /* operand 5 */ (stack1)
  %s6 = inlined_call_operand.vmem [shape: u32[8], index: 6, kind: input, shape index: {}] /* operand 6 */ (stack1)
  %s7 = inlined_call_operand.hbm [shape: bf16[8,2048,128], index: 7, kind: output, shape index: {}] /* operand 7 */ (stack2)
  %v8 = vstv %s0 (stack3)
  %v9 = vstv %s1 (stack3)
  %v10 = vstv %s2 (stack3)
  $region1: #{fusion.21} parent=0
    #allocation0 [shape = 'u8[1048576]{0}', space=vmem, size = 0x100000, tag = 'operand span for operand 7'] (stack4)
    #allocation1 [shape = 's32[2]{0}', space=sflag, size = 0x8, tag = 'scoped memory for fusion.21'] (stack5)
    #allocation2 [shape = 's32[2]{0}', space=sflag, size = 0x8, tag = 'scoped memory for fusion.21'] (stack6)
    %11 = vsyncpa [#allocation2], 0 (stack7)
    %s12 = scalar_lea.sflag [#allocation2], 1 (stack8)
    %13 = vsyncpa %s12, 0 (stack7)
    loop: start=0, step=1, limit=10
    $region2: #{fusion.21} parent=1 // loop_pre_header
      _
    $region3: #{fusion.21} parent=1 // loop_header
      %s15 = sphi 0, %s19 /* iteration index, stage = 0 */ (stack9)
      %p16 = scmp.ge.s32.totalorder %s15, 10 /* loop exit test */ (stack10)
      %s22 = sphi 0, %s41 /* iteration index, stage = 0 iter bound = 0 */ (stack11)
      %s23 = sphi 0, %s37 /* iteration index, stage = 0 iter bound = 1 */ (stack11)
      %s24 = sphi 0, %s33 /* iteration index, stage = 0 iter bound = 2 */ (stack11)
      %s25 = sphi 0, %s22 /* iteration index, stage = 1 iter bound = 0 */ (stack11)
      %s26 = sphi 0, %s23 /* iteration index, stage = 1 iter bound = 1 */ (stack11)
      %s27 = sphi 0, %s24 /* iteration index, stage = 1 iter bound = 2 */ (stack11)
      %s48 = sphi 0, %s50 (stack12)
      %s51 = sphi 0, %s48 (stack13)
      %s78 = sphi 0, %s80 (stack12)
      %s81 = sphi 0, %s78 (stack13)
      %s108 = sphi 0, %s110 (stack12)
      %s111 = sphi 0, %s108 (stack13)
      %s138 = sphi 0, %s140 (stack12)
      %s141 = sphi 0, %s138 (stack13)
    $region4: #{fusion.21} parent=1 // loop_header_branch
      %18 = sbr.rel (%p16) target = $region8 (stack14)
    $region5: #{fusion.21} parent=1 // loop_body
      %s31 = sadd.s32 1, %s24 (stack15)
      %p32 = scmp.ge.s32.totalorder %s31, 1 (stack16)
      %s33 = scalar_select /*predicate=*/%p32, /*on_true=*/0, /*on_false=*/%s31 (stack17)
      %s34 = sadd.s32 1, %s23 (stack15)
      %s35 = scalar_select /*predicate=*/%p32, /*on_true=*/%s34, /*on_false=*/%s23 (stack18)
      %p36 = scmp.ge.s32.totalorder %s35, 8 (stack16)
      %s37 = scalar_select /*predicate=*/%p36, /*on_true=*/0, /*on_false=*/%s35 (stack17)
      %s38 = sadd.s32 1, %s22 (stack15)
      %s39 = scalar_select /*predicate=*/%p36, /*on_true=*/%s38, /*on_false=*/%s22 (stack18)
      %p40 = scmp.ge.s32.totalorder %s39, 1 (stack16)
      %s41 = scalar_select /*predicate=*/%p40, /*on_true=*/0, /*on_false=*/%s39 (stack17)
      %s42 = smul.addr %s23, 256 (stack19)
      %s43 = sshrl.u32 %s42, 10 (stack20)
      %s44 = smul.addr %s37, 256 (stack21)
      %s45 = sshrl.u32 %s44, 10 (stack22)
      %s46 = ssub.s32 %s43, %s45 (stack23)
      %p47 = scmp.eq.s32.totalorder %s46, 0 (stack24)
      %s49 = sadd.s32 %s48, 1 (stack25)
      %s50 = scalar_select /*predicate=*/%p47, /*on_true=*/%s48, /*on_false=*/%s49 (stack26)
      %p56 = scmp.ne.s32.totalorder %s48, %s51 (stack27)
      %p57 = scmp.eq.s32.totalorder %s15, 0 (stack28)
      %p58 = por %p56, %p57 (stack29)
      %s72 = smul.addr %s22, 8 (stack19)
      %s73 = sshrl.u32 %s72, 7 (stack20)
      %s74 = smul.addr %s41, 8 (stack21)
      %s75 = sshrl.u32 %s74, 7 (stack22)
      %s76 = ssub.s32 %s73, %s75 (stack23)
      %p77 = scmp.eq.s32.totalorder %s76, 0 (stack24)
      %s79 = sadd.s32 %s78, 1 (stack25)
      %s80 = scalar_select /*predicate=*/%p77, /*on_true=*/%s78, /*on_false=*/%s79 (stack26)
      %p86 = scmp.ne.s32.totalorder %s78, %s81 (stack27)
      %p87 = scmp.eq.s32.totalorder %s15, 0 (stack28)
      %p88 = por %p86, %p87 (stack29)
      %s102 = smul.addr %s23, 256 (stack19)
      %s103 = sshrl.u32 %s102, 10 (stack20)
      %s104 = smul.addr %s37, 256 (stack21)
      %s105 = sshrl.u32 %s104, 10 (stack22)
      %s106 = ssub.s32 %s103, %s105 (stack23)
      %p107 = scmp.eq.s32.totalorder %s106, 0 (stack24)
      %s109 = sadd.s32 %s108, 1 (stack25)
      %s110 = scalar_select /*predicate=*/%p107, /*on_true=*/%s108, /*on_false=*/%s109 (stack26)
      %p116 = scmp.ne.s32.totalorder %s108, %s111 (stack27)
      %p117 = scmp.eq.s32.totalorder %s15, 0 (stack28)
      %p118 = por %p116, %p117 (stack29)
      %s132 = smul.addr %s22, 8 (stack19)
      %s133 = sshrl.u32 %s132, 7 (stack20)
      %s134 = smul.addr %s41, 8 (stack21)
      %s135 = sshrl.u32 %s134, 7 (stack22)
      %s136 = ssub.s32 %s133, %s135 (stack23)
      %p137 = scmp.eq.s32.totalorder %s136, 0 (stack24)
      %s139 = sadd.s32 %s138, 1 (stack25)
      %s140 = scalar_select /*predicate=*/%p137, /*on_true=*/%s138, /*on_false=*/%s139 (stack26)
      %p146 = scmp.ne.s32.totalorder %s138, %s141 (stack27)
      %p147 = scmp.eq.s32.totalorder %s15, 0 (stack28)
      %p148 = por %p146, %p147 (stack29)
      %p162 = scmp.le.s32.totalorder 1, %s15 (stack30)
      %p163 = scmp.lt.s32.totalorder %s15, 9 (stack31)
      %p164 = pnand %p162, %p163 (stack32)
      // Predicated region
      $region9: #{fusion.21} parent=5 // pred_check
        _
      $region10: #{fusion.21} parent=5 // pred_check_branch
        %167 = sbr.rel (%p164) target = $region12 (stack33)
      $region11: #{fusion.21} parent=5 // pred_region
        _
      $region12: #{fusion.21} parent=5 // pred_fallthru
        _
      %p169 = scmp.lt.s32.totalorder %s15, 8 (stack34)
      // Predicated region
      $region13: #{fusion.21} parent=5 // pred_check
        %p170 = pneg %p169 (stack35)
      $region14: #{fusion.21} parent=5 // pred_check_branch
        %172 = sbr.rel (%p170) target = $region16 (stack36)
      $region15: #{fusion.21} parent=5 // pred_region
        // Predicated region
        $region17: #{fusion.21} parent=15 // pred_check
          %p173 = pneg %p58 (stack37)
        $region18: #{fusion.21} parent=15 // pred_check_branch
          %175 = sbr.rel (%p173) target = $region20 (stack38)
        $region19: #{fusion.21} parent=15 // pred_region
          _
        $region20: #{fusion.21} parent=15 // pred_fallthru
          _
        // Predicated region
        $region21: #{fusion.21} parent=15 // pred_check
          %p188 = pneg %p88 (stack37)
        $region22: #{fusion.21} parent=15 // pred_check_branch
          %190 = sbr.rel (%p188) target = $region24 (stack38)
        $region23: #{fusion.21} parent=15 // pred_region
          _
        $region24: #{fusion.21} parent=15 // pred_fallthru
          _
        // Predicated region
        $region25: #{fusion.21} parent=15 // pred_check
          %p202 = pneg %p118 (stack37)
        $region26: #{fusion.21} parent=15 // pred_check_branch
          %204 = sbr.rel (%p202) target = $region28 (stack38)
        $region27: #{fusion.21} parent=15 // pred_region
          _
        $region28: #{fusion.21} parent=15 // pred_fallthru
          _
        // Predicated region
        $region29: #{fusion.21} parent=15 // pred_check
          %p217 = pneg %p148 (stack37)
        $region30: #{fusion.21} parent=15 // pred_check_branch
          %219 = sbr.rel (%p217) target = $region32 (stack38)
        $region31: #{fusion.21} parent=15 // pred_region
          _
        $region32: #{fusion.21} parent=15 // pred_fallthru
          _
      $region16: #{fusion.21} parent=5 // pred_fallthru
        _
      %p231 = scmp.le.s32.totalorder 1, %s15 (stack39)
      %p232 = scmp.lt.s32.totalorder %s15, 9 (stack40)
      %p233 = pnand %p231, %p232 (stack41)
      // Predicated region
      $region33: #{fusion.21} parent=5 // pred_check
        _
      $region34: #{fusion.21} parent=5 // pred_check_branch
        %236 = sbr.rel (%p233) target = $region36 (stack42)
      $region35: #{fusion.21} parent=5 // pred_region
        %s237 = ssub.s32 %s15, 1 (stack43)
        %s278 = sand.u32 %s237, 1 /* smod.u32 w/div 2 */ (stack44)
        %s279 = smul.addr %s278, 1024 (stack45)
        %s280 = scalar_lea.vmem [#allocation0], %s279 (stack46)
        %s338 = smul.addr %s26, 32 (stack47)
        %s339 = smul.addr %s338, 8 (stack48)
        %s341 = smul.addr %s25, 8 (stack47)
        %s346 = smul.addr %s26, 32 (stack47)
        %s347 = smul.addr %s346, 8 (stack48)
        %s349 = smul.addr %s25, 8 (stack47)
        %s353 = sshrl.u32 %s339, 10 (stack49)
        %p354 = scmp.lt.s32.totalorder 1, %s353 (stack50)
        %s355 = scalar_select /*predicate=*/%p354, /*on_true=*/1, /*on_false=*/%s353 (stack51)
        %s356 = sand.u32 %s339, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s357 = sshrl.u32 %s356, 7 (stack53)
        %s358 = sand.u32 %s356, 127 /* smod.u32 w/div 128 */ (stack54)
        %s359 = smul.addr %s355, 8 (stack55)
        %s360 = scalar_lea.vmem %s3, %s359 (stack56)
        %s362 = scalar_lea.vmem %s360, %s357 (stack57)
        %v363 = vld [vmem:[%s362] ss:$0 sm:$0xff] (stack58)
        %s364 = sand.u32 %s358, 255 (stack59)
        %s366 = sor.u32 256, %s364 (stack60)
        %367 = vbcast.lane.b32.xlu0 %v363, %s366 (stack61)
        %v368 = vpop.permute.xlu0 %367 (stack62)
        %s369 = sshrl.u32 %s341, 7 (stack49)
        %p370 = scmp.lt.s32.totalorder 0, %s369 (stack50)
        %s371 = scalar_select /*predicate=*/%p370, /*on_true=*/0, /*on_false=*/%s369 (stack51)
        %s372 = sand.u32 %s341, 127 /* smod.u32 w/div 128 */ (stack52)
        %s373 = sshrl.u32 %s372, 7 (stack53)
        %s374 = sand.u32 %s372, 127 /* smod.u32 w/div 128 */ (stack54)
        %s375 = scalar_lea.vmem %s4, %s371 (stack56)
        %s377 = scalar_lea.vmem %s375, %s373 (stack57)
        %v378 = vld [vmem:[%s377] ss:$0 sm:$0xff] (stack58)
        %379 = vbcast.lane.b32.xlu0 %v378, %s374 (stack63)
        %v380 = vpop.permute.xlu0 %379 (stack64)
        %s381 = sshrl.u32 %s347, 10 (stack49)
        %p382 = scmp.lt.s32.totalorder 1, %s381 (stack50)
        %s383 = scalar_select /*predicate=*/%p382, /*on_true=*/1, /*on_false=*/%s381 (stack51)
        %s384 = sand.u32 %s347, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s385 = sshrl.u32 %s384, 7 (stack53)
        %s386 = sand.u32 %s384, 127 /* smod.u32 w/div 128 */ (stack54)
        %s387 = smul.addr %s383, 8 (stack55)
        %s388 = scalar_lea.vmem %s5, %s387 (stack56)
        %s390 = scalar_lea.vmem %s388, %s385 (stack57)
        %v391 = vld [vmem:[%s390] ss:$0 sm:$0xff] (stack58)
        %s392 = sand.u32 %s386, 255 (stack59)
        %s394 = sor.u32 256, %s392 (stack60)
        %395 = vbcast.lane.b32.xlu0 %v391, %s394 (stack61)
        %v396 = vpop.permute.xlu0 %395 (stack62)
        %s397 = sshrl.u32 %s349, 7 (stack49)
        %p398 = scmp.lt.s32.totalorder 0, %s397 (stack50)
        %s399 = scalar_select /*predicate=*/%p398, /*on_true=*/0, /*on_false=*/%s397 (stack51)
        %s400 = sand.u32 %s349, 127 /* smod.u32 w/div 128 */ (stack52)
        %s401 = sshrl.u32 %s400, 7 (stack53)
        %s402 = sand.u32 %s400, 127 /* smod.u32 w/div 128 */ (stack54)
        %s403 = scalar_lea.vmem %s6, %s399 (stack56)
        %s405 = scalar_lea.vmem %s403, %s401 (stack57)
        %v406 = vld [vmem:[%s405] ss:$0 sm:$0xff] (stack58)
        %407 = vbcast.lane.b32.xlu0 %v406, %s402 (stack63)
        %v408 = vpop.permute.xlu0 %407 (stack64)
        %v411 = vadd.s32 %v408, %v396 (stack65)
        %s413 = smul.u32 128, %s27 (stack66)
        %v414 = vlaneseq (stack67)
        %v415 = vand.u32 %v414, 127 (stack68)
        %v416 = vstv %s413 (stack69)
        %v417 = vadd.s32 %v415, %v416 (stack70)
        %v421 = vadd.s32 %v411, %v417 (stack65)
        %vm425 = vcmp.lt.u32.totalorder %v421, %v411 (stack71)
        %vm430 = vcmp.lt.u32.totalorder %v411, %v408 (stack71)
        %v435 = vadd.s32 %v380, %v368 (stack65)
        %v439 = vadd.s32 %v435, 1 (stack65)
        %v443 = vsel /*vm=*/%vm430, /*on_true_vy=*/%v439, /*on_false_vx=*/%v435 (stack72)
        %v447 = vadd.s32 %v443, 1 (stack65)
        %v451 = vsel /*vm=*/%vm425, /*on_true_vy=*/%v447, /*on_false_vx=*/%v443 (stack72)
        %v456 = vadd.s32 %v451, %v10 (stack65)
        %v460 = vadd.s32 %v421, %v9 (stack65)
        %v464 = vadd.s32 %v456, %v460 (stack65)
        %v466 = vshll.u32 %v460, 13 (stack73)
        %v467 = vshrl.u32 %v460, 19 (stack74)
        %v468 = vor.u32 %v466, %v467 (stack75)
        %v469 = vxor.u32 %v464, %v468 (stack76)
        %v472 = vadd.s32 %v464, %v469 (stack65)
        %v474 = vshll.u32 %v469, 15 (stack73)
        %v475 = vshrl.u32 %v469, 17 (stack74)
        %v476 = vor.u32 %v474, %v475 (stack75)
        %v477 = vxor.u32 %v472, %v476 (stack76)
        %v480 = vadd.s32 %v472, %v477 (stack65)
        %v482 = vshll.u32 %v477, 26 (stack73)
        %v483 = vshrl.u32 %v477, 6 (stack74)
        %v484 = vor.u32 %v482, %v483 (stack75)
        %v485 = vxor.u32 %v480, %v484 (stack76)
        %v488 = vadd.s32 %v480, %v485 (stack65)
        %v492 = vadd.s32 %v488, %v9 (stack65)
        %v494 = vshll.u32 %v485, 6 (stack73)
        %v495 = vshrl.u32 %v485, 26 (stack74)
        %v496 = vor.u32 %v494, %v495 (stack75)
        %v497 = vxor.u32 %v488, %v496 (stack76)
        %v500 = vadd.s32 %v497, %v8 (stack65)
        %v504 = vadd.s32 %v500, 1 (stack65)
        %v508 = vadd.s32 %v492, %v504 (stack65)
        %v510 = vshll.u32 %v504, 17 (stack73)
        %v511 = vshrl.u32 %v504, 15 (stack74)
        %v512 = vor.u32 %v510, %v511 (stack75)
        %v513 = vxor.u32 %v508, %v512 (stack76)
        %v516 = vadd.s32 %v508, %v513 (stack65)
        %v518 = vshll.u32 %v513, 29 (stack73)
        %v519 = vshrl.u32 %v513, 3 (stack74)
        %v520 = vor.u32 %v518, %v519 (stack75)
        %v521 = vxor.u32 %v516, %v520 (stack76)
        %v524 = vadd.s32 %v516, %v521 (stack65)
        %v526 = vshll.u32 %v521, 16 (stack73)
        %v527 = vshrl.u32 %v521, 16 (stack74)
        %v528 = vor.u32 %v526, %v527 (stack75)
        %v529 = vxor.u32 %v524, %v528 (stack76)
        %v532 = vadd.s32 %v524, %v529 (stack65)
        %v536 = vadd.s32 %v532, %v8 (stack65)
        %v538 = vshll.u32 %v529, 24 (stack73)
        %v539 = vshrl.u32 %v529, 8 (stack74)
        %v540 = vor.u32 %v538, %v539 (stack75)
        %v541 = vxor.u32 %v532, %v540 (stack76)
        %v544 = vadd.s32 %v541, %v10 (stack65)
        %v548 = vadd.s32 %v544, 2 (stack65)
        %v552 = vadd.s32 %v536, %v548 (stack65)
        %v554 = vshll.u32 %v548, 13 (stack73)
        %v555 = vshrl.u32 %v548, 19 (stack74)
        %v556 = vor.u32 %v554, %v555 (stack75)
        %v557 = vxor.u32 %v552, %v556 (stack76)
        %v560 = vadd.s32 %v552, %v557 (stack65)
        %v562 = vshll.u32 %v557, 15 (stack73)
        %v563 = vshrl.u32 %v557, 17 (stack74)
        %v564 = vor.u32 %v562, %v563 (stack75)
        %v565 = vxor.u32 %v560, %v564 (stack76)
        %v568 = vadd.s32 %v560, %v565 (stack65)
        %v570 = vshll.u32 %v565, 26 (stack73)
        %v571 = vshrl.u32 %v565, 6 (stack74)
        %v572 = vor.u32 %v570, %v571 (stack75)
        %v573 = vxor.u32 %v568, %v572 (stack76)
        %v576 = vadd.s32 %v568, %v573 (stack65)
        %v580 = vadd.s32 %v576, %v10 (stack65)
        %v582 = vshll.u32 %v573, 6 (stack73)
        %v583 = vshrl.u32 %v573, 26 (stack74)
        %v584 = vor.u32 %v582, %v583 (stack75)
        %v585 = vxor.u32 %v576, %v584 (stack76)
        %v588 = vadd.s32 %v585, %v9 (stack65)
        %v592 = vadd.s32 %v588, 3 (stack65)
        %v596 = vadd.s32 %v580, %v592 (stack65)
        %v598 = vshll.u32 %v592, 17 (stack73)
        %v599 = vshrl.u32 %v592, 15 (stack74)
        %v600 = vor.u32 %v598, %v599 (stack75)
        %v601 = vxor.u32 %v596, %v600 (stack76)
        %v604 = vadd.s32 %v596, %v601 (stack65)
        %v606 = vshll.u32 %v601, 29 (stack73)
        %v607 = vshrl.u32 %v601, 3 (stack74)
        %v608 = vor.u32 %v606, %v607 (stack75)
        %v609 = vxor.u32 %v604, %v608 (stack76)
        %v612 = vadd.s32 %v604, %v609 (stack65)
        %v614 = vshll.u32 %v609, 16 (stack73)
        %v615 = vshrl.u32 %v609, 16 (stack74)
        %v616 = vor.u32 %v614, %v615 (stack75)
        %v617 = vxor.u32 %v612, %v616 (stack76)
        %v620 = vadd.s32 %v612, %v617 (stack65)
        %v624 = vadd.s32 %v620, %v9 (stack65)
        %v626 = vshll.u32 %v617, 24 (stack73)
        %v627 = vshrl.u32 %v617, 8 (stack74)
        %v628 = vor.u32 %v626, %v627 (stack75)
        %v629 = vxor.u32 %v620, %v628 (stack76)
        %v632 = vadd.s32 %v629, %v8 (stack65)
        %v636 = vadd.s32 %v632, 4 (stack65)
        %v640 = vadd.s32 %v624, %v636 (stack65)
        %v642 = vshll.u32 %v636, 13 (stack73)
        %v643 = vshrl.u32 %v636, 19 (stack74)
        %v644 = vor.u32 %v642, %v643 (stack75)
        %v645 = vxor.u32 %v640, %v644 (stack76)
        %v648 = vadd.s32 %v640, %v645 (stack65)
        %v650 = vshll.u32 %v645, 15 (stack73)
        %v651 = vshrl.u32 %v645, 17 (stack74)
        %v652 = vor.u32 %v650, %v651 (stack75)
        %v653 = vxor.u32 %v648, %v652 (stack76)
        %v656 = vadd.s32 %v648, %v653 (stack65)
        %v658 = vshll.u32 %v653, 26 (stack73)
        %v659 = vshrl.u32 %v653, 6 (stack74)
        %v660 = vor.u32 %v658, %v659 (stack75)
        %v661 = vxor.u32 %v656, %v660 (stack76)
        %v664 = vadd.s32 %v656, %v661 (stack65)
        %v668 = vadd.s32 %v664, %v8 (stack65)
        %v670 = vshll.u32 %v661, 6 (stack73)
        %v671 = vshrl.u32 %v661, 26 (stack74)
        %v672 = vor.u32 %v670, %v671 (stack75)
        %v673 = vxor.u32 %v664, %v672 (stack76)
        %v676 = vadd.s32 %v673, %v10 (stack65)
        %v680 = vadd.s32 %v676, 5 (stack65)
        %v682 = vxor.u32 %v668, %v680 (stack76)
        %v683 = vand.u32.u8 %v682, 255 (stack77)
        %v684 = vand.u32 %v683, 65535 (stack78)
        %v685 = vshrl.u32 %v684, 1 (stack79)
        %v686 = vor.u32 %v685, 16256 (stack75)
        %v687 = vand.u32.u16 %v686, 65535 (stack80)
        %v688 = vunpack.i.l.bf16 %v687 (stack81)
        %v692 = vadd.f32 %v688, -1.0 (stack82)
        %v696 = vmul.f32 %v692, 2.0 (stack83)
        %v700 = vadd.f32 %v696, -0.99609375 (stack82)
        %v704 = vmax.f32 -0.99609375, %v700 (stack84)
        %v706 = vand.u32 2147483647, %v704 (stack85)
        %vm709 = vcmp.eq.f32.partialorder %v706, 1.0 (stack86)
        %v714 = vmul.f32 %v704, inf (stack83)
        %v716 = vxor.u32 %v704, 2147483648 (stack87)
        %v719 = vmul.f32 %v704, %v716 (stack83)
        %v721 = vadd.f32 %v719, 1.0 (stack88)
        %v722 = vlog2.pop %v721 (stack89)
        %v723 = vmul.f32 %v722, 0.6931472 (stack90)
        %v724 = vmul.f32 -0.5, %v719 (stack91)
        %v725 = vadd.f32 %v724, 1.0 (stack92)
        %v726 = vmul.f32 %v725, %v719 (stack93)
        %v727 = vand.u32 2147483647, %v719 (stack94)
        %vm728 = vcmp.lt.f32.partialorder %v727, 0.0004427343 (stack95)
        %v729 = vsel /*vm=*/%vm728, /*on_true_vy=*/%v726, /*on_false_vx=*/%v723 (stack96)
        %v730 = vxor.u32 %v729, 2147483648 (stack87)
        %vm733 = vcmp.lt.f32.partialorder %v730, 5.0 (stack86)
        %v738 = vsel /*vm=*/%vm733, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v742 = vsel /*vm=*/%vm733, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v746 = vsel /*vm=*/%vm733, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v750 = vsel /*vm=*/%vm733, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v754 = vsel /*vm=*/%vm733, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v758 = vsel /*vm=*/%vm733, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v762 = vsel /*vm=*/%vm733, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v766 = vsel /*vm=*/%vm733, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v770 = vsel /*vm=*/%vm733, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v774 = vadd.f32 %v730, -2.5 (stack82)
        %v776 = vrsqrt.pop %v730 (stack97)
        %v777 = vmul.f32 %v730, %v776 (stack98)
        %vm778 = vcmp.eq.f32.partialorder %v730, inf (stack99)
        %v779 = vsel /*vm=*/%vm778, /*on_true_vy=*/%v730, /*on_false_vx=*/%v777 (stack100)
        %vm780 = vcmp.eq.f32.partialorder %v730, 0.0 (stack101)
        %v781 = vand.u32 %v730, 2147483648 (stack102)
        %v782 = vsel /*vm=*/%vm780, /*on_true_vy=*/%v781, /*on_false_vx=*/%v779 (stack103)
        %v785 = vadd.f32 %v782, -3.0 (stack82)
        %v789 = vsel /*vm=*/%vm733, /*on_true_vy=*/%v774, /*on_false_vx=*/%v785 (stack72)
        %v793 = vmul.f32 %v770, %v789 (stack83)
        %v797 = vadd.f32 %v766, %v793 (stack82)
        %v801 = vmul.f32 %v797, %v789 (stack83)
        %v805 = vadd.f32 %v762, %v801 (stack82)
        %v809 = vmul.f32 %v805, %v789 (stack83)
        %v813 = vadd.f32 %v758, %v809 (stack82)
        %v817 = vmul.f32 %v813, %v789 (stack83)
        %v821 = vadd.f32 %v754, %v817 (stack82)
        %v825 = vmul.f32 %v821, %v789 (stack83)
        %v829 = vadd.f32 %v750, %v825 (stack82)
        %v833 = vmul.f32 %v829, %v789 (stack83)
        %v837 = vadd.f32 %v746, %v833 (stack82)
        %v841 = vmul.f32 %v837, %v789 (stack83)
        %v845 = vadd.f32 %v742, %v841 (stack82)
        %v849 = vmul.f32 %v845, %v789 (stack83)
        %v853 = vadd.f32 %v738, %v849 (stack82)
        %v857 = vmul.f32 %v853, %v704 (stack83)
        %v861 = vsel /*vm=*/%vm709, /*on_true_vy=*/%v714, /*on_false_vx=*/%v857 (stack72)
        %v865 = vmul.f32 %v861, 1.4140625 (stack83)
        %v867 = vpack.c.bf16 0.0, %v865 (stack104)
        %868 = vst [vmem:[%s280] sm:$0xf] /*vst_source=*/%v867 (stack105)
        %s869 = sadd.s32 %s341, 1 (stack106)
        %s870 = sshrl.u32 %s869, 7 (stack49)
        %p871 = scmp.lt.s32.totalorder 0, %s870 (stack50)
        %s872 = scalar_select /*predicate=*/%p871, /*on_true=*/0, /*on_false=*/%s870 (stack51)
        %s873 = sand.u32 %s869, 127 /* smod.u32 w/div 128 */ (stack52)
        %s874 = sshrl.u32 %s873, 7 (stack53)
        %s875 = sand.u32 %s873, 127 /* smod.u32 w/div 128 */ (stack54)
        %s876 = scalar_lea.vmem %s4, %s872 (stack56)
        %s878 = scalar_lea.vmem %s876, %s874 (stack57)
        %v879 = vld [vmem:[%s878] ss:$0 sm:$0xff] (stack58)
        %880 = vbcast.lane.b32.xlu0 %v879, %s875 (stack63)
        %v881 = vpop.permute.xlu0 %880 (stack64)
        %s882 = sadd.s32 %s349, 1 (stack106)
        %s883 = sshrl.u32 %s882, 7 (stack49)
        %p884 = scmp.lt.s32.totalorder 0, %s883 (stack50)
        %s885 = scalar_select /*predicate=*/%p884, /*on_true=*/0, /*on_false=*/%s883 (stack51)
        %s886 = sand.u32 %s882, 127 /* smod.u32 w/div 128 */ (stack52)
        %s887 = sshrl.u32 %s886, 7 (stack53)
        %s888 = sand.u32 %s886, 127 /* smod.u32 w/div 128 */ (stack54)
        %s889 = scalar_lea.vmem %s6, %s885 (stack56)
        %s891 = scalar_lea.vmem %s889, %s887 (stack57)
        %v892 = vld [vmem:[%s891] ss:$0 sm:$0xff] (stack58)
        %893 = vbcast.lane.b32.xlu0 %v892, %s888 (stack63)
        %v894 = vpop.permute.xlu0 %893 (stack64)
        %v897 = vadd.s32 %v894, %v396 (stack65)
        %s899 = smul.u32 128, %s27 (stack66)
        %v900 = vlaneseq (stack67)
        %v901 = vand.u32 %v900, 127 (stack68)
        %v902 = vstv %s899 (stack69)
        %v903 = vadd.s32 %v901, %v902 (stack70)
        %v907 = vadd.s32 %v897, %v903 (stack65)
        %vm911 = vcmp.lt.u32.totalorder %v907, %v897 (stack71)
        %vm916 = vcmp.lt.u32.totalorder %v897, %v894 (stack71)
        %v921 = vadd.s32 %v881, %v368 (stack65)
        %v925 = vadd.s32 %v921, 1 (stack65)
        %v929 = vsel /*vm=*/%vm916, /*on_true_vy=*/%v925, /*on_false_vx=*/%v921 (stack72)
        %v933 = vadd.s32 %v929, 1 (stack65)
        %v937 = vsel /*vm=*/%vm911, /*on_true_vy=*/%v933, /*on_false_vx=*/%v929 (stack72)
        %v942 = vadd.s32 %v937, %v10 (stack65)
        %v946 = vadd.s32 %v907, %v9 (stack65)
        %v950 = vadd.s32 %v942, %v946 (stack65)
        %v952 = vshll.u32 %v946, 13 (stack73)
        %v953 = vshrl.u32 %v946, 19 (stack74)
        %v954 = vor.u32 %v952, %v953 (stack75)
        %v955 = vxor.u32 %v950, %v954 (stack76)
        %v958 = vadd.s32 %v950, %v955 (stack65)
        %v960 = vshll.u32 %v955, 15 (stack73)
        %v961 = vshrl.u32 %v955, 17 (stack74)
        %v962 = vor.u32 %v960, %v961 (stack75)
        %v963 = vxor.u32 %v958, %v962 (stack76)
        %v966 = vadd.s32 %v958, %v963 (stack65)
        %v968 = vshll.u32 %v963, 26 (stack73)
        %v969 = vshrl.u32 %v963, 6 (stack74)
        %v970 = vor.u32 %v968, %v969 (stack75)
        %v971 = vxor.u32 %v966, %v970 (stack76)
        %v974 = vadd.s32 %v966, %v971 (stack65)
        %v978 = vadd.s32 %v974, %v9 (stack65)
        %v980 = vshll.u32 %v971, 6 (stack73)
        %v981 = vshrl.u32 %v971, 26 (stack74)
        %v982 = vor.u32 %v980, %v981 (stack75)
        %v983 = vxor.u32 %v974, %v982 (stack76)
        %v986 = vadd.s32 %v983, %v8 (stack65)
        %v990 = vadd.s32 %v986, 1 (stack65)
        %v994 = vadd.s32 %v978, %v990 (stack65)
        %v996 = vshll.u32 %v990, 17 (stack73)
        %v997 = vshrl.u32 %v990, 15 (stack74)
        %v998 = vor.u32 %v996, %v997 (stack75)
        %v999 = vxor.u32 %v994, %v998 (stack76)
        %v1002 = vadd.s32 %v994, %v999 (stack65)
        %v1004 = vshll.u32 %v999, 29 (stack73)
        %v1005 = vshrl.u32 %v999, 3 (stack74)
        %v1006 = vor.u32 %v1004, %v1005 (stack75)
        %v1007 = vxor.u32 %v1002, %v1006 (stack76)
        %v1010 = vadd.s32 %v1002, %v1007 (stack65)
        %v1012 = vshll.u32 %v1007, 16 (stack73)
        %v1013 = vshrl.u32 %v1007, 16 (stack74)
        %v1014 = vor.u32 %v1012, %v1013 (stack75)
        %v1015 = vxor.u32 %v1010, %v1014 (stack76)
        %v1018 = vadd.s32 %v1010, %v1015 (stack65)
        %v1022 = vadd.s32 %v1018, %v8 (stack65)
        %v1024 = vshll.u32 %v1015, 24 (stack73)
        %v1025 = vshrl.u32 %v1015, 8 (stack74)
        %v1026 = vor.u32 %v1024, %v1025 (stack75)
        %v1027 = vxor.u32 %v1018, %v1026 (stack76)
        %v1030 = vadd.s32 %v1027, %v10 (stack65)
        %v1034 = vadd.s32 %v1030, 2 (stack65)
        %v1038 = vadd.s32 %v1022, %v1034 (stack65)
        %v1040 = vshll.u32 %v1034, 13 (stack73)
        %v1041 = vshrl.u32 %v1034, 19 (stack74)
        %v1042 = vor.u32 %v1040, %v1041 (stack75)
        %v1043 = vxor.u32 %v1038, %v1042 (stack76)
        %v1046 = vadd.s32 %v1038, %v1043 (stack65)
        %v1048 = vshll.u32 %v1043, 15 (stack73)
        %v1049 = vshrl.u32 %v1043, 17 (stack74)
        %v1050 = vor.u32 %v1048, %v1049 (stack75)
        %v1051 = vxor.u32 %v1046, %v1050 (stack76)
        %v1054 = vadd.s32 %v1046, %v1051 (stack65)
        %v1056 = vshll.u32 %v1051, 26 (stack73)
        %v1057 = vshrl.u32 %v1051, 6 (stack74)
        %v1058 = vor.u32 %v1056, %v1057 (stack75)
        %v1059 = vxor.u32 %v1054, %v1058 (stack76)
        %v1062 = vadd.s32 %v1054, %v1059 (stack65)
        %v1066 = vadd.s32 %v1062, %v10 (stack65)
        %v1068 = vshll.u32 %v1059, 6 (stack73)
        %v1069 = vshrl.u32 %v1059, 26 (stack74)
        %v1070 = vor.u32 %v1068, %v1069 (stack75)
        %v1071 = vxor.u32 %v1062, %v1070 (stack76)
        %v1074 = vadd.s32 %v1071, %v9 (stack65)
        %v1078 = vadd.s32 %v1074, 3 (stack65)
        %v1082 = vadd.s32 %v1066, %v1078 (stack65)
        %v1084 = vshll.u32 %v1078, 17 (stack73)
        %v1085 = vshrl.u32 %v1078, 15 (stack74)
        %v1086 = vor.u32 %v1084, %v1085 (stack75)
        %v1087 = vxor.u32 %v1082, %v1086 (stack76)
        %v1090 = vadd.s32 %v1082, %v1087 (stack65)
        %v1092 = vshll.u32 %v1087, 29 (stack73)
        %v1093 = vshrl.u32 %v1087, 3 (stack74)
        %v1094 = vor.u32 %v1092, %v1093 (stack75)
        %v1095 = vxor.u32 %v1090, %v1094 (stack76)
        %v1098 = vadd.s32 %v1090, %v1095 (stack65)
        %v1100 = vshll.u32 %v1095, 16 (stack73)
        %v1101 = vshrl.u32 %v1095, 16 (stack74)
        %v1102 = vor.u32 %v1100, %v1101 (stack75)
        %v1103 = vxor.u32 %v1098, %v1102 (stack76)
        %v1106 = vadd.s32 %v1098, %v1103 (stack65)
        %v1110 = vadd.s32 %v1106, %v9 (stack65)
        %v1112 = vshll.u32 %v1103, 24 (stack73)
        %v1113 = vshrl.u32 %v1103, 8 (stack74)
        %v1114 = vor.u32 %v1112, %v1113 (stack75)
        %v1115 = vxor.u32 %v1106, %v1114 (stack76)
        %v1118 = vadd.s32 %v1115, %v8 (stack65)
        %v1122 = vadd.s32 %v1118, 4 (stack65)
        %v1126 = vadd.s32 %v1110, %v1122 (stack65)
        %v1128 = vshll.u32 %v1122, 13 (stack73)
        %v1129 = vshrl.u32 %v1122, 19 (stack74)
        %v1130 = vor.u32 %v1128, %v1129 (stack75)
        %v1131 = vxor.u32 %v1126, %v1130 (stack76)
        %v1134 = vadd.s32 %v1126, %v1131 (stack65)
        %v1136 = vshll.u32 %v1131, 15 (stack73)
        %v1137 = vshrl.u32 %v1131, 17 (stack74)
        %v1138 = vor.u32 %v1136, %v1137 (stack75)
        %v1139 = vxor.u32 %v1134, %v1138 (stack76)
        %v1142 = vadd.s32 %v1134, %v1139 (stack65)
        %v1144 = vshll.u32 %v1139, 26 (stack73)
        %v1145 = vshrl.u32 %v1139, 6 (stack74)
        %v1146 = vor.u32 %v1144, %v1145 (stack75)
        %v1147 = vxor.u32 %v1142, %v1146 (stack76)
        %v1150 = vadd.s32 %v1142, %v1147 (stack65)
        %v1154 = vadd.s32 %v1150, %v8 (stack65)
        %v1156 = vshll.u32 %v1147, 6 (stack73)
        %v1157 = vshrl.u32 %v1147, 26 (stack74)
        %v1158 = vor.u32 %v1156, %v1157 (stack75)
        %v1159 = vxor.u32 %v1150, %v1158 (stack76)
        %v1162 = vadd.s32 %v1159, %v10 (stack65)
        %v1166 = vadd.s32 %v1162, 5 (stack65)
        %v1168 = vxor.u32 %v1154, %v1166 (stack76)
        %v1169 = vand.u32.u8 %v1168, 255 (stack77)
        %v1170 = vand.u32 %v1169, 65535 (stack78)
        %v1171 = vshrl.u32 %v1170, 1 (stack79)
        %v1172 = vor.u32 %v1171, 16256 (stack75)
        %v1173 = vand.u32.u16 %v1172, 65535 (stack80)
        %v1174 = vunpack.i.l.bf16 %v1173 (stack81)
        %v1178 = vadd.f32 %v1174, -1.0 (stack82)
        %v1182 = vmul.f32 %v1178, 2.0 (stack83)
        %v1186 = vadd.f32 %v1182, -0.99609375 (stack82)
        %v1190 = vmax.f32 -0.99609375, %v1186 (stack84)
        %v1192 = vand.u32 2147483647, %v1190 (stack85)
        %vm1195 = vcmp.eq.f32.partialorder %v1192, 1.0 (stack86)
        %v1200 = vmul.f32 %v1190, inf (stack83)
        %v1202 = vxor.u32 %v1190, 2147483648 (stack87)
        %v1205 = vmul.f32 %v1190, %v1202 (stack83)
        %v1207 = vadd.f32 %v1205, 1.0 (stack88)
        %v1208 = vlog2.pop %v1207 (stack89)
        %v1209 = vmul.f32 %v1208, 0.6931472 (stack90)
        %v1210 = vmul.f32 -0.5, %v1205 (stack91)
        %v1211 = vadd.f32 %v1210, 1.0 (stack92)
        %v1212 = vmul.f32 %v1211, %v1205 (stack93)
        %v1213 = vand.u32 2147483647, %v1205 (stack94)
        %vm1214 = vcmp.lt.f32.partialorder %v1213, 0.0004427343 (stack95)
        %v1215 = vsel /*vm=*/%vm1214, /*on_true_vy=*/%v1212, /*on_false_vx=*/%v1209 (stack96)
        %v1216 = vxor.u32 %v1215, 2147483648 (stack87)
        %vm1219 = vcmp.lt.f32.partialorder %v1216, 5.0 (stack86)
        %v1224 = vsel /*vm=*/%vm1219, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v1228 = vsel /*vm=*/%vm1219, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v1232 = vsel /*vm=*/%vm1219, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v1236 = vsel /*vm=*/%vm1219, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v1240 = vsel /*vm=*/%vm1219, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v1244 = vsel /*vm=*/%vm1219, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v1248 = vsel /*vm=*/%vm1219, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v1252 = vsel /*vm=*/%vm1219, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v1256 = vsel /*vm=*/%vm1219, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v1260 = vadd.f32 %v1216, -2.5 (stack82)
        %v1262 = vrsqrt.pop %v1216 (stack97)
        %v1263 = vmul.f32 %v1216, %v1262 (stack98)
        %vm1264 = vcmp.eq.f32.partialorder %v1216, inf (stack99)
        %v1265 = vsel /*vm=*/%vm1264, /*on_true_vy=*/%v1216, /*on_false_vx=*/%v1263 (stack100)
        %vm1266 = vcmp.eq.f32.partialorder %v1216, 0.0 (stack101)
        %v1267 = vand.u32 %v1216, 2147483648 (stack102)
        %v1268 = vsel /*vm=*/%vm1266, /*on_true_vy=*/%v1267, /*on_false_vx=*/%v1265 (stack103)
        %v1271 = vadd.f32 %v1268, -3.0 (stack82)
        %v1275 = vsel /*vm=*/%vm1219, /*on_true_vy=*/%v1260, /*on_false_vx=*/%v1271 (stack72)
        %v1279 = vmul.f32 %v1256, %v1275 (stack83)
        %v1283 = vadd.f32 %v1252, %v1279 (stack82)
        %v1287 = vmul.f32 %v1283, %v1275 (stack83)
        %v1291 = vadd.f32 %v1248, %v1287 (stack82)
        %v1295 = vmul.f32 %v1291, %v1275 (stack83)
        %v1299 = vadd.f32 %v1244, %v1295 (stack82)
        %v1303 = vmul.f32 %v1299, %v1275 (stack83)
        %v1307 = vadd.f32 %v1240, %v1303 (stack82)
        %v1311 = vmul.f32 %v1307, %v1275 (stack83)
        %v1315 = vadd.f32 %v1236, %v1311 (stack82)
        %v1319 = vmul.f32 %v1315, %v1275 (stack83)
        %v1323 = vadd.f32 %v1232, %v1319 (stack82)
        %v1327 = vmul.f32 %v1323, %v1275 (stack83)
        %v1331 = vadd.f32 %v1228, %v1327 (stack82)
        %v1335 = vmul.f32 %v1331, %v1275 (stack83)
        %v1339 = vadd.f32 %v1224, %v1335 (stack82)
        %v1343 = vmul.f32 %v1339, %v1190 (stack83)
        %v1347 = vsel /*vm=*/%vm1195, /*on_true_vy=*/%v1200, /*on_false_vx=*/%v1343 (stack72)
        %v1351 = vmul.f32 %v1347, 1.4140625 (stack83)
        %s1353 = scalar_lea.vmem %s280, 128 [#allocation0] (stack107)
        %v1354 = vpack.c.bf16 0.0, %v1351 (stack104)
        %1355 = vst [vmem:[%s1353] sm:$0xf] /*vst_source=*/%v1354 (stack105)
        %s1356 = sadd.s32 %s341, 2 (stack106)
        %s1357 = sshrl.u32 %s1356, 7 (stack49)
        %p1358 = scmp.lt.s32.totalorder 0, %s1357 (stack50)
        %s1359 = scalar_select /*predicate=*/%p1358, /*on_true=*/0, /*on_false=*/%s1357 (stack51)
        %s1360 = sand.u32 %s1356, 127 /* smod.u32 w/div 128 */ (stack52)
        %s1361 = sshrl.u32 %s1360, 7 (stack53)
        %s1362 = sand.u32 %s1360, 127 /* smod.u32 w/div 128 */ (stack54)
        %s1363 = scalar_lea.vmem %s4, %s1359 (stack56)
        %s1365 = scalar_lea.vmem %s1363, %s1361 (stack57)
        %v1366 = vld [vmem:[%s1365] ss:$0 sm:$0xff] (stack58)
        %1367 = vbcast.lane.b32.xlu0 %v1366, %s1362 (stack63)
        %v1368 = vpop.permute.xlu0 %1367 (stack64)
        %s1369 = sadd.s32 %s349, 2 (stack106)
        %s1370 = sshrl.u32 %s1369, 7 (stack49)
        %p1371 = scmp.lt.s32.totalorder 0, %s1370 (stack50)
        %s1372 = scalar_select /*predicate=*/%p1371, /*on_true=*/0, /*on_false=*/%s1370 (stack51)
        %s1373 = sand.u32 %s1369, 127 /* smod.u32 w/div 128 */ (stack52)
        %s1374 = sshrl.u32 %s1373, 7 (stack53)
        %s1375 = sand.u32 %s1373, 127 /* smod.u32 w/div 128 */ (stack54)
        %s1376 = scalar_lea.vmem %s6, %s1372 (stack56)
        %s1378 = scalar_lea.vmem %s1376, %s1374 (stack57)
        %v1379 = vld [vmem:[%s1378] ss:$0 sm:$0xff] (stack58)
        %1380 = vbcast.lane.b32.xlu0 %v1379, %s1375 (stack63)
        %v1381 = vpop.permute.xlu0 %1380 (stack64)
        %v1384 = vadd.s32 %v1381, %v396 (stack65)
        %s1386 = smul.u32 128, %s27 (stack66)
        %v1387 = vlaneseq (stack67)
        %v1388 = vand.u32 %v1387, 127 (stack68)
        %v1389 = vstv %s1386 (stack69)
        %v1390 = vadd.s32 %v1388, %v1389 (stack70)
        %v1394 = vadd.s32 %v1384, %v1390 (stack65)
        %vm1398 = vcmp.lt.u32.totalorder %v1394, %v1384 (stack71)
        %vm1403 = vcmp.lt.u32.totalorder %v1384, %v1381 (stack71)
        %v1408 = vadd.s32 %v1368, %v368 (stack65)
        %v1412 = vadd.s32 %v1408, 1 (stack65)
        %v1416 = vsel /*vm=*/%vm1403, /*on_true_vy=*/%v1412, /*on_false_vx=*/%v1408 (stack72)
        %v1420 = vadd.s32 %v1416, 1 (stack65)
        %v1424 = vsel /*vm=*/%vm1398, /*on_true_vy=*/%v1420, /*on_false_vx=*/%v1416 (stack72)
        %v1429 = vadd.s32 %v1424, %v10 (stack65)
        %v1433 = vadd.s32 %v1394, %v9 (stack65)
        %v1437 = vadd.s32 %v1429, %v1433 (stack65)
        %v1439 = vshll.u32 %v1433, 13 (stack73)
        %v1440 = vshrl.u32 %v1433, 19 (stack74)
        %v1441 = vor.u32 %v1439, %v1440 (stack75)
        %v1442 = vxor.u32 %v1437, %v1441 (stack76)
        %v1445 = vadd.s32 %v1437, %v1442 (stack65)
        %v1447 = vshll.u32 %v1442, 15 (stack73)
        %v1448 = vshrl.u32 %v1442, 17 (stack74)
        %v1449 = vor.u32 %v1447, %v1448 (stack75)
        %v1450 = vxor.u32 %v1445, %v1449 (stack76)
        %v1453 = vadd.s32 %v1445, %v1450 (stack65)
        %v1455 = vshll.u32 %v1450, 26 (stack73)
        %v1456 = vshrl.u32 %v1450, 6 (stack74)
        %v1457 = vor.u32 %v1455, %v1456 (stack75)
        %v1458 = vxor.u32 %v1453, %v1457 (stack76)
        %v1461 = vadd.s32 %v1453, %v1458 (stack65)
        %v1465 = vadd.s32 %v1461, %v9 (stack65)
        %v1467 = vshll.u32 %v1458, 6 (stack73)
        %v1468 = vshrl.u32 %v1458, 26 (stack74)
        %v1469 = vor.u32 %v1467, %v1468 (stack75)
        %v1470 = vxor.u32 %v1461, %v1469 (stack76)
        %v1473 = vadd.s32 %v1470, %v8 (stack65)
        %v1477 = vadd.s32 %v1473, 1 (stack65)
        %v1481 = vadd.s32 %v1465, %v1477 (stack65)
        %v1483 = vshll.u32 %v1477, 17 (stack73)
        %v1484 = vshrl.u32 %v1477, 15 (stack74)
        %v1485 = vor.u32 %v1483, %v1484 (stack75)
        %v1486 = vxor.u32 %v1481, %v1485 (stack76)
        %v1489 = vadd.s32 %v1481, %v1486 (stack65)
        %v1491 = vshll.u32 %v1486, 29 (stack73)
        %v1492 = vshrl.u32 %v1486, 3 (stack74)
        %v1493 = vor.u32 %v1491, %v1492 (stack75)
        %v1494 = vxor.u32 %v1489, %v1493 (stack76)
        %v1497 = vadd.s32 %v1489, %v1494 (stack65)
        %v1499 = vshll.u32 %v1494, 16 (stack73)
        %v1500 = vshrl.u32 %v1494, 16 (stack74)
        %v1501 = vor.u32 %v1499, %v1500 (stack75)
        %v1502 = vxor.u32 %v1497, %v1501 (stack76)
        %v1505 = vadd.s32 %v1497, %v1502 (stack65)
        %v1509 = vadd.s32 %v1505, %v8 (stack65)
        %v1511 = vshll.u32 %v1502, 24 (stack73)
        %v1512 = vshrl.u32 %v1502, 8 (stack74)
        %v1513 = vor.u32 %v1511, %v1512 (stack75)
        %v1514 = vxor.u32 %v1505, %v1513 (stack76)
        %v1517 = vadd.s32 %v1514, %v10 (stack65)
        %v1521 = vadd.s32 %v1517, 2 (stack65)
        %v1525 = vadd.s32 %v1509, %v1521 (stack65)
        %v1527 = vshll.u32 %v1521, 13 (stack73)
        %v1528 = vshrl.u32 %v1521, 19 (stack74)
        %v1529 = vor.u32 %v1527, %v1528 (stack75)
        %v1530 = vxor.u32 %v1525, %v1529 (stack76)
        %v1533 = vadd.s32 %v1525, %v1530 (stack65)
        %v1535 = vshll.u32 %v1530, 15 (stack73)
        %v1536 = vshrl.u32 %v1530, 17 (stack74)
        %v1537 = vor.u32 %v1535, %v1536 (stack75)
        %v1538 = vxor.u32 %v1533, %v1537 (stack76)
        %v1541 = vadd.s32 %v1533, %v1538 (stack65)
        %v1543 = vshll.u32 %v1538, 26 (stack73)
        %v1544 = vshrl.u32 %v1538, 6 (stack74)
        %v1545 = vor.u32 %v1543, %v1544 (stack75)
        %v1546 = vxor.u32 %v1541, %v1545 (stack76)
        %v1549 = vadd.s32 %v1541, %v1546 (stack65)
        %v1553 = vadd.s32 %v1549, %v10 (stack65)
        %v1555 = vshll.u32 %v1546, 6 (stack73)
        %v1556 = vshrl.u32 %v1546, 26 (stack74)
        %v1557 = vor.u32 %v1555, %v1556 (stack75)
        %v1558 = vxor.u32 %v1549, %v1557 (stack76)
        %v1561 = vadd.s32 %v1558, %v9 (stack65)
        %v1565 = vadd.s32 %v1561, 3 (stack65)
        %v1569 = vadd.s32 %v1553, %v1565 (stack65)
        %v1571 = vshll.u32 %v1565, 17 (stack73)
        %v1572 = vshrl.u32 %v1565, 15 (stack74)
        %v1573 = vor.u32 %v1571, %v1572 (stack75)
        %v1574 = vxor.u32 %v1569, %v1573 (stack76)
        %v1577 = vadd.s32 %v1569, %v1574 (stack65)
        %v1579 = vshll.u32 %v1574, 29 (stack73)
        %v1580 = vshrl.u32 %v1574, 3 (stack74)
        %v1581 = vor.u32 %v1579, %v1580 (stack75)
        %v1582 = vxor.u32 %v1577, %v1581 (stack76)
        %v1585 = vadd.s32 %v1577, %v1582 (stack65)
        %v1587 = vshll.u32 %v1582, 16 (stack73)
        %v1588 = vshrl.u32 %v1582, 16 (stack74)
        %v1589 = vor.u32 %v1587, %v1588 (stack75)
        %v1590 = vxor.u32 %v1585, %v1589 (stack76)
        %v1593 = vadd.s32 %v1585, %v1590 (stack65)
        %v1597 = vadd.s32 %v1593, %v9 (stack65)
        %v1599 = vshll.u32 %v1590, 24 (stack73)
        %v1600 = vshrl.u32 %v1590, 8 (stack74)
        %v1601 = vor.u32 %v1599, %v1600 (stack75)
        %v1602 = vxor.u32 %v1593, %v1601 (stack76)
        %v1605 = vadd.s32 %v1602, %v8 (stack65)
        %v1609 = vadd.s32 %v1605, 4 (stack65)
        %v1613 = vadd.s32 %v1597, %v1609 (stack65)
        %v1615 = vshll.u32 %v1609, 13 (stack73)
        %v1616 = vshrl.u32 %v1609, 19 (stack74)
        %v1617 = vor.u32 %v1615, %v1616 (stack75)
        %v1618 = vxor.u32 %v1613, %v1617 (stack76)
        %v1621 = vadd.s32 %v1613, %v1618 (stack65)
        %v1623 = vshll.u32 %v1618, 15 (stack73)
        %v1624 = vshrl.u32 %v1618, 17 (stack74)
        %v1625 = vor.u32 %v1623, %v1624 (stack75)
        %v1626 = vxor.u32 %v1621, %v1625 (stack76)
        %v1629 = vadd.s32 %v1621, %v1626 (stack65)
        %v1631 = vshll.u32 %v1626, 26 (stack73)
        %v1632 = vshrl.u32 %v1626, 6 (stack74)
        %v1633 = vor.u32 %v1631, %v1632 (stack75)
        %v1634 = vxor.u32 %v1629, %v1633 (stack76)
        %v1637 = vadd.s32 %v1629, %v1634 (stack65)
        %v1641 = vadd.s32 %v1637, %v8 (stack65)
        %v1643 = vshll.u32 %v1634, 6 (stack73)
        %v1644 = vshrl.u32 %v1634, 26 (stack74)
        %v1645 = vor.u32 %v1643, %v1644 (stack75)
        %v1646 = vxor.u32 %v1637, %v1645 (stack76)
        %v1649 = vadd.s32 %v1646, %v10 (stack65)
        %v1653 = vadd.s32 %v1649, 5 (stack65)
        %v1655 = vxor.u32 %v1641, %v1653 (stack76)
        %v1656 = vand.u32.u8 %v1655, 255 (stack77)
        %v1657 = vand.u32 %v1656, 65535 (stack78)
        %v1658 = vshrl.u32 %v1657, 1 (stack79)
        %v1659 = vor.u32 %v1658, 16256 (stack75)
        %v1660 = vand.u32.u16 %v1659, 65535 (stack80)
        %v1661 = vunpack.i.l.bf16 %v1660 (stack81)
        %v1665 = vadd.f32 %v1661, -1.0 (stack82)
        %v1669 = vmul.f32 %v1665, 2.0 (stack83)
        %v1673 = vadd.f32 %v1669, -0.99609375 (stack82)
        %v1677 = vmax.f32 -0.99609375, %v1673 (stack84)
        %v1679 = vand.u32 2147483647, %v1677 (stack85)
        %vm1682 = vcmp.eq.f32.partialorder %v1679, 1.0 (stack86)
        %v1687 = vmul.f32 %v1677, inf (stack83)
        %v1689 = vxor.u32 %v1677, 2147483648 (stack87)
        %v1692 = vmul.f32 %v1677, %v1689 (stack83)
        %v1694 = vadd.f32 %v1692, 1.0 (stack88)
        %v1695 = vlog2.pop %v1694 (stack89)
        %v1696 = vmul.f32 %v1695, 0.6931472 (stack90)
        %v1697 = vmul.f32 -0.5, %v1692 (stack91)
        %v1698 = vadd.f32 %v1697, 1.0 (stack92)
        %v1699 = vmul.f32 %v1698, %v1692 (stack93)
        %v1700 = vand.u32 2147483647, %v1692 (stack94)
        %vm1701 = vcmp.lt.f32.partialorder %v1700, 0.0004427343 (stack95)
        %v1702 = vsel /*vm=*/%vm1701, /*on_true_vy=*/%v1699, /*on_false_vx=*/%v1696 (stack96)
        %v1703 = vxor.u32 %v1702, 2147483648 (stack87)
        %vm1706 = vcmp.lt.f32.partialorder %v1703, 5.0 (stack86)
        %v1711 = vsel /*vm=*/%vm1706, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v1715 = vsel /*vm=*/%vm1706, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v1719 = vsel /*vm=*/%vm1706, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v1723 = vsel /*vm=*/%vm1706, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v1727 = vsel /*vm=*/%vm1706, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v1731 = vsel /*vm=*/%vm1706, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v1735 = vsel /*vm=*/%vm1706, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v1739 = vsel /*vm=*/%vm1706, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v1743 = vsel /*vm=*/%vm1706, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v1747 = vadd.f32 %v1703, -2.5 (stack82)
        %v1749 = vrsqrt.pop %v1703 (stack97)
        %v1750 = vmul.f32 %v1703, %v1749 (stack98)
        %vm1751 = vcmp.eq.f32.partialorder %v1703, inf (stack99)
        %v1752 = vsel /*vm=*/%vm1751, /*on_true_vy=*/%v1703, /*on_false_vx=*/%v1750 (stack100)
        %vm1753 = vcmp.eq.f32.partialorder %v1703, 0.0 (stack101)
        %v1754 = vand.u32 %v1703, 2147483648 (stack102)
        %v1755 = vsel /*vm=*/%vm1753, /*on_true_vy=*/%v1754, /*on_false_vx=*/%v1752 (stack103)
        %v1758 = vadd.f32 %v1755, -3.0 (stack82)
        %v1762 = vsel /*vm=*/%vm1706, /*on_true_vy=*/%v1747, /*on_false_vx=*/%v1758 (stack72)
        %v1766 = vmul.f32 %v1743, %v1762 (stack83)
        %v1770 = vadd.f32 %v1739, %v1766 (stack82)
        %v1774 = vmul.f32 %v1770, %v1762 (stack83)
        %v1778 = vadd.f32 %v1735, %v1774 (stack82)
        %v1782 = vmul.f32 %v1778, %v1762 (stack83)
        %v1786 = vadd.f32 %v1731, %v1782 (stack82)
        %v1790 = vmul.f32 %v1786, %v1762 (stack83)
        %v1794 = vadd.f32 %v1727, %v1790 (stack82)
        %v1798 = vmul.f32 %v1794, %v1762 (stack83)
        %v1802 = vadd.f32 %v1723, %v1798 (stack82)
        %v1806 = vmul.f32 %v1802, %v1762 (stack83)
        %v1810 = vadd.f32 %v1719, %v1806 (stack82)
        %v1814 = vmul.f32 %v1810, %v1762 (stack83)
        %v1818 = vadd.f32 %v1715, %v1814 (stack82)
        %v1822 = vmul.f32 %v1818, %v1762 (stack83)
        %v1826 = vadd.f32 %v1711, %v1822 (stack82)
        %v1830 = vmul.f32 %v1826, %v1677 (stack83)
        %v1834 = vsel /*vm=*/%vm1682, /*on_true_vy=*/%v1687, /*on_false_vx=*/%v1830 (stack72)
        %v1838 = vmul.f32 %v1834, 1.4140625 (stack83)
        %s1840 = scalar_lea.vmem %s280, 256 [#allocation0] (stack107)
        %v1841 = vpack.c.bf16 0.0, %v1838 (stack104)
        %1842 = vst [vmem:[%s1840] sm:$0xf] /*vst_source=*/%v1841 (stack105)
        %s1843 = sadd.s32 %s341, 3 (stack106)
        %s1844 = sshrl.u32 %s1843, 7 (stack49)
        %p1845 = scmp.lt.s32.totalorder 0, %s1844 (stack50)
        %s1846 = scalar_select /*predicate=*/%p1845, /*on_true=*/0, /*on_false=*/%s1844 (stack51)
        %s1847 = sand.u32 %s1843, 127 /* smod.u32 w/div 128 */ (stack52)
        %s1848 = sshrl.u32 %s1847, 7 (stack53)
        %s1849 = sand.u32 %s1847, 127 /* smod.u32 w/div 128 */ (stack54)
        %s1850 = scalar_lea.vmem %s4, %s1846 (stack56)
        %s1852 = scalar_lea.vmem %s1850, %s1848 (stack57)
        %v1853 = vld [vmem:[%s1852] ss:$0 sm:$0xff] (stack58)
        %1854 = vbcast.lane.b32.xlu0 %v1853, %s1849 (stack63)
        %v1855 = vpop.permute.xlu0 %1854 (stack64)
        %s1856 = sadd.s32 %s349, 3 (stack106)
        %s1857 = sshrl.u32 %s1856, 7 (stack49)
        %p1858 = scmp.lt.s32.totalorder 0, %s1857 (stack50)
        %s1859 = scalar_select /*predicate=*/%p1858, /*on_true=*/0, /*on_false=*/%s1857 (stack51)
        %s1860 = sand.u32 %s1856, 127 /* smod.u32 w/div 128 */ (stack52)
        %s1861 = sshrl.u32 %s1860, 7 (stack53)
        %s1862 = sand.u32 %s1860, 127 /* smod.u32 w/div 128 */ (stack54)
        %s1863 = scalar_lea.vmem %s6, %s1859 (stack56)
        %s1865 = scalar_lea.vmem %s1863, %s1861 (stack57)
        %v1866 = vld [vmem:[%s1865] ss:$0 sm:$0xff] (stack58)
        %1867 = vbcast.lane.b32.xlu0 %v1866, %s1862 (stack63)
        %v1868 = vpop.permute.xlu0 %1867 (stack64)
        %v1871 = vadd.s32 %v1868, %v396 (stack65)
        %s1873 = smul.u32 128, %s27 (stack66)
        %v1874 = vlaneseq (stack67)
        %v1875 = vand.u32 %v1874, 127 (stack68)
        %v1876 = vstv %s1873 (stack69)
        %v1877 = vadd.s32 %v1875, %v1876 (stack70)
        %v1881 = vadd.s32 %v1871, %v1877 (stack65)
        %vm1885 = vcmp.lt.u32.totalorder %v1881, %v1871 (stack71)
        %vm1890 = vcmp.lt.u32.totalorder %v1871, %v1868 (stack71)
        %v1895 = vadd.s32 %v1855, %v368 (stack65)
        %v1899 = vadd.s32 %v1895, 1 (stack65)
        %v1903 = vsel /*vm=*/%vm1890, /*on_true_vy=*/%v1899, /*on_false_vx=*/%v1895 (stack72)
        %v1907 = vadd.s32 %v1903, 1 (stack65)
        %v1911 = vsel /*vm=*/%vm1885, /*on_true_vy=*/%v1907, /*on_false_vx=*/%v1903 (stack72)
        %v1916 = vadd.s32 %v1911, %v10 (stack65)
        %v1920 = vadd.s32 %v1881, %v9 (stack65)
        %v1924 = vadd.s32 %v1916, %v1920 (stack65)
        %v1926 = vshll.u32 %v1920, 13 (stack73)
        %v1927 = vshrl.u32 %v1920, 19 (stack74)
        %v1928 = vor.u32 %v1926, %v1927 (stack75)
        %v1929 = vxor.u32 %v1924, %v1928 (stack76)
        %v1932 = vadd.s32 %v1924, %v1929 (stack65)
        %v1934 = vshll.u32 %v1929, 15 (stack73)
        %v1935 = vshrl.u32 %v1929, 17 (stack74)
        %v1936 = vor.u32 %v1934, %v1935 (stack75)
        %v1937 = vxor.u32 %v1932, %v1936 (stack76)
        %v1940 = vadd.s32 %v1932, %v1937 (stack65)
        %v1942 = vshll.u32 %v1937, 26 (stack73)
        %v1943 = vshrl.u32 %v1937, 6 (stack74)
        %v1944 = vor.u32 %v1942, %v1943 (stack75)
        %v1945 = vxor.u32 %v1940, %v1944 (stack76)
        %v1948 = vadd.s32 %v1940, %v1945 (stack65)
        %v1952 = vadd.s32 %v1948, %v9 (stack65)
        %v1954 = vshll.u32 %v1945, 6 (stack73)
        %v1955 = vshrl.u32 %v1945, 26 (stack74)
        %v1956 = vor.u32 %v1954, %v1955 (stack75)
        %v1957 = vxor.u32 %v1948, %v1956 (stack76)
        %v1960 = vadd.s32 %v1957, %v8 (stack65)
        %v1964 = vadd.s32 %v1960, 1 (stack65)
        %v1968 = vadd.s32 %v1952, %v1964 (stack65)
        %v1970 = vshll.u32 %v1964, 17 (stack73)
        %v1971 = vshrl.u32 %v1964, 15 (stack74)
        %v1972 = vor.u32 %v1970, %v1971 (stack75)
        %v1973 = vxor.u32 %v1968, %v1972 (stack76)
        %v1976 = vadd.s32 %v1968, %v1973 (stack65)
        %v1978 = vshll.u32 %v1973, 29 (stack73)
        %v1979 = vshrl.u32 %v1973, 3 (stack74)
        %v1980 = vor.u32 %v1978, %v1979 (stack75)
        %v1981 = vxor.u32 %v1976, %v1980 (stack76)
        %v1984 = vadd.s32 %v1976, %v1981 (stack65)
        %v1986 = vshll.u32 %v1981, 16 (stack73)
        %v1987 = vshrl.u32 %v1981, 16 (stack74)
        %v1988 = vor.u32 %v1986, %v1987 (stack75)
        %v1989 = vxor.u32 %v1984, %v1988 (stack76)
        %v1992 = vadd.s32 %v1984, %v1989 (stack65)
        %v1996 = vadd.s32 %v1992, %v8 (stack65)
        %v1998 = vshll.u32 %v1989, 24 (stack73)
        %v1999 = vshrl.u32 %v1989, 8 (stack74)
        %v2000 = vor.u32 %v1998, %v1999 (stack75)
        %v2001 = vxor.u32 %v1992, %v2000 (stack76)
        %v2004 = vadd.s32 %v2001, %v10 (stack65)
        %v2008 = vadd.s32 %v2004, 2 (stack65)
        %v2012 = vadd.s32 %v1996, %v2008 (stack65)
        %v2014 = vshll.u32 %v2008, 13 (stack73)
        %v2015 = vshrl.u32 %v2008, 19 (stack74)
        %v2016 = vor.u32 %v2014, %v2015 (stack75)
        %v2017 = vxor.u32 %v2012, %v2016 (stack76)
        %v2020 = vadd.s32 %v2012, %v2017 (stack65)
        %v2022 = vshll.u32 %v2017, 15 (stack73)
        %v2023 = vshrl.u32 %v2017, 17 (stack74)
        %v2024 = vor.u32 %v2022, %v2023 (stack75)
        %v2025 = vxor.u32 %v2020, %v2024 (stack76)
        %v2028 = vadd.s32 %v2020, %v2025 (stack65)
        %v2030 = vshll.u32 %v2025, 26 (stack73)
        %v2031 = vshrl.u32 %v2025, 6 (stack74)
        %v2032 = vor.u32 %v2030, %v2031 (stack75)
        %v2033 = vxor.u32 %v2028, %v2032 (stack76)
        %v2036 = vadd.s32 %v2028, %v2033 (stack65)
        %v2040 = vadd.s32 %v2036, %v10 (stack65)
        %v2042 = vshll.u32 %v2033, 6 (stack73)
        %v2043 = vshrl.u32 %v2033, 26 (stack74)
        %v2044 = vor.u32 %v2042, %v2043 (stack75)
        %v2045 = vxor.u32 %v2036, %v2044 (stack76)
        %v2048 = vadd.s32 %v2045, %v9 (stack65)
        %v2052 = vadd.s32 %v2048, 3 (stack65)
        %v2056 = vadd.s32 %v2040, %v2052 (stack65)
        %v2058 = vshll.u32 %v2052, 17 (stack73)
        %v2059 = vshrl.u32 %v2052, 15 (stack74)
        %v2060 = vor.u32 %v2058, %v2059 (stack75)
        %v2061 = vxor.u32 %v2056, %v2060 (stack76)
        %v2064 = vadd.s32 %v2056, %v2061 (stack65)
        %v2066 = vshll.u32 %v2061, 29 (stack73)
        %v2067 = vshrl.u32 %v2061, 3 (stack74)
        %v2068 = vor.u32 %v2066, %v2067 (stack75)
        %v2069 = vxor.u32 %v2064, %v2068 (stack76)
        %v2072 = vadd.s32 %v2064, %v2069 (stack65)
        %v2074 = vshll.u32 %v2069, 16 (stack73)
        %v2075 = vshrl.u32 %v2069, 16 (stack74)
        %v2076 = vor.u32 %v2074, %v2075 (stack75)
        %v2077 = vxor.u32 %v2072, %v2076 (stack76)
        %v2080 = vadd.s32 %v2072, %v2077 (stack65)
        %v2084 = vadd.s32 %v2080, %v9 (stack65)
        %v2086 = vshll.u32 %v2077, 24 (stack73)
        %v2087 = vshrl.u32 %v2077, 8 (stack74)
        %v2088 = vor.u32 %v2086, %v2087 (stack75)
        %v2089 = vxor.u32 %v2080, %v2088 (stack76)
        %v2092 = vadd.s32 %v2089, %v8 (stack65)
        %v2096 = vadd.s32 %v2092, 4 (stack65)
        %v2100 = vadd.s32 %v2084, %v2096 (stack65)
        %v2102 = vshll.u32 %v2096, 13 (stack73)
        %v2103 = vshrl.u32 %v2096, 19 (stack74)
        %v2104 = vor.u32 %v2102, %v2103 (stack75)
        %v2105 = vxor.u32 %v2100, %v2104 (stack76)
        %v2108 = vadd.s32 %v2100, %v2105 (stack65)
        %v2110 = vshll.u32 %v2105, 15 (stack73)
        %v2111 = vshrl.u32 %v2105, 17 (stack74)
        %v2112 = vor.u32 %v2110, %v2111 (stack75)
        %v2113 = vxor.u32 %v2108, %v2112 (stack76)
        %v2116 = vadd.s32 %v2108, %v2113 (stack65)
        %v2118 = vshll.u32 %v2113, 26 (stack73)
        %v2119 = vshrl.u32 %v2113, 6 (stack74)
        %v2120 = vor.u32 %v2118, %v2119 (stack75)
        %v2121 = vxor.u32 %v2116, %v2120 (stack76)
        %v2124 = vadd.s32 %v2116, %v2121 (stack65)
        %v2128 = vadd.s32 %v2124, %v8 (stack65)
        %v2130 = vshll.u32 %v2121, 6 (stack73)
        %v2131 = vshrl.u32 %v2121, 26 (stack74)
        %v2132 = vor.u32 %v2130, %v2131 (stack75)
        %v2133 = vxor.u32 %v2124, %v2132 (stack76)
        %v2136 = vadd.s32 %v2133, %v10 (stack65)
        %v2140 = vadd.s32 %v2136, 5 (stack65)
        %v2142 = vxor.u32 %v2128, %v2140 (stack76)
        %v2143 = vand.u32.u8 %v2142, 255 (stack77)
        %v2144 = vand.u32 %v2143, 65535 (stack78)
        %v2145 = vshrl.u32 %v2144, 1 (stack79)
        %v2146 = vor.u32 %v2145, 16256 (stack75)
        %v2147 = vand.u32.u16 %v2146, 65535 (stack80)
        %v2148 = vunpack.i.l.bf16 %v2147 (stack81)
        %v2152 = vadd.f32 %v2148, -1.0 (stack82)
        %v2156 = vmul.f32 %v2152, 2.0 (stack83)
        %v2160 = vadd.f32 %v2156, -0.99609375 (stack82)
        %v2164 = vmax.f32 -0.99609375, %v2160 (stack84)
        %v2166 = vand.u32 2147483647, %v2164 (stack85)
        %vm2169 = vcmp.eq.f32.partialorder %v2166, 1.0 (stack86)
        %v2174 = vmul.f32 %v2164, inf (stack83)
        %v2176 = vxor.u32 %v2164, 2147483648 (stack87)
        %v2179 = vmul.f32 %v2164, %v2176 (stack83)
        %v2181 = vadd.f32 %v2179, 1.0 (stack88)
        %v2182 = vlog2.pop %v2181 (stack89)
        %v2183 = vmul.f32 %v2182, 0.6931472 (stack90)
        %v2184 = vmul.f32 -0.5, %v2179 (stack91)
        %v2185 = vadd.f32 %v2184, 1.0 (stack92)
        %v2186 = vmul.f32 %v2185, %v2179 (stack93)
        %v2187 = vand.u32 2147483647, %v2179 (stack94)
        %vm2188 = vcmp.lt.f32.partialorder %v2187, 0.0004427343 (stack95)
        %v2189 = vsel /*vm=*/%vm2188, /*on_true_vy=*/%v2186, /*on_false_vx=*/%v2183 (stack96)
        %v2190 = vxor.u32 %v2189, 2147483648 (stack87)
        %vm2193 = vcmp.lt.f32.partialorder %v2190, 5.0 (stack86)
        %v2198 = vsel /*vm=*/%vm2193, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v2202 = vsel /*vm=*/%vm2193, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v2206 = vsel /*vm=*/%vm2193, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v2210 = vsel /*vm=*/%vm2193, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v2214 = vsel /*vm=*/%vm2193, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v2218 = vsel /*vm=*/%vm2193, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v2222 = vsel /*vm=*/%vm2193, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v2226 = vsel /*vm=*/%vm2193, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v2230 = vsel /*vm=*/%vm2193, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v2234 = vadd.f32 %v2190, -2.5 (stack82)
        %v2236 = vrsqrt.pop %v2190 (stack97)
        %v2237 = vmul.f32 %v2190, %v2236 (stack98)
        %vm2238 = vcmp.eq.f32.partialorder %v2190, inf (stack99)
        %v2239 = vsel /*vm=*/%vm2238, /*on_true_vy=*/%v2190, /*on_false_vx=*/%v2237 (stack100)
        %vm2240 = vcmp.eq.f32.partialorder %v2190, 0.0 (stack101)
        %v2241 = vand.u32 %v2190, 2147483648 (stack102)
        %v2242 = vsel /*vm=*/%vm2240, /*on_true_vy=*/%v2241, /*on_false_vx=*/%v2239 (stack103)
        %v2245 = vadd.f32 %v2242, -3.0 (stack82)
        %v2249 = vsel /*vm=*/%vm2193, /*on_true_vy=*/%v2234, /*on_false_vx=*/%v2245 (stack72)
        %v2253 = vmul.f32 %v2230, %v2249 (stack83)
        %v2257 = vadd.f32 %v2226, %v2253 (stack82)
        %v2261 = vmul.f32 %v2257, %v2249 (stack83)
        %v2265 = vadd.f32 %v2222, %v2261 (stack82)
        %v2269 = vmul.f32 %v2265, %v2249 (stack83)
        %v2273 = vadd.f32 %v2218, %v2269 (stack82)
        %v2277 = vmul.f32 %v2273, %v2249 (stack83)
        %v2281 = vadd.f32 %v2214, %v2277 (stack82)
        %v2285 = vmul.f32 %v2281, %v2249 (stack83)
        %v2289 = vadd.f32 %v2210, %v2285 (stack82)
        %v2293 = vmul.f32 %v2289, %v2249 (stack83)
        %v2297 = vadd.f32 %v2206, %v2293 (stack82)
        %v2301 = vmul.f32 %v2297, %v2249 (stack83)
        %v2305 = vadd.f32 %v2202, %v2301 (stack82)
        %v2309 = vmul.f32 %v2305, %v2249 (stack83)
        %v2313 = vadd.f32 %v2198, %v2309 (stack82)
        %v2317 = vmul.f32 %v2313, %v2164 (stack83)
        %v2321 = vsel /*vm=*/%vm2169, /*on_true_vy=*/%v2174, /*on_false_vx=*/%v2317 (stack72)
        %v2325 = vmul.f32 %v2321, 1.4140625 (stack83)
        %s2327 = scalar_lea.vmem %s280, 384 [#allocation0] (stack107)
        %v2328 = vpack.c.bf16 0.0, %v2325 (stack104)
        %2329 = vst [vmem:[%s2327] sm:$0xf] /*vst_source=*/%v2328 (stack105)
        %s2330 = sadd.s32 %s341, 4 (stack106)
        %s2331 = sshrl.u32 %s2330, 7 (stack49)
        %p2332 = scmp.lt.s32.totalorder 0, %s2331 (stack50)
        %s2333 = scalar_select /*predicate=*/%p2332, /*on_true=*/0, /*on_false=*/%s2331 (stack51)
        %s2334 = sand.u32 %s2330, 127 /* smod.u32 w/div 128 */ (stack52)
        %s2335 = sshrl.u32 %s2334, 7 (stack53)
        %s2336 = sand.u32 %s2334, 127 /* smod.u32 w/div 128 */ (stack54)
        %s2337 = scalar_lea.vmem %s4, %s2333 (stack56)
        %s2339 = scalar_lea.vmem %s2337, %s2335 (stack57)
        %v2340 = vld [vmem:[%s2339] ss:$0 sm:$0xff] (stack58)
        %2341 = vbcast.lane.b32.xlu0 %v2340, %s2336 (stack63)
        %v2342 = vpop.permute.xlu0 %2341 (stack64)
        %s2343 = sadd.s32 %s349, 4 (stack106)
        %s2344 = sshrl.u32 %s2343, 7 (stack49)
        %p2345 = scmp.lt.s32.totalorder 0, %s2344 (stack50)
        %s2346 = scalar_select /*predicate=*/%p2345, /*on_true=*/0, /*on_false=*/%s2344 (stack51)
        %s2347 = sand.u32 %s2343, 127 /* smod.u32 w/div 128 */ (stack52)
        %s2348 = sshrl.u32 %s2347, 7 (stack53)
        %s2349 = sand.u32 %s2347, 127 /* smod.u32 w/div 128 */ (stack54)
        %s2350 = scalar_lea.vmem %s6, %s2346 (stack56)
        %s2352 = scalar_lea.vmem %s2350, %s2348 (stack57)
        %v2353 = vld [vmem:[%s2352] ss:$0 sm:$0xff] (stack58)
        %2354 = vbcast.lane.b32.xlu0 %v2353, %s2349 (stack63)
        %v2355 = vpop.permute.xlu0 %2354 (stack64)
        %v2358 = vadd.s32 %v2355, %v396 (stack65)
        %s2360 = smul.u32 128, %s27 (stack66)
        %v2361 = vlaneseq (stack67)
        %v2362 = vand.u32 %v2361, 127 (stack68)
        %v2363 = vstv %s2360 (stack69)
        %v2364 = vadd.s32 %v2362, %v2363 (stack70)
        %v2368 = vadd.s32 %v2358, %v2364 (stack65)
        %vm2372 = vcmp.lt.u32.totalorder %v2368, %v2358 (stack71)
        %vm2377 = vcmp.lt.u32.totalorder %v2358, %v2355 (stack71)
        %v2382 = vadd.s32 %v2342, %v368 (stack65)
        %v2386 = vadd.s32 %v2382, 1 (stack65)
        %v2390 = vsel /*vm=*/%vm2377, /*on_true_vy=*/%v2386, /*on_false_vx=*/%v2382 (stack72)
        %v2394 = vadd.s32 %v2390, 1 (stack65)
        %v2398 = vsel /*vm=*/%vm2372, /*on_true_vy=*/%v2394, /*on_false_vx=*/%v2390 (stack72)
        %v2403 = vadd.s32 %v2398, %v10 (stack65)
        %v2407 = vadd.s32 %v2368, %v9 (stack65)
        %v2411 = vadd.s32 %v2403, %v2407 (stack65)
        %v2413 = vshll.u32 %v2407, 13 (stack73)
        %v2414 = vshrl.u32 %v2407, 19 (stack74)
        %v2415 = vor.u32 %v2413, %v2414 (stack75)
        %v2416 = vxor.u32 %v2411, %v2415 (stack76)
        %v2419 = vadd.s32 %v2411, %v2416 (stack65)
        %v2421 = vshll.u32 %v2416, 15 (stack73)
        %v2422 = vshrl.u32 %v2416, 17 (stack74)
        %v2423 = vor.u32 %v2421, %v2422 (stack75)
        %v2424 = vxor.u32 %v2419, %v2423 (stack76)
        %v2427 = vadd.s32 %v2419, %v2424 (stack65)
        %v2429 = vshll.u32 %v2424, 26 (stack73)
        %v2430 = vshrl.u32 %v2424, 6 (stack74)
        %v2431 = vor.u32 %v2429, %v2430 (stack75)
        %v2432 = vxor.u32 %v2427, %v2431 (stack76)
        %v2435 = vadd.s32 %v2427, %v2432 (stack65)
        %v2439 = vadd.s32 %v2435, %v9 (stack65)
        %v2441 = vshll.u32 %v2432, 6 (stack73)
        %v2442 = vshrl.u32 %v2432, 26 (stack74)
        %v2443 = vor.u32 %v2441, %v2442 (stack75)
        %v2444 = vxor.u32 %v2435, %v2443 (stack76)
        %v2447 = vadd.s32 %v2444, %v8 (stack65)
        %v2451 = vadd.s32 %v2447, 1 (stack65)
        %v2455 = vadd.s32 %v2439, %v2451 (stack65)
        %v2457 = vshll.u32 %v2451, 17 (stack73)
        %v2458 = vshrl.u32 %v2451, 15 (stack74)
        %v2459 = vor.u32 %v2457, %v2458 (stack75)
        %v2460 = vxor.u32 %v2455, %v2459 (stack76)
        %v2463 = vadd.s32 %v2455, %v2460 (stack65)
        %v2465 = vshll.u32 %v2460, 29 (stack73)
        %v2466 = vshrl.u32 %v2460, 3 (stack74)
        %v2467 = vor.u32 %v2465, %v2466 (stack75)
        %v2468 = vxor.u32 %v2463, %v2467 (stack76)
        %v2471 = vadd.s32 %v2463, %v2468 (stack65)
        %v2473 = vshll.u32 %v2468, 16 (stack73)
        %v2474 = vshrl.u32 %v2468, 16 (stack74)
        %v2475 = vor.u32 %v2473, %v2474 (stack75)
        %v2476 = vxor.u32 %v2471, %v2475 (stack76)
        %v2479 = vadd.s32 %v2471, %v2476 (stack65)
        %v2483 = vadd.s32 %v2479, %v8 (stack65)
        %v2485 = vshll.u32 %v2476, 24 (stack73)
        %v2486 = vshrl.u32 %v2476, 8 (stack74)
        %v2487 = vor.u32 %v2485, %v2486 (stack75)
        %v2488 = vxor.u32 %v2479, %v2487 (stack76)
        %v2491 = vadd.s32 %v2488, %v10 (stack65)
        %v2495 = vadd.s32 %v2491, 2 (stack65)
        %v2499 = vadd.s32 %v2483, %v2495 (stack65)
        %v2501 = vshll.u32 %v2495, 13 (stack73)
        %v2502 = vshrl.u32 %v2495, 19 (stack74)
        %v2503 = vor.u32 %v2501, %v2502 (stack75)
        %v2504 = vxor.u32 %v2499, %v2503 (stack76)
        %v2507 = vadd.s32 %v2499, %v2504 (stack65)
        %v2509 = vshll.u32 %v2504, 15 (stack73)
        %v2510 = vshrl.u32 %v2504, 17 (stack74)
        %v2511 = vor.u32 %v2509, %v2510 (stack75)
        %v2512 = vxor.u32 %v2507, %v2511 (stack76)
        %v2515 = vadd.s32 %v2507, %v2512 (stack65)
        %v2517 = vshll.u32 %v2512, 26 (stack73)
        %v2518 = vshrl.u32 %v2512, 6 (stack74)
        %v2519 = vor.u32 %v2517, %v2518 (stack75)
        %v2520 = vxor.u32 %v2515, %v2519 (stack76)
        %v2523 = vadd.s32 %v2515, %v2520 (stack65)
        %v2527 = vadd.s32 %v2523, %v10 (stack65)
        %v2529 = vshll.u32 %v2520, 6 (stack73)
        %v2530 = vshrl.u32 %v2520, 26 (stack74)
        %v2531 = vor.u32 %v2529, %v2530 (stack75)
        %v2532 = vxor.u32 %v2523, %v2531 (stack76)
        %v2535 = vadd.s32 %v2532, %v9 (stack65)
        %v2539 = vadd.s32 %v2535, 3 (stack65)
        %v2543 = vadd.s32 %v2527, %v2539 (stack65)
        %v2545 = vshll.u32 %v2539, 17 (stack73)
        %v2546 = vshrl.u32 %v2539, 15 (stack74)
        %v2547 = vor.u32 %v2545, %v2546 (stack75)
        %v2548 = vxor.u32 %v2543, %v2547 (stack76)
        %v2551 = vadd.s32 %v2543, %v2548 (stack65)
        %v2553 = vshll.u32 %v2548, 29 (stack73)
        %v2554 = vshrl.u32 %v2548, 3 (stack74)
        %v2555 = vor.u32 %v2553, %v2554 (stack75)
        %v2556 = vxor.u32 %v2551, %v2555 (stack76)
        %v2559 = vadd.s32 %v2551, %v2556 (stack65)
        %v2561 = vshll.u32 %v2556, 16 (stack73)
        %v2562 = vshrl.u32 %v2556, 16 (stack74)
        %v2563 = vor.u32 %v2561, %v2562 (stack75)
        %v2564 = vxor.u32 %v2559, %v2563 (stack76)
        %v2567 = vadd.s32 %v2559, %v2564 (stack65)
        %v2571 = vadd.s32 %v2567, %v9 (stack65)
        %v2573 = vshll.u32 %v2564, 24 (stack73)
        %v2574 = vshrl.u32 %v2564, 8 (stack74)
        %v2575 = vor.u32 %v2573, %v2574 (stack75)
        %v2576 = vxor.u32 %v2567, %v2575 (stack76)
        %v2579 = vadd.s32 %v2576, %v8 (stack65)
        %v2583 = vadd.s32 %v2579, 4 (stack65)
        %v2587 = vadd.s32 %v2571, %v2583 (stack65)
        %v2589 = vshll.u32 %v2583, 13 (stack73)
        %v2590 = vshrl.u32 %v2583, 19 (stack74)
        %v2591 = vor.u32 %v2589, %v2590 (stack75)
        %v2592 = vxor.u32 %v2587, %v2591 (stack76)
        %v2595 = vadd.s32 %v2587, %v2592 (stack65)
        %v2597 = vshll.u32 %v2592, 15 (stack73)
        %v2598 = vshrl.u32 %v2592, 17 (stack74)
        %v2599 = vor.u32 %v2597, %v2598 (stack75)
        %v2600 = vxor.u32 %v2595, %v2599 (stack76)
        %v2603 = vadd.s32 %v2595, %v2600 (stack65)
        %v2605 = vshll.u32 %v2600, 26 (stack73)
        %v2606 = vshrl.u32 %v2600, 6 (stack74)
        %v2607 = vor.u32 %v2605, %v2606 (stack75)
        %v2608 = vxor.u32 %v2603, %v2607 (stack76)
        %v2611 = vadd.s32 %v2603, %v2608 (stack65)
        %v2615 = vadd.s32 %v2611, %v8 (stack65)
        %v2617 = vshll.u32 %v2608, 6 (stack73)
        %v2618 = vshrl.u32 %v2608, 26 (stack74)
        %v2619 = vor.u32 %v2617, %v2618 (stack75)
        %v2620 = vxor.u32 %v2611, %v2619 (stack76)
        %v2623 = vadd.s32 %v2620, %v10 (stack65)
        %v2627 = vadd.s32 %v2623, 5 (stack65)
        %v2629 = vxor.u32 %v2615, %v2627 (stack76)
        %v2630 = vand.u32.u8 %v2629, 255 (stack77)
        %v2631 = vand.u32 %v2630, 65535 (stack78)
        %v2632 = vshrl.u32 %v2631, 1 (stack79)
        %v2633 = vor.u32 %v2632, 16256 (stack75)
        %v2634 = vand.u32.u16 %v2633, 65535 (stack80)
        %v2635 = vunpack.i.l.bf16 %v2634 (stack81)
        %v2639 = vadd.f32 %v2635, -1.0 (stack82)
        %v2643 = vmul.f32 %v2639, 2.0 (stack83)
        %v2647 = vadd.f32 %v2643, -0.99609375 (stack82)
        %v2651 = vmax.f32 -0.99609375, %v2647 (stack84)
        %v2653 = vand.u32 2147483647, %v2651 (stack85)
        %vm2656 = vcmp.eq.f32.partialorder %v2653, 1.0 (stack86)
        %v2661 = vmul.f32 %v2651, inf (stack83)
        %v2663 = vxor.u32 %v2651, 2147483648 (stack87)
        %v2666 = vmul.f32 %v2651, %v2663 (stack83)
        %v2668 = vadd.f32 %v2666, 1.0 (stack88)
        %v2669 = vlog2.pop %v2668 (stack89)
        %v2670 = vmul.f32 %v2669, 0.6931472 (stack90)
        %v2671 = vmul.f32 -0.5, %v2666 (stack91)
        %v2672 = vadd.f32 %v2671, 1.0 (stack92)
        %v2673 = vmul.f32 %v2672, %v2666 (stack93)
        %v2674 = vand.u32 2147483647, %v2666 (stack94)
        %vm2675 = vcmp.lt.f32.partialorder %v2674, 0.0004427343 (stack95)
        %v2676 = vsel /*vm=*/%vm2675, /*on_true_vy=*/%v2673, /*on_false_vx=*/%v2670 (stack96)
        %v2677 = vxor.u32 %v2676, 2147483648 (stack87)
        %vm2680 = vcmp.lt.f32.partialorder %v2677, 5.0 (stack86)
        %v2685 = vsel /*vm=*/%vm2680, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v2689 = vsel /*vm=*/%vm2680, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v2693 = vsel /*vm=*/%vm2680, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v2697 = vsel /*vm=*/%vm2680, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v2701 = vsel /*vm=*/%vm2680, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v2705 = vsel /*vm=*/%vm2680, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v2709 = vsel /*vm=*/%vm2680, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v2713 = vsel /*vm=*/%vm2680, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v2717 = vsel /*vm=*/%vm2680, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v2721 = vadd.f32 %v2677, -2.5 (stack82)
        %v2723 = vrsqrt.pop %v2677 (stack97)
        %v2724 = vmul.f32 %v2677, %v2723 (stack98)
        %vm2725 = vcmp.eq.f32.partialorder %v2677, inf (stack99)
        %v2726 = vsel /*vm=*/%vm2725, /*on_true_vy=*/%v2677, /*on_false_vx=*/%v2724 (stack100)
        %vm2727 = vcmp.eq.f32.partialorder %v2677, 0.0 (stack101)
        %v2728 = vand.u32 %v2677, 2147483648 (stack102)
        %v2729 = vsel /*vm=*/%vm2727, /*on_true_vy=*/%v2728, /*on_false_vx=*/%v2726 (stack103)
        %v2732 = vadd.f32 %v2729, -3.0 (stack82)
        %v2736 = vsel /*vm=*/%vm2680, /*on_true_vy=*/%v2721, /*on_false_vx=*/%v2732 (stack72)
        %v2740 = vmul.f32 %v2717, %v2736 (stack83)
        %v2744 = vadd.f32 %v2713, %v2740 (stack82)
        %v2748 = vmul.f32 %v2744, %v2736 (stack83)
        %v2752 = vadd.f32 %v2709, %v2748 (stack82)
        %v2756 = vmul.f32 %v2752, %v2736 (stack83)
        %v2760 = vadd.f32 %v2705, %v2756 (stack82)
        %v2764 = vmul.f32 %v2760, %v2736 (stack83)
        %v2768 = vadd.f32 %v2701, %v2764 (stack82)
        %v2772 = vmul.f32 %v2768, %v2736 (stack83)
        %v2776 = vadd.f32 %v2697, %v2772 (stack82)
        %v2780 = vmul.f32 %v2776, %v2736 (stack83)
        %v2784 = vadd.f32 %v2693, %v2780 (stack82)
        %v2788 = vmul.f32 %v2784, %v2736 (stack83)
        %v2792 = vadd.f32 %v2689, %v2788 (stack82)
        %v2796 = vmul.f32 %v2792, %v2736 (stack83)
        %v2800 = vadd.f32 %v2685, %v2796 (stack82)
        %v2804 = vmul.f32 %v2800, %v2651 (stack83)
        %v2808 = vsel /*vm=*/%vm2656, /*on_true_vy=*/%v2661, /*on_false_vx=*/%v2804 (stack72)
        %v2812 = vmul.f32 %v2808, 1.4140625 (stack83)
        %s2814 = scalar_lea.vmem %s280, 512 [#allocation0] (stack107)
        %v2815 = vpack.c.bf16 0.0, %v2812 (stack104)
        %2816 = vst [vmem:[%s2814] sm:$0xf] /*vst_source=*/%v2815 (stack105)
        %s2817 = sadd.s32 %s341, 5 (stack106)
        %s2818 = sshrl.u32 %s2817, 7 (stack49)
        %p2819 = scmp.lt.s32.totalorder 0, %s2818 (stack50)
        %s2820 = scalar_select /*predicate=*/%p2819, /*on_true=*/0, /*on_false=*/%s2818 (stack51)
        %s2821 = sand.u32 %s2817, 127 /* smod.u32 w/div 128 */ (stack52)
        %s2822 = sshrl.u32 %s2821, 7 (stack53)
        %s2823 = sand.u32 %s2821, 127 /* smod.u32 w/div 128 */ (stack54)
        %s2824 = scalar_lea.vmem %s4, %s2820 (stack56)
        %s2826 = scalar_lea.vmem %s2824, %s2822 (stack57)
        %v2827 = vld [vmem:[%s2826] ss:$0 sm:$0xff] (stack58)
        %2828 = vbcast.lane.b32.xlu0 %v2827, %s2823 (stack63)
        %v2829 = vpop.permute.xlu0 %2828 (stack64)
        %s2830 = sadd.s32 %s349, 5 (stack106)
        %s2831 = sshrl.u32 %s2830, 7 (stack49)
        %p2832 = scmp.lt.s32.totalorder 0, %s2831 (stack50)
        %s2833 = scalar_select /*predicate=*/%p2832, /*on_true=*/0, /*on_false=*/%s2831 (stack51)
        %s2834 = sand.u32 %s2830, 127 /* smod.u32 w/div 128 */ (stack52)
        %s2835 = sshrl.u32 %s2834, 7 (stack53)
        %s2836 = sand.u32 %s2834, 127 /* smod.u32 w/div 128 */ (stack54)
        %s2837 = scalar_lea.vmem %s6, %s2833 (stack56)
        %s2839 = scalar_lea.vmem %s2837, %s2835 (stack57)
        %v2840 = vld [vmem:[%s2839] ss:$0 sm:$0xff] (stack58)
        %2841 = vbcast.lane.b32.xlu0 %v2840, %s2836 (stack63)
        %v2842 = vpop.permute.xlu0 %2841 (stack64)
        %v2845 = vadd.s32 %v2842, %v396 (stack65)
        %s2847 = smul.u32 128, %s27 (stack66)
        %v2848 = vlaneseq (stack67)
        %v2849 = vand.u32 %v2848, 127 (stack68)
        %v2850 = vstv %s2847 (stack69)
        %v2851 = vadd.s32 %v2849, %v2850 (stack70)
        %v2855 = vadd.s32 %v2845, %v2851 (stack65)
        %vm2859 = vcmp.lt.u32.totalorder %v2855, %v2845 (stack71)
        %vm2864 = vcmp.lt.u32.totalorder %v2845, %v2842 (stack71)
        %v2869 = vadd.s32 %v2829, %v368 (stack65)
        %v2873 = vadd.s32 %v2869, 1 (stack65)
        %v2877 = vsel /*vm=*/%vm2864, /*on_true_vy=*/%v2873, /*on_false_vx=*/%v2869 (stack72)
        %v2881 = vadd.s32 %v2877, 1 (stack65)
        %v2885 = vsel /*vm=*/%vm2859, /*on_true_vy=*/%v2881, /*on_false_vx=*/%v2877 (stack72)
        %v2890 = vadd.s32 %v2885, %v10 (stack65)
        %v2894 = vadd.s32 %v2855, %v9 (stack65)
        %v2898 = vadd.s32 %v2890, %v2894 (stack65)
        %v2900 = vshll.u32 %v2894, 13 (stack73)
        %v2901 = vshrl.u32 %v2894, 19 (stack74)
        %v2902 = vor.u32 %v2900, %v2901 (stack75)
        %v2903 = vxor.u32 %v2898, %v2902 (stack76)
        %v2906 = vadd.s32 %v2898, %v2903 (stack65)
        %v2908 = vshll.u32 %v2903, 15 (stack73)
        %v2909 = vshrl.u32 %v2903, 17 (stack74)
        %v2910 = vor.u32 %v2908, %v2909 (stack75)
        %v2911 = vxor.u32 %v2906, %v2910 (stack76)
        %v2914 = vadd.s32 %v2906, %v2911 (stack65)
        %v2916 = vshll.u32 %v2911, 26 (stack73)
        %v2917 = vshrl.u32 %v2911, 6 (stack74)
        %v2918 = vor.u32 %v2916, %v2917 (stack75)
        %v2919 = vxor.u32 %v2914, %v2918 (stack76)
        %v2922 = vadd.s32 %v2914, %v2919 (stack65)
        %v2926 = vadd.s32 %v2922, %v9 (stack65)
        %v2928 = vshll.u32 %v2919, 6 (stack73)
        %v2929 = vshrl.u32 %v2919, 26 (stack74)
        %v2930 = vor.u32 %v2928, %v2929 (stack75)
        %v2931 = vxor.u32 %v2922, %v2930 (stack76)
        %v2934 = vadd.s32 %v2931, %v8 (stack65)
        %v2938 = vadd.s32 %v2934, 1 (stack65)
        %v2942 = vadd.s32 %v2926, %v2938 (stack65)
        %v2944 = vshll.u32 %v2938, 17 (stack73)
        %v2945 = vshrl.u32 %v2938, 15 (stack74)
        %v2946 = vor.u32 %v2944, %v2945 (stack75)
        %v2947 = vxor.u32 %v2942, %v2946 (stack76)
        %v2950 = vadd.s32 %v2942, %v2947 (stack65)
        %v2952 = vshll.u32 %v2947, 29 (stack73)
        %v2953 = vshrl.u32 %v2947, 3 (stack74)
        %v2954 = vor.u32 %v2952, %v2953 (stack75)
        %v2955 = vxor.u32 %v2950, %v2954 (stack76)
        %v2958 = vadd.s32 %v2950, %v2955 (stack65)
        %v2960 = vshll.u32 %v2955, 16 (stack73)
        %v2961 = vshrl.u32 %v2955, 16 (stack74)
        %v2962 = vor.u32 %v2960, %v2961 (stack75)
        %v2963 = vxor.u32 %v2958, %v2962 (stack76)
        %v2966 = vadd.s32 %v2958, %v2963 (stack65)
        %v2970 = vadd.s32 %v2966, %v8 (stack65)
        %v2972 = vshll.u32 %v2963, 24 (stack73)
        %v2973 = vshrl.u32 %v2963, 8 (stack74)
        %v2974 = vor.u32 %v2972, %v2973 (stack75)
        %v2975 = vxor.u32 %v2966, %v2974 (stack76)
        %v2978 = vadd.s32 %v2975, %v10 (stack65)
        %v2982 = vadd.s32 %v2978, 2 (stack65)
        %v2986 = vadd.s32 %v2970, %v2982 (stack65)
        %v2988 = vshll.u32 %v2982, 13 (stack73)
        %v2989 = vshrl.u32 %v2982, 19 (stack74)
        %v2990 = vor.u32 %v2988, %v2989 (stack75)
        %v2991 = vxor.u32 %v2986, %v2990 (stack76)
        %v2994 = vadd.s32 %v2986, %v2991 (stack65)
        %v2996 = vshll.u32 %v2991, 15 (stack73)
        %v2997 = vshrl.u32 %v2991, 17 (stack74)
        %v2998 = vor.u32 %v2996, %v2997 (stack75)
        %v2999 = vxor.u32 %v2994, %v2998 (stack76)
        %v3002 = vadd.s32 %v2994, %v2999 (stack65)
        %v3004 = vshll.u32 %v2999, 26 (stack73)
        %v3005 = vshrl.u32 %v2999, 6 (stack74)
        %v3006 = vor.u32 %v3004, %v3005 (stack75)
        %v3007 = vxor.u32 %v3002, %v3006 (stack76)
        %v3010 = vadd.s32 %v3002, %v3007 (stack65)
        %v3014 = vadd.s32 %v3010, %v10 (stack65)
        %v3016 = vshll.u32 %v3007, 6 (stack73)
        %v3017 = vshrl.u32 %v3007, 26 (stack74)
        %v3018 = vor.u32 %v3016, %v3017 (stack75)
        %v3019 = vxor.u32 %v3010, %v3018 (stack76)
        %v3022 = vadd.s32 %v3019, %v9 (stack65)
        %v3026 = vadd.s32 %v3022, 3 (stack65)
        %v3030 = vadd.s32 %v3014, %v3026 (stack65)
        %v3032 = vshll.u32 %v3026, 17 (stack73)
        %v3033 = vshrl.u32 %v3026, 15 (stack74)
        %v3034 = vor.u32 %v3032, %v3033 (stack75)
        %v3035 = vxor.u32 %v3030, %v3034 (stack76)
        %v3038 = vadd.s32 %v3030, %v3035 (stack65)
        %v3040 = vshll.u32 %v3035, 29 (stack73)
        %v3041 = vshrl.u32 %v3035, 3 (stack74)
        %v3042 = vor.u32 %v3040, %v3041 (stack75)
        %v3043 = vxor.u32 %v3038, %v3042 (stack76)
        %v3046 = vadd.s32 %v3038, %v3043 (stack65)
        %v3048 = vshll.u32 %v3043, 16 (stack73)
        %v3049 = vshrl.u32 %v3043, 16 (stack74)
        %v3050 = vor.u32 %v3048, %v3049 (stack75)
        %v3051 = vxor.u32 %v3046, %v3050 (stack76)
        %v3054 = vadd.s32 %v3046, %v3051 (stack65)
        %v3058 = vadd.s32 %v3054, %v9 (stack65)
        %v3060 = vshll.u32 %v3051, 24 (stack73)
        %v3061 = vshrl.u32 %v3051, 8 (stack74)
        %v3062 = vor.u32 %v3060, %v3061 (stack75)
        %v3063 = vxor.u32 %v3054, %v3062 (stack76)
        %v3066 = vadd.s32 %v3063, %v8 (stack65)
        %v3070 = vadd.s32 %v3066, 4 (stack65)
        %v3074 = vadd.s32 %v3058, %v3070 (stack65)
        %v3076 = vshll.u32 %v3070, 13 (stack73)
        %v3077 = vshrl.u32 %v3070, 19 (stack74)
        %v3078 = vor.u32 %v3076, %v3077 (stack75)
        %v3079 = vxor.u32 %v3074, %v3078 (stack76)
        %v3082 = vadd.s32 %v3074, %v3079 (stack65)
        %v3084 = vshll.u32 %v3079, 15 (stack73)
        %v3085 = vshrl.u32 %v3079, 17 (stack74)
        %v3086 = vor.u32 %v3084, %v3085 (stack75)
        %v3087 = vxor.u32 %v3082, %v3086 (stack76)
        %v3090 = vadd.s32 %v3082, %v3087 (stack65)
        %v3092 = vshll.u32 %v3087, 26 (stack73)
        %v3093 = vshrl.u32 %v3087, 6 (stack74)
        %v3094 = vor.u32 %v3092, %v3093 (stack75)
        %v3095 = vxor.u32 %v3090, %v3094 (stack76)
        %v3098 = vadd.s32 %v3090, %v3095 (stack65)
        %v3102 = vadd.s32 %v3098, %v8 (stack65)
        %v3104 = vshll.u32 %v3095, 6 (stack73)
        %v3105 = vshrl.u32 %v3095, 26 (stack74)
        %v3106 = vor.u32 %v3104, %v3105 (stack75)
        %v3107 = vxor.u32 %v3098, %v3106 (stack76)
        %v3110 = vadd.s32 %v3107, %v10 (stack65)
        %v3114 = vadd.s32 %v3110, 5 (stack65)
        %v3116 = vxor.u32 %v3102, %v3114 (stack76)
        %v3117 = vand.u32.u8 %v3116, 255 (stack77)
        %v3118 = vand.u32 %v3117, 65535 (stack78)
        %v3119 = vshrl.u32 %v3118, 1 (stack79)
        %v3120 = vor.u32 %v3119, 16256 (stack75)
        %v3121 = vand.u32.u16 %v3120, 65535 (stack80)
        %v3122 = vunpack.i.l.bf16 %v3121 (stack81)
        %v3126 = vadd.f32 %v3122, -1.0 (stack82)
        %v3130 = vmul.f32 %v3126, 2.0 (stack83)
        %v3134 = vadd.f32 %v3130, -0.99609375 (stack82)
        %v3138 = vmax.f32 -0.99609375, %v3134 (stack84)
        %v3140 = vand.u32 2147483647, %v3138 (stack85)
        %vm3143 = vcmp.eq.f32.partialorder %v3140, 1.0 (stack86)
        %v3148 = vmul.f32 %v3138, inf (stack83)
        %v3150 = vxor.u32 %v3138, 2147483648 (stack87)
        %v3153 = vmul.f32 %v3138, %v3150 (stack83)
        %v3155 = vadd.f32 %v3153, 1.0 (stack88)
        %v3156 = vlog2.pop %v3155 (stack89)
        %v3157 = vmul.f32 %v3156, 0.6931472 (stack90)
        %v3158 = vmul.f32 -0.5, %v3153 (stack91)
        %v3159 = vadd.f32 %v3158, 1.0 (stack92)
        %v3160 = vmul.f32 %v3159, %v3153 (stack93)
        %v3161 = vand.u32 2147483647, %v3153 (stack94)
        %vm3162 = vcmp.lt.f32.partialorder %v3161, 0.0004427343 (stack95)
        %v3163 = vsel /*vm=*/%vm3162, /*on_true_vy=*/%v3160, /*on_false_vx=*/%v3157 (stack96)
        %v3164 = vxor.u32 %v3163, 2147483648 (stack87)
        %vm3167 = vcmp.lt.f32.partialorder %v3164, 5.0 (stack86)
        %v3172 = vsel /*vm=*/%vm3167, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v3176 = vsel /*vm=*/%vm3167, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v3180 = vsel /*vm=*/%vm3167, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v3184 = vsel /*vm=*/%vm3167, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v3188 = vsel /*vm=*/%vm3167, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v3192 = vsel /*vm=*/%vm3167, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v3196 = vsel /*vm=*/%vm3167, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v3200 = vsel /*vm=*/%vm3167, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v3204 = vsel /*vm=*/%vm3167, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v3208 = vadd.f32 %v3164, -2.5 (stack82)
        %v3210 = vrsqrt.pop %v3164 (stack97)
        %v3211 = vmul.f32 %v3164, %v3210 (stack98)
        %vm3212 = vcmp.eq.f32.partialorder %v3164, inf (stack99)
        %v3213 = vsel /*vm=*/%vm3212, /*on_true_vy=*/%v3164, /*on_false_vx=*/%v3211 (stack100)
        %vm3214 = vcmp.eq.f32.partialorder %v3164, 0.0 (stack101)
        %v3215 = vand.u32 %v3164, 2147483648 (stack102)
        %v3216 = vsel /*vm=*/%vm3214, /*on_true_vy=*/%v3215, /*on_false_vx=*/%v3213 (stack103)
        %v3219 = vadd.f32 %v3216, -3.0 (stack82)
        %v3223 = vsel /*vm=*/%vm3167, /*on_true_vy=*/%v3208, /*on_false_vx=*/%v3219 (stack72)
        %v3227 = vmul.f32 %v3204, %v3223 (stack83)
        %v3231 = vadd.f32 %v3200, %v3227 (stack82)
        %v3235 = vmul.f32 %v3231, %v3223 (stack83)
        %v3239 = vadd.f32 %v3196, %v3235 (stack82)
        %v3243 = vmul.f32 %v3239, %v3223 (stack83)
        %v3247 = vadd.f32 %v3192, %v3243 (stack82)
        %v3251 = vmul.f32 %v3247, %v3223 (stack83)
        %v3255 = vadd.f32 %v3188, %v3251 (stack82)
        %v3259 = vmul.f32 %v3255, %v3223 (stack83)
        %v3263 = vadd.f32 %v3184, %v3259 (stack82)
        %v3267 = vmul.f32 %v3263, %v3223 (stack83)
        %v3271 = vadd.f32 %v3180, %v3267 (stack82)
        %v3275 = vmul.f32 %v3271, %v3223 (stack83)
        %v3279 = vadd.f32 %v3176, %v3275 (stack82)
        %v3283 = vmul.f32 %v3279, %v3223 (stack83)
        %v3287 = vadd.f32 %v3172, %v3283 (stack82)
        %v3291 = vmul.f32 %v3287, %v3138 (stack83)
        %v3295 = vsel /*vm=*/%vm3143, /*on_true_vy=*/%v3148, /*on_false_vx=*/%v3291 (stack72)
        %v3299 = vmul.f32 %v3295, 1.4140625 (stack83)
        %s3301 = scalar_lea.vmem %s280, 640 [#allocation0] (stack107)
        %v3302 = vpack.c.bf16 0.0, %v3299 (stack104)
        %3303 = vst [vmem:[%s3301] sm:$0xf] /*vst_source=*/%v3302 (stack105)
        %s3304 = sadd.s32 %s341, 6 (stack106)
        %s3305 = sshrl.u32 %s3304, 7 (stack49)
        %p3306 = scmp.lt.s32.totalorder 0, %s3305 (stack50)
        %s3307 = scalar_select /*predicate=*/%p3306, /*on_true=*/0, /*on_false=*/%s3305 (stack51)
        %s3308 = sand.u32 %s3304, 127 /* smod.u32 w/div 128 */ (stack52)
        %s3309 = sshrl.u32 %s3308, 7 (stack53)
        %s3310 = sand.u32 %s3308, 127 /* smod.u32 w/div 128 */ (stack54)
        %s3311 = scalar_lea.vmem %s4, %s3307 (stack56)
        %s3313 = scalar_lea.vmem %s3311, %s3309 (stack57)
        %v3314 = vld [vmem:[%s3313] ss:$0 sm:$0xff] (stack58)
        %3315 = vbcast.lane.b32.xlu0 %v3314, %s3310 (stack63)
        %v3316 = vpop.permute.xlu0 %3315 (stack64)
        %s3317 = sadd.s32 %s349, 6 (stack106)
        %s3318 = sshrl.u32 %s3317, 7 (stack49)
        %p3319 = scmp.lt.s32.totalorder 0, %s3318 (stack50)
        %s3320 = scalar_select /*predicate=*/%p3319, /*on_true=*/0, /*on_false=*/%s3318 (stack51)
        %s3321 = sand.u32 %s3317, 127 /* smod.u32 w/div 128 */ (stack52)
        %s3322 = sshrl.u32 %s3321, 7 (stack53)
        %s3323 = sand.u32 %s3321, 127 /* smod.u32 w/div 128 */ (stack54)
        %s3324 = scalar_lea.vmem %s6, %s3320 (stack56)
        %s3326 = scalar_lea.vmem %s3324, %s3322 (stack57)
        %v3327 = vld [vmem:[%s3326] ss:$0 sm:$0xff] (stack58)
        %3328 = vbcast.lane.b32.xlu0 %v3327, %s3323 (stack63)
        %v3329 = vpop.permute.xlu0 %3328 (stack64)
        %v3332 = vadd.s32 %v3329, %v396 (stack65)
        %s3334 = smul.u32 128, %s27 (stack66)
        %v3335 = vlaneseq (stack67)
        %v3336 = vand.u32 %v3335, 127 (stack68)
        %v3337 = vstv %s3334 (stack69)
        %v3338 = vadd.s32 %v3336, %v3337 (stack70)
        %v3342 = vadd.s32 %v3332, %v3338 (stack65)
        %vm3346 = vcmp.lt.u32.totalorder %v3342, %v3332 (stack71)
        %vm3351 = vcmp.lt.u32.totalorder %v3332, %v3329 (stack71)
        %v3356 = vadd.s32 %v3316, %v368 (stack65)
        %v3360 = vadd.s32 %v3356, 1 (stack65)
        %v3364 = vsel /*vm=*/%vm3351, /*on_true_vy=*/%v3360, /*on_false_vx=*/%v3356 (stack72)
        %v3368 = vadd.s32 %v3364, 1 (stack65)
        %v3372 = vsel /*vm=*/%vm3346, /*on_true_vy=*/%v3368, /*on_false_vx=*/%v3364 (stack72)
        %v3377 = vadd.s32 %v3372, %v10 (stack65)
        %v3381 = vadd.s32 %v3342, %v9 (stack65)
        %v3385 = vadd.s32 %v3377, %v3381 (stack65)
        %v3387 = vshll.u32 %v3381, 13 (stack73)
        %v3388 = vshrl.u32 %v3381, 19 (stack74)
        %v3389 = vor.u32 %v3387, %v3388 (stack75)
        %v3390 = vxor.u32 %v3385, %v3389 (stack76)
        %v3393 = vadd.s32 %v3385, %v3390 (stack65)
        %v3395 = vshll.u32 %v3390, 15 (stack73)
        %v3396 = vshrl.u32 %v3390, 17 (stack74)
        %v3397 = vor.u32 %v3395, %v3396 (stack75)
        %v3398 = vxor.u32 %v3393, %v3397 (stack76)
        %v3401 = vadd.s32 %v3393, %v3398 (stack65)
        %v3403 = vshll.u32 %v3398, 26 (stack73)
        %v3404 = vshrl.u32 %v3398, 6 (stack74)
        %v3405 = vor.u32 %v3403, %v3404 (stack75)
        %v3406 = vxor.u32 %v3401, %v3405 (stack76)
        %v3409 = vadd.s32 %v3401, %v3406 (stack65)
        %v3413 = vadd.s32 %v3409, %v9 (stack65)
        %v3415 = vshll.u32 %v3406, 6 (stack73)
        %v3416 = vshrl.u32 %v3406, 26 (stack74)
        %v3417 = vor.u32 %v3415, %v3416 (stack75)
        %v3418 = vxor.u32 %v3409, %v3417 (stack76)
        %v3421 = vadd.s32 %v3418, %v8 (stack65)
        %v3425 = vadd.s32 %v3421, 1 (stack65)
        %v3429 = vadd.s32 %v3413, %v3425 (stack65)
        %v3431 = vshll.u32 %v3425, 17 (stack73)
        %v3432 = vshrl.u32 %v3425, 15 (stack74)
        %v3433 = vor.u32 %v3431, %v3432 (stack75)
        %v3434 = vxor.u32 %v3429, %v3433 (stack76)
        %v3437 = vadd.s32 %v3429, %v3434 (stack65)
        %v3439 = vshll.u32 %v3434, 29 (stack73)
        %v3440 = vshrl.u32 %v3434, 3 (stack74)
        %v3441 = vor.u32 %v3439, %v3440 (stack75)
        %v3442 = vxor.u32 %v3437, %v3441 (stack76)
        %v3445 = vadd.s32 %v3437, %v3442 (stack65)
        %v3447 = vshll.u32 %v3442, 16 (stack73)
        %v3448 = vshrl.u32 %v3442, 16 (stack74)
        %v3449 = vor.u32 %v3447, %v3448 (stack75)
        %v3450 = vxor.u32 %v3445, %v3449 (stack76)
        %v3453 = vadd.s32 %v3445, %v3450 (stack65)
        %v3457 = vadd.s32 %v3453, %v8 (stack65)
        %v3459 = vshll.u32 %v3450, 24 (stack73)
        %v3460 = vshrl.u32 %v3450, 8 (stack74)
        %v3461 = vor.u32 %v3459, %v3460 (stack75)
        %v3462 = vxor.u32 %v3453, %v3461 (stack76)
        %v3465 = vadd.s32 %v3462, %v10 (stack65)
        %v3469 = vadd.s32 %v3465, 2 (stack65)
        %v3473 = vadd.s32 %v3457, %v3469 (stack65)
        %v3475 = vshll.u32 %v3469, 13 (stack73)
        %v3476 = vshrl.u32 %v3469, 19 (stack74)
        %v3477 = vor.u32 %v3475, %v3476 (stack75)
        %v3478 = vxor.u32 %v3473, %v3477 (stack76)
        %v3481 = vadd.s32 %v3473, %v3478 (stack65)
        %v3483 = vshll.u32 %v3478, 15 (stack73)
        %v3484 = vshrl.u32 %v3478, 17 (stack74)
        %v3485 = vor.u32 %v3483, %v3484 (stack75)
        %v3486 = vxor.u32 %v3481, %v3485 (stack76)
        %v3489 = vadd.s32 %v3481, %v3486 (stack65)
        %v3491 = vshll.u32 %v3486, 26 (stack73)
        %v3492 = vshrl.u32 %v3486, 6 (stack74)
        %v3493 = vor.u32 %v3491, %v3492 (stack75)
        %v3494 = vxor.u32 %v3489, %v3493 (stack76)
        %v3497 = vadd.s32 %v3489, %v3494 (stack65)
        %v3501 = vadd.s32 %v3497, %v10 (stack65)
        %v3503 = vshll.u32 %v3494, 6 (stack73)
        %v3504 = vshrl.u32 %v3494, 26 (stack74)
        %v3505 = vor.u32 %v3503, %v3504 (stack75)
        %v3506 = vxor.u32 %v3497, %v3505 (stack76)
        %v3509 = vadd.s32 %v3506, %v9 (stack65)
        %v3513 = vadd.s32 %v3509, 3 (stack65)
        %v3517 = vadd.s32 %v3501, %v3513 (stack65)
        %v3519 = vshll.u32 %v3513, 17 (stack73)
        %v3520 = vshrl.u32 %v3513, 15 (stack74)
        %v3521 = vor.u32 %v3519, %v3520 (stack75)
        %v3522 = vxor.u32 %v3517, %v3521 (stack76)
        %v3525 = vadd.s32 %v3517, %v3522 (stack65)
        %v3527 = vshll.u32 %v3522, 29 (stack73)
        %v3528 = vshrl.u32 %v3522, 3 (stack74)
        %v3529 = vor.u32 %v3527, %v3528 (stack75)
        %v3530 = vxor.u32 %v3525, %v3529 (stack76)
        %v3533 = vadd.s32 %v3525, %v3530 (stack65)
        %v3535 = vshll.u32 %v3530, 16 (stack73)
        %v3536 = vshrl.u32 %v3530, 16 (stack74)
        %v3537 = vor.u32 %v3535, %v3536 (stack75)
        %v3538 = vxor.u32 %v3533, %v3537 (stack76)
        %v3541 = vadd.s32 %v3533, %v3538 (stack65)
        %v3545 = vadd.s32 %v3541, %v9 (stack65)
        %v3547 = vshll.u32 %v3538, 24 (stack73)
        %v3548 = vshrl.u32 %v3538, 8 (stack74)
        %v3549 = vor.u32 %v3547, %v3548 (stack75)
        %v3550 = vxor.u32 %v3541, %v3549 (stack76)
        %v3553 = vadd.s32 %v3550, %v8 (stack65)
        %v3557 = vadd.s32 %v3553, 4 (stack65)
        %v3561 = vadd.s32 %v3545, %v3557 (stack65)
        %v3563 = vshll.u32 %v3557, 13 (stack73)
        %v3564 = vshrl.u32 %v3557, 19 (stack74)
        %v3565 = vor.u32 %v3563, %v3564 (stack75)
        %v3566 = vxor.u32 %v3561, %v3565 (stack76)
        %v3569 = vadd.s32 %v3561, %v3566 (stack65)
        %v3571 = vshll.u32 %v3566, 15 (stack73)
        %v3572 = vshrl.u32 %v3566, 17 (stack74)
        %v3573 = vor.u32 %v3571, %v3572 (stack75)
        %v3574 = vxor.u32 %v3569, %v3573 (stack76)
        %v3577 = vadd.s32 %v3569, %v3574 (stack65)
        %v3579 = vshll.u32 %v3574, 26 (stack73)
        %v3580 = vshrl.u32 %v3574, 6 (stack74)
        %v3581 = vor.u32 %v3579, %v3580 (stack75)
        %v3582 = vxor.u32 %v3577, %v3581 (stack76)
        %v3585 = vadd.s32 %v3577, %v3582 (stack65)
        %v3589 = vadd.s32 %v3585, %v8 (stack65)
        %v3591 = vshll.u32 %v3582, 6 (stack73)
        %v3592 = vshrl.u32 %v3582, 26 (stack74)
        %v3593 = vor.u32 %v3591, %v3592 (stack75)
        %v3594 = vxor.u32 %v3585, %v3593 (stack76)
        %v3597 = vadd.s32 %v3594, %v10 (stack65)
        %v3601 = vadd.s32 %v3597, 5 (stack65)
        %v3603 = vxor.u32 %v3589, %v3601 (stack76)
        %v3604 = vand.u32.u8 %v3603, 255 (stack77)
        %v3605 = vand.u32 %v3604, 65535 (stack78)
        %v3606 = vshrl.u32 %v3605, 1 (stack79)
        %v3607 = vor.u32 %v3606, 16256 (stack75)
        %v3608 = vand.u32.u16 %v3607, 65535 (stack80)
        %v3609 = vunpack.i.l.bf16 %v3608 (stack81)
        %v3613 = vadd.f32 %v3609, -1.0 (stack82)
        %v3617 = vmul.f32 %v3613, 2.0 (stack83)
        %v3621 = vadd.f32 %v3617, -0.99609375 (stack82)
        %v3625 = vmax.f32 -0.99609375, %v3621 (stack84)
        %v3627 = vand.u32 2147483647, %v3625 (stack85)
        %vm3630 = vcmp.eq.f32.partialorder %v3627, 1.0 (stack86)
        %v3635 = vmul.f32 %v3625, inf (stack83)
        %v3637 = vxor.u32 %v3625, 2147483648 (stack87)
        %v3640 = vmul.f32 %v3625, %v3637 (stack83)
        %v3642 = vadd.f32 %v3640, 1.0 (stack88)
        %v3643 = vlog2.pop %v3642 (stack89)
        %v3644 = vmul.f32 %v3643, 0.6931472 (stack90)
        %v3645 = vmul.f32 -0.5, %v3640 (stack91)
        %v3646 = vadd.f32 %v3645, 1.0 (stack92)
        %v3647 = vmul.f32 %v3646, %v3640 (stack93)
        %v3648 = vand.u32 2147483647, %v3640 (stack94)
        %vm3649 = vcmp.lt.f32.partialorder %v3648, 0.0004427343 (stack95)
        %v3650 = vsel /*vm=*/%vm3649, /*on_true_vy=*/%v3647, /*on_false_vx=*/%v3644 (stack96)
        %v3651 = vxor.u32 %v3650, 2147483648 (stack87)
        %vm3654 = vcmp.lt.f32.partialorder %v3651, 5.0 (stack86)
        %v3659 = vsel /*vm=*/%vm3654, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v3663 = vsel /*vm=*/%vm3654, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v3667 = vsel /*vm=*/%vm3654, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v3671 = vsel /*vm=*/%vm3654, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v3675 = vsel /*vm=*/%vm3654, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v3679 = vsel /*vm=*/%vm3654, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v3683 = vsel /*vm=*/%vm3654, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v3687 = vsel /*vm=*/%vm3654, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v3691 = vsel /*vm=*/%vm3654, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v3695 = vadd.f32 %v3651, -2.5 (stack82)
        %v3697 = vrsqrt.pop %v3651 (stack97)
        %v3698 = vmul.f32 %v3651, %v3697 (stack98)
        %vm3699 = vcmp.eq.f32.partialorder %v3651, inf (stack99)
        %v3700 = vsel /*vm=*/%vm3699, /*on_true_vy=*/%v3651, /*on_false_vx=*/%v3698 (stack100)
        %vm3701 = vcmp.eq.f32.partialorder %v3651, 0.0 (stack101)
        %v3702 = vand.u32 %v3651, 2147483648 (stack102)
        %v3703 = vsel /*vm=*/%vm3701, /*on_true_vy=*/%v3702, /*on_false_vx=*/%v3700 (stack103)
        %v3706 = vadd.f32 %v3703, -3.0 (stack82)
        %v3710 = vsel /*vm=*/%vm3654, /*on_true_vy=*/%v3695, /*on_false_vx=*/%v3706 (stack72)
        %v3714 = vmul.f32 %v3691, %v3710 (stack83)
        %v3718 = vadd.f32 %v3687, %v3714 (stack82)
        %v3722 = vmul.f32 %v3718, %v3710 (stack83)
        %v3726 = vadd.f32 %v3683, %v3722 (stack82)
        %v3730 = vmul.f32 %v3726, %v3710 (stack83)
        %v3734 = vadd.f32 %v3679, %v3730 (stack82)
        %v3738 = vmul.f32 %v3734, %v3710 (stack83)
        %v3742 = vadd.f32 %v3675, %v3738 (stack82)
        %v3746 = vmul.f32 %v3742, %v3710 (stack83)
        %v3750 = vadd.f32 %v3671, %v3746 (stack82)
        %v3754 = vmul.f32 %v3750, %v3710 (stack83)
        %v3758 = vadd.f32 %v3667, %v3754 (stack82)
        %v3762 = vmul.f32 %v3758, %v3710 (stack83)
        %v3766 = vadd.f32 %v3663, %v3762 (stack82)
        %v3770 = vmul.f32 %v3766, %v3710 (stack83)
        %v3774 = vadd.f32 %v3659, %v3770 (stack82)
        %v3778 = vmul.f32 %v3774, %v3625 (stack83)
        %v3782 = vsel /*vm=*/%vm3630, /*on_true_vy=*/%v3635, /*on_false_vx=*/%v3778 (stack72)
        %v3786 = vmul.f32 %v3782, 1.4140625 (stack83)
        %s3788 = scalar_lea.vmem %s280, 768 [#allocation0] (stack107)
        %v3789 = vpack.c.bf16 0.0, %v3786 (stack104)
        %3790 = vst [vmem:[%s3788] sm:$0xf] /*vst_source=*/%v3789 (stack105)
        %s3791 = sadd.s32 %s341, 7 (stack106)
        %s3792 = sshrl.u32 %s3791, 7 (stack49)
        %p3793 = scmp.lt.s32.totalorder 0, %s3792 (stack50)
        %s3794 = scalar_select /*predicate=*/%p3793, /*on_true=*/0, /*on_false=*/%s3792 (stack51)
        %s3795 = sand.u32 %s3791, 127 /* smod.u32 w/div 128 */ (stack52)
        %s3796 = sshrl.u32 %s3795, 7 (stack53)
        %s3797 = sand.u32 %s3795, 127 /* smod.u32 w/div 128 */ (stack54)
        %s3798 = scalar_lea.vmem %s4, %s3794 (stack56)
        %s3800 = scalar_lea.vmem %s3798, %s3796 (stack57)
        %v3801 = vld [vmem:[%s3800] ss:$0 sm:$0xff] (stack58)
        %3802 = vbcast.lane.b32.xlu0 %v3801, %s3797 (stack63)
        %v3803 = vpop.permute.xlu0 %3802 (stack64)
        %s3804 = sadd.s32 %s349, 7 (stack106)
        %s3805 = sshrl.u32 %s3804, 7 (stack49)
        %p3806 = scmp.lt.s32.totalorder 0, %s3805 (stack50)
        %s3807 = scalar_select /*predicate=*/%p3806, /*on_true=*/0, /*on_false=*/%s3805 (stack51)
        %s3808 = sand.u32 %s3804, 127 /* smod.u32 w/div 128 */ (stack52)
        %s3809 = sshrl.u32 %s3808, 7 (stack53)
        %s3810 = sand.u32 %s3808, 127 /* smod.u32 w/div 128 */ (stack54)
        %s3811 = scalar_lea.vmem %s6, %s3807 (stack56)
        %s3813 = scalar_lea.vmem %s3811, %s3809 (stack57)
        %v3814 = vld [vmem:[%s3813] ss:$0 sm:$0xff] (stack58)
        %3815 = vbcast.lane.b32.xlu0 %v3814, %s3810 (stack63)
        %v3816 = vpop.permute.xlu0 %3815 (stack64)
        %v3819 = vadd.s32 %v3816, %v396 (stack65)
        %s3821 = smul.u32 128, %s27 (stack66)
        %v3822 = vlaneseq (stack67)
        %v3823 = vand.u32 %v3822, 127 (stack68)
        %v3824 = vstv %s3821 (stack69)
        %v3825 = vadd.s32 %v3823, %v3824 (stack70)
        %v3829 = vadd.s32 %v3819, %v3825 (stack65)
        %vm3833 = vcmp.lt.u32.totalorder %v3829, %v3819 (stack71)
        %vm3838 = vcmp.lt.u32.totalorder %v3819, %v3816 (stack71)
        %v3843 = vadd.s32 %v3803, %v368 (stack65)
        %v3847 = vadd.s32 %v3843, 1 (stack65)
        %v3851 = vsel /*vm=*/%vm3838, /*on_true_vy=*/%v3847, /*on_false_vx=*/%v3843 (stack72)
        %v3855 = vadd.s32 %v3851, 1 (stack65)
        %v3859 = vsel /*vm=*/%vm3833, /*on_true_vy=*/%v3855, /*on_false_vx=*/%v3851 (stack72)
        %v3864 = vadd.s32 %v3859, %v10 (stack65)
        %v3868 = vadd.s32 %v3829, %v9 (stack65)
        %v3872 = vadd.s32 %v3864, %v3868 (stack65)
        %v3874 = vshll.u32 %v3868, 13 (stack73)
        %v3875 = vshrl.u32 %v3868, 19 (stack74)
        %v3876 = vor.u32 %v3874, %v3875 (stack75)
        %v3877 = vxor.u32 %v3872, %v3876 (stack76)
        %v3880 = vadd.s32 %v3872, %v3877 (stack65)
        %v3882 = vshll.u32 %v3877, 15 (stack73)
        %v3883 = vshrl.u32 %v3877, 17 (stack74)
        %v3884 = vor.u32 %v3882, %v3883 (stack75)
        %v3885 = vxor.u32 %v3880, %v3884 (stack76)
        %v3888 = vadd.s32 %v3880, %v3885 (stack65)
        %v3890 = vshll.u32 %v3885, 26 (stack73)
        %v3891 = vshrl.u32 %v3885, 6 (stack74)
        %v3892 = vor.u32 %v3890, %v3891 (stack75)
        %v3893 = vxor.u32 %v3888, %v3892 (stack76)
        %v3896 = vadd.s32 %v3888, %v3893 (stack65)
        %v3900 = vadd.s32 %v3896, %v9 (stack65)
        %v3902 = vshll.u32 %v3893, 6 (stack73)
        %v3903 = vshrl.u32 %v3893, 26 (stack74)
        %v3904 = vor.u32 %v3902, %v3903 (stack75)
        %v3905 = vxor.u32 %v3896, %v3904 (stack76)
        %v3908 = vadd.s32 %v3905, %v8 (stack65)
        %v3912 = vadd.s32 %v3908, 1 (stack65)
        %v3916 = vadd.s32 %v3900, %v3912 (stack65)
        %v3918 = vshll.u32 %v3912, 17 (stack73)
        %v3919 = vshrl.u32 %v3912, 15 (stack74)
        %v3920 = vor.u32 %v3918, %v3919 (stack75)
        %v3921 = vxor.u32 %v3916, %v3920 (stack76)
        %v3924 = vadd.s32 %v3916, %v3921 (stack65)
        %v3926 = vshll.u32 %v3921, 29 (stack73)
        %v3927 = vshrl.u32 %v3921, 3 (stack74)
        %v3928 = vor.u32 %v3926, %v3927 (stack75)
        %v3929 = vxor.u32 %v3924, %v3928 (stack76)
        %v3932 = vadd.s32 %v3924, %v3929 (stack65)
        %v3934 = vshll.u32 %v3929, 16 (stack73)
        %v3935 = vshrl.u32 %v3929, 16 (stack74)
        %v3936 = vor.u32 %v3934, %v3935 (stack75)
        %v3937 = vxor.u32 %v3932, %v3936 (stack76)
        %v3940 = vadd.s32 %v3932, %v3937 (stack65)
        %v3944 = vadd.s32 %v3940, %v8 (stack65)
        %v3946 = vshll.u32 %v3937, 24 (stack73)
        %v3947 = vshrl.u32 %v3937, 8 (stack74)
        %v3948 = vor.u32 %v3946, %v3947 (stack75)
        %v3949 = vxor.u32 %v3940, %v3948 (stack76)
        %v3952 = vadd.s32 %v3949, %v10 (stack65)
        %v3956 = vadd.s32 %v3952, 2 (stack65)
        %v3960 = vadd.s32 %v3944, %v3956 (stack65)
        %v3962 = vshll.u32 %v3956, 13 (stack73)
        %v3963 = vshrl.u32 %v3956, 19 (stack74)
        %v3964 = vor.u32 %v3962, %v3963 (stack75)
        %v3965 = vxor.u32 %v3960, %v3964 (stack76)
        %v3968 = vadd.s32 %v3960, %v3965 (stack65)
        %v3970 = vshll.u32 %v3965, 15 (stack73)
        %v3971 = vshrl.u32 %v3965, 17 (stack74)
        %v3972 = vor.u32 %v3970, %v3971 (stack75)
        %v3973 = vxor.u32 %v3968, %v3972 (stack76)
        %v3976 = vadd.s32 %v3968, %v3973 (stack65)
        %v3978 = vshll.u32 %v3973, 26 (stack73)
        %v3979 = vshrl.u32 %v3973, 6 (stack74)
        %v3980 = vor.u32 %v3978, %v3979 (stack75)
        %v3981 = vxor.u32 %v3976, %v3980 (stack76)
        %v3984 = vadd.s32 %v3976, %v3981 (stack65)
        %v3988 = vadd.s32 %v3984, %v10 (stack65)
        %v3990 = vshll.u32 %v3981, 6 (stack73)
        %v3991 = vshrl.u32 %v3981, 26 (stack74)
        %v3992 = vor.u32 %v3990, %v3991 (stack75)
        %v3993 = vxor.u32 %v3984, %v3992 (stack76)
        %v3996 = vadd.s32 %v3993, %v9 (stack65)
        %v4000 = vadd.s32 %v3996, 3 (stack65)
        %v4004 = vadd.s32 %v3988, %v4000 (stack65)
        %v4006 = vshll.u32 %v4000, 17 (stack73)
        %v4007 = vshrl.u32 %v4000, 15 (stack74)
        %v4008 = vor.u32 %v4006, %v4007 (stack75)
        %v4009 = vxor.u32 %v4004, %v4008 (stack76)
        %v4012 = vadd.s32 %v4004, %v4009 (stack65)
        %v4014 = vshll.u32 %v4009, 29 (stack73)
        %v4015 = vshrl.u32 %v4009, 3 (stack74)
        %v4016 = vor.u32 %v4014, %v4015 (stack75)
        %v4017 = vxor.u32 %v4012, %v4016 (stack76)
        %v4020 = vadd.s32 %v4012, %v4017 (stack65)
        %v4022 = vshll.u32 %v4017, 16 (stack73)
        %v4023 = vshrl.u32 %v4017, 16 (stack74)
        %v4024 = vor.u32 %v4022, %v4023 (stack75)
        %v4025 = vxor.u32 %v4020, %v4024 (stack76)
        %v4028 = vadd.s32 %v4020, %v4025 (stack65)
        %v4032 = vadd.s32 %v4028, %v9 (stack65)
        %v4034 = vshll.u32 %v4025, 24 (stack73)
        %v4035 = vshrl.u32 %v4025, 8 (stack74)
        %v4036 = vor.u32 %v4034, %v4035 (stack75)
        %v4037 = vxor.u32 %v4028, %v4036 (stack76)
        %v4040 = vadd.s32 %v4037, %v8 (stack65)
        %v4044 = vadd.s32 %v4040, 4 (stack65)
        %v4048 = vadd.s32 %v4032, %v4044 (stack65)
        %v4050 = vshll.u32 %v4044, 13 (stack73)
        %v4051 = vshrl.u32 %v4044, 19 (stack74)
        %v4052 = vor.u32 %v4050, %v4051 (stack75)
        %v4053 = vxor.u32 %v4048, %v4052 (stack76)
        %v4056 = vadd.s32 %v4048, %v4053 (stack65)
        %v4058 = vshll.u32 %v4053, 15 (stack73)
        %v4059 = vshrl.u32 %v4053, 17 (stack74)
        %v4060 = vor.u32 %v4058, %v4059 (stack75)
        %v4061 = vxor.u32 %v4056, %v4060 (stack76)
        %v4064 = vadd.s32 %v4056, %v4061 (stack65)
        %v4066 = vshll.u32 %v4061, 26 (stack73)
        %v4067 = vshrl.u32 %v4061, 6 (stack74)
        %v4068 = vor.u32 %v4066, %v4067 (stack75)
        %v4069 = vxor.u32 %v4064, %v4068 (stack76)
        %v4072 = vadd.s32 %v4064, %v4069 (stack65)
        %v4076 = vadd.s32 %v4072, %v8 (stack65)
        %v4078 = vshll.u32 %v4069, 6 (stack73)
        %v4079 = vshrl.u32 %v4069, 26 (stack74)
        %v4080 = vor.u32 %v4078, %v4079 (stack75)
        %v4081 = vxor.u32 %v4072, %v4080 (stack76)
        %v4084 = vadd.s32 %v4081, %v10 (stack65)
        %v4088 = vadd.s32 %v4084, 5 (stack65)
        %v4090 = vxor.u32 %v4076, %v4088 (stack76)
        %v4091 = vand.u32.u8 %v4090, 255 (stack77)
        %v4092 = vand.u32 %v4091, 65535 (stack78)
        %v4093 = vshrl.u32 %v4092, 1 (stack79)
        %v4094 = vor.u32 %v4093, 16256 (stack75)
        %v4095 = vand.u32.u16 %v4094, 65535 (stack80)
        %v4096 = vunpack.i.l.bf16 %v4095 (stack81)
        %v4100 = vadd.f32 %v4096, -1.0 (stack82)
        %v4104 = vmul.f32 %v4100, 2.0 (stack83)
        %v4108 = vadd.f32 %v4104, -0.99609375 (stack82)
        %v4112 = vmax.f32 -0.99609375, %v4108 (stack84)
        %v4114 = vand.u32 2147483647, %v4112 (stack85)
        %vm4117 = vcmp.eq.f32.partialorder %v4114, 1.0 (stack86)
        %v4122 = vmul.f32 %v4112, inf (stack83)
        %v4124 = vxor.u32 %v4112, 2147483648 (stack87)
        %v4127 = vmul.f32 %v4112, %v4124 (stack83)
        %v4129 = vadd.f32 %v4127, 1.0 (stack88)
        %v4130 = vlog2.pop %v4129 (stack89)
        %v4131 = vmul.f32 %v4130, 0.6931472 (stack90)
        %v4132 = vmul.f32 -0.5, %v4127 (stack91)
        %v4133 = vadd.f32 %v4132, 1.0 (stack92)
        %v4134 = vmul.f32 %v4133, %v4127 (stack93)
        %v4135 = vand.u32 2147483647, %v4127 (stack94)
        %vm4136 = vcmp.lt.f32.partialorder %v4135, 0.0004427343 (stack95)
        %v4137 = vsel /*vm=*/%vm4136, /*on_true_vy=*/%v4134, /*on_false_vx=*/%v4131 (stack96)
        %v4138 = vxor.u32 %v4137, 2147483648 (stack87)
        %vm4141 = vcmp.lt.f32.partialorder %v4138, 5.0 (stack86)
        %v4146 = vsel /*vm=*/%vm4141, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v4150 = vsel /*vm=*/%vm4141, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v4154 = vsel /*vm=*/%vm4141, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v4158 = vsel /*vm=*/%vm4141, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v4162 = vsel /*vm=*/%vm4141, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v4166 = vsel /*vm=*/%vm4141, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v4170 = vsel /*vm=*/%vm4141, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v4174 = vsel /*vm=*/%vm4141, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v4178 = vsel /*vm=*/%vm4141, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v4182 = vadd.f32 %v4138, -2.5 (stack82)
        %v4184 = vrsqrt.pop %v4138 (stack97)
        %v4185 = vmul.f32 %v4138, %v4184 (stack98)
        %vm4186 = vcmp.eq.f32.partialorder %v4138, inf (stack99)
        %v4187 = vsel /*vm=*/%vm4186, /*on_true_vy=*/%v4138, /*on_false_vx=*/%v4185 (stack100)
        %vm4188 = vcmp.eq.f32.partialorder %v4138, 0.0 (stack101)
        %v4189 = vand.u32 %v4138, 2147483648 (stack102)
        %v4190 = vsel /*vm=*/%vm4188, /*on_true_vy=*/%v4189, /*on_false_vx=*/%v4187 (stack103)
        %v4193 = vadd.f32 %v4190, -3.0 (stack82)
        %v4197 = vsel /*vm=*/%vm4141, /*on_true_vy=*/%v4182, /*on_false_vx=*/%v4193 (stack72)
        %v4201 = vmul.f32 %v4178, %v4197 (stack83)
        %v4205 = vadd.f32 %v4174, %v4201 (stack82)
        %v4209 = vmul.f32 %v4205, %v4197 (stack83)
        %v4213 = vadd.f32 %v4170, %v4209 (stack82)
        %v4217 = vmul.f32 %v4213, %v4197 (stack83)
        %v4221 = vadd.f32 %v4166, %v4217 (stack82)
        %v4225 = vmul.f32 %v4221, %v4197 (stack83)
        %v4229 = vadd.f32 %v4162, %v4225 (stack82)
        %v4233 = vmul.f32 %v4229, %v4197 (stack83)
        %v4237 = vadd.f32 %v4158, %v4233 (stack82)
        %v4241 = vmul.f32 %v4237, %v4197 (stack83)
        %v4245 = vadd.f32 %v4154, %v4241 (stack82)
        %v4249 = vmul.f32 %v4245, %v4197 (stack83)
        %v4253 = vadd.f32 %v4150, %v4249 (stack82)
        %v4257 = vmul.f32 %v4253, %v4197 (stack83)
        %v4261 = vadd.f32 %v4146, %v4257 (stack82)
        %v4265 = vmul.f32 %v4261, %v4112 (stack83)
        %v4269 = vsel /*vm=*/%vm4117, /*on_true_vy=*/%v4122, /*on_false_vx=*/%v4265 (stack72)
        %v4273 = vmul.f32 %v4269, 1.4140625 (stack83)
        %s4275 = scalar_lea.vmem %s280, 896 [#allocation0] (stack107)
        %v4276 = vpack.c.bf16 0.0, %v4273 (stack104)
        %4277 = vst [vmem:[%s4275] sm:$0xf] /*vst_source=*/%v4276 (stack105)
        %s4278 = sadd.s32 %s339, 8 (stack106)
        %s4279 = sshrl.u32 %s4278, 10 (stack49)
        %p4280 = scmp.lt.s32.totalorder 1, %s4279 (stack50)
        %s4281 = scalar_select /*predicate=*/%p4280, /*on_true=*/1, /*on_false=*/%s4279 (stack51)
        %s4282 = sand.u32 %s4278, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s4283 = sshrl.u32 %s4282, 7 (stack53)
        %s4284 = sand.u32 %s4282, 127 /* smod.u32 w/div 128 */ (stack54)
        %s4285 = smul.addr %s4281, 8 (stack55)
        %s4286 = scalar_lea.vmem %s3, %s4285 (stack56)
        %s4288 = scalar_lea.vmem %s4286, %s4283 (stack57)
        %v4289 = vld [vmem:[%s4288] ss:$0 sm:$0xff] (stack58)
        %s4290 = sand.u32 %s4284, 255 (stack59)
        %s4292 = sor.u32 256, %s4290 (stack60)
        %4293 = vbcast.lane.b32.xlu0 %v4289, %s4292 (stack61)
        %v4294 = vpop.permute.xlu0 %4293 (stack62)
        %s4295 = sadd.s32 %s347, 8 (stack106)
        %s4296 = sshrl.u32 %s4295, 10 (stack49)
        %p4297 = scmp.lt.s32.totalorder 1, %s4296 (stack50)
        %s4298 = scalar_select /*predicate=*/%p4297, /*on_true=*/1, /*on_false=*/%s4296 (stack51)
        %s4299 = sand.u32 %s4295, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s4300 = sshrl.u32 %s4299, 7 (stack53)
        %s4301 = sand.u32 %s4299, 127 /* smod.u32 w/div 128 */ (stack54)
        %s4302 = smul.addr %s4298, 8 (stack55)
        %s4303 = scalar_lea.vmem %s5, %s4302 (stack56)
        %s4305 = scalar_lea.vmem %s4303, %s4300 (stack57)
        %v4306 = vld [vmem:[%s4305] ss:$0 sm:$0xff] (stack58)
        %s4307 = sand.u32 %s4301, 255 (stack59)
        %s4309 = sor.u32 256, %s4307 (stack60)
        %4310 = vbcast.lane.b32.xlu0 %v4306, %s4309 (stack61)
        %v4311 = vpop.permute.xlu0 %4310 (stack62)
        %v4314 = vadd.s32 %v408, %v4311 (stack65)
        %s4316 = smul.u32 128, %s27 (stack66)
        %v4317 = vlaneseq (stack67)
        %v4318 = vand.u32 %v4317, 127 (stack68)
        %v4319 = vstv %s4316 (stack69)
        %v4320 = vadd.s32 %v4318, %v4319 (stack70)
        %v4324 = vadd.s32 %v4314, %v4320 (stack65)
        %vm4328 = vcmp.lt.u32.totalorder %v4324, %v4314 (stack71)
        %vm4333 = vcmp.lt.u32.totalorder %v4314, %v408 (stack71)
        %v4338 = vadd.s32 %v380, %v4294 (stack65)
        %v4342 = vadd.s32 %v4338, 1 (stack65)
        %v4346 = vsel /*vm=*/%vm4333, /*on_true_vy=*/%v4342, /*on_false_vx=*/%v4338 (stack72)
        %v4350 = vadd.s32 %v4346, 1 (stack65)
        %v4354 = vsel /*vm=*/%vm4328, /*on_true_vy=*/%v4350, /*on_false_vx=*/%v4346 (stack72)
        %v4359 = vadd.s32 %v4354, %v10 (stack65)
        %v4363 = vadd.s32 %v4324, %v9 (stack65)
        %v4367 = vadd.s32 %v4359, %v4363 (stack65)
        %v4369 = vshll.u32 %v4363, 13 (stack73)
        %v4370 = vshrl.u32 %v4363, 19 (stack74)
        %v4371 = vor.u32 %v4369, %v4370 (stack75)
        %v4372 = vxor.u32 %v4367, %v4371 (stack76)
        %v4375 = vadd.s32 %v4367, %v4372 (stack65)
        %v4377 = vshll.u32 %v4372, 15 (stack73)
        %v4378 = vshrl.u32 %v4372, 17 (stack74)
        %v4379 = vor.u32 %v4377, %v4378 (stack75)
        %v4380 = vxor.u32 %v4375, %v4379 (stack76)
        %v4383 = vadd.s32 %v4375, %v4380 (stack65)
        %v4385 = vshll.u32 %v4380, 26 (stack73)
        %v4386 = vshrl.u32 %v4380, 6 (stack74)
        %v4387 = vor.u32 %v4385, %v4386 (stack75)
        %v4388 = vxor.u32 %v4383, %v4387 (stack76)
        %v4391 = vadd.s32 %v4383, %v4388 (stack65)
        %v4395 = vadd.s32 %v4391, %v9 (stack65)
        %v4397 = vshll.u32 %v4388, 6 (stack73)
        %v4398 = vshrl.u32 %v4388, 26 (stack74)
        %v4399 = vor.u32 %v4397, %v4398 (stack75)
        %v4400 = vxor.u32 %v4391, %v4399 (stack76)
        %v4403 = vadd.s32 %v4400, %v8 (stack65)
        %v4407 = vadd.s32 %v4403, 1 (stack65)
        %v4411 = vadd.s32 %v4395, %v4407 (stack65)
        %v4413 = vshll.u32 %v4407, 17 (stack73)
        %v4414 = vshrl.u32 %v4407, 15 (stack74)
        %v4415 = vor.u32 %v4413, %v4414 (stack75)
        %v4416 = vxor.u32 %v4411, %v4415 (stack76)
        %v4419 = vadd.s32 %v4411, %v4416 (stack65)
        %v4421 = vshll.u32 %v4416, 29 (stack73)
        %v4422 = vshrl.u32 %v4416, 3 (stack74)
        %v4423 = vor.u32 %v4421, %v4422 (stack75)
        %v4424 = vxor.u32 %v4419, %v4423 (stack76)
        %v4427 = vadd.s32 %v4419, %v4424 (stack65)
        %v4429 = vshll.u32 %v4424, 16 (stack73)
        %v4430 = vshrl.u32 %v4424, 16 (stack74)
        %v4431 = vor.u32 %v4429, %v4430 (stack75)
        %v4432 = vxor.u32 %v4427, %v4431 (stack76)
        %v4435 = vadd.s32 %v4427, %v4432 (stack65)
        %v4439 = vadd.s32 %v4435, %v8 (stack65)
        %v4441 = vshll.u32 %v4432, 24 (stack73)
        %v4442 = vshrl.u32 %v4432, 8 (stack74)
        %v4443 = vor.u32 %v4441, %v4442 (stack75)
        %v4444 = vxor.u32 %v4435, %v4443 (stack76)
        %v4447 = vadd.s32 %v4444, %v10 (stack65)
        %v4451 = vadd.s32 %v4447, 2 (stack65)
        %v4455 = vadd.s32 %v4439, %v4451 (stack65)
        %v4457 = vshll.u32 %v4451, 13 (stack73)
        %v4458 = vshrl.u32 %v4451, 19 (stack74)
        %v4459 = vor.u32 %v4457, %v4458 (stack75)
        %v4460 = vxor.u32 %v4455, %v4459 (stack76)
        %v4463 = vadd.s32 %v4455, %v4460 (stack65)
        %v4465 = vshll.u32 %v4460, 15 (stack73)
        %v4466 = vshrl.u32 %v4460, 17 (stack74)
        %v4467 = vor.u32 %v4465, %v4466 (stack75)
        %v4468 = vxor.u32 %v4463, %v4467 (stack76)
        %v4471 = vadd.s32 %v4463, %v4468 (stack65)
        %v4473 = vshll.u32 %v4468, 26 (stack73)
        %v4474 = vshrl.u32 %v4468, 6 (stack74)
        %v4475 = vor.u32 %v4473, %v4474 (stack75)
        %v4476 = vxor.u32 %v4471, %v4475 (stack76)
        %v4479 = vadd.s32 %v4471, %v4476 (stack65)
        %v4483 = vadd.s32 %v4479, %v10 (stack65)
        %v4485 = vshll.u32 %v4476, 6 (stack73)
        %v4486 = vshrl.u32 %v4476, 26 (stack74)
        %v4487 = vor.u32 %v4485, %v4486 (stack75)
        %v4488 = vxor.u32 %v4479, %v4487 (stack76)
        %v4491 = vadd.s32 %v4488, %v9 (stack65)
        %v4495 = vadd.s32 %v4491, 3 (stack65)
        %v4499 = vadd.s32 %v4483, %v4495 (stack65)
        %v4501 = vshll.u32 %v4495, 17 (stack73)
        %v4502 = vshrl.u32 %v4495, 15 (stack74)
        %v4503 = vor.u32 %v4501, %v4502 (stack75)
        %v4504 = vxor.u32 %v4499, %v4503 (stack76)
        %v4507 = vadd.s32 %v4499, %v4504 (stack65)
        %v4509 = vshll.u32 %v4504, 29 (stack73)
        %v4510 = vshrl.u32 %v4504, 3 (stack74)
        %v4511 = vor.u32 %v4509, %v4510 (stack75)
        %v4512 = vxor.u32 %v4507, %v4511 (stack76)
        %v4515 = vadd.s32 %v4507, %v4512 (stack65)
        %v4517 = vshll.u32 %v4512, 16 (stack73)
        %v4518 = vshrl.u32 %v4512, 16 (stack74)
        %v4519 = vor.u32 %v4517, %v4518 (stack75)
        %v4520 = vxor.u32 %v4515, %v4519 (stack76)
        %v4523 = vadd.s32 %v4515, %v4520 (stack65)
        %v4527 = vadd.s32 %v4523, %v9 (stack65)
        %v4529 = vshll.u32 %v4520, 24 (stack73)
        %v4530 = vshrl.u32 %v4520, 8 (stack74)
        %v4531 = vor.u32 %v4529, %v4530 (stack75)
        %v4532 = vxor.u32 %v4523, %v4531 (stack76)
        %v4535 = vadd.s32 %v4532, %v8 (stack65)
        %v4539 = vadd.s32 %v4535, 4 (stack65)
        %v4543 = vadd.s32 %v4527, %v4539 (stack65)
        %v4545 = vshll.u32 %v4539, 13 (stack73)
        %v4546 = vshrl.u32 %v4539, 19 (stack74)
        %v4547 = vor.u32 %v4545, %v4546 (stack75)
        %v4548 = vxor.u32 %v4543, %v4547 (stack76)
        %v4551 = vadd.s32 %v4543, %v4548 (stack65)
        %v4553 = vshll.u32 %v4548, 15 (stack73)
        %v4554 = vshrl.u32 %v4548, 17 (stack74)
        %v4555 = vor.u32 %v4553, %v4554 (stack75)
        %v4556 = vxor.u32 %v4551, %v4555 (stack76)
        %v4559 = vadd.s32 %v4551, %v4556 (stack65)
        %v4561 = vshll.u32 %v4556, 26 (stack73)
        %v4562 = vshrl.u32 %v4556, 6 (stack74)
        %v4563 = vor.u32 %v4561, %v4562 (stack75)
        %v4564 = vxor.u32 %v4559, %v4563 (stack76)
        %v4567 = vadd.s32 %v4559, %v4564 (stack65)
        %v4571 = vadd.s32 %v4567, %v8 (stack65)
        %v4573 = vshll.u32 %v4564, 6 (stack73)
        %v4574 = vshrl.u32 %v4564, 26 (stack74)
        %v4575 = vor.u32 %v4573, %v4574 (stack75)
        %v4576 = vxor.u32 %v4567, %v4575 (stack76)
        %v4579 = vadd.s32 %v4576, %v10 (stack65)
        %v4583 = vadd.s32 %v4579, 5 (stack65)
        %v4585 = vxor.u32 %v4571, %v4583 (stack76)
        %v4586 = vand.u32.u8 %v4585, 255 (stack77)
        %v4587 = vand.u32 %v4586, 65535 (stack78)
        %v4588 = vshrl.u32 %v4587, 1 (stack79)
        %v4589 = vor.u32 %v4588, 16256 (stack75)
        %v4590 = vand.u32.u16 %v4589, 65535 (stack80)
        %v4591 = vunpack.i.l.bf16 %v4590 (stack81)
        %v4595 = vadd.f32 %v4591, -1.0 (stack82)
        %v4599 = vmul.f32 %v4595, 2.0 (stack83)
        %v4603 = vadd.f32 %v4599, -0.99609375 (stack82)
        %v4607 = vmax.f32 -0.99609375, %v4603 (stack84)
        %v4609 = vand.u32 2147483647, %v4607 (stack85)
        %vm4612 = vcmp.eq.f32.partialorder %v4609, 1.0 (stack86)
        %v4617 = vmul.f32 %v4607, inf (stack83)
        %v4619 = vxor.u32 %v4607, 2147483648 (stack87)
        %v4622 = vmul.f32 %v4607, %v4619 (stack83)
        %v4624 = vadd.f32 %v4622, 1.0 (stack88)
        %v4625 = vlog2.pop %v4624 (stack89)
        %v4626 = vmul.f32 %v4625, 0.6931472 (stack90)
        %v4627 = vmul.f32 -0.5, %v4622 (stack91)
        %v4628 = vadd.f32 %v4627, 1.0 (stack92)
        %v4629 = vmul.f32 %v4628, %v4622 (stack93)
        %v4630 = vand.u32 2147483647, %v4622 (stack94)
        %vm4631 = vcmp.lt.f32.partialorder %v4630, 0.0004427343 (stack95)
        %v4632 = vsel /*vm=*/%vm4631, /*on_true_vy=*/%v4629, /*on_false_vx=*/%v4626 (stack96)
        %v4633 = vxor.u32 %v4632, 2147483648 (stack87)
        %vm4636 = vcmp.lt.f32.partialorder %v4633, 5.0 (stack86)
        %v4641 = vsel /*vm=*/%vm4636, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v4645 = vsel /*vm=*/%vm4636, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v4649 = vsel /*vm=*/%vm4636, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v4653 = vsel /*vm=*/%vm4636, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v4657 = vsel /*vm=*/%vm4636, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v4661 = vsel /*vm=*/%vm4636, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v4665 = vsel /*vm=*/%vm4636, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v4669 = vsel /*vm=*/%vm4636, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v4673 = vsel /*vm=*/%vm4636, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v4677 = vadd.f32 %v4633, -2.5 (stack82)
        %v4679 = vrsqrt.pop %v4633 (stack97)
        %v4680 = vmul.f32 %v4633, %v4679 (stack98)
        %vm4681 = vcmp.eq.f32.partialorder %v4633, inf (stack99)
        %v4682 = vsel /*vm=*/%vm4681, /*on_true_vy=*/%v4633, /*on_false_vx=*/%v4680 (stack100)
        %vm4683 = vcmp.eq.f32.partialorder %v4633, 0.0 (stack101)
        %v4684 = vand.u32 %v4633, 2147483648 (stack102)
        %v4685 = vsel /*vm=*/%vm4683, /*on_true_vy=*/%v4684, /*on_false_vx=*/%v4682 (stack103)
        %v4688 = vadd.f32 %v4685, -3.0 (stack82)
        %v4692 = vsel /*vm=*/%vm4636, /*on_true_vy=*/%v4677, /*on_false_vx=*/%v4688 (stack72)
        %v4696 = vmul.f32 %v4673, %v4692 (stack83)
        %v4700 = vadd.f32 %v4669, %v4696 (stack82)
        %v4704 = vmul.f32 %v4700, %v4692 (stack83)
        %v4708 = vadd.f32 %v4665, %v4704 (stack82)
        %v4712 = vmul.f32 %v4708, %v4692 (stack83)
        %v4716 = vadd.f32 %v4661, %v4712 (stack82)
        %v4720 = vmul.f32 %v4716, %v4692 (stack83)
        %v4724 = vadd.f32 %v4657, %v4720 (stack82)
        %v4728 = vmul.f32 %v4724, %v4692 (stack83)
        %v4732 = vadd.f32 %v4653, %v4728 (stack82)
        %v4736 = vmul.f32 %v4732, %v4692 (stack83)
        %v4740 = vadd.f32 %v4649, %v4736 (stack82)
        %v4744 = vmul.f32 %v4740, %v4692 (stack83)
        %v4748 = vadd.f32 %v4645, %v4744 (stack82)
        %v4752 = vmul.f32 %v4748, %v4692 (stack83)
        %v4756 = vadd.f32 %v4641, %v4752 (stack82)
        %v4760 = vmul.f32 %v4756, %v4607 (stack83)
        %v4764 = vsel /*vm=*/%vm4612, /*on_true_vy=*/%v4617, /*on_false_vx=*/%v4760 (stack72)
        %v4768 = vmul.f32 %v4764, 1.4140625 (stack83)
        %s4770 = scalar_lea.vmem %s280, 4 [#allocation0] (stack107)
        %v4771 = vpack.c.bf16 0.0, %v4768 (stack104)
        %4772 = vst [vmem:[%s4770] sm:$0xf] /*vst_source=*/%v4771 (stack105)
        %v4775 = vadd.s32 %v894, %v4311 (stack65)
        %s4777 = smul.u32 128, %s27 (stack66)
        %v4778 = vlaneseq (stack67)
        %v4779 = vand.u32 %v4778, 127 (stack68)
        %v4780 = vstv %s4777 (stack69)
        %v4781 = vadd.s32 %v4779, %v4780 (stack70)
        %v4785 = vadd.s32 %v4775, %v4781 (stack65)
        %vm4789 = vcmp.lt.u32.totalorder %v4785, %v4775 (stack71)
        %vm4794 = vcmp.lt.u32.totalorder %v4775, %v894 (stack71)
        %v4799 = vadd.s32 %v881, %v4294 (stack65)
        %v4803 = vadd.s32 %v4799, 1 (stack65)
        %v4807 = vsel /*vm=*/%vm4794, /*on_true_vy=*/%v4803, /*on_false_vx=*/%v4799 (stack72)
        %v4811 = vadd.s32 %v4807, 1 (stack65)
        %v4815 = vsel /*vm=*/%vm4789, /*on_true_vy=*/%v4811, /*on_false_vx=*/%v4807 (stack72)
        %v4820 = vadd.s32 %v4815, %v10 (stack65)
        %v4824 = vadd.s32 %v4785, %v9 (stack65)
        %v4828 = vadd.s32 %v4820, %v4824 (stack65)
        %v4830 = vshll.u32 %v4824, 13 (stack73)
        %v4831 = vshrl.u32 %v4824, 19 (stack74)
        %v4832 = vor.u32 %v4830, %v4831 (stack75)
        %v4833 = vxor.u32 %v4828, %v4832 (stack76)
        %v4836 = vadd.s32 %v4828, %v4833 (stack65)
        %v4838 = vshll.u32 %v4833, 15 (stack73)
        %v4839 = vshrl.u32 %v4833, 17 (stack74)
        %v4840 = vor.u32 %v4838, %v4839 (stack75)
        %v4841 = vxor.u32 %v4836, %v4840 (stack76)
        %v4844 = vadd.s32 %v4836, %v4841 (stack65)
        %v4846 = vshll.u32 %v4841, 26 (stack73)
        %v4847 = vshrl.u32 %v4841, 6 (stack74)
        %v4848 = vor.u32 %v4846, %v4847 (stack75)
        %v4849 = vxor.u32 %v4844, %v4848 (stack76)
        %v4852 = vadd.s32 %v4844, %v4849 (stack65)
        %v4856 = vadd.s32 %v4852, %v9 (stack65)
        %v4858 = vshll.u32 %v4849, 6 (stack73)
        %v4859 = vshrl.u32 %v4849, 26 (stack74)
        %v4860 = vor.u32 %v4858, %v4859 (stack75)
        %v4861 = vxor.u32 %v4852, %v4860 (stack76)
        %v4864 = vadd.s32 %v4861, %v8 (stack65)
        %v4868 = vadd.s32 %v4864, 1 (stack65)
        %v4872 = vadd.s32 %v4856, %v4868 (stack65)
        %v4874 = vshll.u32 %v4868, 17 (stack73)
        %v4875 = vshrl.u32 %v4868, 15 (stack74)
        %v4876 = vor.u32 %v4874, %v4875 (stack75)
        %v4877 = vxor.u32 %v4872, %v4876 (stack76)
        %v4880 = vadd.s32 %v4872, %v4877 (stack65)
        %v4882 = vshll.u32 %v4877, 29 (stack73)
        %v4883 = vshrl.u32 %v4877, 3 (stack74)
        %v4884 = vor.u32 %v4882, %v4883 (stack75)
        %v4885 = vxor.u32 %v4880, %v4884 (stack76)
        %v4888 = vadd.s32 %v4880, %v4885 (stack65)
        %v4890 = vshll.u32 %v4885, 16 (stack73)
        %v4891 = vshrl.u32 %v4885, 16 (stack74)
        %v4892 = vor.u32 %v4890, %v4891 (stack75)
        %v4893 = vxor.u32 %v4888, %v4892 (stack76)
        %v4896 = vadd.s32 %v4888, %v4893 (stack65)
        %v4900 = vadd.s32 %v4896, %v8 (stack65)
        %v4902 = vshll.u32 %v4893, 24 (stack73)
        %v4903 = vshrl.u32 %v4893, 8 (stack74)
        %v4904 = vor.u32 %v4902, %v4903 (stack75)
        %v4905 = vxor.u32 %v4896, %v4904 (stack76)
        %v4908 = vadd.s32 %v4905, %v10 (stack65)
        %v4912 = vadd.s32 %v4908, 2 (stack65)
        %v4916 = vadd.s32 %v4900, %v4912 (stack65)
        %v4918 = vshll.u32 %v4912, 13 (stack73)
        %v4919 = vshrl.u32 %v4912, 19 (stack74)
        %v4920 = vor.u32 %v4918, %v4919 (stack75)
        %v4921 = vxor.u32 %v4916, %v4920 (stack76)
        %v4924 = vadd.s32 %v4916, %v4921 (stack65)
        %v4926 = vshll.u32 %v4921, 15 (stack73)
        %v4927 = vshrl.u32 %v4921, 17 (stack74)
        %v4928 = vor.u32 %v4926, %v4927 (stack75)
        %v4929 = vxor.u32 %v4924, %v4928 (stack76)
        %v4932 = vadd.s32 %v4924, %v4929 (stack65)
        %v4934 = vshll.u32 %v4929, 26 (stack73)
        %v4935 = vshrl.u32 %v4929, 6 (stack74)
        %v4936 = vor.u32 %v4934, %v4935 (stack75)
        %v4937 = vxor.u32 %v4932, %v4936 (stack76)
        %v4940 = vadd.s32 %v4932, %v4937 (stack65)
        %v4944 = vadd.s32 %v4940, %v10 (stack65)
        %v4946 = vshll.u32 %v4937, 6 (stack73)
        %v4947 = vshrl.u32 %v4937, 26 (stack74)
        %v4948 = vor.u32 %v4946, %v4947 (stack75)
        %v4949 = vxor.u32 %v4940, %v4948 (stack76)
        %v4952 = vadd.s32 %v4949, %v9 (stack65)
        %v4956 = vadd.s32 %v4952, 3 (stack65)
        %v4960 = vadd.s32 %v4944, %v4956 (stack65)
        %v4962 = vshll.u32 %v4956, 17 (stack73)
        %v4963 = vshrl.u32 %v4956, 15 (stack74)
        %v4964 = vor.u32 %v4962, %v4963 (stack75)
        %v4965 = vxor.u32 %v4960, %v4964 (stack76)
        %v4968 = vadd.s32 %v4960, %v4965 (stack65)
        %v4970 = vshll.u32 %v4965, 29 (stack73)
        %v4971 = vshrl.u32 %v4965, 3 (stack74)
        %v4972 = vor.u32 %v4970, %v4971 (stack75)
        %v4973 = vxor.u32 %v4968, %v4972 (stack76)
        %v4976 = vadd.s32 %v4968, %v4973 (stack65)
        %v4978 = vshll.u32 %v4973, 16 (stack73)
        %v4979 = vshrl.u32 %v4973, 16 (stack74)
        %v4980 = vor.u32 %v4978, %v4979 (stack75)
        %v4981 = vxor.u32 %v4976, %v4980 (stack76)
        %v4984 = vadd.s32 %v4976, %v4981 (stack65)
        %v4988 = vadd.s32 %v4984, %v9 (stack65)
        %v4990 = vshll.u32 %v4981, 24 (stack73)
        %v4991 = vshrl.u32 %v4981, 8 (stack74)
        %v4992 = vor.u32 %v4990, %v4991 (stack75)
        %v4993 = vxor.u32 %v4984, %v4992 (stack76)
        %v4996 = vadd.s32 %v4993, %v8 (stack65)
        %v5000 = vadd.s32 %v4996, 4 (stack65)
        %v5004 = vadd.s32 %v4988, %v5000 (stack65)
        %v5006 = vshll.u32 %v5000, 13 (stack73)
        %v5007 = vshrl.u32 %v5000, 19 (stack74)
        %v5008 = vor.u32 %v5006, %v5007 (stack75)
        %v5009 = vxor.u32 %v5004, %v5008 (stack76)
        %v5012 = vadd.s32 %v5004, %v5009 (stack65)
        %v5014 = vshll.u32 %v5009, 15 (stack73)
        %v5015 = vshrl.u32 %v5009, 17 (stack74)
        %v5016 = vor.u32 %v5014, %v5015 (stack75)
        %v5017 = vxor.u32 %v5012, %v5016 (stack76)
        %v5020 = vadd.s32 %v5012, %v5017 (stack65)
        %v5022 = vshll.u32 %v5017, 26 (stack73)
        %v5023 = vshrl.u32 %v5017, 6 (stack74)
        %v5024 = vor.u32 %v5022, %v5023 (stack75)
        %v5025 = vxor.u32 %v5020, %v5024 (stack76)
        %v5028 = vadd.s32 %v5020, %v5025 (stack65)
        %v5032 = vadd.s32 %v5028, %v8 (stack65)
        %v5034 = vshll.u32 %v5025, 6 (stack73)
        %v5035 = vshrl.u32 %v5025, 26 (stack74)
        %v5036 = vor.u32 %v5034, %v5035 (stack75)
        %v5037 = vxor.u32 %v5028, %v5036 (stack76)
        %v5040 = vadd.s32 %v5037, %v10 (stack65)
        %v5044 = vadd.s32 %v5040, 5 (stack65)
        %v5046 = vxor.u32 %v5032, %v5044 (stack76)
        %v5047 = vand.u32.u8 %v5046, 255 (stack77)
        %v5048 = vand.u32 %v5047, 65535 (stack78)
        %v5049 = vshrl.u32 %v5048, 1 (stack79)
        %v5050 = vor.u32 %v5049, 16256 (stack75)
        %v5051 = vand.u32.u16 %v5050, 65535 (stack80)
        %v5052 = vunpack.i.l.bf16 %v5051 (stack81)
        %v5056 = vadd.f32 %v5052, -1.0 (stack82)
        %v5060 = vmul.f32 %v5056, 2.0 (stack83)
        %v5064 = vadd.f32 %v5060, -0.99609375 (stack82)
        %v5068 = vmax.f32 -0.99609375, %v5064 (stack84)
        %v5070 = vand.u32 2147483647, %v5068 (stack85)
        %vm5073 = vcmp.eq.f32.partialorder %v5070, 1.0 (stack86)
        %v5078 = vmul.f32 %v5068, inf (stack83)
        %v5080 = vxor.u32 %v5068, 2147483648 (stack87)
        %v5083 = vmul.f32 %v5068, %v5080 (stack83)
        %v5085 = vadd.f32 %v5083, 1.0 (stack88)
        %v5086 = vlog2.pop %v5085 (stack89)
        %v5087 = vmul.f32 %v5086, 0.6931472 (stack90)
        %v5088 = vmul.f32 -0.5, %v5083 (stack91)
        %v5089 = vadd.f32 %v5088, 1.0 (stack92)
        %v5090 = vmul.f32 %v5089, %v5083 (stack93)
        %v5091 = vand.u32 2147483647, %v5083 (stack94)
        %vm5092 = vcmp.lt.f32.partialorder %v5091, 0.0004427343 (stack95)
        %v5093 = vsel /*vm=*/%vm5092, /*on_true_vy=*/%v5090, /*on_false_vx=*/%v5087 (stack96)
        %v5094 = vxor.u32 %v5093, 2147483648 (stack87)
        %vm5097 = vcmp.lt.f32.partialorder %v5094, 5.0 (stack86)
        %v5102 = vsel /*vm=*/%vm5097, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v5106 = vsel /*vm=*/%vm5097, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v5110 = vsel /*vm=*/%vm5097, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v5114 = vsel /*vm=*/%vm5097, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v5118 = vsel /*vm=*/%vm5097, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v5122 = vsel /*vm=*/%vm5097, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v5126 = vsel /*vm=*/%vm5097, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v5130 = vsel /*vm=*/%vm5097, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v5134 = vsel /*vm=*/%vm5097, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v5138 = vadd.f32 %v5094, -2.5 (stack82)
        %v5140 = vrsqrt.pop %v5094 (stack97)
        %v5141 = vmul.f32 %v5094, %v5140 (stack98)
        %vm5142 = vcmp.eq.f32.partialorder %v5094, inf (stack99)
        %v5143 = vsel /*vm=*/%vm5142, /*on_true_vy=*/%v5094, /*on_false_vx=*/%v5141 (stack100)
        %vm5144 = vcmp.eq.f32.partialorder %v5094, 0.0 (stack101)
        %v5145 = vand.u32 %v5094, 2147483648 (stack102)
        %v5146 = vsel /*vm=*/%vm5144, /*on_true_vy=*/%v5145, /*on_false_vx=*/%v5143 (stack103)
        %v5149 = vadd.f32 %v5146, -3.0 (stack82)
        %v5153 = vsel /*vm=*/%vm5097, /*on_true_vy=*/%v5138, /*on_false_vx=*/%v5149 (stack72)
        %v5157 = vmul.f32 %v5134, %v5153 (stack83)
        %v5161 = vadd.f32 %v5130, %v5157 (stack82)
        %v5165 = vmul.f32 %v5161, %v5153 (stack83)
        %v5169 = vadd.f32 %v5126, %v5165 (stack82)
        %v5173 = vmul.f32 %v5169, %v5153 (stack83)
        %v5177 = vadd.f32 %v5122, %v5173 (stack82)
        %v5181 = vmul.f32 %v5177, %v5153 (stack83)
        %v5185 = vadd.f32 %v5118, %v5181 (stack82)
        %v5189 = vmul.f32 %v5185, %v5153 (stack83)
        %v5193 = vadd.f32 %v5114, %v5189 (stack82)
        %v5197 = vmul.f32 %v5193, %v5153 (stack83)
        %v5201 = vadd.f32 %v5110, %v5197 (stack82)
        %v5205 = vmul.f32 %v5201, %v5153 (stack83)
        %v5209 = vadd.f32 %v5106, %v5205 (stack82)
        %v5213 = vmul.f32 %v5209, %v5153 (stack83)
        %v5217 = vadd.f32 %v5102, %v5213 (stack82)
        %v5221 = vmul.f32 %v5217, %v5068 (stack83)
        %v5225 = vsel /*vm=*/%vm5073, /*on_true_vy=*/%v5078, /*on_false_vx=*/%v5221 (stack72)
        %v5229 = vmul.f32 %v5225, 1.4140625 (stack83)
        %s5231 = scalar_lea.vmem %s280, 132 [#allocation0] (stack107)
        %v5232 = vpack.c.bf16 0.0, %v5229 (stack104)
        %5233 = vst [vmem:[%s5231] sm:$0xf] /*vst_source=*/%v5232 (stack105)
        %v5236 = vadd.s32 %v1381, %v4311 (stack65)
        %s5238 = smul.u32 128, %s27 (stack66)
        %v5239 = vlaneseq (stack67)
        %v5240 = vand.u32 %v5239, 127 (stack68)
        %v5241 = vstv %s5238 (stack69)
        %v5242 = vadd.s32 %v5240, %v5241 (stack70)
        %v5246 = vadd.s32 %v5236, %v5242 (stack65)
        %vm5250 = vcmp.lt.u32.totalorder %v5246, %v5236 (stack71)
        %vm5255 = vcmp.lt.u32.totalorder %v5236, %v1381 (stack71)
        %v5260 = vadd.s32 %v1368, %v4294 (stack65)
        %v5264 = vadd.s32 %v5260, 1 (stack65)
        %v5268 = vsel /*vm=*/%vm5255, /*on_true_vy=*/%v5264, /*on_false_vx=*/%v5260 (stack72)
        %v5272 = vadd.s32 %v5268, 1 (stack65)
        %v5276 = vsel /*vm=*/%vm5250, /*on_true_vy=*/%v5272, /*on_false_vx=*/%v5268 (stack72)
        %v5281 = vadd.s32 %v5276, %v10 (stack65)
        %v5285 = vadd.s32 %v5246, %v9 (stack65)
        %v5289 = vadd.s32 %v5281, %v5285 (stack65)
        %v5291 = vshll.u32 %v5285, 13 (stack73)
        %v5292 = vshrl.u32 %v5285, 19 (stack74)
        %v5293 = vor.u32 %v5291, %v5292 (stack75)
        %v5294 = vxor.u32 %v5289, %v5293 (stack76)
        %v5297 = vadd.s32 %v5289, %v5294 (stack65)
        %v5299 = vshll.u32 %v5294, 15 (stack73)
        %v5300 = vshrl.u32 %v5294, 17 (stack74)
        %v5301 = vor.u32 %v5299, %v5300 (stack75)
        %v5302 = vxor.u32 %v5297, %v5301 (stack76)
        %v5305 = vadd.s32 %v5297, %v5302 (stack65)
        %v5307 = vshll.u32 %v5302, 26 (stack73)
        %v5308 = vshrl.u32 %v5302, 6 (stack74)
        %v5309 = vor.u32 %v5307, %v5308 (stack75)
        %v5310 = vxor.u32 %v5305, %v5309 (stack76)
        %v5313 = vadd.s32 %v5305, %v5310 (stack65)
        %v5317 = vadd.s32 %v5313, %v9 (stack65)
        %v5319 = vshll.u32 %v5310, 6 (stack73)
        %v5320 = vshrl.u32 %v5310, 26 (stack74)
        %v5321 = vor.u32 %v5319, %v5320 (stack75)
        %v5322 = vxor.u32 %v5313, %v5321 (stack76)
        %v5325 = vadd.s32 %v5322, %v8 (stack65)
        %v5329 = vadd.s32 %v5325, 1 (stack65)
        %v5333 = vadd.s32 %v5317, %v5329 (stack65)
        %v5335 = vshll.u32 %v5329, 17 (stack73)
        %v5336 = vshrl.u32 %v5329, 15 (stack74)
        %v5337 = vor.u32 %v5335, %v5336 (stack75)
        %v5338 = vxor.u32 %v5333, %v5337 (stack76)
        %v5341 = vadd.s32 %v5333, %v5338 (stack65)
        %v5343 = vshll.u32 %v5338, 29 (stack73)
        %v5344 = vshrl.u32 %v5338, 3 (stack74)
        %v5345 = vor.u32 %v5343, %v5344 (stack75)
        %v5346 = vxor.u32 %v5341, %v5345 (stack76)
        %v5349 = vadd.s32 %v5341, %v5346 (stack65)
        %v5351 = vshll.u32 %v5346, 16 (stack73)
        %v5352 = vshrl.u32 %v5346, 16 (stack74)
        %v5353 = vor.u32 %v5351, %v5352 (stack75)
        %v5354 = vxor.u32 %v5349, %v5353 (stack76)
        %v5357 = vadd.s32 %v5349, %v5354 (stack65)
        %v5361 = vadd.s32 %v5357, %v8 (stack65)
        %v5363 = vshll.u32 %v5354, 24 (stack73)
        %v5364 = vshrl.u32 %v5354, 8 (stack74)
        %v5365 = vor.u32 %v5363, %v5364 (stack75)
        %v5366 = vxor.u32 %v5357, %v5365 (stack76)
        %v5369 = vadd.s32 %v5366, %v10 (stack65)
        %v5373 = vadd.s32 %v5369, 2 (stack65)
        %v5377 = vadd.s32 %v5361, %v5373 (stack65)
        %v5379 = vshll.u32 %v5373, 13 (stack73)
        %v5380 = vshrl.u32 %v5373, 19 (stack74)
        %v5381 = vor.u32 %v5379, %v5380 (stack75)
        %v5382 = vxor.u32 %v5377, %v5381 (stack76)
        %v5385 = vadd.s32 %v5377, %v5382 (stack65)
        %v5387 = vshll.u32 %v5382, 15 (stack73)
        %v5388 = vshrl.u32 %v5382, 17 (stack74)
        %v5389 = vor.u32 %v5387, %v5388 (stack75)
        %v5390 = vxor.u32 %v5385, %v5389 (stack76)
        %v5393 = vadd.s32 %v5385, %v5390 (stack65)
        %v5395 = vshll.u32 %v5390, 26 (stack73)
        %v5396 = vshrl.u32 %v5390, 6 (stack74)
        %v5397 = vor.u32 %v5395, %v5396 (stack75)
        %v5398 = vxor.u32 %v5393, %v5397 (stack76)
        %v5401 = vadd.s32 %v5393, %v5398 (stack65)
        %v5405 = vadd.s32 %v5401, %v10 (stack65)
        %v5407 = vshll.u32 %v5398, 6 (stack73)
        %v5408 = vshrl.u32 %v5398, 26 (stack74)
        %v5409 = vor.u32 %v5407, %v5408 (stack75)
        %v5410 = vxor.u32 %v5401, %v5409 (stack76)
        %v5413 = vadd.s32 %v5410, %v9 (stack65)
        %v5417 = vadd.s32 %v5413, 3 (stack65)
        %v5421 = vadd.s32 %v5405, %v5417 (stack65)
        %v5423 = vshll.u32 %v5417, 17 (stack73)
        %v5424 = vshrl.u32 %v5417, 15 (stack74)
        %v5425 = vor.u32 %v5423, %v5424 (stack75)
        %v5426 = vxor.u32 %v5421, %v5425 (stack76)
        %v5429 = vadd.s32 %v5421, %v5426 (stack65)
        %v5431 = vshll.u32 %v5426, 29 (stack73)
        %v5432 = vshrl.u32 %v5426, 3 (stack74)
        %v5433 = vor.u32 %v5431, %v5432 (stack75)
        %v5434 = vxor.u32 %v5429, %v5433 (stack76)
        %v5437 = vadd.s32 %v5429, %v5434 (stack65)
        %v5439 = vshll.u32 %v5434, 16 (stack73)
        %v5440 = vshrl.u32 %v5434, 16 (stack74)
        %v5441 = vor.u32 %v5439, %v5440 (stack75)
        %v5442 = vxor.u32 %v5437, %v5441 (stack76)
        %v5445 = vadd.s32 %v5437, %v5442 (stack65)
        %v5449 = vadd.s32 %v5445, %v9 (stack65)
        %v5451 = vshll.u32 %v5442, 24 (stack73)
        %v5452 = vshrl.u32 %v5442, 8 (stack74)
        %v5453 = vor.u32 %v5451, %v5452 (stack75)
        %v5454 = vxor.u32 %v5445, %v5453 (stack76)
        %v5457 = vadd.s32 %v5454, %v8 (stack65)
        %v5461 = vadd.s32 %v5457, 4 (stack65)
        %v5465 = vadd.s32 %v5449, %v5461 (stack65)
        %v5467 = vshll.u32 %v5461, 13 (stack73)
        %v5468 = vshrl.u32 %v5461, 19 (stack74)
        %v5469 = vor.u32 %v5467, %v5468 (stack75)
        %v5470 = vxor.u32 %v5465, %v5469 (stack76)
        %v5473 = vadd.s32 %v5465, %v5470 (stack65)
        %v5475 = vshll.u32 %v5470, 15 (stack73)
        %v5476 = vshrl.u32 %v5470, 17 (stack74)
        %v5477 = vor.u32 %v5475, %v5476 (stack75)
        %v5478 = vxor.u32 %v5473, %v5477 (stack76)
        %v5481 = vadd.s32 %v5473, %v5478 (stack65)
        %v5483 = vshll.u32 %v5478, 26 (stack73)
        %v5484 = vshrl.u32 %v5478, 6 (stack74)
        %v5485 = vor.u32 %v5483, %v5484 (stack75)
        %v5486 = vxor.u32 %v5481, %v5485 (stack76)
        %v5489 = vadd.s32 %v5481, %v5486 (stack65)
        %v5493 = vadd.s32 %v5489, %v8 (stack65)
        %v5495 = vshll.u32 %v5486, 6 (stack73)
        %v5496 = vshrl.u32 %v5486, 26 (stack74)
        %v5497 = vor.u32 %v5495, %v5496 (stack75)
        %v5498 = vxor.u32 %v5489, %v5497 (stack76)
        %v5501 = vadd.s32 %v5498, %v10 (stack65)
        %v5505 = vadd.s32 %v5501, 5 (stack65)
        %v5507 = vxor.u32 %v5493, %v5505 (stack76)
        %v5508 = vand.u32.u8 %v5507, 255 (stack77)
        %v5509 = vand.u32 %v5508, 65535 (stack78)
        %v5510 = vshrl.u32 %v5509, 1 (stack79)
        %v5511 = vor.u32 %v5510, 16256 (stack75)
        %v5512 = vand.u32.u16 %v5511, 65535 (stack80)
        %v5513 = vunpack.i.l.bf16 %v5512 (stack81)
        %v5517 = vadd.f32 %v5513, -1.0 (stack82)
        %v5521 = vmul.f32 %v5517, 2.0 (stack83)
        %v5525 = vadd.f32 %v5521, -0.99609375 (stack82)
        %v5529 = vmax.f32 -0.99609375, %v5525 (stack84)
        %v5531 = vand.u32 2147483647, %v5529 (stack85)
        %vm5534 = vcmp.eq.f32.partialorder %v5531, 1.0 (stack86)
        %v5539 = vmul.f32 %v5529, inf (stack83)
        %v5541 = vxor.u32 %v5529, 2147483648 (stack87)
        %v5544 = vmul.f32 %v5529, %v5541 (stack83)
        %v5546 = vadd.f32 %v5544, 1.0 (stack88)
        %v5547 = vlog2.pop %v5546 (stack89)
        %v5548 = vmul.f32 %v5547, 0.6931472 (stack90)
        %v5549 = vmul.f32 -0.5, %v5544 (stack91)
        %v5550 = vadd.f32 %v5549, 1.0 (stack92)
        %v5551 = vmul.f32 %v5550, %v5544 (stack93)
        %v5552 = vand.u32 2147483647, %v5544 (stack94)
        %vm5553 = vcmp.lt.f32.partialorder %v5552, 0.0004427343 (stack95)
        %v5554 = vsel /*vm=*/%vm5553, /*on_true_vy=*/%v5551, /*on_false_vx=*/%v5548 (stack96)
        %v5555 = vxor.u32 %v5554, 2147483648 (stack87)
        %vm5558 = vcmp.lt.f32.partialorder %v5555, 5.0 (stack86)
        %v5563 = vsel /*vm=*/%vm5558, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v5567 = vsel /*vm=*/%vm5558, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v5571 = vsel /*vm=*/%vm5558, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v5575 = vsel /*vm=*/%vm5558, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v5579 = vsel /*vm=*/%vm5558, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v5583 = vsel /*vm=*/%vm5558, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v5587 = vsel /*vm=*/%vm5558, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v5591 = vsel /*vm=*/%vm5558, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v5595 = vsel /*vm=*/%vm5558, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v5599 = vadd.f32 %v5555, -2.5 (stack82)
        %v5601 = vrsqrt.pop %v5555 (stack97)
        %v5602 = vmul.f32 %v5555, %v5601 (stack98)
        %vm5603 = vcmp.eq.f32.partialorder %v5555, inf (stack99)
        %v5604 = vsel /*vm=*/%vm5603, /*on_true_vy=*/%v5555, /*on_false_vx=*/%v5602 (stack100)
        %vm5605 = vcmp.eq.f32.partialorder %v5555, 0.0 (stack101)
        %v5606 = vand.u32 %v5555, 2147483648 (stack102)
        %v5607 = vsel /*vm=*/%vm5605, /*on_true_vy=*/%v5606, /*on_false_vx=*/%v5604 (stack103)
        %v5610 = vadd.f32 %v5607, -3.0 (stack82)
        %v5614 = vsel /*vm=*/%vm5558, /*on_true_vy=*/%v5599, /*on_false_vx=*/%v5610 (stack72)
        %v5618 = vmul.f32 %v5595, %v5614 (stack83)
        %v5622 = vadd.f32 %v5591, %v5618 (stack82)
        %v5626 = vmul.f32 %v5622, %v5614 (stack83)
        %v5630 = vadd.f32 %v5587, %v5626 (stack82)
        %v5634 = vmul.f32 %v5630, %v5614 (stack83)
        %v5638 = vadd.f32 %v5583, %v5634 (stack82)
        %v5642 = vmul.f32 %v5638, %v5614 (stack83)
        %v5646 = vadd.f32 %v5579, %v5642 (stack82)
        %v5650 = vmul.f32 %v5646, %v5614 (stack83)
        %v5654 = vadd.f32 %v5575, %v5650 (stack82)
        %v5658 = vmul.f32 %v5654, %v5614 (stack83)
        %v5662 = vadd.f32 %v5571, %v5658 (stack82)
        %v5666 = vmul.f32 %v5662, %v5614 (stack83)
        %v5670 = vadd.f32 %v5567, %v5666 (stack82)
        %v5674 = vmul.f32 %v5670, %v5614 (stack83)
        %v5678 = vadd.f32 %v5563, %v5674 (stack82)
        %v5682 = vmul.f32 %v5678, %v5529 (stack83)
        %v5686 = vsel /*vm=*/%vm5534, /*on_true_vy=*/%v5539, /*on_false_vx=*/%v5682 (stack72)
        %v5690 = vmul.f32 %v5686, 1.4140625 (stack83)
        %s5692 = scalar_lea.vmem %s280, 260 [#allocation0] (stack107)
        %v5693 = vpack.c.bf16 0.0, %v5690 (stack104)
        %5694 = vst [vmem:[%s5692] sm:$0xf] /*vst_source=*/%v5693 (stack105)
        %v5697 = vadd.s32 %v1868, %v4311 (stack65)
        %s5699 = smul.u32 128, %s27 (stack66)
        %v5700 = vlaneseq (stack67)
        %v5701 = vand.u32 %v5700, 127 (stack68)
        %v5702 = vstv %s5699 (stack69)
        %v5703 = vadd.s32 %v5701, %v5702 (stack70)
        %v5707 = vadd.s32 %v5697, %v5703 (stack65)
        %vm5711 = vcmp.lt.u32.totalorder %v5707, %v5697 (stack71)
        %vm5716 = vcmp.lt.u32.totalorder %v5697, %v1868 (stack71)
        %v5721 = vadd.s32 %v1855, %v4294 (stack65)
        %v5725 = vadd.s32 %v5721, 1 (stack65)
        %v5729 = vsel /*vm=*/%vm5716, /*on_true_vy=*/%v5725, /*on_false_vx=*/%v5721 (stack72)
        %v5733 = vadd.s32 %v5729, 1 (stack65)
        %v5737 = vsel /*vm=*/%vm5711, /*on_true_vy=*/%v5733, /*on_false_vx=*/%v5729 (stack72)
        %v5742 = vadd.s32 %v5737, %v10 (stack65)
        %v5746 = vadd.s32 %v5707, %v9 (stack65)
        %v5750 = vadd.s32 %v5742, %v5746 (stack65)
        %v5752 = vshll.u32 %v5746, 13 (stack73)
        %v5753 = vshrl.u32 %v5746, 19 (stack74)
        %v5754 = vor.u32 %v5752, %v5753 (stack75)
        %v5755 = vxor.u32 %v5750, %v5754 (stack76)
        %v5758 = vadd.s32 %v5750, %v5755 (stack65)
        %v5760 = vshll.u32 %v5755, 15 (stack73)
        %v5761 = vshrl.u32 %v5755, 17 (stack74)
        %v5762 = vor.u32 %v5760, %v5761 (stack75)
        %v5763 = vxor.u32 %v5758, %v5762 (stack76)
        %v5766 = vadd.s32 %v5758, %v5763 (stack65)
        %v5768 = vshll.u32 %v5763, 26 (stack73)
        %v5769 = vshrl.u32 %v5763, 6 (stack74)
        %v5770 = vor.u32 %v5768, %v5769 (stack75)
        %v5771 = vxor.u32 %v5766, %v5770 (stack76)
        %v5774 = vadd.s32 %v5766, %v5771 (stack65)
        %v5778 = vadd.s32 %v5774, %v9 (stack65)
        %v5780 = vshll.u32 %v5771, 6 (stack73)
        %v5781 = vshrl.u32 %v5771, 26 (stack74)
        %v5782 = vor.u32 %v5780, %v5781 (stack75)
        %v5783 = vxor.u32 %v5774, %v5782 (stack76)
        %v5786 = vadd.s32 %v5783, %v8 (stack65)
        %v5790 = vadd.s32 %v5786, 1 (stack65)
        %v5794 = vadd.s32 %v5778, %v5790 (stack65)
        %v5796 = vshll.u32 %v5790, 17 (stack73)
        %v5797 = vshrl.u32 %v5790, 15 (stack74)
        %v5798 = vor.u32 %v5796, %v5797 (stack75)
        %v5799 = vxor.u32 %v5794, %v5798 (stack76)
        %v5802 = vadd.s32 %v5794, %v5799 (stack65)
        %v5804 = vshll.u32 %v5799, 29 (stack73)
        %v5805 = vshrl.u32 %v5799, 3 (stack74)
        %v5806 = vor.u32 %v5804, %v5805 (stack75)
        %v5807 = vxor.u32 %v5802, %v5806 (stack76)
        %v5810 = vadd.s32 %v5802, %v5807 (stack65)
        %v5812 = vshll.u32 %v5807, 16 (stack73)
        %v5813 = vshrl.u32 %v5807, 16 (stack74)
        %v5814 = vor.u32 %v5812, %v5813 (stack75)
        %v5815 = vxor.u32 %v5810, %v5814 (stack76)
        %v5818 = vadd.s32 %v5810, %v5815 (stack65)
        %v5822 = vadd.s32 %v5818, %v8 (stack65)
        %v5824 = vshll.u32 %v5815, 24 (stack73)
        %v5825 = vshrl.u32 %v5815, 8 (stack74)
        %v5826 = vor.u32 %v5824, %v5825 (stack75)
        %v5827 = vxor.u32 %v5818, %v5826 (stack76)
        %v5830 = vadd.s32 %v5827, %v10 (stack65)
        %v5834 = vadd.s32 %v5830, 2 (stack65)
        %v5838 = vadd.s32 %v5822, %v5834 (stack65)
        %v5840 = vshll.u32 %v5834, 13 (stack73)
        %v5841 = vshrl.u32 %v5834, 19 (stack74)
        %v5842 = vor.u32 %v5840, %v5841 (stack75)
        %v5843 = vxor.u32 %v5838, %v5842 (stack76)
        %v5846 = vadd.s32 %v5838, %v5843 (stack65)
        %v5848 = vshll.u32 %v5843, 15 (stack73)
        %v5849 = vshrl.u32 %v5843, 17 (stack74)
        %v5850 = vor.u32 %v5848, %v5849 (stack75)
        %v5851 = vxor.u32 %v5846, %v5850 (stack76)
        %v5854 = vadd.s32 %v5846, %v5851 (stack65)
        %v5856 = vshll.u32 %v5851, 26 (stack73)
        %v5857 = vshrl.u32 %v5851, 6 (stack74)
        %v5858 = vor.u32 %v5856, %v5857 (stack75)
        %v5859 = vxor.u32 %v5854, %v5858 (stack76)
        %v5862 = vadd.s32 %v5854, %v5859 (stack65)
        %v5866 = vadd.s32 %v5862, %v10 (stack65)
        %v5868 = vshll.u32 %v5859, 6 (stack73)
        %v5869 = vshrl.u32 %v5859, 26 (stack74)
        %v5870 = vor.u32 %v5868, %v5869 (stack75)
        %v5871 = vxor.u32 %v5862, %v5870 (stack76)
        %v5874 = vadd.s32 %v5871, %v9 (stack65)
        %v5878 = vadd.s32 %v5874, 3 (stack65)
        %v5882 = vadd.s32 %v5866, %v5878 (stack65)
        %v5884 = vshll.u32 %v5878, 17 (stack73)
        %v5885 = vshrl.u32 %v5878, 15 (stack74)
        %v5886 = vor.u32 %v5884, %v5885 (stack75)
        %v5887 = vxor.u32 %v5882, %v5886 (stack76)
        %v5890 = vadd.s32 %v5882, %v5887 (stack65)
        %v5892 = vshll.u32 %v5887, 29 (stack73)
        %v5893 = vshrl.u32 %v5887, 3 (stack74)
        %v5894 = vor.u32 %v5892, %v5893 (stack75)
        %v5895 = vxor.u32 %v5890, %v5894 (stack76)
        %v5898 = vadd.s32 %v5890, %v5895 (stack65)
        %v5900 = vshll.u32 %v5895, 16 (stack73)
        %v5901 = vshrl.u32 %v5895, 16 (stack74)
        %v5902 = vor.u32 %v5900, %v5901 (stack75)
        %v5903 = vxor.u32 %v5898, %v5902 (stack76)
        %v5906 = vadd.s32 %v5898, %v5903 (stack65)
        %v5910 = vadd.s32 %v5906, %v9 (stack65)
        %v5912 = vshll.u32 %v5903, 24 (stack73)
        %v5913 = vshrl.u32 %v5903, 8 (stack74)
        %v5914 = vor.u32 %v5912, %v5913 (stack75)
        %v5915 = vxor.u32 %v5906, %v5914 (stack76)
        %v5918 = vadd.s32 %v5915, %v8 (stack65)
        %v5922 = vadd.s32 %v5918, 4 (stack65)
        %v5926 = vadd.s32 %v5910, %v5922 (stack65)
        %v5928 = vshll.u32 %v5922, 13 (stack73)
        %v5929 = vshrl.u32 %v5922, 19 (stack74)
        %v5930 = vor.u32 %v5928, %v5929 (stack75)
        %v5931 = vxor.u32 %v5926, %v5930 (stack76)
        %v5934 = vadd.s32 %v5926, %v5931 (stack65)
        %v5936 = vshll.u32 %v5931, 15 (stack73)
        %v5937 = vshrl.u32 %v5931, 17 (stack74)
        %v5938 = vor.u32 %v5936, %v5937 (stack75)
        %v5939 = vxor.u32 %v5934, %v5938 (stack76)
        %v5942 = vadd.s32 %v5934, %v5939 (stack65)
        %v5944 = vshll.u32 %v5939, 26 (stack73)
        %v5945 = vshrl.u32 %v5939, 6 (stack74)
        %v5946 = vor.u32 %v5944, %v5945 (stack75)
        %v5947 = vxor.u32 %v5942, %v5946 (stack76)
        %v5950 = vadd.s32 %v5942, %v5947 (stack65)
        %v5954 = vadd.s32 %v5950, %v8 (stack65)
        %v5956 = vshll.u32 %v5947, 6 (stack73)
        %v5957 = vshrl.u32 %v5947, 26 (stack74)
        %v5958 = vor.u32 %v5956, %v5957 (stack75)
        %v5959 = vxor.u32 %v5950, %v5958 (stack76)
        %v5962 = vadd.s32 %v5959, %v10 (stack65)
        %v5966 = vadd.s32 %v5962, 5 (stack65)
        %v5968 = vxor.u32 %v5954, %v5966 (stack76)
        %v5969 = vand.u32.u8 %v5968, 255 (stack77)
        %v5970 = vand.u32 %v5969, 65535 (stack78)
        %v5971 = vshrl.u32 %v5970, 1 (stack79)
        %v5972 = vor.u32 %v5971, 16256 (stack75)
        %v5973 = vand.u32.u16 %v5972, 65535 (stack80)
        %v5974 = vunpack.i.l.bf16 %v5973 (stack81)
        %v5978 = vadd.f32 %v5974, -1.0 (stack82)
        %v5982 = vmul.f32 %v5978, 2.0 (stack83)
        %v5986 = vadd.f32 %v5982, -0.99609375 (stack82)
        %v5990 = vmax.f32 -0.99609375, %v5986 (stack84)
        %v5992 = vand.u32 2147483647, %v5990 (stack85)
        %vm5995 = vcmp.eq.f32.partialorder %v5992, 1.0 (stack86)
        %v6000 = vmul.f32 %v5990, inf (stack83)
        %v6002 = vxor.u32 %v5990, 2147483648 (stack87)
        %v6005 = vmul.f32 %v5990, %v6002 (stack83)
        %v6007 = vadd.f32 %v6005, 1.0 (stack88)
        %v6008 = vlog2.pop %v6007 (stack89)
        %v6009 = vmul.f32 %v6008, 0.6931472 (stack90)
        %v6010 = vmul.f32 -0.5, %v6005 (stack91)
        %v6011 = vadd.f32 %v6010, 1.0 (stack92)
        %v6012 = vmul.f32 %v6011, %v6005 (stack93)
        %v6013 = vand.u32 2147483647, %v6005 (stack94)
        %vm6014 = vcmp.lt.f32.partialorder %v6013, 0.0004427343 (stack95)
        %v6015 = vsel /*vm=*/%vm6014, /*on_true_vy=*/%v6012, /*on_false_vx=*/%v6009 (stack96)
        %v6016 = vxor.u32 %v6015, 2147483648 (stack87)
        %vm6019 = vcmp.lt.f32.partialorder %v6016, 5.0 (stack86)
        %v6024 = vsel /*vm=*/%vm6019, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v6028 = vsel /*vm=*/%vm6019, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v6032 = vsel /*vm=*/%vm6019, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v6036 = vsel /*vm=*/%vm6019, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v6040 = vsel /*vm=*/%vm6019, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v6044 = vsel /*vm=*/%vm6019, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v6048 = vsel /*vm=*/%vm6019, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v6052 = vsel /*vm=*/%vm6019, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v6056 = vsel /*vm=*/%vm6019, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v6060 = vadd.f32 %v6016, -2.5 (stack82)
        %v6062 = vrsqrt.pop %v6016 (stack97)
        %v6063 = vmul.f32 %v6016, %v6062 (stack98)
        %vm6064 = vcmp.eq.f32.partialorder %v6016, inf (stack99)
        %v6065 = vsel /*vm=*/%vm6064, /*on_true_vy=*/%v6016, /*on_false_vx=*/%v6063 (stack100)
        %vm6066 = vcmp.eq.f32.partialorder %v6016, 0.0 (stack101)
        %v6067 = vand.u32 %v6016, 2147483648 (stack102)
        %v6068 = vsel /*vm=*/%vm6066, /*on_true_vy=*/%v6067, /*on_false_vx=*/%v6065 (stack103)
        %v6071 = vadd.f32 %v6068, -3.0 (stack82)
        %v6075 = vsel /*vm=*/%vm6019, /*on_true_vy=*/%v6060, /*on_false_vx=*/%v6071 (stack72)
        %v6079 = vmul.f32 %v6056, %v6075 (stack83)
        %v6083 = vadd.f32 %v6052, %v6079 (stack82)
        %v6087 = vmul.f32 %v6083, %v6075 (stack83)
        %v6091 = vadd.f32 %v6048, %v6087 (stack82)
        %v6095 = vmul.f32 %v6091, %v6075 (stack83)
        %v6099 = vadd.f32 %v6044, %v6095 (stack82)
        %v6103 = vmul.f32 %v6099, %v6075 (stack83)
        %v6107 = vadd.f32 %v6040, %v6103 (stack82)
        %v6111 = vmul.f32 %v6107, %v6075 (stack83)
        %v6115 = vadd.f32 %v6036, %v6111 (stack82)
        %v6119 = vmul.f32 %v6115, %v6075 (stack83)
        %v6123 = vadd.f32 %v6032, %v6119 (stack82)
        %v6127 = vmul.f32 %v6123, %v6075 (stack83)
        %v6131 = vadd.f32 %v6028, %v6127 (stack82)
        %v6135 = vmul.f32 %v6131, %v6075 (stack83)
        %v6139 = vadd.f32 %v6024, %v6135 (stack82)
        %v6143 = vmul.f32 %v6139, %v5990 (stack83)
        %v6147 = vsel /*vm=*/%vm5995, /*on_true_vy=*/%v6000, /*on_false_vx=*/%v6143 (stack72)
        %v6151 = vmul.f32 %v6147, 1.4140625 (stack83)
        %s6153 = scalar_lea.vmem %s280, 388 [#allocation0] (stack107)
        %v6154 = vpack.c.bf16 0.0, %v6151 (stack104)
        %6155 = vst [vmem:[%s6153] sm:$0xf] /*vst_source=*/%v6154 (stack105)
        %v6158 = vadd.s32 %v2355, %v4311 (stack65)
        %s6160 = smul.u32 128, %s27 (stack66)
        %v6161 = vlaneseq (stack67)
        %v6162 = vand.u32 %v6161, 127 (stack68)
        %v6163 = vstv %s6160 (stack69)
        %v6164 = vadd.s32 %v6162, %v6163 (stack70)
        %v6168 = vadd.s32 %v6158, %v6164 (stack65)
        %vm6172 = vcmp.lt.u32.totalorder %v6168, %v6158 (stack71)
        %vm6177 = vcmp.lt.u32.totalorder %v6158, %v2355 (stack71)
        %v6182 = vadd.s32 %v2342, %v4294 (stack65)
        %v6186 = vadd.s32 %v6182, 1 (stack65)
        %v6190 = vsel /*vm=*/%vm6177, /*on_true_vy=*/%v6186, /*on_false_vx=*/%v6182 (stack72)
        %v6194 = vadd.s32 %v6190, 1 (stack65)
        %v6198 = vsel /*vm=*/%vm6172, /*on_true_vy=*/%v6194, /*on_false_vx=*/%v6190 (stack72)
        %v6203 = vadd.s32 %v6198, %v10 (stack65)
        %v6207 = vadd.s32 %v6168, %v9 (stack65)
        %v6211 = vadd.s32 %v6203, %v6207 (stack65)
        %v6213 = vshll.u32 %v6207, 13 (stack73)
        %v6214 = vshrl.u32 %v6207, 19 (stack74)
        %v6215 = vor.u32 %v6213, %v6214 (stack75)
        %v6216 = vxor.u32 %v6211, %v6215 (stack76)
        %v6219 = vadd.s32 %v6211, %v6216 (stack65)
        %v6221 = vshll.u32 %v6216, 15 (stack73)
        %v6222 = vshrl.u32 %v6216, 17 (stack74)
        %v6223 = vor.u32 %v6221, %v6222 (stack75)
        %v6224 = vxor.u32 %v6219, %v6223 (stack76)
        %v6227 = vadd.s32 %v6219, %v6224 (stack65)
        %v6229 = vshll.u32 %v6224, 26 (stack73)
        %v6230 = vshrl.u32 %v6224, 6 (stack74)
        %v6231 = vor.u32 %v6229, %v6230 (stack75)
        %v6232 = vxor.u32 %v6227, %v6231 (stack76)
        %v6235 = vadd.s32 %v6227, %v6232 (stack65)
        %v6239 = vadd.s32 %v6235, %v9 (stack65)
        %v6241 = vshll.u32 %v6232, 6 (stack73)
        %v6242 = vshrl.u32 %v6232, 26 (stack74)
        %v6243 = vor.u32 %v6241, %v6242 (stack75)
        %v6244 = vxor.u32 %v6235, %v6243 (stack76)
        %v6247 = vadd.s32 %v6244, %v8 (stack65)
        %v6251 = vadd.s32 %v6247, 1 (stack65)
        %v6255 = vadd.s32 %v6239, %v6251 (stack65)
        %v6257 = vshll.u32 %v6251, 17 (stack73)
        %v6258 = vshrl.u32 %v6251, 15 (stack74)
        %v6259 = vor.u32 %v6257, %v6258 (stack75)
        %v6260 = vxor.u32 %v6255, %v6259 (stack76)
        %v6263 = vadd.s32 %v6255, %v6260 (stack65)
        %v6265 = vshll.u32 %v6260, 29 (stack73)
        %v6266 = vshrl.u32 %v6260, 3 (stack74)
        %v6267 = vor.u32 %v6265, %v6266 (stack75)
        %v6268 = vxor.u32 %v6263, %v6267 (stack76)
        %v6271 = vadd.s32 %v6263, %v6268 (stack65)
        %v6273 = vshll.u32 %v6268, 16 (stack73)
        %v6274 = vshrl.u32 %v6268, 16 (stack74)
        %v6275 = vor.u32 %v6273, %v6274 (stack75)
        %v6276 = vxor.u32 %v6271, %v6275 (stack76)
        %v6279 = vadd.s32 %v6271, %v6276 (stack65)
        %v6283 = vadd.s32 %v6279, %v8 (stack65)
        %v6285 = vshll.u32 %v6276, 24 (stack73)
        %v6286 = vshrl.u32 %v6276, 8 (stack74)
        %v6287 = vor.u32 %v6285, %v6286 (stack75)
        %v6288 = vxor.u32 %v6279, %v6287 (stack76)
        %v6291 = vadd.s32 %v6288, %v10 (stack65)
        %v6295 = vadd.s32 %v6291, 2 (stack65)
        %v6299 = vadd.s32 %v6283, %v6295 (stack65)
        %v6301 = vshll.u32 %v6295, 13 (stack73)
        %v6302 = vshrl.u32 %v6295, 19 (stack74)
        %v6303 = vor.u32 %v6301, %v6302 (stack75)
        %v6304 = vxor.u32 %v6299, %v6303 (stack76)
        %v6307 = vadd.s32 %v6299, %v6304 (stack65)
        %v6309 = vshll.u32 %v6304, 15 (stack73)
        %v6310 = vshrl.u32 %v6304, 17 (stack74)
        %v6311 = vor.u32 %v6309, %v6310 (stack75)
        %v6312 = vxor.u32 %v6307, %v6311 (stack76)
        %v6315 = vadd.s32 %v6307, %v6312 (stack65)
        %v6317 = vshll.u32 %v6312, 26 (stack73)
        %v6318 = vshrl.u32 %v6312, 6 (stack74)
        %v6319 = vor.u32 %v6317, %v6318 (stack75)
        %v6320 = vxor.u32 %v6315, %v6319 (stack76)
        %v6323 = vadd.s32 %v6315, %v6320 (stack65)
        %v6327 = vadd.s32 %v6323, %v10 (stack65)
        %v6329 = vshll.u32 %v6320, 6 (stack73)
        %v6330 = vshrl.u32 %v6320, 26 (stack74)
        %v6331 = vor.u32 %v6329, %v6330 (stack75)
        %v6332 = vxor.u32 %v6323, %v6331 (stack76)
        %v6335 = vadd.s32 %v6332, %v9 (stack65)
        %v6339 = vadd.s32 %v6335, 3 (stack65)
        %v6343 = vadd.s32 %v6327, %v6339 (stack65)
        %v6345 = vshll.u32 %v6339, 17 (stack73)
        %v6346 = vshrl.u32 %v6339, 15 (stack74)
        %v6347 = vor.u32 %v6345, %v6346 (stack75)
        %v6348 = vxor.u32 %v6343, %v6347 (stack76)
        %v6351 = vadd.s32 %v6343, %v6348 (stack65)
        %v6353 = vshll.u32 %v6348, 29 (stack73)
        %v6354 = vshrl.u32 %v6348, 3 (stack74)
        %v6355 = vor.u32 %v6353, %v6354 (stack75)
        %v6356 = vxor.u32 %v6351, %v6355 (stack76)
        %v6359 = vadd.s32 %v6351, %v6356 (stack65)
        %v6361 = vshll.u32 %v6356, 16 (stack73)
        %v6362 = vshrl.u32 %v6356, 16 (stack74)
        %v6363 = vor.u32 %v6361, %v6362 (stack75)
        %v6364 = vxor.u32 %v6359, %v6363 (stack76)
        %v6367 = vadd.s32 %v6359, %v6364 (stack65)
        %v6371 = vadd.s32 %v6367, %v9 (stack65)
        %v6373 = vshll.u32 %v6364, 24 (stack73)
        %v6374 = vshrl.u32 %v6364, 8 (stack74)
        %v6375 = vor.u32 %v6373, %v6374 (stack75)
        %v6376 = vxor.u32 %v6367, %v6375 (stack76)
        %v6379 = vadd.s32 %v6376, %v8 (stack65)
        %v6383 = vadd.s32 %v6379, 4 (stack65)
        %v6387 = vadd.s32 %v6371, %v6383 (stack65)
        %v6389 = vshll.u32 %v6383, 13 (stack73)
        %v6390 = vshrl.u32 %v6383, 19 (stack74)
        %v6391 = vor.u32 %v6389, %v6390 (stack75)
        %v6392 = vxor.u32 %v6387, %v6391 (stack76)
        %v6395 = vadd.s32 %v6387, %v6392 (stack65)
        %v6397 = vshll.u32 %v6392, 15 (stack73)
        %v6398 = vshrl.u32 %v6392, 17 (stack74)
        %v6399 = vor.u32 %v6397, %v6398 (stack75)
        %v6400 = vxor.u32 %v6395, %v6399 (stack76)
        %v6403 = vadd.s32 %v6395, %v6400 (stack65)
        %v6405 = vshll.u32 %v6400, 26 (stack73)
        %v6406 = vshrl.u32 %v6400, 6 (stack74)
        %v6407 = vor.u32 %v6405, %v6406 (stack75)
        %v6408 = vxor.u32 %v6403, %v6407 (stack76)
        %v6411 = vadd.s32 %v6403, %v6408 (stack65)
        %v6415 = vadd.s32 %v6411, %v8 (stack65)
        %v6417 = vshll.u32 %v6408, 6 (stack73)
        %v6418 = vshrl.u32 %v6408, 26 (stack74)
        %v6419 = vor.u32 %v6417, %v6418 (stack75)
        %v6420 = vxor.u32 %v6411, %v6419 (stack76)
        %v6423 = vadd.s32 %v6420, %v10 (stack65)
        %v6427 = vadd.s32 %v6423, 5 (stack65)
        %v6429 = vxor.u32 %v6415, %v6427 (stack76)
        %v6430 = vand.u32.u8 %v6429, 255 (stack77)
        %v6431 = vand.u32 %v6430, 65535 (stack78)
        %v6432 = vshrl.u32 %v6431, 1 (stack79)
        %v6433 = vor.u32 %v6432, 16256 (stack75)
        %v6434 = vand.u32.u16 %v6433, 65535 (stack80)
        %v6435 = vunpack.i.l.bf16 %v6434 (stack81)
        %v6439 = vadd.f32 %v6435, -1.0 (stack82)
        %v6443 = vmul.f32 %v6439, 2.0 (stack83)
        %v6447 = vadd.f32 %v6443, -0.99609375 (stack82)
        %v6451 = vmax.f32 -0.99609375, %v6447 (stack84)
        %v6453 = vand.u32 2147483647, %v6451 (stack85)
        %vm6456 = vcmp.eq.f32.partialorder %v6453, 1.0 (stack86)
        %v6461 = vmul.f32 %v6451, inf (stack83)
        %v6463 = vxor.u32 %v6451, 2147483648 (stack87)
        %v6466 = vmul.f32 %v6451, %v6463 (stack83)
        %v6468 = vadd.f32 %v6466, 1.0 (stack88)
        %v6469 = vlog2.pop %v6468 (stack89)
        %v6470 = vmul.f32 %v6469, 0.6931472 (stack90)
        %v6471 = vmul.f32 -0.5, %v6466 (stack91)
        %v6472 = vadd.f32 %v6471, 1.0 (stack92)
        %v6473 = vmul.f32 %v6472, %v6466 (stack93)
        %v6474 = vand.u32 2147483647, %v6466 (stack94)
        %vm6475 = vcmp.lt.f32.partialorder %v6474, 0.0004427343 (stack95)
        %v6476 = vsel /*vm=*/%vm6475, /*on_true_vy=*/%v6473, /*on_false_vx=*/%v6470 (stack96)
        %v6477 = vxor.u32 %v6476, 2147483648 (stack87)
        %vm6480 = vcmp.lt.f32.partialorder %v6477, 5.0 (stack86)
        %v6485 = vsel /*vm=*/%vm6480, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v6489 = vsel /*vm=*/%vm6480, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v6493 = vsel /*vm=*/%vm6480, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v6497 = vsel /*vm=*/%vm6480, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v6501 = vsel /*vm=*/%vm6480, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v6505 = vsel /*vm=*/%vm6480, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v6509 = vsel /*vm=*/%vm6480, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v6513 = vsel /*vm=*/%vm6480, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v6517 = vsel /*vm=*/%vm6480, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v6521 = vadd.f32 %v6477, -2.5 (stack82)
        %v6523 = vrsqrt.pop %v6477 (stack97)
        %v6524 = vmul.f32 %v6477, %v6523 (stack98)
        %vm6525 = vcmp.eq.f32.partialorder %v6477, inf (stack99)
        %v6526 = vsel /*vm=*/%vm6525, /*on_true_vy=*/%v6477, /*on_false_vx=*/%v6524 (stack100)
        %vm6527 = vcmp.eq.f32.partialorder %v6477, 0.0 (stack101)
        %v6528 = vand.u32 %v6477, 2147483648 (stack102)
        %v6529 = vsel /*vm=*/%vm6527, /*on_true_vy=*/%v6528, /*on_false_vx=*/%v6526 (stack103)
        %v6532 = vadd.f32 %v6529, -3.0 (stack82)
        %v6536 = vsel /*vm=*/%vm6480, /*on_true_vy=*/%v6521, /*on_false_vx=*/%v6532 (stack72)
        %v6540 = vmul.f32 %v6517, %v6536 (stack83)
        %v6544 = vadd.f32 %v6513, %v6540 (stack82)
        %v6548 = vmul.f32 %v6544, %v6536 (stack83)
        %v6552 = vadd.f32 %v6509, %v6548 (stack82)
        %v6556 = vmul.f32 %v6552, %v6536 (stack83)
        %v6560 = vadd.f32 %v6505, %v6556 (stack82)
        %v6564 = vmul.f32 %v6560, %v6536 (stack83)
        %v6568 = vadd.f32 %v6501, %v6564 (stack82)
        %v6572 = vmul.f32 %v6568, %v6536 (stack83)
        %v6576 = vadd.f32 %v6497, %v6572 (stack82)
        %v6580 = vmul.f32 %v6576, %v6536 (stack83)
        %v6584 = vadd.f32 %v6493, %v6580 (stack82)
        %v6588 = vmul.f32 %v6584, %v6536 (stack83)
        %v6592 = vadd.f32 %v6489, %v6588 (stack82)
        %v6596 = vmul.f32 %v6592, %v6536 (stack83)
        %v6600 = vadd.f32 %v6485, %v6596 (stack82)
        %v6604 = vmul.f32 %v6600, %v6451 (stack83)
        %v6608 = vsel /*vm=*/%vm6456, /*on_true_vy=*/%v6461, /*on_false_vx=*/%v6604 (stack72)
        %v6612 = vmul.f32 %v6608, 1.4140625 (stack83)
        %s6614 = scalar_lea.vmem %s280, 516 [#allocation0] (stack107)
        %v6615 = vpack.c.bf16 0.0, %v6612 (stack104)
        %6616 = vst [vmem:[%s6614] sm:$0xf] /*vst_source=*/%v6615 (stack105)
        %v6619 = vadd.s32 %v2842, %v4311 (stack65)
        %s6621 = smul.u32 128, %s27 (stack66)
        %v6622 = vlaneseq (stack67)
        %v6623 = vand.u32 %v6622, 127 (stack68)
        %v6624 = vstv %s6621 (stack69)
        %v6625 = vadd.s32 %v6623, %v6624 (stack70)
        %v6629 = vadd.s32 %v6619, %v6625 (stack65)
        %vm6633 = vcmp.lt.u32.totalorder %v6629, %v6619 (stack71)
        %vm6638 = vcmp.lt.u32.totalorder %v6619, %v2842 (stack71)
        %v6643 = vadd.s32 %v2829, %v4294 (stack65)
        %v6647 = vadd.s32 %v6643, 1 (stack65)
        %v6651 = vsel /*vm=*/%vm6638, /*on_true_vy=*/%v6647, /*on_false_vx=*/%v6643 (stack72)
        %v6655 = vadd.s32 %v6651, 1 (stack65)
        %v6659 = vsel /*vm=*/%vm6633, /*on_true_vy=*/%v6655, /*on_false_vx=*/%v6651 (stack72)
        %v6664 = vadd.s32 %v6659, %v10 (stack65)
        %v6668 = vadd.s32 %v6629, %v9 (stack65)
        %v6672 = vadd.s32 %v6664, %v6668 (stack65)
        %v6674 = vshll.u32 %v6668, 13 (stack73)
        %v6675 = vshrl.u32 %v6668, 19 (stack74)
        %v6676 = vor.u32 %v6674, %v6675 (stack75)
        %v6677 = vxor.u32 %v6672, %v6676 (stack76)
        %v6680 = vadd.s32 %v6672, %v6677 (stack65)
        %v6682 = vshll.u32 %v6677, 15 (stack73)
        %v6683 = vshrl.u32 %v6677, 17 (stack74)
        %v6684 = vor.u32 %v6682, %v6683 (stack75)
        %v6685 = vxor.u32 %v6680, %v6684 (stack76)
        %v6688 = vadd.s32 %v6680, %v6685 (stack65)
        %v6690 = vshll.u32 %v6685, 26 (stack73)
        %v6691 = vshrl.u32 %v6685, 6 (stack74)
        %v6692 = vor.u32 %v6690, %v6691 (stack75)
        %v6693 = vxor.u32 %v6688, %v6692 (stack76)
        %v6696 = vadd.s32 %v6688, %v6693 (stack65)
        %v6700 = vadd.s32 %v6696, %v9 (stack65)
        %v6702 = vshll.u32 %v6693, 6 (stack73)
        %v6703 = vshrl.u32 %v6693, 26 (stack74)
        %v6704 = vor.u32 %v6702, %v6703 (stack75)
        %v6705 = vxor.u32 %v6696, %v6704 (stack76)
        %v6708 = vadd.s32 %v6705, %v8 (stack65)
        %v6712 = vadd.s32 %v6708, 1 (stack65)
        %v6716 = vadd.s32 %v6700, %v6712 (stack65)
        %v6718 = vshll.u32 %v6712, 17 (stack73)
        %v6719 = vshrl.u32 %v6712, 15 (stack74)
        %v6720 = vor.u32 %v6718, %v6719 (stack75)
        %v6721 = vxor.u32 %v6716, %v6720 (stack76)
        %v6724 = vadd.s32 %v6716, %v6721 (stack65)
        %v6726 = vshll.u32 %v6721, 29 (stack73)
        %v6727 = vshrl.u32 %v6721, 3 (stack74)
        %v6728 = vor.u32 %v6726, %v6727 (stack75)
        %v6729 = vxor.u32 %v6724, %v6728 (stack76)
        %v6732 = vadd.s32 %v6724, %v6729 (stack65)
        %v6734 = vshll.u32 %v6729, 16 (stack73)
        %v6735 = vshrl.u32 %v6729, 16 (stack74)
        %v6736 = vor.u32 %v6734, %v6735 (stack75)
        %v6737 = vxor.u32 %v6732, %v6736 (stack76)
        %v6740 = vadd.s32 %v6732, %v6737 (stack65)
        %v6744 = vadd.s32 %v6740, %v8 (stack65)
        %v6746 = vshll.u32 %v6737, 24 (stack73)
        %v6747 = vshrl.u32 %v6737, 8 (stack74)
        %v6748 = vor.u32 %v6746, %v6747 (stack75)
        %v6749 = vxor.u32 %v6740, %v6748 (stack76)
        %v6752 = vadd.s32 %v6749, %v10 (stack65)
        %v6756 = vadd.s32 %v6752, 2 (stack65)
        %v6760 = vadd.s32 %v6744, %v6756 (stack65)
        %v6762 = vshll.u32 %v6756, 13 (stack73)
        %v6763 = vshrl.u32 %v6756, 19 (stack74)
        %v6764 = vor.u32 %v6762, %v6763 (stack75)
        %v6765 = vxor.u32 %v6760, %v6764 (stack76)
        %v6768 = vadd.s32 %v6760, %v6765 (stack65)
        %v6770 = vshll.u32 %v6765, 15 (stack73)
        %v6771 = vshrl.u32 %v6765, 17 (stack74)
        %v6772 = vor.u32 %v6770, %v6771 (stack75)
        %v6773 = vxor.u32 %v6768, %v6772 (stack76)
        %v6776 = vadd.s32 %v6768, %v6773 (stack65)
        %v6778 = vshll.u32 %v6773, 26 (stack73)
        %v6779 = vshrl.u32 %v6773, 6 (stack74)
        %v6780 = vor.u32 %v6778, %v6779 (stack75)
        %v6781 = vxor.u32 %v6776, %v6780 (stack76)
        %v6784 = vadd.s32 %v6776, %v6781 (stack65)
        %v6788 = vadd.s32 %v6784, %v10 (stack65)
        %v6790 = vshll.u32 %v6781, 6 (stack73)
        %v6791 = vshrl.u32 %v6781, 26 (stack74)
        %v6792 = vor.u32 %v6790, %v6791 (stack75)
        %v6793 = vxor.u32 %v6784, %v6792 (stack76)
        %v6796 = vadd.s32 %v6793, %v9 (stack65)
        %v6800 = vadd.s32 %v6796, 3 (stack65)
        %v6804 = vadd.s32 %v6788, %v6800 (stack65)
        %v6806 = vshll.u32 %v6800, 17 (stack73)
        %v6807 = vshrl.u32 %v6800, 15 (stack74)
        %v6808 = vor.u32 %v6806, %v6807 (stack75)
        %v6809 = vxor.u32 %v6804, %v6808 (stack76)
        %v6812 = vadd.s32 %v6804, %v6809 (stack65)
        %v6814 = vshll.u32 %v6809, 29 (stack73)
        %v6815 = vshrl.u32 %v6809, 3 (stack74)
        %v6816 = vor.u32 %v6814, %v6815 (stack75)
        %v6817 = vxor.u32 %v6812, %v6816 (stack76)
        %v6820 = vadd.s32 %v6812, %v6817 (stack65)
        %v6822 = vshll.u32 %v6817, 16 (stack73)
        %v6823 = vshrl.u32 %v6817, 16 (stack74)
        %v6824 = vor.u32 %v6822, %v6823 (stack75)
        %v6825 = vxor.u32 %v6820, %v6824 (stack76)
        %v6828 = vadd.s32 %v6820, %v6825 (stack65)
        %v6832 = vadd.s32 %v6828, %v9 (stack65)
        %v6834 = vshll.u32 %v6825, 24 (stack73)
        %v6835 = vshrl.u32 %v6825, 8 (stack74)
        %v6836 = vor.u32 %v6834, %v6835 (stack75)
        %v6837 = vxor.u32 %v6828, %v6836 (stack76)
        %v6840 = vadd.s32 %v6837, %v8 (stack65)
        %v6844 = vadd.s32 %v6840, 4 (stack65)
        %v6848 = vadd.s32 %v6832, %v6844 (stack65)
        %v6850 = vshll.u32 %v6844, 13 (stack73)
        %v6851 = vshrl.u32 %v6844, 19 (stack74)
        %v6852 = vor.u32 %v6850, %v6851 (stack75)
        %v6853 = vxor.u32 %v6848, %v6852 (stack76)
        %v6856 = vadd.s32 %v6848, %v6853 (stack65)
        %v6858 = vshll.u32 %v6853, 15 (stack73)
        %v6859 = vshrl.u32 %v6853, 17 (stack74)
        %v6860 = vor.u32 %v6858, %v6859 (stack75)
        %v6861 = vxor.u32 %v6856, %v6860 (stack76)
        %v6864 = vadd.s32 %v6856, %v6861 (stack65)
        %v6866 = vshll.u32 %v6861, 26 (stack73)
        %v6867 = vshrl.u32 %v6861, 6 (stack74)
        %v6868 = vor.u32 %v6866, %v6867 (stack75)
        %v6869 = vxor.u32 %v6864, %v6868 (stack76)
        %v6872 = vadd.s32 %v6864, %v6869 (stack65)
        %v6876 = vadd.s32 %v6872, %v8 (stack65)
        %v6878 = vshll.u32 %v6869, 6 (stack73)
        %v6879 = vshrl.u32 %v6869, 26 (stack74)
        %v6880 = vor.u32 %v6878, %v6879 (stack75)
        %v6881 = vxor.u32 %v6872, %v6880 (stack76)
        %v6884 = vadd.s32 %v6881, %v10 (stack65)
        %v6888 = vadd.s32 %v6884, 5 (stack65)
        %v6890 = vxor.u32 %v6876, %v6888 (stack76)
        %v6891 = vand.u32.u8 %v6890, 255 (stack77)
        %v6892 = vand.u32 %v6891, 65535 (stack78)
        %v6893 = vshrl.u32 %v6892, 1 (stack79)
        %v6894 = vor.u32 %v6893, 16256 (stack75)
        %v6895 = vand.u32.u16 %v6894, 65535 (stack80)
        %v6896 = vunpack.i.l.bf16 %v6895 (stack81)
        %v6900 = vadd.f32 %v6896, -1.0 (stack82)
        %v6904 = vmul.f32 %v6900, 2.0 (stack83)
        %v6908 = vadd.f32 %v6904, -0.99609375 (stack82)
        %v6912 = vmax.f32 -0.99609375, %v6908 (stack84)
        %v6914 = vand.u32 2147483647, %v6912 (stack85)
        %vm6917 = vcmp.eq.f32.partialorder %v6914, 1.0 (stack86)
        %v6922 = vmul.f32 %v6912, inf (stack83)
        %v6924 = vxor.u32 %v6912, 2147483648 (stack87)
        %v6927 = vmul.f32 %v6912, %v6924 (stack83)
        %v6929 = vadd.f32 %v6927, 1.0 (stack88)
        %v6930 = vlog2.pop %v6929 (stack89)
        %v6931 = vmul.f32 %v6930, 0.6931472 (stack90)
        %v6932 = vmul.f32 -0.5, %v6927 (stack91)
        %v6933 = vadd.f32 %v6932, 1.0 (stack92)
        %v6934 = vmul.f32 %v6933, %v6927 (stack93)
        %v6935 = vand.u32 2147483647, %v6927 (stack94)
        %vm6936 = vcmp.lt.f32.partialorder %v6935, 0.0004427343 (stack95)
        %v6937 = vsel /*vm=*/%vm6936, /*on_true_vy=*/%v6934, /*on_false_vx=*/%v6931 (stack96)
        %v6938 = vxor.u32 %v6937, 2147483648 (stack87)
        %vm6941 = vcmp.lt.f32.partialorder %v6938, 5.0 (stack86)
        %v6946 = vsel /*vm=*/%vm6941, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v6950 = vsel /*vm=*/%vm6941, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v6954 = vsel /*vm=*/%vm6941, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v6958 = vsel /*vm=*/%vm6941, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v6962 = vsel /*vm=*/%vm6941, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v6966 = vsel /*vm=*/%vm6941, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v6970 = vsel /*vm=*/%vm6941, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v6974 = vsel /*vm=*/%vm6941, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v6978 = vsel /*vm=*/%vm6941, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v6982 = vadd.f32 %v6938, -2.5 (stack82)
        %v6984 = vrsqrt.pop %v6938 (stack97)
        %v6985 = vmul.f32 %v6938, %v6984 (stack98)
        %vm6986 = vcmp.eq.f32.partialorder %v6938, inf (stack99)
        %v6987 = vsel /*vm=*/%vm6986, /*on_true_vy=*/%v6938, /*on_false_vx=*/%v6985 (stack100)
        %vm6988 = vcmp.eq.f32.partialorder %v6938, 0.0 (stack101)
        %v6989 = vand.u32 %v6938, 2147483648 (stack102)
        %v6990 = vsel /*vm=*/%vm6988, /*on_true_vy=*/%v6989, /*on_false_vx=*/%v6987 (stack103)
        %v6993 = vadd.f32 %v6990, -3.0 (stack82)
        %v6997 = vsel /*vm=*/%vm6941, /*on_true_vy=*/%v6982, /*on_false_vx=*/%v6993 (stack72)
        %v7001 = vmul.f32 %v6978, %v6997 (stack83)
        %v7005 = vadd.f32 %v6974, %v7001 (stack82)
        %v7009 = vmul.f32 %v7005, %v6997 (stack83)
        %v7013 = vadd.f32 %v6970, %v7009 (stack82)
        %v7017 = vmul.f32 %v7013, %v6997 (stack83)
        %v7021 = vadd.f32 %v6966, %v7017 (stack82)
        %v7025 = vmul.f32 %v7021, %v6997 (stack83)
        %v7029 = vadd.f32 %v6962, %v7025 (stack82)
        %v7033 = vmul.f32 %v7029, %v6997 (stack83)
        %v7037 = vadd.f32 %v6958, %v7033 (stack82)
        %v7041 = vmul.f32 %v7037, %v6997 (stack83)
        %v7045 = vadd.f32 %v6954, %v7041 (stack82)
        %v7049 = vmul.f32 %v7045, %v6997 (stack83)
        %v7053 = vadd.f32 %v6950, %v7049 (stack82)
        %v7057 = vmul.f32 %v7053, %v6997 (stack83)
        %v7061 = vadd.f32 %v6946, %v7057 (stack82)
        %v7065 = vmul.f32 %v7061, %v6912 (stack83)
        %v7069 = vsel /*vm=*/%vm6917, /*on_true_vy=*/%v6922, /*on_false_vx=*/%v7065 (stack72)
        %v7073 = vmul.f32 %v7069, 1.4140625 (stack83)
        %s7075 = scalar_lea.vmem %s280, 644 [#allocation0] (stack107)
        %v7076 = vpack.c.bf16 0.0, %v7073 (stack104)
        %7077 = vst [vmem:[%s7075] sm:$0xf] /*vst_source=*/%v7076 (stack105)
        %v7080 = vadd.s32 %v3329, %v4311 (stack65)
        %s7082 = smul.u32 128, %s27 (stack66)
        %v7083 = vlaneseq (stack67)
        %v7084 = vand.u32 %v7083, 127 (stack68)
        %v7085 = vstv %s7082 (stack69)
        %v7086 = vadd.s32 %v7084, %v7085 (stack70)
        %v7090 = vadd.s32 %v7080, %v7086 (stack65)
        %vm7094 = vcmp.lt.u32.totalorder %v7090, %v7080 (stack71)
        %vm7099 = vcmp.lt.u32.totalorder %v7080, %v3329 (stack71)
        %v7104 = vadd.s32 %v3316, %v4294 (stack65)
        %v7108 = vadd.s32 %v7104, 1 (stack65)
        %v7112 = vsel /*vm=*/%vm7099, /*on_true_vy=*/%v7108, /*on_false_vx=*/%v7104 (stack72)
        %v7116 = vadd.s32 %v7112, 1 (stack65)
        %v7120 = vsel /*vm=*/%vm7094, /*on_true_vy=*/%v7116, /*on_false_vx=*/%v7112 (stack72)
        %v7125 = vadd.s32 %v7120, %v10 (stack65)
        %v7129 = vadd.s32 %v7090, %v9 (stack65)
        %v7133 = vadd.s32 %v7125, %v7129 (stack65)
        %v7135 = vshll.u32 %v7129, 13 (stack73)
        %v7136 = vshrl.u32 %v7129, 19 (stack74)
        %v7137 = vor.u32 %v7135, %v7136 (stack75)
        %v7138 = vxor.u32 %v7133, %v7137 (stack76)
        %v7141 = vadd.s32 %v7133, %v7138 (stack65)
        %v7143 = vshll.u32 %v7138, 15 (stack73)
        %v7144 = vshrl.u32 %v7138, 17 (stack74)
        %v7145 = vor.u32 %v7143, %v7144 (stack75)
        %v7146 = vxor.u32 %v7141, %v7145 (stack76)
        %v7149 = vadd.s32 %v7141, %v7146 (stack65)
        %v7151 = vshll.u32 %v7146, 26 (stack73)
        %v7152 = vshrl.u32 %v7146, 6 (stack74)
        %v7153 = vor.u32 %v7151, %v7152 (stack75)
        %v7154 = vxor.u32 %v7149, %v7153 (stack76)
        %v7157 = vadd.s32 %v7149, %v7154 (stack65)
        %v7161 = vadd.s32 %v7157, %v9 (stack65)
        %v7163 = vshll.u32 %v7154, 6 (stack73)
        %v7164 = vshrl.u32 %v7154, 26 (stack74)
        %v7165 = vor.u32 %v7163, %v7164 (stack75)
        %v7166 = vxor.u32 %v7157, %v7165 (stack76)
        %v7169 = vadd.s32 %v7166, %v8 (stack65)
        %v7173 = vadd.s32 %v7169, 1 (stack65)
        %v7177 = vadd.s32 %v7161, %v7173 (stack65)
        %v7179 = vshll.u32 %v7173, 17 (stack73)
        %v7180 = vshrl.u32 %v7173, 15 (stack74)
        %v7181 = vor.u32 %v7179, %v7180 (stack75)
        %v7182 = vxor.u32 %v7177, %v7181 (stack76)
        %v7185 = vadd.s32 %v7177, %v7182 (stack65)
        %v7187 = vshll.u32 %v7182, 29 (stack73)
        %v7188 = vshrl.u32 %v7182, 3 (stack74)
        %v7189 = vor.u32 %v7187, %v7188 (stack75)
        %v7190 = vxor.u32 %v7185, %v7189 (stack76)
        %v7193 = vadd.s32 %v7185, %v7190 (stack65)
        %v7195 = vshll.u32 %v7190, 16 (stack73)
        %v7196 = vshrl.u32 %v7190, 16 (stack74)
        %v7197 = vor.u32 %v7195, %v7196 (stack75)
        %v7198 = vxor.u32 %v7193, %v7197 (stack76)
        %v7201 = vadd.s32 %v7193, %v7198 (stack65)
        %v7205 = vadd.s32 %v7201, %v8 (stack65)
        %v7207 = vshll.u32 %v7198, 24 (stack73)
        %v7208 = vshrl.u32 %v7198, 8 (stack74)
        %v7209 = vor.u32 %v7207, %v7208 (stack75)
        %v7210 = vxor.u32 %v7201, %v7209 (stack76)
        %v7213 = vadd.s32 %v7210, %v10 (stack65)
        %v7217 = vadd.s32 %v7213, 2 (stack65)
        %v7221 = vadd.s32 %v7205, %v7217 (stack65)
        %v7223 = vshll.u32 %v7217, 13 (stack73)
        %v7224 = vshrl.u32 %v7217, 19 (stack74)
        %v7225 = vor.u32 %v7223, %v7224 (stack75)
        %v7226 = vxor.u32 %v7221, %v7225 (stack76)
        %v7229 = vadd.s32 %v7221, %v7226 (stack65)
        %v7231 = vshll.u32 %v7226, 15 (stack73)
        %v7232 = vshrl.u32 %v7226, 17 (stack74)
        %v7233 = vor.u32 %v7231, %v7232 (stack75)
        %v7234 = vxor.u32 %v7229, %v7233 (stack76)
        %v7237 = vadd.s32 %v7229, %v7234 (stack65)
        %v7239 = vshll.u32 %v7234, 26 (stack73)
        %v7240 = vshrl.u32 %v7234, 6 (stack74)
        %v7241 = vor.u32 %v7239, %v7240 (stack75)
        %v7242 = vxor.u32 %v7237, %v7241 (stack76)
        %v7245 = vadd.s32 %v7237, %v7242 (stack65)
        %v7249 = vadd.s32 %v7245, %v10 (stack65)
        %v7251 = vshll.u32 %v7242, 6 (stack73)
        %v7252 = vshrl.u32 %v7242, 26 (stack74)
        %v7253 = vor.u32 %v7251, %v7252 (stack75)
        %v7254 = vxor.u32 %v7245, %v7253 (stack76)
        %v7257 = vadd.s32 %v7254, %v9 (stack65)
        %v7261 = vadd.s32 %v7257, 3 (stack65)
        %v7265 = vadd.s32 %v7249, %v7261 (stack65)
        %v7267 = vshll.u32 %v7261, 17 (stack73)
        %v7268 = vshrl.u32 %v7261, 15 (stack74)
        %v7269 = vor.u32 %v7267, %v7268 (stack75)
        %v7270 = vxor.u32 %v7265, %v7269 (stack76)
        %v7273 = vadd.s32 %v7265, %v7270 (stack65)
        %v7275 = vshll.u32 %v7270, 29 (stack73)
        %v7276 = vshrl.u32 %v7270, 3 (stack74)
        %v7277 = vor.u32 %v7275, %v7276 (stack75)
        %v7278 = vxor.u32 %v7273, %v7277 (stack76)
        %v7281 = vadd.s32 %v7273, %v7278 (stack65)
        %v7283 = vshll.u32 %v7278, 16 (stack73)
        %v7284 = vshrl.u32 %v7278, 16 (stack74)
        %v7285 = vor.u32 %v7283, %v7284 (stack75)
        %v7286 = vxor.u32 %v7281, %v7285 (stack76)
        %v7289 = vadd.s32 %v7281, %v7286 (stack65)
        %v7293 = vadd.s32 %v7289, %v9 (stack65)
        %v7295 = vshll.u32 %v7286, 24 (stack73)
        %v7296 = vshrl.u32 %v7286, 8 (stack74)
        %v7297 = vor.u32 %v7295, %v7296 (stack75)
        %v7298 = vxor.u32 %v7289, %v7297 (stack76)
        %v7301 = vadd.s32 %v7298, %v8 (stack65)
        %v7305 = vadd.s32 %v7301, 4 (stack65)
        %v7309 = vadd.s32 %v7293, %v7305 (stack65)
        %v7311 = vshll.u32 %v7305, 13 (stack73)
        %v7312 = vshrl.u32 %v7305, 19 (stack74)
        %v7313 = vor.u32 %v7311, %v7312 (stack75)
        %v7314 = vxor.u32 %v7309, %v7313 (stack76)
        %v7317 = vadd.s32 %v7309, %v7314 (stack65)
        %v7319 = vshll.u32 %v7314, 15 (stack73)
        %v7320 = vshrl.u32 %v7314, 17 (stack74)
        %v7321 = vor.u32 %v7319, %v7320 (stack75)
        %v7322 = vxor.u32 %v7317, %v7321 (stack76)
        %v7325 = vadd.s32 %v7317, %v7322 (stack65)
        %v7327 = vshll.u32 %v7322, 26 (stack73)
        %v7328 = vshrl.u32 %v7322, 6 (stack74)
        %v7329 = vor.u32 %v7327, %v7328 (stack75)
        %v7330 = vxor.u32 %v7325, %v7329 (stack76)
        %v7333 = vadd.s32 %v7325, %v7330 (stack65)
        %v7337 = vadd.s32 %v7333, %v8 (stack65)
        %v7339 = vshll.u32 %v7330, 6 (stack73)
        %v7340 = vshrl.u32 %v7330, 26 (stack74)
        %v7341 = vor.u32 %v7339, %v7340 (stack75)
        %v7342 = vxor.u32 %v7333, %v7341 (stack76)
        %v7345 = vadd.s32 %v7342, %v10 (stack65)
        %v7349 = vadd.s32 %v7345, 5 (stack65)
        %v7351 = vxor.u32 %v7337, %v7349 (stack76)
        %v7352 = vand.u32.u8 %v7351, 255 (stack77)
        %v7353 = vand.u32 %v7352, 65535 (stack78)
        %v7354 = vshrl.u32 %v7353, 1 (stack79)
        %v7355 = vor.u32 %v7354, 16256 (stack75)
        %v7356 = vand.u32.u16 %v7355, 65535 (stack80)
        %v7357 = vunpack.i.l.bf16 %v7356 (stack81)
        %v7361 = vadd.f32 %v7357, -1.0 (stack82)
        %v7365 = vmul.f32 %v7361, 2.0 (stack83)
        %v7369 = vadd.f32 %v7365, -0.99609375 (stack82)
        %v7373 = vmax.f32 -0.99609375, %v7369 (stack84)
        %v7375 = vand.u32 2147483647, %v7373 (stack85)
        %vm7378 = vcmp.eq.f32.partialorder %v7375, 1.0 (stack86)
        %v7383 = vmul.f32 %v7373, inf (stack83)
        %v7385 = vxor.u32 %v7373, 2147483648 (stack87)
        %v7388 = vmul.f32 %v7373, %v7385 (stack83)
        %v7390 = vadd.f32 %v7388, 1.0 (stack88)
        %v7391 = vlog2.pop %v7390 (stack89)
        %v7392 = vmul.f32 %v7391, 0.6931472 (stack90)
        %v7393 = vmul.f32 -0.5, %v7388 (stack91)
        %v7394 = vadd.f32 %v7393, 1.0 (stack92)
        %v7395 = vmul.f32 %v7394, %v7388 (stack93)
        %v7396 = vand.u32 2147483647, %v7388 (stack94)
        %vm7397 = vcmp.lt.f32.partialorder %v7396, 0.0004427343 (stack95)
        %v7398 = vsel /*vm=*/%vm7397, /*on_true_vy=*/%v7395, /*on_false_vx=*/%v7392 (stack96)
        %v7399 = vxor.u32 %v7398, 2147483648 (stack87)
        %vm7402 = vcmp.lt.f32.partialorder %v7399, 5.0 (stack86)
        %v7407 = vsel /*vm=*/%vm7402, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v7411 = vsel /*vm=*/%vm7402, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v7415 = vsel /*vm=*/%vm7402, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v7419 = vsel /*vm=*/%vm7402, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v7423 = vsel /*vm=*/%vm7402, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v7427 = vsel /*vm=*/%vm7402, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v7431 = vsel /*vm=*/%vm7402, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v7435 = vsel /*vm=*/%vm7402, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v7439 = vsel /*vm=*/%vm7402, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v7443 = vadd.f32 %v7399, -2.5 (stack82)
        %v7445 = vrsqrt.pop %v7399 (stack97)
        %v7446 = vmul.f32 %v7399, %v7445 (stack98)
        %vm7447 = vcmp.eq.f32.partialorder %v7399, inf (stack99)
        %v7448 = vsel /*vm=*/%vm7447, /*on_true_vy=*/%v7399, /*on_false_vx=*/%v7446 (stack100)
        %vm7449 = vcmp.eq.f32.partialorder %v7399, 0.0 (stack101)
        %v7450 = vand.u32 %v7399, 2147483648 (stack102)
        %v7451 = vsel /*vm=*/%vm7449, /*on_true_vy=*/%v7450, /*on_false_vx=*/%v7448 (stack103)
        %v7454 = vadd.f32 %v7451, -3.0 (stack82)
        %v7458 = vsel /*vm=*/%vm7402, /*on_true_vy=*/%v7443, /*on_false_vx=*/%v7454 (stack72)
        %v7462 = vmul.f32 %v7439, %v7458 (stack83)
        %v7466 = vadd.f32 %v7435, %v7462 (stack82)
        %v7470 = vmul.f32 %v7466, %v7458 (stack83)
        %v7474 = vadd.f32 %v7431, %v7470 (stack82)
        %v7478 = vmul.f32 %v7474, %v7458 (stack83)
        %v7482 = vadd.f32 %v7427, %v7478 (stack82)
        %v7486 = vmul.f32 %v7482, %v7458 (stack83)
        %v7490 = vadd.f32 %v7423, %v7486 (stack82)
        %v7494 = vmul.f32 %v7490, %v7458 (stack83)
        %v7498 = vadd.f32 %v7419, %v7494 (stack82)
        %v7502 = vmul.f32 %v7498, %v7458 (stack83)
        %v7506 = vadd.f32 %v7415, %v7502 (stack82)
        %v7510 = vmul.f32 %v7506, %v7458 (stack83)
        %v7514 = vadd.f32 %v7411, %v7510 (stack82)
        %v7518 = vmul.f32 %v7514, %v7458 (stack83)
        %v7522 = vadd.f32 %v7407, %v7518 (stack82)
        %v7526 = vmul.f32 %v7522, %v7373 (stack83)
        %v7530 = vsel /*vm=*/%vm7378, /*on_true_vy=*/%v7383, /*on_false_vx=*/%v7526 (stack72)
        %v7534 = vmul.f32 %v7530, 1.4140625 (stack83)
        %s7536 = scalar_lea.vmem %s280, 772 [#allocation0] (stack107)
        %v7537 = vpack.c.bf16 0.0, %v7534 (stack104)
        %7538 = vst [vmem:[%s7536] sm:$0xf] /*vst_source=*/%v7537 (stack105)
        %v7541 = vadd.s32 %v3816, %v4311 (stack65)
        %s7543 = smul.u32 128, %s27 (stack66)
        %v7544 = vlaneseq (stack67)
        %v7545 = vand.u32 %v7544, 127 (stack68)
        %v7546 = vstv %s7543 (stack69)
        %v7547 = vadd.s32 %v7545, %v7546 (stack70)
        %v7551 = vadd.s32 %v7541, %v7547 (stack65)
        %vm7555 = vcmp.lt.u32.totalorder %v7551, %v7541 (stack71)
        %vm7560 = vcmp.lt.u32.totalorder %v7541, %v3816 (stack71)
        %v7565 = vadd.s32 %v3803, %v4294 (stack65)
        %v7569 = vadd.s32 %v7565, 1 (stack65)
        %v7573 = vsel /*vm=*/%vm7560, /*on_true_vy=*/%v7569, /*on_false_vx=*/%v7565 (stack72)
        %v7577 = vadd.s32 %v7573, 1 (stack65)
        %v7581 = vsel /*vm=*/%vm7555, /*on_true_vy=*/%v7577, /*on_false_vx=*/%v7573 (stack72)
        %v7586 = vadd.s32 %v7581, %v10 (stack65)
        %v7590 = vadd.s32 %v7551, %v9 (stack65)
        %v7594 = vadd.s32 %v7586, %v7590 (stack65)
        %v7596 = vshll.u32 %v7590, 13 (stack73)
        %v7597 = vshrl.u32 %v7590, 19 (stack74)
        %v7598 = vor.u32 %v7596, %v7597 (stack75)
        %v7599 = vxor.u32 %v7594, %v7598 (stack76)
        %v7602 = vadd.s32 %v7594, %v7599 (stack65)
        %v7604 = vshll.u32 %v7599, 15 (stack73)
        %v7605 = vshrl.u32 %v7599, 17 (stack74)
        %v7606 = vor.u32 %v7604, %v7605 (stack75)
        %v7607 = vxor.u32 %v7602, %v7606 (stack76)
        %v7610 = vadd.s32 %v7602, %v7607 (stack65)
        %v7612 = vshll.u32 %v7607, 26 (stack73)
        %v7613 = vshrl.u32 %v7607, 6 (stack74)
        %v7614 = vor.u32 %v7612, %v7613 (stack75)
        %v7615 = vxor.u32 %v7610, %v7614 (stack76)
        %v7618 = vadd.s32 %v7610, %v7615 (stack65)
        %v7622 = vadd.s32 %v7618, %v9 (stack65)
        %v7624 = vshll.u32 %v7615, 6 (stack73)
        %v7625 = vshrl.u32 %v7615, 26 (stack74)
        %v7626 = vor.u32 %v7624, %v7625 (stack75)
        %v7627 = vxor.u32 %v7618, %v7626 (stack76)
        %v7630 = vadd.s32 %v7627, %v8 (stack65)
        %v7634 = vadd.s32 %v7630, 1 (stack65)
        %v7638 = vadd.s32 %v7622, %v7634 (stack65)
        %v7640 = vshll.u32 %v7634, 17 (stack73)
        %v7641 = vshrl.u32 %v7634, 15 (stack74)
        %v7642 = vor.u32 %v7640, %v7641 (stack75)
        %v7643 = vxor.u32 %v7638, %v7642 (stack76)
        %v7646 = vadd.s32 %v7638, %v7643 (stack65)
        %v7648 = vshll.u32 %v7643, 29 (stack73)
        %v7649 = vshrl.u32 %v7643, 3 (stack74)
        %v7650 = vor.u32 %v7648, %v7649 (stack75)
        %v7651 = vxor.u32 %v7646, %v7650 (stack76)
        %v7654 = vadd.s32 %v7646, %v7651 (stack65)
        %v7656 = vshll.u32 %v7651, 16 (stack73)
        %v7657 = vshrl.u32 %v7651, 16 (stack74)
        %v7658 = vor.u32 %v7656, %v7657 (stack75)
        %v7659 = vxor.u32 %v7654, %v7658 (stack76)
        %v7662 = vadd.s32 %v7654, %v7659 (stack65)
        %v7666 = vadd.s32 %v7662, %v8 (stack65)
        %v7668 = vshll.u32 %v7659, 24 (stack73)
        %v7669 = vshrl.u32 %v7659, 8 (stack74)
        %v7670 = vor.u32 %v7668, %v7669 (stack75)
        %v7671 = vxor.u32 %v7662, %v7670 (stack76)
        %v7674 = vadd.s32 %v7671, %v10 (stack65)
        %v7678 = vadd.s32 %v7674, 2 (stack65)
        %v7682 = vadd.s32 %v7666, %v7678 (stack65)
        %v7684 = vshll.u32 %v7678, 13 (stack73)
        %v7685 = vshrl.u32 %v7678, 19 (stack74)
        %v7686 = vor.u32 %v7684, %v7685 (stack75)
        %v7687 = vxor.u32 %v7682, %v7686 (stack76)
        %v7690 = vadd.s32 %v7682, %v7687 (stack65)
        %v7692 = vshll.u32 %v7687, 15 (stack73)
        %v7693 = vshrl.u32 %v7687, 17 (stack74)
        %v7694 = vor.u32 %v7692, %v7693 (stack75)
        %v7695 = vxor.u32 %v7690, %v7694 (stack76)
        %v7698 = vadd.s32 %v7690, %v7695 (stack65)
        %v7700 = vshll.u32 %v7695, 26 (stack73)
        %v7701 = vshrl.u32 %v7695, 6 (stack74)
        %v7702 = vor.u32 %v7700, %v7701 (stack75)
        %v7703 = vxor.u32 %v7698, %v7702 (stack76)
        %v7706 = vadd.s32 %v7698, %v7703 (stack65)
        %v7710 = vadd.s32 %v7706, %v10 (stack65)
        %v7712 = vshll.u32 %v7703, 6 (stack73)
        %v7713 = vshrl.u32 %v7703, 26 (stack74)
        %v7714 = vor.u32 %v7712, %v7713 (stack75)
        %v7715 = vxor.u32 %v7706, %v7714 (stack76)
        %v7718 = vadd.s32 %v7715, %v9 (stack65)
        %v7722 = vadd.s32 %v7718, 3 (stack65)
        %v7726 = vadd.s32 %v7710, %v7722 (stack65)
        %v7728 = vshll.u32 %v7722, 17 (stack73)
        %v7729 = vshrl.u32 %v7722, 15 (stack74)
        %v7730 = vor.u32 %v7728, %v7729 (stack75)
        %v7731 = vxor.u32 %v7726, %v7730 (stack76)
        %v7734 = vadd.s32 %v7726, %v7731 (stack65)
        %v7736 = vshll.u32 %v7731, 29 (stack73)
        %v7737 = vshrl.u32 %v7731, 3 (stack74)
        %v7738 = vor.u32 %v7736, %v7737 (stack75)
        %v7739 = vxor.u32 %v7734, %v7738 (stack76)
        %v7742 = vadd.s32 %v7734, %v7739 (stack65)
        %v7744 = vshll.u32 %v7739, 16 (stack73)
        %v7745 = vshrl.u32 %v7739, 16 (stack74)
        %v7746 = vor.u32 %v7744, %v7745 (stack75)
        %v7747 = vxor.u32 %v7742, %v7746 (stack76)
        %v7750 = vadd.s32 %v7742, %v7747 (stack65)
        %v7754 = vadd.s32 %v7750, %v9 (stack65)
        %v7756 = vshll.u32 %v7747, 24 (stack73)
        %v7757 = vshrl.u32 %v7747, 8 (stack74)
        %v7758 = vor.u32 %v7756, %v7757 (stack75)
        %v7759 = vxor.u32 %v7750, %v7758 (stack76)
        %v7762 = vadd.s32 %v7759, %v8 (stack65)
        %v7766 = vadd.s32 %v7762, 4 (stack65)
        %v7770 = vadd.s32 %v7754, %v7766 (stack65)
        %v7772 = vshll.u32 %v7766, 13 (stack73)
        %v7773 = vshrl.u32 %v7766, 19 (stack74)
        %v7774 = vor.u32 %v7772, %v7773 (stack75)
        %v7775 = vxor.u32 %v7770, %v7774 (stack76)
        %v7778 = vadd.s32 %v7770, %v7775 (stack65)
        %v7780 = vshll.u32 %v7775, 15 (stack73)
        %v7781 = vshrl.u32 %v7775, 17 (stack74)
        %v7782 = vor.u32 %v7780, %v7781 (stack75)
        %v7783 = vxor.u32 %v7778, %v7782 (stack76)
        %v7786 = vadd.s32 %v7778, %v7783 (stack65)
        %v7788 = vshll.u32 %v7783, 26 (stack73)
        %v7789 = vshrl.u32 %v7783, 6 (stack74)
        %v7790 = vor.u32 %v7788, %v7789 (stack75)
        %v7791 = vxor.u32 %v7786, %v7790 (stack76)
        %v7794 = vadd.s32 %v7786, %v7791 (stack65)
        %v7798 = vadd.s32 %v7794, %v8 (stack65)
        %v7800 = vshll.u32 %v7791, 6 (stack73)
        %v7801 = vshrl.u32 %v7791, 26 (stack74)
        %v7802 = vor.u32 %v7800, %v7801 (stack75)
        %v7803 = vxor.u32 %v7794, %v7802 (stack76)
        %v7806 = vadd.s32 %v7803, %v10 (stack65)
        %v7810 = vadd.s32 %v7806, 5 (stack65)
        %v7812 = vxor.u32 %v7798, %v7810 (stack76)
        %v7813 = vand.u32.u8 %v7812, 255 (stack77)
        %v7814 = vand.u32 %v7813, 65535 (stack78)
        %v7815 = vshrl.u32 %v7814, 1 (stack79)
        %v7816 = vor.u32 %v7815, 16256 (stack75)
        %v7817 = vand.u32.u16 %v7816, 65535 (stack80)
        %v7818 = vunpack.i.l.bf16 %v7817 (stack81)
        %v7822 = vadd.f32 %v7818, -1.0 (stack82)
        %v7826 = vmul.f32 %v7822, 2.0 (stack83)
        %v7830 = vadd.f32 %v7826, -0.99609375 (stack82)
        %v7834 = vmax.f32 -0.99609375, %v7830 (stack84)
        %v7836 = vand.u32 2147483647, %v7834 (stack85)
        %vm7839 = vcmp.eq.f32.partialorder %v7836, 1.0 (stack86)
        %v7844 = vmul.f32 %v7834, inf (stack83)
        %v7846 = vxor.u32 %v7834, 2147483648 (stack87)
        %v7849 = vmul.f32 %v7834, %v7846 (stack83)
        %v7851 = vadd.f32 %v7849, 1.0 (stack88)
        %v7852 = vlog2.pop %v7851 (stack89)
        %v7853 = vmul.f32 %v7852, 0.6931472 (stack90)
        %v7854 = vmul.f32 -0.5, %v7849 (stack91)
        %v7855 = vadd.f32 %v7854, 1.0 (stack92)
        %v7856 = vmul.f32 %v7855, %v7849 (stack93)
        %v7857 = vand.u32 2147483647, %v7849 (stack94)
        %vm7858 = vcmp.lt.f32.partialorder %v7857, 0.0004427343 (stack95)
        %v7859 = vsel /*vm=*/%vm7858, /*on_true_vy=*/%v7856, /*on_false_vx=*/%v7853 (stack96)
        %v7860 = vxor.u32 %v7859, 2147483648 (stack87)
        %vm7863 = vcmp.lt.f32.partialorder %v7860, 5.0 (stack86)
        %v7868 = vsel /*vm=*/%vm7863, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v7872 = vsel /*vm=*/%vm7863, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v7876 = vsel /*vm=*/%vm7863, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v7880 = vsel /*vm=*/%vm7863, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v7884 = vsel /*vm=*/%vm7863, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v7888 = vsel /*vm=*/%vm7863, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v7892 = vsel /*vm=*/%vm7863, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v7896 = vsel /*vm=*/%vm7863, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v7900 = vsel /*vm=*/%vm7863, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v7904 = vadd.f32 %v7860, -2.5 (stack82)
        %v7906 = vrsqrt.pop %v7860 (stack97)
        %v7907 = vmul.f32 %v7860, %v7906 (stack98)
        %vm7908 = vcmp.eq.f32.partialorder %v7860, inf (stack99)
        %v7909 = vsel /*vm=*/%vm7908, /*on_true_vy=*/%v7860, /*on_false_vx=*/%v7907 (stack100)
        %vm7910 = vcmp.eq.f32.partialorder %v7860, 0.0 (stack101)
        %v7911 = vand.u32 %v7860, 2147483648 (stack102)
        %v7912 = vsel /*vm=*/%vm7910, /*on_true_vy=*/%v7911, /*on_false_vx=*/%v7909 (stack103)
        %v7915 = vadd.f32 %v7912, -3.0 (stack82)
        %v7919 = vsel /*vm=*/%vm7863, /*on_true_vy=*/%v7904, /*on_false_vx=*/%v7915 (stack72)
        %v7923 = vmul.f32 %v7900, %v7919 (stack83)
        %v7927 = vadd.f32 %v7896, %v7923 (stack82)
        %v7931 = vmul.f32 %v7927, %v7919 (stack83)
        %v7935 = vadd.f32 %v7892, %v7931 (stack82)
        %v7939 = vmul.f32 %v7935, %v7919 (stack83)
        %v7943 = vadd.f32 %v7888, %v7939 (stack82)
        %v7947 = vmul.f32 %v7943, %v7919 (stack83)
        %v7951 = vadd.f32 %v7884, %v7947 (stack82)
        %v7955 = vmul.f32 %v7951, %v7919 (stack83)
        %v7959 = vadd.f32 %v7880, %v7955 (stack82)
        %v7963 = vmul.f32 %v7959, %v7919 (stack83)
        %v7967 = vadd.f32 %v7876, %v7963 (stack82)
        %v7971 = vmul.f32 %v7967, %v7919 (stack83)
        %v7975 = vadd.f32 %v7872, %v7971 (stack82)
        %v7979 = vmul.f32 %v7975, %v7919 (stack83)
        %v7983 = vadd.f32 %v7868, %v7979 (stack82)
        %v7987 = vmul.f32 %v7983, %v7834 (stack83)
        %v7991 = vsel /*vm=*/%vm7839, /*on_true_vy=*/%v7844, /*on_false_vx=*/%v7987 (stack72)
        %v7995 = vmul.f32 %v7991, 1.4140625 (stack83)
        %s7997 = scalar_lea.vmem %s280, 900 [#allocation0] (stack107)
        %v7998 = vpack.c.bf16 0.0, %v7995 (stack104)
        %7999 = vst [vmem:[%s7997] sm:$0xf] /*vst_source=*/%v7998 (stack105)
        %s8000 = sadd.s32 %s339, 16 (stack106)
        %s8001 = sshrl.u32 %s8000, 10 (stack49)
        %p8002 = scmp.lt.s32.totalorder 1, %s8001 (stack50)
        %s8003 = scalar_select /*predicate=*/%p8002, /*on_true=*/1, /*on_false=*/%s8001 (stack51)
        %s8004 = sand.u32 %s8000, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s8005 = sshrl.u32 %s8004, 7 (stack53)
        %s8006 = sand.u32 %s8004, 127 /* smod.u32 w/div 128 */ (stack54)
        %s8007 = smul.addr %s8003, 8 (stack55)
        %s8008 = scalar_lea.vmem %s3, %s8007 (stack56)
        %s8010 = scalar_lea.vmem %s8008, %s8005 (stack57)
        %v8011 = vld [vmem:[%s8010] ss:$0 sm:$0xff] (stack58)
        %s8012 = sand.u32 %s8006, 255 (stack59)
        %s8014 = sor.u32 256, %s8012 (stack60)
        %8015 = vbcast.lane.b32.xlu0 %v8011, %s8014 (stack61)
        %v8016 = vpop.permute.xlu0 %8015 (stack62)
        %s8017 = sadd.s32 %s347, 16 (stack106)
        %s8018 = sshrl.u32 %s8017, 10 (stack49)
        %p8019 = scmp.lt.s32.totalorder 1, %s8018 (stack50)
        %s8020 = scalar_select /*predicate=*/%p8019, /*on_true=*/1, /*on_false=*/%s8018 (stack51)
        %s8021 = sand.u32 %s8017, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s8022 = sshrl.u32 %s8021, 7 (stack53)
        %s8023 = sand.u32 %s8021, 127 /* smod.u32 w/div 128 */ (stack54)
        %s8024 = smul.addr %s8020, 8 (stack55)
        %s8025 = scalar_lea.vmem %s5, %s8024 (stack56)
        %s8027 = scalar_lea.vmem %s8025, %s8022 (stack57)
        %v8028 = vld [vmem:[%s8027] ss:$0 sm:$0xff] (stack58)
        %s8029 = sand.u32 %s8023, 255 (stack59)
        %s8031 = sor.u32 256, %s8029 (stack60)
        %8032 = vbcast.lane.b32.xlu0 %v8028, %s8031 (stack61)
        %v8033 = vpop.permute.xlu0 %8032 (stack62)
        %v8036 = vadd.s32 %v408, %v8033 (stack65)
        %s8038 = smul.u32 128, %s27 (stack66)
        %v8039 = vlaneseq (stack67)
        %v8040 = vand.u32 %v8039, 127 (stack68)
        %v8041 = vstv %s8038 (stack69)
        %v8042 = vadd.s32 %v8040, %v8041 (stack70)
        %v8046 = vadd.s32 %v8036, %v8042 (stack65)
        %vm8050 = vcmp.lt.u32.totalorder %v8046, %v8036 (stack71)
        %vm8055 = vcmp.lt.u32.totalorder %v8036, %v408 (stack71)
        %v8060 = vadd.s32 %v380, %v8016 (stack65)
        %v8064 = vadd.s32 %v8060, 1 (stack65)
        %v8068 = vsel /*vm=*/%vm8055, /*on_true_vy=*/%v8064, /*on_false_vx=*/%v8060 (stack72)
        %v8072 = vadd.s32 %v8068, 1 (stack65)
        %v8076 = vsel /*vm=*/%vm8050, /*on_true_vy=*/%v8072, /*on_false_vx=*/%v8068 (stack72)
        %v8081 = vadd.s32 %v8076, %v10 (stack65)
        %v8085 = vadd.s32 %v8046, %v9 (stack65)
        %v8089 = vadd.s32 %v8081, %v8085 (stack65)
        %v8091 = vshll.u32 %v8085, 13 (stack73)
        %v8092 = vshrl.u32 %v8085, 19 (stack74)
        %v8093 = vor.u32 %v8091, %v8092 (stack75)
        %v8094 = vxor.u32 %v8089, %v8093 (stack76)
        %v8097 = vadd.s32 %v8089, %v8094 (stack65)
        %v8099 = vshll.u32 %v8094, 15 (stack73)
        %v8100 = vshrl.u32 %v8094, 17 (stack74)
        %v8101 = vor.u32 %v8099, %v8100 (stack75)
        %v8102 = vxor.u32 %v8097, %v8101 (stack76)
        %v8105 = vadd.s32 %v8097, %v8102 (stack65)
        %v8107 = vshll.u32 %v8102, 26 (stack73)
        %v8108 = vshrl.u32 %v8102, 6 (stack74)
        %v8109 = vor.u32 %v8107, %v8108 (stack75)
        %v8110 = vxor.u32 %v8105, %v8109 (stack76)
        %v8113 = vadd.s32 %v8105, %v8110 (stack65)
        %v8117 = vadd.s32 %v8113, %v9 (stack65)
        %v8119 = vshll.u32 %v8110, 6 (stack73)
        %v8120 = vshrl.u32 %v8110, 26 (stack74)
        %v8121 = vor.u32 %v8119, %v8120 (stack75)
        %v8122 = vxor.u32 %v8113, %v8121 (stack76)
        %v8125 = vadd.s32 %v8122, %v8 (stack65)
        %v8129 = vadd.s32 %v8125, 1 (stack65)
        %v8133 = vadd.s32 %v8117, %v8129 (stack65)
        %v8135 = vshll.u32 %v8129, 17 (stack73)
        %v8136 = vshrl.u32 %v8129, 15 (stack74)
        %v8137 = vor.u32 %v8135, %v8136 (stack75)
        %v8138 = vxor.u32 %v8133, %v8137 (stack76)
        %v8141 = vadd.s32 %v8133, %v8138 (stack65)
        %v8143 = vshll.u32 %v8138, 29 (stack73)
        %v8144 = vshrl.u32 %v8138, 3 (stack74)
        %v8145 = vor.u32 %v8143, %v8144 (stack75)
        %v8146 = vxor.u32 %v8141, %v8145 (stack76)
        %v8149 = vadd.s32 %v8141, %v8146 (stack65)
        %v8151 = vshll.u32 %v8146, 16 (stack73)
        %v8152 = vshrl.u32 %v8146, 16 (stack74)
        %v8153 = vor.u32 %v8151, %v8152 (stack75)
        %v8154 = vxor.u32 %v8149, %v8153 (stack76)
        %v8157 = vadd.s32 %v8149, %v8154 (stack65)
        %v8161 = vadd.s32 %v8157, %v8 (stack65)
        %v8163 = vshll.u32 %v8154, 24 (stack73)
        %v8164 = vshrl.u32 %v8154, 8 (stack74)
        %v8165 = vor.u32 %v8163, %v8164 (stack75)
        %v8166 = vxor.u32 %v8157, %v8165 (stack76)
        %v8169 = vadd.s32 %v8166, %v10 (stack65)
        %v8173 = vadd.s32 %v8169, 2 (stack65)
        %v8177 = vadd.s32 %v8161, %v8173 (stack65)
        %v8179 = vshll.u32 %v8173, 13 (stack73)
        %v8180 = vshrl.u32 %v8173, 19 (stack74)
        %v8181 = vor.u32 %v8179, %v8180 (stack75)
        %v8182 = vxor.u32 %v8177, %v8181 (stack76)
        %v8185 = vadd.s32 %v8177, %v8182 (stack65)
        %v8187 = vshll.u32 %v8182, 15 (stack73)
        %v8188 = vshrl.u32 %v8182, 17 (stack74)
        %v8189 = vor.u32 %v8187, %v8188 (stack75)
        %v8190 = vxor.u32 %v8185, %v8189 (stack76)
        %v8193 = vadd.s32 %v8185, %v8190 (stack65)
        %v8195 = vshll.u32 %v8190, 26 (stack73)
        %v8196 = vshrl.u32 %v8190, 6 (stack74)
        %v8197 = vor.u32 %v8195, %v8196 (stack75)
        %v8198 = vxor.u32 %v8193, %v8197 (stack76)
        %v8201 = vadd.s32 %v8193, %v8198 (stack65)
        %v8205 = vadd.s32 %v8201, %v10 (stack65)
        %v8207 = vshll.u32 %v8198, 6 (stack73)
        %v8208 = vshrl.u32 %v8198, 26 (stack74)
        %v8209 = vor.u32 %v8207, %v8208 (stack75)
        %v8210 = vxor.u32 %v8201, %v8209 (stack76)
        %v8213 = vadd.s32 %v8210, %v9 (stack65)
        %v8217 = vadd.s32 %v8213, 3 (stack65)
        %v8221 = vadd.s32 %v8205, %v8217 (stack65)
        %v8223 = vshll.u32 %v8217, 17 (stack73)
        %v8224 = vshrl.u32 %v8217, 15 (stack74)
        %v8225 = vor.u32 %v8223, %v8224 (stack75)
        %v8226 = vxor.u32 %v8221, %v8225 (stack76)
        %v8229 = vadd.s32 %v8221, %v8226 (stack65)
        %v8231 = vshll.u32 %v8226, 29 (stack73)
        %v8232 = vshrl.u32 %v8226, 3 (stack74)
        %v8233 = vor.u32 %v8231, %v8232 (stack75)
        %v8234 = vxor.u32 %v8229, %v8233 (stack76)
        %v8237 = vadd.s32 %v8229, %v8234 (stack65)
        %v8239 = vshll.u32 %v8234, 16 (stack73)
        %v8240 = vshrl.u32 %v8234, 16 (stack74)
        %v8241 = vor.u32 %v8239, %v8240 (stack75)
        %v8242 = vxor.u32 %v8237, %v8241 (stack76)
        %v8245 = vadd.s32 %v8237, %v8242 (stack65)
        %v8249 = vadd.s32 %v8245, %v9 (stack65)
        %v8251 = vshll.u32 %v8242, 24 (stack73)
        %v8252 = vshrl.u32 %v8242, 8 (stack74)
        %v8253 = vor.u32 %v8251, %v8252 (stack75)
        %v8254 = vxor.u32 %v8245, %v8253 (stack76)
        %v8257 = vadd.s32 %v8254, %v8 (stack65)
        %v8261 = vadd.s32 %v8257, 4 (stack65)
        %v8265 = vadd.s32 %v8249, %v8261 (stack65)
        %v8267 = vshll.u32 %v8261, 13 (stack73)
        %v8268 = vshrl.u32 %v8261, 19 (stack74)
        %v8269 = vor.u32 %v8267, %v8268 (stack75)
        %v8270 = vxor.u32 %v8265, %v8269 (stack76)
        %v8273 = vadd.s32 %v8265, %v8270 (stack65)
        %v8275 = vshll.u32 %v8270, 15 (stack73)
        %v8276 = vshrl.u32 %v8270, 17 (stack74)
        %v8277 = vor.u32 %v8275, %v8276 (stack75)
        %v8278 = vxor.u32 %v8273, %v8277 (stack76)
        %v8281 = vadd.s32 %v8273, %v8278 (stack65)
        %v8283 = vshll.u32 %v8278, 26 (stack73)
        %v8284 = vshrl.u32 %v8278, 6 (stack74)
        %v8285 = vor.u32 %v8283, %v8284 (stack75)
        %v8286 = vxor.u32 %v8281, %v8285 (stack76)
        %v8289 = vadd.s32 %v8281, %v8286 (stack65)
        %v8293 = vadd.s32 %v8289, %v8 (stack65)
        %v8295 = vshll.u32 %v8286, 6 (stack73)
        %v8296 = vshrl.u32 %v8286, 26 (stack74)
        %v8297 = vor.u32 %v8295, %v8296 (stack75)
        %v8298 = vxor.u32 %v8289, %v8297 (stack76)
        %v8301 = vadd.s32 %v8298, %v10 (stack65)
        %v8305 = vadd.s32 %v8301, 5 (stack65)
        %v8307 = vxor.u32 %v8293, %v8305 (stack76)
        %v8308 = vand.u32.u8 %v8307, 255 (stack77)
        %v8309 = vand.u32 %v8308, 65535 (stack78)
        %v8310 = vshrl.u32 %v8309, 1 (stack79)
        %v8311 = vor.u32 %v8310, 16256 (stack75)
        %v8312 = vand.u32.u16 %v8311, 65535 (stack80)
        %v8313 = vunpack.i.l.bf16 %v8312 (stack81)
        %v8317 = vadd.f32 %v8313, -1.0 (stack82)
        %v8321 = vmul.f32 %v8317, 2.0 (stack83)
        %v8325 = vadd.f32 %v8321, -0.99609375 (stack82)
        %v8329 = vmax.f32 -0.99609375, %v8325 (stack84)
        %v8331 = vand.u32 2147483647, %v8329 (stack85)
        %vm8334 = vcmp.eq.f32.partialorder %v8331, 1.0 (stack86)
        %v8339 = vmul.f32 %v8329, inf (stack83)
        %v8341 = vxor.u32 %v8329, 2147483648 (stack87)
        %v8344 = vmul.f32 %v8329, %v8341 (stack83)
        %v8346 = vadd.f32 %v8344, 1.0 (stack88)
        %v8347 = vlog2.pop %v8346 (stack89)
        %v8348 = vmul.f32 %v8347, 0.6931472 (stack90)
        %v8349 = vmul.f32 -0.5, %v8344 (stack91)
        %v8350 = vadd.f32 %v8349, 1.0 (stack92)
        %v8351 = vmul.f32 %v8350, %v8344 (stack93)
        %v8352 = vand.u32 2147483647, %v8344 (stack94)
        %vm8353 = vcmp.lt.f32.partialorder %v8352, 0.0004427343 (stack95)
        %v8354 = vsel /*vm=*/%vm8353, /*on_true_vy=*/%v8351, /*on_false_vx=*/%v8348 (stack96)
        %v8355 = vxor.u32 %v8354, 2147483648 (stack87)
        %vm8358 = vcmp.lt.f32.partialorder %v8355, 5.0 (stack86)
        %v8363 = vsel /*vm=*/%vm8358, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v8367 = vsel /*vm=*/%vm8358, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v8371 = vsel /*vm=*/%vm8358, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v8375 = vsel /*vm=*/%vm8358, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v8379 = vsel /*vm=*/%vm8358, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v8383 = vsel /*vm=*/%vm8358, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v8387 = vsel /*vm=*/%vm8358, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v8391 = vsel /*vm=*/%vm8358, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v8395 = vsel /*vm=*/%vm8358, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v8399 = vadd.f32 %v8355, -2.5 (stack82)
        %v8401 = vrsqrt.pop %v8355 (stack97)
        %v8402 = vmul.f32 %v8355, %v8401 (stack98)
        %vm8403 = vcmp.eq.f32.partialorder %v8355, inf (stack99)
        %v8404 = vsel /*vm=*/%vm8403, /*on_true_vy=*/%v8355, /*on_false_vx=*/%v8402 (stack100)
        %vm8405 = vcmp.eq.f32.partialorder %v8355, 0.0 (stack101)
        %v8406 = vand.u32 %v8355, 2147483648 (stack102)
        %v8407 = vsel /*vm=*/%vm8405, /*on_true_vy=*/%v8406, /*on_false_vx=*/%v8404 (stack103)
        %v8410 = vadd.f32 %v8407, -3.0 (stack82)
        %v8414 = vsel /*vm=*/%vm8358, /*on_true_vy=*/%v8399, /*on_false_vx=*/%v8410 (stack72)
        %v8418 = vmul.f32 %v8395, %v8414 (stack83)
        %v8422 = vadd.f32 %v8391, %v8418 (stack82)
        %v8426 = vmul.f32 %v8422, %v8414 (stack83)
        %v8430 = vadd.f32 %v8387, %v8426 (stack82)
        %v8434 = vmul.f32 %v8430, %v8414 (stack83)
        %v8438 = vadd.f32 %v8383, %v8434 (stack82)
        %v8442 = vmul.f32 %v8438, %v8414 (stack83)
        %v8446 = vadd.f32 %v8379, %v8442 (stack82)
        %v8450 = vmul.f32 %v8446, %v8414 (stack83)
        %v8454 = vadd.f32 %v8375, %v8450 (stack82)
        %v8458 = vmul.f32 %v8454, %v8414 (stack83)
        %v8462 = vadd.f32 %v8371, %v8458 (stack82)
        %v8466 = vmul.f32 %v8462, %v8414 (stack83)
        %v8470 = vadd.f32 %v8367, %v8466 (stack82)
        %v8474 = vmul.f32 %v8470, %v8414 (stack83)
        %v8478 = vadd.f32 %v8363, %v8474 (stack82)
        %v8482 = vmul.f32 %v8478, %v8329 (stack83)
        %v8486 = vsel /*vm=*/%vm8334, /*on_true_vy=*/%v8339, /*on_false_vx=*/%v8482 (stack72)
        %v8490 = vmul.f32 %v8486, 1.4140625 (stack83)
        %s8492 = scalar_lea.vmem %s280, 8 [#allocation0] (stack107)
        %v8493 = vpack.c.bf16 0.0, %v8490 (stack104)
        %8494 = vst [vmem:[%s8492] sm:$0xf] /*vst_source=*/%v8493 (stack105)
        %v8497 = vadd.s32 %v894, %v8033 (stack65)
        %s8499 = smul.u32 128, %s27 (stack66)
        %v8500 = vlaneseq (stack67)
        %v8501 = vand.u32 %v8500, 127 (stack68)
        %v8502 = vstv %s8499 (stack69)
        %v8503 = vadd.s32 %v8501, %v8502 (stack70)
        %v8507 = vadd.s32 %v8497, %v8503 (stack65)
        %vm8511 = vcmp.lt.u32.totalorder %v8507, %v8497 (stack71)
        %vm8516 = vcmp.lt.u32.totalorder %v8497, %v894 (stack71)
        %v8521 = vadd.s32 %v881, %v8016 (stack65)
        %v8525 = vadd.s32 %v8521, 1 (stack65)
        %v8529 = vsel /*vm=*/%vm8516, /*on_true_vy=*/%v8525, /*on_false_vx=*/%v8521 (stack72)
        %v8533 = vadd.s32 %v8529, 1 (stack65)
        %v8537 = vsel /*vm=*/%vm8511, /*on_true_vy=*/%v8533, /*on_false_vx=*/%v8529 (stack72)
        %v8542 = vadd.s32 %v8537, %v10 (stack65)
        %v8546 = vadd.s32 %v8507, %v9 (stack65)
        %v8550 = vadd.s32 %v8542, %v8546 (stack65)
        %v8552 = vshll.u32 %v8546, 13 (stack73)
        %v8553 = vshrl.u32 %v8546, 19 (stack74)
        %v8554 = vor.u32 %v8552, %v8553 (stack75)
        %v8555 = vxor.u32 %v8550, %v8554 (stack76)
        %v8558 = vadd.s32 %v8550, %v8555 (stack65)
        %v8560 = vshll.u32 %v8555, 15 (stack73)
        %v8561 = vshrl.u32 %v8555, 17 (stack74)
        %v8562 = vor.u32 %v8560, %v8561 (stack75)
        %v8563 = vxor.u32 %v8558, %v8562 (stack76)
        %v8566 = vadd.s32 %v8558, %v8563 (stack65)
        %v8568 = vshll.u32 %v8563, 26 (stack73)
        %v8569 = vshrl.u32 %v8563, 6 (stack74)
        %v8570 = vor.u32 %v8568, %v8569 (stack75)
        %v8571 = vxor.u32 %v8566, %v8570 (stack76)
        %v8574 = vadd.s32 %v8566, %v8571 (stack65)
        %v8578 = vadd.s32 %v8574, %v9 (stack65)
        %v8580 = vshll.u32 %v8571, 6 (stack73)
        %v8581 = vshrl.u32 %v8571, 26 (stack74)
        %v8582 = vor.u32 %v8580, %v8581 (stack75)
        %v8583 = vxor.u32 %v8574, %v8582 (stack76)
        %v8586 = vadd.s32 %v8583, %v8 (stack65)
        %v8590 = vadd.s32 %v8586, 1 (stack65)
        %v8594 = vadd.s32 %v8578, %v8590 (stack65)
        %v8596 = vshll.u32 %v8590, 17 (stack73)
        %v8597 = vshrl.u32 %v8590, 15 (stack74)
        %v8598 = vor.u32 %v8596, %v8597 (stack75)
        %v8599 = vxor.u32 %v8594, %v8598 (stack76)
        %v8602 = vadd.s32 %v8594, %v8599 (stack65)
        %v8604 = vshll.u32 %v8599, 29 (stack73)
        %v8605 = vshrl.u32 %v8599, 3 (stack74)
        %v8606 = vor.u32 %v8604, %v8605 (stack75)
        %v8607 = vxor.u32 %v8602, %v8606 (stack76)
        %v8610 = vadd.s32 %v8602, %v8607 (stack65)
        %v8612 = vshll.u32 %v8607, 16 (stack73)
        %v8613 = vshrl.u32 %v8607, 16 (stack74)
        %v8614 = vor.u32 %v8612, %v8613 (stack75)
        %v8615 = vxor.u32 %v8610, %v8614 (stack76)
        %v8618 = vadd.s32 %v8610, %v8615 (stack65)
        %v8622 = vadd.s32 %v8618, %v8 (stack65)
        %v8624 = vshll.u32 %v8615, 24 (stack73)
        %v8625 = vshrl.u32 %v8615, 8 (stack74)
        %v8626 = vor.u32 %v8624, %v8625 (stack75)
        %v8627 = vxor.u32 %v8618, %v8626 (stack76)
        %v8630 = vadd.s32 %v8627, %v10 (stack65)
        %v8634 = vadd.s32 %v8630, 2 (stack65)
        %v8638 = vadd.s32 %v8622, %v8634 (stack65)
        %v8640 = vshll.u32 %v8634, 13 (stack73)
        %v8641 = vshrl.u32 %v8634, 19 (stack74)
        %v8642 = vor.u32 %v8640, %v8641 (stack75)
        %v8643 = vxor.u32 %v8638, %v8642 (stack76)
        %v8646 = vadd.s32 %v8638, %v8643 (stack65)
        %v8648 = vshll.u32 %v8643, 15 (stack73)
        %v8649 = vshrl.u32 %v8643, 17 (stack74)
        %v8650 = vor.u32 %v8648, %v8649 (stack75)
        %v8651 = vxor.u32 %v8646, %v8650 (stack76)
        %v8654 = vadd.s32 %v8646, %v8651 (stack65)
        %v8656 = vshll.u32 %v8651, 26 (stack73)
        %v8657 = vshrl.u32 %v8651, 6 (stack74)
        %v8658 = vor.u32 %v8656, %v8657 (stack75)
        %v8659 = vxor.u32 %v8654, %v8658 (stack76)
        %v8662 = vadd.s32 %v8654, %v8659 (stack65)
        %v8666 = vadd.s32 %v8662, %v10 (stack65)
        %v8668 = vshll.u32 %v8659, 6 (stack73)
        %v8669 = vshrl.u32 %v8659, 26 (stack74)
        %v8670 = vor.u32 %v8668, %v8669 (stack75)
        %v8671 = vxor.u32 %v8662, %v8670 (stack76)
        %v8674 = vadd.s32 %v8671, %v9 (stack65)
        %v8678 = vadd.s32 %v8674, 3 (stack65)
        %v8682 = vadd.s32 %v8666, %v8678 (stack65)
        %v8684 = vshll.u32 %v8678, 17 (stack73)
        %v8685 = vshrl.u32 %v8678, 15 (stack74)
        %v8686 = vor.u32 %v8684, %v8685 (stack75)
        %v8687 = vxor.u32 %v8682, %v8686 (stack76)
        %v8690 = vadd.s32 %v8682, %v8687 (stack65)
        %v8692 = vshll.u32 %v8687, 29 (stack73)
        %v8693 = vshrl.u32 %v8687, 3 (stack74)
        %v8694 = vor.u32 %v8692, %v8693 (stack75)
        %v8695 = vxor.u32 %v8690, %v8694 (stack76)
        %v8698 = vadd.s32 %v8690, %v8695 (stack65)
        %v8700 = vshll.u32 %v8695, 16 (stack73)
        %v8701 = vshrl.u32 %v8695, 16 (stack74)
        %v8702 = vor.u32 %v8700, %v8701 (stack75)
        %v8703 = vxor.u32 %v8698, %v8702 (stack76)
        %v8706 = vadd.s32 %v8698, %v8703 (stack65)
        %v8710 = vadd.s32 %v8706, %v9 (stack65)
        %v8712 = vshll.u32 %v8703, 24 (stack73)
        %v8713 = vshrl.u32 %v8703, 8 (stack74)
        %v8714 = vor.u32 %v8712, %v8713 (stack75)
        %v8715 = vxor.u32 %v8706, %v8714 (stack76)
        %v8718 = vadd.s32 %v8715, %v8 (stack65)
        %v8722 = vadd.s32 %v8718, 4 (stack65)
        %v8726 = vadd.s32 %v8710, %v8722 (stack65)
        %v8728 = vshll.u32 %v8722, 13 (stack73)
        %v8729 = vshrl.u32 %v8722, 19 (stack74)
        %v8730 = vor.u32 %v8728, %v8729 (stack75)
        %v8731 = vxor.u32 %v8726, %v8730 (stack76)
        %v8734 = vadd.s32 %v8726, %v8731 (stack65)
        %v8736 = vshll.u32 %v8731, 15 (stack73)
        %v8737 = vshrl.u32 %v8731, 17 (stack74)
        %v8738 = vor.u32 %v8736, %v8737 (stack75)
        %v8739 = vxor.u32 %v8734, %v8738 (stack76)
        %v8742 = vadd.s32 %v8734, %v8739 (stack65)
        %v8744 = vshll.u32 %v8739, 26 (stack73)
        %v8745 = vshrl.u32 %v8739, 6 (stack74)
        %v8746 = vor.u32 %v8744, %v8745 (stack75)
        %v8747 = vxor.u32 %v8742, %v8746 (stack76)
        %v8750 = vadd.s32 %v8742, %v8747 (stack65)
        %v8754 = vadd.s32 %v8750, %v8 (stack65)
        %v8756 = vshll.u32 %v8747, 6 (stack73)
        %v8757 = vshrl.u32 %v8747, 26 (stack74)
        %v8758 = vor.u32 %v8756, %v8757 (stack75)
        %v8759 = vxor.u32 %v8750, %v8758 (stack76)
        %v8762 = vadd.s32 %v8759, %v10 (stack65)
        %v8766 = vadd.s32 %v8762, 5 (stack65)
        %v8768 = vxor.u32 %v8754, %v8766 (stack76)
        %v8769 = vand.u32.u8 %v8768, 255 (stack77)
        %v8770 = vand.u32 %v8769, 65535 (stack78)
        %v8771 = vshrl.u32 %v8770, 1 (stack79)
        %v8772 = vor.u32 %v8771, 16256 (stack75)
        %v8773 = vand.u32.u16 %v8772, 65535 (stack80)
        %v8774 = vunpack.i.l.bf16 %v8773 (stack81)
        %v8778 = vadd.f32 %v8774, -1.0 (stack82)
        %v8782 = vmul.f32 %v8778, 2.0 (stack83)
        %v8786 = vadd.f32 %v8782, -0.99609375 (stack82)
        %v8790 = vmax.f32 -0.99609375, %v8786 (stack84)
        %v8792 = vand.u32 2147483647, %v8790 (stack85)
        %vm8795 = vcmp.eq.f32.partialorder %v8792, 1.0 (stack86)
        %v8800 = vmul.f32 %v8790, inf (stack83)
        %v8802 = vxor.u32 %v8790, 2147483648 (stack87)
        %v8805 = vmul.f32 %v8790, %v8802 (stack83)
        %v8807 = vadd.f32 %v8805, 1.0 (stack88)
        %v8808 = vlog2.pop %v8807 (stack89)
        %v8809 = vmul.f32 %v8808, 0.6931472 (stack90)
        %v8810 = vmul.f32 -0.5, %v8805 (stack91)
        %v8811 = vadd.f32 %v8810, 1.0 (stack92)
        %v8812 = vmul.f32 %v8811, %v8805 (stack93)
        %v8813 = vand.u32 2147483647, %v8805 (stack94)
        %vm8814 = vcmp.lt.f32.partialorder %v8813, 0.0004427343 (stack95)
        %v8815 = vsel /*vm=*/%vm8814, /*on_true_vy=*/%v8812, /*on_false_vx=*/%v8809 (stack96)
        %v8816 = vxor.u32 %v8815, 2147483648 (stack87)
        %vm8819 = vcmp.lt.f32.partialorder %v8816, 5.0 (stack86)
        %v8824 = vsel /*vm=*/%vm8819, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v8828 = vsel /*vm=*/%vm8819, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v8832 = vsel /*vm=*/%vm8819, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v8836 = vsel /*vm=*/%vm8819, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v8840 = vsel /*vm=*/%vm8819, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v8844 = vsel /*vm=*/%vm8819, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v8848 = vsel /*vm=*/%vm8819, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v8852 = vsel /*vm=*/%vm8819, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v8856 = vsel /*vm=*/%vm8819, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v8860 = vadd.f32 %v8816, -2.5 (stack82)
        %v8862 = vrsqrt.pop %v8816 (stack97)
        %v8863 = vmul.f32 %v8816, %v8862 (stack98)
        %vm8864 = vcmp.eq.f32.partialorder %v8816, inf (stack99)
        %v8865 = vsel /*vm=*/%vm8864, /*on_true_vy=*/%v8816, /*on_false_vx=*/%v8863 (stack100)
        %vm8866 = vcmp.eq.f32.partialorder %v8816, 0.0 (stack101)
        %v8867 = vand.u32 %v8816, 2147483648 (stack102)
        %v8868 = vsel /*vm=*/%vm8866, /*on_true_vy=*/%v8867, /*on_false_vx=*/%v8865 (stack103)
        %v8871 = vadd.f32 %v8868, -3.0 (stack82)
        %v8875 = vsel /*vm=*/%vm8819, /*on_true_vy=*/%v8860, /*on_false_vx=*/%v8871 (stack72)
        %v8879 = vmul.f32 %v8856, %v8875 (stack83)
        %v8883 = vadd.f32 %v8852, %v8879 (stack82)
        %v8887 = vmul.f32 %v8883, %v8875 (stack83)
        %v8891 = vadd.f32 %v8848, %v8887 (stack82)
        %v8895 = vmul.f32 %v8891, %v8875 (stack83)
        %v8899 = vadd.f32 %v8844, %v8895 (stack82)
        %v8903 = vmul.f32 %v8899, %v8875 (stack83)
        %v8907 = vadd.f32 %v8840, %v8903 (stack82)
        %v8911 = vmul.f32 %v8907, %v8875 (stack83)
        %v8915 = vadd.f32 %v8836, %v8911 (stack82)
        %v8919 = vmul.f32 %v8915, %v8875 (stack83)
        %v8923 = vadd.f32 %v8832, %v8919 (stack82)
        %v8927 = vmul.f32 %v8923, %v8875 (stack83)
        %v8931 = vadd.f32 %v8828, %v8927 (stack82)
        %v8935 = vmul.f32 %v8931, %v8875 (stack83)
        %v8939 = vadd.f32 %v8824, %v8935 (stack82)
        %v8943 = vmul.f32 %v8939, %v8790 (stack83)
        %v8947 = vsel /*vm=*/%vm8795, /*on_true_vy=*/%v8800, /*on_false_vx=*/%v8943 (stack72)
        %v8951 = vmul.f32 %v8947, 1.4140625 (stack83)
        %s8953 = scalar_lea.vmem %s280, 136 [#allocation0] (stack107)
        %v8954 = vpack.c.bf16 0.0, %v8951 (stack104)
        %8955 = vst [vmem:[%s8953] sm:$0xf] /*vst_source=*/%v8954 (stack105)
        %v8958 = vadd.s32 %v1381, %v8033 (stack65)
        %s8960 = smul.u32 128, %s27 (stack66)
        %v8961 = vlaneseq (stack67)
        %v8962 = vand.u32 %v8961, 127 (stack68)
        %v8963 = vstv %s8960 (stack69)
        %v8964 = vadd.s32 %v8962, %v8963 (stack70)
        %v8968 = vadd.s32 %v8958, %v8964 (stack65)
        %vm8972 = vcmp.lt.u32.totalorder %v8968, %v8958 (stack71)
        %vm8977 = vcmp.lt.u32.totalorder %v8958, %v1381 (stack71)
        %v8982 = vadd.s32 %v1368, %v8016 (stack65)
        %v8986 = vadd.s32 %v8982, 1 (stack65)
        %v8990 = vsel /*vm=*/%vm8977, /*on_true_vy=*/%v8986, /*on_false_vx=*/%v8982 (stack72)
        %v8994 = vadd.s32 %v8990, 1 (stack65)
        %v8998 = vsel /*vm=*/%vm8972, /*on_true_vy=*/%v8994, /*on_false_vx=*/%v8990 (stack72)
        %v9003 = vadd.s32 %v8998, %v10 (stack65)
        %v9007 = vadd.s32 %v8968, %v9 (stack65)
        %v9011 = vadd.s32 %v9003, %v9007 (stack65)
        %v9013 = vshll.u32 %v9007, 13 (stack73)
        %v9014 = vshrl.u32 %v9007, 19 (stack74)
        %v9015 = vor.u32 %v9013, %v9014 (stack75)
        %v9016 = vxor.u32 %v9011, %v9015 (stack76)
        %v9019 = vadd.s32 %v9011, %v9016 (stack65)
        %v9021 = vshll.u32 %v9016, 15 (stack73)
        %v9022 = vshrl.u32 %v9016, 17 (stack74)
        %v9023 = vor.u32 %v9021, %v9022 (stack75)
        %v9024 = vxor.u32 %v9019, %v9023 (stack76)
        %v9027 = vadd.s32 %v9019, %v9024 (stack65)
        %v9029 = vshll.u32 %v9024, 26 (stack73)
        %v9030 = vshrl.u32 %v9024, 6 (stack74)
        %v9031 = vor.u32 %v9029, %v9030 (stack75)
        %v9032 = vxor.u32 %v9027, %v9031 (stack76)
        %v9035 = vadd.s32 %v9027, %v9032 (stack65)
        %v9039 = vadd.s32 %v9035, %v9 (stack65)
        %v9041 = vshll.u32 %v9032, 6 (stack73)
        %v9042 = vshrl.u32 %v9032, 26 (stack74)
        %v9043 = vor.u32 %v9041, %v9042 (stack75)
        %v9044 = vxor.u32 %v9035, %v9043 (stack76)
        %v9047 = vadd.s32 %v9044, %v8 (stack65)
        %v9051 = vadd.s32 %v9047, 1 (stack65)
        %v9055 = vadd.s32 %v9039, %v9051 (stack65)
        %v9057 = vshll.u32 %v9051, 17 (stack73)
        %v9058 = vshrl.u32 %v9051, 15 (stack74)
        %v9059 = vor.u32 %v9057, %v9058 (stack75)
        %v9060 = vxor.u32 %v9055, %v9059 (stack76)
        %v9063 = vadd.s32 %v9055, %v9060 (stack65)
        %v9065 = vshll.u32 %v9060, 29 (stack73)
        %v9066 = vshrl.u32 %v9060, 3 (stack74)
        %v9067 = vor.u32 %v9065, %v9066 (stack75)
        %v9068 = vxor.u32 %v9063, %v9067 (stack76)
        %v9071 = vadd.s32 %v9063, %v9068 (stack65)
        %v9073 = vshll.u32 %v9068, 16 (stack73)
        %v9074 = vshrl.u32 %v9068, 16 (stack74)
        %v9075 = vor.u32 %v9073, %v9074 (stack75)
        %v9076 = vxor.u32 %v9071, %v9075 (stack76)
        %v9079 = vadd.s32 %v9071, %v9076 (stack65)
        %v9083 = vadd.s32 %v9079, %v8 (stack65)
        %v9085 = vshll.u32 %v9076, 24 (stack73)
        %v9086 = vshrl.u32 %v9076, 8 (stack74)
        %v9087 = vor.u32 %v9085, %v9086 (stack75)
        %v9088 = vxor.u32 %v9079, %v9087 (stack76)
        %v9091 = vadd.s32 %v9088, %v10 (stack65)
        %v9095 = vadd.s32 %v9091, 2 (stack65)
        %v9099 = vadd.s32 %v9083, %v9095 (stack65)
        %v9101 = vshll.u32 %v9095, 13 (stack73)
        %v9102 = vshrl.u32 %v9095, 19 (stack74)
        %v9103 = vor.u32 %v9101, %v9102 (stack75)
        %v9104 = vxor.u32 %v9099, %v9103 (stack76)
        %v9107 = vadd.s32 %v9099, %v9104 (stack65)
        %v9109 = vshll.u32 %v9104, 15 (stack73)
        %v9110 = vshrl.u32 %v9104, 17 (stack74)
        %v9111 = vor.u32 %v9109, %v9110 (stack75)
        %v9112 = vxor.u32 %v9107, %v9111 (stack76)
        %v9115 = vadd.s32 %v9107, %v9112 (stack65)
        %v9117 = vshll.u32 %v9112, 26 (stack73)
        %v9118 = vshrl.u32 %v9112, 6 (stack74)
        %v9119 = vor.u32 %v9117, %v9118 (stack75)
        %v9120 = vxor.u32 %v9115, %v9119 (stack76)
        %v9123 = vadd.s32 %v9115, %v9120 (stack65)
        %v9127 = vadd.s32 %v9123, %v10 (stack65)
        %v9129 = vshll.u32 %v9120, 6 (stack73)
        %v9130 = vshrl.u32 %v9120, 26 (stack74)
        %v9131 = vor.u32 %v9129, %v9130 (stack75)
        %v9132 = vxor.u32 %v9123, %v9131 (stack76)
        %v9135 = vadd.s32 %v9132, %v9 (stack65)
        %v9139 = vadd.s32 %v9135, 3 (stack65)
        %v9143 = vadd.s32 %v9127, %v9139 (stack65)
        %v9145 = vshll.u32 %v9139, 17 (stack73)
        %v9146 = vshrl.u32 %v9139, 15 (stack74)
        %v9147 = vor.u32 %v9145, %v9146 (stack75)
        %v9148 = vxor.u32 %v9143, %v9147 (stack76)
        %v9151 = vadd.s32 %v9143, %v9148 (stack65)
        %v9153 = vshll.u32 %v9148, 29 (stack73)
        %v9154 = vshrl.u32 %v9148, 3 (stack74)
        %v9155 = vor.u32 %v9153, %v9154 (stack75)
        %v9156 = vxor.u32 %v9151, %v9155 (stack76)
        %v9159 = vadd.s32 %v9151, %v9156 (stack65)
        %v9161 = vshll.u32 %v9156, 16 (stack73)
        %v9162 = vshrl.u32 %v9156, 16 (stack74)
        %v9163 = vor.u32 %v9161, %v9162 (stack75)
        %v9164 = vxor.u32 %v9159, %v9163 (stack76)
        %v9167 = vadd.s32 %v9159, %v9164 (stack65)
        %v9171 = vadd.s32 %v9167, %v9 (stack65)
        %v9173 = vshll.u32 %v9164, 24 (stack73)
        %v9174 = vshrl.u32 %v9164, 8 (stack74)
        %v9175 = vor.u32 %v9173, %v9174 (stack75)
        %v9176 = vxor.u32 %v9167, %v9175 (stack76)
        %v9179 = vadd.s32 %v9176, %v8 (stack65)
        %v9183 = vadd.s32 %v9179, 4 (stack65)
        %v9187 = vadd.s32 %v9171, %v9183 (stack65)
        %v9189 = vshll.u32 %v9183, 13 (stack73)
        %v9190 = vshrl.u32 %v9183, 19 (stack74)
        %v9191 = vor.u32 %v9189, %v9190 (stack75)
        %v9192 = vxor.u32 %v9187, %v9191 (stack76)
        %v9195 = vadd.s32 %v9187, %v9192 (stack65)
        %v9197 = vshll.u32 %v9192, 15 (stack73)
        %v9198 = vshrl.u32 %v9192, 17 (stack74)
        %v9199 = vor.u32 %v9197, %v9198 (stack75)
        %v9200 = vxor.u32 %v9195, %v9199 (stack76)
        %v9203 = vadd.s32 %v9195, %v9200 (stack65)
        %v9205 = vshll.u32 %v9200, 26 (stack73)
        %v9206 = vshrl.u32 %v9200, 6 (stack74)
        %v9207 = vor.u32 %v9205, %v9206 (stack75)
        %v9208 = vxor.u32 %v9203, %v9207 (stack76)
        %v9211 = vadd.s32 %v9203, %v9208 (stack65)
        %v9215 = vadd.s32 %v9211, %v8 (stack65)
        %v9217 = vshll.u32 %v9208, 6 (stack73)
        %v9218 = vshrl.u32 %v9208, 26 (stack74)
        %v9219 = vor.u32 %v9217, %v9218 (stack75)
        %v9220 = vxor.u32 %v9211, %v9219 (stack76)
        %v9223 = vadd.s32 %v9220, %v10 (stack65)
        %v9227 = vadd.s32 %v9223, 5 (stack65)
        %v9229 = vxor.u32 %v9215, %v9227 (stack76)
        %v9230 = vand.u32.u8 %v9229, 255 (stack77)
        %v9231 = vand.u32 %v9230, 65535 (stack78)
        %v9232 = vshrl.u32 %v9231, 1 (stack79)
        %v9233 = vor.u32 %v9232, 16256 (stack75)
        %v9234 = vand.u32.u16 %v9233, 65535 (stack80)
        %v9235 = vunpack.i.l.bf16 %v9234 (stack81)
        %v9239 = vadd.f32 %v9235, -1.0 (stack82)
        %v9243 = vmul.f32 %v9239, 2.0 (stack83)
        %v9247 = vadd.f32 %v9243, -0.99609375 (stack82)
        %v9251 = vmax.f32 -0.99609375, %v9247 (stack84)
        %v9253 = vand.u32 2147483647, %v9251 (stack85)
        %vm9256 = vcmp.eq.f32.partialorder %v9253, 1.0 (stack86)
        %v9261 = vmul.f32 %v9251, inf (stack83)
        %v9263 = vxor.u32 %v9251, 2147483648 (stack87)
        %v9266 = vmul.f32 %v9251, %v9263 (stack83)
        %v9268 = vadd.f32 %v9266, 1.0 (stack88)
        %v9269 = vlog2.pop %v9268 (stack89)
        %v9270 = vmul.f32 %v9269, 0.6931472 (stack90)
        %v9271 = vmul.f32 -0.5, %v9266 (stack91)
        %v9272 = vadd.f32 %v9271, 1.0 (stack92)
        %v9273 = vmul.f32 %v9272, %v9266 (stack93)
        %v9274 = vand.u32 2147483647, %v9266 (stack94)
        %vm9275 = vcmp.lt.f32.partialorder %v9274, 0.0004427343 (stack95)
        %v9276 = vsel /*vm=*/%vm9275, /*on_true_vy=*/%v9273, /*on_false_vx=*/%v9270 (stack96)
        %v9277 = vxor.u32 %v9276, 2147483648 (stack87)
        %vm9280 = vcmp.lt.f32.partialorder %v9277, 5.0 (stack86)
        %v9285 = vsel /*vm=*/%vm9280, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v9289 = vsel /*vm=*/%vm9280, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v9293 = vsel /*vm=*/%vm9280, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v9297 = vsel /*vm=*/%vm9280, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v9301 = vsel /*vm=*/%vm9280, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v9305 = vsel /*vm=*/%vm9280, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v9309 = vsel /*vm=*/%vm9280, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v9313 = vsel /*vm=*/%vm9280, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v9317 = vsel /*vm=*/%vm9280, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v9321 = vadd.f32 %v9277, -2.5 (stack82)
        %v9323 = vrsqrt.pop %v9277 (stack97)
        %v9324 = vmul.f32 %v9277, %v9323 (stack98)
        %vm9325 = vcmp.eq.f32.partialorder %v9277, inf (stack99)
        %v9326 = vsel /*vm=*/%vm9325, /*on_true_vy=*/%v9277, /*on_false_vx=*/%v9324 (stack100)
        %vm9327 = vcmp.eq.f32.partialorder %v9277, 0.0 (stack101)
        %v9328 = vand.u32 %v9277, 2147483648 (stack102)
        %v9329 = vsel /*vm=*/%vm9327, /*on_true_vy=*/%v9328, /*on_false_vx=*/%v9326 (stack103)
        %v9332 = vadd.f32 %v9329, -3.0 (stack82)
        %v9336 = vsel /*vm=*/%vm9280, /*on_true_vy=*/%v9321, /*on_false_vx=*/%v9332 (stack72)
        %v9340 = vmul.f32 %v9317, %v9336 (stack83)
        %v9344 = vadd.f32 %v9313, %v9340 (stack82)
        %v9348 = vmul.f32 %v9344, %v9336 (stack83)
        %v9352 = vadd.f32 %v9309, %v9348 (stack82)
        %v9356 = vmul.f32 %v9352, %v9336 (stack83)
        %v9360 = vadd.f32 %v9305, %v9356 (stack82)
        %v9364 = vmul.f32 %v9360, %v9336 (stack83)
        %v9368 = vadd.f32 %v9301, %v9364 (stack82)
        %v9372 = vmul.f32 %v9368, %v9336 (stack83)
        %v9376 = vadd.f32 %v9297, %v9372 (stack82)
        %v9380 = vmul.f32 %v9376, %v9336 (stack83)
        %v9384 = vadd.f32 %v9293, %v9380 (stack82)
        %v9388 = vmul.f32 %v9384, %v9336 (stack83)
        %v9392 = vadd.f32 %v9289, %v9388 (stack82)
        %v9396 = vmul.f32 %v9392, %v9336 (stack83)
        %v9400 = vadd.f32 %v9285, %v9396 (stack82)
        %v9404 = vmul.f32 %v9400, %v9251 (stack83)
        %v9408 = vsel /*vm=*/%vm9256, /*on_true_vy=*/%v9261, /*on_false_vx=*/%v9404 (stack72)
        %v9412 = vmul.f32 %v9408, 1.4140625 (stack83)
        %s9414 = scalar_lea.vmem %s280, 264 [#allocation0] (stack107)
        %v9415 = vpack.c.bf16 0.0, %v9412 (stack104)
        %9416 = vst [vmem:[%s9414] sm:$0xf] /*vst_source=*/%v9415 (stack105)
        %v9419 = vadd.s32 %v1868, %v8033 (stack65)
        %s9421 = smul.u32 128, %s27 (stack66)
        %v9422 = vlaneseq (stack67)
        %v9423 = vand.u32 %v9422, 127 (stack68)
        %v9424 = vstv %s9421 (stack69)
        %v9425 = vadd.s32 %v9423, %v9424 (stack70)
        %v9429 = vadd.s32 %v9419, %v9425 (stack65)
        %vm9433 = vcmp.lt.u32.totalorder %v9429, %v9419 (stack71)
        %vm9438 = vcmp.lt.u32.totalorder %v9419, %v1868 (stack71)
        %v9443 = vadd.s32 %v1855, %v8016 (stack65)
        %v9447 = vadd.s32 %v9443, 1 (stack65)
        %v9451 = vsel /*vm=*/%vm9438, /*on_true_vy=*/%v9447, /*on_false_vx=*/%v9443 (stack72)
        %v9455 = vadd.s32 %v9451, 1 (stack65)
        %v9459 = vsel /*vm=*/%vm9433, /*on_true_vy=*/%v9455, /*on_false_vx=*/%v9451 (stack72)
        %v9464 = vadd.s32 %v9459, %v10 (stack65)
        %v9468 = vadd.s32 %v9429, %v9 (stack65)
        %v9472 = vadd.s32 %v9464, %v9468 (stack65)
        %v9474 = vshll.u32 %v9468, 13 (stack73)
        %v9475 = vshrl.u32 %v9468, 19 (stack74)
        %v9476 = vor.u32 %v9474, %v9475 (stack75)
        %v9477 = vxor.u32 %v9472, %v9476 (stack76)
        %v9480 = vadd.s32 %v9472, %v9477 (stack65)
        %v9482 = vshll.u32 %v9477, 15 (stack73)
        %v9483 = vshrl.u32 %v9477, 17 (stack74)
        %v9484 = vor.u32 %v9482, %v9483 (stack75)
        %v9485 = vxor.u32 %v9480, %v9484 (stack76)
        %v9488 = vadd.s32 %v9480, %v9485 (stack65)
        %v9490 = vshll.u32 %v9485, 26 (stack73)
        %v9491 = vshrl.u32 %v9485, 6 (stack74)
        %v9492 = vor.u32 %v9490, %v9491 (stack75)
        %v9493 = vxor.u32 %v9488, %v9492 (stack76)
        %v9496 = vadd.s32 %v9488, %v9493 (stack65)
        %v9500 = vadd.s32 %v9496, %v9 (stack65)
        %v9502 = vshll.u32 %v9493, 6 (stack73)
        %v9503 = vshrl.u32 %v9493, 26 (stack74)
        %v9504 = vor.u32 %v9502, %v9503 (stack75)
        %v9505 = vxor.u32 %v9496, %v9504 (stack76)
        %v9508 = vadd.s32 %v9505, %v8 (stack65)
        %v9512 = vadd.s32 %v9508, 1 (stack65)
        %v9516 = vadd.s32 %v9500, %v9512 (stack65)
        %v9518 = vshll.u32 %v9512, 17 (stack73)
        %v9519 = vshrl.u32 %v9512, 15 (stack74)
        %v9520 = vor.u32 %v9518, %v9519 (stack75)
        %v9521 = vxor.u32 %v9516, %v9520 (stack76)
        %v9524 = vadd.s32 %v9516, %v9521 (stack65)
        %v9526 = vshll.u32 %v9521, 29 (stack73)
        %v9527 = vshrl.u32 %v9521, 3 (stack74)
        %v9528 = vor.u32 %v9526, %v9527 (stack75)
        %v9529 = vxor.u32 %v9524, %v9528 (stack76)
        %v9532 = vadd.s32 %v9524, %v9529 (stack65)
        %v9534 = vshll.u32 %v9529, 16 (stack73)
        %v9535 = vshrl.u32 %v9529, 16 (stack74)
        %v9536 = vor.u32 %v9534, %v9535 (stack75)
        %v9537 = vxor.u32 %v9532, %v9536 (stack76)
        %v9540 = vadd.s32 %v9532, %v9537 (stack65)
        %v9544 = vadd.s32 %v9540, %v8 (stack65)
        %v9546 = vshll.u32 %v9537, 24 (stack73)
        %v9547 = vshrl.u32 %v9537, 8 (stack74)
        %v9548 = vor.u32 %v9546, %v9547 (stack75)
        %v9549 = vxor.u32 %v9540, %v9548 (stack76)
        %v9552 = vadd.s32 %v9549, %v10 (stack65)
        %v9556 = vadd.s32 %v9552, 2 (stack65)
        %v9560 = vadd.s32 %v9544, %v9556 (stack65)
        %v9562 = vshll.u32 %v9556, 13 (stack73)
        %v9563 = vshrl.u32 %v9556, 19 (stack74)
        %v9564 = vor.u32 %v9562, %v9563 (stack75)
        %v9565 = vxor.u32 %v9560, %v9564 (stack76)
        %v9568 = vadd.s32 %v9560, %v9565 (stack65)
        %v9570 = vshll.u32 %v9565, 15 (stack73)
        %v9571 = vshrl.u32 %v9565, 17 (stack74)
        %v9572 = vor.u32 %v9570, %v9571 (stack75)
        %v9573 = vxor.u32 %v9568, %v9572 (stack76)
        %v9576 = vadd.s32 %v9568, %v9573 (stack65)
        %v9578 = vshll.u32 %v9573, 26 (stack73)
        %v9579 = vshrl.u32 %v9573, 6 (stack74)
        %v9580 = vor.u32 %v9578, %v9579 (stack75)
        %v9581 = vxor.u32 %v9576, %v9580 (stack76)
        %v9584 = vadd.s32 %v9576, %v9581 (stack65)
        %v9588 = vadd.s32 %v9584, %v10 (stack65)
        %v9590 = vshll.u32 %v9581, 6 (stack73)
        %v9591 = vshrl.u32 %v9581, 26 (stack74)
        %v9592 = vor.u32 %v9590, %v9591 (stack75)
        %v9593 = vxor.u32 %v9584, %v9592 (stack76)
        %v9596 = vadd.s32 %v9593, %v9 (stack65)
        %v9600 = vadd.s32 %v9596, 3 (stack65)
        %v9604 = vadd.s32 %v9588, %v9600 (stack65)
        %v9606 = vshll.u32 %v9600, 17 (stack73)
        %v9607 = vshrl.u32 %v9600, 15 (stack74)
        %v9608 = vor.u32 %v9606, %v9607 (stack75)
        %v9609 = vxor.u32 %v9604, %v9608 (stack76)
        %v9612 = vadd.s32 %v9604, %v9609 (stack65)
        %v9614 = vshll.u32 %v9609, 29 (stack73)
        %v9615 = vshrl.u32 %v9609, 3 (stack74)
        %v9616 = vor.u32 %v9614, %v9615 (stack75)
        %v9617 = vxor.u32 %v9612, %v9616 (stack76)
        %v9620 = vadd.s32 %v9612, %v9617 (stack65)
        %v9622 = vshll.u32 %v9617, 16 (stack73)
        %v9623 = vshrl.u32 %v9617, 16 (stack74)
        %v9624 = vor.u32 %v9622, %v9623 (stack75)
        %v9625 = vxor.u32 %v9620, %v9624 (stack76)
        %v9628 = vadd.s32 %v9620, %v9625 (stack65)
        %v9632 = vadd.s32 %v9628, %v9 (stack65)
        %v9634 = vshll.u32 %v9625, 24 (stack73)
        %v9635 = vshrl.u32 %v9625, 8 (stack74)
        %v9636 = vor.u32 %v9634, %v9635 (stack75)
        %v9637 = vxor.u32 %v9628, %v9636 (stack76)
        %v9640 = vadd.s32 %v9637, %v8 (stack65)
        %v9644 = vadd.s32 %v9640, 4 (stack65)
        %v9648 = vadd.s32 %v9632, %v9644 (stack65)
        %v9650 = vshll.u32 %v9644, 13 (stack73)
        %v9651 = vshrl.u32 %v9644, 19 (stack74)
        %v9652 = vor.u32 %v9650, %v9651 (stack75)
        %v9653 = vxor.u32 %v9648, %v9652 (stack76)
        %v9656 = vadd.s32 %v9648, %v9653 (stack65)
        %v9658 = vshll.u32 %v9653, 15 (stack73)
        %v9659 = vshrl.u32 %v9653, 17 (stack74)
        %v9660 = vor.u32 %v9658, %v9659 (stack75)
        %v9661 = vxor.u32 %v9656, %v9660 (stack76)
        %v9664 = vadd.s32 %v9656, %v9661 (stack65)
        %v9666 = vshll.u32 %v9661, 26 (stack73)
        %v9667 = vshrl.u32 %v9661, 6 (stack74)
        %v9668 = vor.u32 %v9666, %v9667 (stack75)
        %v9669 = vxor.u32 %v9664, %v9668 (stack76)
        %v9672 = vadd.s32 %v9664, %v9669 (stack65)
        %v9676 = vadd.s32 %v9672, %v8 (stack65)
        %v9678 = vshll.u32 %v9669, 6 (stack73)
        %v9679 = vshrl.u32 %v9669, 26 (stack74)
        %v9680 = vor.u32 %v9678, %v9679 (stack75)
        %v9681 = vxor.u32 %v9672, %v9680 (stack76)
        %v9684 = vadd.s32 %v9681, %v10 (stack65)
        %v9688 = vadd.s32 %v9684, 5 (stack65)
        %v9690 = vxor.u32 %v9676, %v9688 (stack76)
        %v9691 = vand.u32.u8 %v9690, 255 (stack77)
        %v9692 = vand.u32 %v9691, 65535 (stack78)
        %v9693 = vshrl.u32 %v9692, 1 (stack79)
        %v9694 = vor.u32 %v9693, 16256 (stack75)
        %v9695 = vand.u32.u16 %v9694, 65535 (stack80)
        %v9696 = vunpack.i.l.bf16 %v9695 (stack81)
        %v9700 = vadd.f32 %v9696, -1.0 (stack82)
        %v9704 = vmul.f32 %v9700, 2.0 (stack83)
        %v9708 = vadd.f32 %v9704, -0.99609375 (stack82)
        %v9712 = vmax.f32 -0.99609375, %v9708 (stack84)
        %v9714 = vand.u32 2147483647, %v9712 (stack85)
        %vm9717 = vcmp.eq.f32.partialorder %v9714, 1.0 (stack86)
        %v9722 = vmul.f32 %v9712, inf (stack83)
        %v9724 = vxor.u32 %v9712, 2147483648 (stack87)
        %v9727 = vmul.f32 %v9712, %v9724 (stack83)
        %v9729 = vadd.f32 %v9727, 1.0 (stack88)
        %v9730 = vlog2.pop %v9729 (stack89)
        %v9731 = vmul.f32 %v9730, 0.6931472 (stack90)
        %v9732 = vmul.f32 -0.5, %v9727 (stack91)
        %v9733 = vadd.f32 %v9732, 1.0 (stack92)
        %v9734 = vmul.f32 %v9733, %v9727 (stack93)
        %v9735 = vand.u32 2147483647, %v9727 (stack94)
        %vm9736 = vcmp.lt.f32.partialorder %v9735, 0.0004427343 (stack95)
        %v9737 = vsel /*vm=*/%vm9736, /*on_true_vy=*/%v9734, /*on_false_vx=*/%v9731 (stack96)
        %v9738 = vxor.u32 %v9737, 2147483648 (stack87)
        %vm9741 = vcmp.lt.f32.partialorder %v9738, 5.0 (stack86)
        %v9746 = vsel /*vm=*/%vm9741, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v9750 = vsel /*vm=*/%vm9741, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v9754 = vsel /*vm=*/%vm9741, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v9758 = vsel /*vm=*/%vm9741, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v9762 = vsel /*vm=*/%vm9741, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v9766 = vsel /*vm=*/%vm9741, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v9770 = vsel /*vm=*/%vm9741, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v9774 = vsel /*vm=*/%vm9741, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v9778 = vsel /*vm=*/%vm9741, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v9782 = vadd.f32 %v9738, -2.5 (stack82)
        %v9784 = vrsqrt.pop %v9738 (stack97)
        %v9785 = vmul.f32 %v9738, %v9784 (stack98)
        %vm9786 = vcmp.eq.f32.partialorder %v9738, inf (stack99)
        %v9787 = vsel /*vm=*/%vm9786, /*on_true_vy=*/%v9738, /*on_false_vx=*/%v9785 (stack100)
        %vm9788 = vcmp.eq.f32.partialorder %v9738, 0.0 (stack101)
        %v9789 = vand.u32 %v9738, 2147483648 (stack102)
        %v9790 = vsel /*vm=*/%vm9788, /*on_true_vy=*/%v9789, /*on_false_vx=*/%v9787 (stack103)
        %v9793 = vadd.f32 %v9790, -3.0 (stack82)
        %v9797 = vsel /*vm=*/%vm9741, /*on_true_vy=*/%v9782, /*on_false_vx=*/%v9793 (stack72)
        %v9801 = vmul.f32 %v9778, %v9797 (stack83)
        %v9805 = vadd.f32 %v9774, %v9801 (stack82)
        %v9809 = vmul.f32 %v9805, %v9797 (stack83)
        %v9813 = vadd.f32 %v9770, %v9809 (stack82)
        %v9817 = vmul.f32 %v9813, %v9797 (stack83)
        %v9821 = vadd.f32 %v9766, %v9817 (stack82)
        %v9825 = vmul.f32 %v9821, %v9797 (stack83)
        %v9829 = vadd.f32 %v9762, %v9825 (stack82)
        %v9833 = vmul.f32 %v9829, %v9797 (stack83)
        %v9837 = vadd.f32 %v9758, %v9833 (stack82)
        %v9841 = vmul.f32 %v9837, %v9797 (stack83)
        %v9845 = vadd.f32 %v9754, %v9841 (stack82)
        %v9849 = vmul.f32 %v9845, %v9797 (stack83)
        %v9853 = vadd.f32 %v9750, %v9849 (stack82)
        %v9857 = vmul.f32 %v9853, %v9797 (stack83)
        %v9861 = vadd.f32 %v9746, %v9857 (stack82)
        %v9865 = vmul.f32 %v9861, %v9712 (stack83)
        %v9869 = vsel /*vm=*/%vm9717, /*on_true_vy=*/%v9722, /*on_false_vx=*/%v9865 (stack72)
        %v9873 = vmul.f32 %v9869, 1.4140625 (stack83)
        %s9875 = scalar_lea.vmem %s280, 392 [#allocation0] (stack107)
        %v9876 = vpack.c.bf16 0.0, %v9873 (stack104)
        %9877 = vst [vmem:[%s9875] sm:$0xf] /*vst_source=*/%v9876 (stack105)
        %v9880 = vadd.s32 %v2355, %v8033 (stack65)
        %s9882 = smul.u32 128, %s27 (stack66)
        %v9883 = vlaneseq (stack67)
        %v9884 = vand.u32 %v9883, 127 (stack68)
        %v9885 = vstv %s9882 (stack69)
        %v9886 = vadd.s32 %v9884, %v9885 (stack70)
        %v9890 = vadd.s32 %v9880, %v9886 (stack65)
        %vm9894 = vcmp.lt.u32.totalorder %v9890, %v9880 (stack71)
        %vm9899 = vcmp.lt.u32.totalorder %v9880, %v2355 (stack71)
        %v9904 = vadd.s32 %v2342, %v8016 (stack65)
        %v9908 = vadd.s32 %v9904, 1 (stack65)
        %v9912 = vsel /*vm=*/%vm9899, /*on_true_vy=*/%v9908, /*on_false_vx=*/%v9904 (stack72)
        %v9916 = vadd.s32 %v9912, 1 (stack65)
        %v9920 = vsel /*vm=*/%vm9894, /*on_true_vy=*/%v9916, /*on_false_vx=*/%v9912 (stack72)
        %v9925 = vadd.s32 %v9920, %v10 (stack65)
        %v9929 = vadd.s32 %v9890, %v9 (stack65)
        %v9933 = vadd.s32 %v9925, %v9929 (stack65)
        %v9935 = vshll.u32 %v9929, 13 (stack73)
        %v9936 = vshrl.u32 %v9929, 19 (stack74)
        %v9937 = vor.u32 %v9935, %v9936 (stack75)
        %v9938 = vxor.u32 %v9933, %v9937 (stack76)
        %v9941 = vadd.s32 %v9933, %v9938 (stack65)
        %v9943 = vshll.u32 %v9938, 15 (stack73)
        %v9944 = vshrl.u32 %v9938, 17 (stack74)
        %v9945 = vor.u32 %v9943, %v9944 (stack75)
        %v9946 = vxor.u32 %v9941, %v9945 (stack76)
        %v9949 = vadd.s32 %v9941, %v9946 (stack65)
        %v9951 = vshll.u32 %v9946, 26 (stack73)
        %v9952 = vshrl.u32 %v9946, 6 (stack74)
        %v9953 = vor.u32 %v9951, %v9952 (stack75)
        %v9954 = vxor.u32 %v9949, %v9953 (stack76)
        %v9957 = vadd.s32 %v9949, %v9954 (stack65)
        %v9961 = vadd.s32 %v9957, %v9 (stack65)
        %v9963 = vshll.u32 %v9954, 6 (stack73)
        %v9964 = vshrl.u32 %v9954, 26 (stack74)
        %v9965 = vor.u32 %v9963, %v9964 (stack75)
        %v9966 = vxor.u32 %v9957, %v9965 (stack76)
        %v9969 = vadd.s32 %v9966, %v8 (stack65)
        %v9973 = vadd.s32 %v9969, 1 (stack65)
        %v9977 = vadd.s32 %v9961, %v9973 (stack65)
        %v9979 = vshll.u32 %v9973, 17 (stack73)
        %v9980 = vshrl.u32 %v9973, 15 (stack74)
        %v9981 = vor.u32 %v9979, %v9980 (stack75)
        %v9982 = vxor.u32 %v9977, %v9981 (stack76)
        %v9985 = vadd.s32 %v9977, %v9982 (stack65)
        %v9987 = vshll.u32 %v9982, 29 (stack73)
        %v9988 = vshrl.u32 %v9982, 3 (stack74)
        %v9989 = vor.u32 %v9987, %v9988 (stack75)
        %v9990 = vxor.u32 %v9985, %v9989 (stack76)
        %v9993 = vadd.s32 %v9985, %v9990 (stack65)
        %v9995 = vshll.u32 %v9990, 16 (stack73)
        %v9996 = vshrl.u32 %v9990, 16 (stack74)
        %v9997 = vor.u32 %v9995, %v9996 (stack75)
        %v9998 = vxor.u32 %v9993, %v9997 (stack76)
        %v10001 = vadd.s32 %v9993, %v9998 (stack65)
        %v10005 = vadd.s32 %v10001, %v8 (stack65)
        %v10007 = vshll.u32 %v9998, 24 (stack73)
        %v10008 = vshrl.u32 %v9998, 8 (stack74)
        %v10009 = vor.u32 %v10007, %v10008 (stack75)
        %v10010 = vxor.u32 %v10001, %v10009 (stack76)
        %v10013 = vadd.s32 %v10010, %v10 (stack65)
        %v10017 = vadd.s32 %v10013, 2 (stack65)
        %v10021 = vadd.s32 %v10005, %v10017 (stack65)
        %v10023 = vshll.u32 %v10017, 13 (stack73)
        %v10024 = vshrl.u32 %v10017, 19 (stack74)
        %v10025 = vor.u32 %v10023, %v10024 (stack75)
        %v10026 = vxor.u32 %v10021, %v10025 (stack76)
        %v10029 = vadd.s32 %v10021, %v10026 (stack65)
        %v10031 = vshll.u32 %v10026, 15 (stack73)
        %v10032 = vshrl.u32 %v10026, 17 (stack74)
        %v10033 = vor.u32 %v10031, %v10032 (stack75)
        %v10034 = vxor.u32 %v10029, %v10033 (stack76)
        %v10037 = vadd.s32 %v10029, %v10034 (stack65)
        %v10039 = vshll.u32 %v10034, 26 (stack73)
        %v10040 = vshrl.u32 %v10034, 6 (stack74)
        %v10041 = vor.u32 %v10039, %v10040 (stack75)
        %v10042 = vxor.u32 %v10037, %v10041 (stack76)
        %v10045 = vadd.s32 %v10037, %v10042 (stack65)
        %v10049 = vadd.s32 %v10045, %v10 (stack65)
        %v10051 = vshll.u32 %v10042, 6 (stack73)
        %v10052 = vshrl.u32 %v10042, 26 (stack74)
        %v10053 = vor.u32 %v10051, %v10052 (stack75)
        %v10054 = vxor.u32 %v10045, %v10053 (stack76)
        %v10057 = vadd.s32 %v10054, %v9 (stack65)
        %v10061 = vadd.s32 %v10057, 3 (stack65)
        %v10065 = vadd.s32 %v10049, %v10061 (stack65)
        %v10067 = vshll.u32 %v10061, 17 (stack73)
        %v10068 = vshrl.u32 %v10061, 15 (stack74)
        %v10069 = vor.u32 %v10067, %v10068 (stack75)
        %v10070 = vxor.u32 %v10065, %v10069 (stack76)
        %v10073 = vadd.s32 %v10065, %v10070 (stack65)
        %v10075 = vshll.u32 %v10070, 29 (stack73)
        %v10076 = vshrl.u32 %v10070, 3 (stack74)
        %v10077 = vor.u32 %v10075, %v10076 (stack75)
        %v10078 = vxor.u32 %v10073, %v10077 (stack76)
        %v10081 = vadd.s32 %v10073, %v10078 (stack65)
        %v10083 = vshll.u32 %v10078, 16 (stack73)
        %v10084 = vshrl.u32 %v10078, 16 (stack74)
        %v10085 = vor.u32 %v10083, %v10084 (stack75)
        %v10086 = vxor.u32 %v10081, %v10085 (stack76)
        %v10089 = vadd.s32 %v10081, %v10086 (stack65)
        %v10093 = vadd.s32 %v10089, %v9 (stack65)
        %v10095 = vshll.u32 %v10086, 24 (stack73)
        %v10096 = vshrl.u32 %v10086, 8 (stack74)
        %v10097 = vor.u32 %v10095, %v10096 (stack75)
        %v10098 = vxor.u32 %v10089, %v10097 (stack76)
        %v10101 = vadd.s32 %v10098, %v8 (stack65)
        %v10105 = vadd.s32 %v10101, 4 (stack65)
        %v10109 = vadd.s32 %v10093, %v10105 (stack65)
        %v10111 = vshll.u32 %v10105, 13 (stack73)
        %v10112 = vshrl.u32 %v10105, 19 (stack74)
        %v10113 = vor.u32 %v10111, %v10112 (stack75)
        %v10114 = vxor.u32 %v10109, %v10113 (stack76)
        %v10117 = vadd.s32 %v10109, %v10114 (stack65)
        %v10119 = vshll.u32 %v10114, 15 (stack73)
        %v10120 = vshrl.u32 %v10114, 17 (stack74)
        %v10121 = vor.u32 %v10119, %v10120 (stack75)
        %v10122 = vxor.u32 %v10117, %v10121 (stack76)
        %v10125 = vadd.s32 %v10117, %v10122 (stack65)
        %v10127 = vshll.u32 %v10122, 26 (stack73)
        %v10128 = vshrl.u32 %v10122, 6 (stack74)
        %v10129 = vor.u32 %v10127, %v10128 (stack75)
        %v10130 = vxor.u32 %v10125, %v10129 (stack76)
        %v10133 = vadd.s32 %v10125, %v10130 (stack65)
        %v10137 = vadd.s32 %v10133, %v8 (stack65)
        %v10139 = vshll.u32 %v10130, 6 (stack73)
        %v10140 = vshrl.u32 %v10130, 26 (stack74)
        %v10141 = vor.u32 %v10139, %v10140 (stack75)
        %v10142 = vxor.u32 %v10133, %v10141 (stack76)
        %v10145 = vadd.s32 %v10142, %v10 (stack65)
        %v10149 = vadd.s32 %v10145, 5 (stack65)
        %v10151 = vxor.u32 %v10137, %v10149 (stack76)
        %v10152 = vand.u32.u8 %v10151, 255 (stack77)
        %v10153 = vand.u32 %v10152, 65535 (stack78)
        %v10154 = vshrl.u32 %v10153, 1 (stack79)
        %v10155 = vor.u32 %v10154, 16256 (stack75)
        %v10156 = vand.u32.u16 %v10155, 65535 (stack80)
        %v10157 = vunpack.i.l.bf16 %v10156 (stack81)
        %v10161 = vadd.f32 %v10157, -1.0 (stack82)
        %v10165 = vmul.f32 %v10161, 2.0 (stack83)
        %v10169 = vadd.f32 %v10165, -0.99609375 (stack82)
        %v10173 = vmax.f32 -0.99609375, %v10169 (stack84)
        %v10175 = vand.u32 2147483647, %v10173 (stack85)
        %vm10178 = vcmp.eq.f32.partialorder %v10175, 1.0 (stack86)
        %v10183 = vmul.f32 %v10173, inf (stack83)
        %v10185 = vxor.u32 %v10173, 2147483648 (stack87)
        %v10188 = vmul.f32 %v10173, %v10185 (stack83)
        %v10190 = vadd.f32 %v10188, 1.0 (stack88)
        %v10191 = vlog2.pop %v10190 (stack89)
        %v10192 = vmul.f32 %v10191, 0.6931472 (stack90)
        %v10193 = vmul.f32 -0.5, %v10188 (stack91)
        %v10194 = vadd.f32 %v10193, 1.0 (stack92)
        %v10195 = vmul.f32 %v10194, %v10188 (stack93)
        %v10196 = vand.u32 2147483647, %v10188 (stack94)
        %vm10197 = vcmp.lt.f32.partialorder %v10196, 0.0004427343 (stack95)
        %v10198 = vsel /*vm=*/%vm10197, /*on_true_vy=*/%v10195, /*on_false_vx=*/%v10192 (stack96)
        %v10199 = vxor.u32 %v10198, 2147483648 (stack87)
        %vm10202 = vcmp.lt.f32.partialorder %v10199, 5.0 (stack86)
        %v10207 = vsel /*vm=*/%vm10202, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v10211 = vsel /*vm=*/%vm10202, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v10215 = vsel /*vm=*/%vm10202, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v10219 = vsel /*vm=*/%vm10202, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v10223 = vsel /*vm=*/%vm10202, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v10227 = vsel /*vm=*/%vm10202, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v10231 = vsel /*vm=*/%vm10202, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v10235 = vsel /*vm=*/%vm10202, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v10239 = vsel /*vm=*/%vm10202, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v10243 = vadd.f32 %v10199, -2.5 (stack82)
        %v10245 = vrsqrt.pop %v10199 (stack97)
        %v10246 = vmul.f32 %v10199, %v10245 (stack98)
        %vm10247 = vcmp.eq.f32.partialorder %v10199, inf (stack99)
        %v10248 = vsel /*vm=*/%vm10247, /*on_true_vy=*/%v10199, /*on_false_vx=*/%v10246 (stack100)
        %vm10249 = vcmp.eq.f32.partialorder %v10199, 0.0 (stack101)
        %v10250 = vand.u32 %v10199, 2147483648 (stack102)
        %v10251 = vsel /*vm=*/%vm10249, /*on_true_vy=*/%v10250, /*on_false_vx=*/%v10248 (stack103)
        %v10254 = vadd.f32 %v10251, -3.0 (stack82)
        %v10258 = vsel /*vm=*/%vm10202, /*on_true_vy=*/%v10243, /*on_false_vx=*/%v10254 (stack72)
        %v10262 = vmul.f32 %v10239, %v10258 (stack83)
        %v10266 = vadd.f32 %v10235, %v10262 (stack82)
        %v10270 = vmul.f32 %v10266, %v10258 (stack83)
        %v10274 = vadd.f32 %v10231, %v10270 (stack82)
        %v10278 = vmul.f32 %v10274, %v10258 (stack83)
        %v10282 = vadd.f32 %v10227, %v10278 (stack82)
        %v10286 = vmul.f32 %v10282, %v10258 (stack83)
        %v10290 = vadd.f32 %v10223, %v10286 (stack82)
        %v10294 = vmul.f32 %v10290, %v10258 (stack83)
        %v10298 = vadd.f32 %v10219, %v10294 (stack82)
        %v10302 = vmul.f32 %v10298, %v10258 (stack83)
        %v10306 = vadd.f32 %v10215, %v10302 (stack82)
        %v10310 = vmul.f32 %v10306, %v10258 (stack83)
        %v10314 = vadd.f32 %v10211, %v10310 (stack82)
        %v10318 = vmul.f32 %v10314, %v10258 (stack83)
        %v10322 = vadd.f32 %v10207, %v10318 (stack82)
        %v10326 = vmul.f32 %v10322, %v10173 (stack83)
        %v10330 = vsel /*vm=*/%vm10178, /*on_true_vy=*/%v10183, /*on_false_vx=*/%v10326 (stack72)
        %v10334 = vmul.f32 %v10330, 1.4140625 (stack83)
        %s10336 = scalar_lea.vmem %s280, 520 [#allocation0] (stack107)
        %v10337 = vpack.c.bf16 0.0, %v10334 (stack104)
        %10338 = vst [vmem:[%s10336] sm:$0xf] /*vst_source=*/%v10337 (stack105)
        %v10341 = vadd.s32 %v2842, %v8033 (stack65)
        %s10343 = smul.u32 128, %s27 (stack66)
        %v10344 = vlaneseq (stack67)
        %v10345 = vand.u32 %v10344, 127 (stack68)
        %v10346 = vstv %s10343 (stack69)
        %v10347 = vadd.s32 %v10345, %v10346 (stack70)
        %v10351 = vadd.s32 %v10341, %v10347 (stack65)
        %vm10355 = vcmp.lt.u32.totalorder %v10351, %v10341 (stack71)
        %vm10360 = vcmp.lt.u32.totalorder %v10341, %v2842 (stack71)
        %v10365 = vadd.s32 %v2829, %v8016 (stack65)
        %v10369 = vadd.s32 %v10365, 1 (stack65)
        %v10373 = vsel /*vm=*/%vm10360, /*on_true_vy=*/%v10369, /*on_false_vx=*/%v10365 (stack72)
        %v10377 = vadd.s32 %v10373, 1 (stack65)
        %v10381 = vsel /*vm=*/%vm10355, /*on_true_vy=*/%v10377, /*on_false_vx=*/%v10373 (stack72)
        %v10386 = vadd.s32 %v10381, %v10 (stack65)
        %v10390 = vadd.s32 %v10351, %v9 (stack65)
        %v10394 = vadd.s32 %v10386, %v10390 (stack65)
        %v10396 = vshll.u32 %v10390, 13 (stack73)
        %v10397 = vshrl.u32 %v10390, 19 (stack74)
        %v10398 = vor.u32 %v10396, %v10397 (stack75)
        %v10399 = vxor.u32 %v10394, %v10398 (stack76)
        %v10402 = vadd.s32 %v10394, %v10399 (stack65)
        %v10404 = vshll.u32 %v10399, 15 (stack73)
        %v10405 = vshrl.u32 %v10399, 17 (stack74)
        %v10406 = vor.u32 %v10404, %v10405 (stack75)
        %v10407 = vxor.u32 %v10402, %v10406 (stack76)
        %v10410 = vadd.s32 %v10402, %v10407 (stack65)
        %v10412 = vshll.u32 %v10407, 26 (stack73)
        %v10413 = vshrl.u32 %v10407, 6 (stack74)
        %v10414 = vor.u32 %v10412, %v10413 (stack75)
        %v10415 = vxor.u32 %v10410, %v10414 (stack76)
        %v10418 = vadd.s32 %v10410, %v10415 (stack65)
        %v10422 = vadd.s32 %v10418, %v9 (stack65)
        %v10424 = vshll.u32 %v10415, 6 (stack73)
        %v10425 = vshrl.u32 %v10415, 26 (stack74)
        %v10426 = vor.u32 %v10424, %v10425 (stack75)
        %v10427 = vxor.u32 %v10418, %v10426 (stack76)
        %v10430 = vadd.s32 %v10427, %v8 (stack65)
        %v10434 = vadd.s32 %v10430, 1 (stack65)
        %v10438 = vadd.s32 %v10422, %v10434 (stack65)
        %v10440 = vshll.u32 %v10434, 17 (stack73)
        %v10441 = vshrl.u32 %v10434, 15 (stack74)
        %v10442 = vor.u32 %v10440, %v10441 (stack75)
        %v10443 = vxor.u32 %v10438, %v10442 (stack76)
        %v10446 = vadd.s32 %v10438, %v10443 (stack65)
        %v10448 = vshll.u32 %v10443, 29 (stack73)
        %v10449 = vshrl.u32 %v10443, 3 (stack74)
        %v10450 = vor.u32 %v10448, %v10449 (stack75)
        %v10451 = vxor.u32 %v10446, %v10450 (stack76)
        %v10454 = vadd.s32 %v10446, %v10451 (stack65)
        %v10456 = vshll.u32 %v10451, 16 (stack73)
        %v10457 = vshrl.u32 %v10451, 16 (stack74)
        %v10458 = vor.u32 %v10456, %v10457 (stack75)
        %v10459 = vxor.u32 %v10454, %v10458 (stack76)
        %v10462 = vadd.s32 %v10454, %v10459 (stack65)
        %v10466 = vadd.s32 %v10462, %v8 (stack65)
        %v10468 = vshll.u32 %v10459, 24 (stack73)
        %v10469 = vshrl.u32 %v10459, 8 (stack74)
        %v10470 = vor.u32 %v10468, %v10469 (stack75)
        %v10471 = vxor.u32 %v10462, %v10470 (stack76)
        %v10474 = vadd.s32 %v10471, %v10 (stack65)
        %v10478 = vadd.s32 %v10474, 2 (stack65)
        %v10482 = vadd.s32 %v10466, %v10478 (stack65)
        %v10484 = vshll.u32 %v10478, 13 (stack73)
        %v10485 = vshrl.u32 %v10478, 19 (stack74)
        %v10486 = vor.u32 %v10484, %v10485 (stack75)
        %v10487 = vxor.u32 %v10482, %v10486 (stack76)
        %v10490 = vadd.s32 %v10482, %v10487 (stack65)
        %v10492 = vshll.u32 %v10487, 15 (stack73)
        %v10493 = vshrl.u32 %v10487, 17 (stack74)
        %v10494 = vor.u32 %v10492, %v10493 (stack75)
        %v10495 = vxor.u32 %v10490, %v10494 (stack76)
        %v10498 = vadd.s32 %v10490, %v10495 (stack65)
        %v10500 = vshll.u32 %v10495, 26 (stack73)
        %v10501 = vshrl.u32 %v10495, 6 (stack74)
        %v10502 = vor.u32 %v10500, %v10501 (stack75)
        %v10503 = vxor.u32 %v10498, %v10502 (stack76)
        %v10506 = vadd.s32 %v10498, %v10503 (stack65)
        %v10510 = vadd.s32 %v10506, %v10 (stack65)
        %v10512 = vshll.u32 %v10503, 6 (stack73)
        %v10513 = vshrl.u32 %v10503, 26 (stack74)
        %v10514 = vor.u32 %v10512, %v10513 (stack75)
        %v10515 = vxor.u32 %v10506, %v10514 (stack76)
        %v10518 = vadd.s32 %v10515, %v9 (stack65)
        %v10522 = vadd.s32 %v10518, 3 (stack65)
        %v10526 = vadd.s32 %v10510, %v10522 (stack65)
        %v10528 = vshll.u32 %v10522, 17 (stack73)
        %v10529 = vshrl.u32 %v10522, 15 (stack74)
        %v10530 = vor.u32 %v10528, %v10529 (stack75)
        %v10531 = vxor.u32 %v10526, %v10530 (stack76)
        %v10534 = vadd.s32 %v10526, %v10531 (stack65)
        %v10536 = vshll.u32 %v10531, 29 (stack73)
        %v10537 = vshrl.u32 %v10531, 3 (stack74)
        %v10538 = vor.u32 %v10536, %v10537 (stack75)
        %v10539 = vxor.u32 %v10534, %v10538 (stack76)
        %v10542 = vadd.s32 %v10534, %v10539 (stack65)
        %v10544 = vshll.u32 %v10539, 16 (stack73)
        %v10545 = vshrl.u32 %v10539, 16 (stack74)
        %v10546 = vor.u32 %v10544, %v10545 (stack75)
        %v10547 = vxor.u32 %v10542, %v10546 (stack76)
        %v10550 = vadd.s32 %v10542, %v10547 (stack65)
        %v10554 = vadd.s32 %v10550, %v9 (stack65)
        %v10556 = vshll.u32 %v10547, 24 (stack73)
        %v10557 = vshrl.u32 %v10547, 8 (stack74)
        %v10558 = vor.u32 %v10556, %v10557 (stack75)
        %v10559 = vxor.u32 %v10550, %v10558 (stack76)
        %v10562 = vadd.s32 %v10559, %v8 (stack65)
        %v10566 = vadd.s32 %v10562, 4 (stack65)
        %v10570 = vadd.s32 %v10554, %v10566 (stack65)
        %v10572 = vshll.u32 %v10566, 13 (stack73)
        %v10573 = vshrl.u32 %v10566, 19 (stack74)
        %v10574 = vor.u32 %v10572, %v10573 (stack75)
        %v10575 = vxor.u32 %v10570, %v10574 (stack76)
        %v10578 = vadd.s32 %v10570, %v10575 (stack65)
        %v10580 = vshll.u32 %v10575, 15 (stack73)
        %v10581 = vshrl.u32 %v10575, 17 (stack74)
        %v10582 = vor.u32 %v10580, %v10581 (stack75)
        %v10583 = vxor.u32 %v10578, %v10582 (stack76)
        %v10586 = vadd.s32 %v10578, %v10583 (stack65)
        %v10588 = vshll.u32 %v10583, 26 (stack73)
        %v10589 = vshrl.u32 %v10583, 6 (stack74)
        %v10590 = vor.u32 %v10588, %v10589 (stack75)
        %v10591 = vxor.u32 %v10586, %v10590 (stack76)
        %v10594 = vadd.s32 %v10586, %v10591 (stack65)
        %v10598 = vadd.s32 %v10594, %v8 (stack65)
        %v10600 = vshll.u32 %v10591, 6 (stack73)
        %v10601 = vshrl.u32 %v10591, 26 (stack74)
        %v10602 = vor.u32 %v10600, %v10601 (stack75)
        %v10603 = vxor.u32 %v10594, %v10602 (stack76)
        %v10606 = vadd.s32 %v10603, %v10 (stack65)
        %v10610 = vadd.s32 %v10606, 5 (stack65)
        %v10612 = vxor.u32 %v10598, %v10610 (stack76)
        %v10613 = vand.u32.u8 %v10612, 255 (stack77)
        %v10614 = vand.u32 %v10613, 65535 (stack78)
        %v10615 = vshrl.u32 %v10614, 1 (stack79)
        %v10616 = vor.u32 %v10615, 16256 (stack75)
        %v10617 = vand.u32.u16 %v10616, 65535 (stack80)
        %v10618 = vunpack.i.l.bf16 %v10617 (stack81)
        %v10622 = vadd.f32 %v10618, -1.0 (stack82)
        %v10626 = vmul.f32 %v10622, 2.0 (stack83)
        %v10630 = vadd.f32 %v10626, -0.99609375 (stack82)
        %v10634 = vmax.f32 -0.99609375, %v10630 (stack84)
        %v10636 = vand.u32 2147483647, %v10634 (stack85)
        %vm10639 = vcmp.eq.f32.partialorder %v10636, 1.0 (stack86)
        %v10644 = vmul.f32 %v10634, inf (stack83)
        %v10646 = vxor.u32 %v10634, 2147483648 (stack87)
        %v10649 = vmul.f32 %v10634, %v10646 (stack83)
        %v10651 = vadd.f32 %v10649, 1.0 (stack88)
        %v10652 = vlog2.pop %v10651 (stack89)
        %v10653 = vmul.f32 %v10652, 0.6931472 (stack90)
        %v10654 = vmul.f32 -0.5, %v10649 (stack91)
        %v10655 = vadd.f32 %v10654, 1.0 (stack92)
        %v10656 = vmul.f32 %v10655, %v10649 (stack93)
        %v10657 = vand.u32 2147483647, %v10649 (stack94)
        %vm10658 = vcmp.lt.f32.partialorder %v10657, 0.0004427343 (stack95)
        %v10659 = vsel /*vm=*/%vm10658, /*on_true_vy=*/%v10656, /*on_false_vx=*/%v10653 (stack96)
        %v10660 = vxor.u32 %v10659, 2147483648 (stack87)
        %vm10663 = vcmp.lt.f32.partialorder %v10660, 5.0 (stack86)
        %v10668 = vsel /*vm=*/%vm10663, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v10672 = vsel /*vm=*/%vm10663, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v10676 = vsel /*vm=*/%vm10663, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v10680 = vsel /*vm=*/%vm10663, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v10684 = vsel /*vm=*/%vm10663, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v10688 = vsel /*vm=*/%vm10663, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v10692 = vsel /*vm=*/%vm10663, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v10696 = vsel /*vm=*/%vm10663, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v10700 = vsel /*vm=*/%vm10663, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v10704 = vadd.f32 %v10660, -2.5 (stack82)
        %v10706 = vrsqrt.pop %v10660 (stack97)
        %v10707 = vmul.f32 %v10660, %v10706 (stack98)
        %vm10708 = vcmp.eq.f32.partialorder %v10660, inf (stack99)
        %v10709 = vsel /*vm=*/%vm10708, /*on_true_vy=*/%v10660, /*on_false_vx=*/%v10707 (stack100)
        %vm10710 = vcmp.eq.f32.partialorder %v10660, 0.0 (stack101)
        %v10711 = vand.u32 %v10660, 2147483648 (stack102)
        %v10712 = vsel /*vm=*/%vm10710, /*on_true_vy=*/%v10711, /*on_false_vx=*/%v10709 (stack103)
        %v10715 = vadd.f32 %v10712, -3.0 (stack82)
        %v10719 = vsel /*vm=*/%vm10663, /*on_true_vy=*/%v10704, /*on_false_vx=*/%v10715 (stack72)
        %v10723 = vmul.f32 %v10700, %v10719 (stack83)
        %v10727 = vadd.f32 %v10696, %v10723 (stack82)
        %v10731 = vmul.f32 %v10727, %v10719 (stack83)
        %v10735 = vadd.f32 %v10692, %v10731 (stack82)
        %v10739 = vmul.f32 %v10735, %v10719 (stack83)
        %v10743 = vadd.f32 %v10688, %v10739 (stack82)
        %v10747 = vmul.f32 %v10743, %v10719 (stack83)
        %v10751 = vadd.f32 %v10684, %v10747 (stack82)
        %v10755 = vmul.f32 %v10751, %v10719 (stack83)
        %v10759 = vadd.f32 %v10680, %v10755 (stack82)
        %v10763 = vmul.f32 %v10759, %v10719 (stack83)
        %v10767 = vadd.f32 %v10676, %v10763 (stack82)
        %v10771 = vmul.f32 %v10767, %v10719 (stack83)
        %v10775 = vadd.f32 %v10672, %v10771 (stack82)
        %v10779 = vmul.f32 %v10775, %v10719 (stack83)
        %v10783 = vadd.f32 %v10668, %v10779 (stack82)
        %v10787 = vmul.f32 %v10783, %v10634 (stack83)
        %v10791 = vsel /*vm=*/%vm10639, /*on_true_vy=*/%v10644, /*on_false_vx=*/%v10787 (stack72)
        %v10795 = vmul.f32 %v10791, 1.4140625 (stack83)
        %s10797 = scalar_lea.vmem %s280, 648 [#allocation0] (stack107)
        %v10798 = vpack.c.bf16 0.0, %v10795 (stack104)
        %10799 = vst [vmem:[%s10797] sm:$0xf] /*vst_source=*/%v10798 (stack105)
        %v10802 = vadd.s32 %v3329, %v8033 (stack65)
        %s10804 = smul.u32 128, %s27 (stack66)
        %v10805 = vlaneseq (stack67)
        %v10806 = vand.u32 %v10805, 127 (stack68)
        %v10807 = vstv %s10804 (stack69)
        %v10808 = vadd.s32 %v10806, %v10807 (stack70)
        %v10812 = vadd.s32 %v10802, %v10808 (stack65)
        %vm10816 = vcmp.lt.u32.totalorder %v10812, %v10802 (stack71)
        %vm10821 = vcmp.lt.u32.totalorder %v10802, %v3329 (stack71)
        %v10826 = vadd.s32 %v3316, %v8016 (stack65)
        %v10830 = vadd.s32 %v10826, 1 (stack65)
        %v10834 = vsel /*vm=*/%vm10821, /*on_true_vy=*/%v10830, /*on_false_vx=*/%v10826 (stack72)
        %v10838 = vadd.s32 %v10834, 1 (stack65)
        %v10842 = vsel /*vm=*/%vm10816, /*on_true_vy=*/%v10838, /*on_false_vx=*/%v10834 (stack72)
        %v10847 = vadd.s32 %v10842, %v10 (stack65)
        %v10851 = vadd.s32 %v10812, %v9 (stack65)
        %v10855 = vadd.s32 %v10847, %v10851 (stack65)
        %v10857 = vshll.u32 %v10851, 13 (stack73)
        %v10858 = vshrl.u32 %v10851, 19 (stack74)
        %v10859 = vor.u32 %v10857, %v10858 (stack75)
        %v10860 = vxor.u32 %v10855, %v10859 (stack76)
        %v10863 = vadd.s32 %v10855, %v10860 (stack65)
        %v10865 = vshll.u32 %v10860, 15 (stack73)
        %v10866 = vshrl.u32 %v10860, 17 (stack74)
        %v10867 = vor.u32 %v10865, %v10866 (stack75)
        %v10868 = vxor.u32 %v10863, %v10867 (stack76)
        %v10871 = vadd.s32 %v10863, %v10868 (stack65)
        %v10873 = vshll.u32 %v10868, 26 (stack73)
        %v10874 = vshrl.u32 %v10868, 6 (stack74)
        %v10875 = vor.u32 %v10873, %v10874 (stack75)
        %v10876 = vxor.u32 %v10871, %v10875 (stack76)
        %v10879 = vadd.s32 %v10871, %v10876 (stack65)
        %v10883 = vadd.s32 %v10879, %v9 (stack65)
        %v10885 = vshll.u32 %v10876, 6 (stack73)
        %v10886 = vshrl.u32 %v10876, 26 (stack74)
        %v10887 = vor.u32 %v10885, %v10886 (stack75)
        %v10888 = vxor.u32 %v10879, %v10887 (stack76)
        %v10891 = vadd.s32 %v10888, %v8 (stack65)
        %v10895 = vadd.s32 %v10891, 1 (stack65)
        %v10899 = vadd.s32 %v10883, %v10895 (stack65)
        %v10901 = vshll.u32 %v10895, 17 (stack73)
        %v10902 = vshrl.u32 %v10895, 15 (stack74)
        %v10903 = vor.u32 %v10901, %v10902 (stack75)
        %v10904 = vxor.u32 %v10899, %v10903 (stack76)
        %v10907 = vadd.s32 %v10899, %v10904 (stack65)
        %v10909 = vshll.u32 %v10904, 29 (stack73)
        %v10910 = vshrl.u32 %v10904, 3 (stack74)
        %v10911 = vor.u32 %v10909, %v10910 (stack75)
        %v10912 = vxor.u32 %v10907, %v10911 (stack76)
        %v10915 = vadd.s32 %v10907, %v10912 (stack65)
        %v10917 = vshll.u32 %v10912, 16 (stack73)
        %v10918 = vshrl.u32 %v10912, 16 (stack74)
        %v10919 = vor.u32 %v10917, %v10918 (stack75)
        %v10920 = vxor.u32 %v10915, %v10919 (stack76)
        %v10923 = vadd.s32 %v10915, %v10920 (stack65)
        %v10927 = vadd.s32 %v10923, %v8 (stack65)
        %v10929 = vshll.u32 %v10920, 24 (stack73)
        %v10930 = vshrl.u32 %v10920, 8 (stack74)
        %v10931 = vor.u32 %v10929, %v10930 (stack75)
        %v10932 = vxor.u32 %v10923, %v10931 (stack76)
        %v10935 = vadd.s32 %v10932, %v10 (stack65)
        %v10939 = vadd.s32 %v10935, 2 (stack65)
        %v10943 = vadd.s32 %v10927, %v10939 (stack65)
        %v10945 = vshll.u32 %v10939, 13 (stack73)
        %v10946 = vshrl.u32 %v10939, 19 (stack74)
        %v10947 = vor.u32 %v10945, %v10946 (stack75)
        %v10948 = vxor.u32 %v10943, %v10947 (stack76)
        %v10951 = vadd.s32 %v10943, %v10948 (stack65)
        %v10953 = vshll.u32 %v10948, 15 (stack73)
        %v10954 = vshrl.u32 %v10948, 17 (stack74)
        %v10955 = vor.u32 %v10953, %v10954 (stack75)
        %v10956 = vxor.u32 %v10951, %v10955 (stack76)
        %v10959 = vadd.s32 %v10951, %v10956 (stack65)
        %v10961 = vshll.u32 %v10956, 26 (stack73)
        %v10962 = vshrl.u32 %v10956, 6 (stack74)
        %v10963 = vor.u32 %v10961, %v10962 (stack75)
        %v10964 = vxor.u32 %v10959, %v10963 (stack76)
        %v10967 = vadd.s32 %v10959, %v10964 (stack65)
        %v10971 = vadd.s32 %v10967, %v10 (stack65)
        %v10973 = vshll.u32 %v10964, 6 (stack73)
        %v10974 = vshrl.u32 %v10964, 26 (stack74)
        %v10975 = vor.u32 %v10973, %v10974 (stack75)
        %v10976 = vxor.u32 %v10967, %v10975 (stack76)
        %v10979 = vadd.s32 %v10976, %v9 (stack65)
        %v10983 = vadd.s32 %v10979, 3 (stack65)
        %v10987 = vadd.s32 %v10971, %v10983 (stack65)
        %v10989 = vshll.u32 %v10983, 17 (stack73)
        %v10990 = vshrl.u32 %v10983, 15 (stack74)
        %v10991 = vor.u32 %v10989, %v10990 (stack75)
        %v10992 = vxor.u32 %v10987, %v10991 (stack76)
        %v10995 = vadd.s32 %v10987, %v10992 (stack65)
        %v10997 = vshll.u32 %v10992, 29 (stack73)
        %v10998 = vshrl.u32 %v10992, 3 (stack74)
        %v10999 = vor.u32 %v10997, %v10998 (stack75)
        %v11000 = vxor.u32 %v10995, %v10999 (stack76)
        %v11003 = vadd.s32 %v10995, %v11000 (stack65)
        %v11005 = vshll.u32 %v11000, 16 (stack73)
        %v11006 = vshrl.u32 %v11000, 16 (stack74)
        %v11007 = vor.u32 %v11005, %v11006 (stack75)
        %v11008 = vxor.u32 %v11003, %v11007 (stack76)
        %v11011 = vadd.s32 %v11003, %v11008 (stack65)
        %v11015 = vadd.s32 %v11011, %v9 (stack65)
        %v11017 = vshll.u32 %v11008, 24 (stack73)
        %v11018 = vshrl.u32 %v11008, 8 (stack74)
        %v11019 = vor.u32 %v11017, %v11018 (stack75)
        %v11020 = vxor.u32 %v11011, %v11019 (stack76)
        %v11023 = vadd.s32 %v11020, %v8 (stack65)
        %v11027 = vadd.s32 %v11023, 4 (stack65)
        %v11031 = vadd.s32 %v11015, %v11027 (stack65)
        %v11033 = vshll.u32 %v11027, 13 (stack73)
        %v11034 = vshrl.u32 %v11027, 19 (stack74)
        %v11035 = vor.u32 %v11033, %v11034 (stack75)
        %v11036 = vxor.u32 %v11031, %v11035 (stack76)
        %v11039 = vadd.s32 %v11031, %v11036 (stack65)
        %v11041 = vshll.u32 %v11036, 15 (stack73)
        %v11042 = vshrl.u32 %v11036, 17 (stack74)
        %v11043 = vor.u32 %v11041, %v11042 (stack75)
        %v11044 = vxor.u32 %v11039, %v11043 (stack76)
        %v11047 = vadd.s32 %v11039, %v11044 (stack65)
        %v11049 = vshll.u32 %v11044, 26 (stack73)
        %v11050 = vshrl.u32 %v11044, 6 (stack74)
        %v11051 = vor.u32 %v11049, %v11050 (stack75)
        %v11052 = vxor.u32 %v11047, %v11051 (stack76)
        %v11055 = vadd.s32 %v11047, %v11052 (stack65)
        %v11059 = vadd.s32 %v11055, %v8 (stack65)
        %v11061 = vshll.u32 %v11052, 6 (stack73)
        %v11062 = vshrl.u32 %v11052, 26 (stack74)
        %v11063 = vor.u32 %v11061, %v11062 (stack75)
        %v11064 = vxor.u32 %v11055, %v11063 (stack76)
        %v11067 = vadd.s32 %v11064, %v10 (stack65)
        %v11071 = vadd.s32 %v11067, 5 (stack65)
        %v11073 = vxor.u32 %v11059, %v11071 (stack76)
        %v11074 = vand.u32.u8 %v11073, 255 (stack77)
        %v11075 = vand.u32 %v11074, 65535 (stack78)
        %v11076 = vshrl.u32 %v11075, 1 (stack79)
        %v11077 = vor.u32 %v11076, 16256 (stack75)
        %v11078 = vand.u32.u16 %v11077, 65535 (stack80)
        %v11079 = vunpack.i.l.bf16 %v11078 (stack81)
        %v11083 = vadd.f32 %v11079, -1.0 (stack82)
        %v11087 = vmul.f32 %v11083, 2.0 (stack83)
        %v11091 = vadd.f32 %v11087, -0.99609375 (stack82)
        %v11095 = vmax.f32 -0.99609375, %v11091 (stack84)
        %v11097 = vand.u32 2147483647, %v11095 (stack85)
        %vm11100 = vcmp.eq.f32.partialorder %v11097, 1.0 (stack86)
        %v11105 = vmul.f32 %v11095, inf (stack83)
        %v11107 = vxor.u32 %v11095, 2147483648 (stack87)
        %v11110 = vmul.f32 %v11095, %v11107 (stack83)
        %v11112 = vadd.f32 %v11110, 1.0 (stack88)
        %v11113 = vlog2.pop %v11112 (stack89)
        %v11114 = vmul.f32 %v11113, 0.6931472 (stack90)
        %v11115 = vmul.f32 -0.5, %v11110 (stack91)
        %v11116 = vadd.f32 %v11115, 1.0 (stack92)
        %v11117 = vmul.f32 %v11116, %v11110 (stack93)
        %v11118 = vand.u32 2147483647, %v11110 (stack94)
        %vm11119 = vcmp.lt.f32.partialorder %v11118, 0.0004427343 (stack95)
        %v11120 = vsel /*vm=*/%vm11119, /*on_true_vy=*/%v11117, /*on_false_vx=*/%v11114 (stack96)
        %v11121 = vxor.u32 %v11120, 2147483648 (stack87)
        %vm11124 = vcmp.lt.f32.partialorder %v11121, 5.0 (stack86)
        %v11129 = vsel /*vm=*/%vm11124, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v11133 = vsel /*vm=*/%vm11124, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v11137 = vsel /*vm=*/%vm11124, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v11141 = vsel /*vm=*/%vm11124, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v11145 = vsel /*vm=*/%vm11124, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v11149 = vsel /*vm=*/%vm11124, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v11153 = vsel /*vm=*/%vm11124, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v11157 = vsel /*vm=*/%vm11124, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v11161 = vsel /*vm=*/%vm11124, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v11165 = vadd.f32 %v11121, -2.5 (stack82)
        %v11167 = vrsqrt.pop %v11121 (stack97)
        %v11168 = vmul.f32 %v11121, %v11167 (stack98)
        %vm11169 = vcmp.eq.f32.partialorder %v11121, inf (stack99)
        %v11170 = vsel /*vm=*/%vm11169, /*on_true_vy=*/%v11121, /*on_false_vx=*/%v11168 (stack100)
        %vm11171 = vcmp.eq.f32.partialorder %v11121, 0.0 (stack101)
        %v11172 = vand.u32 %v11121, 2147483648 (stack102)
        %v11173 = vsel /*vm=*/%vm11171, /*on_true_vy=*/%v11172, /*on_false_vx=*/%v11170 (stack103)
        %v11176 = vadd.f32 %v11173, -3.0 (stack82)
        %v11180 = vsel /*vm=*/%vm11124, /*on_true_vy=*/%v11165, /*on_false_vx=*/%v11176 (stack72)
        %v11184 = vmul.f32 %v11161, %v11180 (stack83)
        %v11188 = vadd.f32 %v11157, %v11184 (stack82)
        %v11192 = vmul.f32 %v11188, %v11180 (stack83)
        %v11196 = vadd.f32 %v11153, %v11192 (stack82)
        %v11200 = vmul.f32 %v11196, %v11180 (stack83)
        %v11204 = vadd.f32 %v11149, %v11200 (stack82)
        %v11208 = vmul.f32 %v11204, %v11180 (stack83)
        %v11212 = vadd.f32 %v11145, %v11208 (stack82)
        %v11216 = vmul.f32 %v11212, %v11180 (stack83)
        %v11220 = vadd.f32 %v11141, %v11216 (stack82)
        %v11224 = vmul.f32 %v11220, %v11180 (stack83)
        %v11228 = vadd.f32 %v11137, %v11224 (stack82)
        %v11232 = vmul.f32 %v11228, %v11180 (stack83)
        %v11236 = vadd.f32 %v11133, %v11232 (stack82)
        %v11240 = vmul.f32 %v11236, %v11180 (stack83)
        %v11244 = vadd.f32 %v11129, %v11240 (stack82)
        %v11248 = vmul.f32 %v11244, %v11095 (stack83)
        %v11252 = vsel /*vm=*/%vm11100, /*on_true_vy=*/%v11105, /*on_false_vx=*/%v11248 (stack72)
        %v11256 = vmul.f32 %v11252, 1.4140625 (stack83)
        %s11258 = scalar_lea.vmem %s280, 776 [#allocation0] (stack107)
        %v11259 = vpack.c.bf16 0.0, %v11256 (stack104)
        %11260 = vst [vmem:[%s11258] sm:$0xf] /*vst_source=*/%v11259 (stack105)
        %v11263 = vadd.s32 %v3816, %v8033 (stack65)
        %s11265 = smul.u32 128, %s27 (stack66)
        %v11266 = vlaneseq (stack67)
        %v11267 = vand.u32 %v11266, 127 (stack68)
        %v11268 = vstv %s11265 (stack69)
        %v11269 = vadd.s32 %v11267, %v11268 (stack70)
        %v11273 = vadd.s32 %v11263, %v11269 (stack65)
        %vm11277 = vcmp.lt.u32.totalorder %v11273, %v11263 (stack71)
        %vm11282 = vcmp.lt.u32.totalorder %v11263, %v3816 (stack71)
        %v11287 = vadd.s32 %v3803, %v8016 (stack65)
        %v11291 = vadd.s32 %v11287, 1 (stack65)
        %v11295 = vsel /*vm=*/%vm11282, /*on_true_vy=*/%v11291, /*on_false_vx=*/%v11287 (stack72)
        %v11299 = vadd.s32 %v11295, 1 (stack65)
        %v11303 = vsel /*vm=*/%vm11277, /*on_true_vy=*/%v11299, /*on_false_vx=*/%v11295 (stack72)
        %v11308 = vadd.s32 %v11303, %v10 (stack65)
        %v11312 = vadd.s32 %v11273, %v9 (stack65)
        %v11316 = vadd.s32 %v11308, %v11312 (stack65)
        %v11318 = vshll.u32 %v11312, 13 (stack73)
        %v11319 = vshrl.u32 %v11312, 19 (stack74)
        %v11320 = vor.u32 %v11318, %v11319 (stack75)
        %v11321 = vxor.u32 %v11316, %v11320 (stack76)
        %v11324 = vadd.s32 %v11316, %v11321 (stack65)
        %v11326 = vshll.u32 %v11321, 15 (stack73)
        %v11327 = vshrl.u32 %v11321, 17 (stack74)
        %v11328 = vor.u32 %v11326, %v11327 (stack75)
        %v11329 = vxor.u32 %v11324, %v11328 (stack76)
        %v11332 = vadd.s32 %v11324, %v11329 (stack65)
        %v11334 = vshll.u32 %v11329, 26 (stack73)
        %v11335 = vshrl.u32 %v11329, 6 (stack74)
        %v11336 = vor.u32 %v11334, %v11335 (stack75)
        %v11337 = vxor.u32 %v11332, %v11336 (stack76)
        %v11340 = vadd.s32 %v11332, %v11337 (stack65)
        %v11344 = vadd.s32 %v11340, %v9 (stack65)
        %v11346 = vshll.u32 %v11337, 6 (stack73)
        %v11347 = vshrl.u32 %v11337, 26 (stack74)
        %v11348 = vor.u32 %v11346, %v11347 (stack75)
        %v11349 = vxor.u32 %v11340, %v11348 (stack76)
        %v11352 = vadd.s32 %v11349, %v8 (stack65)
        %v11356 = vadd.s32 %v11352, 1 (stack65)
        %v11360 = vadd.s32 %v11344, %v11356 (stack65)
        %v11362 = vshll.u32 %v11356, 17 (stack73)
        %v11363 = vshrl.u32 %v11356, 15 (stack74)
        %v11364 = vor.u32 %v11362, %v11363 (stack75)
        %v11365 = vxor.u32 %v11360, %v11364 (stack76)
        %v11368 = vadd.s32 %v11360, %v11365 (stack65)
        %v11370 = vshll.u32 %v11365, 29 (stack73)
        %v11371 = vshrl.u32 %v11365, 3 (stack74)
        %v11372 = vor.u32 %v11370, %v11371 (stack75)
        %v11373 = vxor.u32 %v11368, %v11372 (stack76)
        %v11376 = vadd.s32 %v11368, %v11373 (stack65)
        %v11378 = vshll.u32 %v11373, 16 (stack73)
        %v11379 = vshrl.u32 %v11373, 16 (stack74)
        %v11380 = vor.u32 %v11378, %v11379 (stack75)
        %v11381 = vxor.u32 %v11376, %v11380 (stack76)
        %v11384 = vadd.s32 %v11376, %v11381 (stack65)
        %v11388 = vadd.s32 %v11384, %v8 (stack65)
        %v11390 = vshll.u32 %v11381, 24 (stack73)
        %v11391 = vshrl.u32 %v11381, 8 (stack74)
        %v11392 = vor.u32 %v11390, %v11391 (stack75)
        %v11393 = vxor.u32 %v11384, %v11392 (stack76)
        %v11396 = vadd.s32 %v11393, %v10 (stack65)
        %v11400 = vadd.s32 %v11396, 2 (stack65)
        %v11404 = vadd.s32 %v11388, %v11400 (stack65)
        %v11406 = vshll.u32 %v11400, 13 (stack73)
        %v11407 = vshrl.u32 %v11400, 19 (stack74)
        %v11408 = vor.u32 %v11406, %v11407 (stack75)
        %v11409 = vxor.u32 %v11404, %v11408 (stack76)
        %v11412 = vadd.s32 %v11404, %v11409 (stack65)
        %v11414 = vshll.u32 %v11409, 15 (stack73)
        %v11415 = vshrl.u32 %v11409, 17 (stack74)
        %v11416 = vor.u32 %v11414, %v11415 (stack75)
        %v11417 = vxor.u32 %v11412, %v11416 (stack76)
        %v11420 = vadd.s32 %v11412, %v11417 (stack65)
        %v11422 = vshll.u32 %v11417, 26 (stack73)
        %v11423 = vshrl.u32 %v11417, 6 (stack74)
        %v11424 = vor.u32 %v11422, %v11423 (stack75)
        %v11425 = vxor.u32 %v11420, %v11424 (stack76)
        %v11428 = vadd.s32 %v11420, %v11425 (stack65)
        %v11432 = vadd.s32 %v11428, %v10 (stack65)
        %v11434 = vshll.u32 %v11425, 6 (stack73)
        %v11435 = vshrl.u32 %v11425, 26 (stack74)
        %v11436 = vor.u32 %v11434, %v11435 (stack75)
        %v11437 = vxor.u32 %v11428, %v11436 (stack76)
        %v11440 = vadd.s32 %v11437, %v9 (stack65)
        %v11444 = vadd.s32 %v11440, 3 (stack65)
        %v11448 = vadd.s32 %v11432, %v11444 (stack65)
        %v11450 = vshll.u32 %v11444, 17 (stack73)
        %v11451 = vshrl.u32 %v11444, 15 (stack74)
        %v11452 = vor.u32 %v11450, %v11451 (stack75)
        %v11453 = vxor.u32 %v11448, %v11452 (stack76)
        %v11456 = vadd.s32 %v11448, %v11453 (stack65)
        %v11458 = vshll.u32 %v11453, 29 (stack73)
        %v11459 = vshrl.u32 %v11453, 3 (stack74)
        %v11460 = vor.u32 %v11458, %v11459 (stack75)
        %v11461 = vxor.u32 %v11456, %v11460 (stack76)
        %v11464 = vadd.s32 %v11456, %v11461 (stack65)
        %v11466 = vshll.u32 %v11461, 16 (stack73)
        %v11467 = vshrl.u32 %v11461, 16 (stack74)
        %v11468 = vor.u32 %v11466, %v11467 (stack75)
        %v11469 = vxor.u32 %v11464, %v11468 (stack76)
        %v11472 = vadd.s32 %v11464, %v11469 (stack65)
        %v11476 = vadd.s32 %v11472, %v9 (stack65)
        %v11478 = vshll.u32 %v11469, 24 (stack73)
        %v11479 = vshrl.u32 %v11469, 8 (stack74)
        %v11480 = vor.u32 %v11478, %v11479 (stack75)
        %v11481 = vxor.u32 %v11472, %v11480 (stack76)
        %v11484 = vadd.s32 %v11481, %v8 (stack65)
        %v11488 = vadd.s32 %v11484, 4 (stack65)
        %v11492 = vadd.s32 %v11476, %v11488 (stack65)
        %v11494 = vshll.u32 %v11488, 13 (stack73)
        %v11495 = vshrl.u32 %v11488, 19 (stack74)
        %v11496 = vor.u32 %v11494, %v11495 (stack75)
        %v11497 = vxor.u32 %v11492, %v11496 (stack76)
        %v11500 = vadd.s32 %v11492, %v11497 (stack65)
        %v11502 = vshll.u32 %v11497, 15 (stack73)
        %v11503 = vshrl.u32 %v11497, 17 (stack74)
        %v11504 = vor.u32 %v11502, %v11503 (stack75)
        %v11505 = vxor.u32 %v11500, %v11504 (stack76)
        %v11508 = vadd.s32 %v11500, %v11505 (stack65)
        %v11510 = vshll.u32 %v11505, 26 (stack73)
        %v11511 = vshrl.u32 %v11505, 6 (stack74)
        %v11512 = vor.u32 %v11510, %v11511 (stack75)
        %v11513 = vxor.u32 %v11508, %v11512 (stack76)
        %v11516 = vadd.s32 %v11508, %v11513 (stack65)
        %v11520 = vadd.s32 %v11516, %v8 (stack65)
        %v11522 = vshll.u32 %v11513, 6 (stack73)
        %v11523 = vshrl.u32 %v11513, 26 (stack74)
        %v11524 = vor.u32 %v11522, %v11523 (stack75)
        %v11525 = vxor.u32 %v11516, %v11524 (stack76)
        %v11528 = vadd.s32 %v11525, %v10 (stack65)
        %v11532 = vadd.s32 %v11528, 5 (stack65)
        %v11534 = vxor.u32 %v11520, %v11532 (stack76)
        %v11535 = vand.u32.u8 %v11534, 255 (stack77)
        %v11536 = vand.u32 %v11535, 65535 (stack78)
        %v11537 = vshrl.u32 %v11536, 1 (stack79)
        %v11538 = vor.u32 %v11537, 16256 (stack75)
        %v11539 = vand.u32.u16 %v11538, 65535 (stack80)
        %v11540 = vunpack.i.l.bf16 %v11539 (stack81)
        %v11544 = vadd.f32 %v11540, -1.0 (stack82)
        %v11548 = vmul.f32 %v11544, 2.0 (stack83)
        %v11552 = vadd.f32 %v11548, -0.99609375 (stack82)
        %v11556 = vmax.f32 -0.99609375, %v11552 (stack84)
        %v11558 = vand.u32 2147483647, %v11556 (stack85)
        %vm11561 = vcmp.eq.f32.partialorder %v11558, 1.0 (stack86)
        %v11566 = vmul.f32 %v11556, inf (stack83)
        %v11568 = vxor.u32 %v11556, 2147483648 (stack87)
        %v11571 = vmul.f32 %v11556, %v11568 (stack83)
        %v11573 = vadd.f32 %v11571, 1.0 (stack88)
        %v11574 = vlog2.pop %v11573 (stack89)
        %v11575 = vmul.f32 %v11574, 0.6931472 (stack90)
        %v11576 = vmul.f32 -0.5, %v11571 (stack91)
        %v11577 = vadd.f32 %v11576, 1.0 (stack92)
        %v11578 = vmul.f32 %v11577, %v11571 (stack93)
        %v11579 = vand.u32 2147483647, %v11571 (stack94)
        %vm11580 = vcmp.lt.f32.partialorder %v11579, 0.0004427343 (stack95)
        %v11581 = vsel /*vm=*/%vm11580, /*on_true_vy=*/%v11578, /*on_false_vx=*/%v11575 (stack96)
        %v11582 = vxor.u32 %v11581, 2147483648 (stack87)
        %vm11585 = vcmp.lt.f32.partialorder %v11582, 5.0 (stack86)
        %v11590 = vsel /*vm=*/%vm11585, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v11594 = vsel /*vm=*/%vm11585, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v11598 = vsel /*vm=*/%vm11585, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v11602 = vsel /*vm=*/%vm11585, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v11606 = vsel /*vm=*/%vm11585, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v11610 = vsel /*vm=*/%vm11585, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v11614 = vsel /*vm=*/%vm11585, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v11618 = vsel /*vm=*/%vm11585, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v11622 = vsel /*vm=*/%vm11585, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v11626 = vadd.f32 %v11582, -2.5 (stack82)
        %v11628 = vrsqrt.pop %v11582 (stack97)
        %v11629 = vmul.f32 %v11582, %v11628 (stack98)
        %vm11630 = vcmp.eq.f32.partialorder %v11582, inf (stack99)
        %v11631 = vsel /*vm=*/%vm11630, /*on_true_vy=*/%v11582, /*on_false_vx=*/%v11629 (stack100)
        %vm11632 = vcmp.eq.f32.partialorder %v11582, 0.0 (stack101)
        %v11633 = vand.u32 %v11582, 2147483648 (stack102)
        %v11634 = vsel /*vm=*/%vm11632, /*on_true_vy=*/%v11633, /*on_false_vx=*/%v11631 (stack103)
        %v11637 = vadd.f32 %v11634, -3.0 (stack82)
        %v11641 = vsel /*vm=*/%vm11585, /*on_true_vy=*/%v11626, /*on_false_vx=*/%v11637 (stack72)
        %v11645 = vmul.f32 %v11622, %v11641 (stack83)
        %v11649 = vadd.f32 %v11618, %v11645 (stack82)
        %v11653 = vmul.f32 %v11649, %v11641 (stack83)
        %v11657 = vadd.f32 %v11614, %v11653 (stack82)
        %v11661 = vmul.f32 %v11657, %v11641 (stack83)
        %v11665 = vadd.f32 %v11610, %v11661 (stack82)
        %v11669 = vmul.f32 %v11665, %v11641 (stack83)
        %v11673 = vadd.f32 %v11606, %v11669 (stack82)
        %v11677 = vmul.f32 %v11673, %v11641 (stack83)
        %v11681 = vadd.f32 %v11602, %v11677 (stack82)
        %v11685 = vmul.f32 %v11681, %v11641 (stack83)
        %v11689 = vadd.f32 %v11598, %v11685 (stack82)
        %v11693 = vmul.f32 %v11689, %v11641 (stack83)
        %v11697 = vadd.f32 %v11594, %v11693 (stack82)
        %v11701 = vmul.f32 %v11697, %v11641 (stack83)
        %v11705 = vadd.f32 %v11590, %v11701 (stack82)
        %v11709 = vmul.f32 %v11705, %v11556 (stack83)
        %v11713 = vsel /*vm=*/%vm11561, /*on_true_vy=*/%v11566, /*on_false_vx=*/%v11709 (stack72)
        %v11717 = vmul.f32 %v11713, 1.4140625 (stack83)
        %s11719 = scalar_lea.vmem %s280, 904 [#allocation0] (stack107)
        %v11720 = vpack.c.bf16 0.0, %v11717 (stack104)
        %11721 = vst [vmem:[%s11719] sm:$0xf] /*vst_source=*/%v11720 (stack105)
        %s11722 = sadd.s32 %s339, 24 (stack106)
        %s11723 = sshrl.u32 %s11722, 10 (stack49)
        %p11724 = scmp.lt.s32.totalorder 1, %s11723 (stack50)
        %s11725 = scalar_select /*predicate=*/%p11724, /*on_true=*/1, /*on_false=*/%s11723 (stack51)
        %s11726 = sand.u32 %s11722, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s11727 = sshrl.u32 %s11726, 7 (stack53)
        %s11728 = sand.u32 %s11726, 127 /* smod.u32 w/div 128 */ (stack54)
        %s11729 = smul.addr %s11725, 8 (stack55)
        %s11730 = scalar_lea.vmem %s3, %s11729 (stack56)
        %s11732 = scalar_lea.vmem %s11730, %s11727 (stack57)
        %v11733 = vld [vmem:[%s11732] ss:$0 sm:$0xff] (stack58)
        %s11734 = sand.u32 %s11728, 255 (stack59)
        %s11736 = sor.u32 256, %s11734 (stack60)
        %11737 = vbcast.lane.b32.xlu0 %v11733, %s11736 (stack61)
        %v11738 = vpop.permute.xlu0 %11737 (stack62)
        %s11739 = sadd.s32 %s347, 24 (stack106)
        %s11740 = sshrl.u32 %s11739, 10 (stack49)
        %p11741 = scmp.lt.s32.totalorder 1, %s11740 (stack50)
        %s11742 = scalar_select /*predicate=*/%p11741, /*on_true=*/1, /*on_false=*/%s11740 (stack51)
        %s11743 = sand.u32 %s11739, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s11744 = sshrl.u32 %s11743, 7 (stack53)
        %s11745 = sand.u32 %s11743, 127 /* smod.u32 w/div 128 */ (stack54)
        %s11746 = smul.addr %s11742, 8 (stack55)
        %s11747 = scalar_lea.vmem %s5, %s11746 (stack56)
        %s11749 = scalar_lea.vmem %s11747, %s11744 (stack57)
        %v11750 = vld [vmem:[%s11749] ss:$0 sm:$0xff] (stack58)
        %s11751 = sand.u32 %s11745, 255 (stack59)
        %s11753 = sor.u32 256, %s11751 (stack60)
        %11754 = vbcast.lane.b32.xlu0 %v11750, %s11753 (stack61)
        %v11755 = vpop.permute.xlu0 %11754 (stack62)
        %v11758 = vadd.s32 %v408, %v11755 (stack65)
        %s11760 = smul.u32 128, %s27 (stack66)
        %v11761 = vlaneseq (stack67)
        %v11762 = vand.u32 %v11761, 127 (stack68)
        %v11763 = vstv %s11760 (stack69)
        %v11764 = vadd.s32 %v11762, %v11763 (stack70)
        %v11768 = vadd.s32 %v11758, %v11764 (stack65)
        %vm11772 = vcmp.lt.u32.totalorder %v11768, %v11758 (stack71)
        %vm11777 = vcmp.lt.u32.totalorder %v11758, %v408 (stack71)
        %v11782 = vadd.s32 %v380, %v11738 (stack65)
        %v11786 = vadd.s32 %v11782, 1 (stack65)
        %v11790 = vsel /*vm=*/%vm11777, /*on_true_vy=*/%v11786, /*on_false_vx=*/%v11782 (stack72)
        %v11794 = vadd.s32 %v11790, 1 (stack65)
        %v11798 = vsel /*vm=*/%vm11772, /*on_true_vy=*/%v11794, /*on_false_vx=*/%v11790 (stack72)
        %v11803 = vadd.s32 %v11798, %v10 (stack65)
        %v11807 = vadd.s32 %v11768, %v9 (stack65)
        %v11811 = vadd.s32 %v11803, %v11807 (stack65)
        %v11813 = vshll.u32 %v11807, 13 (stack73)
        %v11814 = vshrl.u32 %v11807, 19 (stack74)
        %v11815 = vor.u32 %v11813, %v11814 (stack75)
        %v11816 = vxor.u32 %v11811, %v11815 (stack76)
        %v11819 = vadd.s32 %v11811, %v11816 (stack65)
        %v11821 = vshll.u32 %v11816, 15 (stack73)
        %v11822 = vshrl.u32 %v11816, 17 (stack74)
        %v11823 = vor.u32 %v11821, %v11822 (stack75)
        %v11824 = vxor.u32 %v11819, %v11823 (stack76)
        %v11827 = vadd.s32 %v11819, %v11824 (stack65)
        %v11829 = vshll.u32 %v11824, 26 (stack73)
        %v11830 = vshrl.u32 %v11824, 6 (stack74)
        %v11831 = vor.u32 %v11829, %v11830 (stack75)
        %v11832 = vxor.u32 %v11827, %v11831 (stack76)
        %v11835 = vadd.s32 %v11827, %v11832 (stack65)
        %v11839 = vadd.s32 %v11835, %v9 (stack65)
        %v11841 = vshll.u32 %v11832, 6 (stack73)
        %v11842 = vshrl.u32 %v11832, 26 (stack74)
        %v11843 = vor.u32 %v11841, %v11842 (stack75)
        %v11844 = vxor.u32 %v11835, %v11843 (stack76)
        %v11847 = vadd.s32 %v11844, %v8 (stack65)
        %v11851 = vadd.s32 %v11847, 1 (stack65)
        %v11855 = vadd.s32 %v11839, %v11851 (stack65)
        %v11857 = vshll.u32 %v11851, 17 (stack73)
        %v11858 = vshrl.u32 %v11851, 15 (stack74)
        %v11859 = vor.u32 %v11857, %v11858 (stack75)
        %v11860 = vxor.u32 %v11855, %v11859 (stack76)
        %v11863 = vadd.s32 %v11855, %v11860 (stack65)
        %v11865 = vshll.u32 %v11860, 29 (stack73)
        %v11866 = vshrl.u32 %v11860, 3 (stack74)
        %v11867 = vor.u32 %v11865, %v11866 (stack75)
        %v11868 = vxor.u32 %v11863, %v11867 (stack76)
        %v11871 = vadd.s32 %v11863, %v11868 (stack65)
        %v11873 = vshll.u32 %v11868, 16 (stack73)
        %v11874 = vshrl.u32 %v11868, 16 (stack74)
        %v11875 = vor.u32 %v11873, %v11874 (stack75)
        %v11876 = vxor.u32 %v11871, %v11875 (stack76)
        %v11879 = vadd.s32 %v11871, %v11876 (stack65)
        %v11883 = vadd.s32 %v11879, %v8 (stack65)
        %v11885 = vshll.u32 %v11876, 24 (stack73)
        %v11886 = vshrl.u32 %v11876, 8 (stack74)
        %v11887 = vor.u32 %v11885, %v11886 (stack75)
        %v11888 = vxor.u32 %v11879, %v11887 (stack76)
        %v11891 = vadd.s32 %v11888, %v10 (stack65)
        %v11895 = vadd.s32 %v11891, 2 (stack65)
        %v11899 = vadd.s32 %v11883, %v11895 (stack65)
        %v11901 = vshll.u32 %v11895, 13 (stack73)
        %v11902 = vshrl.u32 %v11895, 19 (stack74)
        %v11903 = vor.u32 %v11901, %v11902 (stack75)
        %v11904 = vxor.u32 %v11899, %v11903 (stack76)
        %v11907 = vadd.s32 %v11899, %v11904 (stack65)
        %v11909 = vshll.u32 %v11904, 15 (stack73)
        %v11910 = vshrl.u32 %v11904, 17 (stack74)
        %v11911 = vor.u32 %v11909, %v11910 (stack75)
        %v11912 = vxor.u32 %v11907, %v11911 (stack76)
        %v11915 = vadd.s32 %v11907, %v11912 (stack65)
        %v11917 = vshll.u32 %v11912, 26 (stack73)
        %v11918 = vshrl.u32 %v11912, 6 (stack74)
        %v11919 = vor.u32 %v11917, %v11918 (stack75)
        %v11920 = vxor.u32 %v11915, %v11919 (stack76)
        %v11923 = vadd.s32 %v11915, %v11920 (stack65)
        %v11927 = vadd.s32 %v11923, %v10 (stack65)
        %v11929 = vshll.u32 %v11920, 6 (stack73)
        %v11930 = vshrl.u32 %v11920, 26 (stack74)
        %v11931 = vor.u32 %v11929, %v11930 (stack75)
        %v11932 = vxor.u32 %v11923, %v11931 (stack76)
        %v11935 = vadd.s32 %v11932, %v9 (stack65)
        %v11939 = vadd.s32 %v11935, 3 (stack65)
        %v11943 = vadd.s32 %v11927, %v11939 (stack65)
        %v11945 = vshll.u32 %v11939, 17 (stack73)
        %v11946 = vshrl.u32 %v11939, 15 (stack74)
        %v11947 = vor.u32 %v11945, %v11946 (stack75)
        %v11948 = vxor.u32 %v11943, %v11947 (stack76)
        %v11951 = vadd.s32 %v11943, %v11948 (stack65)
        %v11953 = vshll.u32 %v11948, 29 (stack73)
        %v11954 = vshrl.u32 %v11948, 3 (stack74)
        %v11955 = vor.u32 %v11953, %v11954 (stack75)
        %v11956 = vxor.u32 %v11951, %v11955 (stack76)
        %v11959 = vadd.s32 %v11951, %v11956 (stack65)
        %v11961 = vshll.u32 %v11956, 16 (stack73)
        %v11962 = vshrl.u32 %v11956, 16 (stack74)
        %v11963 = vor.u32 %v11961, %v11962 (stack75)
        %v11964 = vxor.u32 %v11959, %v11963 (stack76)
        %v11967 = vadd.s32 %v11959, %v11964 (stack65)
        %v11971 = vadd.s32 %v11967, %v9 (stack65)
        %v11973 = vshll.u32 %v11964, 24 (stack73)
        %v11974 = vshrl.u32 %v11964, 8 (stack74)
        %v11975 = vor.u32 %v11973, %v11974 (stack75)
        %v11976 = vxor.u32 %v11967, %v11975 (stack76)
        %v11979 = vadd.s32 %v11976, %v8 (stack65)
        %v11983 = vadd.s32 %v11979, 4 (stack65)
        %v11987 = vadd.s32 %v11971, %v11983 (stack65)
        %v11989 = vshll.u32 %v11983, 13 (stack73)
        %v11990 = vshrl.u32 %v11983, 19 (stack74)
        %v11991 = vor.u32 %v11989, %v11990 (stack75)
        %v11992 = vxor.u32 %v11987, %v11991 (stack76)
        %v11995 = vadd.s32 %v11987, %v11992 (stack65)
        %v11997 = vshll.u32 %v11992, 15 (stack73)
        %v11998 = vshrl.u32 %v11992, 17 (stack74)
        %v11999 = vor.u32 %v11997, %v11998 (stack75)
        %v12000 = vxor.u32 %v11995, %v11999 (stack76)
        %v12003 = vadd.s32 %v11995, %v12000 (stack65)
        %v12005 = vshll.u32 %v12000, 26 (stack73)
        %v12006 = vshrl.u32 %v12000, 6 (stack74)
        %v12007 = vor.u32 %v12005, %v12006 (stack75)
        %v12008 = vxor.u32 %v12003, %v12007 (stack76)
        %v12011 = vadd.s32 %v12003, %v12008 (stack65)
        %v12015 = vadd.s32 %v12011, %v8 (stack65)
        %v12017 = vshll.u32 %v12008, 6 (stack73)
        %v12018 = vshrl.u32 %v12008, 26 (stack74)
        %v12019 = vor.u32 %v12017, %v12018 (stack75)
        %v12020 = vxor.u32 %v12011, %v12019 (stack76)
        %v12023 = vadd.s32 %v12020, %v10 (stack65)
        %v12027 = vadd.s32 %v12023, 5 (stack65)
        %v12029 = vxor.u32 %v12015, %v12027 (stack76)
        %v12030 = vand.u32.u8 %v12029, 255 (stack77)
        %v12031 = vand.u32 %v12030, 65535 (stack78)
        %v12032 = vshrl.u32 %v12031, 1 (stack79)
        %v12033 = vor.u32 %v12032, 16256 (stack75)
        %v12034 = vand.u32.u16 %v12033, 65535 (stack80)
        %v12035 = vunpack.i.l.bf16 %v12034 (stack81)
        %v12039 = vadd.f32 %v12035, -1.0 (stack82)
        %v12043 = vmul.f32 %v12039, 2.0 (stack83)
        %v12047 = vadd.f32 %v12043, -0.99609375 (stack82)
        %v12051 = vmax.f32 -0.99609375, %v12047 (stack84)
        %v12053 = vand.u32 2147483647, %v12051 (stack85)
        %vm12056 = vcmp.eq.f32.partialorder %v12053, 1.0 (stack86)
        %v12061 = vmul.f32 %v12051, inf (stack83)
        %v12063 = vxor.u32 %v12051, 2147483648 (stack87)
        %v12066 = vmul.f32 %v12051, %v12063 (stack83)
        %v12068 = vadd.f32 %v12066, 1.0 (stack88)
        %v12069 = vlog2.pop %v12068 (stack89)
        %v12070 = vmul.f32 %v12069, 0.6931472 (stack90)
        %v12071 = vmul.f32 -0.5, %v12066 (stack91)
        %v12072 = vadd.f32 %v12071, 1.0 (stack92)
        %v12073 = vmul.f32 %v12072, %v12066 (stack93)
        %v12074 = vand.u32 2147483647, %v12066 (stack94)
        %vm12075 = vcmp.lt.f32.partialorder %v12074, 0.0004427343 (stack95)
        %v12076 = vsel /*vm=*/%vm12075, /*on_true_vy=*/%v12073, /*on_false_vx=*/%v12070 (stack96)
        %v12077 = vxor.u32 %v12076, 2147483648 (stack87)
        %vm12080 = vcmp.lt.f32.partialorder %v12077, 5.0 (stack86)
        %v12085 = vsel /*vm=*/%vm12080, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v12089 = vsel /*vm=*/%vm12080, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v12093 = vsel /*vm=*/%vm12080, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v12097 = vsel /*vm=*/%vm12080, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v12101 = vsel /*vm=*/%vm12080, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v12105 = vsel /*vm=*/%vm12080, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v12109 = vsel /*vm=*/%vm12080, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v12113 = vsel /*vm=*/%vm12080, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v12117 = vsel /*vm=*/%vm12080, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v12121 = vadd.f32 %v12077, -2.5 (stack82)
        %v12123 = vrsqrt.pop %v12077 (stack97)
        %v12124 = vmul.f32 %v12077, %v12123 (stack98)
        %vm12125 = vcmp.eq.f32.partialorder %v12077, inf (stack99)
        %v12126 = vsel /*vm=*/%vm12125, /*on_true_vy=*/%v12077, /*on_false_vx=*/%v12124 (stack100)
        %vm12127 = vcmp.eq.f32.partialorder %v12077, 0.0 (stack101)
        %v12128 = vand.u32 %v12077, 2147483648 (stack102)
        %v12129 = vsel /*vm=*/%vm12127, /*on_true_vy=*/%v12128, /*on_false_vx=*/%v12126 (stack103)
        %v12132 = vadd.f32 %v12129, -3.0 (stack82)
        %v12136 = vsel /*vm=*/%vm12080, /*on_true_vy=*/%v12121, /*on_false_vx=*/%v12132 (stack72)
        %v12140 = vmul.f32 %v12117, %v12136 (stack83)
        %v12144 = vadd.f32 %v12113, %v12140 (stack82)
        %v12148 = vmul.f32 %v12144, %v12136 (stack83)
        %v12152 = vadd.f32 %v12109, %v12148 (stack82)
        %v12156 = vmul.f32 %v12152, %v12136 (stack83)
        %v12160 = vadd.f32 %v12105, %v12156 (stack82)
        %v12164 = vmul.f32 %v12160, %v12136 (stack83)
        %v12168 = vadd.f32 %v12101, %v12164 (stack82)
        %v12172 = vmul.f32 %v12168, %v12136 (stack83)
        %v12176 = vadd.f32 %v12097, %v12172 (stack82)
        %v12180 = vmul.f32 %v12176, %v12136 (stack83)
        %v12184 = vadd.f32 %v12093, %v12180 (stack82)
        %v12188 = vmul.f32 %v12184, %v12136 (stack83)
        %v12192 = vadd.f32 %v12089, %v12188 (stack82)
        %v12196 = vmul.f32 %v12192, %v12136 (stack83)
        %v12200 = vadd.f32 %v12085, %v12196 (stack82)
        %v12204 = vmul.f32 %v12200, %v12051 (stack83)
        %v12208 = vsel /*vm=*/%vm12056, /*on_true_vy=*/%v12061, /*on_false_vx=*/%v12204 (stack72)
        %v12212 = vmul.f32 %v12208, 1.4140625 (stack83)
        %s12214 = scalar_lea.vmem %s280, 12 [#allocation0] (stack107)
        %v12215 = vpack.c.bf16 0.0, %v12212 (stack104)
        %12216 = vst [vmem:[%s12214] sm:$0xf] /*vst_source=*/%v12215 (stack105)
        %v12219 = vadd.s32 %v894, %v11755 (stack65)
        %s12221 = smul.u32 128, %s27 (stack66)
        %v12222 = vlaneseq (stack67)
        %v12223 = vand.u32 %v12222, 127 (stack68)
        %v12224 = vstv %s12221 (stack69)
        %v12225 = vadd.s32 %v12223, %v12224 (stack70)
        %v12229 = vadd.s32 %v12219, %v12225 (stack65)
        %vm12233 = vcmp.lt.u32.totalorder %v12229, %v12219 (stack71)
        %vm12238 = vcmp.lt.u32.totalorder %v12219, %v894 (stack71)
        %v12243 = vadd.s32 %v881, %v11738 (stack65)
        %v12247 = vadd.s32 %v12243, 1 (stack65)
        %v12251 = vsel /*vm=*/%vm12238, /*on_true_vy=*/%v12247, /*on_false_vx=*/%v12243 (stack72)
        %v12255 = vadd.s32 %v12251, 1 (stack65)
        %v12259 = vsel /*vm=*/%vm12233, /*on_true_vy=*/%v12255, /*on_false_vx=*/%v12251 (stack72)
        %v12264 = vadd.s32 %v12259, %v10 (stack65)
        %v12268 = vadd.s32 %v12229, %v9 (stack65)
        %v12272 = vadd.s32 %v12264, %v12268 (stack65)
        %v12274 = vshll.u32 %v12268, 13 (stack73)
        %v12275 = vshrl.u32 %v12268, 19 (stack74)
        %v12276 = vor.u32 %v12274, %v12275 (stack75)
        %v12277 = vxor.u32 %v12272, %v12276 (stack76)
        %v12280 = vadd.s32 %v12272, %v12277 (stack65)
        %v12282 = vshll.u32 %v12277, 15 (stack73)
        %v12283 = vshrl.u32 %v12277, 17 (stack74)
        %v12284 = vor.u32 %v12282, %v12283 (stack75)
        %v12285 = vxor.u32 %v12280, %v12284 (stack76)
        %v12288 = vadd.s32 %v12280, %v12285 (stack65)
        %v12290 = vshll.u32 %v12285, 26 (stack73)
        %v12291 = vshrl.u32 %v12285, 6 (stack74)
        %v12292 = vor.u32 %v12290, %v12291 (stack75)
        %v12293 = vxor.u32 %v12288, %v12292 (stack76)
        %v12296 = vadd.s32 %v12288, %v12293 (stack65)
        %v12300 = vadd.s32 %v12296, %v9 (stack65)
        %v12302 = vshll.u32 %v12293, 6 (stack73)
        %v12303 = vshrl.u32 %v12293, 26 (stack74)
        %v12304 = vor.u32 %v12302, %v12303 (stack75)
        %v12305 = vxor.u32 %v12296, %v12304 (stack76)
        %v12308 = vadd.s32 %v12305, %v8 (stack65)
        %v12312 = vadd.s32 %v12308, 1 (stack65)
        %v12316 = vadd.s32 %v12300, %v12312 (stack65)
        %v12318 = vshll.u32 %v12312, 17 (stack73)
        %v12319 = vshrl.u32 %v12312, 15 (stack74)
        %v12320 = vor.u32 %v12318, %v12319 (stack75)
        %v12321 = vxor.u32 %v12316, %v12320 (stack76)
        %v12324 = vadd.s32 %v12316, %v12321 (stack65)
        %v12326 = vshll.u32 %v12321, 29 (stack73)
        %v12327 = vshrl.u32 %v12321, 3 (stack74)
        %v12328 = vor.u32 %v12326, %v12327 (stack75)
        %v12329 = vxor.u32 %v12324, %v12328 (stack76)
        %v12332 = vadd.s32 %v12324, %v12329 (stack65)
        %v12334 = vshll.u32 %v12329, 16 (stack73)
        %v12335 = vshrl.u32 %v12329, 16 (stack74)
        %v12336 = vor.u32 %v12334, %v12335 (stack75)
        %v12337 = vxor.u32 %v12332, %v12336 (stack76)
        %v12340 = vadd.s32 %v12332, %v12337 (stack65)
        %v12344 = vadd.s32 %v12340, %v8 (stack65)
        %v12346 = vshll.u32 %v12337, 24 (stack73)
        %v12347 = vshrl.u32 %v12337, 8 (stack74)
        %v12348 = vor.u32 %v12346, %v12347 (stack75)
        %v12349 = vxor.u32 %v12340, %v12348 (stack76)
        %v12352 = vadd.s32 %v12349, %v10 (stack65)
        %v12356 = vadd.s32 %v12352, 2 (stack65)
        %v12360 = vadd.s32 %v12344, %v12356 (stack65)
        %v12362 = vshll.u32 %v12356, 13 (stack73)
        %v12363 = vshrl.u32 %v12356, 19 (stack74)
        %v12364 = vor.u32 %v12362, %v12363 (stack75)
        %v12365 = vxor.u32 %v12360, %v12364 (stack76)
        %v12368 = vadd.s32 %v12360, %v12365 (stack65)
        %v12370 = vshll.u32 %v12365, 15 (stack73)
        %v12371 = vshrl.u32 %v12365, 17 (stack74)
        %v12372 = vor.u32 %v12370, %v12371 (stack75)
        %v12373 = vxor.u32 %v12368, %v12372 (stack76)
        %v12376 = vadd.s32 %v12368, %v12373 (stack65)
        %v12378 = vshll.u32 %v12373, 26 (stack73)
        %v12379 = vshrl.u32 %v12373, 6 (stack74)
        %v12380 = vor.u32 %v12378, %v12379 (stack75)
        %v12381 = vxor.u32 %v12376, %v12380 (stack76)
        %v12384 = vadd.s32 %v12376, %v12381 (stack65)
        %v12388 = vadd.s32 %v12384, %v10 (stack65)
        %v12390 = vshll.u32 %v12381, 6 (stack73)
        %v12391 = vshrl.u32 %v12381, 26 (stack74)
        %v12392 = vor.u32 %v12390, %v12391 (stack75)
        %v12393 = vxor.u32 %v12384, %v12392 (stack76)
        %v12396 = vadd.s32 %v12393, %v9 (stack65)
        %v12400 = vadd.s32 %v12396, 3 (stack65)
        %v12404 = vadd.s32 %v12388, %v12400 (stack65)
        %v12406 = vshll.u32 %v12400, 17 (stack73)
        %v12407 = vshrl.u32 %v12400, 15 (stack74)
        %v12408 = vor.u32 %v12406, %v12407 (stack75)
        %v12409 = vxor.u32 %v12404, %v12408 (stack76)
        %v12412 = vadd.s32 %v12404, %v12409 (stack65)
        %v12414 = vshll.u32 %v12409, 29 (stack73)
        %v12415 = vshrl.u32 %v12409, 3 (stack74)
        %v12416 = vor.u32 %v12414, %v12415 (stack75)
        %v12417 = vxor.u32 %v12412, %v12416 (stack76)
        %v12420 = vadd.s32 %v12412, %v12417 (stack65)
        %v12422 = vshll.u32 %v12417, 16 (stack73)
        %v12423 = vshrl.u32 %v12417, 16 (stack74)
        %v12424 = vor.u32 %v12422, %v12423 (stack75)
        %v12425 = vxor.u32 %v12420, %v12424 (stack76)
        %v12428 = vadd.s32 %v12420, %v12425 (stack65)
        %v12432 = vadd.s32 %v12428, %v9 (stack65)
        %v12434 = vshll.u32 %v12425, 24 (stack73)
        %v12435 = vshrl.u32 %v12425, 8 (stack74)
        %v12436 = vor.u32 %v12434, %v12435 (stack75)
        %v12437 = vxor.u32 %v12428, %v12436 (stack76)
        %v12440 = vadd.s32 %v12437, %v8 (stack65)
        %v12444 = vadd.s32 %v12440, 4 (stack65)
        %v12448 = vadd.s32 %v12432, %v12444 (stack65)
        %v12450 = vshll.u32 %v12444, 13 (stack73)
        %v12451 = vshrl.u32 %v12444, 19 (stack74)
        %v12452 = vor.u32 %v12450, %v12451 (stack75)
        %v12453 = vxor.u32 %v12448, %v12452 (stack76)
        %v12456 = vadd.s32 %v12448, %v12453 (stack65)
        %v12458 = vshll.u32 %v12453, 15 (stack73)
        %v12459 = vshrl.u32 %v12453, 17 (stack74)
        %v12460 = vor.u32 %v12458, %v12459 (stack75)
        %v12461 = vxor.u32 %v12456, %v12460 (stack76)
        %v12464 = vadd.s32 %v12456, %v12461 (stack65)
        %v12466 = vshll.u32 %v12461, 26 (stack73)
        %v12467 = vshrl.u32 %v12461, 6 (stack74)
        %v12468 = vor.u32 %v12466, %v12467 (stack75)
        %v12469 = vxor.u32 %v12464, %v12468 (stack76)
        %v12472 = vadd.s32 %v12464, %v12469 (stack65)
        %v12476 = vadd.s32 %v12472, %v8 (stack65)
        %v12478 = vshll.u32 %v12469, 6 (stack73)
        %v12479 = vshrl.u32 %v12469, 26 (stack74)
        %v12480 = vor.u32 %v12478, %v12479 (stack75)
        %v12481 = vxor.u32 %v12472, %v12480 (stack76)
        %v12484 = vadd.s32 %v12481, %v10 (stack65)
        %v12488 = vadd.s32 %v12484, 5 (stack65)
        %v12490 = vxor.u32 %v12476, %v12488 (stack76)
        %v12491 = vand.u32.u8 %v12490, 255 (stack77)
        %v12492 = vand.u32 %v12491, 65535 (stack78)
        %v12493 = vshrl.u32 %v12492, 1 (stack79)
        %v12494 = vor.u32 %v12493, 16256 (stack75)
        %v12495 = vand.u32.u16 %v12494, 65535 (stack80)
        %v12496 = vunpack.i.l.bf16 %v12495 (stack81)
        %v12500 = vadd.f32 %v12496, -1.0 (stack82)
        %v12504 = vmul.f32 %v12500, 2.0 (stack83)
        %v12508 = vadd.f32 %v12504, -0.99609375 (stack82)
        %v12512 = vmax.f32 -0.99609375, %v12508 (stack84)
        %v12514 = vand.u32 2147483647, %v12512 (stack85)
        %vm12517 = vcmp.eq.f32.partialorder %v12514, 1.0 (stack86)
        %v12522 = vmul.f32 %v12512, inf (stack83)
        %v12524 = vxor.u32 %v12512, 2147483648 (stack87)
        %v12527 = vmul.f32 %v12512, %v12524 (stack83)
        %v12529 = vadd.f32 %v12527, 1.0 (stack88)
        %v12530 = vlog2.pop %v12529 (stack89)
        %v12531 = vmul.f32 %v12530, 0.6931472 (stack90)
        %v12532 = vmul.f32 -0.5, %v12527 (stack91)
        %v12533 = vadd.f32 %v12532, 1.0 (stack92)
        %v12534 = vmul.f32 %v12533, %v12527 (stack93)
        %v12535 = vand.u32 2147483647, %v12527 (stack94)
        %vm12536 = vcmp.lt.f32.partialorder %v12535, 0.0004427343 (stack95)
        %v12537 = vsel /*vm=*/%vm12536, /*on_true_vy=*/%v12534, /*on_false_vx=*/%v12531 (stack96)
        %v12538 = vxor.u32 %v12537, 2147483648 (stack87)
        %vm12541 = vcmp.lt.f32.partialorder %v12538, 5.0 (stack86)
        %v12546 = vsel /*vm=*/%vm12541, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v12550 = vsel /*vm=*/%vm12541, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v12554 = vsel /*vm=*/%vm12541, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v12558 = vsel /*vm=*/%vm12541, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v12562 = vsel /*vm=*/%vm12541, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v12566 = vsel /*vm=*/%vm12541, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v12570 = vsel /*vm=*/%vm12541, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v12574 = vsel /*vm=*/%vm12541, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v12578 = vsel /*vm=*/%vm12541, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v12582 = vadd.f32 %v12538, -2.5 (stack82)
        %v12584 = vrsqrt.pop %v12538 (stack97)
        %v12585 = vmul.f32 %v12538, %v12584 (stack98)
        %vm12586 = vcmp.eq.f32.partialorder %v12538, inf (stack99)
        %v12587 = vsel /*vm=*/%vm12586, /*on_true_vy=*/%v12538, /*on_false_vx=*/%v12585 (stack100)
        %vm12588 = vcmp.eq.f32.partialorder %v12538, 0.0 (stack101)
        %v12589 = vand.u32 %v12538, 2147483648 (stack102)
        %v12590 = vsel /*vm=*/%vm12588, /*on_true_vy=*/%v12589, /*on_false_vx=*/%v12587 (stack103)
        %v12593 = vadd.f32 %v12590, -3.0 (stack82)
        %v12597 = vsel /*vm=*/%vm12541, /*on_true_vy=*/%v12582, /*on_false_vx=*/%v12593 (stack72)
        %v12601 = vmul.f32 %v12578, %v12597 (stack83)
        %v12605 = vadd.f32 %v12574, %v12601 (stack82)
        %v12609 = vmul.f32 %v12605, %v12597 (stack83)
        %v12613 = vadd.f32 %v12570, %v12609 (stack82)
        %v12617 = vmul.f32 %v12613, %v12597 (stack83)
        %v12621 = vadd.f32 %v12566, %v12617 (stack82)
        %v12625 = vmul.f32 %v12621, %v12597 (stack83)
        %v12629 = vadd.f32 %v12562, %v12625 (stack82)
        %v12633 = vmul.f32 %v12629, %v12597 (stack83)
        %v12637 = vadd.f32 %v12558, %v12633 (stack82)
        %v12641 = vmul.f32 %v12637, %v12597 (stack83)
        %v12645 = vadd.f32 %v12554, %v12641 (stack82)
        %v12649 = vmul.f32 %v12645, %v12597 (stack83)
        %v12653 = vadd.f32 %v12550, %v12649 (stack82)
        %v12657 = vmul.f32 %v12653, %v12597 (stack83)
        %v12661 = vadd.f32 %v12546, %v12657 (stack82)
        %v12665 = vmul.f32 %v12661, %v12512 (stack83)
        %v12669 = vsel /*vm=*/%vm12517, /*on_true_vy=*/%v12522, /*on_false_vx=*/%v12665 (stack72)
        %v12673 = vmul.f32 %v12669, 1.4140625 (stack83)
        %s12675 = scalar_lea.vmem %s280, 140 [#allocation0] (stack107)
        %v12676 = vpack.c.bf16 0.0, %v12673 (stack104)
        %12677 = vst [vmem:[%s12675] sm:$0xf] /*vst_source=*/%v12676 (stack105)
        %v12680 = vadd.s32 %v1381, %v11755 (stack65)
        %s12682 = smul.u32 128, %s27 (stack66)
        %v12683 = vlaneseq (stack67)
        %v12684 = vand.u32 %v12683, 127 (stack68)
        %v12685 = vstv %s12682 (stack69)
        %v12686 = vadd.s32 %v12684, %v12685 (stack70)
        %v12690 = vadd.s32 %v12680, %v12686 (stack65)
        %vm12694 = vcmp.lt.u32.totalorder %v12690, %v12680 (stack71)
        %vm12699 = vcmp.lt.u32.totalorder %v12680, %v1381 (stack71)
        %v12704 = vadd.s32 %v1368, %v11738 (stack65)
        %v12708 = vadd.s32 %v12704, 1 (stack65)
        %v12712 = vsel /*vm=*/%vm12699, /*on_true_vy=*/%v12708, /*on_false_vx=*/%v12704 (stack72)
        %v12716 = vadd.s32 %v12712, 1 (stack65)
        %v12720 = vsel /*vm=*/%vm12694, /*on_true_vy=*/%v12716, /*on_false_vx=*/%v12712 (stack72)
        %v12725 = vadd.s32 %v12720, %v10 (stack65)
        %v12729 = vadd.s32 %v12690, %v9 (stack65)
        %v12733 = vadd.s32 %v12725, %v12729 (stack65)
        %v12735 = vshll.u32 %v12729, 13 (stack73)
        %v12736 = vshrl.u32 %v12729, 19 (stack74)
        %v12737 = vor.u32 %v12735, %v12736 (stack75)
        %v12738 = vxor.u32 %v12733, %v12737 (stack76)
        %v12741 = vadd.s32 %v12733, %v12738 (stack65)
        %v12743 = vshll.u32 %v12738, 15 (stack73)
        %v12744 = vshrl.u32 %v12738, 17 (stack74)
        %v12745 = vor.u32 %v12743, %v12744 (stack75)
        %v12746 = vxor.u32 %v12741, %v12745 (stack76)
        %v12749 = vadd.s32 %v12741, %v12746 (stack65)
        %v12751 = vshll.u32 %v12746, 26 (stack73)
        %v12752 = vshrl.u32 %v12746, 6 (stack74)
        %v12753 = vor.u32 %v12751, %v12752 (stack75)
        %v12754 = vxor.u32 %v12749, %v12753 (stack76)
        %v12757 = vadd.s32 %v12749, %v12754 (stack65)
        %v12761 = vadd.s32 %v12757, %v9 (stack65)
        %v12763 = vshll.u32 %v12754, 6 (stack73)
        %v12764 = vshrl.u32 %v12754, 26 (stack74)
        %v12765 = vor.u32 %v12763, %v12764 (stack75)
        %v12766 = vxor.u32 %v12757, %v12765 (stack76)
        %v12769 = vadd.s32 %v12766, %v8 (stack65)
        %v12773 = vadd.s32 %v12769, 1 (stack65)
        %v12777 = vadd.s32 %v12761, %v12773 (stack65)
        %v12779 = vshll.u32 %v12773, 17 (stack73)
        %v12780 = vshrl.u32 %v12773, 15 (stack74)
        %v12781 = vor.u32 %v12779, %v12780 (stack75)
        %v12782 = vxor.u32 %v12777, %v12781 (stack76)
        %v12785 = vadd.s32 %v12777, %v12782 (stack65)
        %v12787 = vshll.u32 %v12782, 29 (stack73)
        %v12788 = vshrl.u32 %v12782, 3 (stack74)
        %v12789 = vor.u32 %v12787, %v12788 (stack75)
        %v12790 = vxor.u32 %v12785, %v12789 (stack76)
        %v12793 = vadd.s32 %v12785, %v12790 (stack65)
        %v12795 = vshll.u32 %v12790, 16 (stack73)
        %v12796 = vshrl.u32 %v12790, 16 (stack74)
        %v12797 = vor.u32 %v12795, %v12796 (stack75)
        %v12798 = vxor.u32 %v12793, %v12797 (stack76)
        %v12801 = vadd.s32 %v12793, %v12798 (stack65)
        %v12805 = vadd.s32 %v12801, %v8 (stack65)
        %v12807 = vshll.u32 %v12798, 24 (stack73)
        %v12808 = vshrl.u32 %v12798, 8 (stack74)
        %v12809 = vor.u32 %v12807, %v12808 (stack75)
        %v12810 = vxor.u32 %v12801, %v12809 (stack76)
        %v12813 = vadd.s32 %v12810, %v10 (stack65)
        %v12817 = vadd.s32 %v12813, 2 (stack65)
        %v12821 = vadd.s32 %v12805, %v12817 (stack65)
        %v12823 = vshll.u32 %v12817, 13 (stack73)
        %v12824 = vshrl.u32 %v12817, 19 (stack74)
        %v12825 = vor.u32 %v12823, %v12824 (stack75)
        %v12826 = vxor.u32 %v12821, %v12825 (stack76)
        %v12829 = vadd.s32 %v12821, %v12826 (stack65)
        %v12831 = vshll.u32 %v12826, 15 (stack73)
        %v12832 = vshrl.u32 %v12826, 17 (stack74)
        %v12833 = vor.u32 %v12831, %v12832 (stack75)
        %v12834 = vxor.u32 %v12829, %v12833 (stack76)
        %v12837 = vadd.s32 %v12829, %v12834 (stack65)
        %v12839 = vshll.u32 %v12834, 26 (stack73)
        %v12840 = vshrl.u32 %v12834, 6 (stack74)
        %v12841 = vor.u32 %v12839, %v12840 (stack75)
        %v12842 = vxor.u32 %v12837, %v12841 (stack76)
        %v12845 = vadd.s32 %v12837, %v12842 (stack65)
        %v12849 = vadd.s32 %v12845, %v10 (stack65)
        %v12851 = vshll.u32 %v12842, 6 (stack73)
        %v12852 = vshrl.u32 %v12842, 26 (stack74)
        %v12853 = vor.u32 %v12851, %v12852 (stack75)
        %v12854 = vxor.u32 %v12845, %v12853 (stack76)
        %v12857 = vadd.s32 %v12854, %v9 (stack65)
        %v12861 = vadd.s32 %v12857, 3 (stack65)
        %v12865 = vadd.s32 %v12849, %v12861 (stack65)
        %v12867 = vshll.u32 %v12861, 17 (stack73)
        %v12868 = vshrl.u32 %v12861, 15 (stack74)
        %v12869 = vor.u32 %v12867, %v12868 (stack75)
        %v12870 = vxor.u32 %v12865, %v12869 (stack76)
        %v12873 = vadd.s32 %v12865, %v12870 (stack65)
        %v12875 = vshll.u32 %v12870, 29 (stack73)
        %v12876 = vshrl.u32 %v12870, 3 (stack74)
        %v12877 = vor.u32 %v12875, %v12876 (stack75)
        %v12878 = vxor.u32 %v12873, %v12877 (stack76)
        %v12881 = vadd.s32 %v12873, %v12878 (stack65)
        %v12883 = vshll.u32 %v12878, 16 (stack73)
        %v12884 = vshrl.u32 %v12878, 16 (stack74)
        %v12885 = vor.u32 %v12883, %v12884 (stack75)
        %v12886 = vxor.u32 %v12881, %v12885 (stack76)
        %v12889 = vadd.s32 %v12881, %v12886 (stack65)
        %v12893 = vadd.s32 %v12889, %v9 (stack65)
        %v12895 = vshll.u32 %v12886, 24 (stack73)
        %v12896 = vshrl.u32 %v12886, 8 (stack74)
        %v12897 = vor.u32 %v12895, %v12896 (stack75)
        %v12898 = vxor.u32 %v12889, %v12897 (stack76)
        %v12901 = vadd.s32 %v12898, %v8 (stack65)
        %v12905 = vadd.s32 %v12901, 4 (stack65)
        %v12909 = vadd.s32 %v12893, %v12905 (stack65)
        %v12911 = vshll.u32 %v12905, 13 (stack73)
        %v12912 = vshrl.u32 %v12905, 19 (stack74)
        %v12913 = vor.u32 %v12911, %v12912 (stack75)
        %v12914 = vxor.u32 %v12909, %v12913 (stack76)
        %v12917 = vadd.s32 %v12909, %v12914 (stack65)
        %v12919 = vshll.u32 %v12914, 15 (stack73)
        %v12920 = vshrl.u32 %v12914, 17 (stack74)
        %v12921 = vor.u32 %v12919, %v12920 (stack75)
        %v12922 = vxor.u32 %v12917, %v12921 (stack76)
        %v12925 = vadd.s32 %v12917, %v12922 (stack65)
        %v12927 = vshll.u32 %v12922, 26 (stack73)
        %v12928 = vshrl.u32 %v12922, 6 (stack74)
        %v12929 = vor.u32 %v12927, %v12928 (stack75)
        %v12930 = vxor.u32 %v12925, %v12929 (stack76)
        %v12933 = vadd.s32 %v12925, %v12930 (stack65)
        %v12937 = vadd.s32 %v12933, %v8 (stack65)
        %v12939 = vshll.u32 %v12930, 6 (stack73)
        %v12940 = vshrl.u32 %v12930, 26 (stack74)
        %v12941 = vor.u32 %v12939, %v12940 (stack75)
        %v12942 = vxor.u32 %v12933, %v12941 (stack76)
        %v12945 = vadd.s32 %v12942, %v10 (stack65)
        %v12949 = vadd.s32 %v12945, 5 (stack65)
        %v12951 = vxor.u32 %v12937, %v12949 (stack76)
        %v12952 = vand.u32.u8 %v12951, 255 (stack77)
        %v12953 = vand.u32 %v12952, 65535 (stack78)
        %v12954 = vshrl.u32 %v12953, 1 (stack79)
        %v12955 = vor.u32 %v12954, 16256 (stack75)
        %v12956 = vand.u32.u16 %v12955, 65535 (stack80)
        %v12957 = vunpack.i.l.bf16 %v12956 (stack81)
        %v12961 = vadd.f32 %v12957, -1.0 (stack82)
        %v12965 = vmul.f32 %v12961, 2.0 (stack83)
        %v12969 = vadd.f32 %v12965, -0.99609375 (stack82)
        %v12973 = vmax.f32 -0.99609375, %v12969 (stack84)
        %v12975 = vand.u32 2147483647, %v12973 (stack85)
        %vm12978 = vcmp.eq.f32.partialorder %v12975, 1.0 (stack86)
        %v12983 = vmul.f32 %v12973, inf (stack83)
        %v12985 = vxor.u32 %v12973, 2147483648 (stack87)
        %v12988 = vmul.f32 %v12973, %v12985 (stack83)
        %v12990 = vadd.f32 %v12988, 1.0 (stack88)
        %v12991 = vlog2.pop %v12990 (stack89)
        %v12992 = vmul.f32 %v12991, 0.6931472 (stack90)
        %v12993 = vmul.f32 -0.5, %v12988 (stack91)
        %v12994 = vadd.f32 %v12993, 1.0 (stack92)
        %v12995 = vmul.f32 %v12994, %v12988 (stack93)
        %v12996 = vand.u32 2147483647, %v12988 (stack94)
        %vm12997 = vcmp.lt.f32.partialorder %v12996, 0.0004427343 (stack95)
        %v12998 = vsel /*vm=*/%vm12997, /*on_true_vy=*/%v12995, /*on_false_vx=*/%v12992 (stack96)
        %v12999 = vxor.u32 %v12998, 2147483648 (stack87)
        %vm13002 = vcmp.lt.f32.partialorder %v12999, 5.0 (stack86)
        %v13007 = vsel /*vm=*/%vm13002, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v13011 = vsel /*vm=*/%vm13002, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v13015 = vsel /*vm=*/%vm13002, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v13019 = vsel /*vm=*/%vm13002, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v13023 = vsel /*vm=*/%vm13002, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v13027 = vsel /*vm=*/%vm13002, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v13031 = vsel /*vm=*/%vm13002, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v13035 = vsel /*vm=*/%vm13002, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v13039 = vsel /*vm=*/%vm13002, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v13043 = vadd.f32 %v12999, -2.5 (stack82)
        %v13045 = vrsqrt.pop %v12999 (stack97)
        %v13046 = vmul.f32 %v12999, %v13045 (stack98)
        %vm13047 = vcmp.eq.f32.partialorder %v12999, inf (stack99)
        %v13048 = vsel /*vm=*/%vm13047, /*on_true_vy=*/%v12999, /*on_false_vx=*/%v13046 (stack100)
        %vm13049 = vcmp.eq.f32.partialorder %v12999, 0.0 (stack101)
        %v13050 = vand.u32 %v12999, 2147483648 (stack102)
        %v13051 = vsel /*vm=*/%vm13049, /*on_true_vy=*/%v13050, /*on_false_vx=*/%v13048 (stack103)
        %v13054 = vadd.f32 %v13051, -3.0 (stack82)
        %v13058 = vsel /*vm=*/%vm13002, /*on_true_vy=*/%v13043, /*on_false_vx=*/%v13054 (stack72)
        %v13062 = vmul.f32 %v13039, %v13058 (stack83)
        %v13066 = vadd.f32 %v13035, %v13062 (stack82)
        %v13070 = vmul.f32 %v13066, %v13058 (stack83)
        %v13074 = vadd.f32 %v13031, %v13070 (stack82)
        %v13078 = vmul.f32 %v13074, %v13058 (stack83)
        %v13082 = vadd.f32 %v13027, %v13078 (stack82)
        %v13086 = vmul.f32 %v13082, %v13058 (stack83)
        %v13090 = vadd.f32 %v13023, %v13086 (stack82)
        %v13094 = vmul.f32 %v13090, %v13058 (stack83)
        %v13098 = vadd.f32 %v13019, %v13094 (stack82)
        %v13102 = vmul.f32 %v13098, %v13058 (stack83)
        %v13106 = vadd.f32 %v13015, %v13102 (stack82)
        %v13110 = vmul.f32 %v13106, %v13058 (stack83)
        %v13114 = vadd.f32 %v13011, %v13110 (stack82)
        %v13118 = vmul.f32 %v13114, %v13058 (stack83)
        %v13122 = vadd.f32 %v13007, %v13118 (stack82)
        %v13126 = vmul.f32 %v13122, %v12973 (stack83)
        %v13130 = vsel /*vm=*/%vm12978, /*on_true_vy=*/%v12983, /*on_false_vx=*/%v13126 (stack72)
        %v13134 = vmul.f32 %v13130, 1.4140625 (stack83)
        %s13136 = scalar_lea.vmem %s280, 268 [#allocation0] (stack107)
        %v13137 = vpack.c.bf16 0.0, %v13134 (stack104)
        %13138 = vst [vmem:[%s13136] sm:$0xf] /*vst_source=*/%v13137 (stack105)
        %v13141 = vadd.s32 %v1868, %v11755 (stack65)
        %s13143 = smul.u32 128, %s27 (stack66)
        %v13144 = vlaneseq (stack67)
        %v13145 = vand.u32 %v13144, 127 (stack68)
        %v13146 = vstv %s13143 (stack69)
        %v13147 = vadd.s32 %v13145, %v13146 (stack70)
        %v13151 = vadd.s32 %v13141, %v13147 (stack65)
        %vm13155 = vcmp.lt.u32.totalorder %v13151, %v13141 (stack71)
        %vm13160 = vcmp.lt.u32.totalorder %v13141, %v1868 (stack71)
        %v13165 = vadd.s32 %v1855, %v11738 (stack65)
        %v13169 = vadd.s32 %v13165, 1 (stack65)
        %v13173 = vsel /*vm=*/%vm13160, /*on_true_vy=*/%v13169, /*on_false_vx=*/%v13165 (stack72)
        %v13177 = vadd.s32 %v13173, 1 (stack65)
        %v13181 = vsel /*vm=*/%vm13155, /*on_true_vy=*/%v13177, /*on_false_vx=*/%v13173 (stack72)
        %v13186 = vadd.s32 %v13181, %v10 (stack65)
        %v13190 = vadd.s32 %v13151, %v9 (stack65)
        %v13194 = vadd.s32 %v13186, %v13190 (stack65)
        %v13196 = vshll.u32 %v13190, 13 (stack73)
        %v13197 = vshrl.u32 %v13190, 19 (stack74)
        %v13198 = vor.u32 %v13196, %v13197 (stack75)
        %v13199 = vxor.u32 %v13194, %v13198 (stack76)
        %v13202 = vadd.s32 %v13194, %v13199 (stack65)
        %v13204 = vshll.u32 %v13199, 15 (stack73)
        %v13205 = vshrl.u32 %v13199, 17 (stack74)
        %v13206 = vor.u32 %v13204, %v13205 (stack75)
        %v13207 = vxor.u32 %v13202, %v13206 (stack76)
        %v13210 = vadd.s32 %v13202, %v13207 (stack65)
        %v13212 = vshll.u32 %v13207, 26 (stack73)
        %v13213 = vshrl.u32 %v13207, 6 (stack74)
        %v13214 = vor.u32 %v13212, %v13213 (stack75)
        %v13215 = vxor.u32 %v13210, %v13214 (stack76)
        %v13218 = vadd.s32 %v13210, %v13215 (stack65)
        %v13222 = vadd.s32 %v13218, %v9 (stack65)
        %v13224 = vshll.u32 %v13215, 6 (stack73)
        %v13225 = vshrl.u32 %v13215, 26 (stack74)
        %v13226 = vor.u32 %v13224, %v13225 (stack75)
        %v13227 = vxor.u32 %v13218, %v13226 (stack76)
        %v13230 = vadd.s32 %v13227, %v8 (stack65)
        %v13234 = vadd.s32 %v13230, 1 (stack65)
        %v13238 = vadd.s32 %v13222, %v13234 (stack65)
        %v13240 = vshll.u32 %v13234, 17 (stack73)
        %v13241 = vshrl.u32 %v13234, 15 (stack74)
        %v13242 = vor.u32 %v13240, %v13241 (stack75)
        %v13243 = vxor.u32 %v13238, %v13242 (stack76)
        %v13246 = vadd.s32 %v13238, %v13243 (stack65)
        %v13248 = vshll.u32 %v13243, 29 (stack73)
        %v13249 = vshrl.u32 %v13243, 3 (stack74)
        %v13250 = vor.u32 %v13248, %v13249 (stack75)
        %v13251 = vxor.u32 %v13246, %v13250 (stack76)
        %v13254 = vadd.s32 %v13246, %v13251 (stack65)
        %v13256 = vshll.u32 %v13251, 16 (stack73)
        %v13257 = vshrl.u32 %v13251, 16 (stack74)
        %v13258 = vor.u32 %v13256, %v13257 (stack75)
        %v13259 = vxor.u32 %v13254, %v13258 (stack76)
        %v13262 = vadd.s32 %v13254, %v13259 (stack65)
        %v13266 = vadd.s32 %v13262, %v8 (stack65)
        %v13268 = vshll.u32 %v13259, 24 (stack73)
        %v13269 = vshrl.u32 %v13259, 8 (stack74)
        %v13270 = vor.u32 %v13268, %v13269 (stack75)
        %v13271 = vxor.u32 %v13262, %v13270 (stack76)
        %v13274 = vadd.s32 %v13271, %v10 (stack65)
        %v13278 = vadd.s32 %v13274, 2 (stack65)
        %v13282 = vadd.s32 %v13266, %v13278 (stack65)
        %v13284 = vshll.u32 %v13278, 13 (stack73)
        %v13285 = vshrl.u32 %v13278, 19 (stack74)
        %v13286 = vor.u32 %v13284, %v13285 (stack75)
        %v13287 = vxor.u32 %v13282, %v13286 (stack76)
        %v13290 = vadd.s32 %v13282, %v13287 (stack65)
        %v13292 = vshll.u32 %v13287, 15 (stack73)
        %v13293 = vshrl.u32 %v13287, 17 (stack74)
        %v13294 = vor.u32 %v13292, %v13293 (stack75)
        %v13295 = vxor.u32 %v13290, %v13294 (stack76)
        %v13298 = vadd.s32 %v13290, %v13295 (stack65)
        %v13300 = vshll.u32 %v13295, 26 (stack73)
        %v13301 = vshrl.u32 %v13295, 6 (stack74)
        %v13302 = vor.u32 %v13300, %v13301 (stack75)
        %v13303 = vxor.u32 %v13298, %v13302 (stack76)
        %v13306 = vadd.s32 %v13298, %v13303 (stack65)
        %v13310 = vadd.s32 %v13306, %v10 (stack65)
        %v13312 = vshll.u32 %v13303, 6 (stack73)
        %v13313 = vshrl.u32 %v13303, 26 (stack74)
        %v13314 = vor.u32 %v13312, %v13313 (stack75)
        %v13315 = vxor.u32 %v13306, %v13314 (stack76)
        %v13318 = vadd.s32 %v13315, %v9 (stack65)
        %v13322 = vadd.s32 %v13318, 3 (stack65)
        %v13326 = vadd.s32 %v13310, %v13322 (stack65)
        %v13328 = vshll.u32 %v13322, 17 (stack73)
        %v13329 = vshrl.u32 %v13322, 15 (stack74)
        %v13330 = vor.u32 %v13328, %v13329 (stack75)
        %v13331 = vxor.u32 %v13326, %v13330 (stack76)
        %v13334 = vadd.s32 %v13326, %v13331 (stack65)
        %v13336 = vshll.u32 %v13331, 29 (stack73)
        %v13337 = vshrl.u32 %v13331, 3 (stack74)
        %v13338 = vor.u32 %v13336, %v13337 (stack75)
        %v13339 = vxor.u32 %v13334, %v13338 (stack76)
        %v13342 = vadd.s32 %v13334, %v13339 (stack65)
        %v13344 = vshll.u32 %v13339, 16 (stack73)
        %v13345 = vshrl.u32 %v13339, 16 (stack74)
        %v13346 = vor.u32 %v13344, %v13345 (stack75)
        %v13347 = vxor.u32 %v13342, %v13346 (stack76)
        %v13350 = vadd.s32 %v13342, %v13347 (stack65)
        %v13354 = vadd.s32 %v13350, %v9 (stack65)
        %v13356 = vshll.u32 %v13347, 24 (stack73)
        %v13357 = vshrl.u32 %v13347, 8 (stack74)
        %v13358 = vor.u32 %v13356, %v13357 (stack75)
        %v13359 = vxor.u32 %v13350, %v13358 (stack76)
        %v13362 = vadd.s32 %v13359, %v8 (stack65)
        %v13366 = vadd.s32 %v13362, 4 (stack65)
        %v13370 = vadd.s32 %v13354, %v13366 (stack65)
        %v13372 = vshll.u32 %v13366, 13 (stack73)
        %v13373 = vshrl.u32 %v13366, 19 (stack74)
        %v13374 = vor.u32 %v13372, %v13373 (stack75)
        %v13375 = vxor.u32 %v13370, %v13374 (stack76)
        %v13378 = vadd.s32 %v13370, %v13375 (stack65)
        %v13380 = vshll.u32 %v13375, 15 (stack73)
        %v13381 = vshrl.u32 %v13375, 17 (stack74)
        %v13382 = vor.u32 %v13380, %v13381 (stack75)
        %v13383 = vxor.u32 %v13378, %v13382 (stack76)
        %v13386 = vadd.s32 %v13378, %v13383 (stack65)
        %v13388 = vshll.u32 %v13383, 26 (stack73)
        %v13389 = vshrl.u32 %v13383, 6 (stack74)
        %v13390 = vor.u32 %v13388, %v13389 (stack75)
        %v13391 = vxor.u32 %v13386, %v13390 (stack76)
        %v13394 = vadd.s32 %v13386, %v13391 (stack65)
        %v13398 = vadd.s32 %v13394, %v8 (stack65)
        %v13400 = vshll.u32 %v13391, 6 (stack73)
        %v13401 = vshrl.u32 %v13391, 26 (stack74)
        %v13402 = vor.u32 %v13400, %v13401 (stack75)
        %v13403 = vxor.u32 %v13394, %v13402 (stack76)
        %v13406 = vadd.s32 %v13403, %v10 (stack65)
        %v13410 = vadd.s32 %v13406, 5 (stack65)
        %v13412 = vxor.u32 %v13398, %v13410 (stack76)
        %v13413 = vand.u32.u8 %v13412, 255 (stack77)
        %v13414 = vand.u32 %v13413, 65535 (stack78)
        %v13415 = vshrl.u32 %v13414, 1 (stack79)
        %v13416 = vor.u32 %v13415, 16256 (stack75)
        %v13417 = vand.u32.u16 %v13416, 65535 (stack80)
        %v13418 = vunpack.i.l.bf16 %v13417 (stack81)
        %v13422 = vadd.f32 %v13418, -1.0 (stack82)
        %v13426 = vmul.f32 %v13422, 2.0 (stack83)
        %v13430 = vadd.f32 %v13426, -0.99609375 (stack82)
        %v13434 = vmax.f32 -0.99609375, %v13430 (stack84)
        %v13436 = vand.u32 2147483647, %v13434 (stack85)
        %vm13439 = vcmp.eq.f32.partialorder %v13436, 1.0 (stack86)
        %v13444 = vmul.f32 %v13434, inf (stack83)
        %v13446 = vxor.u32 %v13434, 2147483648 (stack87)
        %v13449 = vmul.f32 %v13434, %v13446 (stack83)
        %v13451 = vadd.f32 %v13449, 1.0 (stack88)
        %v13452 = vlog2.pop %v13451 (stack89)
        %v13453 = vmul.f32 %v13452, 0.6931472 (stack90)
        %v13454 = vmul.f32 -0.5, %v13449 (stack91)
        %v13455 = vadd.f32 %v13454, 1.0 (stack92)
        %v13456 = vmul.f32 %v13455, %v13449 (stack93)
        %v13457 = vand.u32 2147483647, %v13449 (stack94)
        %vm13458 = vcmp.lt.f32.partialorder %v13457, 0.0004427343 (stack95)
        %v13459 = vsel /*vm=*/%vm13458, /*on_true_vy=*/%v13456, /*on_false_vx=*/%v13453 (stack96)
        %v13460 = vxor.u32 %v13459, 2147483648 (stack87)
        %vm13463 = vcmp.lt.f32.partialorder %v13460, 5.0 (stack86)
        %v13468 = vsel /*vm=*/%vm13463, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v13472 = vsel /*vm=*/%vm13463, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v13476 = vsel /*vm=*/%vm13463, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v13480 = vsel /*vm=*/%vm13463, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v13484 = vsel /*vm=*/%vm13463, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v13488 = vsel /*vm=*/%vm13463, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v13492 = vsel /*vm=*/%vm13463, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v13496 = vsel /*vm=*/%vm13463, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v13500 = vsel /*vm=*/%vm13463, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v13504 = vadd.f32 %v13460, -2.5 (stack82)
        %v13506 = vrsqrt.pop %v13460 (stack97)
        %v13507 = vmul.f32 %v13460, %v13506 (stack98)
        %vm13508 = vcmp.eq.f32.partialorder %v13460, inf (stack99)
        %v13509 = vsel /*vm=*/%vm13508, /*on_true_vy=*/%v13460, /*on_false_vx=*/%v13507 (stack100)
        %vm13510 = vcmp.eq.f32.partialorder %v13460, 0.0 (stack101)
        %v13511 = vand.u32 %v13460, 2147483648 (stack102)
        %v13512 = vsel /*vm=*/%vm13510, /*on_true_vy=*/%v13511, /*on_false_vx=*/%v13509 (stack103)
        %v13515 = vadd.f32 %v13512, -3.0 (stack82)
        %v13519 = vsel /*vm=*/%vm13463, /*on_true_vy=*/%v13504, /*on_false_vx=*/%v13515 (stack72)
        %v13523 = vmul.f32 %v13500, %v13519 (stack83)
        %v13527 = vadd.f32 %v13496, %v13523 (stack82)
        %v13531 = vmul.f32 %v13527, %v13519 (stack83)
        %v13535 = vadd.f32 %v13492, %v13531 (stack82)
        %v13539 = vmul.f32 %v13535, %v13519 (stack83)
        %v13543 = vadd.f32 %v13488, %v13539 (stack82)
        %v13547 = vmul.f32 %v13543, %v13519 (stack83)
        %v13551 = vadd.f32 %v13484, %v13547 (stack82)
        %v13555 = vmul.f32 %v13551, %v13519 (stack83)
        %v13559 = vadd.f32 %v13480, %v13555 (stack82)
        %v13563 = vmul.f32 %v13559, %v13519 (stack83)
        %v13567 = vadd.f32 %v13476, %v13563 (stack82)
        %v13571 = vmul.f32 %v13567, %v13519 (stack83)
        %v13575 = vadd.f32 %v13472, %v13571 (stack82)
        %v13579 = vmul.f32 %v13575, %v13519 (stack83)
        %v13583 = vadd.f32 %v13468, %v13579 (stack82)
        %v13587 = vmul.f32 %v13583, %v13434 (stack83)
        %v13591 = vsel /*vm=*/%vm13439, /*on_true_vy=*/%v13444, /*on_false_vx=*/%v13587 (stack72)
        %v13595 = vmul.f32 %v13591, 1.4140625 (stack83)
        %s13597 = scalar_lea.vmem %s280, 396 [#allocation0] (stack107)
        %v13598 = vpack.c.bf16 0.0, %v13595 (stack104)
        %13599 = vst [vmem:[%s13597] sm:$0xf] /*vst_source=*/%v13598 (stack105)
        %v13602 = vadd.s32 %v2355, %v11755 (stack65)
        %s13604 = smul.u32 128, %s27 (stack66)
        %v13605 = vlaneseq (stack67)
        %v13606 = vand.u32 %v13605, 127 (stack68)
        %v13607 = vstv %s13604 (stack69)
        %v13608 = vadd.s32 %v13606, %v13607 (stack70)
        %v13612 = vadd.s32 %v13602, %v13608 (stack65)
        %vm13616 = vcmp.lt.u32.totalorder %v13612, %v13602 (stack71)
        %vm13621 = vcmp.lt.u32.totalorder %v13602, %v2355 (stack71)
        %v13626 = vadd.s32 %v2342, %v11738 (stack65)
        %v13630 = vadd.s32 %v13626, 1 (stack65)
        %v13634 = vsel /*vm=*/%vm13621, /*on_true_vy=*/%v13630, /*on_false_vx=*/%v13626 (stack72)
        %v13638 = vadd.s32 %v13634, 1 (stack65)
        %v13642 = vsel /*vm=*/%vm13616, /*on_true_vy=*/%v13638, /*on_false_vx=*/%v13634 (stack72)
        %v13647 = vadd.s32 %v13642, %v10 (stack65)
        %v13651 = vadd.s32 %v13612, %v9 (stack65)
        %v13655 = vadd.s32 %v13647, %v13651 (stack65)
        %v13657 = vshll.u32 %v13651, 13 (stack73)
        %v13658 = vshrl.u32 %v13651, 19 (stack74)
        %v13659 = vor.u32 %v13657, %v13658 (stack75)
        %v13660 = vxor.u32 %v13655, %v13659 (stack76)
        %v13663 = vadd.s32 %v13655, %v13660 (stack65)
        %v13665 = vshll.u32 %v13660, 15 (stack73)
        %v13666 = vshrl.u32 %v13660, 17 (stack74)
        %v13667 = vor.u32 %v13665, %v13666 (stack75)
        %v13668 = vxor.u32 %v13663, %v13667 (stack76)
        %v13671 = vadd.s32 %v13663, %v13668 (stack65)
        %v13673 = vshll.u32 %v13668, 26 (stack73)
        %v13674 = vshrl.u32 %v13668, 6 (stack74)
        %v13675 = vor.u32 %v13673, %v13674 (stack75)
        %v13676 = vxor.u32 %v13671, %v13675 (stack76)
        %v13679 = vadd.s32 %v13671, %v13676 (stack65)
        %v13683 = vadd.s32 %v13679, %v9 (stack65)
        %v13685 = vshll.u32 %v13676, 6 (stack73)
        %v13686 = vshrl.u32 %v13676, 26 (stack74)
        %v13687 = vor.u32 %v13685, %v13686 (stack75)
        %v13688 = vxor.u32 %v13679, %v13687 (stack76)
        %v13691 = vadd.s32 %v13688, %v8 (stack65)
        %v13695 = vadd.s32 %v13691, 1 (stack65)
        %v13699 = vadd.s32 %v13683, %v13695 (stack65)
        %v13701 = vshll.u32 %v13695, 17 (stack73)
        %v13702 = vshrl.u32 %v13695, 15 (stack74)
        %v13703 = vor.u32 %v13701, %v13702 (stack75)
        %v13704 = vxor.u32 %v13699, %v13703 (stack76)
        %v13707 = vadd.s32 %v13699, %v13704 (stack65)
        %v13709 = vshll.u32 %v13704, 29 (stack73)
        %v13710 = vshrl.u32 %v13704, 3 (stack74)
        %v13711 = vor.u32 %v13709, %v13710 (stack75)
        %v13712 = vxor.u32 %v13707, %v13711 (stack76)
        %v13715 = vadd.s32 %v13707, %v13712 (stack65)
        %v13717 = vshll.u32 %v13712, 16 (stack73)
        %v13718 = vshrl.u32 %v13712, 16 (stack74)
        %v13719 = vor.u32 %v13717, %v13718 (stack75)
        %v13720 = vxor.u32 %v13715, %v13719 (stack76)
        %v13723 = vadd.s32 %v13715, %v13720 (stack65)
        %v13727 = vadd.s32 %v13723, %v8 (stack65)
        %v13729 = vshll.u32 %v13720, 24 (stack73)
        %v13730 = vshrl.u32 %v13720, 8 (stack74)
        %v13731 = vor.u32 %v13729, %v13730 (stack75)
        %v13732 = vxor.u32 %v13723, %v13731 (stack76)
        %v13735 = vadd.s32 %v13732, %v10 (stack65)
        %v13739 = vadd.s32 %v13735, 2 (stack65)
        %v13743 = vadd.s32 %v13727, %v13739 (stack65)
        %v13745 = vshll.u32 %v13739, 13 (stack73)
        %v13746 = vshrl.u32 %v13739, 19 (stack74)
        %v13747 = vor.u32 %v13745, %v13746 (stack75)
        %v13748 = vxor.u32 %v13743, %v13747 (stack76)
        %v13751 = vadd.s32 %v13743, %v13748 (stack65)
        %v13753 = vshll.u32 %v13748, 15 (stack73)
        %v13754 = vshrl.u32 %v13748, 17 (stack74)
        %v13755 = vor.u32 %v13753, %v13754 (stack75)
        %v13756 = vxor.u32 %v13751, %v13755 (stack76)
        %v13759 = vadd.s32 %v13751, %v13756 (stack65)
        %v13761 = vshll.u32 %v13756, 26 (stack73)
        %v13762 = vshrl.u32 %v13756, 6 (stack74)
        %v13763 = vor.u32 %v13761, %v13762 (stack75)
        %v13764 = vxor.u32 %v13759, %v13763 (stack76)
        %v13767 = vadd.s32 %v13759, %v13764 (stack65)
        %v13771 = vadd.s32 %v13767, %v10 (stack65)
        %v13773 = vshll.u32 %v13764, 6 (stack73)
        %v13774 = vshrl.u32 %v13764, 26 (stack74)
        %v13775 = vor.u32 %v13773, %v13774 (stack75)
        %v13776 = vxor.u32 %v13767, %v13775 (stack76)
        %v13779 = vadd.s32 %v13776, %v9 (stack65)
        %v13783 = vadd.s32 %v13779, 3 (stack65)
        %v13787 = vadd.s32 %v13771, %v13783 (stack65)
        %v13789 = vshll.u32 %v13783, 17 (stack73)
        %v13790 = vshrl.u32 %v13783, 15 (stack74)
        %v13791 = vor.u32 %v13789, %v13790 (stack75)
        %v13792 = vxor.u32 %v13787, %v13791 (stack76)
        %v13795 = vadd.s32 %v13787, %v13792 (stack65)
        %v13797 = vshll.u32 %v13792, 29 (stack73)
        %v13798 = vshrl.u32 %v13792, 3 (stack74)
        %v13799 = vor.u32 %v13797, %v13798 (stack75)
        %v13800 = vxor.u32 %v13795, %v13799 (stack76)
        %v13803 = vadd.s32 %v13795, %v13800 (stack65)
        %v13805 = vshll.u32 %v13800, 16 (stack73)
        %v13806 = vshrl.u32 %v13800, 16 (stack74)
        %v13807 = vor.u32 %v13805, %v13806 (stack75)
        %v13808 = vxor.u32 %v13803, %v13807 (stack76)
        %v13811 = vadd.s32 %v13803, %v13808 (stack65)
        %v13815 = vadd.s32 %v13811, %v9 (stack65)
        %v13817 = vshll.u32 %v13808, 24 (stack73)
        %v13818 = vshrl.u32 %v13808, 8 (stack74)
        %v13819 = vor.u32 %v13817, %v13818 (stack75)
        %v13820 = vxor.u32 %v13811, %v13819 (stack76)
        %v13823 = vadd.s32 %v13820, %v8 (stack65)
        %v13827 = vadd.s32 %v13823, 4 (stack65)
        %v13831 = vadd.s32 %v13815, %v13827 (stack65)
        %v13833 = vshll.u32 %v13827, 13 (stack73)
        %v13834 = vshrl.u32 %v13827, 19 (stack74)
        %v13835 = vor.u32 %v13833, %v13834 (stack75)
        %v13836 = vxor.u32 %v13831, %v13835 (stack76)
        %v13839 = vadd.s32 %v13831, %v13836 (stack65)
        %v13841 = vshll.u32 %v13836, 15 (stack73)
        %v13842 = vshrl.u32 %v13836, 17 (stack74)
        %v13843 = vor.u32 %v13841, %v13842 (stack75)
        %v13844 = vxor.u32 %v13839, %v13843 (stack76)
        %v13847 = vadd.s32 %v13839, %v13844 (stack65)
        %v13849 = vshll.u32 %v13844, 26 (stack73)
        %v13850 = vshrl.u32 %v13844, 6 (stack74)
        %v13851 = vor.u32 %v13849, %v13850 (stack75)
        %v13852 = vxor.u32 %v13847, %v13851 (stack76)
        %v13855 = vadd.s32 %v13847, %v13852 (stack65)
        %v13859 = vadd.s32 %v13855, %v8 (stack65)
        %v13861 = vshll.u32 %v13852, 6 (stack73)
        %v13862 = vshrl.u32 %v13852, 26 (stack74)
        %v13863 = vor.u32 %v13861, %v13862 (stack75)
        %v13864 = vxor.u32 %v13855, %v13863 (stack76)
        %v13867 = vadd.s32 %v13864, %v10 (stack65)
        %v13871 = vadd.s32 %v13867, 5 (stack65)
        %v13873 = vxor.u32 %v13859, %v13871 (stack76)
        %v13874 = vand.u32.u8 %v13873, 255 (stack77)
        %v13875 = vand.u32 %v13874, 65535 (stack78)
        %v13876 = vshrl.u32 %v13875, 1 (stack79)
        %v13877 = vor.u32 %v13876, 16256 (stack75)
        %v13878 = vand.u32.u16 %v13877, 65535 (stack80)
        %v13879 = vunpack.i.l.bf16 %v13878 (stack81)
        %v13883 = vadd.f32 %v13879, -1.0 (stack82)
        %v13887 = vmul.f32 %v13883, 2.0 (stack83)
        %v13891 = vadd.f32 %v13887, -0.99609375 (stack82)
        %v13895 = vmax.f32 -0.99609375, %v13891 (stack84)
        %v13897 = vand.u32 2147483647, %v13895 (stack85)
        %vm13900 = vcmp.eq.f32.partialorder %v13897, 1.0 (stack86)
        %v13905 = vmul.f32 %v13895, inf (stack83)
        %v13907 = vxor.u32 %v13895, 2147483648 (stack87)
        %v13910 = vmul.f32 %v13895, %v13907 (stack83)
        %v13912 = vadd.f32 %v13910, 1.0 (stack88)
        %v13913 = vlog2.pop %v13912 (stack89)
        %v13914 = vmul.f32 %v13913, 0.6931472 (stack90)
        %v13915 = vmul.f32 -0.5, %v13910 (stack91)
        %v13916 = vadd.f32 %v13915, 1.0 (stack92)
        %v13917 = vmul.f32 %v13916, %v13910 (stack93)
        %v13918 = vand.u32 2147483647, %v13910 (stack94)
        %vm13919 = vcmp.lt.f32.partialorder %v13918, 0.0004427343 (stack95)
        %v13920 = vsel /*vm=*/%vm13919, /*on_true_vy=*/%v13917, /*on_false_vx=*/%v13914 (stack96)
        %v13921 = vxor.u32 %v13920, 2147483648 (stack87)
        %vm13924 = vcmp.lt.f32.partialorder %v13921, 5.0 (stack86)
        %v13929 = vsel /*vm=*/%vm13924, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v13933 = vsel /*vm=*/%vm13924, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v13937 = vsel /*vm=*/%vm13924, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v13941 = vsel /*vm=*/%vm13924, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v13945 = vsel /*vm=*/%vm13924, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v13949 = vsel /*vm=*/%vm13924, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v13953 = vsel /*vm=*/%vm13924, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v13957 = vsel /*vm=*/%vm13924, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v13961 = vsel /*vm=*/%vm13924, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v13965 = vadd.f32 %v13921, -2.5 (stack82)
        %v13967 = vrsqrt.pop %v13921 (stack97)
        %v13968 = vmul.f32 %v13921, %v13967 (stack98)
        %vm13969 = vcmp.eq.f32.partialorder %v13921, inf (stack99)
        %v13970 = vsel /*vm=*/%vm13969, /*on_true_vy=*/%v13921, /*on_false_vx=*/%v13968 (stack100)
        %vm13971 = vcmp.eq.f32.partialorder %v13921, 0.0 (stack101)
        %v13972 = vand.u32 %v13921, 2147483648 (stack102)
        %v13973 = vsel /*vm=*/%vm13971, /*on_true_vy=*/%v13972, /*on_false_vx=*/%v13970 (stack103)
        %v13976 = vadd.f32 %v13973, -3.0 (stack82)
        %v13980 = vsel /*vm=*/%vm13924, /*on_true_vy=*/%v13965, /*on_false_vx=*/%v13976 (stack72)
        %v13984 = vmul.f32 %v13961, %v13980 (stack83)
        %v13988 = vadd.f32 %v13957, %v13984 (stack82)
        %v13992 = vmul.f32 %v13988, %v13980 (stack83)
        %v13996 = vadd.f32 %v13953, %v13992 (stack82)
        %v14000 = vmul.f32 %v13996, %v13980 (stack83)
        %v14004 = vadd.f32 %v13949, %v14000 (stack82)
        %v14008 = vmul.f32 %v14004, %v13980 (stack83)
        %v14012 = vadd.f32 %v13945, %v14008 (stack82)
        %v14016 = vmul.f32 %v14012, %v13980 (stack83)
        %v14020 = vadd.f32 %v13941, %v14016 (stack82)
        %v14024 = vmul.f32 %v14020, %v13980 (stack83)
        %v14028 = vadd.f32 %v13937, %v14024 (stack82)
        %v14032 = vmul.f32 %v14028, %v13980 (stack83)
        %v14036 = vadd.f32 %v13933, %v14032 (stack82)
        %v14040 = vmul.f32 %v14036, %v13980 (stack83)
        %v14044 = vadd.f32 %v13929, %v14040 (stack82)
        %v14048 = vmul.f32 %v14044, %v13895 (stack83)
        %v14052 = vsel /*vm=*/%vm13900, /*on_true_vy=*/%v13905, /*on_false_vx=*/%v14048 (stack72)
        %v14056 = vmul.f32 %v14052, 1.4140625 (stack83)
        %s14058 = scalar_lea.vmem %s280, 524 [#allocation0] (stack107)
        %v14059 = vpack.c.bf16 0.0, %v14056 (stack104)
        %14060 = vst [vmem:[%s14058] sm:$0xf] /*vst_source=*/%v14059 (stack105)
        %v14063 = vadd.s32 %v2842, %v11755 (stack65)
        %s14065 = smul.u32 128, %s27 (stack66)
        %v14066 = vlaneseq (stack67)
        %v14067 = vand.u32 %v14066, 127 (stack68)
        %v14068 = vstv %s14065 (stack69)
        %v14069 = vadd.s32 %v14067, %v14068 (stack70)
        %v14073 = vadd.s32 %v14063, %v14069 (stack65)
        %vm14077 = vcmp.lt.u32.totalorder %v14073, %v14063 (stack71)
        %vm14082 = vcmp.lt.u32.totalorder %v14063, %v2842 (stack71)
        %v14087 = vadd.s32 %v2829, %v11738 (stack65)
        %v14091 = vadd.s32 %v14087, 1 (stack65)
        %v14095 = vsel /*vm=*/%vm14082, /*on_true_vy=*/%v14091, /*on_false_vx=*/%v14087 (stack72)
        %v14099 = vadd.s32 %v14095, 1 (stack65)
        %v14103 = vsel /*vm=*/%vm14077, /*on_true_vy=*/%v14099, /*on_false_vx=*/%v14095 (stack72)
        %v14108 = vadd.s32 %v14103, %v10 (stack65)
        %v14112 = vadd.s32 %v14073, %v9 (stack65)
        %v14116 = vadd.s32 %v14108, %v14112 (stack65)
        %v14118 = vshll.u32 %v14112, 13 (stack73)
        %v14119 = vshrl.u32 %v14112, 19 (stack74)
        %v14120 = vor.u32 %v14118, %v14119 (stack75)
        %v14121 = vxor.u32 %v14116, %v14120 (stack76)
        %v14124 = vadd.s32 %v14116, %v14121 (stack65)
        %v14126 = vshll.u32 %v14121, 15 (stack73)
        %v14127 = vshrl.u32 %v14121, 17 (stack74)
        %v14128 = vor.u32 %v14126, %v14127 (stack75)
        %v14129 = vxor.u32 %v14124, %v14128 (stack76)
        %v14132 = vadd.s32 %v14124, %v14129 (stack65)
        %v14134 = vshll.u32 %v14129, 26 (stack73)
        %v14135 = vshrl.u32 %v14129, 6 (stack74)
        %v14136 = vor.u32 %v14134, %v14135 (stack75)
        %v14137 = vxor.u32 %v14132, %v14136 (stack76)
        %v14140 = vadd.s32 %v14132, %v14137 (stack65)
        %v14144 = vadd.s32 %v14140, %v9 (stack65)
        %v14146 = vshll.u32 %v14137, 6 (stack73)
        %v14147 = vshrl.u32 %v14137, 26 (stack74)
        %v14148 = vor.u32 %v14146, %v14147 (stack75)
        %v14149 = vxor.u32 %v14140, %v14148 (stack76)
        %v14152 = vadd.s32 %v14149, %v8 (stack65)
        %v14156 = vadd.s32 %v14152, 1 (stack65)
        %v14160 = vadd.s32 %v14144, %v14156 (stack65)
        %v14162 = vshll.u32 %v14156, 17 (stack73)
        %v14163 = vshrl.u32 %v14156, 15 (stack74)
        %v14164 = vor.u32 %v14162, %v14163 (stack75)
        %v14165 = vxor.u32 %v14160, %v14164 (stack76)
        %v14168 = vadd.s32 %v14160, %v14165 (stack65)
        %v14170 = vshll.u32 %v14165, 29 (stack73)
        %v14171 = vshrl.u32 %v14165, 3 (stack74)
        %v14172 = vor.u32 %v14170, %v14171 (stack75)
        %v14173 = vxor.u32 %v14168, %v14172 (stack76)
        %v14176 = vadd.s32 %v14168, %v14173 (stack65)
        %v14178 = vshll.u32 %v14173, 16 (stack73)
        %v14179 = vshrl.u32 %v14173, 16 (stack74)
        %v14180 = vor.u32 %v14178, %v14179 (stack75)
        %v14181 = vxor.u32 %v14176, %v14180 (stack76)
        %v14184 = vadd.s32 %v14176, %v14181 (stack65)
        %v14188 = vadd.s32 %v14184, %v8 (stack65)
        %v14190 = vshll.u32 %v14181, 24 (stack73)
        %v14191 = vshrl.u32 %v14181, 8 (stack74)
        %v14192 = vor.u32 %v14190, %v14191 (stack75)
        %v14193 = vxor.u32 %v14184, %v14192 (stack76)
        %v14196 = vadd.s32 %v14193, %v10 (stack65)
        %v14200 = vadd.s32 %v14196, 2 (stack65)
        %v14204 = vadd.s32 %v14188, %v14200 (stack65)
        %v14206 = vshll.u32 %v14200, 13 (stack73)
        %v14207 = vshrl.u32 %v14200, 19 (stack74)
        %v14208 = vor.u32 %v14206, %v14207 (stack75)
        %v14209 = vxor.u32 %v14204, %v14208 (stack76)
        %v14212 = vadd.s32 %v14204, %v14209 (stack65)
        %v14214 = vshll.u32 %v14209, 15 (stack73)
        %v14215 = vshrl.u32 %v14209, 17 (stack74)
        %v14216 = vor.u32 %v14214, %v14215 (stack75)
        %v14217 = vxor.u32 %v14212, %v14216 (stack76)
        %v14220 = vadd.s32 %v14212, %v14217 (stack65)
        %v14222 = vshll.u32 %v14217, 26 (stack73)
        %v14223 = vshrl.u32 %v14217, 6 (stack74)
        %v14224 = vor.u32 %v14222, %v14223 (stack75)
        %v14225 = vxor.u32 %v14220, %v14224 (stack76)
        %v14228 = vadd.s32 %v14220, %v14225 (stack65)
        %v14232 = vadd.s32 %v14228, %v10 (stack65)
        %v14234 = vshll.u32 %v14225, 6 (stack73)
        %v14235 = vshrl.u32 %v14225, 26 (stack74)
        %v14236 = vor.u32 %v14234, %v14235 (stack75)
        %v14237 = vxor.u32 %v14228, %v14236 (stack76)
        %v14240 = vadd.s32 %v14237, %v9 (stack65)
        %v14244 = vadd.s32 %v14240, 3 (stack65)
        %v14248 = vadd.s32 %v14232, %v14244 (stack65)
        %v14250 = vshll.u32 %v14244, 17 (stack73)
        %v14251 = vshrl.u32 %v14244, 15 (stack74)
        %v14252 = vor.u32 %v14250, %v14251 (stack75)
        %v14253 = vxor.u32 %v14248, %v14252 (stack76)
        %v14256 = vadd.s32 %v14248, %v14253 (stack65)
        %v14258 = vshll.u32 %v14253, 29 (stack73)
        %v14259 = vshrl.u32 %v14253, 3 (stack74)
        %v14260 = vor.u32 %v14258, %v14259 (stack75)
        %v14261 = vxor.u32 %v14256, %v14260 (stack76)
        %v14264 = vadd.s32 %v14256, %v14261 (stack65)
        %v14266 = vshll.u32 %v14261, 16 (stack73)
        %v14267 = vshrl.u32 %v14261, 16 (stack74)
        %v14268 = vor.u32 %v14266, %v14267 (stack75)
        %v14269 = vxor.u32 %v14264, %v14268 (stack76)
        %v14272 = vadd.s32 %v14264, %v14269 (stack65)
        %v14276 = vadd.s32 %v14272, %v9 (stack65)
        %v14278 = vshll.u32 %v14269, 24 (stack73)
        %v14279 = vshrl.u32 %v14269, 8 (stack74)
        %v14280 = vor.u32 %v14278, %v14279 (stack75)
        %v14281 = vxor.u32 %v14272, %v14280 (stack76)
        %v14284 = vadd.s32 %v14281, %v8 (stack65)
        %v14288 = vadd.s32 %v14284, 4 (stack65)
        %v14292 = vadd.s32 %v14276, %v14288 (stack65)
        %v14294 = vshll.u32 %v14288, 13 (stack73)
        %v14295 = vshrl.u32 %v14288, 19 (stack74)
        %v14296 = vor.u32 %v14294, %v14295 (stack75)
        %v14297 = vxor.u32 %v14292, %v14296 (stack76)
        %v14300 = vadd.s32 %v14292, %v14297 (stack65)
        %v14302 = vshll.u32 %v14297, 15 (stack73)
        %v14303 = vshrl.u32 %v14297, 17 (stack74)
        %v14304 = vor.u32 %v14302, %v14303 (stack75)
        %v14305 = vxor.u32 %v14300, %v14304 (stack76)
        %v14308 = vadd.s32 %v14300, %v14305 (stack65)
        %v14310 = vshll.u32 %v14305, 26 (stack73)
        %v14311 = vshrl.u32 %v14305, 6 (stack74)
        %v14312 = vor.u32 %v14310, %v14311 (stack75)
        %v14313 = vxor.u32 %v14308, %v14312 (stack76)
        %v14316 = vadd.s32 %v14308, %v14313 (stack65)
        %v14320 = vadd.s32 %v14316, %v8 (stack65)
        %v14322 = vshll.u32 %v14313, 6 (stack73)
        %v14323 = vshrl.u32 %v14313, 26 (stack74)
        %v14324 = vor.u32 %v14322, %v14323 (stack75)
        %v14325 = vxor.u32 %v14316, %v14324 (stack76)
        %v14328 = vadd.s32 %v14325, %v10 (stack65)
        %v14332 = vadd.s32 %v14328, 5 (stack65)
        %v14334 = vxor.u32 %v14320, %v14332 (stack76)
        %v14335 = vand.u32.u8 %v14334, 255 (stack77)
        %v14336 = vand.u32 %v14335, 65535 (stack78)
        %v14337 = vshrl.u32 %v14336, 1 (stack79)
        %v14338 = vor.u32 %v14337, 16256 (stack75)
        %v14339 = vand.u32.u16 %v14338, 65535 (stack80)
        %v14340 = vunpack.i.l.bf16 %v14339 (stack81)
        %v14344 = vadd.f32 %v14340, -1.0 (stack82)
        %v14348 = vmul.f32 %v14344, 2.0 (stack83)
        %v14352 = vadd.f32 %v14348, -0.99609375 (stack82)
        %v14356 = vmax.f32 -0.99609375, %v14352 (stack84)
        %v14358 = vand.u32 2147483647, %v14356 (stack85)
        %vm14361 = vcmp.eq.f32.partialorder %v14358, 1.0 (stack86)
        %v14366 = vmul.f32 %v14356, inf (stack83)
        %v14368 = vxor.u32 %v14356, 2147483648 (stack87)
        %v14371 = vmul.f32 %v14356, %v14368 (stack83)
        %v14373 = vadd.f32 %v14371, 1.0 (stack88)
        %v14374 = vlog2.pop %v14373 (stack89)
        %v14375 = vmul.f32 %v14374, 0.6931472 (stack90)
        %v14376 = vmul.f32 -0.5, %v14371 (stack91)
        %v14377 = vadd.f32 %v14376, 1.0 (stack92)
        %v14378 = vmul.f32 %v14377, %v14371 (stack93)
        %v14379 = vand.u32 2147483647, %v14371 (stack94)
        %vm14380 = vcmp.lt.f32.partialorder %v14379, 0.0004427343 (stack95)
        %v14381 = vsel /*vm=*/%vm14380, /*on_true_vy=*/%v14378, /*on_false_vx=*/%v14375 (stack96)
        %v14382 = vxor.u32 %v14381, 2147483648 (stack87)
        %vm14385 = vcmp.lt.f32.partialorder %v14382, 5.0 (stack86)
        %v14390 = vsel /*vm=*/%vm14385, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v14394 = vsel /*vm=*/%vm14385, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v14398 = vsel /*vm=*/%vm14385, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v14402 = vsel /*vm=*/%vm14385, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v14406 = vsel /*vm=*/%vm14385, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v14410 = vsel /*vm=*/%vm14385, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v14414 = vsel /*vm=*/%vm14385, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v14418 = vsel /*vm=*/%vm14385, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v14422 = vsel /*vm=*/%vm14385, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v14426 = vadd.f32 %v14382, -2.5 (stack82)
        %v14428 = vrsqrt.pop %v14382 (stack97)
        %v14429 = vmul.f32 %v14382, %v14428 (stack98)
        %vm14430 = vcmp.eq.f32.partialorder %v14382, inf (stack99)
        %v14431 = vsel /*vm=*/%vm14430, /*on_true_vy=*/%v14382, /*on_false_vx=*/%v14429 (stack100)
        %vm14432 = vcmp.eq.f32.partialorder %v14382, 0.0 (stack101)
        %v14433 = vand.u32 %v14382, 2147483648 (stack102)
        %v14434 = vsel /*vm=*/%vm14432, /*on_true_vy=*/%v14433, /*on_false_vx=*/%v14431 (stack103)
        %v14437 = vadd.f32 %v14434, -3.0 (stack82)
        %v14441 = vsel /*vm=*/%vm14385, /*on_true_vy=*/%v14426, /*on_false_vx=*/%v14437 (stack72)
        %v14445 = vmul.f32 %v14422, %v14441 (stack83)
        %v14449 = vadd.f32 %v14418, %v14445 (stack82)
        %v14453 = vmul.f32 %v14449, %v14441 (stack83)
        %v14457 = vadd.f32 %v14414, %v14453 (stack82)
        %v14461 = vmul.f32 %v14457, %v14441 (stack83)
        %v14465 = vadd.f32 %v14410, %v14461 (stack82)
        %v14469 = vmul.f32 %v14465, %v14441 (stack83)
        %v14473 = vadd.f32 %v14406, %v14469 (stack82)
        %v14477 = vmul.f32 %v14473, %v14441 (stack83)
        %v14481 = vadd.f32 %v14402, %v14477 (stack82)
        %v14485 = vmul.f32 %v14481, %v14441 (stack83)
        %v14489 = vadd.f32 %v14398, %v14485 (stack82)
        %v14493 = vmul.f32 %v14489, %v14441 (stack83)
        %v14497 = vadd.f32 %v14394, %v14493 (stack82)
        %v14501 = vmul.f32 %v14497, %v14441 (stack83)
        %v14505 = vadd.f32 %v14390, %v14501 (stack82)
        %v14509 = vmul.f32 %v14505, %v14356 (stack83)
        %v14513 = vsel /*vm=*/%vm14361, /*on_true_vy=*/%v14366, /*on_false_vx=*/%v14509 (stack72)
        %v14517 = vmul.f32 %v14513, 1.4140625 (stack83)
        %s14519 = scalar_lea.vmem %s280, 652 [#allocation0] (stack107)
        %v14520 = vpack.c.bf16 0.0, %v14517 (stack104)
        %14521 = vst [vmem:[%s14519] sm:$0xf] /*vst_source=*/%v14520 (stack105)
        %v14524 = vadd.s32 %v3329, %v11755 (stack65)
        %s14526 = smul.u32 128, %s27 (stack66)
        %v14527 = vlaneseq (stack67)
        %v14528 = vand.u32 %v14527, 127 (stack68)
        %v14529 = vstv %s14526 (stack69)
        %v14530 = vadd.s32 %v14528, %v14529 (stack70)
        %v14534 = vadd.s32 %v14524, %v14530 (stack65)
        %vm14538 = vcmp.lt.u32.totalorder %v14534, %v14524 (stack71)
        %vm14543 = vcmp.lt.u32.totalorder %v14524, %v3329 (stack71)
        %v14548 = vadd.s32 %v3316, %v11738 (stack65)
        %v14552 = vadd.s32 %v14548, 1 (stack65)
        %v14556 = vsel /*vm=*/%vm14543, /*on_true_vy=*/%v14552, /*on_false_vx=*/%v14548 (stack72)
        %v14560 = vadd.s32 %v14556, 1 (stack65)
        %v14564 = vsel /*vm=*/%vm14538, /*on_true_vy=*/%v14560, /*on_false_vx=*/%v14556 (stack72)
        %v14569 = vadd.s32 %v14564, %v10 (stack65)
        %v14573 = vadd.s32 %v14534, %v9 (stack65)
        %v14577 = vadd.s32 %v14569, %v14573 (stack65)
        %v14579 = vshll.u32 %v14573, 13 (stack73)
        %v14580 = vshrl.u32 %v14573, 19 (stack74)
        %v14581 = vor.u32 %v14579, %v14580 (stack75)
        %v14582 = vxor.u32 %v14577, %v14581 (stack76)
        %v14585 = vadd.s32 %v14577, %v14582 (stack65)
        %v14587 = vshll.u32 %v14582, 15 (stack73)
        %v14588 = vshrl.u32 %v14582, 17 (stack74)
        %v14589 = vor.u32 %v14587, %v14588 (stack75)
        %v14590 = vxor.u32 %v14585, %v14589 (stack76)
        %v14593 = vadd.s32 %v14585, %v14590 (stack65)
        %v14595 = vshll.u32 %v14590, 26 (stack73)
        %v14596 = vshrl.u32 %v14590, 6 (stack74)
        %v14597 = vor.u32 %v14595, %v14596 (stack75)
        %v14598 = vxor.u32 %v14593, %v14597 (stack76)
        %v14601 = vadd.s32 %v14593, %v14598 (stack65)
        %v14605 = vadd.s32 %v14601, %v9 (stack65)
        %v14607 = vshll.u32 %v14598, 6 (stack73)
        %v14608 = vshrl.u32 %v14598, 26 (stack74)
        %v14609 = vor.u32 %v14607, %v14608 (stack75)
        %v14610 = vxor.u32 %v14601, %v14609 (stack76)
        %v14613 = vadd.s32 %v14610, %v8 (stack65)
        %v14617 = vadd.s32 %v14613, 1 (stack65)
        %v14621 = vadd.s32 %v14605, %v14617 (stack65)
        %v14623 = vshll.u32 %v14617, 17 (stack73)
        %v14624 = vshrl.u32 %v14617, 15 (stack74)
        %v14625 = vor.u32 %v14623, %v14624 (stack75)
        %v14626 = vxor.u32 %v14621, %v14625 (stack76)
        %v14629 = vadd.s32 %v14621, %v14626 (stack65)
        %v14631 = vshll.u32 %v14626, 29 (stack73)
        %v14632 = vshrl.u32 %v14626, 3 (stack74)
        %v14633 = vor.u32 %v14631, %v14632 (stack75)
        %v14634 = vxor.u32 %v14629, %v14633 (stack76)
        %v14637 = vadd.s32 %v14629, %v14634 (stack65)
        %v14639 = vshll.u32 %v14634, 16 (stack73)
        %v14640 = vshrl.u32 %v14634, 16 (stack74)
        %v14641 = vor.u32 %v14639, %v14640 (stack75)
        %v14642 = vxor.u32 %v14637, %v14641 (stack76)
        %v14645 = vadd.s32 %v14637, %v14642 (stack65)
        %v14649 = vadd.s32 %v14645, %v8 (stack65)
        %v14651 = vshll.u32 %v14642, 24 (stack73)
        %v14652 = vshrl.u32 %v14642, 8 (stack74)
        %v14653 = vor.u32 %v14651, %v14652 (stack75)
        %v14654 = vxor.u32 %v14645, %v14653 (stack76)
        %v14657 = vadd.s32 %v14654, %v10 (stack65)
        %v14661 = vadd.s32 %v14657, 2 (stack65)
        %v14665 = vadd.s32 %v14649, %v14661 (stack65)
        %v14667 = vshll.u32 %v14661, 13 (stack73)
        %v14668 = vshrl.u32 %v14661, 19 (stack74)
        %v14669 = vor.u32 %v14667, %v14668 (stack75)
        %v14670 = vxor.u32 %v14665, %v14669 (stack76)
        %v14673 = vadd.s32 %v14665, %v14670 (stack65)
        %v14675 = vshll.u32 %v14670, 15 (stack73)
        %v14676 = vshrl.u32 %v14670, 17 (stack74)
        %v14677 = vor.u32 %v14675, %v14676 (stack75)
        %v14678 = vxor.u32 %v14673, %v14677 (stack76)
        %v14681 = vadd.s32 %v14673, %v14678 (stack65)
        %v14683 = vshll.u32 %v14678, 26 (stack73)
        %v14684 = vshrl.u32 %v14678, 6 (stack74)
        %v14685 = vor.u32 %v14683, %v14684 (stack75)
        %v14686 = vxor.u32 %v14681, %v14685 (stack76)
        %v14689 = vadd.s32 %v14681, %v14686 (stack65)
        %v14693 = vadd.s32 %v14689, %v10 (stack65)
        %v14695 = vshll.u32 %v14686, 6 (stack73)
        %v14696 = vshrl.u32 %v14686, 26 (stack74)
        %v14697 = vor.u32 %v14695, %v14696 (stack75)
        %v14698 = vxor.u32 %v14689, %v14697 (stack76)
        %v14701 = vadd.s32 %v14698, %v9 (stack65)
        %v14705 = vadd.s32 %v14701, 3 (stack65)
        %v14709 = vadd.s32 %v14693, %v14705 (stack65)
        %v14711 = vshll.u32 %v14705, 17 (stack73)
        %v14712 = vshrl.u32 %v14705, 15 (stack74)
        %v14713 = vor.u32 %v14711, %v14712 (stack75)
        %v14714 = vxor.u32 %v14709, %v14713 (stack76)
        %v14717 = vadd.s32 %v14709, %v14714 (stack65)
        %v14719 = vshll.u32 %v14714, 29 (stack73)
        %v14720 = vshrl.u32 %v14714, 3 (stack74)
        %v14721 = vor.u32 %v14719, %v14720 (stack75)
        %v14722 = vxor.u32 %v14717, %v14721 (stack76)
        %v14725 = vadd.s32 %v14717, %v14722 (stack65)
        %v14727 = vshll.u32 %v14722, 16 (stack73)
        %v14728 = vshrl.u32 %v14722, 16 (stack74)
        %v14729 = vor.u32 %v14727, %v14728 (stack75)
        %v14730 = vxor.u32 %v14725, %v14729 (stack76)
        %v14733 = vadd.s32 %v14725, %v14730 (stack65)
        %v14737 = vadd.s32 %v14733, %v9 (stack65)
        %v14739 = vshll.u32 %v14730, 24 (stack73)
        %v14740 = vshrl.u32 %v14730, 8 (stack74)
        %v14741 = vor.u32 %v14739, %v14740 (stack75)
        %v14742 = vxor.u32 %v14733, %v14741 (stack76)
        %v14745 = vadd.s32 %v14742, %v8 (stack65)
        %v14749 = vadd.s32 %v14745, 4 (stack65)
        %v14753 = vadd.s32 %v14737, %v14749 (stack65)
        %v14755 = vshll.u32 %v14749, 13 (stack73)
        %v14756 = vshrl.u32 %v14749, 19 (stack74)
        %v14757 = vor.u32 %v14755, %v14756 (stack75)
        %v14758 = vxor.u32 %v14753, %v14757 (stack76)
        %v14761 = vadd.s32 %v14753, %v14758 (stack65)
        %v14763 = vshll.u32 %v14758, 15 (stack73)
        %v14764 = vshrl.u32 %v14758, 17 (stack74)
        %v14765 = vor.u32 %v14763, %v14764 (stack75)
        %v14766 = vxor.u32 %v14761, %v14765 (stack76)
        %v14769 = vadd.s32 %v14761, %v14766 (stack65)
        %v14771 = vshll.u32 %v14766, 26 (stack73)
        %v14772 = vshrl.u32 %v14766, 6 (stack74)
        %v14773 = vor.u32 %v14771, %v14772 (stack75)
        %v14774 = vxor.u32 %v14769, %v14773 (stack76)
        %v14777 = vadd.s32 %v14769, %v14774 (stack65)
        %v14781 = vadd.s32 %v14777, %v8 (stack65)
        %v14783 = vshll.u32 %v14774, 6 (stack73)
        %v14784 = vshrl.u32 %v14774, 26 (stack74)
        %v14785 = vor.u32 %v14783, %v14784 (stack75)
        %v14786 = vxor.u32 %v14777, %v14785 (stack76)
        %v14789 = vadd.s32 %v14786, %v10 (stack65)
        %v14793 = vadd.s32 %v14789, 5 (stack65)
        %v14795 = vxor.u32 %v14781, %v14793 (stack76)
        %v14796 = vand.u32.u8 %v14795, 255 (stack77)
        %v14797 = vand.u32 %v14796, 65535 (stack78)
        %v14798 = vshrl.u32 %v14797, 1 (stack79)
        %v14799 = vor.u32 %v14798, 16256 (stack75)
        %v14800 = vand.u32.u16 %v14799, 65535 (stack80)
        %v14801 = vunpack.i.l.bf16 %v14800 (stack81)
        %v14805 = vadd.f32 %v14801, -1.0 (stack82)
        %v14809 = vmul.f32 %v14805, 2.0 (stack83)
        %v14813 = vadd.f32 %v14809, -0.99609375 (stack82)
        %v14817 = vmax.f32 -0.99609375, %v14813 (stack84)
        %v14819 = vand.u32 2147483647, %v14817 (stack85)
        %vm14822 = vcmp.eq.f32.partialorder %v14819, 1.0 (stack86)
        %v14827 = vmul.f32 %v14817, inf (stack83)
        %v14829 = vxor.u32 %v14817, 2147483648 (stack87)
        %v14832 = vmul.f32 %v14817, %v14829 (stack83)
        %v14834 = vadd.f32 %v14832, 1.0 (stack88)
        %v14835 = vlog2.pop %v14834 (stack89)
        %v14836 = vmul.f32 %v14835, 0.6931472 (stack90)
        %v14837 = vmul.f32 -0.5, %v14832 (stack91)
        %v14838 = vadd.f32 %v14837, 1.0 (stack92)
        %v14839 = vmul.f32 %v14838, %v14832 (stack93)
        %v14840 = vand.u32 2147483647, %v14832 (stack94)
        %vm14841 = vcmp.lt.f32.partialorder %v14840, 0.0004427343 (stack95)
        %v14842 = vsel /*vm=*/%vm14841, /*on_true_vy=*/%v14839, /*on_false_vx=*/%v14836 (stack96)
        %v14843 = vxor.u32 %v14842, 2147483648 (stack87)
        %vm14846 = vcmp.lt.f32.partialorder %v14843, 5.0 (stack86)
        %v14851 = vsel /*vm=*/%vm14846, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v14855 = vsel /*vm=*/%vm14846, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v14859 = vsel /*vm=*/%vm14846, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v14863 = vsel /*vm=*/%vm14846, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v14867 = vsel /*vm=*/%vm14846, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v14871 = vsel /*vm=*/%vm14846, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v14875 = vsel /*vm=*/%vm14846, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v14879 = vsel /*vm=*/%vm14846, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v14883 = vsel /*vm=*/%vm14846, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v14887 = vadd.f32 %v14843, -2.5 (stack82)
        %v14889 = vrsqrt.pop %v14843 (stack97)
        %v14890 = vmul.f32 %v14843, %v14889 (stack98)
        %vm14891 = vcmp.eq.f32.partialorder %v14843, inf (stack99)
        %v14892 = vsel /*vm=*/%vm14891, /*on_true_vy=*/%v14843, /*on_false_vx=*/%v14890 (stack100)
        %vm14893 = vcmp.eq.f32.partialorder %v14843, 0.0 (stack101)
        %v14894 = vand.u32 %v14843, 2147483648 (stack102)
        %v14895 = vsel /*vm=*/%vm14893, /*on_true_vy=*/%v14894, /*on_false_vx=*/%v14892 (stack103)
        %v14898 = vadd.f32 %v14895, -3.0 (stack82)
        %v14902 = vsel /*vm=*/%vm14846, /*on_true_vy=*/%v14887, /*on_false_vx=*/%v14898 (stack72)
        %v14906 = vmul.f32 %v14883, %v14902 (stack83)
        %v14910 = vadd.f32 %v14879, %v14906 (stack82)
        %v14914 = vmul.f32 %v14910, %v14902 (stack83)
        %v14918 = vadd.f32 %v14875, %v14914 (stack82)
        %v14922 = vmul.f32 %v14918, %v14902 (stack83)
        %v14926 = vadd.f32 %v14871, %v14922 (stack82)
        %v14930 = vmul.f32 %v14926, %v14902 (stack83)
        %v14934 = vadd.f32 %v14867, %v14930 (stack82)
        %v14938 = vmul.f32 %v14934, %v14902 (stack83)
        %v14942 = vadd.f32 %v14863, %v14938 (stack82)
        %v14946 = vmul.f32 %v14942, %v14902 (stack83)
        %v14950 = vadd.f32 %v14859, %v14946 (stack82)
        %v14954 = vmul.f32 %v14950, %v14902 (stack83)
        %v14958 = vadd.f32 %v14855, %v14954 (stack82)
        %v14962 = vmul.f32 %v14958, %v14902 (stack83)
        %v14966 = vadd.f32 %v14851, %v14962 (stack82)
        %v14970 = vmul.f32 %v14966, %v14817 (stack83)
        %v14974 = vsel /*vm=*/%vm14822, /*on_true_vy=*/%v14827, /*on_false_vx=*/%v14970 (stack72)
        %v14978 = vmul.f32 %v14974, 1.4140625 (stack83)
        %s14980 = scalar_lea.vmem %s280, 780 [#allocation0] (stack107)
        %v14981 = vpack.c.bf16 0.0, %v14978 (stack104)
        %14982 = vst [vmem:[%s14980] sm:$0xf] /*vst_source=*/%v14981 (stack105)
        %v14985 = vadd.s32 %v3816, %v11755 (stack65)
        %s14987 = smul.u32 128, %s27 (stack66)
        %v14988 = vlaneseq (stack67)
        %v14989 = vand.u32 %v14988, 127 (stack68)
        %v14990 = vstv %s14987 (stack69)
        %v14991 = vadd.s32 %v14989, %v14990 (stack70)
        %v14995 = vadd.s32 %v14985, %v14991 (stack65)
        %vm14999 = vcmp.lt.u32.totalorder %v14995, %v14985 (stack71)
        %vm15004 = vcmp.lt.u32.totalorder %v14985, %v3816 (stack71)
        %v15009 = vadd.s32 %v3803, %v11738 (stack65)
        %v15013 = vadd.s32 %v15009, 1 (stack65)
        %v15017 = vsel /*vm=*/%vm15004, /*on_true_vy=*/%v15013, /*on_false_vx=*/%v15009 (stack72)
        %v15021 = vadd.s32 %v15017, 1 (stack65)
        %v15025 = vsel /*vm=*/%vm14999, /*on_true_vy=*/%v15021, /*on_false_vx=*/%v15017 (stack72)
        %v15030 = vadd.s32 %v15025, %v10 (stack65)
        %v15034 = vadd.s32 %v14995, %v9 (stack65)
        %v15038 = vadd.s32 %v15030, %v15034 (stack65)
        %v15040 = vshll.u32 %v15034, 13 (stack73)
        %v15041 = vshrl.u32 %v15034, 19 (stack74)
        %v15042 = vor.u32 %v15040, %v15041 (stack75)
        %v15043 = vxor.u32 %v15038, %v15042 (stack76)
        %v15046 = vadd.s32 %v15038, %v15043 (stack65)
        %v15048 = vshll.u32 %v15043, 15 (stack73)
        %v15049 = vshrl.u32 %v15043, 17 (stack74)
        %v15050 = vor.u32 %v15048, %v15049 (stack75)
        %v15051 = vxor.u32 %v15046, %v15050 (stack76)
        %v15054 = vadd.s32 %v15046, %v15051 (stack65)
        %v15056 = vshll.u32 %v15051, 26 (stack73)
        %v15057 = vshrl.u32 %v15051, 6 (stack74)
        %v15058 = vor.u32 %v15056, %v15057 (stack75)
        %v15059 = vxor.u32 %v15054, %v15058 (stack76)
        %v15062 = vadd.s32 %v15054, %v15059 (stack65)
        %v15066 = vadd.s32 %v15062, %v9 (stack65)
        %v15068 = vshll.u32 %v15059, 6 (stack73)
        %v15069 = vshrl.u32 %v15059, 26 (stack74)
        %v15070 = vor.u32 %v15068, %v15069 (stack75)
        %v15071 = vxor.u32 %v15062, %v15070 (stack76)
        %v15074 = vadd.s32 %v15071, %v8 (stack65)
        %v15078 = vadd.s32 %v15074, 1 (stack65)
        %v15082 = vadd.s32 %v15066, %v15078 (stack65)
        %v15084 = vshll.u32 %v15078, 17 (stack73)
        %v15085 = vshrl.u32 %v15078, 15 (stack74)
        %v15086 = vor.u32 %v15084, %v15085 (stack75)
        %v15087 = vxor.u32 %v15082, %v15086 (stack76)
        %v15090 = vadd.s32 %v15082, %v15087 (stack65)
        %v15092 = vshll.u32 %v15087, 29 (stack73)
        %v15093 = vshrl.u32 %v15087, 3 (stack74)
        %v15094 = vor.u32 %v15092, %v15093 (stack75)
        %v15095 = vxor.u32 %v15090, %v15094 (stack76)
        %v15098 = vadd.s32 %v15090, %v15095 (stack65)
        %v15100 = vshll.u32 %v15095, 16 (stack73)
        %v15101 = vshrl.u32 %v15095, 16 (stack74)
        %v15102 = vor.u32 %v15100, %v15101 (stack75)
        %v15103 = vxor.u32 %v15098, %v15102 (stack76)
        %v15106 = vadd.s32 %v15098, %v15103 (stack65)
        %v15110 = vadd.s32 %v15106, %v8 (stack65)
        %v15112 = vshll.u32 %v15103, 24 (stack73)
        %v15113 = vshrl.u32 %v15103, 8 (stack74)
        %v15114 = vor.u32 %v15112, %v15113 (stack75)
        %v15115 = vxor.u32 %v15106, %v15114 (stack76)
        %v15118 = vadd.s32 %v15115, %v10 (stack65)
        %v15122 = vadd.s32 %v15118, 2 (stack65)
        %v15126 = vadd.s32 %v15110, %v15122 (stack65)
        %v15128 = vshll.u32 %v15122, 13 (stack73)
        %v15129 = vshrl.u32 %v15122, 19 (stack74)
        %v15130 = vor.u32 %v15128, %v15129 (stack75)
        %v15131 = vxor.u32 %v15126, %v15130 (stack76)
        %v15134 = vadd.s32 %v15126, %v15131 (stack65)
        %v15136 = vshll.u32 %v15131, 15 (stack73)
        %v15137 = vshrl.u32 %v15131, 17 (stack74)
        %v15138 = vor.u32 %v15136, %v15137 (stack75)
        %v15139 = vxor.u32 %v15134, %v15138 (stack76)
        %v15142 = vadd.s32 %v15134, %v15139 (stack65)
        %v15144 = vshll.u32 %v15139, 26 (stack73)
        %v15145 = vshrl.u32 %v15139, 6 (stack74)
        %v15146 = vor.u32 %v15144, %v15145 (stack75)
        %v15147 = vxor.u32 %v15142, %v15146 (stack76)
        %v15150 = vadd.s32 %v15142, %v15147 (stack65)
        %v15154 = vadd.s32 %v15150, %v10 (stack65)
        %v15156 = vshll.u32 %v15147, 6 (stack73)
        %v15157 = vshrl.u32 %v15147, 26 (stack74)
        %v15158 = vor.u32 %v15156, %v15157 (stack75)
        %v15159 = vxor.u32 %v15150, %v15158 (stack76)
        %v15162 = vadd.s32 %v15159, %v9 (stack65)
        %v15166 = vadd.s32 %v15162, 3 (stack65)
        %v15170 = vadd.s32 %v15154, %v15166 (stack65)
        %v15172 = vshll.u32 %v15166, 17 (stack73)
        %v15173 = vshrl.u32 %v15166, 15 (stack74)
        %v15174 = vor.u32 %v15172, %v15173 (stack75)
        %v15175 = vxor.u32 %v15170, %v15174 (stack76)
        %v15178 = vadd.s32 %v15170, %v15175 (stack65)
        %v15180 = vshll.u32 %v15175, 29 (stack73)
        %v15181 = vshrl.u32 %v15175, 3 (stack74)
        %v15182 = vor.u32 %v15180, %v15181 (stack75)
        %v15183 = vxor.u32 %v15178, %v15182 (stack76)
        %v15186 = vadd.s32 %v15178, %v15183 (stack65)
        %v15188 = vshll.u32 %v15183, 16 (stack73)
        %v15189 = vshrl.u32 %v15183, 16 (stack74)
        %v15190 = vor.u32 %v15188, %v15189 (stack75)
        %v15191 = vxor.u32 %v15186, %v15190 (stack76)
        %v15194 = vadd.s32 %v15186, %v15191 (stack65)
        %v15198 = vadd.s32 %v15194, %v9 (stack65)
        %v15200 = vshll.u32 %v15191, 24 (stack73)
        %v15201 = vshrl.u32 %v15191, 8 (stack74)
        %v15202 = vor.u32 %v15200, %v15201 (stack75)
        %v15203 = vxor.u32 %v15194, %v15202 (stack76)
        %v15206 = vadd.s32 %v15203, %v8 (stack65)
        %v15210 = vadd.s32 %v15206, 4 (stack65)
        %v15214 = vadd.s32 %v15198, %v15210 (stack65)
        %v15216 = vshll.u32 %v15210, 13 (stack73)
        %v15217 = vshrl.u32 %v15210, 19 (stack74)
        %v15218 = vor.u32 %v15216, %v15217 (stack75)
        %v15219 = vxor.u32 %v15214, %v15218 (stack76)
        %v15222 = vadd.s32 %v15214, %v15219 (stack65)
        %v15224 = vshll.u32 %v15219, 15 (stack73)
        %v15225 = vshrl.u32 %v15219, 17 (stack74)
        %v15226 = vor.u32 %v15224, %v15225 (stack75)
        %v15227 = vxor.u32 %v15222, %v15226 (stack76)
        %v15230 = vadd.s32 %v15222, %v15227 (stack65)
        %v15232 = vshll.u32 %v15227, 26 (stack73)
        %v15233 = vshrl.u32 %v15227, 6 (stack74)
        %v15234 = vor.u32 %v15232, %v15233 (stack75)
        %v15235 = vxor.u32 %v15230, %v15234 (stack76)
        %v15238 = vadd.s32 %v15230, %v15235 (stack65)
        %v15242 = vadd.s32 %v15238, %v8 (stack65)
        %v15244 = vshll.u32 %v15235, 6 (stack73)
        %v15245 = vshrl.u32 %v15235, 26 (stack74)
        %v15246 = vor.u32 %v15244, %v15245 (stack75)
        %v15247 = vxor.u32 %v15238, %v15246 (stack76)
        %v15250 = vadd.s32 %v15247, %v10 (stack65)
        %v15254 = vadd.s32 %v15250, 5 (stack65)
        %v15256 = vxor.u32 %v15242, %v15254 (stack76)
        %v15257 = vand.u32.u8 %v15256, 255 (stack77)
        %v15258 = vand.u32 %v15257, 65535 (stack78)
        %v15259 = vshrl.u32 %v15258, 1 (stack79)
        %v15260 = vor.u32 %v15259, 16256 (stack75)
        %v15261 = vand.u32.u16 %v15260, 65535 (stack80)
        %v15262 = vunpack.i.l.bf16 %v15261 (stack81)
        %v15266 = vadd.f32 %v15262, -1.0 (stack82)
        %v15270 = vmul.f32 %v15266, 2.0 (stack83)
        %v15274 = vadd.f32 %v15270, -0.99609375 (stack82)
        %v15278 = vmax.f32 -0.99609375, %v15274 (stack84)
        %v15280 = vand.u32 2147483647, %v15278 (stack85)
        %vm15283 = vcmp.eq.f32.partialorder %v15280, 1.0 (stack86)
        %v15288 = vmul.f32 %v15278, inf (stack83)
        %v15290 = vxor.u32 %v15278, 2147483648 (stack87)
        %v15293 = vmul.f32 %v15278, %v15290 (stack83)
        %v15295 = vadd.f32 %v15293, 1.0 (stack88)
        %v15296 = vlog2.pop %v15295 (stack89)
        %v15297 = vmul.f32 %v15296, 0.6931472 (stack90)
        %v15298 = vmul.f32 -0.5, %v15293 (stack91)
        %v15299 = vadd.f32 %v15298, 1.0 (stack92)
        %v15300 = vmul.f32 %v15299, %v15293 (stack93)
        %v15301 = vand.u32 2147483647, %v15293 (stack94)
        %vm15302 = vcmp.lt.f32.partialorder %v15301, 0.0004427343 (stack95)
        %v15303 = vsel /*vm=*/%vm15302, /*on_true_vy=*/%v15300, /*on_false_vx=*/%v15297 (stack96)
        %v15304 = vxor.u32 %v15303, 2147483648 (stack87)
        %vm15307 = vcmp.lt.f32.partialorder %v15304, 5.0 (stack86)
        %v15312 = vsel /*vm=*/%vm15307, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v15316 = vsel /*vm=*/%vm15307, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v15320 = vsel /*vm=*/%vm15307, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v15324 = vsel /*vm=*/%vm15307, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v15328 = vsel /*vm=*/%vm15307, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v15332 = vsel /*vm=*/%vm15307, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v15336 = vsel /*vm=*/%vm15307, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v15340 = vsel /*vm=*/%vm15307, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v15344 = vsel /*vm=*/%vm15307, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v15348 = vadd.f32 %v15304, -2.5 (stack82)
        %v15350 = vrsqrt.pop %v15304 (stack97)
        %v15351 = vmul.f32 %v15304, %v15350 (stack98)
        %vm15352 = vcmp.eq.f32.partialorder %v15304, inf (stack99)
        %v15353 = vsel /*vm=*/%vm15352, /*on_true_vy=*/%v15304, /*on_false_vx=*/%v15351 (stack100)
        %vm15354 = vcmp.eq.f32.partialorder %v15304, 0.0 (stack101)
        %v15355 = vand.u32 %v15304, 2147483648 (stack102)
        %v15356 = vsel /*vm=*/%vm15354, /*on_true_vy=*/%v15355, /*on_false_vx=*/%v15353 (stack103)
        %v15359 = vadd.f32 %v15356, -3.0 (stack82)
        %v15363 = vsel /*vm=*/%vm15307, /*on_true_vy=*/%v15348, /*on_false_vx=*/%v15359 (stack72)
        %v15367 = vmul.f32 %v15344, %v15363 (stack83)
        %v15371 = vadd.f32 %v15340, %v15367 (stack82)
        %v15375 = vmul.f32 %v15371, %v15363 (stack83)
        %v15379 = vadd.f32 %v15336, %v15375 (stack82)
        %v15383 = vmul.f32 %v15379, %v15363 (stack83)
        %v15387 = vadd.f32 %v15332, %v15383 (stack82)
        %v15391 = vmul.f32 %v15387, %v15363 (stack83)
        %v15395 = vadd.f32 %v15328, %v15391 (stack82)
        %v15399 = vmul.f32 %v15395, %v15363 (stack83)
        %v15403 = vadd.f32 %v15324, %v15399 (stack82)
        %v15407 = vmul.f32 %v15403, %v15363 (stack83)
        %v15411 = vadd.f32 %v15320, %v15407 (stack82)
        %v15415 = vmul.f32 %v15411, %v15363 (stack83)
        %v15419 = vadd.f32 %v15316, %v15415 (stack82)
        %v15423 = vmul.f32 %v15419, %v15363 (stack83)
        %v15427 = vadd.f32 %v15312, %v15423 (stack82)
        %v15431 = vmul.f32 %v15427, %v15278 (stack83)
        %v15435 = vsel /*vm=*/%vm15283, /*on_true_vy=*/%v15288, /*on_false_vx=*/%v15431 (stack72)
        %v15439 = vmul.f32 %v15435, 1.4140625 (stack83)
        %s15441 = scalar_lea.vmem %s280, 908 [#allocation0] (stack107)
        %v15442 = vpack.c.bf16 0.0, %v15439 (stack104)
        %15443 = vst [vmem:[%s15441] sm:$0xf] /*vst_source=*/%v15442 (stack105)
        %s15444 = sadd.s32 %s339, 32 (stack106)
        %s15445 = sshrl.u32 %s15444, 10 (stack49)
        %p15446 = scmp.lt.s32.totalorder 1, %s15445 (stack50)
        %s15447 = scalar_select /*predicate=*/%p15446, /*on_true=*/1, /*on_false=*/%s15445 (stack51)
        %s15448 = sand.u32 %s15444, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s15449 = sshrl.u32 %s15448, 7 (stack53)
        %s15450 = sand.u32 %s15448, 127 /* smod.u32 w/div 128 */ (stack54)
        %s15451 = smul.addr %s15447, 8 (stack55)
        %s15452 = scalar_lea.vmem %s3, %s15451 (stack56)
        %s15454 = scalar_lea.vmem %s15452, %s15449 (stack57)
        %v15455 = vld [vmem:[%s15454] ss:$0 sm:$0xff] (stack58)
        %s15456 = sand.u32 %s15450, 255 (stack59)
        %s15458 = sor.u32 256, %s15456 (stack60)
        %15459 = vbcast.lane.b32.xlu0 %v15455, %s15458 (stack61)
        %v15460 = vpop.permute.xlu0 %15459 (stack62)
        %s15461 = sadd.s32 %s347, 32 (stack106)
        %s15462 = sshrl.u32 %s15461, 10 (stack49)
        %p15463 = scmp.lt.s32.totalorder 1, %s15462 (stack50)
        %s15464 = scalar_select /*predicate=*/%p15463, /*on_true=*/1, /*on_false=*/%s15462 (stack51)
        %s15465 = sand.u32 %s15461, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s15466 = sshrl.u32 %s15465, 7 (stack53)
        %s15467 = sand.u32 %s15465, 127 /* smod.u32 w/div 128 */ (stack54)
        %s15468 = smul.addr %s15464, 8 (stack55)
        %s15469 = scalar_lea.vmem %s5, %s15468 (stack56)
        %s15471 = scalar_lea.vmem %s15469, %s15466 (stack57)
        %v15472 = vld [vmem:[%s15471] ss:$0 sm:$0xff] (stack58)
        %s15473 = sand.u32 %s15467, 255 (stack59)
        %s15475 = sor.u32 256, %s15473 (stack60)
        %15476 = vbcast.lane.b32.xlu0 %v15472, %s15475 (stack61)
        %v15477 = vpop.permute.xlu0 %15476 (stack62)
        %v15480 = vadd.s32 %v408, %v15477 (stack65)
        %s15482 = smul.u32 128, %s27 (stack66)
        %v15483 = vlaneseq (stack67)
        %v15484 = vand.u32 %v15483, 127 (stack68)
        %v15485 = vstv %s15482 (stack69)
        %v15486 = vadd.s32 %v15484, %v15485 (stack70)
        %v15490 = vadd.s32 %v15480, %v15486 (stack65)
        %vm15494 = vcmp.lt.u32.totalorder %v15490, %v15480 (stack71)
        %vm15499 = vcmp.lt.u32.totalorder %v15480, %v408 (stack71)
        %v15504 = vadd.s32 %v380, %v15460 (stack65)
        %v15508 = vadd.s32 %v15504, 1 (stack65)
        %v15512 = vsel /*vm=*/%vm15499, /*on_true_vy=*/%v15508, /*on_false_vx=*/%v15504 (stack72)
        %v15516 = vadd.s32 %v15512, 1 (stack65)
        %v15520 = vsel /*vm=*/%vm15494, /*on_true_vy=*/%v15516, /*on_false_vx=*/%v15512 (stack72)
        %v15525 = vadd.s32 %v15520, %v10 (stack65)
        %v15529 = vadd.s32 %v15490, %v9 (stack65)
        %v15533 = vadd.s32 %v15525, %v15529 (stack65)
        %v15535 = vshll.u32 %v15529, 13 (stack73)
        %v15536 = vshrl.u32 %v15529, 19 (stack74)
        %v15537 = vor.u32 %v15535, %v15536 (stack75)
        %v15538 = vxor.u32 %v15533, %v15537 (stack76)
        %v15541 = vadd.s32 %v15533, %v15538 (stack65)
        %v15543 = vshll.u32 %v15538, 15 (stack73)
        %v15544 = vshrl.u32 %v15538, 17 (stack74)
        %v15545 = vor.u32 %v15543, %v15544 (stack75)
        %v15546 = vxor.u32 %v15541, %v15545 (stack76)
        %v15549 = vadd.s32 %v15541, %v15546 (stack65)
        %v15551 = vshll.u32 %v15546, 26 (stack73)
        %v15552 = vshrl.u32 %v15546, 6 (stack74)
        %v15553 = vor.u32 %v15551, %v15552 (stack75)
        %v15554 = vxor.u32 %v15549, %v15553 (stack76)
        %v15557 = vadd.s32 %v15549, %v15554 (stack65)
        %v15561 = vadd.s32 %v15557, %v9 (stack65)
        %v15563 = vshll.u32 %v15554, 6 (stack73)
        %v15564 = vshrl.u32 %v15554, 26 (stack74)
        %v15565 = vor.u32 %v15563, %v15564 (stack75)
        %v15566 = vxor.u32 %v15557, %v15565 (stack76)
        %v15569 = vadd.s32 %v15566, %v8 (stack65)
        %v15573 = vadd.s32 %v15569, 1 (stack65)
        %v15577 = vadd.s32 %v15561, %v15573 (stack65)
        %v15579 = vshll.u32 %v15573, 17 (stack73)
        %v15580 = vshrl.u32 %v15573, 15 (stack74)
        %v15581 = vor.u32 %v15579, %v15580 (stack75)
        %v15582 = vxor.u32 %v15577, %v15581 (stack76)
        %v15585 = vadd.s32 %v15577, %v15582 (stack65)
        %v15587 = vshll.u32 %v15582, 29 (stack73)
        %v15588 = vshrl.u32 %v15582, 3 (stack74)
        %v15589 = vor.u32 %v15587, %v15588 (stack75)
        %v15590 = vxor.u32 %v15585, %v15589 (stack76)
        %v15593 = vadd.s32 %v15585, %v15590 (stack65)
        %v15595 = vshll.u32 %v15590, 16 (stack73)
        %v15596 = vshrl.u32 %v15590, 16 (stack74)
        %v15597 = vor.u32 %v15595, %v15596 (stack75)
        %v15598 = vxor.u32 %v15593, %v15597 (stack76)
        %v15601 = vadd.s32 %v15593, %v15598 (stack65)
        %v15605 = vadd.s32 %v15601, %v8 (stack65)
        %v15607 = vshll.u32 %v15598, 24 (stack73)
        %v15608 = vshrl.u32 %v15598, 8 (stack74)
        %v15609 = vor.u32 %v15607, %v15608 (stack75)
        %v15610 = vxor.u32 %v15601, %v15609 (stack76)
        %v15613 = vadd.s32 %v15610, %v10 (stack65)
        %v15617 = vadd.s32 %v15613, 2 (stack65)
        %v15621 = vadd.s32 %v15605, %v15617 (stack65)
        %v15623 = vshll.u32 %v15617, 13 (stack73)
        %v15624 = vshrl.u32 %v15617, 19 (stack74)
        %v15625 = vor.u32 %v15623, %v15624 (stack75)
        %v15626 = vxor.u32 %v15621, %v15625 (stack76)
        %v15629 = vadd.s32 %v15621, %v15626 (stack65)
        %v15631 = vshll.u32 %v15626, 15 (stack73)
        %v15632 = vshrl.u32 %v15626, 17 (stack74)
        %v15633 = vor.u32 %v15631, %v15632 (stack75)
        %v15634 = vxor.u32 %v15629, %v15633 (stack76)
        %v15637 = vadd.s32 %v15629, %v15634 (stack65)
        %v15639 = vshll.u32 %v15634, 26 (stack73)
        %v15640 = vshrl.u32 %v15634, 6 (stack74)
        %v15641 = vor.u32 %v15639, %v15640 (stack75)
        %v15642 = vxor.u32 %v15637, %v15641 (stack76)
        %v15645 = vadd.s32 %v15637, %v15642 (stack65)
        %v15649 = vadd.s32 %v15645, %v10 (stack65)
        %v15651 = vshll.u32 %v15642, 6 (stack73)
        %v15652 = vshrl.u32 %v15642, 26 (stack74)
        %v15653 = vor.u32 %v15651, %v15652 (stack75)
        %v15654 = vxor.u32 %v15645, %v15653 (stack76)
        %v15657 = vadd.s32 %v15654, %v9 (stack65)
        %v15661 = vadd.s32 %v15657, 3 (stack65)
        %v15665 = vadd.s32 %v15649, %v15661 (stack65)
        %v15667 = vshll.u32 %v15661, 17 (stack73)
        %v15668 = vshrl.u32 %v15661, 15 (stack74)
        %v15669 = vor.u32 %v15667, %v15668 (stack75)
        %v15670 = vxor.u32 %v15665, %v15669 (stack76)
        %v15673 = vadd.s32 %v15665, %v15670 (stack65)
        %v15675 = vshll.u32 %v15670, 29 (stack73)
        %v15676 = vshrl.u32 %v15670, 3 (stack74)
        %v15677 = vor.u32 %v15675, %v15676 (stack75)
        %v15678 = vxor.u32 %v15673, %v15677 (stack76)
        %v15681 = vadd.s32 %v15673, %v15678 (stack65)
        %v15683 = vshll.u32 %v15678, 16 (stack73)
        %v15684 = vshrl.u32 %v15678, 16 (stack74)
        %v15685 = vor.u32 %v15683, %v15684 (stack75)
        %v15686 = vxor.u32 %v15681, %v15685 (stack76)
        %v15689 = vadd.s32 %v15681, %v15686 (stack65)
        %v15693 = vadd.s32 %v15689, %v9 (stack65)
        %v15695 = vshll.u32 %v15686, 24 (stack73)
        %v15696 = vshrl.u32 %v15686, 8 (stack74)
        %v15697 = vor.u32 %v15695, %v15696 (stack75)
        %v15698 = vxor.u32 %v15689, %v15697 (stack76)
        %v15701 = vadd.s32 %v15698, %v8 (stack65)
        %v15705 = vadd.s32 %v15701, 4 (stack65)
        %v15709 = vadd.s32 %v15693, %v15705 (stack65)
        %v15711 = vshll.u32 %v15705, 13 (stack73)
        %v15712 = vshrl.u32 %v15705, 19 (stack74)
        %v15713 = vor.u32 %v15711, %v15712 (stack75)
        %v15714 = vxor.u32 %v15709, %v15713 (stack76)
        %v15717 = vadd.s32 %v15709, %v15714 (stack65)
        %v15719 = vshll.u32 %v15714, 15 (stack73)
        %v15720 = vshrl.u32 %v15714, 17 (stack74)
        %v15721 = vor.u32 %v15719, %v15720 (stack75)
        %v15722 = vxor.u32 %v15717, %v15721 (stack76)
        %v15725 = vadd.s32 %v15717, %v15722 (stack65)
        %v15727 = vshll.u32 %v15722, 26 (stack73)
        %v15728 = vshrl.u32 %v15722, 6 (stack74)
        %v15729 = vor.u32 %v15727, %v15728 (stack75)
        %v15730 = vxor.u32 %v15725, %v15729 (stack76)
        %v15733 = vadd.s32 %v15725, %v15730 (stack65)
        %v15737 = vadd.s32 %v15733, %v8 (stack65)
        %v15739 = vshll.u32 %v15730, 6 (stack73)
        %v15740 = vshrl.u32 %v15730, 26 (stack74)
        %v15741 = vor.u32 %v15739, %v15740 (stack75)
        %v15742 = vxor.u32 %v15733, %v15741 (stack76)
        %v15745 = vadd.s32 %v15742, %v10 (stack65)
        %v15749 = vadd.s32 %v15745, 5 (stack65)
        %v15751 = vxor.u32 %v15737, %v15749 (stack76)
        %v15752 = vand.u32.u8 %v15751, 255 (stack77)
        %v15753 = vand.u32 %v15752, 65535 (stack78)
        %v15754 = vshrl.u32 %v15753, 1 (stack79)
        %v15755 = vor.u32 %v15754, 16256 (stack75)
        %v15756 = vand.u32.u16 %v15755, 65535 (stack80)
        %v15757 = vunpack.i.l.bf16 %v15756 (stack81)
        %v15761 = vadd.f32 %v15757, -1.0 (stack82)
        %v15765 = vmul.f32 %v15761, 2.0 (stack83)
        %v15769 = vadd.f32 %v15765, -0.99609375 (stack82)
        %v15773 = vmax.f32 -0.99609375, %v15769 (stack84)
        %v15775 = vand.u32 2147483647, %v15773 (stack85)
        %vm15778 = vcmp.eq.f32.partialorder %v15775, 1.0 (stack86)
        %v15783 = vmul.f32 %v15773, inf (stack83)
        %v15785 = vxor.u32 %v15773, 2147483648 (stack87)
        %v15788 = vmul.f32 %v15773, %v15785 (stack83)
        %v15790 = vadd.f32 %v15788, 1.0 (stack88)
        %v15791 = vlog2.pop %v15790 (stack89)
        %v15792 = vmul.f32 %v15791, 0.6931472 (stack90)
        %v15793 = vmul.f32 -0.5, %v15788 (stack91)
        %v15794 = vadd.f32 %v15793, 1.0 (stack92)
        %v15795 = vmul.f32 %v15794, %v15788 (stack93)
        %v15796 = vand.u32 2147483647, %v15788 (stack94)
        %vm15797 = vcmp.lt.f32.partialorder %v15796, 0.0004427343 (stack95)
        %v15798 = vsel /*vm=*/%vm15797, /*on_true_vy=*/%v15795, /*on_false_vx=*/%v15792 (stack96)
        %v15799 = vxor.u32 %v15798, 2147483648 (stack87)
        %vm15802 = vcmp.lt.f32.partialorder %v15799, 5.0 (stack86)
        %v15807 = vsel /*vm=*/%vm15802, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v15811 = vsel /*vm=*/%vm15802, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v15815 = vsel /*vm=*/%vm15802, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v15819 = vsel /*vm=*/%vm15802, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v15823 = vsel /*vm=*/%vm15802, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v15827 = vsel /*vm=*/%vm15802, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v15831 = vsel /*vm=*/%vm15802, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v15835 = vsel /*vm=*/%vm15802, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v15839 = vsel /*vm=*/%vm15802, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v15843 = vadd.f32 %v15799, -2.5 (stack82)
        %v15845 = vrsqrt.pop %v15799 (stack97)
        %v15846 = vmul.f32 %v15799, %v15845 (stack98)
        %vm15847 = vcmp.eq.f32.partialorder %v15799, inf (stack99)
        %v15848 = vsel /*vm=*/%vm15847, /*on_true_vy=*/%v15799, /*on_false_vx=*/%v15846 (stack100)
        %vm15849 = vcmp.eq.f32.partialorder %v15799, 0.0 (stack101)
        %v15850 = vand.u32 %v15799, 2147483648 (stack102)
        %v15851 = vsel /*vm=*/%vm15849, /*on_true_vy=*/%v15850, /*on_false_vx=*/%v15848 (stack103)
        %v15854 = vadd.f32 %v15851, -3.0 (stack82)
        %v15858 = vsel /*vm=*/%vm15802, /*on_true_vy=*/%v15843, /*on_false_vx=*/%v15854 (stack72)
        %v15862 = vmul.f32 %v15839, %v15858 (stack83)
        %v15866 = vadd.f32 %v15835, %v15862 (stack82)
        %v15870 = vmul.f32 %v15866, %v15858 (stack83)
        %v15874 = vadd.f32 %v15831, %v15870 (stack82)
        %v15878 = vmul.f32 %v15874, %v15858 (stack83)
        %v15882 = vadd.f32 %v15827, %v15878 (stack82)
        %v15886 = vmul.f32 %v15882, %v15858 (stack83)
        %v15890 = vadd.f32 %v15823, %v15886 (stack82)
        %v15894 = vmul.f32 %v15890, %v15858 (stack83)
        %v15898 = vadd.f32 %v15819, %v15894 (stack82)
        %v15902 = vmul.f32 %v15898, %v15858 (stack83)
        %v15906 = vadd.f32 %v15815, %v15902 (stack82)
        %v15910 = vmul.f32 %v15906, %v15858 (stack83)
        %v15914 = vadd.f32 %v15811, %v15910 (stack82)
        %v15918 = vmul.f32 %v15914, %v15858 (stack83)
        %v15922 = vadd.f32 %v15807, %v15918 (stack82)
        %v15926 = vmul.f32 %v15922, %v15773 (stack83)
        %v15930 = vsel /*vm=*/%vm15778, /*on_true_vy=*/%v15783, /*on_false_vx=*/%v15926 (stack72)
        %v15934 = vmul.f32 %v15930, 1.4140625 (stack83)
        %s15936 = scalar_lea.vmem %s280, 16 [#allocation0] (stack107)
        %v15937 = vpack.c.bf16 0.0, %v15934 (stack104)
        %15938 = vst [vmem:[%s15936] sm:$0xf] /*vst_source=*/%v15937 (stack105)
        %v15941 = vadd.s32 %v894, %v15477 (stack65)
        %s15943 = smul.u32 128, %s27 (stack66)
        %v15944 = vlaneseq (stack67)
        %v15945 = vand.u32 %v15944, 127 (stack68)
        %v15946 = vstv %s15943 (stack69)
        %v15947 = vadd.s32 %v15945, %v15946 (stack70)
        %v15951 = vadd.s32 %v15941, %v15947 (stack65)
        %vm15955 = vcmp.lt.u32.totalorder %v15951, %v15941 (stack71)
        %vm15960 = vcmp.lt.u32.totalorder %v15941, %v894 (stack71)
        %v15965 = vadd.s32 %v881, %v15460 (stack65)
        %v15969 = vadd.s32 %v15965, 1 (stack65)
        %v15973 = vsel /*vm=*/%vm15960, /*on_true_vy=*/%v15969, /*on_false_vx=*/%v15965 (stack72)
        %v15977 = vadd.s32 %v15973, 1 (stack65)
        %v15981 = vsel /*vm=*/%vm15955, /*on_true_vy=*/%v15977, /*on_false_vx=*/%v15973 (stack72)
        %v15986 = vadd.s32 %v15981, %v10 (stack65)
        %v15990 = vadd.s32 %v15951, %v9 (stack65)
        %v15994 = vadd.s32 %v15986, %v15990 (stack65)
        %v15996 = vshll.u32 %v15990, 13 (stack73)
        %v15997 = vshrl.u32 %v15990, 19 (stack74)
        %v15998 = vor.u32 %v15996, %v15997 (stack75)
        %v15999 = vxor.u32 %v15994, %v15998 (stack76)
        %v16002 = vadd.s32 %v15994, %v15999 (stack65)
        %v16004 = vshll.u32 %v15999, 15 (stack73)
        %v16005 = vshrl.u32 %v15999, 17 (stack74)
        %v16006 = vor.u32 %v16004, %v16005 (stack75)
        %v16007 = vxor.u32 %v16002, %v16006 (stack76)
        %v16010 = vadd.s32 %v16002, %v16007 (stack65)
        %v16012 = vshll.u32 %v16007, 26 (stack73)
        %v16013 = vshrl.u32 %v16007, 6 (stack74)
        %v16014 = vor.u32 %v16012, %v16013 (stack75)
        %v16015 = vxor.u32 %v16010, %v16014 (stack76)
        %v16018 = vadd.s32 %v16010, %v16015 (stack65)
        %v16022 = vadd.s32 %v16018, %v9 (stack65)
        %v16024 = vshll.u32 %v16015, 6 (stack73)
        %v16025 = vshrl.u32 %v16015, 26 (stack74)
        %v16026 = vor.u32 %v16024, %v16025 (stack75)
        %v16027 = vxor.u32 %v16018, %v16026 (stack76)
        %v16030 = vadd.s32 %v16027, %v8 (stack65)
        %v16034 = vadd.s32 %v16030, 1 (stack65)
        %v16038 = vadd.s32 %v16022, %v16034 (stack65)
        %v16040 = vshll.u32 %v16034, 17 (stack73)
        %v16041 = vshrl.u32 %v16034, 15 (stack74)
        %v16042 = vor.u32 %v16040, %v16041 (stack75)
        %v16043 = vxor.u32 %v16038, %v16042 (stack76)
        %v16046 = vadd.s32 %v16038, %v16043 (stack65)
        %v16048 = vshll.u32 %v16043, 29 (stack73)
        %v16049 = vshrl.u32 %v16043, 3 (stack74)
        %v16050 = vor.u32 %v16048, %v16049 (stack75)
        %v16051 = vxor.u32 %v16046, %v16050 (stack76)
        %v16054 = vadd.s32 %v16046, %v16051 (stack65)
        %v16056 = vshll.u32 %v16051, 16 (stack73)
        %v16057 = vshrl.u32 %v16051, 16 (stack74)
        %v16058 = vor.u32 %v16056, %v16057 (stack75)
        %v16059 = vxor.u32 %v16054, %v16058 (stack76)
        %v16062 = vadd.s32 %v16054, %v16059 (stack65)
        %v16066 = vadd.s32 %v16062, %v8 (stack65)
        %v16068 = vshll.u32 %v16059, 24 (stack73)
        %v16069 = vshrl.u32 %v16059, 8 (stack74)
        %v16070 = vor.u32 %v16068, %v16069 (stack75)
        %v16071 = vxor.u32 %v16062, %v16070 (stack76)
        %v16074 = vadd.s32 %v16071, %v10 (stack65)
        %v16078 = vadd.s32 %v16074, 2 (stack65)
        %v16082 = vadd.s32 %v16066, %v16078 (stack65)
        %v16084 = vshll.u32 %v16078, 13 (stack73)
        %v16085 = vshrl.u32 %v16078, 19 (stack74)
        %v16086 = vor.u32 %v16084, %v16085 (stack75)
        %v16087 = vxor.u32 %v16082, %v16086 (stack76)
        %v16090 = vadd.s32 %v16082, %v16087 (stack65)
        %v16092 = vshll.u32 %v16087, 15 (stack73)
        %v16093 = vshrl.u32 %v16087, 17 (stack74)
        %v16094 = vor.u32 %v16092, %v16093 (stack75)
        %v16095 = vxor.u32 %v16090, %v16094 (stack76)
        %v16098 = vadd.s32 %v16090, %v16095 (stack65)
        %v16100 = vshll.u32 %v16095, 26 (stack73)
        %v16101 = vshrl.u32 %v16095, 6 (stack74)
        %v16102 = vor.u32 %v16100, %v16101 (stack75)
        %v16103 = vxor.u32 %v16098, %v16102 (stack76)
        %v16106 = vadd.s32 %v16098, %v16103 (stack65)
        %v16110 = vadd.s32 %v16106, %v10 (stack65)
        %v16112 = vshll.u32 %v16103, 6 (stack73)
        %v16113 = vshrl.u32 %v16103, 26 (stack74)
        %v16114 = vor.u32 %v16112, %v16113 (stack75)
        %v16115 = vxor.u32 %v16106, %v16114 (stack76)
        %v16118 = vadd.s32 %v16115, %v9 (stack65)
        %v16122 = vadd.s32 %v16118, 3 (stack65)
        %v16126 = vadd.s32 %v16110, %v16122 (stack65)
        %v16128 = vshll.u32 %v16122, 17 (stack73)
        %v16129 = vshrl.u32 %v16122, 15 (stack74)
        %v16130 = vor.u32 %v16128, %v16129 (stack75)
        %v16131 = vxor.u32 %v16126, %v16130 (stack76)
        %v16134 = vadd.s32 %v16126, %v16131 (stack65)
        %v16136 = vshll.u32 %v16131, 29 (stack73)
        %v16137 = vshrl.u32 %v16131, 3 (stack74)
        %v16138 = vor.u32 %v16136, %v16137 (stack75)
        %v16139 = vxor.u32 %v16134, %v16138 (stack76)
        %v16142 = vadd.s32 %v16134, %v16139 (stack65)
        %v16144 = vshll.u32 %v16139, 16 (stack73)
        %v16145 = vshrl.u32 %v16139, 16 (stack74)
        %v16146 = vor.u32 %v16144, %v16145 (stack75)
        %v16147 = vxor.u32 %v16142, %v16146 (stack76)
        %v16150 = vadd.s32 %v16142, %v16147 (stack65)
        %v16154 = vadd.s32 %v16150, %v9 (stack65)
        %v16156 = vshll.u32 %v16147, 24 (stack73)
        %v16157 = vshrl.u32 %v16147, 8 (stack74)
        %v16158 = vor.u32 %v16156, %v16157 (stack75)
        %v16159 = vxor.u32 %v16150, %v16158 (stack76)
        %v16162 = vadd.s32 %v16159, %v8 (stack65)
        %v16166 = vadd.s32 %v16162, 4 (stack65)
        %v16170 = vadd.s32 %v16154, %v16166 (stack65)
        %v16172 = vshll.u32 %v16166, 13 (stack73)
        %v16173 = vshrl.u32 %v16166, 19 (stack74)
        %v16174 = vor.u32 %v16172, %v16173 (stack75)
        %v16175 = vxor.u32 %v16170, %v16174 (stack76)
        %v16178 = vadd.s32 %v16170, %v16175 (stack65)
        %v16180 = vshll.u32 %v16175, 15 (stack73)
        %v16181 = vshrl.u32 %v16175, 17 (stack74)
        %v16182 = vor.u32 %v16180, %v16181 (stack75)
        %v16183 = vxor.u32 %v16178, %v16182 (stack76)
        %v16186 = vadd.s32 %v16178, %v16183 (stack65)
        %v16188 = vshll.u32 %v16183, 26 (stack73)
        %v16189 = vshrl.u32 %v16183, 6 (stack74)
        %v16190 = vor.u32 %v16188, %v16189 (stack75)
        %v16191 = vxor.u32 %v16186, %v16190 (stack76)
        %v16194 = vadd.s32 %v16186, %v16191 (stack65)
        %v16198 = vadd.s32 %v16194, %v8 (stack65)
        %v16200 = vshll.u32 %v16191, 6 (stack73)
        %v16201 = vshrl.u32 %v16191, 26 (stack74)
        %v16202 = vor.u32 %v16200, %v16201 (stack75)
        %v16203 = vxor.u32 %v16194, %v16202 (stack76)
        %v16206 = vadd.s32 %v16203, %v10 (stack65)
        %v16210 = vadd.s32 %v16206, 5 (stack65)
        %v16212 = vxor.u32 %v16198, %v16210 (stack76)
        %v16213 = vand.u32.u8 %v16212, 255 (stack77)
        %v16214 = vand.u32 %v16213, 65535 (stack78)
        %v16215 = vshrl.u32 %v16214, 1 (stack79)
        %v16216 = vor.u32 %v16215, 16256 (stack75)
        %v16217 = vand.u32.u16 %v16216, 65535 (stack80)
        %v16218 = vunpack.i.l.bf16 %v16217 (stack81)
        %v16222 = vadd.f32 %v16218, -1.0 (stack82)
        %v16226 = vmul.f32 %v16222, 2.0 (stack83)
        %v16230 = vadd.f32 %v16226, -0.99609375 (stack82)
        %v16234 = vmax.f32 -0.99609375, %v16230 (stack84)
        %v16236 = vand.u32 2147483647, %v16234 (stack85)
        %vm16239 = vcmp.eq.f32.partialorder %v16236, 1.0 (stack86)
        %v16244 = vmul.f32 %v16234, inf (stack83)
        %v16246 = vxor.u32 %v16234, 2147483648 (stack87)
        %v16249 = vmul.f32 %v16234, %v16246 (stack83)
        %v16251 = vadd.f32 %v16249, 1.0 (stack88)
        %v16252 = vlog2.pop %v16251 (stack89)
        %v16253 = vmul.f32 %v16252, 0.6931472 (stack90)
        %v16254 = vmul.f32 -0.5, %v16249 (stack91)
        %v16255 = vadd.f32 %v16254, 1.0 (stack92)
        %v16256 = vmul.f32 %v16255, %v16249 (stack93)
        %v16257 = vand.u32 2147483647, %v16249 (stack94)
        %vm16258 = vcmp.lt.f32.partialorder %v16257, 0.0004427343 (stack95)
        %v16259 = vsel /*vm=*/%vm16258, /*on_true_vy=*/%v16256, /*on_false_vx=*/%v16253 (stack96)
        %v16260 = vxor.u32 %v16259, 2147483648 (stack87)
        %vm16263 = vcmp.lt.f32.partialorder %v16260, 5.0 (stack86)
        %v16268 = vsel /*vm=*/%vm16263, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v16272 = vsel /*vm=*/%vm16263, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v16276 = vsel /*vm=*/%vm16263, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v16280 = vsel /*vm=*/%vm16263, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v16284 = vsel /*vm=*/%vm16263, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v16288 = vsel /*vm=*/%vm16263, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v16292 = vsel /*vm=*/%vm16263, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v16296 = vsel /*vm=*/%vm16263, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v16300 = vsel /*vm=*/%vm16263, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v16304 = vadd.f32 %v16260, -2.5 (stack82)
        %v16306 = vrsqrt.pop %v16260 (stack97)
        %v16307 = vmul.f32 %v16260, %v16306 (stack98)
        %vm16308 = vcmp.eq.f32.partialorder %v16260, inf (stack99)
        %v16309 = vsel /*vm=*/%vm16308, /*on_true_vy=*/%v16260, /*on_false_vx=*/%v16307 (stack100)
        %vm16310 = vcmp.eq.f32.partialorder %v16260, 0.0 (stack101)
        %v16311 = vand.u32 %v16260, 2147483648 (stack102)
        %v16312 = vsel /*vm=*/%vm16310, /*on_true_vy=*/%v16311, /*on_false_vx=*/%v16309 (stack103)
        %v16315 = vadd.f32 %v16312, -3.0 (stack82)
        %v16319 = vsel /*vm=*/%vm16263, /*on_true_vy=*/%v16304, /*on_false_vx=*/%v16315 (stack72)
        %v16323 = vmul.f32 %v16300, %v16319 (stack83)
        %v16327 = vadd.f32 %v16296, %v16323 (stack82)
        %v16331 = vmul.f32 %v16327, %v16319 (stack83)
        %v16335 = vadd.f32 %v16292, %v16331 (stack82)
        %v16339 = vmul.f32 %v16335, %v16319 (stack83)
        %v16343 = vadd.f32 %v16288, %v16339 (stack82)
        %v16347 = vmul.f32 %v16343, %v16319 (stack83)
        %v16351 = vadd.f32 %v16284, %v16347 (stack82)
        %v16355 = vmul.f32 %v16351, %v16319 (stack83)
        %v16359 = vadd.f32 %v16280, %v16355 (stack82)
        %v16363 = vmul.f32 %v16359, %v16319 (stack83)
        %v16367 = vadd.f32 %v16276, %v16363 (stack82)
        %v16371 = vmul.f32 %v16367, %v16319 (stack83)
        %v16375 = vadd.f32 %v16272, %v16371 (stack82)
        %v16379 = vmul.f32 %v16375, %v16319 (stack83)
        %v16383 = vadd.f32 %v16268, %v16379 (stack82)
        %v16387 = vmul.f32 %v16383, %v16234 (stack83)
        %v16391 = vsel /*vm=*/%vm16239, /*on_true_vy=*/%v16244, /*on_false_vx=*/%v16387 (stack72)
        %v16395 = vmul.f32 %v16391, 1.4140625 (stack83)
        %s16397 = scalar_lea.vmem %s280, 144 [#allocation0] (stack107)
        %v16398 = vpack.c.bf16 0.0, %v16395 (stack104)
        %16399 = vst [vmem:[%s16397] sm:$0xf] /*vst_source=*/%v16398 (stack105)
        %v16402 = vadd.s32 %v1381, %v15477 (stack65)
        %s16404 = smul.u32 128, %s27 (stack66)
        %v16405 = vlaneseq (stack67)
        %v16406 = vand.u32 %v16405, 127 (stack68)
        %v16407 = vstv %s16404 (stack69)
        %v16408 = vadd.s32 %v16406, %v16407 (stack70)
        %v16412 = vadd.s32 %v16402, %v16408 (stack65)
        %vm16416 = vcmp.lt.u32.totalorder %v16412, %v16402 (stack71)
        %vm16421 = vcmp.lt.u32.totalorder %v16402, %v1381 (stack71)
        %v16426 = vadd.s32 %v1368, %v15460 (stack65)
        %v16430 = vadd.s32 %v16426, 1 (stack65)
        %v16434 = vsel /*vm=*/%vm16421, /*on_true_vy=*/%v16430, /*on_false_vx=*/%v16426 (stack72)
        %v16438 = vadd.s32 %v16434, 1 (stack65)
        %v16442 = vsel /*vm=*/%vm16416, /*on_true_vy=*/%v16438, /*on_false_vx=*/%v16434 (stack72)
        %v16447 = vadd.s32 %v16442, %v10 (stack65)
        %v16451 = vadd.s32 %v16412, %v9 (stack65)
        %v16455 = vadd.s32 %v16447, %v16451 (stack65)
        %v16457 = vshll.u32 %v16451, 13 (stack73)
        %v16458 = vshrl.u32 %v16451, 19 (stack74)
        %v16459 = vor.u32 %v16457, %v16458 (stack75)
        %v16460 = vxor.u32 %v16455, %v16459 (stack76)
        %v16463 = vadd.s32 %v16455, %v16460 (stack65)
        %v16465 = vshll.u32 %v16460, 15 (stack73)
        %v16466 = vshrl.u32 %v16460, 17 (stack74)
        %v16467 = vor.u32 %v16465, %v16466 (stack75)
        %v16468 = vxor.u32 %v16463, %v16467 (stack76)
        %v16471 = vadd.s32 %v16463, %v16468 (stack65)
        %v16473 = vshll.u32 %v16468, 26 (stack73)
        %v16474 = vshrl.u32 %v16468, 6 (stack74)
        %v16475 = vor.u32 %v16473, %v16474 (stack75)
        %v16476 = vxor.u32 %v16471, %v16475 (stack76)
        %v16479 = vadd.s32 %v16471, %v16476 (stack65)
        %v16483 = vadd.s32 %v16479, %v9 (stack65)
        %v16485 = vshll.u32 %v16476, 6 (stack73)
        %v16486 = vshrl.u32 %v16476, 26 (stack74)
        %v16487 = vor.u32 %v16485, %v16486 (stack75)
        %v16488 = vxor.u32 %v16479, %v16487 (stack76)
        %v16491 = vadd.s32 %v16488, %v8 (stack65)
        %v16495 = vadd.s32 %v16491, 1 (stack65)
        %v16499 = vadd.s32 %v16483, %v16495 (stack65)
        %v16501 = vshll.u32 %v16495, 17 (stack73)
        %v16502 = vshrl.u32 %v16495, 15 (stack74)
        %v16503 = vor.u32 %v16501, %v16502 (stack75)
        %v16504 = vxor.u32 %v16499, %v16503 (stack76)
        %v16507 = vadd.s32 %v16499, %v16504 (stack65)
        %v16509 = vshll.u32 %v16504, 29 (stack73)
        %v16510 = vshrl.u32 %v16504, 3 (stack74)
        %v16511 = vor.u32 %v16509, %v16510 (stack75)
        %v16512 = vxor.u32 %v16507, %v16511 (stack76)
        %v16515 = vadd.s32 %v16507, %v16512 (stack65)
        %v16517 = vshll.u32 %v16512, 16 (stack73)
        %v16518 = vshrl.u32 %v16512, 16 (stack74)
        %v16519 = vor.u32 %v16517, %v16518 (stack75)
        %v16520 = vxor.u32 %v16515, %v16519 (stack76)
        %v16523 = vadd.s32 %v16515, %v16520 (stack65)
        %v16527 = vadd.s32 %v16523, %v8 (stack65)
        %v16529 = vshll.u32 %v16520, 24 (stack73)
        %v16530 = vshrl.u32 %v16520, 8 (stack74)
        %v16531 = vor.u32 %v16529, %v16530 (stack75)
        %v16532 = vxor.u32 %v16523, %v16531 (stack76)
        %v16535 = vadd.s32 %v16532, %v10 (stack65)
        %v16539 = vadd.s32 %v16535, 2 (stack65)
        %v16543 = vadd.s32 %v16527, %v16539 (stack65)
        %v16545 = vshll.u32 %v16539, 13 (stack73)
        %v16546 = vshrl.u32 %v16539, 19 (stack74)
        %v16547 = vor.u32 %v16545, %v16546 (stack75)
        %v16548 = vxor.u32 %v16543, %v16547 (stack76)
        %v16551 = vadd.s32 %v16543, %v16548 (stack65)
        %v16553 = vshll.u32 %v16548, 15 (stack73)
        %v16554 = vshrl.u32 %v16548, 17 (stack74)
        %v16555 = vor.u32 %v16553, %v16554 (stack75)
        %v16556 = vxor.u32 %v16551, %v16555 (stack76)
        %v16559 = vadd.s32 %v16551, %v16556 (stack65)
        %v16561 = vshll.u32 %v16556, 26 (stack73)
        %v16562 = vshrl.u32 %v16556, 6 (stack74)
        %v16563 = vor.u32 %v16561, %v16562 (stack75)
        %v16564 = vxor.u32 %v16559, %v16563 (stack76)
        %v16567 = vadd.s32 %v16559, %v16564 (stack65)
        %v16571 = vadd.s32 %v16567, %v10 (stack65)
        %v16573 = vshll.u32 %v16564, 6 (stack73)
        %v16574 = vshrl.u32 %v16564, 26 (stack74)
        %v16575 = vor.u32 %v16573, %v16574 (stack75)
        %v16576 = vxor.u32 %v16567, %v16575 (stack76)
        %v16579 = vadd.s32 %v16576, %v9 (stack65)
        %v16583 = vadd.s32 %v16579, 3 (stack65)
        %v16587 = vadd.s32 %v16571, %v16583 (stack65)
        %v16589 = vshll.u32 %v16583, 17 (stack73)
        %v16590 = vshrl.u32 %v16583, 15 (stack74)
        %v16591 = vor.u32 %v16589, %v16590 (stack75)
        %v16592 = vxor.u32 %v16587, %v16591 (stack76)
        %v16595 = vadd.s32 %v16587, %v16592 (stack65)
        %v16597 = vshll.u32 %v16592, 29 (stack73)
        %v16598 = vshrl.u32 %v16592, 3 (stack74)
        %v16599 = vor.u32 %v16597, %v16598 (stack75)
        %v16600 = vxor.u32 %v16595, %v16599 (stack76)
        %v16603 = vadd.s32 %v16595, %v16600 (stack65)
        %v16605 = vshll.u32 %v16600, 16 (stack73)
        %v16606 = vshrl.u32 %v16600, 16 (stack74)
        %v16607 = vor.u32 %v16605, %v16606 (stack75)
        %v16608 = vxor.u32 %v16603, %v16607 (stack76)
        %v16611 = vadd.s32 %v16603, %v16608 (stack65)
        %v16615 = vadd.s32 %v16611, %v9 (stack65)
        %v16617 = vshll.u32 %v16608, 24 (stack73)
        %v16618 = vshrl.u32 %v16608, 8 (stack74)
        %v16619 = vor.u32 %v16617, %v16618 (stack75)
        %v16620 = vxor.u32 %v16611, %v16619 (stack76)
        %v16623 = vadd.s32 %v16620, %v8 (stack65)
        %v16627 = vadd.s32 %v16623, 4 (stack65)
        %v16631 = vadd.s32 %v16615, %v16627 (stack65)
        %v16633 = vshll.u32 %v16627, 13 (stack73)
        %v16634 = vshrl.u32 %v16627, 19 (stack74)
        %v16635 = vor.u32 %v16633, %v16634 (stack75)
        %v16636 = vxor.u32 %v16631, %v16635 (stack76)
        %v16639 = vadd.s32 %v16631, %v16636 (stack65)
        %v16641 = vshll.u32 %v16636, 15 (stack73)
        %v16642 = vshrl.u32 %v16636, 17 (stack74)
        %v16643 = vor.u32 %v16641, %v16642 (stack75)
        %v16644 = vxor.u32 %v16639, %v16643 (stack76)
        %v16647 = vadd.s32 %v16639, %v16644 (stack65)
        %v16649 = vshll.u32 %v16644, 26 (stack73)
        %v16650 = vshrl.u32 %v16644, 6 (stack74)
        %v16651 = vor.u32 %v16649, %v16650 (stack75)
        %v16652 = vxor.u32 %v16647, %v16651 (stack76)
        %v16655 = vadd.s32 %v16647, %v16652 (stack65)
        %v16659 = vadd.s32 %v16655, %v8 (stack65)
        %v16661 = vshll.u32 %v16652, 6 (stack73)
        %v16662 = vshrl.u32 %v16652, 26 (stack74)
        %v16663 = vor.u32 %v16661, %v16662 (stack75)
        %v16664 = vxor.u32 %v16655, %v16663 (stack76)
        %v16667 = vadd.s32 %v16664, %v10 (stack65)
        %v16671 = vadd.s32 %v16667, 5 (stack65)
        %v16673 = vxor.u32 %v16659, %v16671 (stack76)
        %v16674 = vand.u32.u8 %v16673, 255 (stack77)
        %v16675 = vand.u32 %v16674, 65535 (stack78)
        %v16676 = vshrl.u32 %v16675, 1 (stack79)
        %v16677 = vor.u32 %v16676, 16256 (stack75)
        %v16678 = vand.u32.u16 %v16677, 65535 (stack80)
        %v16679 = vunpack.i.l.bf16 %v16678 (stack81)
        %v16683 = vadd.f32 %v16679, -1.0 (stack82)
        %v16687 = vmul.f32 %v16683, 2.0 (stack83)
        %v16691 = vadd.f32 %v16687, -0.99609375 (stack82)
        %v16695 = vmax.f32 -0.99609375, %v16691 (stack84)
        %v16697 = vand.u32 2147483647, %v16695 (stack85)
        %vm16700 = vcmp.eq.f32.partialorder %v16697, 1.0 (stack86)
        %v16705 = vmul.f32 %v16695, inf (stack83)
        %v16707 = vxor.u32 %v16695, 2147483648 (stack87)
        %v16710 = vmul.f32 %v16695, %v16707 (stack83)
        %v16712 = vadd.f32 %v16710, 1.0 (stack88)
        %v16713 = vlog2.pop %v16712 (stack89)
        %v16714 = vmul.f32 %v16713, 0.6931472 (stack90)
        %v16715 = vmul.f32 -0.5, %v16710 (stack91)
        %v16716 = vadd.f32 %v16715, 1.0 (stack92)
        %v16717 = vmul.f32 %v16716, %v16710 (stack93)
        %v16718 = vand.u32 2147483647, %v16710 (stack94)
        %vm16719 = vcmp.lt.f32.partialorder %v16718, 0.0004427343 (stack95)
        %v16720 = vsel /*vm=*/%vm16719, /*on_true_vy=*/%v16717, /*on_false_vx=*/%v16714 (stack96)
        %v16721 = vxor.u32 %v16720, 2147483648 (stack87)
        %vm16724 = vcmp.lt.f32.partialorder %v16721, 5.0 (stack86)
        %v16729 = vsel /*vm=*/%vm16724, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v16733 = vsel /*vm=*/%vm16724, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v16737 = vsel /*vm=*/%vm16724, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v16741 = vsel /*vm=*/%vm16724, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v16745 = vsel /*vm=*/%vm16724, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v16749 = vsel /*vm=*/%vm16724, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v16753 = vsel /*vm=*/%vm16724, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v16757 = vsel /*vm=*/%vm16724, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v16761 = vsel /*vm=*/%vm16724, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v16765 = vadd.f32 %v16721, -2.5 (stack82)
        %v16767 = vrsqrt.pop %v16721 (stack97)
        %v16768 = vmul.f32 %v16721, %v16767 (stack98)
        %vm16769 = vcmp.eq.f32.partialorder %v16721, inf (stack99)
        %v16770 = vsel /*vm=*/%vm16769, /*on_true_vy=*/%v16721, /*on_false_vx=*/%v16768 (stack100)
        %vm16771 = vcmp.eq.f32.partialorder %v16721, 0.0 (stack101)
        %v16772 = vand.u32 %v16721, 2147483648 (stack102)
        %v16773 = vsel /*vm=*/%vm16771, /*on_true_vy=*/%v16772, /*on_false_vx=*/%v16770 (stack103)
        %v16776 = vadd.f32 %v16773, -3.0 (stack82)
        %v16780 = vsel /*vm=*/%vm16724, /*on_true_vy=*/%v16765, /*on_false_vx=*/%v16776 (stack72)
        %v16784 = vmul.f32 %v16761, %v16780 (stack83)
        %v16788 = vadd.f32 %v16757, %v16784 (stack82)
        %v16792 = vmul.f32 %v16788, %v16780 (stack83)
        %v16796 = vadd.f32 %v16753, %v16792 (stack82)
        %v16800 = vmul.f32 %v16796, %v16780 (stack83)
        %v16804 = vadd.f32 %v16749, %v16800 (stack82)
        %v16808 = vmul.f32 %v16804, %v16780 (stack83)
        %v16812 = vadd.f32 %v16745, %v16808 (stack82)
        %v16816 = vmul.f32 %v16812, %v16780 (stack83)
        %v16820 = vadd.f32 %v16741, %v16816 (stack82)
        %v16824 = vmul.f32 %v16820, %v16780 (stack83)
        %v16828 = vadd.f32 %v16737, %v16824 (stack82)
        %v16832 = vmul.f32 %v16828, %v16780 (stack83)
        %v16836 = vadd.f32 %v16733, %v16832 (stack82)
        %v16840 = vmul.f32 %v16836, %v16780 (stack83)
        %v16844 = vadd.f32 %v16729, %v16840 (stack82)
        %v16848 = vmul.f32 %v16844, %v16695 (stack83)
        %v16852 = vsel /*vm=*/%vm16700, /*on_true_vy=*/%v16705, /*on_false_vx=*/%v16848 (stack72)
        %v16856 = vmul.f32 %v16852, 1.4140625 (stack83)
        %s16858 = scalar_lea.vmem %s280, 272 [#allocation0] (stack107)
        %v16859 = vpack.c.bf16 0.0, %v16856 (stack104)
        %16860 = vst [vmem:[%s16858] sm:$0xf] /*vst_source=*/%v16859 (stack105)
        %v16863 = vadd.s32 %v1868, %v15477 (stack65)
        %s16865 = smul.u32 128, %s27 (stack66)
        %v16866 = vlaneseq (stack67)
        %v16867 = vand.u32 %v16866, 127 (stack68)
        %v16868 = vstv %s16865 (stack69)
        %v16869 = vadd.s32 %v16867, %v16868 (stack70)
        %v16873 = vadd.s32 %v16863, %v16869 (stack65)
        %vm16877 = vcmp.lt.u32.totalorder %v16873, %v16863 (stack71)
        %vm16882 = vcmp.lt.u32.totalorder %v16863, %v1868 (stack71)
        %v16887 = vadd.s32 %v1855, %v15460 (stack65)
        %v16891 = vadd.s32 %v16887, 1 (stack65)
        %v16895 = vsel /*vm=*/%vm16882, /*on_true_vy=*/%v16891, /*on_false_vx=*/%v16887 (stack72)
        %v16899 = vadd.s32 %v16895, 1 (stack65)
        %v16903 = vsel /*vm=*/%vm16877, /*on_true_vy=*/%v16899, /*on_false_vx=*/%v16895 (stack72)
        %v16908 = vadd.s32 %v16903, %v10 (stack65)
        %v16912 = vadd.s32 %v16873, %v9 (stack65)
        %v16916 = vadd.s32 %v16908, %v16912 (stack65)
        %v16918 = vshll.u32 %v16912, 13 (stack73)
        %v16919 = vshrl.u32 %v16912, 19 (stack74)
        %v16920 = vor.u32 %v16918, %v16919 (stack75)
        %v16921 = vxor.u32 %v16916, %v16920 (stack76)
        %v16924 = vadd.s32 %v16916, %v16921 (stack65)
        %v16926 = vshll.u32 %v16921, 15 (stack73)
        %v16927 = vshrl.u32 %v16921, 17 (stack74)
        %v16928 = vor.u32 %v16926, %v16927 (stack75)
        %v16929 = vxor.u32 %v16924, %v16928 (stack76)
        %v16932 = vadd.s32 %v16924, %v16929 (stack65)
        %v16934 = vshll.u32 %v16929, 26 (stack73)
        %v16935 = vshrl.u32 %v16929, 6 (stack74)
        %v16936 = vor.u32 %v16934, %v16935 (stack75)
        %v16937 = vxor.u32 %v16932, %v16936 (stack76)
        %v16940 = vadd.s32 %v16932, %v16937 (stack65)
        %v16944 = vadd.s32 %v16940, %v9 (stack65)
        %v16946 = vshll.u32 %v16937, 6 (stack73)
        %v16947 = vshrl.u32 %v16937, 26 (stack74)
        %v16948 = vor.u32 %v16946, %v16947 (stack75)
        %v16949 = vxor.u32 %v16940, %v16948 (stack76)
        %v16952 = vadd.s32 %v16949, %v8 (stack65)
        %v16956 = vadd.s32 %v16952, 1 (stack65)
        %v16960 = vadd.s32 %v16944, %v16956 (stack65)
        %v16962 = vshll.u32 %v16956, 17 (stack73)
        %v16963 = vshrl.u32 %v16956, 15 (stack74)
        %v16964 = vor.u32 %v16962, %v16963 (stack75)
        %v16965 = vxor.u32 %v16960, %v16964 (stack76)
        %v16968 = vadd.s32 %v16960, %v16965 (stack65)
        %v16970 = vshll.u32 %v16965, 29 (stack73)
        %v16971 = vshrl.u32 %v16965, 3 (stack74)
        %v16972 = vor.u32 %v16970, %v16971 (stack75)
        %v16973 = vxor.u32 %v16968, %v16972 (stack76)
        %v16976 = vadd.s32 %v16968, %v16973 (stack65)
        %v16978 = vshll.u32 %v16973, 16 (stack73)
        %v16979 = vshrl.u32 %v16973, 16 (stack74)
        %v16980 = vor.u32 %v16978, %v16979 (stack75)
        %v16981 = vxor.u32 %v16976, %v16980 (stack76)
        %v16984 = vadd.s32 %v16976, %v16981 (stack65)
        %v16988 = vadd.s32 %v16984, %v8 (stack65)
        %v16990 = vshll.u32 %v16981, 24 (stack73)
        %v16991 = vshrl.u32 %v16981, 8 (stack74)
        %v16992 = vor.u32 %v16990, %v16991 (stack75)
        %v16993 = vxor.u32 %v16984, %v16992 (stack76)
        %v16996 = vadd.s32 %v16993, %v10 (stack65)
        %v17000 = vadd.s32 %v16996, 2 (stack65)
        %v17004 = vadd.s32 %v16988, %v17000 (stack65)
        %v17006 = vshll.u32 %v17000, 13 (stack73)
        %v17007 = vshrl.u32 %v17000, 19 (stack74)
        %v17008 = vor.u32 %v17006, %v17007 (stack75)
        %v17009 = vxor.u32 %v17004, %v17008 (stack76)
        %v17012 = vadd.s32 %v17004, %v17009 (stack65)
        %v17014 = vshll.u32 %v17009, 15 (stack73)
        %v17015 = vshrl.u32 %v17009, 17 (stack74)
        %v17016 = vor.u32 %v17014, %v17015 (stack75)
        %v17017 = vxor.u32 %v17012, %v17016 (stack76)
        %v17020 = vadd.s32 %v17012, %v17017 (stack65)
        %v17022 = vshll.u32 %v17017, 26 (stack73)
        %v17023 = vshrl.u32 %v17017, 6 (stack74)
        %v17024 = vor.u32 %v17022, %v17023 (stack75)
        %v17025 = vxor.u32 %v17020, %v17024 (stack76)
        %v17028 = vadd.s32 %v17020, %v17025 (stack65)
        %v17032 = vadd.s32 %v17028, %v10 (stack65)
        %v17034 = vshll.u32 %v17025, 6 (stack73)
        %v17035 = vshrl.u32 %v17025, 26 (stack74)
        %v17036 = vor.u32 %v17034, %v17035 (stack75)
        %v17037 = vxor.u32 %v17028, %v17036 (stack76)
        %v17040 = vadd.s32 %v17037, %v9 (stack65)
        %v17044 = vadd.s32 %v17040, 3 (stack65)
        %v17048 = vadd.s32 %v17032, %v17044 (stack65)
        %v17050 = vshll.u32 %v17044, 17 (stack73)
        %v17051 = vshrl.u32 %v17044, 15 (stack74)
        %v17052 = vor.u32 %v17050, %v17051 (stack75)
        %v17053 = vxor.u32 %v17048, %v17052 (stack76)
        %v17056 = vadd.s32 %v17048, %v17053 (stack65)
        %v17058 = vshll.u32 %v17053, 29 (stack73)
        %v17059 = vshrl.u32 %v17053, 3 (stack74)
        %v17060 = vor.u32 %v17058, %v17059 (stack75)
        %v17061 = vxor.u32 %v17056, %v17060 (stack76)
        %v17064 = vadd.s32 %v17056, %v17061 (stack65)
        %v17066 = vshll.u32 %v17061, 16 (stack73)
        %v17067 = vshrl.u32 %v17061, 16 (stack74)
        %v17068 = vor.u32 %v17066, %v17067 (stack75)
        %v17069 = vxor.u32 %v17064, %v17068 (stack76)
        %v17072 = vadd.s32 %v17064, %v17069 (stack65)
        %v17076 = vadd.s32 %v17072, %v9 (stack65)
        %v17078 = vshll.u32 %v17069, 24 (stack73)
        %v17079 = vshrl.u32 %v17069, 8 (stack74)
        %v17080 = vor.u32 %v17078, %v17079 (stack75)
        %v17081 = vxor.u32 %v17072, %v17080 (stack76)
        %v17084 = vadd.s32 %v17081, %v8 (stack65)
        %v17088 = vadd.s32 %v17084, 4 (stack65)
        %v17092 = vadd.s32 %v17076, %v17088 (stack65)
        %v17094 = vshll.u32 %v17088, 13 (stack73)
        %v17095 = vshrl.u32 %v17088, 19 (stack74)
        %v17096 = vor.u32 %v17094, %v17095 (stack75)
        %v17097 = vxor.u32 %v17092, %v17096 (stack76)
        %v17100 = vadd.s32 %v17092, %v17097 (stack65)
        %v17102 = vshll.u32 %v17097, 15 (stack73)
        %v17103 = vshrl.u32 %v17097, 17 (stack74)
        %v17104 = vor.u32 %v17102, %v17103 (stack75)
        %v17105 = vxor.u32 %v17100, %v17104 (stack76)
        %v17108 = vadd.s32 %v17100, %v17105 (stack65)
        %v17110 = vshll.u32 %v17105, 26 (stack73)
        %v17111 = vshrl.u32 %v17105, 6 (stack74)
        %v17112 = vor.u32 %v17110, %v17111 (stack75)
        %v17113 = vxor.u32 %v17108, %v17112 (stack76)
        %v17116 = vadd.s32 %v17108, %v17113 (stack65)
        %v17120 = vadd.s32 %v17116, %v8 (stack65)
        %v17122 = vshll.u32 %v17113, 6 (stack73)
        %v17123 = vshrl.u32 %v17113, 26 (stack74)
        %v17124 = vor.u32 %v17122, %v17123 (stack75)
        %v17125 = vxor.u32 %v17116, %v17124 (stack76)
        %v17128 = vadd.s32 %v17125, %v10 (stack65)
        %v17132 = vadd.s32 %v17128, 5 (stack65)
        %v17134 = vxor.u32 %v17120, %v17132 (stack76)
        %v17135 = vand.u32.u8 %v17134, 255 (stack77)
        %v17136 = vand.u32 %v17135, 65535 (stack78)
        %v17137 = vshrl.u32 %v17136, 1 (stack79)
        %v17138 = vor.u32 %v17137, 16256 (stack75)
        %v17139 = vand.u32.u16 %v17138, 65535 (stack80)
        %v17140 = vunpack.i.l.bf16 %v17139 (stack81)
        %v17144 = vadd.f32 %v17140, -1.0 (stack82)
        %v17148 = vmul.f32 %v17144, 2.0 (stack83)
        %v17152 = vadd.f32 %v17148, -0.99609375 (stack82)
        %v17156 = vmax.f32 -0.99609375, %v17152 (stack84)
        %v17158 = vand.u32 2147483647, %v17156 (stack85)
        %vm17161 = vcmp.eq.f32.partialorder %v17158, 1.0 (stack86)
        %v17166 = vmul.f32 %v17156, inf (stack83)
        %v17168 = vxor.u32 %v17156, 2147483648 (stack87)
        %v17171 = vmul.f32 %v17156, %v17168 (stack83)
        %v17173 = vadd.f32 %v17171, 1.0 (stack88)
        %v17174 = vlog2.pop %v17173 (stack89)
        %v17175 = vmul.f32 %v17174, 0.6931472 (stack90)
        %v17176 = vmul.f32 -0.5, %v17171 (stack91)
        %v17177 = vadd.f32 %v17176, 1.0 (stack92)
        %v17178 = vmul.f32 %v17177, %v17171 (stack93)
        %v17179 = vand.u32 2147483647, %v17171 (stack94)
        %vm17180 = vcmp.lt.f32.partialorder %v17179, 0.0004427343 (stack95)
        %v17181 = vsel /*vm=*/%vm17180, /*on_true_vy=*/%v17178, /*on_false_vx=*/%v17175 (stack96)
        %v17182 = vxor.u32 %v17181, 2147483648 (stack87)
        %vm17185 = vcmp.lt.f32.partialorder %v17182, 5.0 (stack86)
        %v17190 = vsel /*vm=*/%vm17185, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v17194 = vsel /*vm=*/%vm17185, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v17198 = vsel /*vm=*/%vm17185, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v17202 = vsel /*vm=*/%vm17185, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v17206 = vsel /*vm=*/%vm17185, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v17210 = vsel /*vm=*/%vm17185, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v17214 = vsel /*vm=*/%vm17185, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v17218 = vsel /*vm=*/%vm17185, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v17222 = vsel /*vm=*/%vm17185, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v17226 = vadd.f32 %v17182, -2.5 (stack82)
        %v17228 = vrsqrt.pop %v17182 (stack97)
        %v17229 = vmul.f32 %v17182, %v17228 (stack98)
        %vm17230 = vcmp.eq.f32.partialorder %v17182, inf (stack99)
        %v17231 = vsel /*vm=*/%vm17230, /*on_true_vy=*/%v17182, /*on_false_vx=*/%v17229 (stack100)
        %vm17232 = vcmp.eq.f32.partialorder %v17182, 0.0 (stack101)
        %v17233 = vand.u32 %v17182, 2147483648 (stack102)
        %v17234 = vsel /*vm=*/%vm17232, /*on_true_vy=*/%v17233, /*on_false_vx=*/%v17231 (stack103)
        %v17237 = vadd.f32 %v17234, -3.0 (stack82)
        %v17241 = vsel /*vm=*/%vm17185, /*on_true_vy=*/%v17226, /*on_false_vx=*/%v17237 (stack72)
        %v17245 = vmul.f32 %v17222, %v17241 (stack83)
        %v17249 = vadd.f32 %v17218, %v17245 (stack82)
        %v17253 = vmul.f32 %v17249, %v17241 (stack83)
        %v17257 = vadd.f32 %v17214, %v17253 (stack82)
        %v17261 = vmul.f32 %v17257, %v17241 (stack83)
        %v17265 = vadd.f32 %v17210, %v17261 (stack82)
        %v17269 = vmul.f32 %v17265, %v17241 (stack83)
        %v17273 = vadd.f32 %v17206, %v17269 (stack82)
        %v17277 = vmul.f32 %v17273, %v17241 (stack83)
        %v17281 = vadd.f32 %v17202, %v17277 (stack82)
        %v17285 = vmul.f32 %v17281, %v17241 (stack83)
        %v17289 = vadd.f32 %v17198, %v17285 (stack82)
        %v17293 = vmul.f32 %v17289, %v17241 (stack83)
        %v17297 = vadd.f32 %v17194, %v17293 (stack82)
        %v17301 = vmul.f32 %v17297, %v17241 (stack83)
        %v17305 = vadd.f32 %v17190, %v17301 (stack82)
        %v17309 = vmul.f32 %v17305, %v17156 (stack83)
        %v17313 = vsel /*vm=*/%vm17161, /*on_true_vy=*/%v17166, /*on_false_vx=*/%v17309 (stack72)
        %v17317 = vmul.f32 %v17313, 1.4140625 (stack83)
        %s17319 = scalar_lea.vmem %s280, 400 [#allocation0] (stack107)
        %v17320 = vpack.c.bf16 0.0, %v17317 (stack104)
        %17321 = vst [vmem:[%s17319] sm:$0xf] /*vst_source=*/%v17320 (stack105)
        %v17324 = vadd.s32 %v2355, %v15477 (stack65)
        %s17326 = smul.u32 128, %s27 (stack66)
        %v17327 = vlaneseq (stack67)
        %v17328 = vand.u32 %v17327, 127 (stack68)
        %v17329 = vstv %s17326 (stack69)
        %v17330 = vadd.s32 %v17328, %v17329 (stack70)
        %v17334 = vadd.s32 %v17324, %v17330 (stack65)
        %vm17338 = vcmp.lt.u32.totalorder %v17334, %v17324 (stack71)
        %vm17343 = vcmp.lt.u32.totalorder %v17324, %v2355 (stack71)
        %v17348 = vadd.s32 %v2342, %v15460 (stack65)
        %v17352 = vadd.s32 %v17348, 1 (stack65)
        %v17356 = vsel /*vm=*/%vm17343, /*on_true_vy=*/%v17352, /*on_false_vx=*/%v17348 (stack72)
        %v17360 = vadd.s32 %v17356, 1 (stack65)
        %v17364 = vsel /*vm=*/%vm17338, /*on_true_vy=*/%v17360, /*on_false_vx=*/%v17356 (stack72)
        %v17369 = vadd.s32 %v17364, %v10 (stack65)
        %v17373 = vadd.s32 %v17334, %v9 (stack65)
        %v17377 = vadd.s32 %v17369, %v17373 (stack65)
        %v17379 = vshll.u32 %v17373, 13 (stack73)
        %v17380 = vshrl.u32 %v17373, 19 (stack74)
        %v17381 = vor.u32 %v17379, %v17380 (stack75)
        %v17382 = vxor.u32 %v17377, %v17381 (stack76)
        %v17385 = vadd.s32 %v17377, %v17382 (stack65)
        %v17387 = vshll.u32 %v17382, 15 (stack73)
        %v17388 = vshrl.u32 %v17382, 17 (stack74)
        %v17389 = vor.u32 %v17387, %v17388 (stack75)
        %v17390 = vxor.u32 %v17385, %v17389 (stack76)
        %v17393 = vadd.s32 %v17385, %v17390 (stack65)
        %v17395 = vshll.u32 %v17390, 26 (stack73)
        %v17396 = vshrl.u32 %v17390, 6 (stack74)
        %v17397 = vor.u32 %v17395, %v17396 (stack75)
        %v17398 = vxor.u32 %v17393, %v17397 (stack76)
        %v17401 = vadd.s32 %v17393, %v17398 (stack65)
        %v17405 = vadd.s32 %v17401, %v9 (stack65)
        %v17407 = vshll.u32 %v17398, 6 (stack73)
        %v17408 = vshrl.u32 %v17398, 26 (stack74)
        %v17409 = vor.u32 %v17407, %v17408 (stack75)
        %v17410 = vxor.u32 %v17401, %v17409 (stack76)
        %v17413 = vadd.s32 %v17410, %v8 (stack65)
        %v17417 = vadd.s32 %v17413, 1 (stack65)
        %v17421 = vadd.s32 %v17405, %v17417 (stack65)
        %v17423 = vshll.u32 %v17417, 17 (stack73)
        %v17424 = vshrl.u32 %v17417, 15 (stack74)
        %v17425 = vor.u32 %v17423, %v17424 (stack75)
        %v17426 = vxor.u32 %v17421, %v17425 (stack76)
        %v17429 = vadd.s32 %v17421, %v17426 (stack65)
        %v17431 = vshll.u32 %v17426, 29 (stack73)
        %v17432 = vshrl.u32 %v17426, 3 (stack74)
        %v17433 = vor.u32 %v17431, %v17432 (stack75)
        %v17434 = vxor.u32 %v17429, %v17433 (stack76)
        %v17437 = vadd.s32 %v17429, %v17434 (stack65)
        %v17439 = vshll.u32 %v17434, 16 (stack73)
        %v17440 = vshrl.u32 %v17434, 16 (stack74)
        %v17441 = vor.u32 %v17439, %v17440 (stack75)
        %v17442 = vxor.u32 %v17437, %v17441 (stack76)
        %v17445 = vadd.s32 %v17437, %v17442 (stack65)
        %v17449 = vadd.s32 %v17445, %v8 (stack65)
        %v17451 = vshll.u32 %v17442, 24 (stack73)
        %v17452 = vshrl.u32 %v17442, 8 (stack74)
        %v17453 = vor.u32 %v17451, %v17452 (stack75)
        %v17454 = vxor.u32 %v17445, %v17453 (stack76)
        %v17457 = vadd.s32 %v17454, %v10 (stack65)
        %v17461 = vadd.s32 %v17457, 2 (stack65)
        %v17465 = vadd.s32 %v17449, %v17461 (stack65)
        %v17467 = vshll.u32 %v17461, 13 (stack73)
        %v17468 = vshrl.u32 %v17461, 19 (stack74)
        %v17469 = vor.u32 %v17467, %v17468 (stack75)
        %v17470 = vxor.u32 %v17465, %v17469 (stack76)
        %v17473 = vadd.s32 %v17465, %v17470 (stack65)
        %v17475 = vshll.u32 %v17470, 15 (stack73)
        %v17476 = vshrl.u32 %v17470, 17 (stack74)
        %v17477 = vor.u32 %v17475, %v17476 (stack75)
        %v17478 = vxor.u32 %v17473, %v17477 (stack76)
        %v17481 = vadd.s32 %v17473, %v17478 (stack65)
        %v17483 = vshll.u32 %v17478, 26 (stack73)
        %v17484 = vshrl.u32 %v17478, 6 (stack74)
        %v17485 = vor.u32 %v17483, %v17484 (stack75)
        %v17486 = vxor.u32 %v17481, %v17485 (stack76)
        %v17489 = vadd.s32 %v17481, %v17486 (stack65)
        %v17493 = vadd.s32 %v17489, %v10 (stack65)
        %v17495 = vshll.u32 %v17486, 6 (stack73)
        %v17496 = vshrl.u32 %v17486, 26 (stack74)
        %v17497 = vor.u32 %v17495, %v17496 (stack75)
        %v17498 = vxor.u32 %v17489, %v17497 (stack76)
        %v17501 = vadd.s32 %v17498, %v9 (stack65)
        %v17505 = vadd.s32 %v17501, 3 (stack65)
        %v17509 = vadd.s32 %v17493, %v17505 (stack65)
        %v17511 = vshll.u32 %v17505, 17 (stack73)
        %v17512 = vshrl.u32 %v17505, 15 (stack74)
        %v17513 = vor.u32 %v17511, %v17512 (stack75)
        %v17514 = vxor.u32 %v17509, %v17513 (stack76)
        %v17517 = vadd.s32 %v17509, %v17514 (stack65)
        %v17519 = vshll.u32 %v17514, 29 (stack73)
        %v17520 = vshrl.u32 %v17514, 3 (stack74)
        %v17521 = vor.u32 %v17519, %v17520 (stack75)
        %v17522 = vxor.u32 %v17517, %v17521 (stack76)
        %v17525 = vadd.s32 %v17517, %v17522 (stack65)
        %v17527 = vshll.u32 %v17522, 16 (stack73)
        %v17528 = vshrl.u32 %v17522, 16 (stack74)
        %v17529 = vor.u32 %v17527, %v17528 (stack75)
        %v17530 = vxor.u32 %v17525, %v17529 (stack76)
        %v17533 = vadd.s32 %v17525, %v17530 (stack65)
        %v17537 = vadd.s32 %v17533, %v9 (stack65)
        %v17539 = vshll.u32 %v17530, 24 (stack73)
        %v17540 = vshrl.u32 %v17530, 8 (stack74)
        %v17541 = vor.u32 %v17539, %v17540 (stack75)
        %v17542 = vxor.u32 %v17533, %v17541 (stack76)
        %v17545 = vadd.s32 %v17542, %v8 (stack65)
        %v17549 = vadd.s32 %v17545, 4 (stack65)
        %v17553 = vadd.s32 %v17537, %v17549 (stack65)
        %v17555 = vshll.u32 %v17549, 13 (stack73)
        %v17556 = vshrl.u32 %v17549, 19 (stack74)
        %v17557 = vor.u32 %v17555, %v17556 (stack75)
        %v17558 = vxor.u32 %v17553, %v17557 (stack76)
        %v17561 = vadd.s32 %v17553, %v17558 (stack65)
        %v17563 = vshll.u32 %v17558, 15 (stack73)
        %v17564 = vshrl.u32 %v17558, 17 (stack74)
        %v17565 = vor.u32 %v17563, %v17564 (stack75)
        %v17566 = vxor.u32 %v17561, %v17565 (stack76)
        %v17569 = vadd.s32 %v17561, %v17566 (stack65)
        %v17571 = vshll.u32 %v17566, 26 (stack73)
        %v17572 = vshrl.u32 %v17566, 6 (stack74)
        %v17573 = vor.u32 %v17571, %v17572 (stack75)
        %v17574 = vxor.u32 %v17569, %v17573 (stack76)
        %v17577 = vadd.s32 %v17569, %v17574 (stack65)
        %v17581 = vadd.s32 %v17577, %v8 (stack65)
        %v17583 = vshll.u32 %v17574, 6 (stack73)
        %v17584 = vshrl.u32 %v17574, 26 (stack74)
        %v17585 = vor.u32 %v17583, %v17584 (stack75)
        %v17586 = vxor.u32 %v17577, %v17585 (stack76)
        %v17589 = vadd.s32 %v17586, %v10 (stack65)
        %v17593 = vadd.s32 %v17589, 5 (stack65)
        %v17595 = vxor.u32 %v17581, %v17593 (stack76)
        %v17596 = vand.u32.u8 %v17595, 255 (stack77)
        %v17597 = vand.u32 %v17596, 65535 (stack78)
        %v17598 = vshrl.u32 %v17597, 1 (stack79)
        %v17599 = vor.u32 %v17598, 16256 (stack75)
        %v17600 = vand.u32.u16 %v17599, 65535 (stack80)
        %v17601 = vunpack.i.l.bf16 %v17600 (stack81)
        %v17605 = vadd.f32 %v17601, -1.0 (stack82)
        %v17609 = vmul.f32 %v17605, 2.0 (stack83)
        %v17613 = vadd.f32 %v17609, -0.99609375 (stack82)
        %v17617 = vmax.f32 -0.99609375, %v17613 (stack84)
        %v17619 = vand.u32 2147483647, %v17617 (stack85)
        %vm17622 = vcmp.eq.f32.partialorder %v17619, 1.0 (stack86)
        %v17627 = vmul.f32 %v17617, inf (stack83)
        %v17629 = vxor.u32 %v17617, 2147483648 (stack87)
        %v17632 = vmul.f32 %v17617, %v17629 (stack83)
        %v17634 = vadd.f32 %v17632, 1.0 (stack88)
        %v17635 = vlog2.pop %v17634 (stack89)
        %v17636 = vmul.f32 %v17635, 0.6931472 (stack90)
        %v17637 = vmul.f32 -0.5, %v17632 (stack91)
        %v17638 = vadd.f32 %v17637, 1.0 (stack92)
        %v17639 = vmul.f32 %v17638, %v17632 (stack93)
        %v17640 = vand.u32 2147483647, %v17632 (stack94)
        %vm17641 = vcmp.lt.f32.partialorder %v17640, 0.0004427343 (stack95)
        %v17642 = vsel /*vm=*/%vm17641, /*on_true_vy=*/%v17639, /*on_false_vx=*/%v17636 (stack96)
        %v17643 = vxor.u32 %v17642, 2147483648 (stack87)
        %vm17646 = vcmp.lt.f32.partialorder %v17643, 5.0 (stack86)
        %v17651 = vsel /*vm=*/%vm17646, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v17655 = vsel /*vm=*/%vm17646, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v17659 = vsel /*vm=*/%vm17646, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v17663 = vsel /*vm=*/%vm17646, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v17667 = vsel /*vm=*/%vm17646, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v17671 = vsel /*vm=*/%vm17646, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v17675 = vsel /*vm=*/%vm17646, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v17679 = vsel /*vm=*/%vm17646, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v17683 = vsel /*vm=*/%vm17646, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v17687 = vadd.f32 %v17643, -2.5 (stack82)
        %v17689 = vrsqrt.pop %v17643 (stack97)
        %v17690 = vmul.f32 %v17643, %v17689 (stack98)
        %vm17691 = vcmp.eq.f32.partialorder %v17643, inf (stack99)
        %v17692 = vsel /*vm=*/%vm17691, /*on_true_vy=*/%v17643, /*on_false_vx=*/%v17690 (stack100)
        %vm17693 = vcmp.eq.f32.partialorder %v17643, 0.0 (stack101)
        %v17694 = vand.u32 %v17643, 2147483648 (stack102)
        %v17695 = vsel /*vm=*/%vm17693, /*on_true_vy=*/%v17694, /*on_false_vx=*/%v17692 (stack103)
        %v17698 = vadd.f32 %v17695, -3.0 (stack82)
        %v17702 = vsel /*vm=*/%vm17646, /*on_true_vy=*/%v17687, /*on_false_vx=*/%v17698 (stack72)
        %v17706 = vmul.f32 %v17683, %v17702 (stack83)
        %v17710 = vadd.f32 %v17679, %v17706 (stack82)
        %v17714 = vmul.f32 %v17710, %v17702 (stack83)
        %v17718 = vadd.f32 %v17675, %v17714 (stack82)
        %v17722 = vmul.f32 %v17718, %v17702 (stack83)
        %v17726 = vadd.f32 %v17671, %v17722 (stack82)
        %v17730 = vmul.f32 %v17726, %v17702 (stack83)
        %v17734 = vadd.f32 %v17667, %v17730 (stack82)
        %v17738 = vmul.f32 %v17734, %v17702 (stack83)
        %v17742 = vadd.f32 %v17663, %v17738 (stack82)
        %v17746 = vmul.f32 %v17742, %v17702 (stack83)
        %v17750 = vadd.f32 %v17659, %v17746 (stack82)
        %v17754 = vmul.f32 %v17750, %v17702 (stack83)
        %v17758 = vadd.f32 %v17655, %v17754 (stack82)
        %v17762 = vmul.f32 %v17758, %v17702 (stack83)
        %v17766 = vadd.f32 %v17651, %v17762 (stack82)
        %v17770 = vmul.f32 %v17766, %v17617 (stack83)
        %v17774 = vsel /*vm=*/%vm17622, /*on_true_vy=*/%v17627, /*on_false_vx=*/%v17770 (stack72)
        %v17778 = vmul.f32 %v17774, 1.4140625 (stack83)
        %s17780 = scalar_lea.vmem %s280, 528 [#allocation0] (stack107)
        %v17781 = vpack.c.bf16 0.0, %v17778 (stack104)
        %17782 = vst [vmem:[%s17780] sm:$0xf] /*vst_source=*/%v17781 (stack105)
        %v17785 = vadd.s32 %v2842, %v15477 (stack65)
        %s17787 = smul.u32 128, %s27 (stack66)
        %v17788 = vlaneseq (stack67)
        %v17789 = vand.u32 %v17788, 127 (stack68)
        %v17790 = vstv %s17787 (stack69)
        %v17791 = vadd.s32 %v17789, %v17790 (stack70)
        %v17795 = vadd.s32 %v17785, %v17791 (stack65)
        %vm17799 = vcmp.lt.u32.totalorder %v17795, %v17785 (stack71)
        %vm17804 = vcmp.lt.u32.totalorder %v17785, %v2842 (stack71)
        %v17809 = vadd.s32 %v2829, %v15460 (stack65)
        %v17813 = vadd.s32 %v17809, 1 (stack65)
        %v17817 = vsel /*vm=*/%vm17804, /*on_true_vy=*/%v17813, /*on_false_vx=*/%v17809 (stack72)
        %v17821 = vadd.s32 %v17817, 1 (stack65)
        %v17825 = vsel /*vm=*/%vm17799, /*on_true_vy=*/%v17821, /*on_false_vx=*/%v17817 (stack72)
        %v17830 = vadd.s32 %v17825, %v10 (stack65)
        %v17834 = vadd.s32 %v17795, %v9 (stack65)
        %v17838 = vadd.s32 %v17830, %v17834 (stack65)
        %v17840 = vshll.u32 %v17834, 13 (stack73)
        %v17841 = vshrl.u32 %v17834, 19 (stack74)
        %v17842 = vor.u32 %v17840, %v17841 (stack75)
        %v17843 = vxor.u32 %v17838, %v17842 (stack76)
        %v17846 = vadd.s32 %v17838, %v17843 (stack65)
        %v17848 = vshll.u32 %v17843, 15 (stack73)
        %v17849 = vshrl.u32 %v17843, 17 (stack74)
        %v17850 = vor.u32 %v17848, %v17849 (stack75)
        %v17851 = vxor.u32 %v17846, %v17850 (stack76)
        %v17854 = vadd.s32 %v17846, %v17851 (stack65)
        %v17856 = vshll.u32 %v17851, 26 (stack73)
        %v17857 = vshrl.u32 %v17851, 6 (stack74)
        %v17858 = vor.u32 %v17856, %v17857 (stack75)
        %v17859 = vxor.u32 %v17854, %v17858 (stack76)
        %v17862 = vadd.s32 %v17854, %v17859 (stack65)
        %v17866 = vadd.s32 %v17862, %v9 (stack65)
        %v17868 = vshll.u32 %v17859, 6 (stack73)
        %v17869 = vshrl.u32 %v17859, 26 (stack74)
        %v17870 = vor.u32 %v17868, %v17869 (stack75)
        %v17871 = vxor.u32 %v17862, %v17870 (stack76)
        %v17874 = vadd.s32 %v17871, %v8 (stack65)
        %v17878 = vadd.s32 %v17874, 1 (stack65)
        %v17882 = vadd.s32 %v17866, %v17878 (stack65)
        %v17884 = vshll.u32 %v17878, 17 (stack73)
        %v17885 = vshrl.u32 %v17878, 15 (stack74)
        %v17886 = vor.u32 %v17884, %v17885 (stack75)
        %v17887 = vxor.u32 %v17882, %v17886 (stack76)
        %v17890 = vadd.s32 %v17882, %v17887 (stack65)
        %v17892 = vshll.u32 %v17887, 29 (stack73)
        %v17893 = vshrl.u32 %v17887, 3 (stack74)
        %v17894 = vor.u32 %v17892, %v17893 (stack75)
        %v17895 = vxor.u32 %v17890, %v17894 (stack76)
        %v17898 = vadd.s32 %v17890, %v17895 (stack65)
        %v17900 = vshll.u32 %v17895, 16 (stack73)
        %v17901 = vshrl.u32 %v17895, 16 (stack74)
        %v17902 = vor.u32 %v17900, %v17901 (stack75)
        %v17903 = vxor.u32 %v17898, %v17902 (stack76)
        %v17906 = vadd.s32 %v17898, %v17903 (stack65)
        %v17910 = vadd.s32 %v17906, %v8 (stack65)
        %v17912 = vshll.u32 %v17903, 24 (stack73)
        %v17913 = vshrl.u32 %v17903, 8 (stack74)
        %v17914 = vor.u32 %v17912, %v17913 (stack75)
        %v17915 = vxor.u32 %v17906, %v17914 (stack76)
        %v17918 = vadd.s32 %v17915, %v10 (stack65)
        %v17922 = vadd.s32 %v17918, 2 (stack65)
        %v17926 = vadd.s32 %v17910, %v17922 (stack65)
        %v17928 = vshll.u32 %v17922, 13 (stack73)
        %v17929 = vshrl.u32 %v17922, 19 (stack74)
        %v17930 = vor.u32 %v17928, %v17929 (stack75)
        %v17931 = vxor.u32 %v17926, %v17930 (stack76)
        %v17934 = vadd.s32 %v17926, %v17931 (stack65)
        %v17936 = vshll.u32 %v17931, 15 (stack73)
        %v17937 = vshrl.u32 %v17931, 17 (stack74)
        %v17938 = vor.u32 %v17936, %v17937 (stack75)
        %v17939 = vxor.u32 %v17934, %v17938 (stack76)
        %v17942 = vadd.s32 %v17934, %v17939 (stack65)
        %v17944 = vshll.u32 %v17939, 26 (stack73)
        %v17945 = vshrl.u32 %v17939, 6 (stack74)
        %v17946 = vor.u32 %v17944, %v17945 (stack75)
        %v17947 = vxor.u32 %v17942, %v17946 (stack76)
        %v17950 = vadd.s32 %v17942, %v17947 (stack65)
        %v17954 = vadd.s32 %v17950, %v10 (stack65)
        %v17956 = vshll.u32 %v17947, 6 (stack73)
        %v17957 = vshrl.u32 %v17947, 26 (stack74)
        %v17958 = vor.u32 %v17956, %v17957 (stack75)
        %v17959 = vxor.u32 %v17950, %v17958 (stack76)
        %v17962 = vadd.s32 %v17959, %v9 (stack65)
        %v17966 = vadd.s32 %v17962, 3 (stack65)
        %v17970 = vadd.s32 %v17954, %v17966 (stack65)
        %v17972 = vshll.u32 %v17966, 17 (stack73)
        %v17973 = vshrl.u32 %v17966, 15 (stack74)
        %v17974 = vor.u32 %v17972, %v17973 (stack75)
        %v17975 = vxor.u32 %v17970, %v17974 (stack76)
        %v17978 = vadd.s32 %v17970, %v17975 (stack65)
        %v17980 = vshll.u32 %v17975, 29 (stack73)
        %v17981 = vshrl.u32 %v17975, 3 (stack74)
        %v17982 = vor.u32 %v17980, %v17981 (stack75)
        %v17983 = vxor.u32 %v17978, %v17982 (stack76)
        %v17986 = vadd.s32 %v17978, %v17983 (stack65)
        %v17988 = vshll.u32 %v17983, 16 (stack73)
        %v17989 = vshrl.u32 %v17983, 16 (stack74)
        %v17990 = vor.u32 %v17988, %v17989 (stack75)
        %v17991 = vxor.u32 %v17986, %v17990 (stack76)
        %v17994 = vadd.s32 %v17986, %v17991 (stack65)
        %v17998 = vadd.s32 %v17994, %v9 (stack65)
        %v18000 = vshll.u32 %v17991, 24 (stack73)
        %v18001 = vshrl.u32 %v17991, 8 (stack74)
        %v18002 = vor.u32 %v18000, %v18001 (stack75)
        %v18003 = vxor.u32 %v17994, %v18002 (stack76)
        %v18006 = vadd.s32 %v18003, %v8 (stack65)
        %v18010 = vadd.s32 %v18006, 4 (stack65)
        %v18014 = vadd.s32 %v17998, %v18010 (stack65)
        %v18016 = vshll.u32 %v18010, 13 (stack73)
        %v18017 = vshrl.u32 %v18010, 19 (stack74)
        %v18018 = vor.u32 %v18016, %v18017 (stack75)
        %v18019 = vxor.u32 %v18014, %v18018 (stack76)
        %v18022 = vadd.s32 %v18014, %v18019 (stack65)
        %v18024 = vshll.u32 %v18019, 15 (stack73)
        %v18025 = vshrl.u32 %v18019, 17 (stack74)
        %v18026 = vor.u32 %v18024, %v18025 (stack75)
        %v18027 = vxor.u32 %v18022, %v18026 (stack76)
        %v18030 = vadd.s32 %v18022, %v18027 (stack65)
        %v18032 = vshll.u32 %v18027, 26 (stack73)
        %v18033 = vshrl.u32 %v18027, 6 (stack74)
        %v18034 = vor.u32 %v18032, %v18033 (stack75)
        %v18035 = vxor.u32 %v18030, %v18034 (stack76)
        %v18038 = vadd.s32 %v18030, %v18035 (stack65)
        %v18042 = vadd.s32 %v18038, %v8 (stack65)
        %v18044 = vshll.u32 %v18035, 6 (stack73)
        %v18045 = vshrl.u32 %v18035, 26 (stack74)
        %v18046 = vor.u32 %v18044, %v18045 (stack75)
        %v18047 = vxor.u32 %v18038, %v18046 (stack76)
        %v18050 = vadd.s32 %v18047, %v10 (stack65)
        %v18054 = vadd.s32 %v18050, 5 (stack65)
        %v18056 = vxor.u32 %v18042, %v18054 (stack76)
        %v18057 = vand.u32.u8 %v18056, 255 (stack77)
        %v18058 = vand.u32 %v18057, 65535 (stack78)
        %v18059 = vshrl.u32 %v18058, 1 (stack79)
        %v18060 = vor.u32 %v18059, 16256 (stack75)
        %v18061 = vand.u32.u16 %v18060, 65535 (stack80)
        %v18062 = vunpack.i.l.bf16 %v18061 (stack81)
        %v18066 = vadd.f32 %v18062, -1.0 (stack82)
        %v18070 = vmul.f32 %v18066, 2.0 (stack83)
        %v18074 = vadd.f32 %v18070, -0.99609375 (stack82)
        %v18078 = vmax.f32 -0.99609375, %v18074 (stack84)
        %v18080 = vand.u32 2147483647, %v18078 (stack85)
        %vm18083 = vcmp.eq.f32.partialorder %v18080, 1.0 (stack86)
        %v18088 = vmul.f32 %v18078, inf (stack83)
        %v18090 = vxor.u32 %v18078, 2147483648 (stack87)
        %v18093 = vmul.f32 %v18078, %v18090 (stack83)
        %v18095 = vadd.f32 %v18093, 1.0 (stack88)
        %v18096 = vlog2.pop %v18095 (stack89)
        %v18097 = vmul.f32 %v18096, 0.6931472 (stack90)
        %v18098 = vmul.f32 -0.5, %v18093 (stack91)
        %v18099 = vadd.f32 %v18098, 1.0 (stack92)
        %v18100 = vmul.f32 %v18099, %v18093 (stack93)
        %v18101 = vand.u32 2147483647, %v18093 (stack94)
        %vm18102 = vcmp.lt.f32.partialorder %v18101, 0.0004427343 (stack95)
        %v18103 = vsel /*vm=*/%vm18102, /*on_true_vy=*/%v18100, /*on_false_vx=*/%v18097 (stack96)
        %v18104 = vxor.u32 %v18103, 2147483648 (stack87)
        %vm18107 = vcmp.lt.f32.partialorder %v18104, 5.0 (stack86)
        %v18112 = vsel /*vm=*/%vm18107, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v18116 = vsel /*vm=*/%vm18107, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v18120 = vsel /*vm=*/%vm18107, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v18124 = vsel /*vm=*/%vm18107, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v18128 = vsel /*vm=*/%vm18107, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v18132 = vsel /*vm=*/%vm18107, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v18136 = vsel /*vm=*/%vm18107, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v18140 = vsel /*vm=*/%vm18107, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v18144 = vsel /*vm=*/%vm18107, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v18148 = vadd.f32 %v18104, -2.5 (stack82)
        %v18150 = vrsqrt.pop %v18104 (stack97)
        %v18151 = vmul.f32 %v18104, %v18150 (stack98)
        %vm18152 = vcmp.eq.f32.partialorder %v18104, inf (stack99)
        %v18153 = vsel /*vm=*/%vm18152, /*on_true_vy=*/%v18104, /*on_false_vx=*/%v18151 (stack100)
        %vm18154 = vcmp.eq.f32.partialorder %v18104, 0.0 (stack101)
        %v18155 = vand.u32 %v18104, 2147483648 (stack102)
        %v18156 = vsel /*vm=*/%vm18154, /*on_true_vy=*/%v18155, /*on_false_vx=*/%v18153 (stack103)
        %v18159 = vadd.f32 %v18156, -3.0 (stack82)
        %v18163 = vsel /*vm=*/%vm18107, /*on_true_vy=*/%v18148, /*on_false_vx=*/%v18159 (stack72)
        %v18167 = vmul.f32 %v18144, %v18163 (stack83)
        %v18171 = vadd.f32 %v18140, %v18167 (stack82)
        %v18175 = vmul.f32 %v18171, %v18163 (stack83)
        %v18179 = vadd.f32 %v18136, %v18175 (stack82)
        %v18183 = vmul.f32 %v18179, %v18163 (stack83)
        %v18187 = vadd.f32 %v18132, %v18183 (stack82)
        %v18191 = vmul.f32 %v18187, %v18163 (stack83)
        %v18195 = vadd.f32 %v18128, %v18191 (stack82)
        %v18199 = vmul.f32 %v18195, %v18163 (stack83)
        %v18203 = vadd.f32 %v18124, %v18199 (stack82)
        %v18207 = vmul.f32 %v18203, %v18163 (stack83)
        %v18211 = vadd.f32 %v18120, %v18207 (stack82)
        %v18215 = vmul.f32 %v18211, %v18163 (stack83)
        %v18219 = vadd.f32 %v18116, %v18215 (stack82)
        %v18223 = vmul.f32 %v18219, %v18163 (stack83)
        %v18227 = vadd.f32 %v18112, %v18223 (stack82)
        %v18231 = vmul.f32 %v18227, %v18078 (stack83)
        %v18235 = vsel /*vm=*/%vm18083, /*on_true_vy=*/%v18088, /*on_false_vx=*/%v18231 (stack72)
        %v18239 = vmul.f32 %v18235, 1.4140625 (stack83)
        %s18241 = scalar_lea.vmem %s280, 656 [#allocation0] (stack107)
        %v18242 = vpack.c.bf16 0.0, %v18239 (stack104)
        %18243 = vst [vmem:[%s18241] sm:$0xf] /*vst_source=*/%v18242 (stack105)
        %v18246 = vadd.s32 %v3329, %v15477 (stack65)
        %s18248 = smul.u32 128, %s27 (stack66)
        %v18249 = vlaneseq (stack67)
        %v18250 = vand.u32 %v18249, 127 (stack68)
        %v18251 = vstv %s18248 (stack69)
        %v18252 = vadd.s32 %v18250, %v18251 (stack70)
        %v18256 = vadd.s32 %v18246, %v18252 (stack65)
        %vm18260 = vcmp.lt.u32.totalorder %v18256, %v18246 (stack71)
        %vm18265 = vcmp.lt.u32.totalorder %v18246, %v3329 (stack71)
        %v18270 = vadd.s32 %v3316, %v15460 (stack65)
        %v18274 = vadd.s32 %v18270, 1 (stack65)
        %v18278 = vsel /*vm=*/%vm18265, /*on_true_vy=*/%v18274, /*on_false_vx=*/%v18270 (stack72)
        %v18282 = vadd.s32 %v18278, 1 (stack65)
        %v18286 = vsel /*vm=*/%vm18260, /*on_true_vy=*/%v18282, /*on_false_vx=*/%v18278 (stack72)
        %v18291 = vadd.s32 %v18286, %v10 (stack65)
        %v18295 = vadd.s32 %v18256, %v9 (stack65)
        %v18299 = vadd.s32 %v18291, %v18295 (stack65)
        %v18301 = vshll.u32 %v18295, 13 (stack73)
        %v18302 = vshrl.u32 %v18295, 19 (stack74)
        %v18303 = vor.u32 %v18301, %v18302 (stack75)
        %v18304 = vxor.u32 %v18299, %v18303 (stack76)
        %v18307 = vadd.s32 %v18299, %v18304 (stack65)
        %v18309 = vshll.u32 %v18304, 15 (stack73)
        %v18310 = vshrl.u32 %v18304, 17 (stack74)
        %v18311 = vor.u32 %v18309, %v18310 (stack75)
        %v18312 = vxor.u32 %v18307, %v18311 (stack76)
        %v18315 = vadd.s32 %v18307, %v18312 (stack65)
        %v18317 = vshll.u32 %v18312, 26 (stack73)
        %v18318 = vshrl.u32 %v18312, 6 (stack74)
        %v18319 = vor.u32 %v18317, %v18318 (stack75)
        %v18320 = vxor.u32 %v18315, %v18319 (stack76)
        %v18323 = vadd.s32 %v18315, %v18320 (stack65)
        %v18327 = vadd.s32 %v18323, %v9 (stack65)
        %v18329 = vshll.u32 %v18320, 6 (stack73)
        %v18330 = vshrl.u32 %v18320, 26 (stack74)
        %v18331 = vor.u32 %v18329, %v18330 (stack75)
        %v18332 = vxor.u32 %v18323, %v18331 (stack76)
        %v18335 = vadd.s32 %v18332, %v8 (stack65)
        %v18339 = vadd.s32 %v18335, 1 (stack65)
        %v18343 = vadd.s32 %v18327, %v18339 (stack65)
        %v18345 = vshll.u32 %v18339, 17 (stack73)
        %v18346 = vshrl.u32 %v18339, 15 (stack74)
        %v18347 = vor.u32 %v18345, %v18346 (stack75)
        %v18348 = vxor.u32 %v18343, %v18347 (stack76)
        %v18351 = vadd.s32 %v18343, %v18348 (stack65)
        %v18353 = vshll.u32 %v18348, 29 (stack73)
        %v18354 = vshrl.u32 %v18348, 3 (stack74)
        %v18355 = vor.u32 %v18353, %v18354 (stack75)
        %v18356 = vxor.u32 %v18351, %v18355 (stack76)
        %v18359 = vadd.s32 %v18351, %v18356 (stack65)
        %v18361 = vshll.u32 %v18356, 16 (stack73)
        %v18362 = vshrl.u32 %v18356, 16 (stack74)
        %v18363 = vor.u32 %v18361, %v18362 (stack75)
        %v18364 = vxor.u32 %v18359, %v18363 (stack76)
        %v18367 = vadd.s32 %v18359, %v18364 (stack65)
        %v18371 = vadd.s32 %v18367, %v8 (stack65)
        %v18373 = vshll.u32 %v18364, 24 (stack73)
        %v18374 = vshrl.u32 %v18364, 8 (stack74)
        %v18375 = vor.u32 %v18373, %v18374 (stack75)
        %v18376 = vxor.u32 %v18367, %v18375 (stack76)
        %v18379 = vadd.s32 %v18376, %v10 (stack65)
        %v18383 = vadd.s32 %v18379, 2 (stack65)
        %v18387 = vadd.s32 %v18371, %v18383 (stack65)
        %v18389 = vshll.u32 %v18383, 13 (stack73)
        %v18390 = vshrl.u32 %v18383, 19 (stack74)
        %v18391 = vor.u32 %v18389, %v18390 (stack75)
        %v18392 = vxor.u32 %v18387, %v18391 (stack76)
        %v18395 = vadd.s32 %v18387, %v18392 (stack65)
        %v18397 = vshll.u32 %v18392, 15 (stack73)
        %v18398 = vshrl.u32 %v18392, 17 (stack74)
        %v18399 = vor.u32 %v18397, %v18398 (stack75)
        %v18400 = vxor.u32 %v18395, %v18399 (stack76)
        %v18403 = vadd.s32 %v18395, %v18400 (stack65)
        %v18405 = vshll.u32 %v18400, 26 (stack73)
        %v18406 = vshrl.u32 %v18400, 6 (stack74)
        %v18407 = vor.u32 %v18405, %v18406 (stack75)
        %v18408 = vxor.u32 %v18403, %v18407 (stack76)
        %v18411 = vadd.s32 %v18403, %v18408 (stack65)
        %v18415 = vadd.s32 %v18411, %v10 (stack65)
        %v18417 = vshll.u32 %v18408, 6 (stack73)
        %v18418 = vshrl.u32 %v18408, 26 (stack74)
        %v18419 = vor.u32 %v18417, %v18418 (stack75)
        %v18420 = vxor.u32 %v18411, %v18419 (stack76)
        %v18423 = vadd.s32 %v18420, %v9 (stack65)
        %v18427 = vadd.s32 %v18423, 3 (stack65)
        %v18431 = vadd.s32 %v18415, %v18427 (stack65)
        %v18433 = vshll.u32 %v18427, 17 (stack73)
        %v18434 = vshrl.u32 %v18427, 15 (stack74)
        %v18435 = vor.u32 %v18433, %v18434 (stack75)
        %v18436 = vxor.u32 %v18431, %v18435 (stack76)
        %v18439 = vadd.s32 %v18431, %v18436 (stack65)
        %v18441 = vshll.u32 %v18436, 29 (stack73)
        %v18442 = vshrl.u32 %v18436, 3 (stack74)
        %v18443 = vor.u32 %v18441, %v18442 (stack75)
        %v18444 = vxor.u32 %v18439, %v18443 (stack76)
        %v18447 = vadd.s32 %v18439, %v18444 (stack65)
        %v18449 = vshll.u32 %v18444, 16 (stack73)
        %v18450 = vshrl.u32 %v18444, 16 (stack74)
        %v18451 = vor.u32 %v18449, %v18450 (stack75)
        %v18452 = vxor.u32 %v18447, %v18451 (stack76)
        %v18455 = vadd.s32 %v18447, %v18452 (stack65)
        %v18459 = vadd.s32 %v18455, %v9 (stack65)
        %v18461 = vshll.u32 %v18452, 24 (stack73)
        %v18462 = vshrl.u32 %v18452, 8 (stack74)
        %v18463 = vor.u32 %v18461, %v18462 (stack75)
        %v18464 = vxor.u32 %v18455, %v18463 (stack76)
        %v18467 = vadd.s32 %v18464, %v8 (stack65)
        %v18471 = vadd.s32 %v18467, 4 (stack65)
        %v18475 = vadd.s32 %v18459, %v18471 (stack65)
        %v18477 = vshll.u32 %v18471, 13 (stack73)
        %v18478 = vshrl.u32 %v18471, 19 (stack74)
        %v18479 = vor.u32 %v18477, %v18478 (stack75)
        %v18480 = vxor.u32 %v18475, %v18479 (stack76)
        %v18483 = vadd.s32 %v18475, %v18480 (stack65)
        %v18485 = vshll.u32 %v18480, 15 (stack73)
        %v18486 = vshrl.u32 %v18480, 17 (stack74)
        %v18487 = vor.u32 %v18485, %v18486 (stack75)
        %v18488 = vxor.u32 %v18483, %v18487 (stack76)
        %v18491 = vadd.s32 %v18483, %v18488 (stack65)
        %v18493 = vshll.u32 %v18488, 26 (stack73)
        %v18494 = vshrl.u32 %v18488, 6 (stack74)
        %v18495 = vor.u32 %v18493, %v18494 (stack75)
        %v18496 = vxor.u32 %v18491, %v18495 (stack76)
        %v18499 = vadd.s32 %v18491, %v18496 (stack65)
        %v18503 = vadd.s32 %v18499, %v8 (stack65)
        %v18505 = vshll.u32 %v18496, 6 (stack73)
        %v18506 = vshrl.u32 %v18496, 26 (stack74)
        %v18507 = vor.u32 %v18505, %v18506 (stack75)
        %v18508 = vxor.u32 %v18499, %v18507 (stack76)
        %v18511 = vadd.s32 %v18508, %v10 (stack65)
        %v18515 = vadd.s32 %v18511, 5 (stack65)
        %v18517 = vxor.u32 %v18503, %v18515 (stack76)
        %v18518 = vand.u32.u8 %v18517, 255 (stack77)
        %v18519 = vand.u32 %v18518, 65535 (stack78)
        %v18520 = vshrl.u32 %v18519, 1 (stack79)
        %v18521 = vor.u32 %v18520, 16256 (stack75)
        %v18522 = vand.u32.u16 %v18521, 65535 (stack80)
        %v18523 = vunpack.i.l.bf16 %v18522 (stack81)
        %v18527 = vadd.f32 %v18523, -1.0 (stack82)
        %v18531 = vmul.f32 %v18527, 2.0 (stack83)
        %v18535 = vadd.f32 %v18531, -0.99609375 (stack82)
        %v18539 = vmax.f32 -0.99609375, %v18535 (stack84)
        %v18541 = vand.u32 2147483647, %v18539 (stack85)
        %vm18544 = vcmp.eq.f32.partialorder %v18541, 1.0 (stack86)
        %v18549 = vmul.f32 %v18539, inf (stack83)
        %v18551 = vxor.u32 %v18539, 2147483648 (stack87)
        %v18554 = vmul.f32 %v18539, %v18551 (stack83)
        %v18556 = vadd.f32 %v18554, 1.0 (stack88)
        %v18557 = vlog2.pop %v18556 (stack89)
        %v18558 = vmul.f32 %v18557, 0.6931472 (stack90)
        %v18559 = vmul.f32 -0.5, %v18554 (stack91)
        %v18560 = vadd.f32 %v18559, 1.0 (stack92)
        %v18561 = vmul.f32 %v18560, %v18554 (stack93)
        %v18562 = vand.u32 2147483647, %v18554 (stack94)
        %vm18563 = vcmp.lt.f32.partialorder %v18562, 0.0004427343 (stack95)
        %v18564 = vsel /*vm=*/%vm18563, /*on_true_vy=*/%v18561, /*on_false_vx=*/%v18558 (stack96)
        %v18565 = vxor.u32 %v18564, 2147483648 (stack87)
        %vm18568 = vcmp.lt.f32.partialorder %v18565, 5.0 (stack86)
        %v18573 = vsel /*vm=*/%vm18568, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v18577 = vsel /*vm=*/%vm18568, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v18581 = vsel /*vm=*/%vm18568, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v18585 = vsel /*vm=*/%vm18568, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v18589 = vsel /*vm=*/%vm18568, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v18593 = vsel /*vm=*/%vm18568, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v18597 = vsel /*vm=*/%vm18568, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v18601 = vsel /*vm=*/%vm18568, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v18605 = vsel /*vm=*/%vm18568, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v18609 = vadd.f32 %v18565, -2.5 (stack82)
        %v18611 = vrsqrt.pop %v18565 (stack97)
        %v18612 = vmul.f32 %v18565, %v18611 (stack98)
        %vm18613 = vcmp.eq.f32.partialorder %v18565, inf (stack99)
        %v18614 = vsel /*vm=*/%vm18613, /*on_true_vy=*/%v18565, /*on_false_vx=*/%v18612 (stack100)
        %vm18615 = vcmp.eq.f32.partialorder %v18565, 0.0 (stack101)
        %v18616 = vand.u32 %v18565, 2147483648 (stack102)
        %v18617 = vsel /*vm=*/%vm18615, /*on_true_vy=*/%v18616, /*on_false_vx=*/%v18614 (stack103)
        %v18620 = vadd.f32 %v18617, -3.0 (stack82)
        %v18624 = vsel /*vm=*/%vm18568, /*on_true_vy=*/%v18609, /*on_false_vx=*/%v18620 (stack72)
        %v18628 = vmul.f32 %v18605, %v18624 (stack83)
        %v18632 = vadd.f32 %v18601, %v18628 (stack82)
        %v18636 = vmul.f32 %v18632, %v18624 (stack83)
        %v18640 = vadd.f32 %v18597, %v18636 (stack82)
        %v18644 = vmul.f32 %v18640, %v18624 (stack83)
        %v18648 = vadd.f32 %v18593, %v18644 (stack82)
        %v18652 = vmul.f32 %v18648, %v18624 (stack83)
        %v18656 = vadd.f32 %v18589, %v18652 (stack82)
        %v18660 = vmul.f32 %v18656, %v18624 (stack83)
        %v18664 = vadd.f32 %v18585, %v18660 (stack82)
        %v18668 = vmul.f32 %v18664, %v18624 (stack83)
        %v18672 = vadd.f32 %v18581, %v18668 (stack82)
        %v18676 = vmul.f32 %v18672, %v18624 (stack83)
        %v18680 = vadd.f32 %v18577, %v18676 (stack82)
        %v18684 = vmul.f32 %v18680, %v18624 (stack83)
        %v18688 = vadd.f32 %v18573, %v18684 (stack82)
        %v18692 = vmul.f32 %v18688, %v18539 (stack83)
        %v18696 = vsel /*vm=*/%vm18544, /*on_true_vy=*/%v18549, /*on_false_vx=*/%v18692 (stack72)
        %v18700 = vmul.f32 %v18696, 1.4140625 (stack83)
        %s18702 = scalar_lea.vmem %s280, 784 [#allocation0] (stack107)
        %v18703 = vpack.c.bf16 0.0, %v18700 (stack104)
        %18704 = vst [vmem:[%s18702] sm:$0xf] /*vst_source=*/%v18703 (stack105)
        %v18707 = vadd.s32 %v3816, %v15477 (stack65)
        %s18709 = smul.u32 128, %s27 (stack66)
        %v18710 = vlaneseq (stack67)
        %v18711 = vand.u32 %v18710, 127 (stack68)
        %v18712 = vstv %s18709 (stack69)
        %v18713 = vadd.s32 %v18711, %v18712 (stack70)
        %v18717 = vadd.s32 %v18707, %v18713 (stack65)
        %vm18721 = vcmp.lt.u32.totalorder %v18717, %v18707 (stack71)
        %vm18726 = vcmp.lt.u32.totalorder %v18707, %v3816 (stack71)
        %v18731 = vadd.s32 %v3803, %v15460 (stack65)
        %v18735 = vadd.s32 %v18731, 1 (stack65)
        %v18739 = vsel /*vm=*/%vm18726, /*on_true_vy=*/%v18735, /*on_false_vx=*/%v18731 (stack72)
        %v18743 = vadd.s32 %v18739, 1 (stack65)
        %v18747 = vsel /*vm=*/%vm18721, /*on_true_vy=*/%v18743, /*on_false_vx=*/%v18739 (stack72)
        %v18752 = vadd.s32 %v18747, %v10 (stack65)
        %v18756 = vadd.s32 %v18717, %v9 (stack65)
        %v18760 = vadd.s32 %v18752, %v18756 (stack65)
        %v18762 = vshll.u32 %v18756, 13 (stack73)
        %v18763 = vshrl.u32 %v18756, 19 (stack74)
        %v18764 = vor.u32 %v18762, %v18763 (stack75)
        %v18765 = vxor.u32 %v18760, %v18764 (stack76)
        %v18768 = vadd.s32 %v18760, %v18765 (stack65)
        %v18770 = vshll.u32 %v18765, 15 (stack73)
        %v18771 = vshrl.u32 %v18765, 17 (stack74)
        %v18772 = vor.u32 %v18770, %v18771 (stack75)
        %v18773 = vxor.u32 %v18768, %v18772 (stack76)
        %v18776 = vadd.s32 %v18768, %v18773 (stack65)
        %v18778 = vshll.u32 %v18773, 26 (stack73)
        %v18779 = vshrl.u32 %v18773, 6 (stack74)
        %v18780 = vor.u32 %v18778, %v18779 (stack75)
        %v18781 = vxor.u32 %v18776, %v18780 (stack76)
        %v18784 = vadd.s32 %v18776, %v18781 (stack65)
        %v18788 = vadd.s32 %v18784, %v9 (stack65)
        %v18790 = vshll.u32 %v18781, 6 (stack73)
        %v18791 = vshrl.u32 %v18781, 26 (stack74)
        %v18792 = vor.u32 %v18790, %v18791 (stack75)
        %v18793 = vxor.u32 %v18784, %v18792 (stack76)
        %v18796 = vadd.s32 %v18793, %v8 (stack65)
        %v18800 = vadd.s32 %v18796, 1 (stack65)
        %v18804 = vadd.s32 %v18788, %v18800 (stack65)
        %v18806 = vshll.u32 %v18800, 17 (stack73)
        %v18807 = vshrl.u32 %v18800, 15 (stack74)
        %v18808 = vor.u32 %v18806, %v18807 (stack75)
        %v18809 = vxor.u32 %v18804, %v18808 (stack76)
        %v18812 = vadd.s32 %v18804, %v18809 (stack65)
        %v18814 = vshll.u32 %v18809, 29 (stack73)
        %v18815 = vshrl.u32 %v18809, 3 (stack74)
        %v18816 = vor.u32 %v18814, %v18815 (stack75)
        %v18817 = vxor.u32 %v18812, %v18816 (stack76)
        %v18820 = vadd.s32 %v18812, %v18817 (stack65)
        %v18822 = vshll.u32 %v18817, 16 (stack73)
        %v18823 = vshrl.u32 %v18817, 16 (stack74)
        %v18824 = vor.u32 %v18822, %v18823 (stack75)
        %v18825 = vxor.u32 %v18820, %v18824 (stack76)
        %v18828 = vadd.s32 %v18820, %v18825 (stack65)
        %v18832 = vadd.s32 %v18828, %v8 (stack65)
        %v18834 = vshll.u32 %v18825, 24 (stack73)
        %v18835 = vshrl.u32 %v18825, 8 (stack74)
        %v18836 = vor.u32 %v18834, %v18835 (stack75)
        %v18837 = vxor.u32 %v18828, %v18836 (stack76)
        %v18840 = vadd.s32 %v18837, %v10 (stack65)
        %v18844 = vadd.s32 %v18840, 2 (stack65)
        %v18848 = vadd.s32 %v18832, %v18844 (stack65)
        %v18850 = vshll.u32 %v18844, 13 (stack73)
        %v18851 = vshrl.u32 %v18844, 19 (stack74)
        %v18852 = vor.u32 %v18850, %v18851 (stack75)
        %v18853 = vxor.u32 %v18848, %v18852 (stack76)
        %v18856 = vadd.s32 %v18848, %v18853 (stack65)
        %v18858 = vshll.u32 %v18853, 15 (stack73)
        %v18859 = vshrl.u32 %v18853, 17 (stack74)
        %v18860 = vor.u32 %v18858, %v18859 (stack75)
        %v18861 = vxor.u32 %v18856, %v18860 (stack76)
        %v18864 = vadd.s32 %v18856, %v18861 (stack65)
        %v18866 = vshll.u32 %v18861, 26 (stack73)
        %v18867 = vshrl.u32 %v18861, 6 (stack74)
        %v18868 = vor.u32 %v18866, %v18867 (stack75)
        %v18869 = vxor.u32 %v18864, %v18868 (stack76)
        %v18872 = vadd.s32 %v18864, %v18869 (stack65)
        %v18876 = vadd.s32 %v18872, %v10 (stack65)
        %v18878 = vshll.u32 %v18869, 6 (stack73)
        %v18879 = vshrl.u32 %v18869, 26 (stack74)
        %v18880 = vor.u32 %v18878, %v18879 (stack75)
        %v18881 = vxor.u32 %v18872, %v18880 (stack76)
        %v18884 = vadd.s32 %v18881, %v9 (stack65)
        %v18888 = vadd.s32 %v18884, 3 (stack65)
        %v18892 = vadd.s32 %v18876, %v18888 (stack65)
        %v18894 = vshll.u32 %v18888, 17 (stack73)
        %v18895 = vshrl.u32 %v18888, 15 (stack74)
        %v18896 = vor.u32 %v18894, %v18895 (stack75)
        %v18897 = vxor.u32 %v18892, %v18896 (stack76)
        %v18900 = vadd.s32 %v18892, %v18897 (stack65)
        %v18902 = vshll.u32 %v18897, 29 (stack73)
        %v18903 = vshrl.u32 %v18897, 3 (stack74)
        %v18904 = vor.u32 %v18902, %v18903 (stack75)
        %v18905 = vxor.u32 %v18900, %v18904 (stack76)
        %v18908 = vadd.s32 %v18900, %v18905 (stack65)
        %v18910 = vshll.u32 %v18905, 16 (stack73)
        %v18911 = vshrl.u32 %v18905, 16 (stack74)
        %v18912 = vor.u32 %v18910, %v18911 (stack75)
        %v18913 = vxor.u32 %v18908, %v18912 (stack76)
        %v18916 = vadd.s32 %v18908, %v18913 (stack65)
        %v18920 = vadd.s32 %v18916, %v9 (stack65)
        %v18922 = vshll.u32 %v18913, 24 (stack73)
        %v18923 = vshrl.u32 %v18913, 8 (stack74)
        %v18924 = vor.u32 %v18922, %v18923 (stack75)
        %v18925 = vxor.u32 %v18916, %v18924 (stack76)
        %v18928 = vadd.s32 %v18925, %v8 (stack65)
        %v18932 = vadd.s32 %v18928, 4 (stack65)
        %v18936 = vadd.s32 %v18920, %v18932 (stack65)
        %v18938 = vshll.u32 %v18932, 13 (stack73)
        %v18939 = vshrl.u32 %v18932, 19 (stack74)
        %v18940 = vor.u32 %v18938, %v18939 (stack75)
        %v18941 = vxor.u32 %v18936, %v18940 (stack76)
        %v18944 = vadd.s32 %v18936, %v18941 (stack65)
        %v18946 = vshll.u32 %v18941, 15 (stack73)
        %v18947 = vshrl.u32 %v18941, 17 (stack74)
        %v18948 = vor.u32 %v18946, %v18947 (stack75)
        %v18949 = vxor.u32 %v18944, %v18948 (stack76)
        %v18952 = vadd.s32 %v18944, %v18949 (stack65)
        %v18954 = vshll.u32 %v18949, 26 (stack73)
        %v18955 = vshrl.u32 %v18949, 6 (stack74)
        %v18956 = vor.u32 %v18954, %v18955 (stack75)
        %v18957 = vxor.u32 %v18952, %v18956 (stack76)
        %v18960 = vadd.s32 %v18952, %v18957 (stack65)
        %v18964 = vadd.s32 %v18960, %v8 (stack65)
        %v18966 = vshll.u32 %v18957, 6 (stack73)
        %v18967 = vshrl.u32 %v18957, 26 (stack74)
        %v18968 = vor.u32 %v18966, %v18967 (stack75)
        %v18969 = vxor.u32 %v18960, %v18968 (stack76)
        %v18972 = vadd.s32 %v18969, %v10 (stack65)
        %v18976 = vadd.s32 %v18972, 5 (stack65)
        %v18978 = vxor.u32 %v18964, %v18976 (stack76)
        %v18979 = vand.u32.u8 %v18978, 255 (stack77)
        %v18980 = vand.u32 %v18979, 65535 (stack78)
        %v18981 = vshrl.u32 %v18980, 1 (stack79)
        %v18982 = vor.u32 %v18981, 16256 (stack75)
        %v18983 = vand.u32.u16 %v18982, 65535 (stack80)
        %v18984 = vunpack.i.l.bf16 %v18983 (stack81)
        %v18988 = vadd.f32 %v18984, -1.0 (stack82)
        %v18992 = vmul.f32 %v18988, 2.0 (stack83)
        %v18996 = vadd.f32 %v18992, -0.99609375 (stack82)
        %v19000 = vmax.f32 -0.99609375, %v18996 (stack84)
        %v19002 = vand.u32 2147483647, %v19000 (stack85)
        %vm19005 = vcmp.eq.f32.partialorder %v19002, 1.0 (stack86)
        %v19010 = vmul.f32 %v19000, inf (stack83)
        %v19012 = vxor.u32 %v19000, 2147483648 (stack87)
        %v19015 = vmul.f32 %v19000, %v19012 (stack83)
        %v19017 = vadd.f32 %v19015, 1.0 (stack88)
        %v19018 = vlog2.pop %v19017 (stack89)
        %v19019 = vmul.f32 %v19018, 0.6931472 (stack90)
        %v19020 = vmul.f32 -0.5, %v19015 (stack91)
        %v19021 = vadd.f32 %v19020, 1.0 (stack92)
        %v19022 = vmul.f32 %v19021, %v19015 (stack93)
        %v19023 = vand.u32 2147483647, %v19015 (stack94)
        %vm19024 = vcmp.lt.f32.partialorder %v19023, 0.0004427343 (stack95)
        %v19025 = vsel /*vm=*/%vm19024, /*on_true_vy=*/%v19022, /*on_false_vx=*/%v19019 (stack96)
        %v19026 = vxor.u32 %v19025, 2147483648 (stack87)
        %vm19029 = vcmp.lt.f32.partialorder %v19026, 5.0 (stack86)
        %v19034 = vsel /*vm=*/%vm19029, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v19038 = vsel /*vm=*/%vm19029, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v19042 = vsel /*vm=*/%vm19029, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v19046 = vsel /*vm=*/%vm19029, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v19050 = vsel /*vm=*/%vm19029, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v19054 = vsel /*vm=*/%vm19029, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v19058 = vsel /*vm=*/%vm19029, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v19062 = vsel /*vm=*/%vm19029, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v19066 = vsel /*vm=*/%vm19029, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v19070 = vadd.f32 %v19026, -2.5 (stack82)
        %v19072 = vrsqrt.pop %v19026 (stack97)
        %v19073 = vmul.f32 %v19026, %v19072 (stack98)
        %vm19074 = vcmp.eq.f32.partialorder %v19026, inf (stack99)
        %v19075 = vsel /*vm=*/%vm19074, /*on_true_vy=*/%v19026, /*on_false_vx=*/%v19073 (stack100)
        %vm19076 = vcmp.eq.f32.partialorder %v19026, 0.0 (stack101)
        %v19077 = vand.u32 %v19026, 2147483648 (stack102)
        %v19078 = vsel /*vm=*/%vm19076, /*on_true_vy=*/%v19077, /*on_false_vx=*/%v19075 (stack103)
        %v19081 = vadd.f32 %v19078, -3.0 (stack82)
        %v19085 = vsel /*vm=*/%vm19029, /*on_true_vy=*/%v19070, /*on_false_vx=*/%v19081 (stack72)
        %v19089 = vmul.f32 %v19066, %v19085 (stack83)
        %v19093 = vadd.f32 %v19062, %v19089 (stack82)
        %v19097 = vmul.f32 %v19093, %v19085 (stack83)
        %v19101 = vadd.f32 %v19058, %v19097 (stack82)
        %v19105 = vmul.f32 %v19101, %v19085 (stack83)
        %v19109 = vadd.f32 %v19054, %v19105 (stack82)
        %v19113 = vmul.f32 %v19109, %v19085 (stack83)
        %v19117 = vadd.f32 %v19050, %v19113 (stack82)
        %v19121 = vmul.f32 %v19117, %v19085 (stack83)
        %v19125 = vadd.f32 %v19046, %v19121 (stack82)
        %v19129 = vmul.f32 %v19125, %v19085 (stack83)
        %v19133 = vadd.f32 %v19042, %v19129 (stack82)
        %v19137 = vmul.f32 %v19133, %v19085 (stack83)
        %v19141 = vadd.f32 %v19038, %v19137 (stack82)
        %v19145 = vmul.f32 %v19141, %v19085 (stack83)
        %v19149 = vadd.f32 %v19034, %v19145 (stack82)
        %v19153 = vmul.f32 %v19149, %v19000 (stack83)
        %v19157 = vsel /*vm=*/%vm19005, /*on_true_vy=*/%v19010, /*on_false_vx=*/%v19153 (stack72)
        %v19161 = vmul.f32 %v19157, 1.4140625 (stack83)
        %s19163 = scalar_lea.vmem %s280, 912 [#allocation0] (stack107)
        %v19164 = vpack.c.bf16 0.0, %v19161 (stack104)
        %19165 = vst [vmem:[%s19163] sm:$0xf] /*vst_source=*/%v19164 (stack105)
        %s19166 = sadd.s32 %s339, 40 (stack106)
        %s19167 = sshrl.u32 %s19166, 10 (stack49)
        %p19168 = scmp.lt.s32.totalorder 1, %s19167 (stack50)
        %s19169 = scalar_select /*predicate=*/%p19168, /*on_true=*/1, /*on_false=*/%s19167 (stack51)
        %s19170 = sand.u32 %s19166, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s19171 = sshrl.u32 %s19170, 7 (stack53)
        %s19172 = sand.u32 %s19170, 127 /* smod.u32 w/div 128 */ (stack54)
        %s19173 = smul.addr %s19169, 8 (stack55)
        %s19174 = scalar_lea.vmem %s3, %s19173 (stack56)
        %s19176 = scalar_lea.vmem %s19174, %s19171 (stack57)
        %v19177 = vld [vmem:[%s19176] ss:$0 sm:$0xff] (stack58)
        %s19178 = sand.u32 %s19172, 255 (stack59)
        %s19180 = sor.u32 256, %s19178 (stack60)
        %19181 = vbcast.lane.b32.xlu0 %v19177, %s19180 (stack61)
        %v19182 = vpop.permute.xlu0 %19181 (stack62)
        %s19183 = sadd.s32 %s347, 40 (stack106)
        %s19184 = sshrl.u32 %s19183, 10 (stack49)
        %p19185 = scmp.lt.s32.totalorder 1, %s19184 (stack50)
        %s19186 = scalar_select /*predicate=*/%p19185, /*on_true=*/1, /*on_false=*/%s19184 (stack51)
        %s19187 = sand.u32 %s19183, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s19188 = sshrl.u32 %s19187, 7 (stack53)
        %s19189 = sand.u32 %s19187, 127 /* smod.u32 w/div 128 */ (stack54)
        %s19190 = smul.addr %s19186, 8 (stack55)
        %s19191 = scalar_lea.vmem %s5, %s19190 (stack56)
        %s19193 = scalar_lea.vmem %s19191, %s19188 (stack57)
        %v19194 = vld [vmem:[%s19193] ss:$0 sm:$0xff] (stack58)
        %s19195 = sand.u32 %s19189, 255 (stack59)
        %s19197 = sor.u32 256, %s19195 (stack60)
        %19198 = vbcast.lane.b32.xlu0 %v19194, %s19197 (stack61)
        %v19199 = vpop.permute.xlu0 %19198 (stack62)
        %v19202 = vadd.s32 %v408, %v19199 (stack65)
        %s19204 = smul.u32 128, %s27 (stack66)
        %v19205 = vlaneseq (stack67)
        %v19206 = vand.u32 %v19205, 127 (stack68)
        %v19207 = vstv %s19204 (stack69)
        %v19208 = vadd.s32 %v19206, %v19207 (stack70)
        %v19212 = vadd.s32 %v19202, %v19208 (stack65)
        %vm19216 = vcmp.lt.u32.totalorder %v19212, %v19202 (stack71)
        %vm19221 = vcmp.lt.u32.totalorder %v19202, %v408 (stack71)
        %v19226 = vadd.s32 %v380, %v19182 (stack65)
        %v19230 = vadd.s32 %v19226, 1 (stack65)
        %v19234 = vsel /*vm=*/%vm19221, /*on_true_vy=*/%v19230, /*on_false_vx=*/%v19226 (stack72)
        %v19238 = vadd.s32 %v19234, 1 (stack65)
        %v19242 = vsel /*vm=*/%vm19216, /*on_true_vy=*/%v19238, /*on_false_vx=*/%v19234 (stack72)
        %v19247 = vadd.s32 %v19242, %v10 (stack65)
        %v19251 = vadd.s32 %v19212, %v9 (stack65)
        %v19255 = vadd.s32 %v19247, %v19251 (stack65)
        %v19257 = vshll.u32 %v19251, 13 (stack73)
        %v19258 = vshrl.u32 %v19251, 19 (stack74)
        %v19259 = vor.u32 %v19257, %v19258 (stack75)
        %v19260 = vxor.u32 %v19255, %v19259 (stack76)
        %v19263 = vadd.s32 %v19255, %v19260 (stack65)
        %v19265 = vshll.u32 %v19260, 15 (stack73)
        %v19266 = vshrl.u32 %v19260, 17 (stack74)
        %v19267 = vor.u32 %v19265, %v19266 (stack75)
        %v19268 = vxor.u32 %v19263, %v19267 (stack76)
        %v19271 = vadd.s32 %v19263, %v19268 (stack65)
        %v19273 = vshll.u32 %v19268, 26 (stack73)
        %v19274 = vshrl.u32 %v19268, 6 (stack74)
        %v19275 = vor.u32 %v19273, %v19274 (stack75)
        %v19276 = vxor.u32 %v19271, %v19275 (stack76)
        %v19279 = vadd.s32 %v19271, %v19276 (stack65)
        %v19283 = vadd.s32 %v19279, %v9 (stack65)
        %v19285 = vshll.u32 %v19276, 6 (stack73)
        %v19286 = vshrl.u32 %v19276, 26 (stack74)
        %v19287 = vor.u32 %v19285, %v19286 (stack75)
        %v19288 = vxor.u32 %v19279, %v19287 (stack76)
        %v19291 = vadd.s32 %v19288, %v8 (stack65)
        %v19295 = vadd.s32 %v19291, 1 (stack65)
        %v19299 = vadd.s32 %v19283, %v19295 (stack65)
        %v19301 = vshll.u32 %v19295, 17 (stack73)
        %v19302 = vshrl.u32 %v19295, 15 (stack74)
        %v19303 = vor.u32 %v19301, %v19302 (stack75)
        %v19304 = vxor.u32 %v19299, %v19303 (stack76)
        %v19307 = vadd.s32 %v19299, %v19304 (stack65)
        %v19309 = vshll.u32 %v19304, 29 (stack73)
        %v19310 = vshrl.u32 %v19304, 3 (stack74)
        %v19311 = vor.u32 %v19309, %v19310 (stack75)
        %v19312 = vxor.u32 %v19307, %v19311 (stack76)
        %v19315 = vadd.s32 %v19307, %v19312 (stack65)
        %v19317 = vshll.u32 %v19312, 16 (stack73)
        %v19318 = vshrl.u32 %v19312, 16 (stack74)
        %v19319 = vor.u32 %v19317, %v19318 (stack75)
        %v19320 = vxor.u32 %v19315, %v19319 (stack76)
        %v19323 = vadd.s32 %v19315, %v19320 (stack65)
        %v19327 = vadd.s32 %v19323, %v8 (stack65)
        %v19329 = vshll.u32 %v19320, 24 (stack73)
        %v19330 = vshrl.u32 %v19320, 8 (stack74)
        %v19331 = vor.u32 %v19329, %v19330 (stack75)
        %v19332 = vxor.u32 %v19323, %v19331 (stack76)
        %v19335 = vadd.s32 %v19332, %v10 (stack65)
        %v19339 = vadd.s32 %v19335, 2 (stack65)
        %v19343 = vadd.s32 %v19327, %v19339 (stack65)
        %v19345 = vshll.u32 %v19339, 13 (stack73)
        %v19346 = vshrl.u32 %v19339, 19 (stack74)
        %v19347 = vor.u32 %v19345, %v19346 (stack75)
        %v19348 = vxor.u32 %v19343, %v19347 (stack76)
        %v19351 = vadd.s32 %v19343, %v19348 (stack65)
        %v19353 = vshll.u32 %v19348, 15 (stack73)
        %v19354 = vshrl.u32 %v19348, 17 (stack74)
        %v19355 = vor.u32 %v19353, %v19354 (stack75)
        %v19356 = vxor.u32 %v19351, %v19355 (stack76)
        %v19359 = vadd.s32 %v19351, %v19356 (stack65)
        %v19361 = vshll.u32 %v19356, 26 (stack73)
        %v19362 = vshrl.u32 %v19356, 6 (stack74)
        %v19363 = vor.u32 %v19361, %v19362 (stack75)
        %v19364 = vxor.u32 %v19359, %v19363 (stack76)
        %v19367 = vadd.s32 %v19359, %v19364 (stack65)
        %v19371 = vadd.s32 %v19367, %v10 (stack65)
        %v19373 = vshll.u32 %v19364, 6 (stack73)
        %v19374 = vshrl.u32 %v19364, 26 (stack74)
        %v19375 = vor.u32 %v19373, %v19374 (stack75)
        %v19376 = vxor.u32 %v19367, %v19375 (stack76)
        %v19379 = vadd.s32 %v19376, %v9 (stack65)
        %v19383 = vadd.s32 %v19379, 3 (stack65)
        %v19387 = vadd.s32 %v19371, %v19383 (stack65)
        %v19389 = vshll.u32 %v19383, 17 (stack73)
        %v19390 = vshrl.u32 %v19383, 15 (stack74)
        %v19391 = vor.u32 %v19389, %v19390 (stack75)
        %v19392 = vxor.u32 %v19387, %v19391 (stack76)
        %v19395 = vadd.s32 %v19387, %v19392 (stack65)
        %v19397 = vshll.u32 %v19392, 29 (stack73)
        %v19398 = vshrl.u32 %v19392, 3 (stack74)
        %v19399 = vor.u32 %v19397, %v19398 (stack75)
        %v19400 = vxor.u32 %v19395, %v19399 (stack76)
        %v19403 = vadd.s32 %v19395, %v19400 (stack65)
        %v19405 = vshll.u32 %v19400, 16 (stack73)
        %v19406 = vshrl.u32 %v19400, 16 (stack74)
        %v19407 = vor.u32 %v19405, %v19406 (stack75)
        %v19408 = vxor.u32 %v19403, %v19407 (stack76)
        %v19411 = vadd.s32 %v19403, %v19408 (stack65)
        %v19415 = vadd.s32 %v19411, %v9 (stack65)
        %v19417 = vshll.u32 %v19408, 24 (stack73)
        %v19418 = vshrl.u32 %v19408, 8 (stack74)
        %v19419 = vor.u32 %v19417, %v19418 (stack75)
        %v19420 = vxor.u32 %v19411, %v19419 (stack76)
        %v19423 = vadd.s32 %v19420, %v8 (stack65)
        %v19427 = vadd.s32 %v19423, 4 (stack65)
        %v19431 = vadd.s32 %v19415, %v19427 (stack65)
        %v19433 = vshll.u32 %v19427, 13 (stack73)
        %v19434 = vshrl.u32 %v19427, 19 (stack74)
        %v19435 = vor.u32 %v19433, %v19434 (stack75)
        %v19436 = vxor.u32 %v19431, %v19435 (stack76)
        %v19439 = vadd.s32 %v19431, %v19436 (stack65)
        %v19441 = vshll.u32 %v19436, 15 (stack73)
        %v19442 = vshrl.u32 %v19436, 17 (stack74)
        %v19443 = vor.u32 %v19441, %v19442 (stack75)
        %v19444 = vxor.u32 %v19439, %v19443 (stack76)
        %v19447 = vadd.s32 %v19439, %v19444 (stack65)
        %v19449 = vshll.u32 %v19444, 26 (stack73)
        %v19450 = vshrl.u32 %v19444, 6 (stack74)
        %v19451 = vor.u32 %v19449, %v19450 (stack75)
        %v19452 = vxor.u32 %v19447, %v19451 (stack76)
        %v19455 = vadd.s32 %v19447, %v19452 (stack65)
        %v19459 = vadd.s32 %v19455, %v8 (stack65)
        %v19461 = vshll.u32 %v19452, 6 (stack73)
        %v19462 = vshrl.u32 %v19452, 26 (stack74)
        %v19463 = vor.u32 %v19461, %v19462 (stack75)
        %v19464 = vxor.u32 %v19455, %v19463 (stack76)
        %v19467 = vadd.s32 %v19464, %v10 (stack65)
        %v19471 = vadd.s32 %v19467, 5 (stack65)
        %v19473 = vxor.u32 %v19459, %v19471 (stack76)
        %v19474 = vand.u32.u8 %v19473, 255 (stack77)
        %v19475 = vand.u32 %v19474, 65535 (stack78)
        %v19476 = vshrl.u32 %v19475, 1 (stack79)
        %v19477 = vor.u32 %v19476, 16256 (stack75)
        %v19478 = vand.u32.u16 %v19477, 65535 (stack80)
        %v19479 = vunpack.i.l.bf16 %v19478 (stack81)
        %v19483 = vadd.f32 %v19479, -1.0 (stack82)
        %v19487 = vmul.f32 %v19483, 2.0 (stack83)
        %v19491 = vadd.f32 %v19487, -0.99609375 (stack82)
        %v19495 = vmax.f32 -0.99609375, %v19491 (stack84)
        %v19497 = vand.u32 2147483647, %v19495 (stack85)
        %vm19500 = vcmp.eq.f32.partialorder %v19497, 1.0 (stack86)
        %v19505 = vmul.f32 %v19495, inf (stack83)
        %v19507 = vxor.u32 %v19495, 2147483648 (stack87)
        %v19510 = vmul.f32 %v19495, %v19507 (stack83)
        %v19512 = vadd.f32 %v19510, 1.0 (stack88)
        %v19513 = vlog2.pop %v19512 (stack89)
        %v19514 = vmul.f32 %v19513, 0.6931472 (stack90)
        %v19515 = vmul.f32 -0.5, %v19510 (stack91)
        %v19516 = vadd.f32 %v19515, 1.0 (stack92)
        %v19517 = vmul.f32 %v19516, %v19510 (stack93)
        %v19518 = vand.u32 2147483647, %v19510 (stack94)
        %vm19519 = vcmp.lt.f32.partialorder %v19518, 0.0004427343 (stack95)
        %v19520 = vsel /*vm=*/%vm19519, /*on_true_vy=*/%v19517, /*on_false_vx=*/%v19514 (stack96)
        %v19521 = vxor.u32 %v19520, 2147483648 (stack87)
        %vm19524 = vcmp.lt.f32.partialorder %v19521, 5.0 (stack86)
        %v19529 = vsel /*vm=*/%vm19524, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v19533 = vsel /*vm=*/%vm19524, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v19537 = vsel /*vm=*/%vm19524, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v19541 = vsel /*vm=*/%vm19524, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v19545 = vsel /*vm=*/%vm19524, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v19549 = vsel /*vm=*/%vm19524, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v19553 = vsel /*vm=*/%vm19524, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v19557 = vsel /*vm=*/%vm19524, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v19561 = vsel /*vm=*/%vm19524, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v19565 = vadd.f32 %v19521, -2.5 (stack82)
        %v19567 = vrsqrt.pop %v19521 (stack97)
        %v19568 = vmul.f32 %v19521, %v19567 (stack98)
        %vm19569 = vcmp.eq.f32.partialorder %v19521, inf (stack99)
        %v19570 = vsel /*vm=*/%vm19569, /*on_true_vy=*/%v19521, /*on_false_vx=*/%v19568 (stack100)
        %vm19571 = vcmp.eq.f32.partialorder %v19521, 0.0 (stack101)
        %v19572 = vand.u32 %v19521, 2147483648 (stack102)
        %v19573 = vsel /*vm=*/%vm19571, /*on_true_vy=*/%v19572, /*on_false_vx=*/%v19570 (stack103)
        %v19576 = vadd.f32 %v19573, -3.0 (stack82)
        %v19580 = vsel /*vm=*/%vm19524, /*on_true_vy=*/%v19565, /*on_false_vx=*/%v19576 (stack72)
        %v19584 = vmul.f32 %v19561, %v19580 (stack83)
        %v19588 = vadd.f32 %v19557, %v19584 (stack82)
        %v19592 = vmul.f32 %v19588, %v19580 (stack83)
        %v19596 = vadd.f32 %v19553, %v19592 (stack82)
        %v19600 = vmul.f32 %v19596, %v19580 (stack83)
        %v19604 = vadd.f32 %v19549, %v19600 (stack82)
        %v19608 = vmul.f32 %v19604, %v19580 (stack83)
        %v19612 = vadd.f32 %v19545, %v19608 (stack82)
        %v19616 = vmul.f32 %v19612, %v19580 (stack83)
        %v19620 = vadd.f32 %v19541, %v19616 (stack82)
        %v19624 = vmul.f32 %v19620, %v19580 (stack83)
        %v19628 = vadd.f32 %v19537, %v19624 (stack82)
        %v19632 = vmul.f32 %v19628, %v19580 (stack83)
        %v19636 = vadd.f32 %v19533, %v19632 (stack82)
        %v19640 = vmul.f32 %v19636, %v19580 (stack83)
        %v19644 = vadd.f32 %v19529, %v19640 (stack82)
        %v19648 = vmul.f32 %v19644, %v19495 (stack83)
        %v19652 = vsel /*vm=*/%vm19500, /*on_true_vy=*/%v19505, /*on_false_vx=*/%v19648 (stack72)
        %v19656 = vmul.f32 %v19652, 1.4140625 (stack83)
        %s19658 = scalar_lea.vmem %s280, 20 [#allocation0] (stack107)
        %v19659 = vpack.c.bf16 0.0, %v19656 (stack104)
        %19660 = vst [vmem:[%s19658] sm:$0xf] /*vst_source=*/%v19659 (stack105)
        %v19663 = vadd.s32 %v894, %v19199 (stack65)
        %s19665 = smul.u32 128, %s27 (stack66)
        %v19666 = vlaneseq (stack67)
        %v19667 = vand.u32 %v19666, 127 (stack68)
        %v19668 = vstv %s19665 (stack69)
        %v19669 = vadd.s32 %v19667, %v19668 (stack70)
        %v19673 = vadd.s32 %v19663, %v19669 (stack65)
        %vm19677 = vcmp.lt.u32.totalorder %v19673, %v19663 (stack71)
        %vm19682 = vcmp.lt.u32.totalorder %v19663, %v894 (stack71)
        %v19687 = vadd.s32 %v881, %v19182 (stack65)
        %v19691 = vadd.s32 %v19687, 1 (stack65)
        %v19695 = vsel /*vm=*/%vm19682, /*on_true_vy=*/%v19691, /*on_false_vx=*/%v19687 (stack72)
        %v19699 = vadd.s32 %v19695, 1 (stack65)
        %v19703 = vsel /*vm=*/%vm19677, /*on_true_vy=*/%v19699, /*on_false_vx=*/%v19695 (stack72)
        %v19708 = vadd.s32 %v19703, %v10 (stack65)
        %v19712 = vadd.s32 %v19673, %v9 (stack65)
        %v19716 = vadd.s32 %v19708, %v19712 (stack65)
        %v19718 = vshll.u32 %v19712, 13 (stack73)
        %v19719 = vshrl.u32 %v19712, 19 (stack74)
        %v19720 = vor.u32 %v19718, %v19719 (stack75)
        %v19721 = vxor.u32 %v19716, %v19720 (stack76)
        %v19724 = vadd.s32 %v19716, %v19721 (stack65)
        %v19726 = vshll.u32 %v19721, 15 (stack73)
        %v19727 = vshrl.u32 %v19721, 17 (stack74)
        %v19728 = vor.u32 %v19726, %v19727 (stack75)
        %v19729 = vxor.u32 %v19724, %v19728 (stack76)
        %v19732 = vadd.s32 %v19724, %v19729 (stack65)
        %v19734 = vshll.u32 %v19729, 26 (stack73)
        %v19735 = vshrl.u32 %v19729, 6 (stack74)
        %v19736 = vor.u32 %v19734, %v19735 (stack75)
        %v19737 = vxor.u32 %v19732, %v19736 (stack76)
        %v19740 = vadd.s32 %v19732, %v19737 (stack65)
        %v19744 = vadd.s32 %v19740, %v9 (stack65)
        %v19746 = vshll.u32 %v19737, 6 (stack73)
        %v19747 = vshrl.u32 %v19737, 26 (stack74)
        %v19748 = vor.u32 %v19746, %v19747 (stack75)
        %v19749 = vxor.u32 %v19740, %v19748 (stack76)
        %v19752 = vadd.s32 %v19749, %v8 (stack65)
        %v19756 = vadd.s32 %v19752, 1 (stack65)
        %v19760 = vadd.s32 %v19744, %v19756 (stack65)
        %v19762 = vshll.u32 %v19756, 17 (stack73)
        %v19763 = vshrl.u32 %v19756, 15 (stack74)
        %v19764 = vor.u32 %v19762, %v19763 (stack75)
        %v19765 = vxor.u32 %v19760, %v19764 (stack76)
        %v19768 = vadd.s32 %v19760, %v19765 (stack65)
        %v19770 = vshll.u32 %v19765, 29 (stack73)
        %v19771 = vshrl.u32 %v19765, 3 (stack74)
        %v19772 = vor.u32 %v19770, %v19771 (stack75)
        %v19773 = vxor.u32 %v19768, %v19772 (stack76)
        %v19776 = vadd.s32 %v19768, %v19773 (stack65)
        %v19778 = vshll.u32 %v19773, 16 (stack73)
        %v19779 = vshrl.u32 %v19773, 16 (stack74)
        %v19780 = vor.u32 %v19778, %v19779 (stack75)
        %v19781 = vxor.u32 %v19776, %v19780 (stack76)
        %v19784 = vadd.s32 %v19776, %v19781 (stack65)
        %v19788 = vadd.s32 %v19784, %v8 (stack65)
        %v19790 = vshll.u32 %v19781, 24 (stack73)
        %v19791 = vshrl.u32 %v19781, 8 (stack74)
        %v19792 = vor.u32 %v19790, %v19791 (stack75)
        %v19793 = vxor.u32 %v19784, %v19792 (stack76)
        %v19796 = vadd.s32 %v19793, %v10 (stack65)
        %v19800 = vadd.s32 %v19796, 2 (stack65)
        %v19804 = vadd.s32 %v19788, %v19800 (stack65)
        %v19806 = vshll.u32 %v19800, 13 (stack73)
        %v19807 = vshrl.u32 %v19800, 19 (stack74)
        %v19808 = vor.u32 %v19806, %v19807 (stack75)
        %v19809 = vxor.u32 %v19804, %v19808 (stack76)
        %v19812 = vadd.s32 %v19804, %v19809 (stack65)
        %v19814 = vshll.u32 %v19809, 15 (stack73)
        %v19815 = vshrl.u32 %v19809, 17 (stack74)
        %v19816 = vor.u32 %v19814, %v19815 (stack75)
        %v19817 = vxor.u32 %v19812, %v19816 (stack76)
        %v19820 = vadd.s32 %v19812, %v19817 (stack65)
        %v19822 = vshll.u32 %v19817, 26 (stack73)
        %v19823 = vshrl.u32 %v19817, 6 (stack74)
        %v19824 = vor.u32 %v19822, %v19823 (stack75)
        %v19825 = vxor.u32 %v19820, %v19824 (stack76)
        %v19828 = vadd.s32 %v19820, %v19825 (stack65)
        %v19832 = vadd.s32 %v19828, %v10 (stack65)
        %v19834 = vshll.u32 %v19825, 6 (stack73)
        %v19835 = vshrl.u32 %v19825, 26 (stack74)
        %v19836 = vor.u32 %v19834, %v19835 (stack75)
        %v19837 = vxor.u32 %v19828, %v19836 (stack76)
        %v19840 = vadd.s32 %v19837, %v9 (stack65)
        %v19844 = vadd.s32 %v19840, 3 (stack65)
        %v19848 = vadd.s32 %v19832, %v19844 (stack65)
        %v19850 = vshll.u32 %v19844, 17 (stack73)
        %v19851 = vshrl.u32 %v19844, 15 (stack74)
        %v19852 = vor.u32 %v19850, %v19851 (stack75)
        %v19853 = vxor.u32 %v19848, %v19852 (stack76)
        %v19856 = vadd.s32 %v19848, %v19853 (stack65)
        %v19858 = vshll.u32 %v19853, 29 (stack73)
        %v19859 = vshrl.u32 %v19853, 3 (stack74)
        %v19860 = vor.u32 %v19858, %v19859 (stack75)
        %v19861 = vxor.u32 %v19856, %v19860 (stack76)
        %v19864 = vadd.s32 %v19856, %v19861 (stack65)
        %v19866 = vshll.u32 %v19861, 16 (stack73)
        %v19867 = vshrl.u32 %v19861, 16 (stack74)
        %v19868 = vor.u32 %v19866, %v19867 (stack75)
        %v19869 = vxor.u32 %v19864, %v19868 (stack76)
        %v19872 = vadd.s32 %v19864, %v19869 (stack65)
        %v19876 = vadd.s32 %v19872, %v9 (stack65)
        %v19878 = vshll.u32 %v19869, 24 (stack73)
        %v19879 = vshrl.u32 %v19869, 8 (stack74)
        %v19880 = vor.u32 %v19878, %v19879 (stack75)
        %v19881 = vxor.u32 %v19872, %v19880 (stack76)
        %v19884 = vadd.s32 %v19881, %v8 (stack65)
        %v19888 = vadd.s32 %v19884, 4 (stack65)
        %v19892 = vadd.s32 %v19876, %v19888 (stack65)
        %v19894 = vshll.u32 %v19888, 13 (stack73)
        %v19895 = vshrl.u32 %v19888, 19 (stack74)
        %v19896 = vor.u32 %v19894, %v19895 (stack75)
        %v19897 = vxor.u32 %v19892, %v19896 (stack76)
        %v19900 = vadd.s32 %v19892, %v19897 (stack65)
        %v19902 = vshll.u32 %v19897, 15 (stack73)
        %v19903 = vshrl.u32 %v19897, 17 (stack74)
        %v19904 = vor.u32 %v19902, %v19903 (stack75)
        %v19905 = vxor.u32 %v19900, %v19904 (stack76)
        %v19908 = vadd.s32 %v19900, %v19905 (stack65)
        %v19910 = vshll.u32 %v19905, 26 (stack73)
        %v19911 = vshrl.u32 %v19905, 6 (stack74)
        %v19912 = vor.u32 %v19910, %v19911 (stack75)
        %v19913 = vxor.u32 %v19908, %v19912 (stack76)
        %v19916 = vadd.s32 %v19908, %v19913 (stack65)
        %v19920 = vadd.s32 %v19916, %v8 (stack65)
        %v19922 = vshll.u32 %v19913, 6 (stack73)
        %v19923 = vshrl.u32 %v19913, 26 (stack74)
        %v19924 = vor.u32 %v19922, %v19923 (stack75)
        %v19925 = vxor.u32 %v19916, %v19924 (stack76)
        %v19928 = vadd.s32 %v19925, %v10 (stack65)
        %v19932 = vadd.s32 %v19928, 5 (stack65)
        %v19934 = vxor.u32 %v19920, %v19932 (stack76)
        %v19935 = vand.u32.u8 %v19934, 255 (stack77)
        %v19936 = vand.u32 %v19935, 65535 (stack78)
        %v19937 = vshrl.u32 %v19936, 1 (stack79)
        %v19938 = vor.u32 %v19937, 16256 (stack75)
        %v19939 = vand.u32.u16 %v19938, 65535 (stack80)
        %v19940 = vunpack.i.l.bf16 %v19939 (stack81)
        %v19944 = vadd.f32 %v19940, -1.0 (stack82)
        %v19948 = vmul.f32 %v19944, 2.0 (stack83)
        %v19952 = vadd.f32 %v19948, -0.99609375 (stack82)
        %v19956 = vmax.f32 -0.99609375, %v19952 (stack84)
        %v19958 = vand.u32 2147483647, %v19956 (stack85)
        %vm19961 = vcmp.eq.f32.partialorder %v19958, 1.0 (stack86)
        %v19966 = vmul.f32 %v19956, inf (stack83)
        %v19968 = vxor.u32 %v19956, 2147483648 (stack87)
        %v19971 = vmul.f32 %v19956, %v19968 (stack83)
        %v19973 = vadd.f32 %v19971, 1.0 (stack88)
        %v19974 = vlog2.pop %v19973 (stack89)
        %v19975 = vmul.f32 %v19974, 0.6931472 (stack90)
        %v19976 = vmul.f32 -0.5, %v19971 (stack91)
        %v19977 = vadd.f32 %v19976, 1.0 (stack92)
        %v19978 = vmul.f32 %v19977, %v19971 (stack93)
        %v19979 = vand.u32 2147483647, %v19971 (stack94)
        %vm19980 = vcmp.lt.f32.partialorder %v19979, 0.0004427343 (stack95)
        %v19981 = vsel /*vm=*/%vm19980, /*on_true_vy=*/%v19978, /*on_false_vx=*/%v19975 (stack96)
        %v19982 = vxor.u32 %v19981, 2147483648 (stack87)
        %vm19985 = vcmp.lt.f32.partialorder %v19982, 5.0 (stack86)
        %v19990 = vsel /*vm=*/%vm19985, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v19994 = vsel /*vm=*/%vm19985, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v19998 = vsel /*vm=*/%vm19985, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v20002 = vsel /*vm=*/%vm19985, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v20006 = vsel /*vm=*/%vm19985, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v20010 = vsel /*vm=*/%vm19985, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v20014 = vsel /*vm=*/%vm19985, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v20018 = vsel /*vm=*/%vm19985, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v20022 = vsel /*vm=*/%vm19985, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v20026 = vadd.f32 %v19982, -2.5 (stack82)
        %v20028 = vrsqrt.pop %v19982 (stack97)
        %v20029 = vmul.f32 %v19982, %v20028 (stack98)
        %vm20030 = vcmp.eq.f32.partialorder %v19982, inf (stack99)
        %v20031 = vsel /*vm=*/%vm20030, /*on_true_vy=*/%v19982, /*on_false_vx=*/%v20029 (stack100)
        %vm20032 = vcmp.eq.f32.partialorder %v19982, 0.0 (stack101)
        %v20033 = vand.u32 %v19982, 2147483648 (stack102)
        %v20034 = vsel /*vm=*/%vm20032, /*on_true_vy=*/%v20033, /*on_false_vx=*/%v20031 (stack103)
        %v20037 = vadd.f32 %v20034, -3.0 (stack82)
        %v20041 = vsel /*vm=*/%vm19985, /*on_true_vy=*/%v20026, /*on_false_vx=*/%v20037 (stack72)
        %v20045 = vmul.f32 %v20022, %v20041 (stack83)
        %v20049 = vadd.f32 %v20018, %v20045 (stack82)
        %v20053 = vmul.f32 %v20049, %v20041 (stack83)
        %v20057 = vadd.f32 %v20014, %v20053 (stack82)
        %v20061 = vmul.f32 %v20057, %v20041 (stack83)
        %v20065 = vadd.f32 %v20010, %v20061 (stack82)
        %v20069 = vmul.f32 %v20065, %v20041 (stack83)
        %v20073 = vadd.f32 %v20006, %v20069 (stack82)
        %v20077 = vmul.f32 %v20073, %v20041 (stack83)
        %v20081 = vadd.f32 %v20002, %v20077 (stack82)
        %v20085 = vmul.f32 %v20081, %v20041 (stack83)
        %v20089 = vadd.f32 %v19998, %v20085 (stack82)
        %v20093 = vmul.f32 %v20089, %v20041 (stack83)
        %v20097 = vadd.f32 %v19994, %v20093 (stack82)
        %v20101 = vmul.f32 %v20097, %v20041 (stack83)
        %v20105 = vadd.f32 %v19990, %v20101 (stack82)
        %v20109 = vmul.f32 %v20105, %v19956 (stack83)
        %v20113 = vsel /*vm=*/%vm19961, /*on_true_vy=*/%v19966, /*on_false_vx=*/%v20109 (stack72)
        %v20117 = vmul.f32 %v20113, 1.4140625 (stack83)
        %s20119 = scalar_lea.vmem %s280, 148 [#allocation0] (stack107)
        %v20120 = vpack.c.bf16 0.0, %v20117 (stack104)
        %20121 = vst [vmem:[%s20119] sm:$0xf] /*vst_source=*/%v20120 (stack105)
        %v20124 = vadd.s32 %v1381, %v19199 (stack65)
        %s20126 = smul.u32 128, %s27 (stack66)
        %v20127 = vlaneseq (stack67)
        %v20128 = vand.u32 %v20127, 127 (stack68)
        %v20129 = vstv %s20126 (stack69)
        %v20130 = vadd.s32 %v20128, %v20129 (stack70)
        %v20134 = vadd.s32 %v20124, %v20130 (stack65)
        %vm20138 = vcmp.lt.u32.totalorder %v20134, %v20124 (stack71)
        %vm20143 = vcmp.lt.u32.totalorder %v20124, %v1381 (stack71)
        %v20148 = vadd.s32 %v1368, %v19182 (stack65)
        %v20152 = vadd.s32 %v20148, 1 (stack65)
        %v20156 = vsel /*vm=*/%vm20143, /*on_true_vy=*/%v20152, /*on_false_vx=*/%v20148 (stack72)
        %v20160 = vadd.s32 %v20156, 1 (stack65)
        %v20164 = vsel /*vm=*/%vm20138, /*on_true_vy=*/%v20160, /*on_false_vx=*/%v20156 (stack72)
        %v20169 = vadd.s32 %v20164, %v10 (stack65)
        %v20173 = vadd.s32 %v20134, %v9 (stack65)
        %v20177 = vadd.s32 %v20169, %v20173 (stack65)
        %v20179 = vshll.u32 %v20173, 13 (stack73)
        %v20180 = vshrl.u32 %v20173, 19 (stack74)
        %v20181 = vor.u32 %v20179, %v20180 (stack75)
        %v20182 = vxor.u32 %v20177, %v20181 (stack76)
        %v20185 = vadd.s32 %v20177, %v20182 (stack65)
        %v20187 = vshll.u32 %v20182, 15 (stack73)
        %v20188 = vshrl.u32 %v20182, 17 (stack74)
        %v20189 = vor.u32 %v20187, %v20188 (stack75)
        %v20190 = vxor.u32 %v20185, %v20189 (stack76)
        %v20193 = vadd.s32 %v20185, %v20190 (stack65)
        %v20195 = vshll.u32 %v20190, 26 (stack73)
        %v20196 = vshrl.u32 %v20190, 6 (stack74)
        %v20197 = vor.u32 %v20195, %v20196 (stack75)
        %v20198 = vxor.u32 %v20193, %v20197 (stack76)
        %v20201 = vadd.s32 %v20193, %v20198 (stack65)
        %v20205 = vadd.s32 %v20201, %v9 (stack65)
        %v20207 = vshll.u32 %v20198, 6 (stack73)
        %v20208 = vshrl.u32 %v20198, 26 (stack74)
        %v20209 = vor.u32 %v20207, %v20208 (stack75)
        %v20210 = vxor.u32 %v20201, %v20209 (stack76)
        %v20213 = vadd.s32 %v20210, %v8 (stack65)
        %v20217 = vadd.s32 %v20213, 1 (stack65)
        %v20221 = vadd.s32 %v20205, %v20217 (stack65)
        %v20223 = vshll.u32 %v20217, 17 (stack73)
        %v20224 = vshrl.u32 %v20217, 15 (stack74)
        %v20225 = vor.u32 %v20223, %v20224 (stack75)
        %v20226 = vxor.u32 %v20221, %v20225 (stack76)
        %v20229 = vadd.s32 %v20221, %v20226 (stack65)
        %v20231 = vshll.u32 %v20226, 29 (stack73)
        %v20232 = vshrl.u32 %v20226, 3 (stack74)
        %v20233 = vor.u32 %v20231, %v20232 (stack75)
        %v20234 = vxor.u32 %v20229, %v20233 (stack76)
        %v20237 = vadd.s32 %v20229, %v20234 (stack65)
        %v20239 = vshll.u32 %v20234, 16 (stack73)
        %v20240 = vshrl.u32 %v20234, 16 (stack74)
        %v20241 = vor.u32 %v20239, %v20240 (stack75)
        %v20242 = vxor.u32 %v20237, %v20241 (stack76)
        %v20245 = vadd.s32 %v20237, %v20242 (stack65)
        %v20249 = vadd.s32 %v20245, %v8 (stack65)
        %v20251 = vshll.u32 %v20242, 24 (stack73)
        %v20252 = vshrl.u32 %v20242, 8 (stack74)
        %v20253 = vor.u32 %v20251, %v20252 (stack75)
        %v20254 = vxor.u32 %v20245, %v20253 (stack76)
        %v20257 = vadd.s32 %v20254, %v10 (stack65)
        %v20261 = vadd.s32 %v20257, 2 (stack65)
        %v20265 = vadd.s32 %v20249, %v20261 (stack65)
        %v20267 = vshll.u32 %v20261, 13 (stack73)
        %v20268 = vshrl.u32 %v20261, 19 (stack74)
        %v20269 = vor.u32 %v20267, %v20268 (stack75)
        %v20270 = vxor.u32 %v20265, %v20269 (stack76)
        %v20273 = vadd.s32 %v20265, %v20270 (stack65)
        %v20275 = vshll.u32 %v20270, 15 (stack73)
        %v20276 = vshrl.u32 %v20270, 17 (stack74)
        %v20277 = vor.u32 %v20275, %v20276 (stack75)
        %v20278 = vxor.u32 %v20273, %v20277 (stack76)
        %v20281 = vadd.s32 %v20273, %v20278 (stack65)
        %v20283 = vshll.u32 %v20278, 26 (stack73)
        %v20284 = vshrl.u32 %v20278, 6 (stack74)
        %v20285 = vor.u32 %v20283, %v20284 (stack75)
        %v20286 = vxor.u32 %v20281, %v20285 (stack76)
        %v20289 = vadd.s32 %v20281, %v20286 (stack65)
        %v20293 = vadd.s32 %v20289, %v10 (stack65)
        %v20295 = vshll.u32 %v20286, 6 (stack73)
        %v20296 = vshrl.u32 %v20286, 26 (stack74)
        %v20297 = vor.u32 %v20295, %v20296 (stack75)
        %v20298 = vxor.u32 %v20289, %v20297 (stack76)
        %v20301 = vadd.s32 %v20298, %v9 (stack65)
        %v20305 = vadd.s32 %v20301, 3 (stack65)
        %v20309 = vadd.s32 %v20293, %v20305 (stack65)
        %v20311 = vshll.u32 %v20305, 17 (stack73)
        %v20312 = vshrl.u32 %v20305, 15 (stack74)
        %v20313 = vor.u32 %v20311, %v20312 (stack75)
        %v20314 = vxor.u32 %v20309, %v20313 (stack76)
        %v20317 = vadd.s32 %v20309, %v20314 (stack65)
        %v20319 = vshll.u32 %v20314, 29 (stack73)
        %v20320 = vshrl.u32 %v20314, 3 (stack74)
        %v20321 = vor.u32 %v20319, %v20320 (stack75)
        %v20322 = vxor.u32 %v20317, %v20321 (stack76)
        %v20325 = vadd.s32 %v20317, %v20322 (stack65)
        %v20327 = vshll.u32 %v20322, 16 (stack73)
        %v20328 = vshrl.u32 %v20322, 16 (stack74)
        %v20329 = vor.u32 %v20327, %v20328 (stack75)
        %v20330 = vxor.u32 %v20325, %v20329 (stack76)
        %v20333 = vadd.s32 %v20325, %v20330 (stack65)
        %v20337 = vadd.s32 %v20333, %v9 (stack65)
        %v20339 = vshll.u32 %v20330, 24 (stack73)
        %v20340 = vshrl.u32 %v20330, 8 (stack74)
        %v20341 = vor.u32 %v20339, %v20340 (stack75)
        %v20342 = vxor.u32 %v20333, %v20341 (stack76)
        %v20345 = vadd.s32 %v20342, %v8 (stack65)
        %v20349 = vadd.s32 %v20345, 4 (stack65)
        %v20353 = vadd.s32 %v20337, %v20349 (stack65)
        %v20355 = vshll.u32 %v20349, 13 (stack73)
        %v20356 = vshrl.u32 %v20349, 19 (stack74)
        %v20357 = vor.u32 %v20355, %v20356 (stack75)
        %v20358 = vxor.u32 %v20353, %v20357 (stack76)
        %v20361 = vadd.s32 %v20353, %v20358 (stack65)
        %v20363 = vshll.u32 %v20358, 15 (stack73)
        %v20364 = vshrl.u32 %v20358, 17 (stack74)
        %v20365 = vor.u32 %v20363, %v20364 (stack75)
        %v20366 = vxor.u32 %v20361, %v20365 (stack76)
        %v20369 = vadd.s32 %v20361, %v20366 (stack65)
        %v20371 = vshll.u32 %v20366, 26 (stack73)
        %v20372 = vshrl.u32 %v20366, 6 (stack74)
        %v20373 = vor.u32 %v20371, %v20372 (stack75)
        %v20374 = vxor.u32 %v20369, %v20373 (stack76)
        %v20377 = vadd.s32 %v20369, %v20374 (stack65)
        %v20381 = vadd.s32 %v20377, %v8 (stack65)
        %v20383 = vshll.u32 %v20374, 6 (stack73)
        %v20384 = vshrl.u32 %v20374, 26 (stack74)
        %v20385 = vor.u32 %v20383, %v20384 (stack75)
        %v20386 = vxor.u32 %v20377, %v20385 (stack76)
        %v20389 = vadd.s32 %v20386, %v10 (stack65)
        %v20393 = vadd.s32 %v20389, 5 (stack65)
        %v20395 = vxor.u32 %v20381, %v20393 (stack76)
        %v20396 = vand.u32.u8 %v20395, 255 (stack77)
        %v20397 = vand.u32 %v20396, 65535 (stack78)
        %v20398 = vshrl.u32 %v20397, 1 (stack79)
        %v20399 = vor.u32 %v20398, 16256 (stack75)
        %v20400 = vand.u32.u16 %v20399, 65535 (stack80)
        %v20401 = vunpack.i.l.bf16 %v20400 (stack81)
        %v20405 = vadd.f32 %v20401, -1.0 (stack82)
        %v20409 = vmul.f32 %v20405, 2.0 (stack83)
        %v20413 = vadd.f32 %v20409, -0.99609375 (stack82)
        %v20417 = vmax.f32 -0.99609375, %v20413 (stack84)
        %v20419 = vand.u32 2147483647, %v20417 (stack85)
        %vm20422 = vcmp.eq.f32.partialorder %v20419, 1.0 (stack86)
        %v20427 = vmul.f32 %v20417, inf (stack83)
        %v20429 = vxor.u32 %v20417, 2147483648 (stack87)
        %v20432 = vmul.f32 %v20417, %v20429 (stack83)
        %v20434 = vadd.f32 %v20432, 1.0 (stack88)
        %v20435 = vlog2.pop %v20434 (stack89)
        %v20436 = vmul.f32 %v20435, 0.6931472 (stack90)
        %v20437 = vmul.f32 -0.5, %v20432 (stack91)
        %v20438 = vadd.f32 %v20437, 1.0 (stack92)
        %v20439 = vmul.f32 %v20438, %v20432 (stack93)
        %v20440 = vand.u32 2147483647, %v20432 (stack94)
        %vm20441 = vcmp.lt.f32.partialorder %v20440, 0.0004427343 (stack95)
        %v20442 = vsel /*vm=*/%vm20441, /*on_true_vy=*/%v20439, /*on_false_vx=*/%v20436 (stack96)
        %v20443 = vxor.u32 %v20442, 2147483648 (stack87)
        %vm20446 = vcmp.lt.f32.partialorder %v20443, 5.0 (stack86)
        %v20451 = vsel /*vm=*/%vm20446, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v20455 = vsel /*vm=*/%vm20446, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v20459 = vsel /*vm=*/%vm20446, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v20463 = vsel /*vm=*/%vm20446, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v20467 = vsel /*vm=*/%vm20446, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v20471 = vsel /*vm=*/%vm20446, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v20475 = vsel /*vm=*/%vm20446, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v20479 = vsel /*vm=*/%vm20446, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v20483 = vsel /*vm=*/%vm20446, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v20487 = vadd.f32 %v20443, -2.5 (stack82)
        %v20489 = vrsqrt.pop %v20443 (stack97)
        %v20490 = vmul.f32 %v20443, %v20489 (stack98)
        %vm20491 = vcmp.eq.f32.partialorder %v20443, inf (stack99)
        %v20492 = vsel /*vm=*/%vm20491, /*on_true_vy=*/%v20443, /*on_false_vx=*/%v20490 (stack100)
        %vm20493 = vcmp.eq.f32.partialorder %v20443, 0.0 (stack101)
        %v20494 = vand.u32 %v20443, 2147483648 (stack102)
        %v20495 = vsel /*vm=*/%vm20493, /*on_true_vy=*/%v20494, /*on_false_vx=*/%v20492 (stack103)
        %v20498 = vadd.f32 %v20495, -3.0 (stack82)
        %v20502 = vsel /*vm=*/%vm20446, /*on_true_vy=*/%v20487, /*on_false_vx=*/%v20498 (stack72)
        %v20506 = vmul.f32 %v20483, %v20502 (stack83)
        %v20510 = vadd.f32 %v20479, %v20506 (stack82)
        %v20514 = vmul.f32 %v20510, %v20502 (stack83)
        %v20518 = vadd.f32 %v20475, %v20514 (stack82)
        %v20522 = vmul.f32 %v20518, %v20502 (stack83)
        %v20526 = vadd.f32 %v20471, %v20522 (stack82)
        %v20530 = vmul.f32 %v20526, %v20502 (stack83)
        %v20534 = vadd.f32 %v20467, %v20530 (stack82)
        %v20538 = vmul.f32 %v20534, %v20502 (stack83)
        %v20542 = vadd.f32 %v20463, %v20538 (stack82)
        %v20546 = vmul.f32 %v20542, %v20502 (stack83)
        %v20550 = vadd.f32 %v20459, %v20546 (stack82)
        %v20554 = vmul.f32 %v20550, %v20502 (stack83)
        %v20558 = vadd.f32 %v20455, %v20554 (stack82)
        %v20562 = vmul.f32 %v20558, %v20502 (stack83)
        %v20566 = vadd.f32 %v20451, %v20562 (stack82)
        %v20570 = vmul.f32 %v20566, %v20417 (stack83)
        %v20574 = vsel /*vm=*/%vm20422, /*on_true_vy=*/%v20427, /*on_false_vx=*/%v20570 (stack72)
        %v20578 = vmul.f32 %v20574, 1.4140625 (stack83)
        %s20580 = scalar_lea.vmem %s280, 276 [#allocation0] (stack107)
        %v20581 = vpack.c.bf16 0.0, %v20578 (stack104)
        %20582 = vst [vmem:[%s20580] sm:$0xf] /*vst_source=*/%v20581 (stack105)
        %v20585 = vadd.s32 %v1868, %v19199 (stack65)
        %s20587 = smul.u32 128, %s27 (stack66)
        %v20588 = vlaneseq (stack67)
        %v20589 = vand.u32 %v20588, 127 (stack68)
        %v20590 = vstv %s20587 (stack69)
        %v20591 = vadd.s32 %v20589, %v20590 (stack70)
        %v20595 = vadd.s32 %v20585, %v20591 (stack65)
        %vm20599 = vcmp.lt.u32.totalorder %v20595, %v20585 (stack71)
        %vm20604 = vcmp.lt.u32.totalorder %v20585, %v1868 (stack71)
        %v20609 = vadd.s32 %v1855, %v19182 (stack65)
        %v20613 = vadd.s32 %v20609, 1 (stack65)
        %v20617 = vsel /*vm=*/%vm20604, /*on_true_vy=*/%v20613, /*on_false_vx=*/%v20609 (stack72)
        %v20621 = vadd.s32 %v20617, 1 (stack65)
        %v20625 = vsel /*vm=*/%vm20599, /*on_true_vy=*/%v20621, /*on_false_vx=*/%v20617 (stack72)
        %v20630 = vadd.s32 %v20625, %v10 (stack65)
        %v20634 = vadd.s32 %v20595, %v9 (stack65)
        %v20638 = vadd.s32 %v20630, %v20634 (stack65)
        %v20640 = vshll.u32 %v20634, 13 (stack73)
        %v20641 = vshrl.u32 %v20634, 19 (stack74)
        %v20642 = vor.u32 %v20640, %v20641 (stack75)
        %v20643 = vxor.u32 %v20638, %v20642 (stack76)
        %v20646 = vadd.s32 %v20638, %v20643 (stack65)
        %v20648 = vshll.u32 %v20643, 15 (stack73)
        %v20649 = vshrl.u32 %v20643, 17 (stack74)
        %v20650 = vor.u32 %v20648, %v20649 (stack75)
        %v20651 = vxor.u32 %v20646, %v20650 (stack76)
        %v20654 = vadd.s32 %v20646, %v20651 (stack65)
        %v20656 = vshll.u32 %v20651, 26 (stack73)
        %v20657 = vshrl.u32 %v20651, 6 (stack74)
        %v20658 = vor.u32 %v20656, %v20657 (stack75)
        %v20659 = vxor.u32 %v20654, %v20658 (stack76)
        %v20662 = vadd.s32 %v20654, %v20659 (stack65)
        %v20666 = vadd.s32 %v20662, %v9 (stack65)
        %v20668 = vshll.u32 %v20659, 6 (stack73)
        %v20669 = vshrl.u32 %v20659, 26 (stack74)
        %v20670 = vor.u32 %v20668, %v20669 (stack75)
        %v20671 = vxor.u32 %v20662, %v20670 (stack76)
        %v20674 = vadd.s32 %v20671, %v8 (stack65)
        %v20678 = vadd.s32 %v20674, 1 (stack65)
        %v20682 = vadd.s32 %v20666, %v20678 (stack65)
        %v20684 = vshll.u32 %v20678, 17 (stack73)
        %v20685 = vshrl.u32 %v20678, 15 (stack74)
        %v20686 = vor.u32 %v20684, %v20685 (stack75)
        %v20687 = vxor.u32 %v20682, %v20686 (stack76)
        %v20690 = vadd.s32 %v20682, %v20687 (stack65)
        %v20692 = vshll.u32 %v20687, 29 (stack73)
        %v20693 = vshrl.u32 %v20687, 3 (stack74)
        %v20694 = vor.u32 %v20692, %v20693 (stack75)
        %v20695 = vxor.u32 %v20690, %v20694 (stack76)
        %v20698 = vadd.s32 %v20690, %v20695 (stack65)
        %v20700 = vshll.u32 %v20695, 16 (stack73)
        %v20701 = vshrl.u32 %v20695, 16 (stack74)
        %v20702 = vor.u32 %v20700, %v20701 (stack75)
        %v20703 = vxor.u32 %v20698, %v20702 (stack76)
        %v20706 = vadd.s32 %v20698, %v20703 (stack65)
        %v20710 = vadd.s32 %v20706, %v8 (stack65)
        %v20712 = vshll.u32 %v20703, 24 (stack73)
        %v20713 = vshrl.u32 %v20703, 8 (stack74)
        %v20714 = vor.u32 %v20712, %v20713 (stack75)
        %v20715 = vxor.u32 %v20706, %v20714 (stack76)
        %v20718 = vadd.s32 %v20715, %v10 (stack65)
        %v20722 = vadd.s32 %v20718, 2 (stack65)
        %v20726 = vadd.s32 %v20710, %v20722 (stack65)
        %v20728 = vshll.u32 %v20722, 13 (stack73)
        %v20729 = vshrl.u32 %v20722, 19 (stack74)
        %v20730 = vor.u32 %v20728, %v20729 (stack75)
        %v20731 = vxor.u32 %v20726, %v20730 (stack76)
        %v20734 = vadd.s32 %v20726, %v20731 (stack65)
        %v20736 = vshll.u32 %v20731, 15 (stack73)
        %v20737 = vshrl.u32 %v20731, 17 (stack74)
        %v20738 = vor.u32 %v20736, %v20737 (stack75)
        %v20739 = vxor.u32 %v20734, %v20738 (stack76)
        %v20742 = vadd.s32 %v20734, %v20739 (stack65)
        %v20744 = vshll.u32 %v20739, 26 (stack73)
        %v20745 = vshrl.u32 %v20739, 6 (stack74)
        %v20746 = vor.u32 %v20744, %v20745 (stack75)
        %v20747 = vxor.u32 %v20742, %v20746 (stack76)
        %v20750 = vadd.s32 %v20742, %v20747 (stack65)
        %v20754 = vadd.s32 %v20750, %v10 (stack65)
        %v20756 = vshll.u32 %v20747, 6 (stack73)
        %v20757 = vshrl.u32 %v20747, 26 (stack74)
        %v20758 = vor.u32 %v20756, %v20757 (stack75)
        %v20759 = vxor.u32 %v20750, %v20758 (stack76)
        %v20762 = vadd.s32 %v20759, %v9 (stack65)
        %v20766 = vadd.s32 %v20762, 3 (stack65)
        %v20770 = vadd.s32 %v20754, %v20766 (stack65)
        %v20772 = vshll.u32 %v20766, 17 (stack73)
        %v20773 = vshrl.u32 %v20766, 15 (stack74)
        %v20774 = vor.u32 %v20772, %v20773 (stack75)
        %v20775 = vxor.u32 %v20770, %v20774 (stack76)
        %v20778 = vadd.s32 %v20770, %v20775 (stack65)
        %v20780 = vshll.u32 %v20775, 29 (stack73)
        %v20781 = vshrl.u32 %v20775, 3 (stack74)
        %v20782 = vor.u32 %v20780, %v20781 (stack75)
        %v20783 = vxor.u32 %v20778, %v20782 (stack76)
        %v20786 = vadd.s32 %v20778, %v20783 (stack65)
        %v20788 = vshll.u32 %v20783, 16 (stack73)
        %v20789 = vshrl.u32 %v20783, 16 (stack74)
        %v20790 = vor.u32 %v20788, %v20789 (stack75)
        %v20791 = vxor.u32 %v20786, %v20790 (stack76)
        %v20794 = vadd.s32 %v20786, %v20791 (stack65)
        %v20798 = vadd.s32 %v20794, %v9 (stack65)
        %v20800 = vshll.u32 %v20791, 24 (stack73)
        %v20801 = vshrl.u32 %v20791, 8 (stack74)
        %v20802 = vor.u32 %v20800, %v20801 (stack75)
        %v20803 = vxor.u32 %v20794, %v20802 (stack76)
        %v20806 = vadd.s32 %v20803, %v8 (stack65)
        %v20810 = vadd.s32 %v20806, 4 (stack65)
        %v20814 = vadd.s32 %v20798, %v20810 (stack65)
        %v20816 = vshll.u32 %v20810, 13 (stack73)
        %v20817 = vshrl.u32 %v20810, 19 (stack74)
        %v20818 = vor.u32 %v20816, %v20817 (stack75)
        %v20819 = vxor.u32 %v20814, %v20818 (stack76)
        %v20822 = vadd.s32 %v20814, %v20819 (stack65)
        %v20824 = vshll.u32 %v20819, 15 (stack73)
        %v20825 = vshrl.u32 %v20819, 17 (stack74)
        %v20826 = vor.u32 %v20824, %v20825 (stack75)
        %v20827 = vxor.u32 %v20822, %v20826 (stack76)
        %v20830 = vadd.s32 %v20822, %v20827 (stack65)
        %v20832 = vshll.u32 %v20827, 26 (stack73)
        %v20833 = vshrl.u32 %v20827, 6 (stack74)
        %v20834 = vor.u32 %v20832, %v20833 (stack75)
        %v20835 = vxor.u32 %v20830, %v20834 (stack76)
        %v20838 = vadd.s32 %v20830, %v20835 (stack65)
        %v20842 = vadd.s32 %v20838, %v8 (stack65)
        %v20844 = vshll.u32 %v20835, 6 (stack73)
        %v20845 = vshrl.u32 %v20835, 26 (stack74)
        %v20846 = vor.u32 %v20844, %v20845 (stack75)
        %v20847 = vxor.u32 %v20838, %v20846 (stack76)
        %v20850 = vadd.s32 %v20847, %v10 (stack65)
        %v20854 = vadd.s32 %v20850, 5 (stack65)
        %v20856 = vxor.u32 %v20842, %v20854 (stack76)
        %v20857 = vand.u32.u8 %v20856, 255 (stack77)
        %v20858 = vand.u32 %v20857, 65535 (stack78)
        %v20859 = vshrl.u32 %v20858, 1 (stack79)
        %v20860 = vor.u32 %v20859, 16256 (stack75)
        %v20861 = vand.u32.u16 %v20860, 65535 (stack80)
        %v20862 = vunpack.i.l.bf16 %v20861 (stack81)
        %v20866 = vadd.f32 %v20862, -1.0 (stack82)
        %v20870 = vmul.f32 %v20866, 2.0 (stack83)
        %v20874 = vadd.f32 %v20870, -0.99609375 (stack82)
        %v20878 = vmax.f32 -0.99609375, %v20874 (stack84)
        %v20880 = vand.u32 2147483647, %v20878 (stack85)
        %vm20883 = vcmp.eq.f32.partialorder %v20880, 1.0 (stack86)
        %v20888 = vmul.f32 %v20878, inf (stack83)
        %v20890 = vxor.u32 %v20878, 2147483648 (stack87)
        %v20893 = vmul.f32 %v20878, %v20890 (stack83)
        %v20895 = vadd.f32 %v20893, 1.0 (stack88)
        %v20896 = vlog2.pop %v20895 (stack89)
        %v20897 = vmul.f32 %v20896, 0.6931472 (stack90)
        %v20898 = vmul.f32 -0.5, %v20893 (stack91)
        %v20899 = vadd.f32 %v20898, 1.0 (stack92)
        %v20900 = vmul.f32 %v20899, %v20893 (stack93)
        %v20901 = vand.u32 2147483647, %v20893 (stack94)
        %vm20902 = vcmp.lt.f32.partialorder %v20901, 0.0004427343 (stack95)
        %v20903 = vsel /*vm=*/%vm20902, /*on_true_vy=*/%v20900, /*on_false_vx=*/%v20897 (stack96)
        %v20904 = vxor.u32 %v20903, 2147483648 (stack87)
        %vm20907 = vcmp.lt.f32.partialorder %v20904, 5.0 (stack86)
        %v20912 = vsel /*vm=*/%vm20907, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v20916 = vsel /*vm=*/%vm20907, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v20920 = vsel /*vm=*/%vm20907, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v20924 = vsel /*vm=*/%vm20907, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v20928 = vsel /*vm=*/%vm20907, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v20932 = vsel /*vm=*/%vm20907, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v20936 = vsel /*vm=*/%vm20907, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v20940 = vsel /*vm=*/%vm20907, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v20944 = vsel /*vm=*/%vm20907, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v20948 = vadd.f32 %v20904, -2.5 (stack82)
        %v20950 = vrsqrt.pop %v20904 (stack97)
        %v20951 = vmul.f32 %v20904, %v20950 (stack98)
        %vm20952 = vcmp.eq.f32.partialorder %v20904, inf (stack99)
        %v20953 = vsel /*vm=*/%vm20952, /*on_true_vy=*/%v20904, /*on_false_vx=*/%v20951 (stack100)
        %vm20954 = vcmp.eq.f32.partialorder %v20904, 0.0 (stack101)
        %v20955 = vand.u32 %v20904, 2147483648 (stack102)
        %v20956 = vsel /*vm=*/%vm20954, /*on_true_vy=*/%v20955, /*on_false_vx=*/%v20953 (stack103)
        %v20959 = vadd.f32 %v20956, -3.0 (stack82)
        %v20963 = vsel /*vm=*/%vm20907, /*on_true_vy=*/%v20948, /*on_false_vx=*/%v20959 (stack72)
        %v20967 = vmul.f32 %v20944, %v20963 (stack83)
        %v20971 = vadd.f32 %v20940, %v20967 (stack82)
        %v20975 = vmul.f32 %v20971, %v20963 (stack83)
        %v20979 = vadd.f32 %v20936, %v20975 (stack82)
        %v20983 = vmul.f32 %v20979, %v20963 (stack83)
        %v20987 = vadd.f32 %v20932, %v20983 (stack82)
        %v20991 = vmul.f32 %v20987, %v20963 (stack83)
        %v20995 = vadd.f32 %v20928, %v20991 (stack82)
        %v20999 = vmul.f32 %v20995, %v20963 (stack83)
        %v21003 = vadd.f32 %v20924, %v20999 (stack82)
        %v21007 = vmul.f32 %v21003, %v20963 (stack83)
        %v21011 = vadd.f32 %v20920, %v21007 (stack82)
        %v21015 = vmul.f32 %v21011, %v20963 (stack83)
        %v21019 = vadd.f32 %v20916, %v21015 (stack82)
        %v21023 = vmul.f32 %v21019, %v20963 (stack83)
        %v21027 = vadd.f32 %v20912, %v21023 (stack82)
        %v21031 = vmul.f32 %v21027, %v20878 (stack83)
        %v21035 = vsel /*vm=*/%vm20883, /*on_true_vy=*/%v20888, /*on_false_vx=*/%v21031 (stack72)
        %v21039 = vmul.f32 %v21035, 1.4140625 (stack83)
        %s21041 = scalar_lea.vmem %s280, 404 [#allocation0] (stack107)
        %v21042 = vpack.c.bf16 0.0, %v21039 (stack104)
        %21043 = vst [vmem:[%s21041] sm:$0xf] /*vst_source=*/%v21042 (stack105)
        %v21046 = vadd.s32 %v2355, %v19199 (stack65)
        %s21048 = smul.u32 128, %s27 (stack66)
        %v21049 = vlaneseq (stack67)
        %v21050 = vand.u32 %v21049, 127 (stack68)
        %v21051 = vstv %s21048 (stack69)
        %v21052 = vadd.s32 %v21050, %v21051 (stack70)
        %v21056 = vadd.s32 %v21046, %v21052 (stack65)
        %vm21060 = vcmp.lt.u32.totalorder %v21056, %v21046 (stack71)
        %vm21065 = vcmp.lt.u32.totalorder %v21046, %v2355 (stack71)
        %v21070 = vadd.s32 %v2342, %v19182 (stack65)
        %v21074 = vadd.s32 %v21070, 1 (stack65)
        %v21078 = vsel /*vm=*/%vm21065, /*on_true_vy=*/%v21074, /*on_false_vx=*/%v21070 (stack72)
        %v21082 = vadd.s32 %v21078, 1 (stack65)
        %v21086 = vsel /*vm=*/%vm21060, /*on_true_vy=*/%v21082, /*on_false_vx=*/%v21078 (stack72)
        %v21091 = vadd.s32 %v21086, %v10 (stack65)
        %v21095 = vadd.s32 %v21056, %v9 (stack65)
        %v21099 = vadd.s32 %v21091, %v21095 (stack65)
        %v21101 = vshll.u32 %v21095, 13 (stack73)
        %v21102 = vshrl.u32 %v21095, 19 (stack74)
        %v21103 = vor.u32 %v21101, %v21102 (stack75)
        %v21104 = vxor.u32 %v21099, %v21103 (stack76)
        %v21107 = vadd.s32 %v21099, %v21104 (stack65)
        %v21109 = vshll.u32 %v21104, 15 (stack73)
        %v21110 = vshrl.u32 %v21104, 17 (stack74)
        %v21111 = vor.u32 %v21109, %v21110 (stack75)
        %v21112 = vxor.u32 %v21107, %v21111 (stack76)
        %v21115 = vadd.s32 %v21107, %v21112 (stack65)
        %v21117 = vshll.u32 %v21112, 26 (stack73)
        %v21118 = vshrl.u32 %v21112, 6 (stack74)
        %v21119 = vor.u32 %v21117, %v21118 (stack75)
        %v21120 = vxor.u32 %v21115, %v21119 (stack76)
        %v21123 = vadd.s32 %v21115, %v21120 (stack65)
        %v21127 = vadd.s32 %v21123, %v9 (stack65)
        %v21129 = vshll.u32 %v21120, 6 (stack73)
        %v21130 = vshrl.u32 %v21120, 26 (stack74)
        %v21131 = vor.u32 %v21129, %v21130 (stack75)
        %v21132 = vxor.u32 %v21123, %v21131 (stack76)
        %v21135 = vadd.s32 %v21132, %v8 (stack65)
        %v21139 = vadd.s32 %v21135, 1 (stack65)
        %v21143 = vadd.s32 %v21127, %v21139 (stack65)
        %v21145 = vshll.u32 %v21139, 17 (stack73)
        %v21146 = vshrl.u32 %v21139, 15 (stack74)
        %v21147 = vor.u32 %v21145, %v21146 (stack75)
        %v21148 = vxor.u32 %v21143, %v21147 (stack76)
        %v21151 = vadd.s32 %v21143, %v21148 (stack65)
        %v21153 = vshll.u32 %v21148, 29 (stack73)
        %v21154 = vshrl.u32 %v21148, 3 (stack74)
        %v21155 = vor.u32 %v21153, %v21154 (stack75)
        %v21156 = vxor.u32 %v21151, %v21155 (stack76)
        %v21159 = vadd.s32 %v21151, %v21156 (stack65)
        %v21161 = vshll.u32 %v21156, 16 (stack73)
        %v21162 = vshrl.u32 %v21156, 16 (stack74)
        %v21163 = vor.u32 %v21161, %v21162 (stack75)
        %v21164 = vxor.u32 %v21159, %v21163 (stack76)
        %v21167 = vadd.s32 %v21159, %v21164 (stack65)
        %v21171 = vadd.s32 %v21167, %v8 (stack65)
        %v21173 = vshll.u32 %v21164, 24 (stack73)
        %v21174 = vshrl.u32 %v21164, 8 (stack74)
        %v21175 = vor.u32 %v21173, %v21174 (stack75)
        %v21176 = vxor.u32 %v21167, %v21175 (stack76)
        %v21179 = vadd.s32 %v21176, %v10 (stack65)
        %v21183 = vadd.s32 %v21179, 2 (stack65)
        %v21187 = vadd.s32 %v21171, %v21183 (stack65)
        %v21189 = vshll.u32 %v21183, 13 (stack73)
        %v21190 = vshrl.u32 %v21183, 19 (stack74)
        %v21191 = vor.u32 %v21189, %v21190 (stack75)
        %v21192 = vxor.u32 %v21187, %v21191 (stack76)
        %v21195 = vadd.s32 %v21187, %v21192 (stack65)
        %v21197 = vshll.u32 %v21192, 15 (stack73)
        %v21198 = vshrl.u32 %v21192, 17 (stack74)
        %v21199 = vor.u32 %v21197, %v21198 (stack75)
        %v21200 = vxor.u32 %v21195, %v21199 (stack76)
        %v21203 = vadd.s32 %v21195, %v21200 (stack65)
        %v21205 = vshll.u32 %v21200, 26 (stack73)
        %v21206 = vshrl.u32 %v21200, 6 (stack74)
        %v21207 = vor.u32 %v21205, %v21206 (stack75)
        %v21208 = vxor.u32 %v21203, %v21207 (stack76)
        %v21211 = vadd.s32 %v21203, %v21208 (stack65)
        %v21215 = vadd.s32 %v21211, %v10 (stack65)
        %v21217 = vshll.u32 %v21208, 6 (stack73)
        %v21218 = vshrl.u32 %v21208, 26 (stack74)
        %v21219 = vor.u32 %v21217, %v21218 (stack75)
        %v21220 = vxor.u32 %v21211, %v21219 (stack76)
        %v21223 = vadd.s32 %v21220, %v9 (stack65)
        %v21227 = vadd.s32 %v21223, 3 (stack65)
        %v21231 = vadd.s32 %v21215, %v21227 (stack65)
        %v21233 = vshll.u32 %v21227, 17 (stack73)
        %v21234 = vshrl.u32 %v21227, 15 (stack74)
        %v21235 = vor.u32 %v21233, %v21234 (stack75)
        %v21236 = vxor.u32 %v21231, %v21235 (stack76)
        %v21239 = vadd.s32 %v21231, %v21236 (stack65)
        %v21241 = vshll.u32 %v21236, 29 (stack73)
        %v21242 = vshrl.u32 %v21236, 3 (stack74)
        %v21243 = vor.u32 %v21241, %v21242 (stack75)
        %v21244 = vxor.u32 %v21239, %v21243 (stack76)
        %v21247 = vadd.s32 %v21239, %v21244 (stack65)
        %v21249 = vshll.u32 %v21244, 16 (stack73)
        %v21250 = vshrl.u32 %v21244, 16 (stack74)
        %v21251 = vor.u32 %v21249, %v21250 (stack75)
        %v21252 = vxor.u32 %v21247, %v21251 (stack76)
        %v21255 = vadd.s32 %v21247, %v21252 (stack65)
        %v21259 = vadd.s32 %v21255, %v9 (stack65)
        %v21261 = vshll.u32 %v21252, 24 (stack73)
        %v21262 = vshrl.u32 %v21252, 8 (stack74)
        %v21263 = vor.u32 %v21261, %v21262 (stack75)
        %v21264 = vxor.u32 %v21255, %v21263 (stack76)
        %v21267 = vadd.s32 %v21264, %v8 (stack65)
        %v21271 = vadd.s32 %v21267, 4 (stack65)
        %v21275 = vadd.s32 %v21259, %v21271 (stack65)
        %v21277 = vshll.u32 %v21271, 13 (stack73)
        %v21278 = vshrl.u32 %v21271, 19 (stack74)
        %v21279 = vor.u32 %v21277, %v21278 (stack75)
        %v21280 = vxor.u32 %v21275, %v21279 (stack76)
        %v21283 = vadd.s32 %v21275, %v21280 (stack65)
        %v21285 = vshll.u32 %v21280, 15 (stack73)
        %v21286 = vshrl.u32 %v21280, 17 (stack74)
        %v21287 = vor.u32 %v21285, %v21286 (stack75)
        %v21288 = vxor.u32 %v21283, %v21287 (stack76)
        %v21291 = vadd.s32 %v21283, %v21288 (stack65)
        %v21293 = vshll.u32 %v21288, 26 (stack73)
        %v21294 = vshrl.u32 %v21288, 6 (stack74)
        %v21295 = vor.u32 %v21293, %v21294 (stack75)
        %v21296 = vxor.u32 %v21291, %v21295 (stack76)
        %v21299 = vadd.s32 %v21291, %v21296 (stack65)
        %v21303 = vadd.s32 %v21299, %v8 (stack65)
        %v21305 = vshll.u32 %v21296, 6 (stack73)
        %v21306 = vshrl.u32 %v21296, 26 (stack74)
        %v21307 = vor.u32 %v21305, %v21306 (stack75)
        %v21308 = vxor.u32 %v21299, %v21307 (stack76)
        %v21311 = vadd.s32 %v21308, %v10 (stack65)
        %v21315 = vadd.s32 %v21311, 5 (stack65)
        %v21317 = vxor.u32 %v21303, %v21315 (stack76)
        %v21318 = vand.u32.u8 %v21317, 255 (stack77)
        %v21319 = vand.u32 %v21318, 65535 (stack78)
        %v21320 = vshrl.u32 %v21319, 1 (stack79)
        %v21321 = vor.u32 %v21320, 16256 (stack75)
        %v21322 = vand.u32.u16 %v21321, 65535 (stack80)
        %v21323 = vunpack.i.l.bf16 %v21322 (stack81)
        %v21327 = vadd.f32 %v21323, -1.0 (stack82)
        %v21331 = vmul.f32 %v21327, 2.0 (stack83)
        %v21335 = vadd.f32 %v21331, -0.99609375 (stack82)
        %v21339 = vmax.f32 -0.99609375, %v21335 (stack84)
        %v21341 = vand.u32 2147483647, %v21339 (stack85)
        %vm21344 = vcmp.eq.f32.partialorder %v21341, 1.0 (stack86)
        %v21349 = vmul.f32 %v21339, inf (stack83)
        %v21351 = vxor.u32 %v21339, 2147483648 (stack87)
        %v21354 = vmul.f32 %v21339, %v21351 (stack83)
        %v21356 = vadd.f32 %v21354, 1.0 (stack88)
        %v21357 = vlog2.pop %v21356 (stack89)
        %v21358 = vmul.f32 %v21357, 0.6931472 (stack90)
        %v21359 = vmul.f32 -0.5, %v21354 (stack91)
        %v21360 = vadd.f32 %v21359, 1.0 (stack92)
        %v21361 = vmul.f32 %v21360, %v21354 (stack93)
        %v21362 = vand.u32 2147483647, %v21354 (stack94)
        %vm21363 = vcmp.lt.f32.partialorder %v21362, 0.0004427343 (stack95)
        %v21364 = vsel /*vm=*/%vm21363, /*on_true_vy=*/%v21361, /*on_false_vx=*/%v21358 (stack96)
        %v21365 = vxor.u32 %v21364, 2147483648 (stack87)
        %vm21368 = vcmp.lt.f32.partialorder %v21365, 5.0 (stack86)
        %v21373 = vsel /*vm=*/%vm21368, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v21377 = vsel /*vm=*/%vm21368, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v21381 = vsel /*vm=*/%vm21368, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v21385 = vsel /*vm=*/%vm21368, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v21389 = vsel /*vm=*/%vm21368, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v21393 = vsel /*vm=*/%vm21368, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v21397 = vsel /*vm=*/%vm21368, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v21401 = vsel /*vm=*/%vm21368, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v21405 = vsel /*vm=*/%vm21368, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v21409 = vadd.f32 %v21365, -2.5 (stack82)
        %v21411 = vrsqrt.pop %v21365 (stack97)
        %v21412 = vmul.f32 %v21365, %v21411 (stack98)
        %vm21413 = vcmp.eq.f32.partialorder %v21365, inf (stack99)
        %v21414 = vsel /*vm=*/%vm21413, /*on_true_vy=*/%v21365, /*on_false_vx=*/%v21412 (stack100)
        %vm21415 = vcmp.eq.f32.partialorder %v21365, 0.0 (stack101)
        %v21416 = vand.u32 %v21365, 2147483648 (stack102)
        %v21417 = vsel /*vm=*/%vm21415, /*on_true_vy=*/%v21416, /*on_false_vx=*/%v21414 (stack103)
        %v21420 = vadd.f32 %v21417, -3.0 (stack82)
        %v21424 = vsel /*vm=*/%vm21368, /*on_true_vy=*/%v21409, /*on_false_vx=*/%v21420 (stack72)
        %v21428 = vmul.f32 %v21405, %v21424 (stack83)
        %v21432 = vadd.f32 %v21401, %v21428 (stack82)
        %v21436 = vmul.f32 %v21432, %v21424 (stack83)
        %v21440 = vadd.f32 %v21397, %v21436 (stack82)
        %v21444 = vmul.f32 %v21440, %v21424 (stack83)
        %v21448 = vadd.f32 %v21393, %v21444 (stack82)
        %v21452 = vmul.f32 %v21448, %v21424 (stack83)
        %v21456 = vadd.f32 %v21389, %v21452 (stack82)
        %v21460 = vmul.f32 %v21456, %v21424 (stack83)
        %v21464 = vadd.f32 %v21385, %v21460 (stack82)
        %v21468 = vmul.f32 %v21464, %v21424 (stack83)
        %v21472 = vadd.f32 %v21381, %v21468 (stack82)
        %v21476 = vmul.f32 %v21472, %v21424 (stack83)
        %v21480 = vadd.f32 %v21377, %v21476 (stack82)
        %v21484 = vmul.f32 %v21480, %v21424 (stack83)
        %v21488 = vadd.f32 %v21373, %v21484 (stack82)
        %v21492 = vmul.f32 %v21488, %v21339 (stack83)
        %v21496 = vsel /*vm=*/%vm21344, /*on_true_vy=*/%v21349, /*on_false_vx=*/%v21492 (stack72)
        %v21500 = vmul.f32 %v21496, 1.4140625 (stack83)
        %s21502 = scalar_lea.vmem %s280, 532 [#allocation0] (stack107)
        %v21503 = vpack.c.bf16 0.0, %v21500 (stack104)
        %21504 = vst [vmem:[%s21502] sm:$0xf] /*vst_source=*/%v21503 (stack105)
        %v21507 = vadd.s32 %v2842, %v19199 (stack65)
        %s21509 = smul.u32 128, %s27 (stack66)
        %v21510 = vlaneseq (stack67)
        %v21511 = vand.u32 %v21510, 127 (stack68)
        %v21512 = vstv %s21509 (stack69)
        %v21513 = vadd.s32 %v21511, %v21512 (stack70)
        %v21517 = vadd.s32 %v21507, %v21513 (stack65)
        %vm21521 = vcmp.lt.u32.totalorder %v21517, %v21507 (stack71)
        %vm21526 = vcmp.lt.u32.totalorder %v21507, %v2842 (stack71)
        %v21531 = vadd.s32 %v2829, %v19182 (stack65)
        %v21535 = vadd.s32 %v21531, 1 (stack65)
        %v21539 = vsel /*vm=*/%vm21526, /*on_true_vy=*/%v21535, /*on_false_vx=*/%v21531 (stack72)
        %v21543 = vadd.s32 %v21539, 1 (stack65)
        %v21547 = vsel /*vm=*/%vm21521, /*on_true_vy=*/%v21543, /*on_false_vx=*/%v21539 (stack72)
        %v21552 = vadd.s32 %v21547, %v10 (stack65)
        %v21556 = vadd.s32 %v21517, %v9 (stack65)
        %v21560 = vadd.s32 %v21552, %v21556 (stack65)
        %v21562 = vshll.u32 %v21556, 13 (stack73)
        %v21563 = vshrl.u32 %v21556, 19 (stack74)
        %v21564 = vor.u32 %v21562, %v21563 (stack75)
        %v21565 = vxor.u32 %v21560, %v21564 (stack76)
        %v21568 = vadd.s32 %v21560, %v21565 (stack65)
        %v21570 = vshll.u32 %v21565, 15 (stack73)
        %v21571 = vshrl.u32 %v21565, 17 (stack74)
        %v21572 = vor.u32 %v21570, %v21571 (stack75)
        %v21573 = vxor.u32 %v21568, %v21572 (stack76)
        %v21576 = vadd.s32 %v21568, %v21573 (stack65)
        %v21578 = vshll.u32 %v21573, 26 (stack73)
        %v21579 = vshrl.u32 %v21573, 6 (stack74)
        %v21580 = vor.u32 %v21578, %v21579 (stack75)
        %v21581 = vxor.u32 %v21576, %v21580 (stack76)
        %v21584 = vadd.s32 %v21576, %v21581 (stack65)
        %v21588 = vadd.s32 %v21584, %v9 (stack65)
        %v21590 = vshll.u32 %v21581, 6 (stack73)
        %v21591 = vshrl.u32 %v21581, 26 (stack74)
        %v21592 = vor.u32 %v21590, %v21591 (stack75)
        %v21593 = vxor.u32 %v21584, %v21592 (stack76)
        %v21596 = vadd.s32 %v21593, %v8 (stack65)
        %v21600 = vadd.s32 %v21596, 1 (stack65)
        %v21604 = vadd.s32 %v21588, %v21600 (stack65)
        %v21606 = vshll.u32 %v21600, 17 (stack73)
        %v21607 = vshrl.u32 %v21600, 15 (stack74)
        %v21608 = vor.u32 %v21606, %v21607 (stack75)
        %v21609 = vxor.u32 %v21604, %v21608 (stack76)
        %v21612 = vadd.s32 %v21604, %v21609 (stack65)
        %v21614 = vshll.u32 %v21609, 29 (stack73)
        %v21615 = vshrl.u32 %v21609, 3 (stack74)
        %v21616 = vor.u32 %v21614, %v21615 (stack75)
        %v21617 = vxor.u32 %v21612, %v21616 (stack76)
        %v21620 = vadd.s32 %v21612, %v21617 (stack65)
        %v21622 = vshll.u32 %v21617, 16 (stack73)
        %v21623 = vshrl.u32 %v21617, 16 (stack74)
        %v21624 = vor.u32 %v21622, %v21623 (stack75)
        %v21625 = vxor.u32 %v21620, %v21624 (stack76)
        %v21628 = vadd.s32 %v21620, %v21625 (stack65)
        %v21632 = vadd.s32 %v21628, %v8 (stack65)
        %v21634 = vshll.u32 %v21625, 24 (stack73)
        %v21635 = vshrl.u32 %v21625, 8 (stack74)
        %v21636 = vor.u32 %v21634, %v21635 (stack75)
        %v21637 = vxor.u32 %v21628, %v21636 (stack76)
        %v21640 = vadd.s32 %v21637, %v10 (stack65)
        %v21644 = vadd.s32 %v21640, 2 (stack65)
        %v21648 = vadd.s32 %v21632, %v21644 (stack65)
        %v21650 = vshll.u32 %v21644, 13 (stack73)
        %v21651 = vshrl.u32 %v21644, 19 (stack74)
        %v21652 = vor.u32 %v21650, %v21651 (stack75)
        %v21653 = vxor.u32 %v21648, %v21652 (stack76)
        %v21656 = vadd.s32 %v21648, %v21653 (stack65)
        %v21658 = vshll.u32 %v21653, 15 (stack73)
        %v21659 = vshrl.u32 %v21653, 17 (stack74)
        %v21660 = vor.u32 %v21658, %v21659 (stack75)
        %v21661 = vxor.u32 %v21656, %v21660 (stack76)
        %v21664 = vadd.s32 %v21656, %v21661 (stack65)
        %v21666 = vshll.u32 %v21661, 26 (stack73)
        %v21667 = vshrl.u32 %v21661, 6 (stack74)
        %v21668 = vor.u32 %v21666, %v21667 (stack75)
        %v21669 = vxor.u32 %v21664, %v21668 (stack76)
        %v21672 = vadd.s32 %v21664, %v21669 (stack65)
        %v21676 = vadd.s32 %v21672, %v10 (stack65)
        %v21678 = vshll.u32 %v21669, 6 (stack73)
        %v21679 = vshrl.u32 %v21669, 26 (stack74)
        %v21680 = vor.u32 %v21678, %v21679 (stack75)
        %v21681 = vxor.u32 %v21672, %v21680 (stack76)
        %v21684 = vadd.s32 %v21681, %v9 (stack65)
        %v21688 = vadd.s32 %v21684, 3 (stack65)
        %v21692 = vadd.s32 %v21676, %v21688 (stack65)
        %v21694 = vshll.u32 %v21688, 17 (stack73)
        %v21695 = vshrl.u32 %v21688, 15 (stack74)
        %v21696 = vor.u32 %v21694, %v21695 (stack75)
        %v21697 = vxor.u32 %v21692, %v21696 (stack76)
        %v21700 = vadd.s32 %v21692, %v21697 (stack65)
        %v21702 = vshll.u32 %v21697, 29 (stack73)
        %v21703 = vshrl.u32 %v21697, 3 (stack74)
        %v21704 = vor.u32 %v21702, %v21703 (stack75)
        %v21705 = vxor.u32 %v21700, %v21704 (stack76)
        %v21708 = vadd.s32 %v21700, %v21705 (stack65)
        %v21710 = vshll.u32 %v21705, 16 (stack73)
        %v21711 = vshrl.u32 %v21705, 16 (stack74)
        %v21712 = vor.u32 %v21710, %v21711 (stack75)
        %v21713 = vxor.u32 %v21708, %v21712 (stack76)
        %v21716 = vadd.s32 %v21708, %v21713 (stack65)
        %v21720 = vadd.s32 %v21716, %v9 (stack65)
        %v21722 = vshll.u32 %v21713, 24 (stack73)
        %v21723 = vshrl.u32 %v21713, 8 (stack74)
        %v21724 = vor.u32 %v21722, %v21723 (stack75)
        %v21725 = vxor.u32 %v21716, %v21724 (stack76)
        %v21728 = vadd.s32 %v21725, %v8 (stack65)
        %v21732 = vadd.s32 %v21728, 4 (stack65)
        %v21736 = vadd.s32 %v21720, %v21732 (stack65)
        %v21738 = vshll.u32 %v21732, 13 (stack73)
        %v21739 = vshrl.u32 %v21732, 19 (stack74)
        %v21740 = vor.u32 %v21738, %v21739 (stack75)
        %v21741 = vxor.u32 %v21736, %v21740 (stack76)
        %v21744 = vadd.s32 %v21736, %v21741 (stack65)
        %v21746 = vshll.u32 %v21741, 15 (stack73)
        %v21747 = vshrl.u32 %v21741, 17 (stack74)
        %v21748 = vor.u32 %v21746, %v21747 (stack75)
        %v21749 = vxor.u32 %v21744, %v21748 (stack76)
        %v21752 = vadd.s32 %v21744, %v21749 (stack65)
        %v21754 = vshll.u32 %v21749, 26 (stack73)
        %v21755 = vshrl.u32 %v21749, 6 (stack74)
        %v21756 = vor.u32 %v21754, %v21755 (stack75)
        %v21757 = vxor.u32 %v21752, %v21756 (stack76)
        %v21760 = vadd.s32 %v21752, %v21757 (stack65)
        %v21764 = vadd.s32 %v21760, %v8 (stack65)
        %v21766 = vshll.u32 %v21757, 6 (stack73)
        %v21767 = vshrl.u32 %v21757, 26 (stack74)
        %v21768 = vor.u32 %v21766, %v21767 (stack75)
        %v21769 = vxor.u32 %v21760, %v21768 (stack76)
        %v21772 = vadd.s32 %v21769, %v10 (stack65)
        %v21776 = vadd.s32 %v21772, 5 (stack65)
        %v21778 = vxor.u32 %v21764, %v21776 (stack76)
        %v21779 = vand.u32.u8 %v21778, 255 (stack77)
        %v21780 = vand.u32 %v21779, 65535 (stack78)
        %v21781 = vshrl.u32 %v21780, 1 (stack79)
        %v21782 = vor.u32 %v21781, 16256 (stack75)
        %v21783 = vand.u32.u16 %v21782, 65535 (stack80)
        %v21784 = vunpack.i.l.bf16 %v21783 (stack81)
        %v21788 = vadd.f32 %v21784, -1.0 (stack82)
        %v21792 = vmul.f32 %v21788, 2.0 (stack83)
        %v21796 = vadd.f32 %v21792, -0.99609375 (stack82)
        %v21800 = vmax.f32 -0.99609375, %v21796 (stack84)
        %v21802 = vand.u32 2147483647, %v21800 (stack85)
        %vm21805 = vcmp.eq.f32.partialorder %v21802, 1.0 (stack86)
        %v21810 = vmul.f32 %v21800, inf (stack83)
        %v21812 = vxor.u32 %v21800, 2147483648 (stack87)
        %v21815 = vmul.f32 %v21800, %v21812 (stack83)
        %v21817 = vadd.f32 %v21815, 1.0 (stack88)
        %v21818 = vlog2.pop %v21817 (stack89)
        %v21819 = vmul.f32 %v21818, 0.6931472 (stack90)
        %v21820 = vmul.f32 -0.5, %v21815 (stack91)
        %v21821 = vadd.f32 %v21820, 1.0 (stack92)
        %v21822 = vmul.f32 %v21821, %v21815 (stack93)
        %v21823 = vand.u32 2147483647, %v21815 (stack94)
        %vm21824 = vcmp.lt.f32.partialorder %v21823, 0.0004427343 (stack95)
        %v21825 = vsel /*vm=*/%vm21824, /*on_true_vy=*/%v21822, /*on_false_vx=*/%v21819 (stack96)
        %v21826 = vxor.u32 %v21825, 2147483648 (stack87)
        %vm21829 = vcmp.lt.f32.partialorder %v21826, 5.0 (stack86)
        %v21834 = vsel /*vm=*/%vm21829, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v21838 = vsel /*vm=*/%vm21829, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v21842 = vsel /*vm=*/%vm21829, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v21846 = vsel /*vm=*/%vm21829, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v21850 = vsel /*vm=*/%vm21829, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v21854 = vsel /*vm=*/%vm21829, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v21858 = vsel /*vm=*/%vm21829, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v21862 = vsel /*vm=*/%vm21829, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v21866 = vsel /*vm=*/%vm21829, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v21870 = vadd.f32 %v21826, -2.5 (stack82)
        %v21872 = vrsqrt.pop %v21826 (stack97)
        %v21873 = vmul.f32 %v21826, %v21872 (stack98)
        %vm21874 = vcmp.eq.f32.partialorder %v21826, inf (stack99)
        %v21875 = vsel /*vm=*/%vm21874, /*on_true_vy=*/%v21826, /*on_false_vx=*/%v21873 (stack100)
        %vm21876 = vcmp.eq.f32.partialorder %v21826, 0.0 (stack101)
        %v21877 = vand.u32 %v21826, 2147483648 (stack102)
        %v21878 = vsel /*vm=*/%vm21876, /*on_true_vy=*/%v21877, /*on_false_vx=*/%v21875 (stack103)
        %v21881 = vadd.f32 %v21878, -3.0 (stack82)
        %v21885 = vsel /*vm=*/%vm21829, /*on_true_vy=*/%v21870, /*on_false_vx=*/%v21881 (stack72)
        %v21889 = vmul.f32 %v21866, %v21885 (stack83)
        %v21893 = vadd.f32 %v21862, %v21889 (stack82)
        %v21897 = vmul.f32 %v21893, %v21885 (stack83)
        %v21901 = vadd.f32 %v21858, %v21897 (stack82)
        %v21905 = vmul.f32 %v21901, %v21885 (stack83)
        %v21909 = vadd.f32 %v21854, %v21905 (stack82)
        %v21913 = vmul.f32 %v21909, %v21885 (stack83)
        %v21917 = vadd.f32 %v21850, %v21913 (stack82)
        %v21921 = vmul.f32 %v21917, %v21885 (stack83)
        %v21925 = vadd.f32 %v21846, %v21921 (stack82)
        %v21929 = vmul.f32 %v21925, %v21885 (stack83)
        %v21933 = vadd.f32 %v21842, %v21929 (stack82)
        %v21937 = vmul.f32 %v21933, %v21885 (stack83)
        %v21941 = vadd.f32 %v21838, %v21937 (stack82)
        %v21945 = vmul.f32 %v21941, %v21885 (stack83)
        %v21949 = vadd.f32 %v21834, %v21945 (stack82)
        %v21953 = vmul.f32 %v21949, %v21800 (stack83)
        %v21957 = vsel /*vm=*/%vm21805, /*on_true_vy=*/%v21810, /*on_false_vx=*/%v21953 (stack72)
        %v21961 = vmul.f32 %v21957, 1.4140625 (stack83)
        %s21963 = scalar_lea.vmem %s280, 660 [#allocation0] (stack107)
        %v21964 = vpack.c.bf16 0.0, %v21961 (stack104)
        %21965 = vst [vmem:[%s21963] sm:$0xf] /*vst_source=*/%v21964 (stack105)
        %v21968 = vadd.s32 %v3329, %v19199 (stack65)
        %s21970 = smul.u32 128, %s27 (stack66)
        %v21971 = vlaneseq (stack67)
        %v21972 = vand.u32 %v21971, 127 (stack68)
        %v21973 = vstv %s21970 (stack69)
        %v21974 = vadd.s32 %v21972, %v21973 (stack70)
        %v21978 = vadd.s32 %v21968, %v21974 (stack65)
        %vm21982 = vcmp.lt.u32.totalorder %v21978, %v21968 (stack71)
        %vm21987 = vcmp.lt.u32.totalorder %v21968, %v3329 (stack71)
        %v21992 = vadd.s32 %v3316, %v19182 (stack65)
        %v21996 = vadd.s32 %v21992, 1 (stack65)
        %v22000 = vsel /*vm=*/%vm21987, /*on_true_vy=*/%v21996, /*on_false_vx=*/%v21992 (stack72)
        %v22004 = vadd.s32 %v22000, 1 (stack65)
        %v22008 = vsel /*vm=*/%vm21982, /*on_true_vy=*/%v22004, /*on_false_vx=*/%v22000 (stack72)
        %v22013 = vadd.s32 %v22008, %v10 (stack65)
        %v22017 = vadd.s32 %v21978, %v9 (stack65)
        %v22021 = vadd.s32 %v22013, %v22017 (stack65)
        %v22023 = vshll.u32 %v22017, 13 (stack73)
        %v22024 = vshrl.u32 %v22017, 19 (stack74)
        %v22025 = vor.u32 %v22023, %v22024 (stack75)
        %v22026 = vxor.u32 %v22021, %v22025 (stack76)
        %v22029 = vadd.s32 %v22021, %v22026 (stack65)
        %v22031 = vshll.u32 %v22026, 15 (stack73)
        %v22032 = vshrl.u32 %v22026, 17 (stack74)
        %v22033 = vor.u32 %v22031, %v22032 (stack75)
        %v22034 = vxor.u32 %v22029, %v22033 (stack76)
        %v22037 = vadd.s32 %v22029, %v22034 (stack65)
        %v22039 = vshll.u32 %v22034, 26 (stack73)
        %v22040 = vshrl.u32 %v22034, 6 (stack74)
        %v22041 = vor.u32 %v22039, %v22040 (stack75)
        %v22042 = vxor.u32 %v22037, %v22041 (stack76)
        %v22045 = vadd.s32 %v22037, %v22042 (stack65)
        %v22049 = vadd.s32 %v22045, %v9 (stack65)
        %v22051 = vshll.u32 %v22042, 6 (stack73)
        %v22052 = vshrl.u32 %v22042, 26 (stack74)
        %v22053 = vor.u32 %v22051, %v22052 (stack75)
        %v22054 = vxor.u32 %v22045, %v22053 (stack76)
        %v22057 = vadd.s32 %v22054, %v8 (stack65)
        %v22061 = vadd.s32 %v22057, 1 (stack65)
        %v22065 = vadd.s32 %v22049, %v22061 (stack65)
        %v22067 = vshll.u32 %v22061, 17 (stack73)
        %v22068 = vshrl.u32 %v22061, 15 (stack74)
        %v22069 = vor.u32 %v22067, %v22068 (stack75)
        %v22070 = vxor.u32 %v22065, %v22069 (stack76)
        %v22073 = vadd.s32 %v22065, %v22070 (stack65)
        %v22075 = vshll.u32 %v22070, 29 (stack73)
        %v22076 = vshrl.u32 %v22070, 3 (stack74)
        %v22077 = vor.u32 %v22075, %v22076 (stack75)
        %v22078 = vxor.u32 %v22073, %v22077 (stack76)
        %v22081 = vadd.s32 %v22073, %v22078 (stack65)
        %v22083 = vshll.u32 %v22078, 16 (stack73)
        %v22084 = vshrl.u32 %v22078, 16 (stack74)
        %v22085 = vor.u32 %v22083, %v22084 (stack75)
        %v22086 = vxor.u32 %v22081, %v22085 (stack76)
        %v22089 = vadd.s32 %v22081, %v22086 (stack65)
        %v22093 = vadd.s32 %v22089, %v8 (stack65)
        %v22095 = vshll.u32 %v22086, 24 (stack73)
        %v22096 = vshrl.u32 %v22086, 8 (stack74)
        %v22097 = vor.u32 %v22095, %v22096 (stack75)
        %v22098 = vxor.u32 %v22089, %v22097 (stack76)
        %v22101 = vadd.s32 %v22098, %v10 (stack65)
        %v22105 = vadd.s32 %v22101, 2 (stack65)
        %v22109 = vadd.s32 %v22093, %v22105 (stack65)
        %v22111 = vshll.u32 %v22105, 13 (stack73)
        %v22112 = vshrl.u32 %v22105, 19 (stack74)
        %v22113 = vor.u32 %v22111, %v22112 (stack75)
        %v22114 = vxor.u32 %v22109, %v22113 (stack76)
        %v22117 = vadd.s32 %v22109, %v22114 (stack65)
        %v22119 = vshll.u32 %v22114, 15 (stack73)
        %v22120 = vshrl.u32 %v22114, 17 (stack74)
        %v22121 = vor.u32 %v22119, %v22120 (stack75)
        %v22122 = vxor.u32 %v22117, %v22121 (stack76)
        %v22125 = vadd.s32 %v22117, %v22122 (stack65)
        %v22127 = vshll.u32 %v22122, 26 (stack73)
        %v22128 = vshrl.u32 %v22122, 6 (stack74)
        %v22129 = vor.u32 %v22127, %v22128 (stack75)
        %v22130 = vxor.u32 %v22125, %v22129 (stack76)
        %v22133 = vadd.s32 %v22125, %v22130 (stack65)
        %v22137 = vadd.s32 %v22133, %v10 (stack65)
        %v22139 = vshll.u32 %v22130, 6 (stack73)
        %v22140 = vshrl.u32 %v22130, 26 (stack74)
        %v22141 = vor.u32 %v22139, %v22140 (stack75)
        %v22142 = vxor.u32 %v22133, %v22141 (stack76)
        %v22145 = vadd.s32 %v22142, %v9 (stack65)
        %v22149 = vadd.s32 %v22145, 3 (stack65)
        %v22153 = vadd.s32 %v22137, %v22149 (stack65)
        %v22155 = vshll.u32 %v22149, 17 (stack73)
        %v22156 = vshrl.u32 %v22149, 15 (stack74)
        %v22157 = vor.u32 %v22155, %v22156 (stack75)
        %v22158 = vxor.u32 %v22153, %v22157 (stack76)
        %v22161 = vadd.s32 %v22153, %v22158 (stack65)
        %v22163 = vshll.u32 %v22158, 29 (stack73)
        %v22164 = vshrl.u32 %v22158, 3 (stack74)
        %v22165 = vor.u32 %v22163, %v22164 (stack75)
        %v22166 = vxor.u32 %v22161, %v22165 (stack76)
        %v22169 = vadd.s32 %v22161, %v22166 (stack65)
        %v22171 = vshll.u32 %v22166, 16 (stack73)
        %v22172 = vshrl.u32 %v22166, 16 (stack74)
        %v22173 = vor.u32 %v22171, %v22172 (stack75)
        %v22174 = vxor.u32 %v22169, %v22173 (stack76)
        %v22177 = vadd.s32 %v22169, %v22174 (stack65)
        %v22181 = vadd.s32 %v22177, %v9 (stack65)
        %v22183 = vshll.u32 %v22174, 24 (stack73)
        %v22184 = vshrl.u32 %v22174, 8 (stack74)
        %v22185 = vor.u32 %v22183, %v22184 (stack75)
        %v22186 = vxor.u32 %v22177, %v22185 (stack76)
        %v22189 = vadd.s32 %v22186, %v8 (stack65)
        %v22193 = vadd.s32 %v22189, 4 (stack65)
        %v22197 = vadd.s32 %v22181, %v22193 (stack65)
        %v22199 = vshll.u32 %v22193, 13 (stack73)
        %v22200 = vshrl.u32 %v22193, 19 (stack74)
        %v22201 = vor.u32 %v22199, %v22200 (stack75)
        %v22202 = vxor.u32 %v22197, %v22201 (stack76)
        %v22205 = vadd.s32 %v22197, %v22202 (stack65)
        %v22207 = vshll.u32 %v22202, 15 (stack73)
        %v22208 = vshrl.u32 %v22202, 17 (stack74)
        %v22209 = vor.u32 %v22207, %v22208 (stack75)
        %v22210 = vxor.u32 %v22205, %v22209 (stack76)
        %v22213 = vadd.s32 %v22205, %v22210 (stack65)
        %v22215 = vshll.u32 %v22210, 26 (stack73)
        %v22216 = vshrl.u32 %v22210, 6 (stack74)
        %v22217 = vor.u32 %v22215, %v22216 (stack75)
        %v22218 = vxor.u32 %v22213, %v22217 (stack76)
        %v22221 = vadd.s32 %v22213, %v22218 (stack65)
        %v22225 = vadd.s32 %v22221, %v8 (stack65)
        %v22227 = vshll.u32 %v22218, 6 (stack73)
        %v22228 = vshrl.u32 %v22218, 26 (stack74)
        %v22229 = vor.u32 %v22227, %v22228 (stack75)
        %v22230 = vxor.u32 %v22221, %v22229 (stack76)
        %v22233 = vadd.s32 %v22230, %v10 (stack65)
        %v22237 = vadd.s32 %v22233, 5 (stack65)
        %v22239 = vxor.u32 %v22225, %v22237 (stack76)
        %v22240 = vand.u32.u8 %v22239, 255 (stack77)
        %v22241 = vand.u32 %v22240, 65535 (stack78)
        %v22242 = vshrl.u32 %v22241, 1 (stack79)
        %v22243 = vor.u32 %v22242, 16256 (stack75)
        %v22244 = vand.u32.u16 %v22243, 65535 (stack80)
        %v22245 = vunpack.i.l.bf16 %v22244 (stack81)
        %v22249 = vadd.f32 %v22245, -1.0 (stack82)
        %v22253 = vmul.f32 %v22249, 2.0 (stack83)
        %v22257 = vadd.f32 %v22253, -0.99609375 (stack82)
        %v22261 = vmax.f32 -0.99609375, %v22257 (stack84)
        %v22263 = vand.u32 2147483647, %v22261 (stack85)
        %vm22266 = vcmp.eq.f32.partialorder %v22263, 1.0 (stack86)
        %v22271 = vmul.f32 %v22261, inf (stack83)
        %v22273 = vxor.u32 %v22261, 2147483648 (stack87)
        %v22276 = vmul.f32 %v22261, %v22273 (stack83)
        %v22278 = vadd.f32 %v22276, 1.0 (stack88)
        %v22279 = vlog2.pop %v22278 (stack89)
        %v22280 = vmul.f32 %v22279, 0.6931472 (stack90)
        %v22281 = vmul.f32 -0.5, %v22276 (stack91)
        %v22282 = vadd.f32 %v22281, 1.0 (stack92)
        %v22283 = vmul.f32 %v22282, %v22276 (stack93)
        %v22284 = vand.u32 2147483647, %v22276 (stack94)
        %vm22285 = vcmp.lt.f32.partialorder %v22284, 0.0004427343 (stack95)
        %v22286 = vsel /*vm=*/%vm22285, /*on_true_vy=*/%v22283, /*on_false_vx=*/%v22280 (stack96)
        %v22287 = vxor.u32 %v22286, 2147483648 (stack87)
        %vm22290 = vcmp.lt.f32.partialorder %v22287, 5.0 (stack86)
        %v22295 = vsel /*vm=*/%vm22290, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v22299 = vsel /*vm=*/%vm22290, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v22303 = vsel /*vm=*/%vm22290, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v22307 = vsel /*vm=*/%vm22290, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v22311 = vsel /*vm=*/%vm22290, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v22315 = vsel /*vm=*/%vm22290, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v22319 = vsel /*vm=*/%vm22290, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v22323 = vsel /*vm=*/%vm22290, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v22327 = vsel /*vm=*/%vm22290, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v22331 = vadd.f32 %v22287, -2.5 (stack82)
        %v22333 = vrsqrt.pop %v22287 (stack97)
        %v22334 = vmul.f32 %v22287, %v22333 (stack98)
        %vm22335 = vcmp.eq.f32.partialorder %v22287, inf (stack99)
        %v22336 = vsel /*vm=*/%vm22335, /*on_true_vy=*/%v22287, /*on_false_vx=*/%v22334 (stack100)
        %vm22337 = vcmp.eq.f32.partialorder %v22287, 0.0 (stack101)
        %v22338 = vand.u32 %v22287, 2147483648 (stack102)
        %v22339 = vsel /*vm=*/%vm22337, /*on_true_vy=*/%v22338, /*on_false_vx=*/%v22336 (stack103)
        %v22342 = vadd.f32 %v22339, -3.0 (stack82)
        %v22346 = vsel /*vm=*/%vm22290, /*on_true_vy=*/%v22331, /*on_false_vx=*/%v22342 (stack72)
        %v22350 = vmul.f32 %v22327, %v22346 (stack83)
        %v22354 = vadd.f32 %v22323, %v22350 (stack82)
        %v22358 = vmul.f32 %v22354, %v22346 (stack83)
        %v22362 = vadd.f32 %v22319, %v22358 (stack82)
        %v22366 = vmul.f32 %v22362, %v22346 (stack83)
        %v22370 = vadd.f32 %v22315, %v22366 (stack82)
        %v22374 = vmul.f32 %v22370, %v22346 (stack83)
        %v22378 = vadd.f32 %v22311, %v22374 (stack82)
        %v22382 = vmul.f32 %v22378, %v22346 (stack83)
        %v22386 = vadd.f32 %v22307, %v22382 (stack82)
        %v22390 = vmul.f32 %v22386, %v22346 (stack83)
        %v22394 = vadd.f32 %v22303, %v22390 (stack82)
        %v22398 = vmul.f32 %v22394, %v22346 (stack83)
        %v22402 = vadd.f32 %v22299, %v22398 (stack82)
        %v22406 = vmul.f32 %v22402, %v22346 (stack83)
        %v22410 = vadd.f32 %v22295, %v22406 (stack82)
        %v22414 = vmul.f32 %v22410, %v22261 (stack83)
        %v22418 = vsel /*vm=*/%vm22266, /*on_true_vy=*/%v22271, /*on_false_vx=*/%v22414 (stack72)
        %v22422 = vmul.f32 %v22418, 1.4140625 (stack83)
        %s22424 = scalar_lea.vmem %s280, 788 [#allocation0] (stack107)
        %v22425 = vpack.c.bf16 0.0, %v22422 (stack104)
        %22426 = vst [vmem:[%s22424] sm:$0xf] /*vst_source=*/%v22425 (stack105)
        %v22429 = vadd.s32 %v3816, %v19199 (stack65)
        %s22431 = smul.u32 128, %s27 (stack66)
        %v22432 = vlaneseq (stack67)
        %v22433 = vand.u32 %v22432, 127 (stack68)
        %v22434 = vstv %s22431 (stack69)
        %v22435 = vadd.s32 %v22433, %v22434 (stack70)
        %v22439 = vadd.s32 %v22429, %v22435 (stack65)
        %vm22443 = vcmp.lt.u32.totalorder %v22439, %v22429 (stack71)
        %vm22448 = vcmp.lt.u32.totalorder %v22429, %v3816 (stack71)
        %v22453 = vadd.s32 %v3803, %v19182 (stack65)
        %v22457 = vadd.s32 %v22453, 1 (stack65)
        %v22461 = vsel /*vm=*/%vm22448, /*on_true_vy=*/%v22457, /*on_false_vx=*/%v22453 (stack72)
        %v22465 = vadd.s32 %v22461, 1 (stack65)
        %v22469 = vsel /*vm=*/%vm22443, /*on_true_vy=*/%v22465, /*on_false_vx=*/%v22461 (stack72)
        %v22474 = vadd.s32 %v22469, %v10 (stack65)
        %v22478 = vadd.s32 %v22439, %v9 (stack65)
        %v22482 = vadd.s32 %v22474, %v22478 (stack65)
        %v22484 = vshll.u32 %v22478, 13 (stack73)
        %v22485 = vshrl.u32 %v22478, 19 (stack74)
        %v22486 = vor.u32 %v22484, %v22485 (stack75)
        %v22487 = vxor.u32 %v22482, %v22486 (stack76)
        %v22490 = vadd.s32 %v22482, %v22487 (stack65)
        %v22492 = vshll.u32 %v22487, 15 (stack73)
        %v22493 = vshrl.u32 %v22487, 17 (stack74)
        %v22494 = vor.u32 %v22492, %v22493 (stack75)
        %v22495 = vxor.u32 %v22490, %v22494 (stack76)
        %v22498 = vadd.s32 %v22490, %v22495 (stack65)
        %v22500 = vshll.u32 %v22495, 26 (stack73)
        %v22501 = vshrl.u32 %v22495, 6 (stack74)
        %v22502 = vor.u32 %v22500, %v22501 (stack75)
        %v22503 = vxor.u32 %v22498, %v22502 (stack76)
        %v22506 = vadd.s32 %v22498, %v22503 (stack65)
        %v22510 = vadd.s32 %v22506, %v9 (stack65)
        %v22512 = vshll.u32 %v22503, 6 (stack73)
        %v22513 = vshrl.u32 %v22503, 26 (stack74)
        %v22514 = vor.u32 %v22512, %v22513 (stack75)
        %v22515 = vxor.u32 %v22506, %v22514 (stack76)
        %v22518 = vadd.s32 %v22515, %v8 (stack65)
        %v22522 = vadd.s32 %v22518, 1 (stack65)
        %v22526 = vadd.s32 %v22510, %v22522 (stack65)
        %v22528 = vshll.u32 %v22522, 17 (stack73)
        %v22529 = vshrl.u32 %v22522, 15 (stack74)
        %v22530 = vor.u32 %v22528, %v22529 (stack75)
        %v22531 = vxor.u32 %v22526, %v22530 (stack76)
        %v22534 = vadd.s32 %v22526, %v22531 (stack65)
        %v22536 = vshll.u32 %v22531, 29 (stack73)
        %v22537 = vshrl.u32 %v22531, 3 (stack74)
        %v22538 = vor.u32 %v22536, %v22537 (stack75)
        %v22539 = vxor.u32 %v22534, %v22538 (stack76)
        %v22542 = vadd.s32 %v22534, %v22539 (stack65)
        %v22544 = vshll.u32 %v22539, 16 (stack73)
        %v22545 = vshrl.u32 %v22539, 16 (stack74)
        %v22546 = vor.u32 %v22544, %v22545 (stack75)
        %v22547 = vxor.u32 %v22542, %v22546 (stack76)
        %v22550 = vadd.s32 %v22542, %v22547 (stack65)
        %v22554 = vadd.s32 %v22550, %v8 (stack65)
        %v22556 = vshll.u32 %v22547, 24 (stack73)
        %v22557 = vshrl.u32 %v22547, 8 (stack74)
        %v22558 = vor.u32 %v22556, %v22557 (stack75)
        %v22559 = vxor.u32 %v22550, %v22558 (stack76)
        %v22562 = vadd.s32 %v22559, %v10 (stack65)
        %v22566 = vadd.s32 %v22562, 2 (stack65)
        %v22570 = vadd.s32 %v22554, %v22566 (stack65)
        %v22572 = vshll.u32 %v22566, 13 (stack73)
        %v22573 = vshrl.u32 %v22566, 19 (stack74)
        %v22574 = vor.u32 %v22572, %v22573 (stack75)
        %v22575 = vxor.u32 %v22570, %v22574 (stack76)
        %v22578 = vadd.s32 %v22570, %v22575 (stack65)
        %v22580 = vshll.u32 %v22575, 15 (stack73)
        %v22581 = vshrl.u32 %v22575, 17 (stack74)
        %v22582 = vor.u32 %v22580, %v22581 (stack75)
        %v22583 = vxor.u32 %v22578, %v22582 (stack76)
        %v22586 = vadd.s32 %v22578, %v22583 (stack65)
        %v22588 = vshll.u32 %v22583, 26 (stack73)
        %v22589 = vshrl.u32 %v22583, 6 (stack74)
        %v22590 = vor.u32 %v22588, %v22589 (stack75)
        %v22591 = vxor.u32 %v22586, %v22590 (stack76)
        %v22594 = vadd.s32 %v22586, %v22591 (stack65)
        %v22598 = vadd.s32 %v22594, %v10 (stack65)
        %v22600 = vshll.u32 %v22591, 6 (stack73)
        %v22601 = vshrl.u32 %v22591, 26 (stack74)
        %v22602 = vor.u32 %v22600, %v22601 (stack75)
        %v22603 = vxor.u32 %v22594, %v22602 (stack76)
        %v22606 = vadd.s32 %v22603, %v9 (stack65)
        %v22610 = vadd.s32 %v22606, 3 (stack65)
        %v22614 = vadd.s32 %v22598, %v22610 (stack65)
        %v22616 = vshll.u32 %v22610, 17 (stack73)
        %v22617 = vshrl.u32 %v22610, 15 (stack74)
        %v22618 = vor.u32 %v22616, %v22617 (stack75)
        %v22619 = vxor.u32 %v22614, %v22618 (stack76)
        %v22622 = vadd.s32 %v22614, %v22619 (stack65)
        %v22624 = vshll.u32 %v22619, 29 (stack73)
        %v22625 = vshrl.u32 %v22619, 3 (stack74)
        %v22626 = vor.u32 %v22624, %v22625 (stack75)
        %v22627 = vxor.u32 %v22622, %v22626 (stack76)
        %v22630 = vadd.s32 %v22622, %v22627 (stack65)
        %v22632 = vshll.u32 %v22627, 16 (stack73)
        %v22633 = vshrl.u32 %v22627, 16 (stack74)
        %v22634 = vor.u32 %v22632, %v22633 (stack75)
        %v22635 = vxor.u32 %v22630, %v22634 (stack76)
        %v22638 = vadd.s32 %v22630, %v22635 (stack65)
        %v22642 = vadd.s32 %v22638, %v9 (stack65)
        %v22644 = vshll.u32 %v22635, 24 (stack73)
        %v22645 = vshrl.u32 %v22635, 8 (stack74)
        %v22646 = vor.u32 %v22644, %v22645 (stack75)
        %v22647 = vxor.u32 %v22638, %v22646 (stack76)
        %v22650 = vadd.s32 %v22647, %v8 (stack65)
        %v22654 = vadd.s32 %v22650, 4 (stack65)
        %v22658 = vadd.s32 %v22642, %v22654 (stack65)
        %v22660 = vshll.u32 %v22654, 13 (stack73)
        %v22661 = vshrl.u32 %v22654, 19 (stack74)
        %v22662 = vor.u32 %v22660, %v22661 (stack75)
        %v22663 = vxor.u32 %v22658, %v22662 (stack76)
        %v22666 = vadd.s32 %v22658, %v22663 (stack65)
        %v22668 = vshll.u32 %v22663, 15 (stack73)
        %v22669 = vshrl.u32 %v22663, 17 (stack74)
        %v22670 = vor.u32 %v22668, %v22669 (stack75)
        %v22671 = vxor.u32 %v22666, %v22670 (stack76)
        %v22674 = vadd.s32 %v22666, %v22671 (stack65)
        %v22676 = vshll.u32 %v22671, 26 (stack73)
        %v22677 = vshrl.u32 %v22671, 6 (stack74)
        %v22678 = vor.u32 %v22676, %v22677 (stack75)
        %v22679 = vxor.u32 %v22674, %v22678 (stack76)
        %v22682 = vadd.s32 %v22674, %v22679 (stack65)
        %v22686 = vadd.s32 %v22682, %v8 (stack65)
        %v22688 = vshll.u32 %v22679, 6 (stack73)
        %v22689 = vshrl.u32 %v22679, 26 (stack74)
        %v22690 = vor.u32 %v22688, %v22689 (stack75)
        %v22691 = vxor.u32 %v22682, %v22690 (stack76)
        %v22694 = vadd.s32 %v22691, %v10 (stack65)
        %v22698 = vadd.s32 %v22694, 5 (stack65)
        %v22700 = vxor.u32 %v22686, %v22698 (stack76)
        %v22701 = vand.u32.u8 %v22700, 255 (stack77)
        %v22702 = vand.u32 %v22701, 65535 (stack78)
        %v22703 = vshrl.u32 %v22702, 1 (stack79)
        %v22704 = vor.u32 %v22703, 16256 (stack75)
        %v22705 = vand.u32.u16 %v22704, 65535 (stack80)
        %v22706 = vunpack.i.l.bf16 %v22705 (stack81)
        %v22710 = vadd.f32 %v22706, -1.0 (stack82)
        %v22714 = vmul.f32 %v22710, 2.0 (stack83)
        %v22718 = vadd.f32 %v22714, -0.99609375 (stack82)
        %v22722 = vmax.f32 -0.99609375, %v22718 (stack84)
        %v22724 = vand.u32 2147483647, %v22722 (stack85)
        %vm22727 = vcmp.eq.f32.partialorder %v22724, 1.0 (stack86)
        %v22732 = vmul.f32 %v22722, inf (stack83)
        %v22734 = vxor.u32 %v22722, 2147483648 (stack87)
        %v22737 = vmul.f32 %v22722, %v22734 (stack83)
        %v22739 = vadd.f32 %v22737, 1.0 (stack88)
        %v22740 = vlog2.pop %v22739 (stack89)
        %v22741 = vmul.f32 %v22740, 0.6931472 (stack90)
        %v22742 = vmul.f32 -0.5, %v22737 (stack91)
        %v22743 = vadd.f32 %v22742, 1.0 (stack92)
        %v22744 = vmul.f32 %v22743, %v22737 (stack93)
        %v22745 = vand.u32 2147483647, %v22737 (stack94)
        %vm22746 = vcmp.lt.f32.partialorder %v22745, 0.0004427343 (stack95)
        %v22747 = vsel /*vm=*/%vm22746, /*on_true_vy=*/%v22744, /*on_false_vx=*/%v22741 (stack96)
        %v22748 = vxor.u32 %v22747, 2147483648 (stack87)
        %vm22751 = vcmp.lt.f32.partialorder %v22748, 5.0 (stack86)
        %v22756 = vsel /*vm=*/%vm22751, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v22760 = vsel /*vm=*/%vm22751, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v22764 = vsel /*vm=*/%vm22751, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v22768 = vsel /*vm=*/%vm22751, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v22772 = vsel /*vm=*/%vm22751, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v22776 = vsel /*vm=*/%vm22751, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v22780 = vsel /*vm=*/%vm22751, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v22784 = vsel /*vm=*/%vm22751, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v22788 = vsel /*vm=*/%vm22751, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v22792 = vadd.f32 %v22748, -2.5 (stack82)
        %v22794 = vrsqrt.pop %v22748 (stack97)
        %v22795 = vmul.f32 %v22748, %v22794 (stack98)
        %vm22796 = vcmp.eq.f32.partialorder %v22748, inf (stack99)
        %v22797 = vsel /*vm=*/%vm22796, /*on_true_vy=*/%v22748, /*on_false_vx=*/%v22795 (stack100)
        %vm22798 = vcmp.eq.f32.partialorder %v22748, 0.0 (stack101)
        %v22799 = vand.u32 %v22748, 2147483648 (stack102)
        %v22800 = vsel /*vm=*/%vm22798, /*on_true_vy=*/%v22799, /*on_false_vx=*/%v22797 (stack103)
        %v22803 = vadd.f32 %v22800, -3.0 (stack82)
        %v22807 = vsel /*vm=*/%vm22751, /*on_true_vy=*/%v22792, /*on_false_vx=*/%v22803 (stack72)
        %v22811 = vmul.f32 %v22788, %v22807 (stack83)
        %v22815 = vadd.f32 %v22784, %v22811 (stack82)
        %v22819 = vmul.f32 %v22815, %v22807 (stack83)
        %v22823 = vadd.f32 %v22780, %v22819 (stack82)
        %v22827 = vmul.f32 %v22823, %v22807 (stack83)
        %v22831 = vadd.f32 %v22776, %v22827 (stack82)
        %v22835 = vmul.f32 %v22831, %v22807 (stack83)
        %v22839 = vadd.f32 %v22772, %v22835 (stack82)
        %v22843 = vmul.f32 %v22839, %v22807 (stack83)
        %v22847 = vadd.f32 %v22768, %v22843 (stack82)
        %v22851 = vmul.f32 %v22847, %v22807 (stack83)
        %v22855 = vadd.f32 %v22764, %v22851 (stack82)
        %v22859 = vmul.f32 %v22855, %v22807 (stack83)
        %v22863 = vadd.f32 %v22760, %v22859 (stack82)
        %v22867 = vmul.f32 %v22863, %v22807 (stack83)
        %v22871 = vadd.f32 %v22756, %v22867 (stack82)
        %v22875 = vmul.f32 %v22871, %v22722 (stack83)
        %v22879 = vsel /*vm=*/%vm22727, /*on_true_vy=*/%v22732, /*on_false_vx=*/%v22875 (stack72)
        %v22883 = vmul.f32 %v22879, 1.4140625 (stack83)
        %s22885 = scalar_lea.vmem %s280, 916 [#allocation0] (stack107)
        %v22886 = vpack.c.bf16 0.0, %v22883 (stack104)
        %22887 = vst [vmem:[%s22885] sm:$0xf] /*vst_source=*/%v22886 (stack105)
        %s22888 = sadd.s32 %s339, 48 (stack106)
        %s22889 = sshrl.u32 %s22888, 10 (stack49)
        %p22890 = scmp.lt.s32.totalorder 1, %s22889 (stack50)
        %s22891 = scalar_select /*predicate=*/%p22890, /*on_true=*/1, /*on_false=*/%s22889 (stack51)
        %s22892 = sand.u32 %s22888, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s22893 = sshrl.u32 %s22892, 7 (stack53)
        %s22894 = sand.u32 %s22892, 127 /* smod.u32 w/div 128 */ (stack54)
        %s22895 = smul.addr %s22891, 8 (stack55)
        %s22896 = scalar_lea.vmem %s3, %s22895 (stack56)
        %s22898 = scalar_lea.vmem %s22896, %s22893 (stack57)
        %v22899 = vld [vmem:[%s22898] ss:$0 sm:$0xff] (stack58)
        %s22900 = sand.u32 %s22894, 255 (stack59)
        %s22902 = sor.u32 256, %s22900 (stack60)
        %22903 = vbcast.lane.b32.xlu0 %v22899, %s22902 (stack61)
        %v22904 = vpop.permute.xlu0 %22903 (stack62)
        %s22905 = sadd.s32 %s347, 48 (stack106)
        %s22906 = sshrl.u32 %s22905, 10 (stack49)
        %p22907 = scmp.lt.s32.totalorder 1, %s22906 (stack50)
        %s22908 = scalar_select /*predicate=*/%p22907, /*on_true=*/1, /*on_false=*/%s22906 (stack51)
        %s22909 = sand.u32 %s22905, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s22910 = sshrl.u32 %s22909, 7 (stack53)
        %s22911 = sand.u32 %s22909, 127 /* smod.u32 w/div 128 */ (stack54)
        %s22912 = smul.addr %s22908, 8 (stack55)
        %s22913 = scalar_lea.vmem %s5, %s22912 (stack56)
        %s22915 = scalar_lea.vmem %s22913, %s22910 (stack57)
        %v22916 = vld [vmem:[%s22915] ss:$0 sm:$0xff] (stack58)
        %s22917 = sand.u32 %s22911, 255 (stack59)
        %s22919 = sor.u32 256, %s22917 (stack60)
        %22920 = vbcast.lane.b32.xlu0 %v22916, %s22919 (stack61)
        %v22921 = vpop.permute.xlu0 %22920 (stack62)
        %v22924 = vadd.s32 %v408, %v22921 (stack65)
        %s22926 = smul.u32 128, %s27 (stack66)
        %v22927 = vlaneseq (stack67)
        %v22928 = vand.u32 %v22927, 127 (stack68)
        %v22929 = vstv %s22926 (stack69)
        %v22930 = vadd.s32 %v22928, %v22929 (stack70)
        %v22934 = vadd.s32 %v22924, %v22930 (stack65)
        %vm22938 = vcmp.lt.u32.totalorder %v22934, %v22924 (stack71)
        %vm22943 = vcmp.lt.u32.totalorder %v22924, %v408 (stack71)
        %v22948 = vadd.s32 %v380, %v22904 (stack65)
        %v22952 = vadd.s32 %v22948, 1 (stack65)
        %v22956 = vsel /*vm=*/%vm22943, /*on_true_vy=*/%v22952, /*on_false_vx=*/%v22948 (stack72)
        %v22960 = vadd.s32 %v22956, 1 (stack65)
        %v22964 = vsel /*vm=*/%vm22938, /*on_true_vy=*/%v22960, /*on_false_vx=*/%v22956 (stack72)
        %v22969 = vadd.s32 %v22964, %v10 (stack65)
        %v22973 = vadd.s32 %v22934, %v9 (stack65)
        %v22977 = vadd.s32 %v22969, %v22973 (stack65)
        %v22979 = vshll.u32 %v22973, 13 (stack73)
        %v22980 = vshrl.u32 %v22973, 19 (stack74)
        %v22981 = vor.u32 %v22979, %v22980 (stack75)
        %v22982 = vxor.u32 %v22977, %v22981 (stack76)
        %v22985 = vadd.s32 %v22977, %v22982 (stack65)
        %v22987 = vshll.u32 %v22982, 15 (stack73)
        %v22988 = vshrl.u32 %v22982, 17 (stack74)
        %v22989 = vor.u32 %v22987, %v22988 (stack75)
        %v22990 = vxor.u32 %v22985, %v22989 (stack76)
        %v22993 = vadd.s32 %v22985, %v22990 (stack65)
        %v22995 = vshll.u32 %v22990, 26 (stack73)
        %v22996 = vshrl.u32 %v22990, 6 (stack74)
        %v22997 = vor.u32 %v22995, %v22996 (stack75)
        %v22998 = vxor.u32 %v22993, %v22997 (stack76)
        %v23001 = vadd.s32 %v22993, %v22998 (stack65)
        %v23005 = vadd.s32 %v23001, %v9 (stack65)
        %v23007 = vshll.u32 %v22998, 6 (stack73)
        %v23008 = vshrl.u32 %v22998, 26 (stack74)
        %v23009 = vor.u32 %v23007, %v23008 (stack75)
        %v23010 = vxor.u32 %v23001, %v23009 (stack76)
        %v23013 = vadd.s32 %v23010, %v8 (stack65)
        %v23017 = vadd.s32 %v23013, 1 (stack65)
        %v23021 = vadd.s32 %v23005, %v23017 (stack65)
        %v23023 = vshll.u32 %v23017, 17 (stack73)
        %v23024 = vshrl.u32 %v23017, 15 (stack74)
        %v23025 = vor.u32 %v23023, %v23024 (stack75)
        %v23026 = vxor.u32 %v23021, %v23025 (stack76)
        %v23029 = vadd.s32 %v23021, %v23026 (stack65)
        %v23031 = vshll.u32 %v23026, 29 (stack73)
        %v23032 = vshrl.u32 %v23026, 3 (stack74)
        %v23033 = vor.u32 %v23031, %v23032 (stack75)
        %v23034 = vxor.u32 %v23029, %v23033 (stack76)
        %v23037 = vadd.s32 %v23029, %v23034 (stack65)
        %v23039 = vshll.u32 %v23034, 16 (stack73)
        %v23040 = vshrl.u32 %v23034, 16 (stack74)
        %v23041 = vor.u32 %v23039, %v23040 (stack75)
        %v23042 = vxor.u32 %v23037, %v23041 (stack76)
        %v23045 = vadd.s32 %v23037, %v23042 (stack65)
        %v23049 = vadd.s32 %v23045, %v8 (stack65)
        %v23051 = vshll.u32 %v23042, 24 (stack73)
        %v23052 = vshrl.u32 %v23042, 8 (stack74)
        %v23053 = vor.u32 %v23051, %v23052 (stack75)
        %v23054 = vxor.u32 %v23045, %v23053 (stack76)
        %v23057 = vadd.s32 %v23054, %v10 (stack65)
        %v23061 = vadd.s32 %v23057, 2 (stack65)
        %v23065 = vadd.s32 %v23049, %v23061 (stack65)
        %v23067 = vshll.u32 %v23061, 13 (stack73)
        %v23068 = vshrl.u32 %v23061, 19 (stack74)
        %v23069 = vor.u32 %v23067, %v23068 (stack75)
        %v23070 = vxor.u32 %v23065, %v23069 (stack76)
        %v23073 = vadd.s32 %v23065, %v23070 (stack65)
        %v23075 = vshll.u32 %v23070, 15 (stack73)
        %v23076 = vshrl.u32 %v23070, 17 (stack74)
        %v23077 = vor.u32 %v23075, %v23076 (stack75)
        %v23078 = vxor.u32 %v23073, %v23077 (stack76)
        %v23081 = vadd.s32 %v23073, %v23078 (stack65)
        %v23083 = vshll.u32 %v23078, 26 (stack73)
        %v23084 = vshrl.u32 %v23078, 6 (stack74)
        %v23085 = vor.u32 %v23083, %v23084 (stack75)
        %v23086 = vxor.u32 %v23081, %v23085 (stack76)
        %v23089 = vadd.s32 %v23081, %v23086 (stack65)
        %v23093 = vadd.s32 %v23089, %v10 (stack65)
        %v23095 = vshll.u32 %v23086, 6 (stack73)
        %v23096 = vshrl.u32 %v23086, 26 (stack74)
        %v23097 = vor.u32 %v23095, %v23096 (stack75)
        %v23098 = vxor.u32 %v23089, %v23097 (stack76)
        %v23101 = vadd.s32 %v23098, %v9 (stack65)
        %v23105 = vadd.s32 %v23101, 3 (stack65)
        %v23109 = vadd.s32 %v23093, %v23105 (stack65)
        %v23111 = vshll.u32 %v23105, 17 (stack73)
        %v23112 = vshrl.u32 %v23105, 15 (stack74)
        %v23113 = vor.u32 %v23111, %v23112 (stack75)
        %v23114 = vxor.u32 %v23109, %v23113 (stack76)
        %v23117 = vadd.s32 %v23109, %v23114 (stack65)
        %v23119 = vshll.u32 %v23114, 29 (stack73)
        %v23120 = vshrl.u32 %v23114, 3 (stack74)
        %v23121 = vor.u32 %v23119, %v23120 (stack75)
        %v23122 = vxor.u32 %v23117, %v23121 (stack76)
        %v23125 = vadd.s32 %v23117, %v23122 (stack65)
        %v23127 = vshll.u32 %v23122, 16 (stack73)
        %v23128 = vshrl.u32 %v23122, 16 (stack74)
        %v23129 = vor.u32 %v23127, %v23128 (stack75)
        %v23130 = vxor.u32 %v23125, %v23129 (stack76)
        %v23133 = vadd.s32 %v23125, %v23130 (stack65)
        %v23137 = vadd.s32 %v23133, %v9 (stack65)
        %v23139 = vshll.u32 %v23130, 24 (stack73)
        %v23140 = vshrl.u32 %v23130, 8 (stack74)
        %v23141 = vor.u32 %v23139, %v23140 (stack75)
        %v23142 = vxor.u32 %v23133, %v23141 (stack76)
        %v23145 = vadd.s32 %v23142, %v8 (stack65)
        %v23149 = vadd.s32 %v23145, 4 (stack65)
        %v23153 = vadd.s32 %v23137, %v23149 (stack65)
        %v23155 = vshll.u32 %v23149, 13 (stack73)
        %v23156 = vshrl.u32 %v23149, 19 (stack74)
        %v23157 = vor.u32 %v23155, %v23156 (stack75)
        %v23158 = vxor.u32 %v23153, %v23157 (stack76)
        %v23161 = vadd.s32 %v23153, %v23158 (stack65)
        %v23163 = vshll.u32 %v23158, 15 (stack73)
        %v23164 = vshrl.u32 %v23158, 17 (stack74)
        %v23165 = vor.u32 %v23163, %v23164 (stack75)
        %v23166 = vxor.u32 %v23161, %v23165 (stack76)
        %v23169 = vadd.s32 %v23161, %v23166 (stack65)
        %v23171 = vshll.u32 %v23166, 26 (stack73)
        %v23172 = vshrl.u32 %v23166, 6 (stack74)
        %v23173 = vor.u32 %v23171, %v23172 (stack75)
        %v23174 = vxor.u32 %v23169, %v23173 (stack76)
        %v23177 = vadd.s32 %v23169, %v23174 (stack65)
        %v23181 = vadd.s32 %v23177, %v8 (stack65)
        %v23183 = vshll.u32 %v23174, 6 (stack73)
        %v23184 = vshrl.u32 %v23174, 26 (stack74)
        %v23185 = vor.u32 %v23183, %v23184 (stack75)
        %v23186 = vxor.u32 %v23177, %v23185 (stack76)
        %v23189 = vadd.s32 %v23186, %v10 (stack65)
        %v23193 = vadd.s32 %v23189, 5 (stack65)
        %v23195 = vxor.u32 %v23181, %v23193 (stack76)
        %v23196 = vand.u32.u8 %v23195, 255 (stack77)
        %v23197 = vand.u32 %v23196, 65535 (stack78)
        %v23198 = vshrl.u32 %v23197, 1 (stack79)
        %v23199 = vor.u32 %v23198, 16256 (stack75)
        %v23200 = vand.u32.u16 %v23199, 65535 (stack80)
        %v23201 = vunpack.i.l.bf16 %v23200 (stack81)
        %v23205 = vadd.f32 %v23201, -1.0 (stack82)
        %v23209 = vmul.f32 %v23205, 2.0 (stack83)
        %v23213 = vadd.f32 %v23209, -0.99609375 (stack82)
        %v23217 = vmax.f32 -0.99609375, %v23213 (stack84)
        %v23219 = vand.u32 2147483647, %v23217 (stack85)
        %vm23222 = vcmp.eq.f32.partialorder %v23219, 1.0 (stack86)
        %v23227 = vmul.f32 %v23217, inf (stack83)
        %v23229 = vxor.u32 %v23217, 2147483648 (stack87)
        %v23232 = vmul.f32 %v23217, %v23229 (stack83)
        %v23234 = vadd.f32 %v23232, 1.0 (stack88)
        %v23235 = vlog2.pop %v23234 (stack89)
        %v23236 = vmul.f32 %v23235, 0.6931472 (stack90)
        %v23237 = vmul.f32 -0.5, %v23232 (stack91)
        %v23238 = vadd.f32 %v23237, 1.0 (stack92)
        %v23239 = vmul.f32 %v23238, %v23232 (stack93)
        %v23240 = vand.u32 2147483647, %v23232 (stack94)
        %vm23241 = vcmp.lt.f32.partialorder %v23240, 0.0004427343 (stack95)
        %v23242 = vsel /*vm=*/%vm23241, /*on_true_vy=*/%v23239, /*on_false_vx=*/%v23236 (stack96)
        %v23243 = vxor.u32 %v23242, 2147483648 (stack87)
        %vm23246 = vcmp.lt.f32.partialorder %v23243, 5.0 (stack86)
        %v23251 = vsel /*vm=*/%vm23246, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v23255 = vsel /*vm=*/%vm23246, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v23259 = vsel /*vm=*/%vm23246, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v23263 = vsel /*vm=*/%vm23246, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v23267 = vsel /*vm=*/%vm23246, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v23271 = vsel /*vm=*/%vm23246, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v23275 = vsel /*vm=*/%vm23246, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v23279 = vsel /*vm=*/%vm23246, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v23283 = vsel /*vm=*/%vm23246, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v23287 = vadd.f32 %v23243, -2.5 (stack82)
        %v23289 = vrsqrt.pop %v23243 (stack97)
        %v23290 = vmul.f32 %v23243, %v23289 (stack98)
        %vm23291 = vcmp.eq.f32.partialorder %v23243, inf (stack99)
        %v23292 = vsel /*vm=*/%vm23291, /*on_true_vy=*/%v23243, /*on_false_vx=*/%v23290 (stack100)
        %vm23293 = vcmp.eq.f32.partialorder %v23243, 0.0 (stack101)
        %v23294 = vand.u32 %v23243, 2147483648 (stack102)
        %v23295 = vsel /*vm=*/%vm23293, /*on_true_vy=*/%v23294, /*on_false_vx=*/%v23292 (stack103)
        %v23298 = vadd.f32 %v23295, -3.0 (stack82)
        %v23302 = vsel /*vm=*/%vm23246, /*on_true_vy=*/%v23287, /*on_false_vx=*/%v23298 (stack72)
        %v23306 = vmul.f32 %v23283, %v23302 (stack83)
        %v23310 = vadd.f32 %v23279, %v23306 (stack82)
        %v23314 = vmul.f32 %v23310, %v23302 (stack83)
        %v23318 = vadd.f32 %v23275, %v23314 (stack82)
        %v23322 = vmul.f32 %v23318, %v23302 (stack83)
        %v23326 = vadd.f32 %v23271, %v23322 (stack82)
        %v23330 = vmul.f32 %v23326, %v23302 (stack83)
        %v23334 = vadd.f32 %v23267, %v23330 (stack82)
        %v23338 = vmul.f32 %v23334, %v23302 (stack83)
        %v23342 = vadd.f32 %v23263, %v23338 (stack82)
        %v23346 = vmul.f32 %v23342, %v23302 (stack83)
        %v23350 = vadd.f32 %v23259, %v23346 (stack82)
        %v23354 = vmul.f32 %v23350, %v23302 (stack83)
        %v23358 = vadd.f32 %v23255, %v23354 (stack82)
        %v23362 = vmul.f32 %v23358, %v23302 (stack83)
        %v23366 = vadd.f32 %v23251, %v23362 (stack82)
        %v23370 = vmul.f32 %v23366, %v23217 (stack83)
        %v23374 = vsel /*vm=*/%vm23222, /*on_true_vy=*/%v23227, /*on_false_vx=*/%v23370 (stack72)
        %v23378 = vmul.f32 %v23374, 1.4140625 (stack83)
        %s23380 = scalar_lea.vmem %s280, 24 [#allocation0] (stack107)
        %v23381 = vpack.c.bf16 0.0, %v23378 (stack104)
        %23382 = vst [vmem:[%s23380] sm:$0xf] /*vst_source=*/%v23381 (stack105)
        %v23385 = vadd.s32 %v894, %v22921 (stack65)
        %s23387 = smul.u32 128, %s27 (stack66)
        %v23388 = vlaneseq (stack67)
        %v23389 = vand.u32 %v23388, 127 (stack68)
        %v23390 = vstv %s23387 (stack69)
        %v23391 = vadd.s32 %v23389, %v23390 (stack70)
        %v23395 = vadd.s32 %v23385, %v23391 (stack65)
        %vm23399 = vcmp.lt.u32.totalorder %v23395, %v23385 (stack71)
        %vm23404 = vcmp.lt.u32.totalorder %v23385, %v894 (stack71)
        %v23409 = vadd.s32 %v881, %v22904 (stack65)
        %v23413 = vadd.s32 %v23409, 1 (stack65)
        %v23417 = vsel /*vm=*/%vm23404, /*on_true_vy=*/%v23413, /*on_false_vx=*/%v23409 (stack72)
        %v23421 = vadd.s32 %v23417, 1 (stack65)
        %v23425 = vsel /*vm=*/%vm23399, /*on_true_vy=*/%v23421, /*on_false_vx=*/%v23417 (stack72)
        %v23430 = vadd.s32 %v23425, %v10 (stack65)
        %v23434 = vadd.s32 %v23395, %v9 (stack65)
        %v23438 = vadd.s32 %v23430, %v23434 (stack65)
        %v23440 = vshll.u32 %v23434, 13 (stack73)
        %v23441 = vshrl.u32 %v23434, 19 (stack74)
        %v23442 = vor.u32 %v23440, %v23441 (stack75)
        %v23443 = vxor.u32 %v23438, %v23442 (stack76)
        %v23446 = vadd.s32 %v23438, %v23443 (stack65)
        %v23448 = vshll.u32 %v23443, 15 (stack73)
        %v23449 = vshrl.u32 %v23443, 17 (stack74)
        %v23450 = vor.u32 %v23448, %v23449 (stack75)
        %v23451 = vxor.u32 %v23446, %v23450 (stack76)
        %v23454 = vadd.s32 %v23446, %v23451 (stack65)
        %v23456 = vshll.u32 %v23451, 26 (stack73)
        %v23457 = vshrl.u32 %v23451, 6 (stack74)
        %v23458 = vor.u32 %v23456, %v23457 (stack75)
        %v23459 = vxor.u32 %v23454, %v23458 (stack76)
        %v23462 = vadd.s32 %v23454, %v23459 (stack65)
        %v23466 = vadd.s32 %v23462, %v9 (stack65)
        %v23468 = vshll.u32 %v23459, 6 (stack73)
        %v23469 = vshrl.u32 %v23459, 26 (stack74)
        %v23470 = vor.u32 %v23468, %v23469 (stack75)
        %v23471 = vxor.u32 %v23462, %v23470 (stack76)
        %v23474 = vadd.s32 %v23471, %v8 (stack65)
        %v23478 = vadd.s32 %v23474, 1 (stack65)
        %v23482 = vadd.s32 %v23466, %v23478 (stack65)
        %v23484 = vshll.u32 %v23478, 17 (stack73)
        %v23485 = vshrl.u32 %v23478, 15 (stack74)
        %v23486 = vor.u32 %v23484, %v23485 (stack75)
        %v23487 = vxor.u32 %v23482, %v23486 (stack76)
        %v23490 = vadd.s32 %v23482, %v23487 (stack65)
        %v23492 = vshll.u32 %v23487, 29 (stack73)
        %v23493 = vshrl.u32 %v23487, 3 (stack74)
        %v23494 = vor.u32 %v23492, %v23493 (stack75)
        %v23495 = vxor.u32 %v23490, %v23494 (stack76)
        %v23498 = vadd.s32 %v23490, %v23495 (stack65)
        %v23500 = vshll.u32 %v23495, 16 (stack73)
        %v23501 = vshrl.u32 %v23495, 16 (stack74)
        %v23502 = vor.u32 %v23500, %v23501 (stack75)
        %v23503 = vxor.u32 %v23498, %v23502 (stack76)
        %v23506 = vadd.s32 %v23498, %v23503 (stack65)
        %v23510 = vadd.s32 %v23506, %v8 (stack65)
        %v23512 = vshll.u32 %v23503, 24 (stack73)
        %v23513 = vshrl.u32 %v23503, 8 (stack74)
        %v23514 = vor.u32 %v23512, %v23513 (stack75)
        %v23515 = vxor.u32 %v23506, %v23514 (stack76)
        %v23518 = vadd.s32 %v23515, %v10 (stack65)
        %v23522 = vadd.s32 %v23518, 2 (stack65)
        %v23526 = vadd.s32 %v23510, %v23522 (stack65)
        %v23528 = vshll.u32 %v23522, 13 (stack73)
        %v23529 = vshrl.u32 %v23522, 19 (stack74)
        %v23530 = vor.u32 %v23528, %v23529 (stack75)
        %v23531 = vxor.u32 %v23526, %v23530 (stack76)
        %v23534 = vadd.s32 %v23526, %v23531 (stack65)
        %v23536 = vshll.u32 %v23531, 15 (stack73)
        %v23537 = vshrl.u32 %v23531, 17 (stack74)
        %v23538 = vor.u32 %v23536, %v23537 (stack75)
        %v23539 = vxor.u32 %v23534, %v23538 (stack76)
        %v23542 = vadd.s32 %v23534, %v23539 (stack65)
        %v23544 = vshll.u32 %v23539, 26 (stack73)
        %v23545 = vshrl.u32 %v23539, 6 (stack74)
        %v23546 = vor.u32 %v23544, %v23545 (stack75)
        %v23547 = vxor.u32 %v23542, %v23546 (stack76)
        %v23550 = vadd.s32 %v23542, %v23547 (stack65)
        %v23554 = vadd.s32 %v23550, %v10 (stack65)
        %v23556 = vshll.u32 %v23547, 6 (stack73)
        %v23557 = vshrl.u32 %v23547, 26 (stack74)
        %v23558 = vor.u32 %v23556, %v23557 (stack75)
        %v23559 = vxor.u32 %v23550, %v23558 (stack76)
        %v23562 = vadd.s32 %v23559, %v9 (stack65)
        %v23566 = vadd.s32 %v23562, 3 (stack65)
        %v23570 = vadd.s32 %v23554, %v23566 (stack65)
        %v23572 = vshll.u32 %v23566, 17 (stack73)
        %v23573 = vshrl.u32 %v23566, 15 (stack74)
        %v23574 = vor.u32 %v23572, %v23573 (stack75)
        %v23575 = vxor.u32 %v23570, %v23574 (stack76)
        %v23578 = vadd.s32 %v23570, %v23575 (stack65)
        %v23580 = vshll.u32 %v23575, 29 (stack73)
        %v23581 = vshrl.u32 %v23575, 3 (stack74)
        %v23582 = vor.u32 %v23580, %v23581 (stack75)
        %v23583 = vxor.u32 %v23578, %v23582 (stack76)
        %v23586 = vadd.s32 %v23578, %v23583 (stack65)
        %v23588 = vshll.u32 %v23583, 16 (stack73)
        %v23589 = vshrl.u32 %v23583, 16 (stack74)
        %v23590 = vor.u32 %v23588, %v23589 (stack75)
        %v23591 = vxor.u32 %v23586, %v23590 (stack76)
        %v23594 = vadd.s32 %v23586, %v23591 (stack65)
        %v23598 = vadd.s32 %v23594, %v9 (stack65)
        %v23600 = vshll.u32 %v23591, 24 (stack73)
        %v23601 = vshrl.u32 %v23591, 8 (stack74)
        %v23602 = vor.u32 %v23600, %v23601 (stack75)
        %v23603 = vxor.u32 %v23594, %v23602 (stack76)
        %v23606 = vadd.s32 %v23603, %v8 (stack65)
        %v23610 = vadd.s32 %v23606, 4 (stack65)
        %v23614 = vadd.s32 %v23598, %v23610 (stack65)
        %v23616 = vshll.u32 %v23610, 13 (stack73)
        %v23617 = vshrl.u32 %v23610, 19 (stack74)
        %v23618 = vor.u32 %v23616, %v23617 (stack75)
        %v23619 = vxor.u32 %v23614, %v23618 (stack76)
        %v23622 = vadd.s32 %v23614, %v23619 (stack65)
        %v23624 = vshll.u32 %v23619, 15 (stack73)
        %v23625 = vshrl.u32 %v23619, 17 (stack74)
        %v23626 = vor.u32 %v23624, %v23625 (stack75)
        %v23627 = vxor.u32 %v23622, %v23626 (stack76)
        %v23630 = vadd.s32 %v23622, %v23627 (stack65)
        %v23632 = vshll.u32 %v23627, 26 (stack73)
        %v23633 = vshrl.u32 %v23627, 6 (stack74)
        %v23634 = vor.u32 %v23632, %v23633 (stack75)
        %v23635 = vxor.u32 %v23630, %v23634 (stack76)
        %v23638 = vadd.s32 %v23630, %v23635 (stack65)
        %v23642 = vadd.s32 %v23638, %v8 (stack65)
        %v23644 = vshll.u32 %v23635, 6 (stack73)
        %v23645 = vshrl.u32 %v23635, 26 (stack74)
        %v23646 = vor.u32 %v23644, %v23645 (stack75)
        %v23647 = vxor.u32 %v23638, %v23646 (stack76)
        %v23650 = vadd.s32 %v23647, %v10 (stack65)
        %v23654 = vadd.s32 %v23650, 5 (stack65)
        %v23656 = vxor.u32 %v23642, %v23654 (stack76)
        %v23657 = vand.u32.u8 %v23656, 255 (stack77)
        %v23658 = vand.u32 %v23657, 65535 (stack78)
        %v23659 = vshrl.u32 %v23658, 1 (stack79)
        %v23660 = vor.u32 %v23659, 16256 (stack75)
        %v23661 = vand.u32.u16 %v23660, 65535 (stack80)
        %v23662 = vunpack.i.l.bf16 %v23661 (stack81)
        %v23666 = vadd.f32 %v23662, -1.0 (stack82)
        %v23670 = vmul.f32 %v23666, 2.0 (stack83)
        %v23674 = vadd.f32 %v23670, -0.99609375 (stack82)
        %v23678 = vmax.f32 -0.99609375, %v23674 (stack84)
        %v23680 = vand.u32 2147483647, %v23678 (stack85)
        %vm23683 = vcmp.eq.f32.partialorder %v23680, 1.0 (stack86)
        %v23688 = vmul.f32 %v23678, inf (stack83)
        %v23690 = vxor.u32 %v23678, 2147483648 (stack87)
        %v23693 = vmul.f32 %v23678, %v23690 (stack83)
        %v23695 = vadd.f32 %v23693, 1.0 (stack88)
        %v23696 = vlog2.pop %v23695 (stack89)
        %v23697 = vmul.f32 %v23696, 0.6931472 (stack90)
        %v23698 = vmul.f32 -0.5, %v23693 (stack91)
        %v23699 = vadd.f32 %v23698, 1.0 (stack92)
        %v23700 = vmul.f32 %v23699, %v23693 (stack93)
        %v23701 = vand.u32 2147483647, %v23693 (stack94)
        %vm23702 = vcmp.lt.f32.partialorder %v23701, 0.0004427343 (stack95)
        %v23703 = vsel /*vm=*/%vm23702, /*on_true_vy=*/%v23700, /*on_false_vx=*/%v23697 (stack96)
        %v23704 = vxor.u32 %v23703, 2147483648 (stack87)
        %vm23707 = vcmp.lt.f32.partialorder %v23704, 5.0 (stack86)
        %v23712 = vsel /*vm=*/%vm23707, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v23716 = vsel /*vm=*/%vm23707, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v23720 = vsel /*vm=*/%vm23707, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v23724 = vsel /*vm=*/%vm23707, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v23728 = vsel /*vm=*/%vm23707, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v23732 = vsel /*vm=*/%vm23707, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v23736 = vsel /*vm=*/%vm23707, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v23740 = vsel /*vm=*/%vm23707, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v23744 = vsel /*vm=*/%vm23707, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v23748 = vadd.f32 %v23704, -2.5 (stack82)
        %v23750 = vrsqrt.pop %v23704 (stack97)
        %v23751 = vmul.f32 %v23704, %v23750 (stack98)
        %vm23752 = vcmp.eq.f32.partialorder %v23704, inf (stack99)
        %v23753 = vsel /*vm=*/%vm23752, /*on_true_vy=*/%v23704, /*on_false_vx=*/%v23751 (stack100)
        %vm23754 = vcmp.eq.f32.partialorder %v23704, 0.0 (stack101)
        %v23755 = vand.u32 %v23704, 2147483648 (stack102)
        %v23756 = vsel /*vm=*/%vm23754, /*on_true_vy=*/%v23755, /*on_false_vx=*/%v23753 (stack103)
        %v23759 = vadd.f32 %v23756, -3.0 (stack82)
        %v23763 = vsel /*vm=*/%vm23707, /*on_true_vy=*/%v23748, /*on_false_vx=*/%v23759 (stack72)
        %v23767 = vmul.f32 %v23744, %v23763 (stack83)
        %v23771 = vadd.f32 %v23740, %v23767 (stack82)
        %v23775 = vmul.f32 %v23771, %v23763 (stack83)
        %v23779 = vadd.f32 %v23736, %v23775 (stack82)
        %v23783 = vmul.f32 %v23779, %v23763 (stack83)
        %v23787 = vadd.f32 %v23732, %v23783 (stack82)
        %v23791 = vmul.f32 %v23787, %v23763 (stack83)
        %v23795 = vadd.f32 %v23728, %v23791 (stack82)
        %v23799 = vmul.f32 %v23795, %v23763 (stack83)
        %v23803 = vadd.f32 %v23724, %v23799 (stack82)
        %v23807 = vmul.f32 %v23803, %v23763 (stack83)
        %v23811 = vadd.f32 %v23720, %v23807 (stack82)
        %v23815 = vmul.f32 %v23811, %v23763 (stack83)
        %v23819 = vadd.f32 %v23716, %v23815 (stack82)
        %v23823 = vmul.f32 %v23819, %v23763 (stack83)
        %v23827 = vadd.f32 %v23712, %v23823 (stack82)
        %v23831 = vmul.f32 %v23827, %v23678 (stack83)
        %v23835 = vsel /*vm=*/%vm23683, /*on_true_vy=*/%v23688, /*on_false_vx=*/%v23831 (stack72)
        %v23839 = vmul.f32 %v23835, 1.4140625 (stack83)
        %s23841 = scalar_lea.vmem %s280, 152 [#allocation0] (stack107)
        %v23842 = vpack.c.bf16 0.0, %v23839 (stack104)
        %23843 = vst [vmem:[%s23841] sm:$0xf] /*vst_source=*/%v23842 (stack105)
        %v23846 = vadd.s32 %v1381, %v22921 (stack65)
        %s23848 = smul.u32 128, %s27 (stack66)
        %v23849 = vlaneseq (stack67)
        %v23850 = vand.u32 %v23849, 127 (stack68)
        %v23851 = vstv %s23848 (stack69)
        %v23852 = vadd.s32 %v23850, %v23851 (stack70)
        %v23856 = vadd.s32 %v23846, %v23852 (stack65)
        %vm23860 = vcmp.lt.u32.totalorder %v23856, %v23846 (stack71)
        %vm23865 = vcmp.lt.u32.totalorder %v23846, %v1381 (stack71)
        %v23870 = vadd.s32 %v1368, %v22904 (stack65)
        %v23874 = vadd.s32 %v23870, 1 (stack65)
        %v23878 = vsel /*vm=*/%vm23865, /*on_true_vy=*/%v23874, /*on_false_vx=*/%v23870 (stack72)
        %v23882 = vadd.s32 %v23878, 1 (stack65)
        %v23886 = vsel /*vm=*/%vm23860, /*on_true_vy=*/%v23882, /*on_false_vx=*/%v23878 (stack72)
        %v23891 = vadd.s32 %v23886, %v10 (stack65)
        %v23895 = vadd.s32 %v23856, %v9 (stack65)
        %v23899 = vadd.s32 %v23891, %v23895 (stack65)
        %v23901 = vshll.u32 %v23895, 13 (stack73)
        %v23902 = vshrl.u32 %v23895, 19 (stack74)
        %v23903 = vor.u32 %v23901, %v23902 (stack75)
        %v23904 = vxor.u32 %v23899, %v23903 (stack76)
        %v23907 = vadd.s32 %v23899, %v23904 (stack65)
        %v23909 = vshll.u32 %v23904, 15 (stack73)
        %v23910 = vshrl.u32 %v23904, 17 (stack74)
        %v23911 = vor.u32 %v23909, %v23910 (stack75)
        %v23912 = vxor.u32 %v23907, %v23911 (stack76)
        %v23915 = vadd.s32 %v23907, %v23912 (stack65)
        %v23917 = vshll.u32 %v23912, 26 (stack73)
        %v23918 = vshrl.u32 %v23912, 6 (stack74)
        %v23919 = vor.u32 %v23917, %v23918 (stack75)
        %v23920 = vxor.u32 %v23915, %v23919 (stack76)
        %v23923 = vadd.s32 %v23915, %v23920 (stack65)
        %v23927 = vadd.s32 %v23923, %v9 (stack65)
        %v23929 = vshll.u32 %v23920, 6 (stack73)
        %v23930 = vshrl.u32 %v23920, 26 (stack74)
        %v23931 = vor.u32 %v23929, %v23930 (stack75)
        %v23932 = vxor.u32 %v23923, %v23931 (stack76)
        %v23935 = vadd.s32 %v23932, %v8 (stack65)
        %v23939 = vadd.s32 %v23935, 1 (stack65)
        %v23943 = vadd.s32 %v23927, %v23939 (stack65)
        %v23945 = vshll.u32 %v23939, 17 (stack73)
        %v23946 = vshrl.u32 %v23939, 15 (stack74)
        %v23947 = vor.u32 %v23945, %v23946 (stack75)
        %v23948 = vxor.u32 %v23943, %v23947 (stack76)
        %v23951 = vadd.s32 %v23943, %v23948 (stack65)
        %v23953 = vshll.u32 %v23948, 29 (stack73)
        %v23954 = vshrl.u32 %v23948, 3 (stack74)
        %v23955 = vor.u32 %v23953, %v23954 (stack75)
        %v23956 = vxor.u32 %v23951, %v23955 (stack76)
        %v23959 = vadd.s32 %v23951, %v23956 (stack65)
        %v23961 = vshll.u32 %v23956, 16 (stack73)
        %v23962 = vshrl.u32 %v23956, 16 (stack74)
        %v23963 = vor.u32 %v23961, %v23962 (stack75)
        %v23964 = vxor.u32 %v23959, %v23963 (stack76)
        %v23967 = vadd.s32 %v23959, %v23964 (stack65)
        %v23971 = vadd.s32 %v23967, %v8 (stack65)
        %v23973 = vshll.u32 %v23964, 24 (stack73)
        %v23974 = vshrl.u32 %v23964, 8 (stack74)
        %v23975 = vor.u32 %v23973, %v23974 (stack75)
        %v23976 = vxor.u32 %v23967, %v23975 (stack76)
        %v23979 = vadd.s32 %v23976, %v10 (stack65)
        %v23983 = vadd.s32 %v23979, 2 (stack65)
        %v23987 = vadd.s32 %v23971, %v23983 (stack65)
        %v23989 = vshll.u32 %v23983, 13 (stack73)
        %v23990 = vshrl.u32 %v23983, 19 (stack74)
        %v23991 = vor.u32 %v23989, %v23990 (stack75)
        %v23992 = vxor.u32 %v23987, %v23991 (stack76)
        %v23995 = vadd.s32 %v23987, %v23992 (stack65)
        %v23997 = vshll.u32 %v23992, 15 (stack73)
        %v23998 = vshrl.u32 %v23992, 17 (stack74)
        %v23999 = vor.u32 %v23997, %v23998 (stack75)
        %v24000 = vxor.u32 %v23995, %v23999 (stack76)
        %v24003 = vadd.s32 %v23995, %v24000 (stack65)
        %v24005 = vshll.u32 %v24000, 26 (stack73)
        %v24006 = vshrl.u32 %v24000, 6 (stack74)
        %v24007 = vor.u32 %v24005, %v24006 (stack75)
        %v24008 = vxor.u32 %v24003, %v24007 (stack76)
        %v24011 = vadd.s32 %v24003, %v24008 (stack65)
        %v24015 = vadd.s32 %v24011, %v10 (stack65)
        %v24017 = vshll.u32 %v24008, 6 (stack73)
        %v24018 = vshrl.u32 %v24008, 26 (stack74)
        %v24019 = vor.u32 %v24017, %v24018 (stack75)
        %v24020 = vxor.u32 %v24011, %v24019 (stack76)
        %v24023 = vadd.s32 %v24020, %v9 (stack65)
        %v24027 = vadd.s32 %v24023, 3 (stack65)
        %v24031 = vadd.s32 %v24015, %v24027 (stack65)
        %v24033 = vshll.u32 %v24027, 17 (stack73)
        %v24034 = vshrl.u32 %v24027, 15 (stack74)
        %v24035 = vor.u32 %v24033, %v24034 (stack75)
        %v24036 = vxor.u32 %v24031, %v24035 (stack76)
        %v24039 = vadd.s32 %v24031, %v24036 (stack65)
        %v24041 = vshll.u32 %v24036, 29 (stack73)
        %v24042 = vshrl.u32 %v24036, 3 (stack74)
        %v24043 = vor.u32 %v24041, %v24042 (stack75)
        %v24044 = vxor.u32 %v24039, %v24043 (stack76)
        %v24047 = vadd.s32 %v24039, %v24044 (stack65)
        %v24049 = vshll.u32 %v24044, 16 (stack73)
        %v24050 = vshrl.u32 %v24044, 16 (stack74)
        %v24051 = vor.u32 %v24049, %v24050 (stack75)
        %v24052 = vxor.u32 %v24047, %v24051 (stack76)
        %v24055 = vadd.s32 %v24047, %v24052 (stack65)
        %v24059 = vadd.s32 %v24055, %v9 (stack65)
        %v24061 = vshll.u32 %v24052, 24 (stack73)
        %v24062 = vshrl.u32 %v24052, 8 (stack74)
        %v24063 = vor.u32 %v24061, %v24062 (stack75)
        %v24064 = vxor.u32 %v24055, %v24063 (stack76)
        %v24067 = vadd.s32 %v24064, %v8 (stack65)
        %v24071 = vadd.s32 %v24067, 4 (stack65)
        %v24075 = vadd.s32 %v24059, %v24071 (stack65)
        %v24077 = vshll.u32 %v24071, 13 (stack73)
        %v24078 = vshrl.u32 %v24071, 19 (stack74)
        %v24079 = vor.u32 %v24077, %v24078 (stack75)
        %v24080 = vxor.u32 %v24075, %v24079 (stack76)
        %v24083 = vadd.s32 %v24075, %v24080 (stack65)
        %v24085 = vshll.u32 %v24080, 15 (stack73)
        %v24086 = vshrl.u32 %v24080, 17 (stack74)
        %v24087 = vor.u32 %v24085, %v24086 (stack75)
        %v24088 = vxor.u32 %v24083, %v24087 (stack76)
        %v24091 = vadd.s32 %v24083, %v24088 (stack65)
        %v24093 = vshll.u32 %v24088, 26 (stack73)
        %v24094 = vshrl.u32 %v24088, 6 (stack74)
        %v24095 = vor.u32 %v24093, %v24094 (stack75)
        %v24096 = vxor.u32 %v24091, %v24095 (stack76)
        %v24099 = vadd.s32 %v24091, %v24096 (stack65)
        %v24103 = vadd.s32 %v24099, %v8 (stack65)
        %v24105 = vshll.u32 %v24096, 6 (stack73)
        %v24106 = vshrl.u32 %v24096, 26 (stack74)
        %v24107 = vor.u32 %v24105, %v24106 (stack75)
        %v24108 = vxor.u32 %v24099, %v24107 (stack76)
        %v24111 = vadd.s32 %v24108, %v10 (stack65)
        %v24115 = vadd.s32 %v24111, 5 (stack65)
        %v24117 = vxor.u32 %v24103, %v24115 (stack76)
        %v24118 = vand.u32.u8 %v24117, 255 (stack77)
        %v24119 = vand.u32 %v24118, 65535 (stack78)
        %v24120 = vshrl.u32 %v24119, 1 (stack79)
        %v24121 = vor.u32 %v24120, 16256 (stack75)
        %v24122 = vand.u32.u16 %v24121, 65535 (stack80)
        %v24123 = vunpack.i.l.bf16 %v24122 (stack81)
        %v24127 = vadd.f32 %v24123, -1.0 (stack82)
        %v24131 = vmul.f32 %v24127, 2.0 (stack83)
        %v24135 = vadd.f32 %v24131, -0.99609375 (stack82)
        %v24139 = vmax.f32 -0.99609375, %v24135 (stack84)
        %v24141 = vand.u32 2147483647, %v24139 (stack85)
        %vm24144 = vcmp.eq.f32.partialorder %v24141, 1.0 (stack86)
        %v24149 = vmul.f32 %v24139, inf (stack83)
        %v24151 = vxor.u32 %v24139, 2147483648 (stack87)
        %v24154 = vmul.f32 %v24139, %v24151 (stack83)
        %v24156 = vadd.f32 %v24154, 1.0 (stack88)
        %v24157 = vlog2.pop %v24156 (stack89)
        %v24158 = vmul.f32 %v24157, 0.6931472 (stack90)
        %v24159 = vmul.f32 -0.5, %v24154 (stack91)
        %v24160 = vadd.f32 %v24159, 1.0 (stack92)
        %v24161 = vmul.f32 %v24160, %v24154 (stack93)
        %v24162 = vand.u32 2147483647, %v24154 (stack94)
        %vm24163 = vcmp.lt.f32.partialorder %v24162, 0.0004427343 (stack95)
        %v24164 = vsel /*vm=*/%vm24163, /*on_true_vy=*/%v24161, /*on_false_vx=*/%v24158 (stack96)
        %v24165 = vxor.u32 %v24164, 2147483648 (stack87)
        %vm24168 = vcmp.lt.f32.partialorder %v24165, 5.0 (stack86)
        %v24173 = vsel /*vm=*/%vm24168, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v24177 = vsel /*vm=*/%vm24168, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v24181 = vsel /*vm=*/%vm24168, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v24185 = vsel /*vm=*/%vm24168, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v24189 = vsel /*vm=*/%vm24168, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v24193 = vsel /*vm=*/%vm24168, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v24197 = vsel /*vm=*/%vm24168, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v24201 = vsel /*vm=*/%vm24168, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v24205 = vsel /*vm=*/%vm24168, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v24209 = vadd.f32 %v24165, -2.5 (stack82)
        %v24211 = vrsqrt.pop %v24165 (stack97)
        %v24212 = vmul.f32 %v24165, %v24211 (stack98)
        %vm24213 = vcmp.eq.f32.partialorder %v24165, inf (stack99)
        %v24214 = vsel /*vm=*/%vm24213, /*on_true_vy=*/%v24165, /*on_false_vx=*/%v24212 (stack100)
        %vm24215 = vcmp.eq.f32.partialorder %v24165, 0.0 (stack101)
        %v24216 = vand.u32 %v24165, 2147483648 (stack102)
        %v24217 = vsel /*vm=*/%vm24215, /*on_true_vy=*/%v24216, /*on_false_vx=*/%v24214 (stack103)
        %v24220 = vadd.f32 %v24217, -3.0 (stack82)
        %v24224 = vsel /*vm=*/%vm24168, /*on_true_vy=*/%v24209, /*on_false_vx=*/%v24220 (stack72)
        %v24228 = vmul.f32 %v24205, %v24224 (stack83)
        %v24232 = vadd.f32 %v24201, %v24228 (stack82)
        %v24236 = vmul.f32 %v24232, %v24224 (stack83)
        %v24240 = vadd.f32 %v24197, %v24236 (stack82)
        %v24244 = vmul.f32 %v24240, %v24224 (stack83)
        %v24248 = vadd.f32 %v24193, %v24244 (stack82)
        %v24252 = vmul.f32 %v24248, %v24224 (stack83)
        %v24256 = vadd.f32 %v24189, %v24252 (stack82)
        %v24260 = vmul.f32 %v24256, %v24224 (stack83)
        %v24264 = vadd.f32 %v24185, %v24260 (stack82)
        %v24268 = vmul.f32 %v24264, %v24224 (stack83)
        %v24272 = vadd.f32 %v24181, %v24268 (stack82)
        %v24276 = vmul.f32 %v24272, %v24224 (stack83)
        %v24280 = vadd.f32 %v24177, %v24276 (stack82)
        %v24284 = vmul.f32 %v24280, %v24224 (stack83)
        %v24288 = vadd.f32 %v24173, %v24284 (stack82)
        %v24292 = vmul.f32 %v24288, %v24139 (stack83)
        %v24296 = vsel /*vm=*/%vm24144, /*on_true_vy=*/%v24149, /*on_false_vx=*/%v24292 (stack72)
        %v24300 = vmul.f32 %v24296, 1.4140625 (stack83)
        %s24302 = scalar_lea.vmem %s280, 280 [#allocation0] (stack107)
        %v24303 = vpack.c.bf16 0.0, %v24300 (stack104)
        %24304 = vst [vmem:[%s24302] sm:$0xf] /*vst_source=*/%v24303 (stack105)
        %v24307 = vadd.s32 %v1868, %v22921 (stack65)
        %s24309 = smul.u32 128, %s27 (stack66)
        %v24310 = vlaneseq (stack67)
        %v24311 = vand.u32 %v24310, 127 (stack68)
        %v24312 = vstv %s24309 (stack69)
        %v24313 = vadd.s32 %v24311, %v24312 (stack70)
        %v24317 = vadd.s32 %v24307, %v24313 (stack65)
        %vm24321 = vcmp.lt.u32.totalorder %v24317, %v24307 (stack71)
        %vm24326 = vcmp.lt.u32.totalorder %v24307, %v1868 (stack71)
        %v24331 = vadd.s32 %v1855, %v22904 (stack65)
        %v24335 = vadd.s32 %v24331, 1 (stack65)
        %v24339 = vsel /*vm=*/%vm24326, /*on_true_vy=*/%v24335, /*on_false_vx=*/%v24331 (stack72)
        %v24343 = vadd.s32 %v24339, 1 (stack65)
        %v24347 = vsel /*vm=*/%vm24321, /*on_true_vy=*/%v24343, /*on_false_vx=*/%v24339 (stack72)
        %v24352 = vadd.s32 %v24347, %v10 (stack65)
        %v24356 = vadd.s32 %v24317, %v9 (stack65)
        %v24360 = vadd.s32 %v24352, %v24356 (stack65)
        %v24362 = vshll.u32 %v24356, 13 (stack73)
        %v24363 = vshrl.u32 %v24356, 19 (stack74)
        %v24364 = vor.u32 %v24362, %v24363 (stack75)
        %v24365 = vxor.u32 %v24360, %v24364 (stack76)
        %v24368 = vadd.s32 %v24360, %v24365 (stack65)
        %v24370 = vshll.u32 %v24365, 15 (stack73)
        %v24371 = vshrl.u32 %v24365, 17 (stack74)
        %v24372 = vor.u32 %v24370, %v24371 (stack75)
        %v24373 = vxor.u32 %v24368, %v24372 (stack76)
        %v24376 = vadd.s32 %v24368, %v24373 (stack65)
        %v24378 = vshll.u32 %v24373, 26 (stack73)
        %v24379 = vshrl.u32 %v24373, 6 (stack74)
        %v24380 = vor.u32 %v24378, %v24379 (stack75)
        %v24381 = vxor.u32 %v24376, %v24380 (stack76)
        %v24384 = vadd.s32 %v24376, %v24381 (stack65)
        %v24388 = vadd.s32 %v24384, %v9 (stack65)
        %v24390 = vshll.u32 %v24381, 6 (stack73)
        %v24391 = vshrl.u32 %v24381, 26 (stack74)
        %v24392 = vor.u32 %v24390, %v24391 (stack75)
        %v24393 = vxor.u32 %v24384, %v24392 (stack76)
        %v24396 = vadd.s32 %v24393, %v8 (stack65)
        %v24400 = vadd.s32 %v24396, 1 (stack65)
        %v24404 = vadd.s32 %v24388, %v24400 (stack65)
        %v24406 = vshll.u32 %v24400, 17 (stack73)
        %v24407 = vshrl.u32 %v24400, 15 (stack74)
        %v24408 = vor.u32 %v24406, %v24407 (stack75)
        %v24409 = vxor.u32 %v24404, %v24408 (stack76)
        %v24412 = vadd.s32 %v24404, %v24409 (stack65)
        %v24414 = vshll.u32 %v24409, 29 (stack73)
        %v24415 = vshrl.u32 %v24409, 3 (stack74)
        %v24416 = vor.u32 %v24414, %v24415 (stack75)
        %v24417 = vxor.u32 %v24412, %v24416 (stack76)
        %v24420 = vadd.s32 %v24412, %v24417 (stack65)
        %v24422 = vshll.u32 %v24417, 16 (stack73)
        %v24423 = vshrl.u32 %v24417, 16 (stack74)
        %v24424 = vor.u32 %v24422, %v24423 (stack75)
        %v24425 = vxor.u32 %v24420, %v24424 (stack76)
        %v24428 = vadd.s32 %v24420, %v24425 (stack65)
        %v24432 = vadd.s32 %v24428, %v8 (stack65)
        %v24434 = vshll.u32 %v24425, 24 (stack73)
        %v24435 = vshrl.u32 %v24425, 8 (stack74)
        %v24436 = vor.u32 %v24434, %v24435 (stack75)
        %v24437 = vxor.u32 %v24428, %v24436 (stack76)
        %v24440 = vadd.s32 %v24437, %v10 (stack65)
        %v24444 = vadd.s32 %v24440, 2 (stack65)
        %v24448 = vadd.s32 %v24432, %v24444 (stack65)
        %v24450 = vshll.u32 %v24444, 13 (stack73)
        %v24451 = vshrl.u32 %v24444, 19 (stack74)
        %v24452 = vor.u32 %v24450, %v24451 (stack75)
        %v24453 = vxor.u32 %v24448, %v24452 (stack76)
        %v24456 = vadd.s32 %v24448, %v24453 (stack65)
        %v24458 = vshll.u32 %v24453, 15 (stack73)
        %v24459 = vshrl.u32 %v24453, 17 (stack74)
        %v24460 = vor.u32 %v24458, %v24459 (stack75)
        %v24461 = vxor.u32 %v24456, %v24460 (stack76)
        %v24464 = vadd.s32 %v24456, %v24461 (stack65)
        %v24466 = vshll.u32 %v24461, 26 (stack73)
        %v24467 = vshrl.u32 %v24461, 6 (stack74)
        %v24468 = vor.u32 %v24466, %v24467 (stack75)
        %v24469 = vxor.u32 %v24464, %v24468 (stack76)
        %v24472 = vadd.s32 %v24464, %v24469 (stack65)
        %v24476 = vadd.s32 %v24472, %v10 (stack65)
        %v24478 = vshll.u32 %v24469, 6 (stack73)
        %v24479 = vshrl.u32 %v24469, 26 (stack74)
        %v24480 = vor.u32 %v24478, %v24479 (stack75)
        %v24481 = vxor.u32 %v24472, %v24480 (stack76)
        %v24484 = vadd.s32 %v24481, %v9 (stack65)
        %v24488 = vadd.s32 %v24484, 3 (stack65)
        %v24492 = vadd.s32 %v24476, %v24488 (stack65)
        %v24494 = vshll.u32 %v24488, 17 (stack73)
        %v24495 = vshrl.u32 %v24488, 15 (stack74)
        %v24496 = vor.u32 %v24494, %v24495 (stack75)
        %v24497 = vxor.u32 %v24492, %v24496 (stack76)
        %v24500 = vadd.s32 %v24492, %v24497 (stack65)
        %v24502 = vshll.u32 %v24497, 29 (stack73)
        %v24503 = vshrl.u32 %v24497, 3 (stack74)
        %v24504 = vor.u32 %v24502, %v24503 (stack75)
        %v24505 = vxor.u32 %v24500, %v24504 (stack76)
        %v24508 = vadd.s32 %v24500, %v24505 (stack65)
        %v24510 = vshll.u32 %v24505, 16 (stack73)
        %v24511 = vshrl.u32 %v24505, 16 (stack74)
        %v24512 = vor.u32 %v24510, %v24511 (stack75)
        %v24513 = vxor.u32 %v24508, %v24512 (stack76)
        %v24516 = vadd.s32 %v24508, %v24513 (stack65)
        %v24520 = vadd.s32 %v24516, %v9 (stack65)
        %v24522 = vshll.u32 %v24513, 24 (stack73)
        %v24523 = vshrl.u32 %v24513, 8 (stack74)
        %v24524 = vor.u32 %v24522, %v24523 (stack75)
        %v24525 = vxor.u32 %v24516, %v24524 (stack76)
        %v24528 = vadd.s32 %v24525, %v8 (stack65)
        %v24532 = vadd.s32 %v24528, 4 (stack65)
        %v24536 = vadd.s32 %v24520, %v24532 (stack65)
        %v24538 = vshll.u32 %v24532, 13 (stack73)
        %v24539 = vshrl.u32 %v24532, 19 (stack74)
        %v24540 = vor.u32 %v24538, %v24539 (stack75)
        %v24541 = vxor.u32 %v24536, %v24540 (stack76)
        %v24544 = vadd.s32 %v24536, %v24541 (stack65)
        %v24546 = vshll.u32 %v24541, 15 (stack73)
        %v24547 = vshrl.u32 %v24541, 17 (stack74)
        %v24548 = vor.u32 %v24546, %v24547 (stack75)
        %v24549 = vxor.u32 %v24544, %v24548 (stack76)
        %v24552 = vadd.s32 %v24544, %v24549 (stack65)
        %v24554 = vshll.u32 %v24549, 26 (stack73)
        %v24555 = vshrl.u32 %v24549, 6 (stack74)
        %v24556 = vor.u32 %v24554, %v24555 (stack75)
        %v24557 = vxor.u32 %v24552, %v24556 (stack76)
        %v24560 = vadd.s32 %v24552, %v24557 (stack65)
        %v24564 = vadd.s32 %v24560, %v8 (stack65)
        %v24566 = vshll.u32 %v24557, 6 (stack73)
        %v24567 = vshrl.u32 %v24557, 26 (stack74)
        %v24568 = vor.u32 %v24566, %v24567 (stack75)
        %v24569 = vxor.u32 %v24560, %v24568 (stack76)
        %v24572 = vadd.s32 %v24569, %v10 (stack65)
        %v24576 = vadd.s32 %v24572, 5 (stack65)
        %v24578 = vxor.u32 %v24564, %v24576 (stack76)
        %v24579 = vand.u32.u8 %v24578, 255 (stack77)
        %v24580 = vand.u32 %v24579, 65535 (stack78)
        %v24581 = vshrl.u32 %v24580, 1 (stack79)
        %v24582 = vor.u32 %v24581, 16256 (stack75)
        %v24583 = vand.u32.u16 %v24582, 65535 (stack80)
        %v24584 = vunpack.i.l.bf16 %v24583 (stack81)
        %v24588 = vadd.f32 %v24584, -1.0 (stack82)
        %v24592 = vmul.f32 %v24588, 2.0 (stack83)
        %v24596 = vadd.f32 %v24592, -0.99609375 (stack82)
        %v24600 = vmax.f32 -0.99609375, %v24596 (stack84)
        %v24602 = vand.u32 2147483647, %v24600 (stack85)
        %vm24605 = vcmp.eq.f32.partialorder %v24602, 1.0 (stack86)
        %v24610 = vmul.f32 %v24600, inf (stack83)
        %v24612 = vxor.u32 %v24600, 2147483648 (stack87)
        %v24615 = vmul.f32 %v24600, %v24612 (stack83)
        %v24617 = vadd.f32 %v24615, 1.0 (stack88)
        %v24618 = vlog2.pop %v24617 (stack89)
        %v24619 = vmul.f32 %v24618, 0.6931472 (stack90)
        %v24620 = vmul.f32 -0.5, %v24615 (stack91)
        %v24621 = vadd.f32 %v24620, 1.0 (stack92)
        %v24622 = vmul.f32 %v24621, %v24615 (stack93)
        %v24623 = vand.u32 2147483647, %v24615 (stack94)
        %vm24624 = vcmp.lt.f32.partialorder %v24623, 0.0004427343 (stack95)
        %v24625 = vsel /*vm=*/%vm24624, /*on_true_vy=*/%v24622, /*on_false_vx=*/%v24619 (stack96)
        %v24626 = vxor.u32 %v24625, 2147483648 (stack87)
        %vm24629 = vcmp.lt.f32.partialorder %v24626, 5.0 (stack86)
        %v24634 = vsel /*vm=*/%vm24629, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v24638 = vsel /*vm=*/%vm24629, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v24642 = vsel /*vm=*/%vm24629, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v24646 = vsel /*vm=*/%vm24629, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v24650 = vsel /*vm=*/%vm24629, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v24654 = vsel /*vm=*/%vm24629, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v24658 = vsel /*vm=*/%vm24629, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v24662 = vsel /*vm=*/%vm24629, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v24666 = vsel /*vm=*/%vm24629, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v24670 = vadd.f32 %v24626, -2.5 (stack82)
        %v24672 = vrsqrt.pop %v24626 (stack97)
        %v24673 = vmul.f32 %v24626, %v24672 (stack98)
        %vm24674 = vcmp.eq.f32.partialorder %v24626, inf (stack99)
        %v24675 = vsel /*vm=*/%vm24674, /*on_true_vy=*/%v24626, /*on_false_vx=*/%v24673 (stack100)
        %vm24676 = vcmp.eq.f32.partialorder %v24626, 0.0 (stack101)
        %v24677 = vand.u32 %v24626, 2147483648 (stack102)
        %v24678 = vsel /*vm=*/%vm24676, /*on_true_vy=*/%v24677, /*on_false_vx=*/%v24675 (stack103)
        %v24681 = vadd.f32 %v24678, -3.0 (stack82)
        %v24685 = vsel /*vm=*/%vm24629, /*on_true_vy=*/%v24670, /*on_false_vx=*/%v24681 (stack72)
        %v24689 = vmul.f32 %v24666, %v24685 (stack83)
        %v24693 = vadd.f32 %v24662, %v24689 (stack82)
        %v24697 = vmul.f32 %v24693, %v24685 (stack83)
        %v24701 = vadd.f32 %v24658, %v24697 (stack82)
        %v24705 = vmul.f32 %v24701, %v24685 (stack83)
        %v24709 = vadd.f32 %v24654, %v24705 (stack82)
        %v24713 = vmul.f32 %v24709, %v24685 (stack83)
        %v24717 = vadd.f32 %v24650, %v24713 (stack82)
        %v24721 = vmul.f32 %v24717, %v24685 (stack83)
        %v24725 = vadd.f32 %v24646, %v24721 (stack82)
        %v24729 = vmul.f32 %v24725, %v24685 (stack83)
        %v24733 = vadd.f32 %v24642, %v24729 (stack82)
        %v24737 = vmul.f32 %v24733, %v24685 (stack83)
        %v24741 = vadd.f32 %v24638, %v24737 (stack82)
        %v24745 = vmul.f32 %v24741, %v24685 (stack83)
        %v24749 = vadd.f32 %v24634, %v24745 (stack82)
        %v24753 = vmul.f32 %v24749, %v24600 (stack83)
        %v24757 = vsel /*vm=*/%vm24605, /*on_true_vy=*/%v24610, /*on_false_vx=*/%v24753 (stack72)
        %v24761 = vmul.f32 %v24757, 1.4140625 (stack83)
        %s24763 = scalar_lea.vmem %s280, 408 [#allocation0] (stack107)
        %v24764 = vpack.c.bf16 0.0, %v24761 (stack104)
        %24765 = vst [vmem:[%s24763] sm:$0xf] /*vst_source=*/%v24764 (stack105)
        %v24768 = vadd.s32 %v2355, %v22921 (stack65)
        %s24770 = smul.u32 128, %s27 (stack66)
        %v24771 = vlaneseq (stack67)
        %v24772 = vand.u32 %v24771, 127 (stack68)
        %v24773 = vstv %s24770 (stack69)
        %v24774 = vadd.s32 %v24772, %v24773 (stack70)
        %v24778 = vadd.s32 %v24768, %v24774 (stack65)
        %vm24782 = vcmp.lt.u32.totalorder %v24778, %v24768 (stack71)
        %vm24787 = vcmp.lt.u32.totalorder %v24768, %v2355 (stack71)
        %v24792 = vadd.s32 %v2342, %v22904 (stack65)
        %v24796 = vadd.s32 %v24792, 1 (stack65)
        %v24800 = vsel /*vm=*/%vm24787, /*on_true_vy=*/%v24796, /*on_false_vx=*/%v24792 (stack72)
        %v24804 = vadd.s32 %v24800, 1 (stack65)
        %v24808 = vsel /*vm=*/%vm24782, /*on_true_vy=*/%v24804, /*on_false_vx=*/%v24800 (stack72)
        %v24813 = vadd.s32 %v24808, %v10 (stack65)
        %v24817 = vadd.s32 %v24778, %v9 (stack65)
        %v24821 = vadd.s32 %v24813, %v24817 (stack65)
        %v24823 = vshll.u32 %v24817, 13 (stack73)
        %v24824 = vshrl.u32 %v24817, 19 (stack74)
        %v24825 = vor.u32 %v24823, %v24824 (stack75)
        %v24826 = vxor.u32 %v24821, %v24825 (stack76)
        %v24829 = vadd.s32 %v24821, %v24826 (stack65)
        %v24831 = vshll.u32 %v24826, 15 (stack73)
        %v24832 = vshrl.u32 %v24826, 17 (stack74)
        %v24833 = vor.u32 %v24831, %v24832 (stack75)
        %v24834 = vxor.u32 %v24829, %v24833 (stack76)
        %v24837 = vadd.s32 %v24829, %v24834 (stack65)
        %v24839 = vshll.u32 %v24834, 26 (stack73)
        %v24840 = vshrl.u32 %v24834, 6 (stack74)
        %v24841 = vor.u32 %v24839, %v24840 (stack75)
        %v24842 = vxor.u32 %v24837, %v24841 (stack76)
        %v24845 = vadd.s32 %v24837, %v24842 (stack65)
        %v24849 = vadd.s32 %v24845, %v9 (stack65)
        %v24851 = vshll.u32 %v24842, 6 (stack73)
        %v24852 = vshrl.u32 %v24842, 26 (stack74)
        %v24853 = vor.u32 %v24851, %v24852 (stack75)
        %v24854 = vxor.u32 %v24845, %v24853 (stack76)
        %v24857 = vadd.s32 %v24854, %v8 (stack65)
        %v24861 = vadd.s32 %v24857, 1 (stack65)
        %v24865 = vadd.s32 %v24849, %v24861 (stack65)
        %v24867 = vshll.u32 %v24861, 17 (stack73)
        %v24868 = vshrl.u32 %v24861, 15 (stack74)
        %v24869 = vor.u32 %v24867, %v24868 (stack75)
        %v24870 = vxor.u32 %v24865, %v24869 (stack76)
        %v24873 = vadd.s32 %v24865, %v24870 (stack65)
        %v24875 = vshll.u32 %v24870, 29 (stack73)
        %v24876 = vshrl.u32 %v24870, 3 (stack74)
        %v24877 = vor.u32 %v24875, %v24876 (stack75)
        %v24878 = vxor.u32 %v24873, %v24877 (stack76)
        %v24881 = vadd.s32 %v24873, %v24878 (stack65)
        %v24883 = vshll.u32 %v24878, 16 (stack73)
        %v24884 = vshrl.u32 %v24878, 16 (stack74)
        %v24885 = vor.u32 %v24883, %v24884 (stack75)
        %v24886 = vxor.u32 %v24881, %v24885 (stack76)
        %v24889 = vadd.s32 %v24881, %v24886 (stack65)
        %v24893 = vadd.s32 %v24889, %v8 (stack65)
        %v24895 = vshll.u32 %v24886, 24 (stack73)
        %v24896 = vshrl.u32 %v24886, 8 (stack74)
        %v24897 = vor.u32 %v24895, %v24896 (stack75)
        %v24898 = vxor.u32 %v24889, %v24897 (stack76)
        %v24901 = vadd.s32 %v24898, %v10 (stack65)
        %v24905 = vadd.s32 %v24901, 2 (stack65)
        %v24909 = vadd.s32 %v24893, %v24905 (stack65)
        %v24911 = vshll.u32 %v24905, 13 (stack73)
        %v24912 = vshrl.u32 %v24905, 19 (stack74)
        %v24913 = vor.u32 %v24911, %v24912 (stack75)
        %v24914 = vxor.u32 %v24909, %v24913 (stack76)
        %v24917 = vadd.s32 %v24909, %v24914 (stack65)
        %v24919 = vshll.u32 %v24914, 15 (stack73)
        %v24920 = vshrl.u32 %v24914, 17 (stack74)
        %v24921 = vor.u32 %v24919, %v24920 (stack75)
        %v24922 = vxor.u32 %v24917, %v24921 (stack76)
        %v24925 = vadd.s32 %v24917, %v24922 (stack65)
        %v24927 = vshll.u32 %v24922, 26 (stack73)
        %v24928 = vshrl.u32 %v24922, 6 (stack74)
        %v24929 = vor.u32 %v24927, %v24928 (stack75)
        %v24930 = vxor.u32 %v24925, %v24929 (stack76)
        %v24933 = vadd.s32 %v24925, %v24930 (stack65)
        %v24937 = vadd.s32 %v24933, %v10 (stack65)
        %v24939 = vshll.u32 %v24930, 6 (stack73)
        %v24940 = vshrl.u32 %v24930, 26 (stack74)
        %v24941 = vor.u32 %v24939, %v24940 (stack75)
        %v24942 = vxor.u32 %v24933, %v24941 (stack76)
        %v24945 = vadd.s32 %v24942, %v9 (stack65)
        %v24949 = vadd.s32 %v24945, 3 (stack65)
        %v24953 = vadd.s32 %v24937, %v24949 (stack65)
        %v24955 = vshll.u32 %v24949, 17 (stack73)
        %v24956 = vshrl.u32 %v24949, 15 (stack74)
        %v24957 = vor.u32 %v24955, %v24956 (stack75)
        %v24958 = vxor.u32 %v24953, %v24957 (stack76)
        %v24961 = vadd.s32 %v24953, %v24958 (stack65)
        %v24963 = vshll.u32 %v24958, 29 (stack73)
        %v24964 = vshrl.u32 %v24958, 3 (stack74)
        %v24965 = vor.u32 %v24963, %v24964 (stack75)
        %v24966 = vxor.u32 %v24961, %v24965 (stack76)
        %v24969 = vadd.s32 %v24961, %v24966 (stack65)
        %v24971 = vshll.u32 %v24966, 16 (stack73)
        %v24972 = vshrl.u32 %v24966, 16 (stack74)
        %v24973 = vor.u32 %v24971, %v24972 (stack75)
        %v24974 = vxor.u32 %v24969, %v24973 (stack76)
        %v24977 = vadd.s32 %v24969, %v24974 (stack65)
        %v24981 = vadd.s32 %v24977, %v9 (stack65)
        %v24983 = vshll.u32 %v24974, 24 (stack73)
        %v24984 = vshrl.u32 %v24974, 8 (stack74)
        %v24985 = vor.u32 %v24983, %v24984 (stack75)
        %v24986 = vxor.u32 %v24977, %v24985 (stack76)
        %v24989 = vadd.s32 %v24986, %v8 (stack65)
        %v24993 = vadd.s32 %v24989, 4 (stack65)
        %v24997 = vadd.s32 %v24981, %v24993 (stack65)
        %v24999 = vshll.u32 %v24993, 13 (stack73)
        %v25000 = vshrl.u32 %v24993, 19 (stack74)
        %v25001 = vor.u32 %v24999, %v25000 (stack75)
        %v25002 = vxor.u32 %v24997, %v25001 (stack76)
        %v25005 = vadd.s32 %v24997, %v25002 (stack65)
        %v25007 = vshll.u32 %v25002, 15 (stack73)
        %v25008 = vshrl.u32 %v25002, 17 (stack74)
        %v25009 = vor.u32 %v25007, %v25008 (stack75)
        %v25010 = vxor.u32 %v25005, %v25009 (stack76)
        %v25013 = vadd.s32 %v25005, %v25010 (stack65)
        %v25015 = vshll.u32 %v25010, 26 (stack73)
        %v25016 = vshrl.u32 %v25010, 6 (stack74)
        %v25017 = vor.u32 %v25015, %v25016 (stack75)
        %v25018 = vxor.u32 %v25013, %v25017 (stack76)
        %v25021 = vadd.s32 %v25013, %v25018 (stack65)
        %v25025 = vadd.s32 %v25021, %v8 (stack65)
        %v25027 = vshll.u32 %v25018, 6 (stack73)
        %v25028 = vshrl.u32 %v25018, 26 (stack74)
        %v25029 = vor.u32 %v25027, %v25028 (stack75)
        %v25030 = vxor.u32 %v25021, %v25029 (stack76)
        %v25033 = vadd.s32 %v25030, %v10 (stack65)
        %v25037 = vadd.s32 %v25033, 5 (stack65)
        %v25039 = vxor.u32 %v25025, %v25037 (stack76)
        %v25040 = vand.u32.u8 %v25039, 255 (stack77)
        %v25041 = vand.u32 %v25040, 65535 (stack78)
        %v25042 = vshrl.u32 %v25041, 1 (stack79)
        %v25043 = vor.u32 %v25042, 16256 (stack75)
        %v25044 = vand.u32.u16 %v25043, 65535 (stack80)
        %v25045 = vunpack.i.l.bf16 %v25044 (stack81)
        %v25049 = vadd.f32 %v25045, -1.0 (stack82)
        %v25053 = vmul.f32 %v25049, 2.0 (stack83)
        %v25057 = vadd.f32 %v25053, -0.99609375 (stack82)
        %v25061 = vmax.f32 -0.99609375, %v25057 (stack84)
        %v25063 = vand.u32 2147483647, %v25061 (stack85)
        %vm25066 = vcmp.eq.f32.partialorder %v25063, 1.0 (stack86)
        %v25071 = vmul.f32 %v25061, inf (stack83)
        %v25073 = vxor.u32 %v25061, 2147483648 (stack87)
        %v25076 = vmul.f32 %v25061, %v25073 (stack83)
        %v25078 = vadd.f32 %v25076, 1.0 (stack88)
        %v25079 = vlog2.pop %v25078 (stack89)
        %v25080 = vmul.f32 %v25079, 0.6931472 (stack90)
        %v25081 = vmul.f32 -0.5, %v25076 (stack91)
        %v25082 = vadd.f32 %v25081, 1.0 (stack92)
        %v25083 = vmul.f32 %v25082, %v25076 (stack93)
        %v25084 = vand.u32 2147483647, %v25076 (stack94)
        %vm25085 = vcmp.lt.f32.partialorder %v25084, 0.0004427343 (stack95)
        %v25086 = vsel /*vm=*/%vm25085, /*on_true_vy=*/%v25083, /*on_false_vx=*/%v25080 (stack96)
        %v25087 = vxor.u32 %v25086, 2147483648 (stack87)
        %vm25090 = vcmp.lt.f32.partialorder %v25087, 5.0 (stack86)
        %v25095 = vsel /*vm=*/%vm25090, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v25099 = vsel /*vm=*/%vm25090, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v25103 = vsel /*vm=*/%vm25090, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v25107 = vsel /*vm=*/%vm25090, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v25111 = vsel /*vm=*/%vm25090, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v25115 = vsel /*vm=*/%vm25090, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v25119 = vsel /*vm=*/%vm25090, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v25123 = vsel /*vm=*/%vm25090, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v25127 = vsel /*vm=*/%vm25090, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v25131 = vadd.f32 %v25087, -2.5 (stack82)
        %v25133 = vrsqrt.pop %v25087 (stack97)
        %v25134 = vmul.f32 %v25087, %v25133 (stack98)
        %vm25135 = vcmp.eq.f32.partialorder %v25087, inf (stack99)
        %v25136 = vsel /*vm=*/%vm25135, /*on_true_vy=*/%v25087, /*on_false_vx=*/%v25134 (stack100)
        %vm25137 = vcmp.eq.f32.partialorder %v25087, 0.0 (stack101)
        %v25138 = vand.u32 %v25087, 2147483648 (stack102)
        %v25139 = vsel /*vm=*/%vm25137, /*on_true_vy=*/%v25138, /*on_false_vx=*/%v25136 (stack103)
        %v25142 = vadd.f32 %v25139, -3.0 (stack82)
        %v25146 = vsel /*vm=*/%vm25090, /*on_true_vy=*/%v25131, /*on_false_vx=*/%v25142 (stack72)
        %v25150 = vmul.f32 %v25127, %v25146 (stack83)
        %v25154 = vadd.f32 %v25123, %v25150 (stack82)
        %v25158 = vmul.f32 %v25154, %v25146 (stack83)
        %v25162 = vadd.f32 %v25119, %v25158 (stack82)
        %v25166 = vmul.f32 %v25162, %v25146 (stack83)
        %v25170 = vadd.f32 %v25115, %v25166 (stack82)
        %v25174 = vmul.f32 %v25170, %v25146 (stack83)
        %v25178 = vadd.f32 %v25111, %v25174 (stack82)
        %v25182 = vmul.f32 %v25178, %v25146 (stack83)
        %v25186 = vadd.f32 %v25107, %v25182 (stack82)
        %v25190 = vmul.f32 %v25186, %v25146 (stack83)
        %v25194 = vadd.f32 %v25103, %v25190 (stack82)
        %v25198 = vmul.f32 %v25194, %v25146 (stack83)
        %v25202 = vadd.f32 %v25099, %v25198 (stack82)
        %v25206 = vmul.f32 %v25202, %v25146 (stack83)
        %v25210 = vadd.f32 %v25095, %v25206 (stack82)
        %v25214 = vmul.f32 %v25210, %v25061 (stack83)
        %v25218 = vsel /*vm=*/%vm25066, /*on_true_vy=*/%v25071, /*on_false_vx=*/%v25214 (stack72)
        %v25222 = vmul.f32 %v25218, 1.4140625 (stack83)
        %s25224 = scalar_lea.vmem %s280, 536 [#allocation0] (stack107)
        %v25225 = vpack.c.bf16 0.0, %v25222 (stack104)
        %25226 = vst [vmem:[%s25224] sm:$0xf] /*vst_source=*/%v25225 (stack105)
        %v25229 = vadd.s32 %v2842, %v22921 (stack65)
        %s25231 = smul.u32 128, %s27 (stack66)
        %v25232 = vlaneseq (stack67)
        %v25233 = vand.u32 %v25232, 127 (stack68)
        %v25234 = vstv %s25231 (stack69)
        %v25235 = vadd.s32 %v25233, %v25234 (stack70)
        %v25239 = vadd.s32 %v25229, %v25235 (stack65)
        %vm25243 = vcmp.lt.u32.totalorder %v25239, %v25229 (stack71)
        %vm25248 = vcmp.lt.u32.totalorder %v25229, %v2842 (stack71)
        %v25253 = vadd.s32 %v2829, %v22904 (stack65)
        %v25257 = vadd.s32 %v25253, 1 (stack65)
        %v25261 = vsel /*vm=*/%vm25248, /*on_true_vy=*/%v25257, /*on_false_vx=*/%v25253 (stack72)
        %v25265 = vadd.s32 %v25261, 1 (stack65)
        %v25269 = vsel /*vm=*/%vm25243, /*on_true_vy=*/%v25265, /*on_false_vx=*/%v25261 (stack72)
        %v25274 = vadd.s32 %v25269, %v10 (stack65)
        %v25278 = vadd.s32 %v25239, %v9 (stack65)
        %v25282 = vadd.s32 %v25274, %v25278 (stack65)
        %v25284 = vshll.u32 %v25278, 13 (stack73)
        %v25285 = vshrl.u32 %v25278, 19 (stack74)
        %v25286 = vor.u32 %v25284, %v25285 (stack75)
        %v25287 = vxor.u32 %v25282, %v25286 (stack76)
        %v25290 = vadd.s32 %v25282, %v25287 (stack65)
        %v25292 = vshll.u32 %v25287, 15 (stack73)
        %v25293 = vshrl.u32 %v25287, 17 (stack74)
        %v25294 = vor.u32 %v25292, %v25293 (stack75)
        %v25295 = vxor.u32 %v25290, %v25294 (stack76)
        %v25298 = vadd.s32 %v25290, %v25295 (stack65)
        %v25300 = vshll.u32 %v25295, 26 (stack73)
        %v25301 = vshrl.u32 %v25295, 6 (stack74)
        %v25302 = vor.u32 %v25300, %v25301 (stack75)
        %v25303 = vxor.u32 %v25298, %v25302 (stack76)
        %v25306 = vadd.s32 %v25298, %v25303 (stack65)
        %v25310 = vadd.s32 %v25306, %v9 (stack65)
        %v25312 = vshll.u32 %v25303, 6 (stack73)
        %v25313 = vshrl.u32 %v25303, 26 (stack74)
        %v25314 = vor.u32 %v25312, %v25313 (stack75)
        %v25315 = vxor.u32 %v25306, %v25314 (stack76)
        %v25318 = vadd.s32 %v25315, %v8 (stack65)
        %v25322 = vadd.s32 %v25318, 1 (stack65)
        %v25326 = vadd.s32 %v25310, %v25322 (stack65)
        %v25328 = vshll.u32 %v25322, 17 (stack73)
        %v25329 = vshrl.u32 %v25322, 15 (stack74)
        %v25330 = vor.u32 %v25328, %v25329 (stack75)
        %v25331 = vxor.u32 %v25326, %v25330 (stack76)
        %v25334 = vadd.s32 %v25326, %v25331 (stack65)
        %v25336 = vshll.u32 %v25331, 29 (stack73)
        %v25337 = vshrl.u32 %v25331, 3 (stack74)
        %v25338 = vor.u32 %v25336, %v25337 (stack75)
        %v25339 = vxor.u32 %v25334, %v25338 (stack76)
        %v25342 = vadd.s32 %v25334, %v25339 (stack65)
        %v25344 = vshll.u32 %v25339, 16 (stack73)
        %v25345 = vshrl.u32 %v25339, 16 (stack74)
        %v25346 = vor.u32 %v25344, %v25345 (stack75)
        %v25347 = vxor.u32 %v25342, %v25346 (stack76)
        %v25350 = vadd.s32 %v25342, %v25347 (stack65)
        %v25354 = vadd.s32 %v25350, %v8 (stack65)
        %v25356 = vshll.u32 %v25347, 24 (stack73)
        %v25357 = vshrl.u32 %v25347, 8 (stack74)
        %v25358 = vor.u32 %v25356, %v25357 (stack75)
        %v25359 = vxor.u32 %v25350, %v25358 (stack76)
        %v25362 = vadd.s32 %v25359, %v10 (stack65)
        %v25366 = vadd.s32 %v25362, 2 (stack65)
        %v25370 = vadd.s32 %v25354, %v25366 (stack65)
        %v25372 = vshll.u32 %v25366, 13 (stack73)
        %v25373 = vshrl.u32 %v25366, 19 (stack74)
        %v25374 = vor.u32 %v25372, %v25373 (stack75)
        %v25375 = vxor.u32 %v25370, %v25374 (stack76)
        %v25378 = vadd.s32 %v25370, %v25375 (stack65)
        %v25380 = vshll.u32 %v25375, 15 (stack73)
        %v25381 = vshrl.u32 %v25375, 17 (stack74)
        %v25382 = vor.u32 %v25380, %v25381 (stack75)
        %v25383 = vxor.u32 %v25378, %v25382 (stack76)
        %v25386 = vadd.s32 %v25378, %v25383 (stack65)
        %v25388 = vshll.u32 %v25383, 26 (stack73)
        %v25389 = vshrl.u32 %v25383, 6 (stack74)
        %v25390 = vor.u32 %v25388, %v25389 (stack75)
        %v25391 = vxor.u32 %v25386, %v25390 (stack76)
        %v25394 = vadd.s32 %v25386, %v25391 (stack65)
        %v25398 = vadd.s32 %v25394, %v10 (stack65)
        %v25400 = vshll.u32 %v25391, 6 (stack73)
        %v25401 = vshrl.u32 %v25391, 26 (stack74)
        %v25402 = vor.u32 %v25400, %v25401 (stack75)
        %v25403 = vxor.u32 %v25394, %v25402 (stack76)
        %v25406 = vadd.s32 %v25403, %v9 (stack65)
        %v25410 = vadd.s32 %v25406, 3 (stack65)
        %v25414 = vadd.s32 %v25398, %v25410 (stack65)
        %v25416 = vshll.u32 %v25410, 17 (stack73)
        %v25417 = vshrl.u32 %v25410, 15 (stack74)
        %v25418 = vor.u32 %v25416, %v25417 (stack75)
        %v25419 = vxor.u32 %v25414, %v25418 (stack76)
        %v25422 = vadd.s32 %v25414, %v25419 (stack65)
        %v25424 = vshll.u32 %v25419, 29 (stack73)
        %v25425 = vshrl.u32 %v25419, 3 (stack74)
        %v25426 = vor.u32 %v25424, %v25425 (stack75)
        %v25427 = vxor.u32 %v25422, %v25426 (stack76)
        %v25430 = vadd.s32 %v25422, %v25427 (stack65)
        %v25432 = vshll.u32 %v25427, 16 (stack73)
        %v25433 = vshrl.u32 %v25427, 16 (stack74)
        %v25434 = vor.u32 %v25432, %v25433 (stack75)
        %v25435 = vxor.u32 %v25430, %v25434 (stack76)
        %v25438 = vadd.s32 %v25430, %v25435 (stack65)
        %v25442 = vadd.s32 %v25438, %v9 (stack65)
        %v25444 = vshll.u32 %v25435, 24 (stack73)
        %v25445 = vshrl.u32 %v25435, 8 (stack74)
        %v25446 = vor.u32 %v25444, %v25445 (stack75)
        %v25447 = vxor.u32 %v25438, %v25446 (stack76)
        %v25450 = vadd.s32 %v25447, %v8 (stack65)
        %v25454 = vadd.s32 %v25450, 4 (stack65)
        %v25458 = vadd.s32 %v25442, %v25454 (stack65)
        %v25460 = vshll.u32 %v25454, 13 (stack73)
        %v25461 = vshrl.u32 %v25454, 19 (stack74)
        %v25462 = vor.u32 %v25460, %v25461 (stack75)
        %v25463 = vxor.u32 %v25458, %v25462 (stack76)
        %v25466 = vadd.s32 %v25458, %v25463 (stack65)
        %v25468 = vshll.u32 %v25463, 15 (stack73)
        %v25469 = vshrl.u32 %v25463, 17 (stack74)
        %v25470 = vor.u32 %v25468, %v25469 (stack75)
        %v25471 = vxor.u32 %v25466, %v25470 (stack76)
        %v25474 = vadd.s32 %v25466, %v25471 (stack65)
        %v25476 = vshll.u32 %v25471, 26 (stack73)
        %v25477 = vshrl.u32 %v25471, 6 (stack74)
        %v25478 = vor.u32 %v25476, %v25477 (stack75)
        %v25479 = vxor.u32 %v25474, %v25478 (stack76)
        %v25482 = vadd.s32 %v25474, %v25479 (stack65)
        %v25486 = vadd.s32 %v25482, %v8 (stack65)
        %v25488 = vshll.u32 %v25479, 6 (stack73)
        %v25489 = vshrl.u32 %v25479, 26 (stack74)
        %v25490 = vor.u32 %v25488, %v25489 (stack75)
        %v25491 = vxor.u32 %v25482, %v25490 (stack76)
        %v25494 = vadd.s32 %v25491, %v10 (stack65)
        %v25498 = vadd.s32 %v25494, 5 (stack65)
        %v25500 = vxor.u32 %v25486, %v25498 (stack76)
        %v25501 = vand.u32.u8 %v25500, 255 (stack77)
        %v25502 = vand.u32 %v25501, 65535 (stack78)
        %v25503 = vshrl.u32 %v25502, 1 (stack79)
        %v25504 = vor.u32 %v25503, 16256 (stack75)
        %v25505 = vand.u32.u16 %v25504, 65535 (stack80)
        %v25506 = vunpack.i.l.bf16 %v25505 (stack81)
        %v25510 = vadd.f32 %v25506, -1.0 (stack82)
        %v25514 = vmul.f32 %v25510, 2.0 (stack83)
        %v25518 = vadd.f32 %v25514, -0.99609375 (stack82)
        %v25522 = vmax.f32 -0.99609375, %v25518 (stack84)
        %v25524 = vand.u32 2147483647, %v25522 (stack85)
        %vm25527 = vcmp.eq.f32.partialorder %v25524, 1.0 (stack86)
        %v25532 = vmul.f32 %v25522, inf (stack83)
        %v25534 = vxor.u32 %v25522, 2147483648 (stack87)
        %v25537 = vmul.f32 %v25522, %v25534 (stack83)
        %v25539 = vadd.f32 %v25537, 1.0 (stack88)
        %v25540 = vlog2.pop %v25539 (stack89)
        %v25541 = vmul.f32 %v25540, 0.6931472 (stack90)
        %v25542 = vmul.f32 -0.5, %v25537 (stack91)
        %v25543 = vadd.f32 %v25542, 1.0 (stack92)
        %v25544 = vmul.f32 %v25543, %v25537 (stack93)
        %v25545 = vand.u32 2147483647, %v25537 (stack94)
        %vm25546 = vcmp.lt.f32.partialorder %v25545, 0.0004427343 (stack95)
        %v25547 = vsel /*vm=*/%vm25546, /*on_true_vy=*/%v25544, /*on_false_vx=*/%v25541 (stack96)
        %v25548 = vxor.u32 %v25547, 2147483648 (stack87)
        %vm25551 = vcmp.lt.f32.partialorder %v25548, 5.0 (stack86)
        %v25556 = vsel /*vm=*/%vm25551, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v25560 = vsel /*vm=*/%vm25551, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v25564 = vsel /*vm=*/%vm25551, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v25568 = vsel /*vm=*/%vm25551, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v25572 = vsel /*vm=*/%vm25551, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v25576 = vsel /*vm=*/%vm25551, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v25580 = vsel /*vm=*/%vm25551, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v25584 = vsel /*vm=*/%vm25551, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v25588 = vsel /*vm=*/%vm25551, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v25592 = vadd.f32 %v25548, -2.5 (stack82)
        %v25594 = vrsqrt.pop %v25548 (stack97)
        %v25595 = vmul.f32 %v25548, %v25594 (stack98)
        %vm25596 = vcmp.eq.f32.partialorder %v25548, inf (stack99)
        %v25597 = vsel /*vm=*/%vm25596, /*on_true_vy=*/%v25548, /*on_false_vx=*/%v25595 (stack100)
        %vm25598 = vcmp.eq.f32.partialorder %v25548, 0.0 (stack101)
        %v25599 = vand.u32 %v25548, 2147483648 (stack102)
        %v25600 = vsel /*vm=*/%vm25598, /*on_true_vy=*/%v25599, /*on_false_vx=*/%v25597 (stack103)
        %v25603 = vadd.f32 %v25600, -3.0 (stack82)
        %v25607 = vsel /*vm=*/%vm25551, /*on_true_vy=*/%v25592, /*on_false_vx=*/%v25603 (stack72)
        %v25611 = vmul.f32 %v25588, %v25607 (stack83)
        %v25615 = vadd.f32 %v25584, %v25611 (stack82)
        %v25619 = vmul.f32 %v25615, %v25607 (stack83)
        %v25623 = vadd.f32 %v25580, %v25619 (stack82)
        %v25627 = vmul.f32 %v25623, %v25607 (stack83)
        %v25631 = vadd.f32 %v25576, %v25627 (stack82)
        %v25635 = vmul.f32 %v25631, %v25607 (stack83)
        %v25639 = vadd.f32 %v25572, %v25635 (stack82)
        %v25643 = vmul.f32 %v25639, %v25607 (stack83)
        %v25647 = vadd.f32 %v25568, %v25643 (stack82)
        %v25651 = vmul.f32 %v25647, %v25607 (stack83)
        %v25655 = vadd.f32 %v25564, %v25651 (stack82)
        %v25659 = vmul.f32 %v25655, %v25607 (stack83)
        %v25663 = vadd.f32 %v25560, %v25659 (stack82)
        %v25667 = vmul.f32 %v25663, %v25607 (stack83)
        %v25671 = vadd.f32 %v25556, %v25667 (stack82)
        %v25675 = vmul.f32 %v25671, %v25522 (stack83)
        %v25679 = vsel /*vm=*/%vm25527, /*on_true_vy=*/%v25532, /*on_false_vx=*/%v25675 (stack72)
        %v25683 = vmul.f32 %v25679, 1.4140625 (stack83)
        %s25685 = scalar_lea.vmem %s280, 664 [#allocation0] (stack107)
        %v25686 = vpack.c.bf16 0.0, %v25683 (stack104)
        %25687 = vst [vmem:[%s25685] sm:$0xf] /*vst_source=*/%v25686 (stack105)
        %v25690 = vadd.s32 %v3329, %v22921 (stack65)
        %s25692 = smul.u32 128, %s27 (stack66)
        %v25693 = vlaneseq (stack67)
        %v25694 = vand.u32 %v25693, 127 (stack68)
        %v25695 = vstv %s25692 (stack69)
        %v25696 = vadd.s32 %v25694, %v25695 (stack70)
        %v25700 = vadd.s32 %v25690, %v25696 (stack65)
        %vm25704 = vcmp.lt.u32.totalorder %v25700, %v25690 (stack71)
        %vm25709 = vcmp.lt.u32.totalorder %v25690, %v3329 (stack71)
        %v25714 = vadd.s32 %v3316, %v22904 (stack65)
        %v25718 = vadd.s32 %v25714, 1 (stack65)
        %v25722 = vsel /*vm=*/%vm25709, /*on_true_vy=*/%v25718, /*on_false_vx=*/%v25714 (stack72)
        %v25726 = vadd.s32 %v25722, 1 (stack65)
        %v25730 = vsel /*vm=*/%vm25704, /*on_true_vy=*/%v25726, /*on_false_vx=*/%v25722 (stack72)
        %v25735 = vadd.s32 %v25730, %v10 (stack65)
        %v25739 = vadd.s32 %v25700, %v9 (stack65)
        %v25743 = vadd.s32 %v25735, %v25739 (stack65)
        %v25745 = vshll.u32 %v25739, 13 (stack73)
        %v25746 = vshrl.u32 %v25739, 19 (stack74)
        %v25747 = vor.u32 %v25745, %v25746 (stack75)
        %v25748 = vxor.u32 %v25743, %v25747 (stack76)
        %v25751 = vadd.s32 %v25743, %v25748 (stack65)
        %v25753 = vshll.u32 %v25748, 15 (stack73)
        %v25754 = vshrl.u32 %v25748, 17 (stack74)
        %v25755 = vor.u32 %v25753, %v25754 (stack75)
        %v25756 = vxor.u32 %v25751, %v25755 (stack76)
        %v25759 = vadd.s32 %v25751, %v25756 (stack65)
        %v25761 = vshll.u32 %v25756, 26 (stack73)
        %v25762 = vshrl.u32 %v25756, 6 (stack74)
        %v25763 = vor.u32 %v25761, %v25762 (stack75)
        %v25764 = vxor.u32 %v25759, %v25763 (stack76)
        %v25767 = vadd.s32 %v25759, %v25764 (stack65)
        %v25771 = vadd.s32 %v25767, %v9 (stack65)
        %v25773 = vshll.u32 %v25764, 6 (stack73)
        %v25774 = vshrl.u32 %v25764, 26 (stack74)
        %v25775 = vor.u32 %v25773, %v25774 (stack75)
        %v25776 = vxor.u32 %v25767, %v25775 (stack76)
        %v25779 = vadd.s32 %v25776, %v8 (stack65)
        %v25783 = vadd.s32 %v25779, 1 (stack65)
        %v25787 = vadd.s32 %v25771, %v25783 (stack65)
        %v25789 = vshll.u32 %v25783, 17 (stack73)
        %v25790 = vshrl.u32 %v25783, 15 (stack74)
        %v25791 = vor.u32 %v25789, %v25790 (stack75)
        %v25792 = vxor.u32 %v25787, %v25791 (stack76)
        %v25795 = vadd.s32 %v25787, %v25792 (stack65)
        %v25797 = vshll.u32 %v25792, 29 (stack73)
        %v25798 = vshrl.u32 %v25792, 3 (stack74)
        %v25799 = vor.u32 %v25797, %v25798 (stack75)
        %v25800 = vxor.u32 %v25795, %v25799 (stack76)
        %v25803 = vadd.s32 %v25795, %v25800 (stack65)
        %v25805 = vshll.u32 %v25800, 16 (stack73)
        %v25806 = vshrl.u32 %v25800, 16 (stack74)
        %v25807 = vor.u32 %v25805, %v25806 (stack75)
        %v25808 = vxor.u32 %v25803, %v25807 (stack76)
        %v25811 = vadd.s32 %v25803, %v25808 (stack65)
        %v25815 = vadd.s32 %v25811, %v8 (stack65)
        %v25817 = vshll.u32 %v25808, 24 (stack73)
        %v25818 = vshrl.u32 %v25808, 8 (stack74)
        %v25819 = vor.u32 %v25817, %v25818 (stack75)
        %v25820 = vxor.u32 %v25811, %v25819 (stack76)
        %v25823 = vadd.s32 %v25820, %v10 (stack65)
        %v25827 = vadd.s32 %v25823, 2 (stack65)
        %v25831 = vadd.s32 %v25815, %v25827 (stack65)
        %v25833 = vshll.u32 %v25827, 13 (stack73)
        %v25834 = vshrl.u32 %v25827, 19 (stack74)
        %v25835 = vor.u32 %v25833, %v25834 (stack75)
        %v25836 = vxor.u32 %v25831, %v25835 (stack76)
        %v25839 = vadd.s32 %v25831, %v25836 (stack65)
        %v25841 = vshll.u32 %v25836, 15 (stack73)
        %v25842 = vshrl.u32 %v25836, 17 (stack74)
        %v25843 = vor.u32 %v25841, %v25842 (stack75)
        %v25844 = vxor.u32 %v25839, %v25843 (stack76)
        %v25847 = vadd.s32 %v25839, %v25844 (stack65)
        %v25849 = vshll.u32 %v25844, 26 (stack73)
        %v25850 = vshrl.u32 %v25844, 6 (stack74)
        %v25851 = vor.u32 %v25849, %v25850 (stack75)
        %v25852 = vxor.u32 %v25847, %v25851 (stack76)
        %v25855 = vadd.s32 %v25847, %v25852 (stack65)
        %v25859 = vadd.s32 %v25855, %v10 (stack65)
        %v25861 = vshll.u32 %v25852, 6 (stack73)
        %v25862 = vshrl.u32 %v25852, 26 (stack74)
        %v25863 = vor.u32 %v25861, %v25862 (stack75)
        %v25864 = vxor.u32 %v25855, %v25863 (stack76)
        %v25867 = vadd.s32 %v25864, %v9 (stack65)
        %v25871 = vadd.s32 %v25867, 3 (stack65)
        %v25875 = vadd.s32 %v25859, %v25871 (stack65)
        %v25877 = vshll.u32 %v25871, 17 (stack73)
        %v25878 = vshrl.u32 %v25871, 15 (stack74)
        %v25879 = vor.u32 %v25877, %v25878 (stack75)
        %v25880 = vxor.u32 %v25875, %v25879 (stack76)
        %v25883 = vadd.s32 %v25875, %v25880 (stack65)
        %v25885 = vshll.u32 %v25880, 29 (stack73)
        %v25886 = vshrl.u32 %v25880, 3 (stack74)
        %v25887 = vor.u32 %v25885, %v25886 (stack75)
        %v25888 = vxor.u32 %v25883, %v25887 (stack76)
        %v25891 = vadd.s32 %v25883, %v25888 (stack65)
        %v25893 = vshll.u32 %v25888, 16 (stack73)
        %v25894 = vshrl.u32 %v25888, 16 (stack74)
        %v25895 = vor.u32 %v25893, %v25894 (stack75)
        %v25896 = vxor.u32 %v25891, %v25895 (stack76)
        %v25899 = vadd.s32 %v25891, %v25896 (stack65)
        %v25903 = vadd.s32 %v25899, %v9 (stack65)
        %v25905 = vshll.u32 %v25896, 24 (stack73)
        %v25906 = vshrl.u32 %v25896, 8 (stack74)
        %v25907 = vor.u32 %v25905, %v25906 (stack75)
        %v25908 = vxor.u32 %v25899, %v25907 (stack76)
        %v25911 = vadd.s32 %v25908, %v8 (stack65)
        %v25915 = vadd.s32 %v25911, 4 (stack65)
        %v25919 = vadd.s32 %v25903, %v25915 (stack65)
        %v25921 = vshll.u32 %v25915, 13 (stack73)
        %v25922 = vshrl.u32 %v25915, 19 (stack74)
        %v25923 = vor.u32 %v25921, %v25922 (stack75)
        %v25924 = vxor.u32 %v25919, %v25923 (stack76)
        %v25927 = vadd.s32 %v25919, %v25924 (stack65)
        %v25929 = vshll.u32 %v25924, 15 (stack73)
        %v25930 = vshrl.u32 %v25924, 17 (stack74)
        %v25931 = vor.u32 %v25929, %v25930 (stack75)
        %v25932 = vxor.u32 %v25927, %v25931 (stack76)
        %v25935 = vadd.s32 %v25927, %v25932 (stack65)
        %v25937 = vshll.u32 %v25932, 26 (stack73)
        %v25938 = vshrl.u32 %v25932, 6 (stack74)
        %v25939 = vor.u32 %v25937, %v25938 (stack75)
        %v25940 = vxor.u32 %v25935, %v25939 (stack76)
        %v25943 = vadd.s32 %v25935, %v25940 (stack65)
        %v25947 = vadd.s32 %v25943, %v8 (stack65)
        %v25949 = vshll.u32 %v25940, 6 (stack73)
        %v25950 = vshrl.u32 %v25940, 26 (stack74)
        %v25951 = vor.u32 %v25949, %v25950 (stack75)
        %v25952 = vxor.u32 %v25943, %v25951 (stack76)
        %v25955 = vadd.s32 %v25952, %v10 (stack65)
        %v25959 = vadd.s32 %v25955, 5 (stack65)
        %v25961 = vxor.u32 %v25947, %v25959 (stack76)
        %v25962 = vand.u32.u8 %v25961, 255 (stack77)
        %v25963 = vand.u32 %v25962, 65535 (stack78)
        %v25964 = vshrl.u32 %v25963, 1 (stack79)
        %v25965 = vor.u32 %v25964, 16256 (stack75)
        %v25966 = vand.u32.u16 %v25965, 65535 (stack80)
        %v25967 = vunpack.i.l.bf16 %v25966 (stack81)
        %v25971 = vadd.f32 %v25967, -1.0 (stack82)
        %v25975 = vmul.f32 %v25971, 2.0 (stack83)
        %v25979 = vadd.f32 %v25975, -0.99609375 (stack82)
        %v25983 = vmax.f32 -0.99609375, %v25979 (stack84)
        %v25985 = vand.u32 2147483647, %v25983 (stack85)
        %vm25988 = vcmp.eq.f32.partialorder %v25985, 1.0 (stack86)
        %v25993 = vmul.f32 %v25983, inf (stack83)
        %v25995 = vxor.u32 %v25983, 2147483648 (stack87)
        %v25998 = vmul.f32 %v25983, %v25995 (stack83)
        %v26000 = vadd.f32 %v25998, 1.0 (stack88)
        %v26001 = vlog2.pop %v26000 (stack89)
        %v26002 = vmul.f32 %v26001, 0.6931472 (stack90)
        %v26003 = vmul.f32 -0.5, %v25998 (stack91)
        %v26004 = vadd.f32 %v26003, 1.0 (stack92)
        %v26005 = vmul.f32 %v26004, %v25998 (stack93)
        %v26006 = vand.u32 2147483647, %v25998 (stack94)
        %vm26007 = vcmp.lt.f32.partialorder %v26006, 0.0004427343 (stack95)
        %v26008 = vsel /*vm=*/%vm26007, /*on_true_vy=*/%v26005, /*on_false_vx=*/%v26002 (stack96)
        %v26009 = vxor.u32 %v26008, 2147483648 (stack87)
        %vm26012 = vcmp.lt.f32.partialorder %v26009, 5.0 (stack86)
        %v26017 = vsel /*vm=*/%vm26012, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v26021 = vsel /*vm=*/%vm26012, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v26025 = vsel /*vm=*/%vm26012, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v26029 = vsel /*vm=*/%vm26012, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v26033 = vsel /*vm=*/%vm26012, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v26037 = vsel /*vm=*/%vm26012, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v26041 = vsel /*vm=*/%vm26012, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v26045 = vsel /*vm=*/%vm26012, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v26049 = vsel /*vm=*/%vm26012, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v26053 = vadd.f32 %v26009, -2.5 (stack82)
        %v26055 = vrsqrt.pop %v26009 (stack97)
        %v26056 = vmul.f32 %v26009, %v26055 (stack98)
        %vm26057 = vcmp.eq.f32.partialorder %v26009, inf (stack99)
        %v26058 = vsel /*vm=*/%vm26057, /*on_true_vy=*/%v26009, /*on_false_vx=*/%v26056 (stack100)
        %vm26059 = vcmp.eq.f32.partialorder %v26009, 0.0 (stack101)
        %v26060 = vand.u32 %v26009, 2147483648 (stack102)
        %v26061 = vsel /*vm=*/%vm26059, /*on_true_vy=*/%v26060, /*on_false_vx=*/%v26058 (stack103)
        %v26064 = vadd.f32 %v26061, -3.0 (stack82)
        %v26068 = vsel /*vm=*/%vm26012, /*on_true_vy=*/%v26053, /*on_false_vx=*/%v26064 (stack72)
        %v26072 = vmul.f32 %v26049, %v26068 (stack83)
        %v26076 = vadd.f32 %v26045, %v26072 (stack82)
        %v26080 = vmul.f32 %v26076, %v26068 (stack83)
        %v26084 = vadd.f32 %v26041, %v26080 (stack82)
        %v26088 = vmul.f32 %v26084, %v26068 (stack83)
        %v26092 = vadd.f32 %v26037, %v26088 (stack82)
        %v26096 = vmul.f32 %v26092, %v26068 (stack83)
        %v26100 = vadd.f32 %v26033, %v26096 (stack82)
        %v26104 = vmul.f32 %v26100, %v26068 (stack83)
        %v26108 = vadd.f32 %v26029, %v26104 (stack82)
        %v26112 = vmul.f32 %v26108, %v26068 (stack83)
        %v26116 = vadd.f32 %v26025, %v26112 (stack82)
        %v26120 = vmul.f32 %v26116, %v26068 (stack83)
        %v26124 = vadd.f32 %v26021, %v26120 (stack82)
        %v26128 = vmul.f32 %v26124, %v26068 (stack83)
        %v26132 = vadd.f32 %v26017, %v26128 (stack82)
        %v26136 = vmul.f32 %v26132, %v25983 (stack83)
        %v26140 = vsel /*vm=*/%vm25988, /*on_true_vy=*/%v25993, /*on_false_vx=*/%v26136 (stack72)
        %v26144 = vmul.f32 %v26140, 1.4140625 (stack83)
        %s26146 = scalar_lea.vmem %s280, 792 [#allocation0] (stack107)
        %v26147 = vpack.c.bf16 0.0, %v26144 (stack104)
        %26148 = vst [vmem:[%s26146] sm:$0xf] /*vst_source=*/%v26147 (stack105)
        %v26151 = vadd.s32 %v3816, %v22921 (stack65)
        %s26153 = smul.u32 128, %s27 (stack66)
        %v26154 = vlaneseq (stack67)
        %v26155 = vand.u32 %v26154, 127 (stack68)
        %v26156 = vstv %s26153 (stack69)
        %v26157 = vadd.s32 %v26155, %v26156 (stack70)
        %v26161 = vadd.s32 %v26151, %v26157 (stack65)
        %vm26165 = vcmp.lt.u32.totalorder %v26161, %v26151 (stack71)
        %vm26170 = vcmp.lt.u32.totalorder %v26151, %v3816 (stack71)
        %v26175 = vadd.s32 %v3803, %v22904 (stack65)
        %v26179 = vadd.s32 %v26175, 1 (stack65)
        %v26183 = vsel /*vm=*/%vm26170, /*on_true_vy=*/%v26179, /*on_false_vx=*/%v26175 (stack72)
        %v26187 = vadd.s32 %v26183, 1 (stack65)
        %v26191 = vsel /*vm=*/%vm26165, /*on_true_vy=*/%v26187, /*on_false_vx=*/%v26183 (stack72)
        %v26196 = vadd.s32 %v26191, %v10 (stack65)
        %v26200 = vadd.s32 %v26161, %v9 (stack65)
        %v26204 = vadd.s32 %v26196, %v26200 (stack65)
        %v26206 = vshll.u32 %v26200, 13 (stack73)
        %v26207 = vshrl.u32 %v26200, 19 (stack74)
        %v26208 = vor.u32 %v26206, %v26207 (stack75)
        %v26209 = vxor.u32 %v26204, %v26208 (stack76)
        %v26212 = vadd.s32 %v26204, %v26209 (stack65)
        %v26214 = vshll.u32 %v26209, 15 (stack73)
        %v26215 = vshrl.u32 %v26209, 17 (stack74)
        %v26216 = vor.u32 %v26214, %v26215 (stack75)
        %v26217 = vxor.u32 %v26212, %v26216 (stack76)
        %v26220 = vadd.s32 %v26212, %v26217 (stack65)
        %v26222 = vshll.u32 %v26217, 26 (stack73)
        %v26223 = vshrl.u32 %v26217, 6 (stack74)
        %v26224 = vor.u32 %v26222, %v26223 (stack75)
        %v26225 = vxor.u32 %v26220, %v26224 (stack76)
        %v26228 = vadd.s32 %v26220, %v26225 (stack65)
        %v26232 = vadd.s32 %v26228, %v9 (stack65)
        %v26234 = vshll.u32 %v26225, 6 (stack73)
        %v26235 = vshrl.u32 %v26225, 26 (stack74)
        %v26236 = vor.u32 %v26234, %v26235 (stack75)
        %v26237 = vxor.u32 %v26228, %v26236 (stack76)
        %v26240 = vadd.s32 %v26237, %v8 (stack65)
        %v26244 = vadd.s32 %v26240, 1 (stack65)
        %v26248 = vadd.s32 %v26232, %v26244 (stack65)
        %v26250 = vshll.u32 %v26244, 17 (stack73)
        %v26251 = vshrl.u32 %v26244, 15 (stack74)
        %v26252 = vor.u32 %v26250, %v26251 (stack75)
        %v26253 = vxor.u32 %v26248, %v26252 (stack76)
        %v26256 = vadd.s32 %v26248, %v26253 (stack65)
        %v26258 = vshll.u32 %v26253, 29 (stack73)
        %v26259 = vshrl.u32 %v26253, 3 (stack74)
        %v26260 = vor.u32 %v26258, %v26259 (stack75)
        %v26261 = vxor.u32 %v26256, %v26260 (stack76)
        %v26264 = vadd.s32 %v26256, %v26261 (stack65)
        %v26266 = vshll.u32 %v26261, 16 (stack73)
        %v26267 = vshrl.u32 %v26261, 16 (stack74)
        %v26268 = vor.u32 %v26266, %v26267 (stack75)
        %v26269 = vxor.u32 %v26264, %v26268 (stack76)
        %v26272 = vadd.s32 %v26264, %v26269 (stack65)
        %v26276 = vadd.s32 %v26272, %v8 (stack65)
        %v26278 = vshll.u32 %v26269, 24 (stack73)
        %v26279 = vshrl.u32 %v26269, 8 (stack74)
        %v26280 = vor.u32 %v26278, %v26279 (stack75)
        %v26281 = vxor.u32 %v26272, %v26280 (stack76)
        %v26284 = vadd.s32 %v26281, %v10 (stack65)
        %v26288 = vadd.s32 %v26284, 2 (stack65)
        %v26292 = vadd.s32 %v26276, %v26288 (stack65)
        %v26294 = vshll.u32 %v26288, 13 (stack73)
        %v26295 = vshrl.u32 %v26288, 19 (stack74)
        %v26296 = vor.u32 %v26294, %v26295 (stack75)
        %v26297 = vxor.u32 %v26292, %v26296 (stack76)
        %v26300 = vadd.s32 %v26292, %v26297 (stack65)
        %v26302 = vshll.u32 %v26297, 15 (stack73)
        %v26303 = vshrl.u32 %v26297, 17 (stack74)
        %v26304 = vor.u32 %v26302, %v26303 (stack75)
        %v26305 = vxor.u32 %v26300, %v26304 (stack76)
        %v26308 = vadd.s32 %v26300, %v26305 (stack65)
        %v26310 = vshll.u32 %v26305, 26 (stack73)
        %v26311 = vshrl.u32 %v26305, 6 (stack74)
        %v26312 = vor.u32 %v26310, %v26311 (stack75)
        %v26313 = vxor.u32 %v26308, %v26312 (stack76)
        %v26316 = vadd.s32 %v26308, %v26313 (stack65)
        %v26320 = vadd.s32 %v26316, %v10 (stack65)
        %v26322 = vshll.u32 %v26313, 6 (stack73)
        %v26323 = vshrl.u32 %v26313, 26 (stack74)
        %v26324 = vor.u32 %v26322, %v26323 (stack75)
        %v26325 = vxor.u32 %v26316, %v26324 (stack76)
        %v26328 = vadd.s32 %v26325, %v9 (stack65)
        %v26332 = vadd.s32 %v26328, 3 (stack65)
        %v26336 = vadd.s32 %v26320, %v26332 (stack65)
        %v26338 = vshll.u32 %v26332, 17 (stack73)
        %v26339 = vshrl.u32 %v26332, 15 (stack74)
        %v26340 = vor.u32 %v26338, %v26339 (stack75)
        %v26341 = vxor.u32 %v26336, %v26340 (stack76)
        %v26344 = vadd.s32 %v26336, %v26341 (stack65)
        %v26346 = vshll.u32 %v26341, 29 (stack73)
        %v26347 = vshrl.u32 %v26341, 3 (stack74)
        %v26348 = vor.u32 %v26346, %v26347 (stack75)
        %v26349 = vxor.u32 %v26344, %v26348 (stack76)
        %v26352 = vadd.s32 %v26344, %v26349 (stack65)
        %v26354 = vshll.u32 %v26349, 16 (stack73)
        %v26355 = vshrl.u32 %v26349, 16 (stack74)
        %v26356 = vor.u32 %v26354, %v26355 (stack75)
        %v26357 = vxor.u32 %v26352, %v26356 (stack76)
        %v26360 = vadd.s32 %v26352, %v26357 (stack65)
        %v26364 = vadd.s32 %v26360, %v9 (stack65)
        %v26366 = vshll.u32 %v26357, 24 (stack73)
        %v26367 = vshrl.u32 %v26357, 8 (stack74)
        %v26368 = vor.u32 %v26366, %v26367 (stack75)
        %v26369 = vxor.u32 %v26360, %v26368 (stack76)
        %v26372 = vadd.s32 %v26369, %v8 (stack65)
        %v26376 = vadd.s32 %v26372, 4 (stack65)
        %v26380 = vadd.s32 %v26364, %v26376 (stack65)
        %v26382 = vshll.u32 %v26376, 13 (stack73)
        %v26383 = vshrl.u32 %v26376, 19 (stack74)
        %v26384 = vor.u32 %v26382, %v26383 (stack75)
        %v26385 = vxor.u32 %v26380, %v26384 (stack76)
        %v26388 = vadd.s32 %v26380, %v26385 (stack65)
        %v26390 = vshll.u32 %v26385, 15 (stack73)
        %v26391 = vshrl.u32 %v26385, 17 (stack74)
        %v26392 = vor.u32 %v26390, %v26391 (stack75)
        %v26393 = vxor.u32 %v26388, %v26392 (stack76)
        %v26396 = vadd.s32 %v26388, %v26393 (stack65)
        %v26398 = vshll.u32 %v26393, 26 (stack73)
        %v26399 = vshrl.u32 %v26393, 6 (stack74)
        %v26400 = vor.u32 %v26398, %v26399 (stack75)
        %v26401 = vxor.u32 %v26396, %v26400 (stack76)
        %v26404 = vadd.s32 %v26396, %v26401 (stack65)
        %v26408 = vadd.s32 %v26404, %v8 (stack65)
        %v26410 = vshll.u32 %v26401, 6 (stack73)
        %v26411 = vshrl.u32 %v26401, 26 (stack74)
        %v26412 = vor.u32 %v26410, %v26411 (stack75)
        %v26413 = vxor.u32 %v26404, %v26412 (stack76)
        %v26416 = vadd.s32 %v26413, %v10 (stack65)
        %v26420 = vadd.s32 %v26416, 5 (stack65)
        %v26422 = vxor.u32 %v26408, %v26420 (stack76)
        %v26423 = vand.u32.u8 %v26422, 255 (stack77)
        %v26424 = vand.u32 %v26423, 65535 (stack78)
        %v26425 = vshrl.u32 %v26424, 1 (stack79)
        %v26426 = vor.u32 %v26425, 16256 (stack75)
        %v26427 = vand.u32.u16 %v26426, 65535 (stack80)
        %v26428 = vunpack.i.l.bf16 %v26427 (stack81)
        %v26432 = vadd.f32 %v26428, -1.0 (stack82)
        %v26436 = vmul.f32 %v26432, 2.0 (stack83)
        %v26440 = vadd.f32 %v26436, -0.99609375 (stack82)
        %v26444 = vmax.f32 -0.99609375, %v26440 (stack84)
        %v26446 = vand.u32 2147483647, %v26444 (stack85)
        %vm26449 = vcmp.eq.f32.partialorder %v26446, 1.0 (stack86)
        %v26454 = vmul.f32 %v26444, inf (stack83)
        %v26456 = vxor.u32 %v26444, 2147483648 (stack87)
        %v26459 = vmul.f32 %v26444, %v26456 (stack83)
        %v26461 = vadd.f32 %v26459, 1.0 (stack88)
        %v26462 = vlog2.pop %v26461 (stack89)
        %v26463 = vmul.f32 %v26462, 0.6931472 (stack90)
        %v26464 = vmul.f32 -0.5, %v26459 (stack91)
        %v26465 = vadd.f32 %v26464, 1.0 (stack92)
        %v26466 = vmul.f32 %v26465, %v26459 (stack93)
        %v26467 = vand.u32 2147483647, %v26459 (stack94)
        %vm26468 = vcmp.lt.f32.partialorder %v26467, 0.0004427343 (stack95)
        %v26469 = vsel /*vm=*/%vm26468, /*on_true_vy=*/%v26466, /*on_false_vx=*/%v26463 (stack96)
        %v26470 = vxor.u32 %v26469, 2147483648 (stack87)
        %vm26473 = vcmp.lt.f32.partialorder %v26470, 5.0 (stack86)
        %v26478 = vsel /*vm=*/%vm26473, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v26482 = vsel /*vm=*/%vm26473, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v26486 = vsel /*vm=*/%vm26473, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v26490 = vsel /*vm=*/%vm26473, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v26494 = vsel /*vm=*/%vm26473, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v26498 = vsel /*vm=*/%vm26473, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v26502 = vsel /*vm=*/%vm26473, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v26506 = vsel /*vm=*/%vm26473, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v26510 = vsel /*vm=*/%vm26473, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v26514 = vadd.f32 %v26470, -2.5 (stack82)
        %v26516 = vrsqrt.pop %v26470 (stack97)
        %v26517 = vmul.f32 %v26470, %v26516 (stack98)
        %vm26518 = vcmp.eq.f32.partialorder %v26470, inf (stack99)
        %v26519 = vsel /*vm=*/%vm26518, /*on_true_vy=*/%v26470, /*on_false_vx=*/%v26517 (stack100)
        %vm26520 = vcmp.eq.f32.partialorder %v26470, 0.0 (stack101)
        %v26521 = vand.u32 %v26470, 2147483648 (stack102)
        %v26522 = vsel /*vm=*/%vm26520, /*on_true_vy=*/%v26521, /*on_false_vx=*/%v26519 (stack103)
        %v26525 = vadd.f32 %v26522, -3.0 (stack82)
        %v26529 = vsel /*vm=*/%vm26473, /*on_true_vy=*/%v26514, /*on_false_vx=*/%v26525 (stack72)
        %v26533 = vmul.f32 %v26510, %v26529 (stack83)
        %v26537 = vadd.f32 %v26506, %v26533 (stack82)
        %v26541 = vmul.f32 %v26537, %v26529 (stack83)
        %v26545 = vadd.f32 %v26502, %v26541 (stack82)
        %v26549 = vmul.f32 %v26545, %v26529 (stack83)
        %v26553 = vadd.f32 %v26498, %v26549 (stack82)
        %v26557 = vmul.f32 %v26553, %v26529 (stack83)
        %v26561 = vadd.f32 %v26494, %v26557 (stack82)
        %v26565 = vmul.f32 %v26561, %v26529 (stack83)
        %v26569 = vadd.f32 %v26490, %v26565 (stack82)
        %v26573 = vmul.f32 %v26569, %v26529 (stack83)
        %v26577 = vadd.f32 %v26486, %v26573 (stack82)
        %v26581 = vmul.f32 %v26577, %v26529 (stack83)
        %v26585 = vadd.f32 %v26482, %v26581 (stack82)
        %v26589 = vmul.f32 %v26585, %v26529 (stack83)
        %v26593 = vadd.f32 %v26478, %v26589 (stack82)
        %v26597 = vmul.f32 %v26593, %v26444 (stack83)
        %v26601 = vsel /*vm=*/%vm26449, /*on_true_vy=*/%v26454, /*on_false_vx=*/%v26597 (stack72)
        %v26605 = vmul.f32 %v26601, 1.4140625 (stack83)
        %s26607 = scalar_lea.vmem %s280, 920 [#allocation0] (stack107)
        %v26608 = vpack.c.bf16 0.0, %v26605 (stack104)
        %26609 = vst [vmem:[%s26607] sm:$0xf] /*vst_source=*/%v26608 (stack105)
        %s26610 = sadd.s32 %s339, 56 (stack106)
        %s26611 = sshrl.u32 %s26610, 10 (stack49)
        %p26612 = scmp.lt.s32.totalorder 1, %s26611 (stack50)
        %s26613 = scalar_select /*predicate=*/%p26612, /*on_true=*/1, /*on_false=*/%s26611 (stack51)
        %s26614 = sand.u32 %s26610, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s26615 = sshrl.u32 %s26614, 7 (stack53)
        %s26616 = sand.u32 %s26614, 127 /* smod.u32 w/div 128 */ (stack54)
        %s26617 = smul.addr %s26613, 8 (stack55)
        %s26618 = scalar_lea.vmem %s3, %s26617 (stack56)
        %s26620 = scalar_lea.vmem %s26618, %s26615 (stack57)
        %v26621 = vld [vmem:[%s26620] ss:$0 sm:$0xff] (stack58)
        %s26622 = sand.u32 %s26616, 255 (stack59)
        %s26624 = sor.u32 256, %s26622 (stack60)
        %26625 = vbcast.lane.b32.xlu0 %v26621, %s26624 (stack61)
        %v26626 = vpop.permute.xlu0 %26625 (stack62)
        %s26627 = sadd.s32 %s347, 56 (stack106)
        %s26628 = sshrl.u32 %s26627, 10 (stack49)
        %p26629 = scmp.lt.s32.totalorder 1, %s26628 (stack50)
        %s26630 = scalar_select /*predicate=*/%p26629, /*on_true=*/1, /*on_false=*/%s26628 (stack51)
        %s26631 = sand.u32 %s26627, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s26632 = sshrl.u32 %s26631, 7 (stack53)
        %s26633 = sand.u32 %s26631, 127 /* smod.u32 w/div 128 */ (stack54)
        %s26634 = smul.addr %s26630, 8 (stack55)
        %s26635 = scalar_lea.vmem %s5, %s26634 (stack56)
        %s26637 = scalar_lea.vmem %s26635, %s26632 (stack57)
        %v26638 = vld [vmem:[%s26637] ss:$0 sm:$0xff] (stack58)
        %s26639 = sand.u32 %s26633, 255 (stack59)
        %s26641 = sor.u32 256, %s26639 (stack60)
        %26642 = vbcast.lane.b32.xlu0 %v26638, %s26641 (stack61)
        %v26643 = vpop.permute.xlu0 %26642 (stack62)
        %v26646 = vadd.s32 %v408, %v26643 (stack65)
        %s26648 = smul.u32 128, %s27 (stack66)
        %v26649 = vlaneseq (stack67)
        %v26650 = vand.u32 %v26649, 127 (stack68)
        %v26651 = vstv %s26648 (stack69)
        %v26652 = vadd.s32 %v26650, %v26651 (stack70)
        %v26656 = vadd.s32 %v26646, %v26652 (stack65)
        %vm26660 = vcmp.lt.u32.totalorder %v26656, %v26646 (stack71)
        %vm26665 = vcmp.lt.u32.totalorder %v26646, %v408 (stack71)
        %v26670 = vadd.s32 %v380, %v26626 (stack65)
        %v26674 = vadd.s32 %v26670, 1 (stack65)
        %v26678 = vsel /*vm=*/%vm26665, /*on_true_vy=*/%v26674, /*on_false_vx=*/%v26670 (stack72)
        %v26682 = vadd.s32 %v26678, 1 (stack65)
        %v26686 = vsel /*vm=*/%vm26660, /*on_true_vy=*/%v26682, /*on_false_vx=*/%v26678 (stack72)
        %v26691 = vadd.s32 %v26686, %v10 (stack65)
        %v26695 = vadd.s32 %v26656, %v9 (stack65)
        %v26699 = vadd.s32 %v26691, %v26695 (stack65)
        %v26701 = vshll.u32 %v26695, 13 (stack73)
        %v26702 = vshrl.u32 %v26695, 19 (stack74)
        %v26703 = vor.u32 %v26701, %v26702 (stack75)
        %v26704 = vxor.u32 %v26699, %v26703 (stack76)
        %v26707 = vadd.s32 %v26699, %v26704 (stack65)
        %v26709 = vshll.u32 %v26704, 15 (stack73)
        %v26710 = vshrl.u32 %v26704, 17 (stack74)
        %v26711 = vor.u32 %v26709, %v26710 (stack75)
        %v26712 = vxor.u32 %v26707, %v26711 (stack76)
        %v26715 = vadd.s32 %v26707, %v26712 (stack65)
        %v26717 = vshll.u32 %v26712, 26 (stack73)
        %v26718 = vshrl.u32 %v26712, 6 (stack74)
        %v26719 = vor.u32 %v26717, %v26718 (stack75)
        %v26720 = vxor.u32 %v26715, %v26719 (stack76)
        %v26723 = vadd.s32 %v26715, %v26720 (stack65)
        %v26727 = vadd.s32 %v26723, %v9 (stack65)
        %v26729 = vshll.u32 %v26720, 6 (stack73)
        %v26730 = vshrl.u32 %v26720, 26 (stack74)
        %v26731 = vor.u32 %v26729, %v26730 (stack75)
        %v26732 = vxor.u32 %v26723, %v26731 (stack76)
        %v26735 = vadd.s32 %v26732, %v8 (stack65)
        %v26739 = vadd.s32 %v26735, 1 (stack65)
        %v26743 = vadd.s32 %v26727, %v26739 (stack65)
        %v26745 = vshll.u32 %v26739, 17 (stack73)
        %v26746 = vshrl.u32 %v26739, 15 (stack74)
        %v26747 = vor.u32 %v26745, %v26746 (stack75)
        %v26748 = vxor.u32 %v26743, %v26747 (stack76)
        %v26751 = vadd.s32 %v26743, %v26748 (stack65)
        %v26753 = vshll.u32 %v26748, 29 (stack73)
        %v26754 = vshrl.u32 %v26748, 3 (stack74)
        %v26755 = vor.u32 %v26753, %v26754 (stack75)
        %v26756 = vxor.u32 %v26751, %v26755 (stack76)
        %v26759 = vadd.s32 %v26751, %v26756 (stack65)
        %v26761 = vshll.u32 %v26756, 16 (stack73)
        %v26762 = vshrl.u32 %v26756, 16 (stack74)
        %v26763 = vor.u32 %v26761, %v26762 (stack75)
        %v26764 = vxor.u32 %v26759, %v26763 (stack76)
        %v26767 = vadd.s32 %v26759, %v26764 (stack65)
        %v26771 = vadd.s32 %v26767, %v8 (stack65)
        %v26773 = vshll.u32 %v26764, 24 (stack73)
        %v26774 = vshrl.u32 %v26764, 8 (stack74)
        %v26775 = vor.u32 %v26773, %v26774 (stack75)
        %v26776 = vxor.u32 %v26767, %v26775 (stack76)
        %v26779 = vadd.s32 %v26776, %v10 (stack65)
        %v26783 = vadd.s32 %v26779, 2 (stack65)
        %v26787 = vadd.s32 %v26771, %v26783 (stack65)
        %v26789 = vshll.u32 %v26783, 13 (stack73)
        %v26790 = vshrl.u32 %v26783, 19 (stack74)
        %v26791 = vor.u32 %v26789, %v26790 (stack75)
        %v26792 = vxor.u32 %v26787, %v26791 (stack76)
        %v26795 = vadd.s32 %v26787, %v26792 (stack65)
        %v26797 = vshll.u32 %v26792, 15 (stack73)
        %v26798 = vshrl.u32 %v26792, 17 (stack74)
        %v26799 = vor.u32 %v26797, %v26798 (stack75)
        %v26800 = vxor.u32 %v26795, %v26799 (stack76)
        %v26803 = vadd.s32 %v26795, %v26800 (stack65)
        %v26805 = vshll.u32 %v26800, 26 (stack73)
        %v26806 = vshrl.u32 %v26800, 6 (stack74)
        %v26807 = vor.u32 %v26805, %v26806 (stack75)
        %v26808 = vxor.u32 %v26803, %v26807 (stack76)
        %v26811 = vadd.s32 %v26803, %v26808 (stack65)
        %v26815 = vadd.s32 %v26811, %v10 (stack65)
        %v26817 = vshll.u32 %v26808, 6 (stack73)
        %v26818 = vshrl.u32 %v26808, 26 (stack74)
        %v26819 = vor.u32 %v26817, %v26818 (stack75)
        %v26820 = vxor.u32 %v26811, %v26819 (stack76)
        %v26823 = vadd.s32 %v26820, %v9 (stack65)
        %v26827 = vadd.s32 %v26823, 3 (stack65)
        %v26831 = vadd.s32 %v26815, %v26827 (stack65)
        %v26833 = vshll.u32 %v26827, 17 (stack73)
        %v26834 = vshrl.u32 %v26827, 15 (stack74)
        %v26835 = vor.u32 %v26833, %v26834 (stack75)
        %v26836 = vxor.u32 %v26831, %v26835 (stack76)
        %v26839 = vadd.s32 %v26831, %v26836 (stack65)
        %v26841 = vshll.u32 %v26836, 29 (stack73)
        %v26842 = vshrl.u32 %v26836, 3 (stack74)
        %v26843 = vor.u32 %v26841, %v26842 (stack75)
        %v26844 = vxor.u32 %v26839, %v26843 (stack76)
        %v26847 = vadd.s32 %v26839, %v26844 (stack65)
        %v26849 = vshll.u32 %v26844, 16 (stack73)
        %v26850 = vshrl.u32 %v26844, 16 (stack74)
        %v26851 = vor.u32 %v26849, %v26850 (stack75)
        %v26852 = vxor.u32 %v26847, %v26851 (stack76)
        %v26855 = vadd.s32 %v26847, %v26852 (stack65)
        %v26859 = vadd.s32 %v26855, %v9 (stack65)
        %v26861 = vshll.u32 %v26852, 24 (stack73)
        %v26862 = vshrl.u32 %v26852, 8 (stack74)
        %v26863 = vor.u32 %v26861, %v26862 (stack75)
        %v26864 = vxor.u32 %v26855, %v26863 (stack76)
        %v26867 = vadd.s32 %v26864, %v8 (stack65)
        %v26871 = vadd.s32 %v26867, 4 (stack65)
        %v26875 = vadd.s32 %v26859, %v26871 (stack65)
        %v26877 = vshll.u32 %v26871, 13 (stack73)
        %v26878 = vshrl.u32 %v26871, 19 (stack74)
        %v26879 = vor.u32 %v26877, %v26878 (stack75)
        %v26880 = vxor.u32 %v26875, %v26879 (stack76)
        %v26883 = vadd.s32 %v26875, %v26880 (stack65)
        %v26885 = vshll.u32 %v26880, 15 (stack73)
        %v26886 = vshrl.u32 %v26880, 17 (stack74)
        %v26887 = vor.u32 %v26885, %v26886 (stack75)
        %v26888 = vxor.u32 %v26883, %v26887 (stack76)
        %v26891 = vadd.s32 %v26883, %v26888 (stack65)
        %v26893 = vshll.u32 %v26888, 26 (stack73)
        %v26894 = vshrl.u32 %v26888, 6 (stack74)
        %v26895 = vor.u32 %v26893, %v26894 (stack75)
        %v26896 = vxor.u32 %v26891, %v26895 (stack76)
        %v26899 = vadd.s32 %v26891, %v26896 (stack65)
        %v26903 = vadd.s32 %v26899, %v8 (stack65)
        %v26905 = vshll.u32 %v26896, 6 (stack73)
        %v26906 = vshrl.u32 %v26896, 26 (stack74)
        %v26907 = vor.u32 %v26905, %v26906 (stack75)
        %v26908 = vxor.u32 %v26899, %v26907 (stack76)
        %v26911 = vadd.s32 %v26908, %v10 (stack65)
        %v26915 = vadd.s32 %v26911, 5 (stack65)
        %v26917 = vxor.u32 %v26903, %v26915 (stack76)
        %v26918 = vand.u32.u8 %v26917, 255 (stack77)
        %v26919 = vand.u32 %v26918, 65535 (stack78)
        %v26920 = vshrl.u32 %v26919, 1 (stack79)
        %v26921 = vor.u32 %v26920, 16256 (stack75)
        %v26922 = vand.u32.u16 %v26921, 65535 (stack80)
        %v26923 = vunpack.i.l.bf16 %v26922 (stack81)
        %v26927 = vadd.f32 %v26923, -1.0 (stack82)
        %v26931 = vmul.f32 %v26927, 2.0 (stack83)
        %v26935 = vadd.f32 %v26931, -0.99609375 (stack82)
        %v26939 = vmax.f32 -0.99609375, %v26935 (stack84)
        %v26941 = vand.u32 2147483647, %v26939 (stack85)
        %vm26944 = vcmp.eq.f32.partialorder %v26941, 1.0 (stack86)
        %v26949 = vmul.f32 %v26939, inf (stack83)
        %v26951 = vxor.u32 %v26939, 2147483648 (stack87)
        %v26954 = vmul.f32 %v26939, %v26951 (stack83)
        %v26956 = vadd.f32 %v26954, 1.0 (stack88)
        %v26957 = vlog2.pop %v26956 (stack89)
        %v26958 = vmul.f32 %v26957, 0.6931472 (stack90)
        %v26959 = vmul.f32 -0.5, %v26954 (stack91)
        %v26960 = vadd.f32 %v26959, 1.0 (stack92)
        %v26961 = vmul.f32 %v26960, %v26954 (stack93)
        %v26962 = vand.u32 2147483647, %v26954 (stack94)
        %vm26963 = vcmp.lt.f32.partialorder %v26962, 0.0004427343 (stack95)
        %v26964 = vsel /*vm=*/%vm26963, /*on_true_vy=*/%v26961, /*on_false_vx=*/%v26958 (stack96)
        %v26965 = vxor.u32 %v26964, 2147483648 (stack87)
        %vm26968 = vcmp.lt.f32.partialorder %v26965, 5.0 (stack86)
        %v26973 = vsel /*vm=*/%vm26968, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v26977 = vsel /*vm=*/%vm26968, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v26981 = vsel /*vm=*/%vm26968, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v26985 = vsel /*vm=*/%vm26968, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v26989 = vsel /*vm=*/%vm26968, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v26993 = vsel /*vm=*/%vm26968, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v26997 = vsel /*vm=*/%vm26968, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v27001 = vsel /*vm=*/%vm26968, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v27005 = vsel /*vm=*/%vm26968, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v27009 = vadd.f32 %v26965, -2.5 (stack82)
        %v27011 = vrsqrt.pop %v26965 (stack97)
        %v27012 = vmul.f32 %v26965, %v27011 (stack98)
        %vm27013 = vcmp.eq.f32.partialorder %v26965, inf (stack99)
        %v27014 = vsel /*vm=*/%vm27013, /*on_true_vy=*/%v26965, /*on_false_vx=*/%v27012 (stack100)
        %vm27015 = vcmp.eq.f32.partialorder %v26965, 0.0 (stack101)
        %v27016 = vand.u32 %v26965, 2147483648 (stack102)
        %v27017 = vsel /*vm=*/%vm27015, /*on_true_vy=*/%v27016, /*on_false_vx=*/%v27014 (stack103)
        %v27020 = vadd.f32 %v27017, -3.0 (stack82)
        %v27024 = vsel /*vm=*/%vm26968, /*on_true_vy=*/%v27009, /*on_false_vx=*/%v27020 (stack72)
        %v27028 = vmul.f32 %v27005, %v27024 (stack83)
        %v27032 = vadd.f32 %v27001, %v27028 (stack82)
        %v27036 = vmul.f32 %v27032, %v27024 (stack83)
        %v27040 = vadd.f32 %v26997, %v27036 (stack82)
        %v27044 = vmul.f32 %v27040, %v27024 (stack83)
        %v27048 = vadd.f32 %v26993, %v27044 (stack82)
        %v27052 = vmul.f32 %v27048, %v27024 (stack83)
        %v27056 = vadd.f32 %v26989, %v27052 (stack82)
        %v27060 = vmul.f32 %v27056, %v27024 (stack83)
        %v27064 = vadd.f32 %v26985, %v27060 (stack82)
        %v27068 = vmul.f32 %v27064, %v27024 (stack83)
        %v27072 = vadd.f32 %v26981, %v27068 (stack82)
        %v27076 = vmul.f32 %v27072, %v27024 (stack83)
        %v27080 = vadd.f32 %v26977, %v27076 (stack82)
        %v27084 = vmul.f32 %v27080, %v27024 (stack83)
        %v27088 = vadd.f32 %v26973, %v27084 (stack82)
        %v27092 = vmul.f32 %v27088, %v26939 (stack83)
        %v27096 = vsel /*vm=*/%vm26944, /*on_true_vy=*/%v26949, /*on_false_vx=*/%v27092 (stack72)
        %v27100 = vmul.f32 %v27096, 1.4140625 (stack83)
        %s27102 = scalar_lea.vmem %s280, 28 [#allocation0] (stack107)
        %v27103 = vpack.c.bf16 0.0, %v27100 (stack104)
        %27104 = vst [vmem:[%s27102] sm:$0xf] /*vst_source=*/%v27103 (stack105)
        %v27107 = vadd.s32 %v894, %v26643 (stack65)
        %s27109 = smul.u32 128, %s27 (stack66)
        %v27110 = vlaneseq (stack67)
        %v27111 = vand.u32 %v27110, 127 (stack68)
        %v27112 = vstv %s27109 (stack69)
        %v27113 = vadd.s32 %v27111, %v27112 (stack70)
        %v27117 = vadd.s32 %v27107, %v27113 (stack65)
        %vm27121 = vcmp.lt.u32.totalorder %v27117, %v27107 (stack71)
        %vm27126 = vcmp.lt.u32.totalorder %v27107, %v894 (stack71)
        %v27131 = vadd.s32 %v881, %v26626 (stack65)
        %v27135 = vadd.s32 %v27131, 1 (stack65)
        %v27139 = vsel /*vm=*/%vm27126, /*on_true_vy=*/%v27135, /*on_false_vx=*/%v27131 (stack72)
        %v27143 = vadd.s32 %v27139, 1 (stack65)
        %v27147 = vsel /*vm=*/%vm27121, /*on_true_vy=*/%v27143, /*on_false_vx=*/%v27139 (stack72)
        %v27152 = vadd.s32 %v27147, %v10 (stack65)
        %v27156 = vadd.s32 %v27117, %v9 (stack65)
        %v27160 = vadd.s32 %v27152, %v27156 (stack65)
        %v27162 = vshll.u32 %v27156, 13 (stack73)
        %v27163 = vshrl.u32 %v27156, 19 (stack74)
        %v27164 = vor.u32 %v27162, %v27163 (stack75)
        %v27165 = vxor.u32 %v27160, %v27164 (stack76)
        %v27168 = vadd.s32 %v27160, %v27165 (stack65)
        %v27170 = vshll.u32 %v27165, 15 (stack73)
        %v27171 = vshrl.u32 %v27165, 17 (stack74)
        %v27172 = vor.u32 %v27170, %v27171 (stack75)
        %v27173 = vxor.u32 %v27168, %v27172 (stack76)
        %v27176 = vadd.s32 %v27168, %v27173 (stack65)
        %v27178 = vshll.u32 %v27173, 26 (stack73)
        %v27179 = vshrl.u32 %v27173, 6 (stack74)
        %v27180 = vor.u32 %v27178, %v27179 (stack75)
        %v27181 = vxor.u32 %v27176, %v27180 (stack76)
        %v27184 = vadd.s32 %v27176, %v27181 (stack65)
        %v27188 = vadd.s32 %v27184, %v9 (stack65)
        %v27190 = vshll.u32 %v27181, 6 (stack73)
        %v27191 = vshrl.u32 %v27181, 26 (stack74)
        %v27192 = vor.u32 %v27190, %v27191 (stack75)
        %v27193 = vxor.u32 %v27184, %v27192 (stack76)
        %v27196 = vadd.s32 %v27193, %v8 (stack65)
        %v27200 = vadd.s32 %v27196, 1 (stack65)
        %v27204 = vadd.s32 %v27188, %v27200 (stack65)
        %v27206 = vshll.u32 %v27200, 17 (stack73)
        %v27207 = vshrl.u32 %v27200, 15 (stack74)
        %v27208 = vor.u32 %v27206, %v27207 (stack75)
        %v27209 = vxor.u32 %v27204, %v27208 (stack76)
        %v27212 = vadd.s32 %v27204, %v27209 (stack65)
        %v27214 = vshll.u32 %v27209, 29 (stack73)
        %v27215 = vshrl.u32 %v27209, 3 (stack74)
        %v27216 = vor.u32 %v27214, %v27215 (stack75)
        %v27217 = vxor.u32 %v27212, %v27216 (stack76)
        %v27220 = vadd.s32 %v27212, %v27217 (stack65)
        %v27222 = vshll.u32 %v27217, 16 (stack73)
        %v27223 = vshrl.u32 %v27217, 16 (stack74)
        %v27224 = vor.u32 %v27222, %v27223 (stack75)
        %v27225 = vxor.u32 %v27220, %v27224 (stack76)
        %v27228 = vadd.s32 %v27220, %v27225 (stack65)
        %v27232 = vadd.s32 %v27228, %v8 (stack65)
        %v27234 = vshll.u32 %v27225, 24 (stack73)
        %v27235 = vshrl.u32 %v27225, 8 (stack74)
        %v27236 = vor.u32 %v27234, %v27235 (stack75)
        %v27237 = vxor.u32 %v27228, %v27236 (stack76)
        %v27240 = vadd.s32 %v27237, %v10 (stack65)
        %v27244 = vadd.s32 %v27240, 2 (stack65)
        %v27248 = vadd.s32 %v27232, %v27244 (stack65)
        %v27250 = vshll.u32 %v27244, 13 (stack73)
        %v27251 = vshrl.u32 %v27244, 19 (stack74)
        %v27252 = vor.u32 %v27250, %v27251 (stack75)
        %v27253 = vxor.u32 %v27248, %v27252 (stack76)
        %v27256 = vadd.s32 %v27248, %v27253 (stack65)
        %v27258 = vshll.u32 %v27253, 15 (stack73)
        %v27259 = vshrl.u32 %v27253, 17 (stack74)
        %v27260 = vor.u32 %v27258, %v27259 (stack75)
        %v27261 = vxor.u32 %v27256, %v27260 (stack76)
        %v27264 = vadd.s32 %v27256, %v27261 (stack65)
        %v27266 = vshll.u32 %v27261, 26 (stack73)
        %v27267 = vshrl.u32 %v27261, 6 (stack74)
        %v27268 = vor.u32 %v27266, %v27267 (stack75)
        %v27269 = vxor.u32 %v27264, %v27268 (stack76)
        %v27272 = vadd.s32 %v27264, %v27269 (stack65)
        %v27276 = vadd.s32 %v27272, %v10 (stack65)
        %v27278 = vshll.u32 %v27269, 6 (stack73)
        %v27279 = vshrl.u32 %v27269, 26 (stack74)
        %v27280 = vor.u32 %v27278, %v27279 (stack75)
        %v27281 = vxor.u32 %v27272, %v27280 (stack76)
        %v27284 = vadd.s32 %v27281, %v9 (stack65)
        %v27288 = vadd.s32 %v27284, 3 (stack65)
        %v27292 = vadd.s32 %v27276, %v27288 (stack65)
        %v27294 = vshll.u32 %v27288, 17 (stack73)
        %v27295 = vshrl.u32 %v27288, 15 (stack74)
        %v27296 = vor.u32 %v27294, %v27295 (stack75)
        %v27297 = vxor.u32 %v27292, %v27296 (stack76)
        %v27300 = vadd.s32 %v27292, %v27297 (stack65)
        %v27302 = vshll.u32 %v27297, 29 (stack73)
        %v27303 = vshrl.u32 %v27297, 3 (stack74)
        %v27304 = vor.u32 %v27302, %v27303 (stack75)
        %v27305 = vxor.u32 %v27300, %v27304 (stack76)
        %v27308 = vadd.s32 %v27300, %v27305 (stack65)
        %v27310 = vshll.u32 %v27305, 16 (stack73)
        %v27311 = vshrl.u32 %v27305, 16 (stack74)
        %v27312 = vor.u32 %v27310, %v27311 (stack75)
        %v27313 = vxor.u32 %v27308, %v27312 (stack76)
        %v27316 = vadd.s32 %v27308, %v27313 (stack65)
        %v27320 = vadd.s32 %v27316, %v9 (stack65)
        %v27322 = vshll.u32 %v27313, 24 (stack73)
        %v27323 = vshrl.u32 %v27313, 8 (stack74)
        %v27324 = vor.u32 %v27322, %v27323 (stack75)
        %v27325 = vxor.u32 %v27316, %v27324 (stack76)
        %v27328 = vadd.s32 %v27325, %v8 (stack65)
        %v27332 = vadd.s32 %v27328, 4 (stack65)
        %v27336 = vadd.s32 %v27320, %v27332 (stack65)
        %v27338 = vshll.u32 %v27332, 13 (stack73)
        %v27339 = vshrl.u32 %v27332, 19 (stack74)
        %v27340 = vor.u32 %v27338, %v27339 (stack75)
        %v27341 = vxor.u32 %v27336, %v27340 (stack76)
        %v27344 = vadd.s32 %v27336, %v27341 (stack65)
        %v27346 = vshll.u32 %v27341, 15 (stack73)
        %v27347 = vshrl.u32 %v27341, 17 (stack74)
        %v27348 = vor.u32 %v27346, %v27347 (stack75)
        %v27349 = vxor.u32 %v27344, %v27348 (stack76)
        %v27352 = vadd.s32 %v27344, %v27349 (stack65)
        %v27354 = vshll.u32 %v27349, 26 (stack73)
        %v27355 = vshrl.u32 %v27349, 6 (stack74)
        %v27356 = vor.u32 %v27354, %v27355 (stack75)
        %v27357 = vxor.u32 %v27352, %v27356 (stack76)
        %v27360 = vadd.s32 %v27352, %v27357 (stack65)
        %v27364 = vadd.s32 %v27360, %v8 (stack65)
        %v27366 = vshll.u32 %v27357, 6 (stack73)
        %v27367 = vshrl.u32 %v27357, 26 (stack74)
        %v27368 = vor.u32 %v27366, %v27367 (stack75)
        %v27369 = vxor.u32 %v27360, %v27368 (stack76)
        %v27372 = vadd.s32 %v27369, %v10 (stack65)
        %v27376 = vadd.s32 %v27372, 5 (stack65)
        %v27378 = vxor.u32 %v27364, %v27376 (stack76)
        %v27379 = vand.u32.u8 %v27378, 255 (stack77)
        %v27380 = vand.u32 %v27379, 65535 (stack78)
        %v27381 = vshrl.u32 %v27380, 1 (stack79)
        %v27382 = vor.u32 %v27381, 16256 (stack75)
        %v27383 = vand.u32.u16 %v27382, 65535 (stack80)
        %v27384 = vunpack.i.l.bf16 %v27383 (stack81)
        %v27388 = vadd.f32 %v27384, -1.0 (stack82)
        %v27392 = vmul.f32 %v27388, 2.0 (stack83)
        %v27396 = vadd.f32 %v27392, -0.99609375 (stack82)
        %v27400 = vmax.f32 -0.99609375, %v27396 (stack84)
        %v27402 = vand.u32 2147483647, %v27400 (stack85)
        %vm27405 = vcmp.eq.f32.partialorder %v27402, 1.0 (stack86)
        %v27410 = vmul.f32 %v27400, inf (stack83)
        %v27412 = vxor.u32 %v27400, 2147483648 (stack87)
        %v27415 = vmul.f32 %v27400, %v27412 (stack83)
        %v27417 = vadd.f32 %v27415, 1.0 (stack88)
        %v27418 = vlog2.pop %v27417 (stack89)
        %v27419 = vmul.f32 %v27418, 0.6931472 (stack90)
        %v27420 = vmul.f32 -0.5, %v27415 (stack91)
        %v27421 = vadd.f32 %v27420, 1.0 (stack92)
        %v27422 = vmul.f32 %v27421, %v27415 (stack93)
        %v27423 = vand.u32 2147483647, %v27415 (stack94)
        %vm27424 = vcmp.lt.f32.partialorder %v27423, 0.0004427343 (stack95)
        %v27425 = vsel /*vm=*/%vm27424, /*on_true_vy=*/%v27422, /*on_false_vx=*/%v27419 (stack96)
        %v27426 = vxor.u32 %v27425, 2147483648 (stack87)
        %vm27429 = vcmp.lt.f32.partialorder %v27426, 5.0 (stack86)
        %v27434 = vsel /*vm=*/%vm27429, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v27438 = vsel /*vm=*/%vm27429, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v27442 = vsel /*vm=*/%vm27429, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v27446 = vsel /*vm=*/%vm27429, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v27450 = vsel /*vm=*/%vm27429, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v27454 = vsel /*vm=*/%vm27429, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v27458 = vsel /*vm=*/%vm27429, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v27462 = vsel /*vm=*/%vm27429, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v27466 = vsel /*vm=*/%vm27429, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v27470 = vadd.f32 %v27426, -2.5 (stack82)
        %v27472 = vrsqrt.pop %v27426 (stack97)
        %v27473 = vmul.f32 %v27426, %v27472 (stack98)
        %vm27474 = vcmp.eq.f32.partialorder %v27426, inf (stack99)
        %v27475 = vsel /*vm=*/%vm27474, /*on_true_vy=*/%v27426, /*on_false_vx=*/%v27473 (stack100)
        %vm27476 = vcmp.eq.f32.partialorder %v27426, 0.0 (stack101)
        %v27477 = vand.u32 %v27426, 2147483648 (stack102)
        %v27478 = vsel /*vm=*/%vm27476, /*on_true_vy=*/%v27477, /*on_false_vx=*/%v27475 (stack103)
        %v27481 = vadd.f32 %v27478, -3.0 (stack82)
        %v27485 = vsel /*vm=*/%vm27429, /*on_true_vy=*/%v27470, /*on_false_vx=*/%v27481 (stack72)
        %v27489 = vmul.f32 %v27466, %v27485 (stack83)
        %v27493 = vadd.f32 %v27462, %v27489 (stack82)
        %v27497 = vmul.f32 %v27493, %v27485 (stack83)
        %v27501 = vadd.f32 %v27458, %v27497 (stack82)
        %v27505 = vmul.f32 %v27501, %v27485 (stack83)
        %v27509 = vadd.f32 %v27454, %v27505 (stack82)
        %v27513 = vmul.f32 %v27509, %v27485 (stack83)
        %v27517 = vadd.f32 %v27450, %v27513 (stack82)
        %v27521 = vmul.f32 %v27517, %v27485 (stack83)
        %v27525 = vadd.f32 %v27446, %v27521 (stack82)
        %v27529 = vmul.f32 %v27525, %v27485 (stack83)
        %v27533 = vadd.f32 %v27442, %v27529 (stack82)
        %v27537 = vmul.f32 %v27533, %v27485 (stack83)
        %v27541 = vadd.f32 %v27438, %v27537 (stack82)
        %v27545 = vmul.f32 %v27541, %v27485 (stack83)
        %v27549 = vadd.f32 %v27434, %v27545 (stack82)
        %v27553 = vmul.f32 %v27549, %v27400 (stack83)
        %v27557 = vsel /*vm=*/%vm27405, /*on_true_vy=*/%v27410, /*on_false_vx=*/%v27553 (stack72)
        %v27561 = vmul.f32 %v27557, 1.4140625 (stack83)
        %s27563 = scalar_lea.vmem %s280, 156 [#allocation0] (stack107)
        %v27564 = vpack.c.bf16 0.0, %v27561 (stack104)
        %27565 = vst [vmem:[%s27563] sm:$0xf] /*vst_source=*/%v27564 (stack105)
        %v27568 = vadd.s32 %v1381, %v26643 (stack65)
        %s27570 = smul.u32 128, %s27 (stack66)
        %v27571 = vlaneseq (stack67)
        %v27572 = vand.u32 %v27571, 127 (stack68)
        %v27573 = vstv %s27570 (stack69)
        %v27574 = vadd.s32 %v27572, %v27573 (stack70)
        %v27578 = vadd.s32 %v27568, %v27574 (stack65)
        %vm27582 = vcmp.lt.u32.totalorder %v27578, %v27568 (stack71)
        %vm27587 = vcmp.lt.u32.totalorder %v27568, %v1381 (stack71)
        %v27592 = vadd.s32 %v1368, %v26626 (stack65)
        %v27596 = vadd.s32 %v27592, 1 (stack65)
        %v27600 = vsel /*vm=*/%vm27587, /*on_true_vy=*/%v27596, /*on_false_vx=*/%v27592 (stack72)
        %v27604 = vadd.s32 %v27600, 1 (stack65)
        %v27608 = vsel /*vm=*/%vm27582, /*on_true_vy=*/%v27604, /*on_false_vx=*/%v27600 (stack72)
        %v27613 = vadd.s32 %v27608, %v10 (stack65)
        %v27617 = vadd.s32 %v27578, %v9 (stack65)
        %v27621 = vadd.s32 %v27613, %v27617 (stack65)
        %v27623 = vshll.u32 %v27617, 13 (stack73)
        %v27624 = vshrl.u32 %v27617, 19 (stack74)
        %v27625 = vor.u32 %v27623, %v27624 (stack75)
        %v27626 = vxor.u32 %v27621, %v27625 (stack76)
        %v27629 = vadd.s32 %v27621, %v27626 (stack65)
        %v27631 = vshll.u32 %v27626, 15 (stack73)
        %v27632 = vshrl.u32 %v27626, 17 (stack74)
        %v27633 = vor.u32 %v27631, %v27632 (stack75)
        %v27634 = vxor.u32 %v27629, %v27633 (stack76)
        %v27637 = vadd.s32 %v27629, %v27634 (stack65)
        %v27639 = vshll.u32 %v27634, 26 (stack73)
        %v27640 = vshrl.u32 %v27634, 6 (stack74)
        %v27641 = vor.u32 %v27639, %v27640 (stack75)
        %v27642 = vxor.u32 %v27637, %v27641 (stack76)
        %v27645 = vadd.s32 %v27637, %v27642 (stack65)
        %v27649 = vadd.s32 %v27645, %v9 (stack65)
        %v27651 = vshll.u32 %v27642, 6 (stack73)
        %v27652 = vshrl.u32 %v27642, 26 (stack74)
        %v27653 = vor.u32 %v27651, %v27652 (stack75)
        %v27654 = vxor.u32 %v27645, %v27653 (stack76)
        %v27657 = vadd.s32 %v27654, %v8 (stack65)
        %v27661 = vadd.s32 %v27657, 1 (stack65)
        %v27665 = vadd.s32 %v27649, %v27661 (stack65)
        %v27667 = vshll.u32 %v27661, 17 (stack73)
        %v27668 = vshrl.u32 %v27661, 15 (stack74)
        %v27669 = vor.u32 %v27667, %v27668 (stack75)
        %v27670 = vxor.u32 %v27665, %v27669 (stack76)
        %v27673 = vadd.s32 %v27665, %v27670 (stack65)
        %v27675 = vshll.u32 %v27670, 29 (stack73)
        %v27676 = vshrl.u32 %v27670, 3 (stack74)
        %v27677 = vor.u32 %v27675, %v27676 (stack75)
        %v27678 = vxor.u32 %v27673, %v27677 (stack76)
        %v27681 = vadd.s32 %v27673, %v27678 (stack65)
        %v27683 = vshll.u32 %v27678, 16 (stack73)
        %v27684 = vshrl.u32 %v27678, 16 (stack74)
        %v27685 = vor.u32 %v27683, %v27684 (stack75)
        %v27686 = vxor.u32 %v27681, %v27685 (stack76)
        %v27689 = vadd.s32 %v27681, %v27686 (stack65)
        %v27693 = vadd.s32 %v27689, %v8 (stack65)
        %v27695 = vshll.u32 %v27686, 24 (stack73)
        %v27696 = vshrl.u32 %v27686, 8 (stack74)
        %v27697 = vor.u32 %v27695, %v27696 (stack75)
        %v27698 = vxor.u32 %v27689, %v27697 (stack76)
        %v27701 = vadd.s32 %v27698, %v10 (stack65)
        %v27705 = vadd.s32 %v27701, 2 (stack65)
        %v27709 = vadd.s32 %v27693, %v27705 (stack65)
        %v27711 = vshll.u32 %v27705, 13 (stack73)
        %v27712 = vshrl.u32 %v27705, 19 (stack74)
        %v27713 = vor.u32 %v27711, %v27712 (stack75)
        %v27714 = vxor.u32 %v27709, %v27713 (stack76)
        %v27717 = vadd.s32 %v27709, %v27714 (stack65)
        %v27719 = vshll.u32 %v27714, 15 (stack73)
        %v27720 = vshrl.u32 %v27714, 17 (stack74)
        %v27721 = vor.u32 %v27719, %v27720 (stack75)
        %v27722 = vxor.u32 %v27717, %v27721 (stack76)
        %v27725 = vadd.s32 %v27717, %v27722 (stack65)
        %v27727 = vshll.u32 %v27722, 26 (stack73)
        %v27728 = vshrl.u32 %v27722, 6 (stack74)
        %v27729 = vor.u32 %v27727, %v27728 (stack75)
        %v27730 = vxor.u32 %v27725, %v27729 (stack76)
        %v27733 = vadd.s32 %v27725, %v27730 (stack65)
        %v27737 = vadd.s32 %v27733, %v10 (stack65)
        %v27739 = vshll.u32 %v27730, 6 (stack73)
        %v27740 = vshrl.u32 %v27730, 26 (stack74)
        %v27741 = vor.u32 %v27739, %v27740 (stack75)
        %v27742 = vxor.u32 %v27733, %v27741 (stack76)
        %v27745 = vadd.s32 %v27742, %v9 (stack65)
        %v27749 = vadd.s32 %v27745, 3 (stack65)
        %v27753 = vadd.s32 %v27737, %v27749 (stack65)
        %v27755 = vshll.u32 %v27749, 17 (stack73)
        %v27756 = vshrl.u32 %v27749, 15 (stack74)
        %v27757 = vor.u32 %v27755, %v27756 (stack75)
        %v27758 = vxor.u32 %v27753, %v27757 (stack76)
        %v27761 = vadd.s32 %v27753, %v27758 (stack65)
        %v27763 = vshll.u32 %v27758, 29 (stack73)
        %v27764 = vshrl.u32 %v27758, 3 (stack74)
        %v27765 = vor.u32 %v27763, %v27764 (stack75)
        %v27766 = vxor.u32 %v27761, %v27765 (stack76)
        %v27769 = vadd.s32 %v27761, %v27766 (stack65)
        %v27771 = vshll.u32 %v27766, 16 (stack73)
        %v27772 = vshrl.u32 %v27766, 16 (stack74)
        %v27773 = vor.u32 %v27771, %v27772 (stack75)
        %v27774 = vxor.u32 %v27769, %v27773 (stack76)
        %v27777 = vadd.s32 %v27769, %v27774 (stack65)
        %v27781 = vadd.s32 %v27777, %v9 (stack65)
        %v27783 = vshll.u32 %v27774, 24 (stack73)
        %v27784 = vshrl.u32 %v27774, 8 (stack74)
        %v27785 = vor.u32 %v27783, %v27784 (stack75)
        %v27786 = vxor.u32 %v27777, %v27785 (stack76)
        %v27789 = vadd.s32 %v27786, %v8 (stack65)
        %v27793 = vadd.s32 %v27789, 4 (stack65)
        %v27797 = vadd.s32 %v27781, %v27793 (stack65)
        %v27799 = vshll.u32 %v27793, 13 (stack73)
        %v27800 = vshrl.u32 %v27793, 19 (stack74)
        %v27801 = vor.u32 %v27799, %v27800 (stack75)
        %v27802 = vxor.u32 %v27797, %v27801 (stack76)
        %v27805 = vadd.s32 %v27797, %v27802 (stack65)
        %v27807 = vshll.u32 %v27802, 15 (stack73)
        %v27808 = vshrl.u32 %v27802, 17 (stack74)
        %v27809 = vor.u32 %v27807, %v27808 (stack75)
        %v27810 = vxor.u32 %v27805, %v27809 (stack76)
        %v27813 = vadd.s32 %v27805, %v27810 (stack65)
        %v27815 = vshll.u32 %v27810, 26 (stack73)
        %v27816 = vshrl.u32 %v27810, 6 (stack74)
        %v27817 = vor.u32 %v27815, %v27816 (stack75)
        %v27818 = vxor.u32 %v27813, %v27817 (stack76)
        %v27821 = vadd.s32 %v27813, %v27818 (stack65)
        %v27825 = vadd.s32 %v27821, %v8 (stack65)
        %v27827 = vshll.u32 %v27818, 6 (stack73)
        %v27828 = vshrl.u32 %v27818, 26 (stack74)
        %v27829 = vor.u32 %v27827, %v27828 (stack75)
        %v27830 = vxor.u32 %v27821, %v27829 (stack76)
        %v27833 = vadd.s32 %v27830, %v10 (stack65)
        %v27837 = vadd.s32 %v27833, 5 (stack65)
        %v27839 = vxor.u32 %v27825, %v27837 (stack76)
        %v27840 = vand.u32.u8 %v27839, 255 (stack77)
        %v27841 = vand.u32 %v27840, 65535 (stack78)
        %v27842 = vshrl.u32 %v27841, 1 (stack79)
        %v27843 = vor.u32 %v27842, 16256 (stack75)
        %v27844 = vand.u32.u16 %v27843, 65535 (stack80)
        %v27845 = vunpack.i.l.bf16 %v27844 (stack81)
        %v27849 = vadd.f32 %v27845, -1.0 (stack82)
        %v27853 = vmul.f32 %v27849, 2.0 (stack83)
        %v27857 = vadd.f32 %v27853, -0.99609375 (stack82)
        %v27861 = vmax.f32 -0.99609375, %v27857 (stack84)
        %v27863 = vand.u32 2147483647, %v27861 (stack85)
        %vm27866 = vcmp.eq.f32.partialorder %v27863, 1.0 (stack86)
        %v27871 = vmul.f32 %v27861, inf (stack83)
        %v27873 = vxor.u32 %v27861, 2147483648 (stack87)
        %v27876 = vmul.f32 %v27861, %v27873 (stack83)
        %v27878 = vadd.f32 %v27876, 1.0 (stack88)
        %v27879 = vlog2.pop %v27878 (stack89)
        %v27880 = vmul.f32 %v27879, 0.6931472 (stack90)
        %v27881 = vmul.f32 -0.5, %v27876 (stack91)
        %v27882 = vadd.f32 %v27881, 1.0 (stack92)
        %v27883 = vmul.f32 %v27882, %v27876 (stack93)
        %v27884 = vand.u32 2147483647, %v27876 (stack94)
        %vm27885 = vcmp.lt.f32.partialorder %v27884, 0.0004427343 (stack95)
        %v27886 = vsel /*vm=*/%vm27885, /*on_true_vy=*/%v27883, /*on_false_vx=*/%v27880 (stack96)
        %v27887 = vxor.u32 %v27886, 2147483648 (stack87)
        %vm27890 = vcmp.lt.f32.partialorder %v27887, 5.0 (stack86)
        %v27895 = vsel /*vm=*/%vm27890, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v27899 = vsel /*vm=*/%vm27890, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v27903 = vsel /*vm=*/%vm27890, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v27907 = vsel /*vm=*/%vm27890, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v27911 = vsel /*vm=*/%vm27890, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v27915 = vsel /*vm=*/%vm27890, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v27919 = vsel /*vm=*/%vm27890, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v27923 = vsel /*vm=*/%vm27890, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v27927 = vsel /*vm=*/%vm27890, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v27931 = vadd.f32 %v27887, -2.5 (stack82)
        %v27933 = vrsqrt.pop %v27887 (stack97)
        %v27934 = vmul.f32 %v27887, %v27933 (stack98)
        %vm27935 = vcmp.eq.f32.partialorder %v27887, inf (stack99)
        %v27936 = vsel /*vm=*/%vm27935, /*on_true_vy=*/%v27887, /*on_false_vx=*/%v27934 (stack100)
        %vm27937 = vcmp.eq.f32.partialorder %v27887, 0.0 (stack101)
        %v27938 = vand.u32 %v27887, 2147483648 (stack102)
        %v27939 = vsel /*vm=*/%vm27937, /*on_true_vy=*/%v27938, /*on_false_vx=*/%v27936 (stack103)
        %v27942 = vadd.f32 %v27939, -3.0 (stack82)
        %v27946 = vsel /*vm=*/%vm27890, /*on_true_vy=*/%v27931, /*on_false_vx=*/%v27942 (stack72)
        %v27950 = vmul.f32 %v27927, %v27946 (stack83)
        %v27954 = vadd.f32 %v27923, %v27950 (stack82)
        %v27958 = vmul.f32 %v27954, %v27946 (stack83)
        %v27962 = vadd.f32 %v27919, %v27958 (stack82)
        %v27966 = vmul.f32 %v27962, %v27946 (stack83)
        %v27970 = vadd.f32 %v27915, %v27966 (stack82)
        %v27974 = vmul.f32 %v27970, %v27946 (stack83)
        %v27978 = vadd.f32 %v27911, %v27974 (stack82)
        %v27982 = vmul.f32 %v27978, %v27946 (stack83)
        %v27986 = vadd.f32 %v27907, %v27982 (stack82)
        %v27990 = vmul.f32 %v27986, %v27946 (stack83)
        %v27994 = vadd.f32 %v27903, %v27990 (stack82)
        %v27998 = vmul.f32 %v27994, %v27946 (stack83)
        %v28002 = vadd.f32 %v27899, %v27998 (stack82)
        %v28006 = vmul.f32 %v28002, %v27946 (stack83)
        %v28010 = vadd.f32 %v27895, %v28006 (stack82)
        %v28014 = vmul.f32 %v28010, %v27861 (stack83)
        %v28018 = vsel /*vm=*/%vm27866, /*on_true_vy=*/%v27871, /*on_false_vx=*/%v28014 (stack72)
        %v28022 = vmul.f32 %v28018, 1.4140625 (stack83)
        %s28024 = scalar_lea.vmem %s280, 284 [#allocation0] (stack107)
        %v28025 = vpack.c.bf16 0.0, %v28022 (stack104)
        %28026 = vst [vmem:[%s28024] sm:$0xf] /*vst_source=*/%v28025 (stack105)
        %v28029 = vadd.s32 %v1868, %v26643 (stack65)
        %s28031 = smul.u32 128, %s27 (stack66)
        %v28032 = vlaneseq (stack67)
        %v28033 = vand.u32 %v28032, 127 (stack68)
        %v28034 = vstv %s28031 (stack69)
        %v28035 = vadd.s32 %v28033, %v28034 (stack70)
        %v28039 = vadd.s32 %v28029, %v28035 (stack65)
        %vm28043 = vcmp.lt.u32.totalorder %v28039, %v28029 (stack71)
        %vm28048 = vcmp.lt.u32.totalorder %v28029, %v1868 (stack71)
        %v28053 = vadd.s32 %v1855, %v26626 (stack65)
        %v28057 = vadd.s32 %v28053, 1 (stack65)
        %v28061 = vsel /*vm=*/%vm28048, /*on_true_vy=*/%v28057, /*on_false_vx=*/%v28053 (stack72)
        %v28065 = vadd.s32 %v28061, 1 (stack65)
        %v28069 = vsel /*vm=*/%vm28043, /*on_true_vy=*/%v28065, /*on_false_vx=*/%v28061 (stack72)
        %v28074 = vadd.s32 %v28069, %v10 (stack65)
        %v28078 = vadd.s32 %v28039, %v9 (stack65)
        %v28082 = vadd.s32 %v28074, %v28078 (stack65)
        %v28084 = vshll.u32 %v28078, 13 (stack73)
        %v28085 = vshrl.u32 %v28078, 19 (stack74)
        %v28086 = vor.u32 %v28084, %v28085 (stack75)
        %v28087 = vxor.u32 %v28082, %v28086 (stack76)
        %v28090 = vadd.s32 %v28082, %v28087 (stack65)
        %v28092 = vshll.u32 %v28087, 15 (stack73)
        %v28093 = vshrl.u32 %v28087, 17 (stack74)
        %v28094 = vor.u32 %v28092, %v28093 (stack75)
        %v28095 = vxor.u32 %v28090, %v28094 (stack76)
        %v28098 = vadd.s32 %v28090, %v28095 (stack65)
        %v28100 = vshll.u32 %v28095, 26 (stack73)
        %v28101 = vshrl.u32 %v28095, 6 (stack74)
        %v28102 = vor.u32 %v28100, %v28101 (stack75)
        %v28103 = vxor.u32 %v28098, %v28102 (stack76)
        %v28106 = vadd.s32 %v28098, %v28103 (stack65)
        %v28110 = vadd.s32 %v28106, %v9 (stack65)
        %v28112 = vshll.u32 %v28103, 6 (stack73)
        %v28113 = vshrl.u32 %v28103, 26 (stack74)
        %v28114 = vor.u32 %v28112, %v28113 (stack75)
        %v28115 = vxor.u32 %v28106, %v28114 (stack76)
        %v28118 = vadd.s32 %v28115, %v8 (stack65)
        %v28122 = vadd.s32 %v28118, 1 (stack65)
        %v28126 = vadd.s32 %v28110, %v28122 (stack65)
        %v28128 = vshll.u32 %v28122, 17 (stack73)
        %v28129 = vshrl.u32 %v28122, 15 (stack74)
        %v28130 = vor.u32 %v28128, %v28129 (stack75)
        %v28131 = vxor.u32 %v28126, %v28130 (stack76)
        %v28134 = vadd.s32 %v28126, %v28131 (stack65)
        %v28136 = vshll.u32 %v28131, 29 (stack73)
        %v28137 = vshrl.u32 %v28131, 3 (stack74)
        %v28138 = vor.u32 %v28136, %v28137 (stack75)
        %v28139 = vxor.u32 %v28134, %v28138 (stack76)
        %v28142 = vadd.s32 %v28134, %v28139 (stack65)
        %v28144 = vshll.u32 %v28139, 16 (stack73)
        %v28145 = vshrl.u32 %v28139, 16 (stack74)
        %v28146 = vor.u32 %v28144, %v28145 (stack75)
        %v28147 = vxor.u32 %v28142, %v28146 (stack76)
        %v28150 = vadd.s32 %v28142, %v28147 (stack65)
        %v28154 = vadd.s32 %v28150, %v8 (stack65)
        %v28156 = vshll.u32 %v28147, 24 (stack73)
        %v28157 = vshrl.u32 %v28147, 8 (stack74)
        %v28158 = vor.u32 %v28156, %v28157 (stack75)
        %v28159 = vxor.u32 %v28150, %v28158 (stack76)
        %v28162 = vadd.s32 %v28159, %v10 (stack65)
        %v28166 = vadd.s32 %v28162, 2 (stack65)
        %v28170 = vadd.s32 %v28154, %v28166 (stack65)
        %v28172 = vshll.u32 %v28166, 13 (stack73)
        %v28173 = vshrl.u32 %v28166, 19 (stack74)
        %v28174 = vor.u32 %v28172, %v28173 (stack75)
        %v28175 = vxor.u32 %v28170, %v28174 (stack76)
        %v28178 = vadd.s32 %v28170, %v28175 (stack65)
        %v28180 = vshll.u32 %v28175, 15 (stack73)
        %v28181 = vshrl.u32 %v28175, 17 (stack74)
        %v28182 = vor.u32 %v28180, %v28181 (stack75)
        %v28183 = vxor.u32 %v28178, %v28182 (stack76)
        %v28186 = vadd.s32 %v28178, %v28183 (stack65)
        %v28188 = vshll.u32 %v28183, 26 (stack73)
        %v28189 = vshrl.u32 %v28183, 6 (stack74)
        %v28190 = vor.u32 %v28188, %v28189 (stack75)
        %v28191 = vxor.u32 %v28186, %v28190 (stack76)
        %v28194 = vadd.s32 %v28186, %v28191 (stack65)
        %v28198 = vadd.s32 %v28194, %v10 (stack65)
        %v28200 = vshll.u32 %v28191, 6 (stack73)
        %v28201 = vshrl.u32 %v28191, 26 (stack74)
        %v28202 = vor.u32 %v28200, %v28201 (stack75)
        %v28203 = vxor.u32 %v28194, %v28202 (stack76)
        %v28206 = vadd.s32 %v28203, %v9 (stack65)
        %v28210 = vadd.s32 %v28206, 3 (stack65)
        %v28214 = vadd.s32 %v28198, %v28210 (stack65)
        %v28216 = vshll.u32 %v28210, 17 (stack73)
        %v28217 = vshrl.u32 %v28210, 15 (stack74)
        %v28218 = vor.u32 %v28216, %v28217 (stack75)
        %v28219 = vxor.u32 %v28214, %v28218 (stack76)
        %v28222 = vadd.s32 %v28214, %v28219 (stack65)
        %v28224 = vshll.u32 %v28219, 29 (stack73)
        %v28225 = vshrl.u32 %v28219, 3 (stack74)
        %v28226 = vor.u32 %v28224, %v28225 (stack75)
        %v28227 = vxor.u32 %v28222, %v28226 (stack76)
        %v28230 = vadd.s32 %v28222, %v28227 (stack65)
        %v28232 = vshll.u32 %v28227, 16 (stack73)
        %v28233 = vshrl.u32 %v28227, 16 (stack74)
        %v28234 = vor.u32 %v28232, %v28233 (stack75)
        %v28235 = vxor.u32 %v28230, %v28234 (stack76)
        %v28238 = vadd.s32 %v28230, %v28235 (stack65)
        %v28242 = vadd.s32 %v28238, %v9 (stack65)
        %v28244 = vshll.u32 %v28235, 24 (stack73)
        %v28245 = vshrl.u32 %v28235, 8 (stack74)
        %v28246 = vor.u32 %v28244, %v28245 (stack75)
        %v28247 = vxor.u32 %v28238, %v28246 (stack76)
        %v28250 = vadd.s32 %v28247, %v8 (stack65)
        %v28254 = vadd.s32 %v28250, 4 (stack65)
        %v28258 = vadd.s32 %v28242, %v28254 (stack65)
        %v28260 = vshll.u32 %v28254, 13 (stack73)
        %v28261 = vshrl.u32 %v28254, 19 (stack74)
        %v28262 = vor.u32 %v28260, %v28261 (stack75)
        %v28263 = vxor.u32 %v28258, %v28262 (stack76)
        %v28266 = vadd.s32 %v28258, %v28263 (stack65)
        %v28268 = vshll.u32 %v28263, 15 (stack73)
        %v28269 = vshrl.u32 %v28263, 17 (stack74)
        %v28270 = vor.u32 %v28268, %v28269 (stack75)
        %v28271 = vxor.u32 %v28266, %v28270 (stack76)
        %v28274 = vadd.s32 %v28266, %v28271 (stack65)
        %v28276 = vshll.u32 %v28271, 26 (stack73)
        %v28277 = vshrl.u32 %v28271, 6 (stack74)
        %v28278 = vor.u32 %v28276, %v28277 (stack75)
        %v28279 = vxor.u32 %v28274, %v28278 (stack76)
        %v28282 = vadd.s32 %v28274, %v28279 (stack65)
        %v28286 = vadd.s32 %v28282, %v8 (stack65)
        %v28288 = vshll.u32 %v28279, 6 (stack73)
        %v28289 = vshrl.u32 %v28279, 26 (stack74)
        %v28290 = vor.u32 %v28288, %v28289 (stack75)
        %v28291 = vxor.u32 %v28282, %v28290 (stack76)
        %v28294 = vadd.s32 %v28291, %v10 (stack65)
        %v28298 = vadd.s32 %v28294, 5 (stack65)
        %v28300 = vxor.u32 %v28286, %v28298 (stack76)
        %v28301 = vand.u32.u8 %v28300, 255 (stack77)
        %v28302 = vand.u32 %v28301, 65535 (stack78)
        %v28303 = vshrl.u32 %v28302, 1 (stack79)
        %v28304 = vor.u32 %v28303, 16256 (stack75)
        %v28305 = vand.u32.u16 %v28304, 65535 (stack80)
        %v28306 = vunpack.i.l.bf16 %v28305 (stack81)
        %v28310 = vadd.f32 %v28306, -1.0 (stack82)
        %v28314 = vmul.f32 %v28310, 2.0 (stack83)
        %v28318 = vadd.f32 %v28314, -0.99609375 (stack82)
        %v28322 = vmax.f32 -0.99609375, %v28318 (stack84)
        %v28324 = vand.u32 2147483647, %v28322 (stack85)
        %vm28327 = vcmp.eq.f32.partialorder %v28324, 1.0 (stack86)
        %v28332 = vmul.f32 %v28322, inf (stack83)
        %v28334 = vxor.u32 %v28322, 2147483648 (stack87)
        %v28337 = vmul.f32 %v28322, %v28334 (stack83)
        %v28339 = vadd.f32 %v28337, 1.0 (stack88)
        %v28340 = vlog2.pop %v28339 (stack89)
        %v28341 = vmul.f32 %v28340, 0.6931472 (stack90)
        %v28342 = vmul.f32 -0.5, %v28337 (stack91)
        %v28343 = vadd.f32 %v28342, 1.0 (stack92)
        %v28344 = vmul.f32 %v28343, %v28337 (stack93)
        %v28345 = vand.u32 2147483647, %v28337 (stack94)
        %vm28346 = vcmp.lt.f32.partialorder %v28345, 0.0004427343 (stack95)
        %v28347 = vsel /*vm=*/%vm28346, /*on_true_vy=*/%v28344, /*on_false_vx=*/%v28341 (stack96)
        %v28348 = vxor.u32 %v28347, 2147483648 (stack87)
        %vm28351 = vcmp.lt.f32.partialorder %v28348, 5.0 (stack86)
        %v28356 = vsel /*vm=*/%vm28351, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v28360 = vsel /*vm=*/%vm28351, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v28364 = vsel /*vm=*/%vm28351, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v28368 = vsel /*vm=*/%vm28351, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v28372 = vsel /*vm=*/%vm28351, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v28376 = vsel /*vm=*/%vm28351, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v28380 = vsel /*vm=*/%vm28351, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v28384 = vsel /*vm=*/%vm28351, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v28388 = vsel /*vm=*/%vm28351, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v28392 = vadd.f32 %v28348, -2.5 (stack82)
        %v28394 = vrsqrt.pop %v28348 (stack97)
        %v28395 = vmul.f32 %v28348, %v28394 (stack98)
        %vm28396 = vcmp.eq.f32.partialorder %v28348, inf (stack99)
        %v28397 = vsel /*vm=*/%vm28396, /*on_true_vy=*/%v28348, /*on_false_vx=*/%v28395 (stack100)
        %vm28398 = vcmp.eq.f32.partialorder %v28348, 0.0 (stack101)
        %v28399 = vand.u32 %v28348, 2147483648 (stack102)
        %v28400 = vsel /*vm=*/%vm28398, /*on_true_vy=*/%v28399, /*on_false_vx=*/%v28397 (stack103)
        %v28403 = vadd.f32 %v28400, -3.0 (stack82)
        %v28407 = vsel /*vm=*/%vm28351, /*on_true_vy=*/%v28392, /*on_false_vx=*/%v28403 (stack72)
        %v28411 = vmul.f32 %v28388, %v28407 (stack83)
        %v28415 = vadd.f32 %v28384, %v28411 (stack82)
        %v28419 = vmul.f32 %v28415, %v28407 (stack83)
        %v28423 = vadd.f32 %v28380, %v28419 (stack82)
        %v28427 = vmul.f32 %v28423, %v28407 (stack83)
        %v28431 = vadd.f32 %v28376, %v28427 (stack82)
        %v28435 = vmul.f32 %v28431, %v28407 (stack83)
        %v28439 = vadd.f32 %v28372, %v28435 (stack82)
        %v28443 = vmul.f32 %v28439, %v28407 (stack83)
        %v28447 = vadd.f32 %v28368, %v28443 (stack82)
        %v28451 = vmul.f32 %v28447, %v28407 (stack83)
        %v28455 = vadd.f32 %v28364, %v28451 (stack82)
        %v28459 = vmul.f32 %v28455, %v28407 (stack83)
        %v28463 = vadd.f32 %v28360, %v28459 (stack82)
        %v28467 = vmul.f32 %v28463, %v28407 (stack83)
        %v28471 = vadd.f32 %v28356, %v28467 (stack82)
        %v28475 = vmul.f32 %v28471, %v28322 (stack83)
        %v28479 = vsel /*vm=*/%vm28327, /*on_true_vy=*/%v28332, /*on_false_vx=*/%v28475 (stack72)
        %v28483 = vmul.f32 %v28479, 1.4140625 (stack83)
        %s28485 = scalar_lea.vmem %s280, 412 [#allocation0] (stack107)
        %v28486 = vpack.c.bf16 0.0, %v28483 (stack104)
        %28487 = vst [vmem:[%s28485] sm:$0xf] /*vst_source=*/%v28486 (stack105)
        %v28490 = vadd.s32 %v2355, %v26643 (stack65)
        %s28492 = smul.u32 128, %s27 (stack66)
        %v28493 = vlaneseq (stack67)
        %v28494 = vand.u32 %v28493, 127 (stack68)
        %v28495 = vstv %s28492 (stack69)
        %v28496 = vadd.s32 %v28494, %v28495 (stack70)
        %v28500 = vadd.s32 %v28490, %v28496 (stack65)
        %vm28504 = vcmp.lt.u32.totalorder %v28500, %v28490 (stack71)
        %vm28509 = vcmp.lt.u32.totalorder %v28490, %v2355 (stack71)
        %v28514 = vadd.s32 %v2342, %v26626 (stack65)
        %v28518 = vadd.s32 %v28514, 1 (stack65)
        %v28522 = vsel /*vm=*/%vm28509, /*on_true_vy=*/%v28518, /*on_false_vx=*/%v28514 (stack72)
        %v28526 = vadd.s32 %v28522, 1 (stack65)
        %v28530 = vsel /*vm=*/%vm28504, /*on_true_vy=*/%v28526, /*on_false_vx=*/%v28522 (stack72)
        %v28535 = vadd.s32 %v28530, %v10 (stack65)
        %v28539 = vadd.s32 %v28500, %v9 (stack65)
        %v28543 = vadd.s32 %v28535, %v28539 (stack65)
        %v28545 = vshll.u32 %v28539, 13 (stack73)
        %v28546 = vshrl.u32 %v28539, 19 (stack74)
        %v28547 = vor.u32 %v28545, %v28546 (stack75)
        %v28548 = vxor.u32 %v28543, %v28547 (stack76)
        %v28551 = vadd.s32 %v28543, %v28548 (stack65)
        %v28553 = vshll.u32 %v28548, 15 (stack73)
        %v28554 = vshrl.u32 %v28548, 17 (stack74)
        %v28555 = vor.u32 %v28553, %v28554 (stack75)
        %v28556 = vxor.u32 %v28551, %v28555 (stack76)
        %v28559 = vadd.s32 %v28551, %v28556 (stack65)
        %v28561 = vshll.u32 %v28556, 26 (stack73)
        %v28562 = vshrl.u32 %v28556, 6 (stack74)
        %v28563 = vor.u32 %v28561, %v28562 (stack75)
        %v28564 = vxor.u32 %v28559, %v28563 (stack76)
        %v28567 = vadd.s32 %v28559, %v28564 (stack65)
        %v28571 = vadd.s32 %v28567, %v9 (stack65)
        %v28573 = vshll.u32 %v28564, 6 (stack73)
        %v28574 = vshrl.u32 %v28564, 26 (stack74)
        %v28575 = vor.u32 %v28573, %v28574 (stack75)
        %v28576 = vxor.u32 %v28567, %v28575 (stack76)
        %v28579 = vadd.s32 %v28576, %v8 (stack65)
        %v28583 = vadd.s32 %v28579, 1 (stack65)
        %v28587 = vadd.s32 %v28571, %v28583 (stack65)
        %v28589 = vshll.u32 %v28583, 17 (stack73)
        %v28590 = vshrl.u32 %v28583, 15 (stack74)
        %v28591 = vor.u32 %v28589, %v28590 (stack75)
        %v28592 = vxor.u32 %v28587, %v28591 (stack76)
        %v28595 = vadd.s32 %v28587, %v28592 (stack65)
        %v28597 = vshll.u32 %v28592, 29 (stack73)
        %v28598 = vshrl.u32 %v28592, 3 (stack74)
        %v28599 = vor.u32 %v28597, %v28598 (stack75)
        %v28600 = vxor.u32 %v28595, %v28599 (stack76)
        %v28603 = vadd.s32 %v28595, %v28600 (stack65)
        %v28605 = vshll.u32 %v28600, 16 (stack73)
        %v28606 = vshrl.u32 %v28600, 16 (stack74)
        %v28607 = vor.u32 %v28605, %v28606 (stack75)
        %v28608 = vxor.u32 %v28603, %v28607 (stack76)
        %v28611 = vadd.s32 %v28603, %v28608 (stack65)
        %v28615 = vadd.s32 %v28611, %v8 (stack65)
        %v28617 = vshll.u32 %v28608, 24 (stack73)
        %v28618 = vshrl.u32 %v28608, 8 (stack74)
        %v28619 = vor.u32 %v28617, %v28618 (stack75)
        %v28620 = vxor.u32 %v28611, %v28619 (stack76)
        %v28623 = vadd.s32 %v28620, %v10 (stack65)
        %v28627 = vadd.s32 %v28623, 2 (stack65)
        %v28631 = vadd.s32 %v28615, %v28627 (stack65)
        %v28633 = vshll.u32 %v28627, 13 (stack73)
        %v28634 = vshrl.u32 %v28627, 19 (stack74)
        %v28635 = vor.u32 %v28633, %v28634 (stack75)
        %v28636 = vxor.u32 %v28631, %v28635 (stack76)
        %v28639 = vadd.s32 %v28631, %v28636 (stack65)
        %v28641 = vshll.u32 %v28636, 15 (stack73)
        %v28642 = vshrl.u32 %v28636, 17 (stack74)
        %v28643 = vor.u32 %v28641, %v28642 (stack75)
        %v28644 = vxor.u32 %v28639, %v28643 (stack76)
        %v28647 = vadd.s32 %v28639, %v28644 (stack65)
        %v28649 = vshll.u32 %v28644, 26 (stack73)
        %v28650 = vshrl.u32 %v28644, 6 (stack74)
        %v28651 = vor.u32 %v28649, %v28650 (stack75)
        %v28652 = vxor.u32 %v28647, %v28651 (stack76)
        %v28655 = vadd.s32 %v28647, %v28652 (stack65)
        %v28659 = vadd.s32 %v28655, %v10 (stack65)
        %v28661 = vshll.u32 %v28652, 6 (stack73)
        %v28662 = vshrl.u32 %v28652, 26 (stack74)
        %v28663 = vor.u32 %v28661, %v28662 (stack75)
        %v28664 = vxor.u32 %v28655, %v28663 (stack76)
        %v28667 = vadd.s32 %v28664, %v9 (stack65)
        %v28671 = vadd.s32 %v28667, 3 (stack65)
        %v28675 = vadd.s32 %v28659, %v28671 (stack65)
        %v28677 = vshll.u32 %v28671, 17 (stack73)
        %v28678 = vshrl.u32 %v28671, 15 (stack74)
        %v28679 = vor.u32 %v28677, %v28678 (stack75)
        %v28680 = vxor.u32 %v28675, %v28679 (stack76)
        %v28683 = vadd.s32 %v28675, %v28680 (stack65)
        %v28685 = vshll.u32 %v28680, 29 (stack73)
        %v28686 = vshrl.u32 %v28680, 3 (stack74)
        %v28687 = vor.u32 %v28685, %v28686 (stack75)
        %v28688 = vxor.u32 %v28683, %v28687 (stack76)
        %v28691 = vadd.s32 %v28683, %v28688 (stack65)
        %v28693 = vshll.u32 %v28688, 16 (stack73)
        %v28694 = vshrl.u32 %v28688, 16 (stack74)
        %v28695 = vor.u32 %v28693, %v28694 (stack75)
        %v28696 = vxor.u32 %v28691, %v28695 (stack76)
        %v28699 = vadd.s32 %v28691, %v28696 (stack65)
        %v28703 = vadd.s32 %v28699, %v9 (stack65)
        %v28705 = vshll.u32 %v28696, 24 (stack73)
        %v28706 = vshrl.u32 %v28696, 8 (stack74)
        %v28707 = vor.u32 %v28705, %v28706 (stack75)
        %v28708 = vxor.u32 %v28699, %v28707 (stack76)
        %v28711 = vadd.s32 %v28708, %v8 (stack65)
        %v28715 = vadd.s32 %v28711, 4 (stack65)
        %v28719 = vadd.s32 %v28703, %v28715 (stack65)
        %v28721 = vshll.u32 %v28715, 13 (stack73)
        %v28722 = vshrl.u32 %v28715, 19 (stack74)
        %v28723 = vor.u32 %v28721, %v28722 (stack75)
        %v28724 = vxor.u32 %v28719, %v28723 (stack76)
        %v28727 = vadd.s32 %v28719, %v28724 (stack65)
        %v28729 = vshll.u32 %v28724, 15 (stack73)
        %v28730 = vshrl.u32 %v28724, 17 (stack74)
        %v28731 = vor.u32 %v28729, %v28730 (stack75)
        %v28732 = vxor.u32 %v28727, %v28731 (stack76)
        %v28735 = vadd.s32 %v28727, %v28732 (stack65)
        %v28737 = vshll.u32 %v28732, 26 (stack73)
        %v28738 = vshrl.u32 %v28732, 6 (stack74)
        %v28739 = vor.u32 %v28737, %v28738 (stack75)
        %v28740 = vxor.u32 %v28735, %v28739 (stack76)
        %v28743 = vadd.s32 %v28735, %v28740 (stack65)
        %v28747 = vadd.s32 %v28743, %v8 (stack65)
        %v28749 = vshll.u32 %v28740, 6 (stack73)
        %v28750 = vshrl.u32 %v28740, 26 (stack74)
        %v28751 = vor.u32 %v28749, %v28750 (stack75)
        %v28752 = vxor.u32 %v28743, %v28751 (stack76)
        %v28755 = vadd.s32 %v28752, %v10 (stack65)
        %v28759 = vadd.s32 %v28755, 5 (stack65)
        %v28761 = vxor.u32 %v28747, %v28759 (stack76)
        %v28762 = vand.u32.u8 %v28761, 255 (stack77)
        %v28763 = vand.u32 %v28762, 65535 (stack78)
        %v28764 = vshrl.u32 %v28763, 1 (stack79)
        %v28765 = vor.u32 %v28764, 16256 (stack75)
        %v28766 = vand.u32.u16 %v28765, 65535 (stack80)
        %v28767 = vunpack.i.l.bf16 %v28766 (stack81)
        %v28771 = vadd.f32 %v28767, -1.0 (stack82)
        %v28775 = vmul.f32 %v28771, 2.0 (stack83)
        %v28779 = vadd.f32 %v28775, -0.99609375 (stack82)
        %v28783 = vmax.f32 -0.99609375, %v28779 (stack84)
        %v28785 = vand.u32 2147483647, %v28783 (stack85)
        %vm28788 = vcmp.eq.f32.partialorder %v28785, 1.0 (stack86)
        %v28793 = vmul.f32 %v28783, inf (stack83)
        %v28795 = vxor.u32 %v28783, 2147483648 (stack87)
        %v28798 = vmul.f32 %v28783, %v28795 (stack83)
        %v28800 = vadd.f32 %v28798, 1.0 (stack88)
        %v28801 = vlog2.pop %v28800 (stack89)
        %v28802 = vmul.f32 %v28801, 0.6931472 (stack90)
        %v28803 = vmul.f32 -0.5, %v28798 (stack91)
        %v28804 = vadd.f32 %v28803, 1.0 (stack92)
        %v28805 = vmul.f32 %v28804, %v28798 (stack93)
        %v28806 = vand.u32 2147483647, %v28798 (stack94)
        %vm28807 = vcmp.lt.f32.partialorder %v28806, 0.0004427343 (stack95)
        %v28808 = vsel /*vm=*/%vm28807, /*on_true_vy=*/%v28805, /*on_false_vx=*/%v28802 (stack96)
        %v28809 = vxor.u32 %v28808, 2147483648 (stack87)
        %vm28812 = vcmp.lt.f32.partialorder %v28809, 5.0 (stack86)
        %v28817 = vsel /*vm=*/%vm28812, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v28821 = vsel /*vm=*/%vm28812, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v28825 = vsel /*vm=*/%vm28812, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v28829 = vsel /*vm=*/%vm28812, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v28833 = vsel /*vm=*/%vm28812, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v28837 = vsel /*vm=*/%vm28812, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v28841 = vsel /*vm=*/%vm28812, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v28845 = vsel /*vm=*/%vm28812, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v28849 = vsel /*vm=*/%vm28812, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v28853 = vadd.f32 %v28809, -2.5 (stack82)
        %v28855 = vrsqrt.pop %v28809 (stack97)
        %v28856 = vmul.f32 %v28809, %v28855 (stack98)
        %vm28857 = vcmp.eq.f32.partialorder %v28809, inf (stack99)
        %v28858 = vsel /*vm=*/%vm28857, /*on_true_vy=*/%v28809, /*on_false_vx=*/%v28856 (stack100)
        %vm28859 = vcmp.eq.f32.partialorder %v28809, 0.0 (stack101)
        %v28860 = vand.u32 %v28809, 2147483648 (stack102)
        %v28861 = vsel /*vm=*/%vm28859, /*on_true_vy=*/%v28860, /*on_false_vx=*/%v28858 (stack103)
        %v28864 = vadd.f32 %v28861, -3.0 (stack82)
        %v28868 = vsel /*vm=*/%vm28812, /*on_true_vy=*/%v28853, /*on_false_vx=*/%v28864 (stack72)
        %v28872 = vmul.f32 %v28849, %v28868 (stack83)
        %v28876 = vadd.f32 %v28845, %v28872 (stack82)
        %v28880 = vmul.f32 %v28876, %v28868 (stack83)
        %v28884 = vadd.f32 %v28841, %v28880 (stack82)
        %v28888 = vmul.f32 %v28884, %v28868 (stack83)
        %v28892 = vadd.f32 %v28837, %v28888 (stack82)
        %v28896 = vmul.f32 %v28892, %v28868 (stack83)
        %v28900 = vadd.f32 %v28833, %v28896 (stack82)
        %v28904 = vmul.f32 %v28900, %v28868 (stack83)
        %v28908 = vadd.f32 %v28829, %v28904 (stack82)
        %v28912 = vmul.f32 %v28908, %v28868 (stack83)
        %v28916 = vadd.f32 %v28825, %v28912 (stack82)
        %v28920 = vmul.f32 %v28916, %v28868 (stack83)
        %v28924 = vadd.f32 %v28821, %v28920 (stack82)
        %v28928 = vmul.f32 %v28924, %v28868 (stack83)
        %v28932 = vadd.f32 %v28817, %v28928 (stack82)
        %v28936 = vmul.f32 %v28932, %v28783 (stack83)
        %v28940 = vsel /*vm=*/%vm28788, /*on_true_vy=*/%v28793, /*on_false_vx=*/%v28936 (stack72)
        %v28944 = vmul.f32 %v28940, 1.4140625 (stack83)
        %s28946 = scalar_lea.vmem %s280, 540 [#allocation0] (stack107)
        %v28947 = vpack.c.bf16 0.0, %v28944 (stack104)
        %28948 = vst [vmem:[%s28946] sm:$0xf] /*vst_source=*/%v28947 (stack105)
        %v28951 = vadd.s32 %v2842, %v26643 (stack65)
        %s28953 = smul.u32 128, %s27 (stack66)
        %v28954 = vlaneseq (stack67)
        %v28955 = vand.u32 %v28954, 127 (stack68)
        %v28956 = vstv %s28953 (stack69)
        %v28957 = vadd.s32 %v28955, %v28956 (stack70)
        %v28961 = vadd.s32 %v28951, %v28957 (stack65)
        %vm28965 = vcmp.lt.u32.totalorder %v28961, %v28951 (stack71)
        %vm28970 = vcmp.lt.u32.totalorder %v28951, %v2842 (stack71)
        %v28975 = vadd.s32 %v2829, %v26626 (stack65)
        %v28979 = vadd.s32 %v28975, 1 (stack65)
        %v28983 = vsel /*vm=*/%vm28970, /*on_true_vy=*/%v28979, /*on_false_vx=*/%v28975 (stack72)
        %v28987 = vadd.s32 %v28983, 1 (stack65)
        %v28991 = vsel /*vm=*/%vm28965, /*on_true_vy=*/%v28987, /*on_false_vx=*/%v28983 (stack72)
        %v28996 = vadd.s32 %v28991, %v10 (stack65)
        %v29000 = vadd.s32 %v28961, %v9 (stack65)
        %v29004 = vadd.s32 %v28996, %v29000 (stack65)
        %v29006 = vshll.u32 %v29000, 13 (stack73)
        %v29007 = vshrl.u32 %v29000, 19 (stack74)
        %v29008 = vor.u32 %v29006, %v29007 (stack75)
        %v29009 = vxor.u32 %v29004, %v29008 (stack76)
        %v29012 = vadd.s32 %v29004, %v29009 (stack65)
        %v29014 = vshll.u32 %v29009, 15 (stack73)
        %v29015 = vshrl.u32 %v29009, 17 (stack74)
        %v29016 = vor.u32 %v29014, %v29015 (stack75)
        %v29017 = vxor.u32 %v29012, %v29016 (stack76)
        %v29020 = vadd.s32 %v29012, %v29017 (stack65)
        %v29022 = vshll.u32 %v29017, 26 (stack73)
        %v29023 = vshrl.u32 %v29017, 6 (stack74)
        %v29024 = vor.u32 %v29022, %v29023 (stack75)
        %v29025 = vxor.u32 %v29020, %v29024 (stack76)
        %v29028 = vadd.s32 %v29020, %v29025 (stack65)
        %v29032 = vadd.s32 %v29028, %v9 (stack65)
        %v29034 = vshll.u32 %v29025, 6 (stack73)
        %v29035 = vshrl.u32 %v29025, 26 (stack74)
        %v29036 = vor.u32 %v29034, %v29035 (stack75)
        %v29037 = vxor.u32 %v29028, %v29036 (stack76)
        %v29040 = vadd.s32 %v29037, %v8 (stack65)
        %v29044 = vadd.s32 %v29040, 1 (stack65)
        %v29048 = vadd.s32 %v29032, %v29044 (stack65)
        %v29050 = vshll.u32 %v29044, 17 (stack73)
        %v29051 = vshrl.u32 %v29044, 15 (stack74)
        %v29052 = vor.u32 %v29050, %v29051 (stack75)
        %v29053 = vxor.u32 %v29048, %v29052 (stack76)
        %v29056 = vadd.s32 %v29048, %v29053 (stack65)
        %v29058 = vshll.u32 %v29053, 29 (stack73)
        %v29059 = vshrl.u32 %v29053, 3 (stack74)
        %v29060 = vor.u32 %v29058, %v29059 (stack75)
        %v29061 = vxor.u32 %v29056, %v29060 (stack76)
        %v29064 = vadd.s32 %v29056, %v29061 (stack65)
        %v29066 = vshll.u32 %v29061, 16 (stack73)
        %v29067 = vshrl.u32 %v29061, 16 (stack74)
        %v29068 = vor.u32 %v29066, %v29067 (stack75)
        %v29069 = vxor.u32 %v29064, %v29068 (stack76)
        %v29072 = vadd.s32 %v29064, %v29069 (stack65)
        %v29076 = vadd.s32 %v29072, %v8 (stack65)
        %v29078 = vshll.u32 %v29069, 24 (stack73)
        %v29079 = vshrl.u32 %v29069, 8 (stack74)
        %v29080 = vor.u32 %v29078, %v29079 (stack75)
        %v29081 = vxor.u32 %v29072, %v29080 (stack76)
        %v29084 = vadd.s32 %v29081, %v10 (stack65)
        %v29088 = vadd.s32 %v29084, 2 (stack65)
        %v29092 = vadd.s32 %v29076, %v29088 (stack65)
        %v29094 = vshll.u32 %v29088, 13 (stack73)
        %v29095 = vshrl.u32 %v29088, 19 (stack74)
        %v29096 = vor.u32 %v29094, %v29095 (stack75)
        %v29097 = vxor.u32 %v29092, %v29096 (stack76)
        %v29100 = vadd.s32 %v29092, %v29097 (stack65)
        %v29102 = vshll.u32 %v29097, 15 (stack73)
        %v29103 = vshrl.u32 %v29097, 17 (stack74)
        %v29104 = vor.u32 %v29102, %v29103 (stack75)
        %v29105 = vxor.u32 %v29100, %v29104 (stack76)
        %v29108 = vadd.s32 %v29100, %v29105 (stack65)
        %v29110 = vshll.u32 %v29105, 26 (stack73)
        %v29111 = vshrl.u32 %v29105, 6 (stack74)
        %v29112 = vor.u32 %v29110, %v29111 (stack75)
        %v29113 = vxor.u32 %v29108, %v29112 (stack76)
        %v29116 = vadd.s32 %v29108, %v29113 (stack65)
        %v29120 = vadd.s32 %v29116, %v10 (stack65)
        %v29122 = vshll.u32 %v29113, 6 (stack73)
        %v29123 = vshrl.u32 %v29113, 26 (stack74)
        %v29124 = vor.u32 %v29122, %v29123 (stack75)
        %v29125 = vxor.u32 %v29116, %v29124 (stack76)
        %v29128 = vadd.s32 %v29125, %v9 (stack65)
        %v29132 = vadd.s32 %v29128, 3 (stack65)
        %v29136 = vadd.s32 %v29120, %v29132 (stack65)
        %v29138 = vshll.u32 %v29132, 17 (stack73)
        %v29139 = vshrl.u32 %v29132, 15 (stack74)
        %v29140 = vor.u32 %v29138, %v29139 (stack75)
        %v29141 = vxor.u32 %v29136, %v29140 (stack76)
        %v29144 = vadd.s32 %v29136, %v29141 (stack65)
        %v29146 = vshll.u32 %v29141, 29 (stack73)
        %v29147 = vshrl.u32 %v29141, 3 (stack74)
        %v29148 = vor.u32 %v29146, %v29147 (stack75)
        %v29149 = vxor.u32 %v29144, %v29148 (stack76)
        %v29152 = vadd.s32 %v29144, %v29149 (stack65)
        %v29154 = vshll.u32 %v29149, 16 (stack73)
        %v29155 = vshrl.u32 %v29149, 16 (stack74)
        %v29156 = vor.u32 %v29154, %v29155 (stack75)
        %v29157 = vxor.u32 %v29152, %v29156 (stack76)
        %v29160 = vadd.s32 %v29152, %v29157 (stack65)
        %v29164 = vadd.s32 %v29160, %v9 (stack65)
        %v29166 = vshll.u32 %v29157, 24 (stack73)
        %v29167 = vshrl.u32 %v29157, 8 (stack74)
        %v29168 = vor.u32 %v29166, %v29167 (stack75)
        %v29169 = vxor.u32 %v29160, %v29168 (stack76)
        %v29172 = vadd.s32 %v29169, %v8 (stack65)
        %v29176 = vadd.s32 %v29172, 4 (stack65)
        %v29180 = vadd.s32 %v29164, %v29176 (stack65)
        %v29182 = vshll.u32 %v29176, 13 (stack73)
        %v29183 = vshrl.u32 %v29176, 19 (stack74)
        %v29184 = vor.u32 %v29182, %v29183 (stack75)
        %v29185 = vxor.u32 %v29180, %v29184 (stack76)
        %v29188 = vadd.s32 %v29180, %v29185 (stack65)
        %v29190 = vshll.u32 %v29185, 15 (stack73)
        %v29191 = vshrl.u32 %v29185, 17 (stack74)
        %v29192 = vor.u32 %v29190, %v29191 (stack75)
        %v29193 = vxor.u32 %v29188, %v29192 (stack76)
        %v29196 = vadd.s32 %v29188, %v29193 (stack65)
        %v29198 = vshll.u32 %v29193, 26 (stack73)
        %v29199 = vshrl.u32 %v29193, 6 (stack74)
        %v29200 = vor.u32 %v29198, %v29199 (stack75)
        %v29201 = vxor.u32 %v29196, %v29200 (stack76)
        %v29204 = vadd.s32 %v29196, %v29201 (stack65)
        %v29208 = vadd.s32 %v29204, %v8 (stack65)
        %v29210 = vshll.u32 %v29201, 6 (stack73)
        %v29211 = vshrl.u32 %v29201, 26 (stack74)
        %v29212 = vor.u32 %v29210, %v29211 (stack75)
        %v29213 = vxor.u32 %v29204, %v29212 (stack76)
        %v29216 = vadd.s32 %v29213, %v10 (stack65)
        %v29220 = vadd.s32 %v29216, 5 (stack65)
        %v29222 = vxor.u32 %v29208, %v29220 (stack76)
        %v29223 = vand.u32.u8 %v29222, 255 (stack77)
        %v29224 = vand.u32 %v29223, 65535 (stack78)
        %v29225 = vshrl.u32 %v29224, 1 (stack79)
        %v29226 = vor.u32 %v29225, 16256 (stack75)
        %v29227 = vand.u32.u16 %v29226, 65535 (stack80)
        %v29228 = vunpack.i.l.bf16 %v29227 (stack81)
        %v29232 = vadd.f32 %v29228, -1.0 (stack82)
        %v29236 = vmul.f32 %v29232, 2.0 (stack83)
        %v29240 = vadd.f32 %v29236, -0.99609375 (stack82)
        %v29244 = vmax.f32 -0.99609375, %v29240 (stack84)
        %v29246 = vand.u32 2147483647, %v29244 (stack85)
        %vm29249 = vcmp.eq.f32.partialorder %v29246, 1.0 (stack86)
        %v29254 = vmul.f32 %v29244, inf (stack83)
        %v29256 = vxor.u32 %v29244, 2147483648 (stack87)
        %v29259 = vmul.f32 %v29244, %v29256 (stack83)
        %v29261 = vadd.f32 %v29259, 1.0 (stack88)
        %v29262 = vlog2.pop %v29261 (stack89)
        %v29263 = vmul.f32 %v29262, 0.6931472 (stack90)
        %v29264 = vmul.f32 -0.5, %v29259 (stack91)
        %v29265 = vadd.f32 %v29264, 1.0 (stack92)
        %v29266 = vmul.f32 %v29265, %v29259 (stack93)
        %v29267 = vand.u32 2147483647, %v29259 (stack94)
        %vm29268 = vcmp.lt.f32.partialorder %v29267, 0.0004427343 (stack95)
        %v29269 = vsel /*vm=*/%vm29268, /*on_true_vy=*/%v29266, /*on_false_vx=*/%v29263 (stack96)
        %v29270 = vxor.u32 %v29269, 2147483648 (stack87)
        %vm29273 = vcmp.lt.f32.partialorder %v29270, 5.0 (stack86)
        %v29278 = vsel /*vm=*/%vm29273, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v29282 = vsel /*vm=*/%vm29273, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v29286 = vsel /*vm=*/%vm29273, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v29290 = vsel /*vm=*/%vm29273, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v29294 = vsel /*vm=*/%vm29273, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v29298 = vsel /*vm=*/%vm29273, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v29302 = vsel /*vm=*/%vm29273, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v29306 = vsel /*vm=*/%vm29273, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v29310 = vsel /*vm=*/%vm29273, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v29314 = vadd.f32 %v29270, -2.5 (stack82)
        %v29316 = vrsqrt.pop %v29270 (stack97)
        %v29317 = vmul.f32 %v29270, %v29316 (stack98)
        %vm29318 = vcmp.eq.f32.partialorder %v29270, inf (stack99)
        %v29319 = vsel /*vm=*/%vm29318, /*on_true_vy=*/%v29270, /*on_false_vx=*/%v29317 (stack100)
        %vm29320 = vcmp.eq.f32.partialorder %v29270, 0.0 (stack101)
        %v29321 = vand.u32 %v29270, 2147483648 (stack102)
        %v29322 = vsel /*vm=*/%vm29320, /*on_true_vy=*/%v29321, /*on_false_vx=*/%v29319 (stack103)
        %v29325 = vadd.f32 %v29322, -3.0 (stack82)
        %v29329 = vsel /*vm=*/%vm29273, /*on_true_vy=*/%v29314, /*on_false_vx=*/%v29325 (stack72)
        %v29333 = vmul.f32 %v29310, %v29329 (stack83)
        %v29337 = vadd.f32 %v29306, %v29333 (stack82)
        %v29341 = vmul.f32 %v29337, %v29329 (stack83)
        %v29345 = vadd.f32 %v29302, %v29341 (stack82)
        %v29349 = vmul.f32 %v29345, %v29329 (stack83)
        %v29353 = vadd.f32 %v29298, %v29349 (stack82)
        %v29357 = vmul.f32 %v29353, %v29329 (stack83)
        %v29361 = vadd.f32 %v29294, %v29357 (stack82)
        %v29365 = vmul.f32 %v29361, %v29329 (stack83)
        %v29369 = vadd.f32 %v29290, %v29365 (stack82)
        %v29373 = vmul.f32 %v29369, %v29329 (stack83)
        %v29377 = vadd.f32 %v29286, %v29373 (stack82)
        %v29381 = vmul.f32 %v29377, %v29329 (stack83)
        %v29385 = vadd.f32 %v29282, %v29381 (stack82)
        %v29389 = vmul.f32 %v29385, %v29329 (stack83)
        %v29393 = vadd.f32 %v29278, %v29389 (stack82)
        %v29397 = vmul.f32 %v29393, %v29244 (stack83)
        %v29401 = vsel /*vm=*/%vm29249, /*on_true_vy=*/%v29254, /*on_false_vx=*/%v29397 (stack72)
        %v29405 = vmul.f32 %v29401, 1.4140625 (stack83)
        %s29407 = scalar_lea.vmem %s280, 668 [#allocation0] (stack107)
        %v29408 = vpack.c.bf16 0.0, %v29405 (stack104)
        %29409 = vst [vmem:[%s29407] sm:$0xf] /*vst_source=*/%v29408 (stack105)
        %v29412 = vadd.s32 %v3329, %v26643 (stack65)
        %s29414 = smul.u32 128, %s27 (stack66)
        %v29415 = vlaneseq (stack67)
        %v29416 = vand.u32 %v29415, 127 (stack68)
        %v29417 = vstv %s29414 (stack69)
        %v29418 = vadd.s32 %v29416, %v29417 (stack70)
        %v29422 = vadd.s32 %v29412, %v29418 (stack65)
        %vm29426 = vcmp.lt.u32.totalorder %v29422, %v29412 (stack71)
        %vm29431 = vcmp.lt.u32.totalorder %v29412, %v3329 (stack71)
        %v29436 = vadd.s32 %v3316, %v26626 (stack65)
        %v29440 = vadd.s32 %v29436, 1 (stack65)
        %v29444 = vsel /*vm=*/%vm29431, /*on_true_vy=*/%v29440, /*on_false_vx=*/%v29436 (stack72)
        %v29448 = vadd.s32 %v29444, 1 (stack65)
        %v29452 = vsel /*vm=*/%vm29426, /*on_true_vy=*/%v29448, /*on_false_vx=*/%v29444 (stack72)
        %v29457 = vadd.s32 %v29452, %v10 (stack65)
        %v29461 = vadd.s32 %v29422, %v9 (stack65)
        %v29465 = vadd.s32 %v29457, %v29461 (stack65)
        %v29467 = vshll.u32 %v29461, 13 (stack73)
        %v29468 = vshrl.u32 %v29461, 19 (stack74)
        %v29469 = vor.u32 %v29467, %v29468 (stack75)
        %v29470 = vxor.u32 %v29465, %v29469 (stack76)
        %v29473 = vadd.s32 %v29465, %v29470 (stack65)
        %v29475 = vshll.u32 %v29470, 15 (stack73)
        %v29476 = vshrl.u32 %v29470, 17 (stack74)
        %v29477 = vor.u32 %v29475, %v29476 (stack75)
        %v29478 = vxor.u32 %v29473, %v29477 (stack76)
        %v29481 = vadd.s32 %v29473, %v29478 (stack65)
        %v29483 = vshll.u32 %v29478, 26 (stack73)
        %v29484 = vshrl.u32 %v29478, 6 (stack74)
        %v29485 = vor.u32 %v29483, %v29484 (stack75)
        %v29486 = vxor.u32 %v29481, %v29485 (stack76)
        %v29489 = vadd.s32 %v29481, %v29486 (stack65)
        %v29493 = vadd.s32 %v29489, %v9 (stack65)
        %v29495 = vshll.u32 %v29486, 6 (stack73)
        %v29496 = vshrl.u32 %v29486, 26 (stack74)
        %v29497 = vor.u32 %v29495, %v29496 (stack75)
        %v29498 = vxor.u32 %v29489, %v29497 (stack76)
        %v29501 = vadd.s32 %v29498, %v8 (stack65)
        %v29505 = vadd.s32 %v29501, 1 (stack65)
        %v29509 = vadd.s32 %v29493, %v29505 (stack65)
        %v29511 = vshll.u32 %v29505, 17 (stack73)
        %v29512 = vshrl.u32 %v29505, 15 (stack74)
        %v29513 = vor.u32 %v29511, %v29512 (stack75)
        %v29514 = vxor.u32 %v29509, %v29513 (stack76)
        %v29517 = vadd.s32 %v29509, %v29514 (stack65)
        %v29519 = vshll.u32 %v29514, 29 (stack73)
        %v29520 = vshrl.u32 %v29514, 3 (stack74)
        %v29521 = vor.u32 %v29519, %v29520 (stack75)
        %v29522 = vxor.u32 %v29517, %v29521 (stack76)
        %v29525 = vadd.s32 %v29517, %v29522 (stack65)
        %v29527 = vshll.u32 %v29522, 16 (stack73)
        %v29528 = vshrl.u32 %v29522, 16 (stack74)
        %v29529 = vor.u32 %v29527, %v29528 (stack75)
        %v29530 = vxor.u32 %v29525, %v29529 (stack76)
        %v29533 = vadd.s32 %v29525, %v29530 (stack65)
        %v29537 = vadd.s32 %v29533, %v8 (stack65)
        %v29539 = vshll.u32 %v29530, 24 (stack73)
        %v29540 = vshrl.u32 %v29530, 8 (stack74)
        %v29541 = vor.u32 %v29539, %v29540 (stack75)
        %v29542 = vxor.u32 %v29533, %v29541 (stack76)
        %v29545 = vadd.s32 %v29542, %v10 (stack65)
        %v29549 = vadd.s32 %v29545, 2 (stack65)
        %v29553 = vadd.s32 %v29537, %v29549 (stack65)
        %v29555 = vshll.u32 %v29549, 13 (stack73)
        %v29556 = vshrl.u32 %v29549, 19 (stack74)
        %v29557 = vor.u32 %v29555, %v29556 (stack75)
        %v29558 = vxor.u32 %v29553, %v29557 (stack76)
        %v29561 = vadd.s32 %v29553, %v29558 (stack65)
        %v29563 = vshll.u32 %v29558, 15 (stack73)
        %v29564 = vshrl.u32 %v29558, 17 (stack74)
        %v29565 = vor.u32 %v29563, %v29564 (stack75)
        %v29566 = vxor.u32 %v29561, %v29565 (stack76)
        %v29569 = vadd.s32 %v29561, %v29566 (stack65)
        %v29571 = vshll.u32 %v29566, 26 (stack73)
        %v29572 = vshrl.u32 %v29566, 6 (stack74)
        %v29573 = vor.u32 %v29571, %v29572 (stack75)
        %v29574 = vxor.u32 %v29569, %v29573 (stack76)
        %v29577 = vadd.s32 %v29569, %v29574 (stack65)
        %v29581 = vadd.s32 %v29577, %v10 (stack65)
        %v29583 = vshll.u32 %v29574, 6 (stack73)
        %v29584 = vshrl.u32 %v29574, 26 (stack74)
        %v29585 = vor.u32 %v29583, %v29584 (stack75)
        %v29586 = vxor.u32 %v29577, %v29585 (stack76)
        %v29589 = vadd.s32 %v29586, %v9 (stack65)
        %v29593 = vadd.s32 %v29589, 3 (stack65)
        %v29597 = vadd.s32 %v29581, %v29593 (stack65)
        %v29599 = vshll.u32 %v29593, 17 (stack73)
        %v29600 = vshrl.u32 %v29593, 15 (stack74)
        %v29601 = vor.u32 %v29599, %v29600 (stack75)
        %v29602 = vxor.u32 %v29597, %v29601 (stack76)
        %v29605 = vadd.s32 %v29597, %v29602 (stack65)
        %v29607 = vshll.u32 %v29602, 29 (stack73)
        %v29608 = vshrl.u32 %v29602, 3 (stack74)
        %v29609 = vor.u32 %v29607, %v29608 (stack75)
        %v29610 = vxor.u32 %v29605, %v29609 (stack76)
        %v29613 = vadd.s32 %v29605, %v29610 (stack65)
        %v29615 = vshll.u32 %v29610, 16 (stack73)
        %v29616 = vshrl.u32 %v29610, 16 (stack74)
        %v29617 = vor.u32 %v29615, %v29616 (stack75)
        %v29618 = vxor.u32 %v29613, %v29617 (stack76)
        %v29621 = vadd.s32 %v29613, %v29618 (stack65)
        %v29625 = vadd.s32 %v29621, %v9 (stack65)
        %v29627 = vshll.u32 %v29618, 24 (stack73)
        %v29628 = vshrl.u32 %v29618, 8 (stack74)
        %v29629 = vor.u32 %v29627, %v29628 (stack75)
        %v29630 = vxor.u32 %v29621, %v29629 (stack76)
        %v29633 = vadd.s32 %v29630, %v8 (stack65)
        %v29637 = vadd.s32 %v29633, 4 (stack65)
        %v29641 = vadd.s32 %v29625, %v29637 (stack65)
        %v29643 = vshll.u32 %v29637, 13 (stack73)
        %v29644 = vshrl.u32 %v29637, 19 (stack74)
        %v29645 = vor.u32 %v29643, %v29644 (stack75)
        %v29646 = vxor.u32 %v29641, %v29645 (stack76)
        %v29649 = vadd.s32 %v29641, %v29646 (stack65)
        %v29651 = vshll.u32 %v29646, 15 (stack73)
        %v29652 = vshrl.u32 %v29646, 17 (stack74)
        %v29653 = vor.u32 %v29651, %v29652 (stack75)
        %v29654 = vxor.u32 %v29649, %v29653 (stack76)
        %v29657 = vadd.s32 %v29649, %v29654 (stack65)
        %v29659 = vshll.u32 %v29654, 26 (stack73)
        %v29660 = vshrl.u32 %v29654, 6 (stack74)
        %v29661 = vor.u32 %v29659, %v29660 (stack75)
        %v29662 = vxor.u32 %v29657, %v29661 (stack76)
        %v29665 = vadd.s32 %v29657, %v29662 (stack65)
        %v29669 = vadd.s32 %v29665, %v8 (stack65)
        %v29671 = vshll.u32 %v29662, 6 (stack73)
        %v29672 = vshrl.u32 %v29662, 26 (stack74)
        %v29673 = vor.u32 %v29671, %v29672 (stack75)
        %v29674 = vxor.u32 %v29665, %v29673 (stack76)
        %v29677 = vadd.s32 %v29674, %v10 (stack65)
        %v29681 = vadd.s32 %v29677, 5 (stack65)
        %v29683 = vxor.u32 %v29669, %v29681 (stack76)
        %v29684 = vand.u32.u8 %v29683, 255 (stack77)
        %v29685 = vand.u32 %v29684, 65535 (stack78)
        %v29686 = vshrl.u32 %v29685, 1 (stack79)
        %v29687 = vor.u32 %v29686, 16256 (stack75)
        %v29688 = vand.u32.u16 %v29687, 65535 (stack80)
        %v29689 = vunpack.i.l.bf16 %v29688 (stack81)
        %v29693 = vadd.f32 %v29689, -1.0 (stack82)
        %v29697 = vmul.f32 %v29693, 2.0 (stack83)
        %v29701 = vadd.f32 %v29697, -0.99609375 (stack82)
        %v29705 = vmax.f32 -0.99609375, %v29701 (stack84)
        %v29707 = vand.u32 2147483647, %v29705 (stack85)
        %vm29710 = vcmp.eq.f32.partialorder %v29707, 1.0 (stack86)
        %v29715 = vmul.f32 %v29705, inf (stack83)
        %v29717 = vxor.u32 %v29705, 2147483648 (stack87)
        %v29720 = vmul.f32 %v29705, %v29717 (stack83)
        %v29722 = vadd.f32 %v29720, 1.0 (stack88)
        %v29723 = vlog2.pop %v29722 (stack89)
        %v29724 = vmul.f32 %v29723, 0.6931472 (stack90)
        %v29725 = vmul.f32 -0.5, %v29720 (stack91)
        %v29726 = vadd.f32 %v29725, 1.0 (stack92)
        %v29727 = vmul.f32 %v29726, %v29720 (stack93)
        %v29728 = vand.u32 2147483647, %v29720 (stack94)
        %vm29729 = vcmp.lt.f32.partialorder %v29728, 0.0004427343 (stack95)
        %v29730 = vsel /*vm=*/%vm29729, /*on_true_vy=*/%v29727, /*on_false_vx=*/%v29724 (stack96)
        %v29731 = vxor.u32 %v29730, 2147483648 (stack87)
        %vm29734 = vcmp.lt.f32.partialorder %v29731, 5.0 (stack86)
        %v29739 = vsel /*vm=*/%vm29734, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v29743 = vsel /*vm=*/%vm29734, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v29747 = vsel /*vm=*/%vm29734, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v29751 = vsel /*vm=*/%vm29734, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v29755 = vsel /*vm=*/%vm29734, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v29759 = vsel /*vm=*/%vm29734, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v29763 = vsel /*vm=*/%vm29734, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v29767 = vsel /*vm=*/%vm29734, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v29771 = vsel /*vm=*/%vm29734, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v29775 = vadd.f32 %v29731, -2.5 (stack82)
        %v29777 = vrsqrt.pop %v29731 (stack97)
        %v29778 = vmul.f32 %v29731, %v29777 (stack98)
        %vm29779 = vcmp.eq.f32.partialorder %v29731, inf (stack99)
        %v29780 = vsel /*vm=*/%vm29779, /*on_true_vy=*/%v29731, /*on_false_vx=*/%v29778 (stack100)
        %vm29781 = vcmp.eq.f32.partialorder %v29731, 0.0 (stack101)
        %v29782 = vand.u32 %v29731, 2147483648 (stack102)
        %v29783 = vsel /*vm=*/%vm29781, /*on_true_vy=*/%v29782, /*on_false_vx=*/%v29780 (stack103)
        %v29786 = vadd.f32 %v29783, -3.0 (stack82)
        %v29790 = vsel /*vm=*/%vm29734, /*on_true_vy=*/%v29775, /*on_false_vx=*/%v29786 (stack72)
        %v29794 = vmul.f32 %v29771, %v29790 (stack83)
        %v29798 = vadd.f32 %v29767, %v29794 (stack82)
        %v29802 = vmul.f32 %v29798, %v29790 (stack83)
        %v29806 = vadd.f32 %v29763, %v29802 (stack82)
        %v29810 = vmul.f32 %v29806, %v29790 (stack83)
        %v29814 = vadd.f32 %v29759, %v29810 (stack82)
        %v29818 = vmul.f32 %v29814, %v29790 (stack83)
        %v29822 = vadd.f32 %v29755, %v29818 (stack82)
        %v29826 = vmul.f32 %v29822, %v29790 (stack83)
        %v29830 = vadd.f32 %v29751, %v29826 (stack82)
        %v29834 = vmul.f32 %v29830, %v29790 (stack83)
        %v29838 = vadd.f32 %v29747, %v29834 (stack82)
        %v29842 = vmul.f32 %v29838, %v29790 (stack83)
        %v29846 = vadd.f32 %v29743, %v29842 (stack82)
        %v29850 = vmul.f32 %v29846, %v29790 (stack83)
        %v29854 = vadd.f32 %v29739, %v29850 (stack82)
        %v29858 = vmul.f32 %v29854, %v29705 (stack83)
        %v29862 = vsel /*vm=*/%vm29710, /*on_true_vy=*/%v29715, /*on_false_vx=*/%v29858 (stack72)
        %v29866 = vmul.f32 %v29862, 1.4140625 (stack83)
        %s29868 = scalar_lea.vmem %s280, 796 [#allocation0] (stack107)
        %v29869 = vpack.c.bf16 0.0, %v29866 (stack104)
        %29870 = vst [vmem:[%s29868] sm:$0xf] /*vst_source=*/%v29869 (stack105)
        %v29873 = vadd.s32 %v3816, %v26643 (stack65)
        %s29875 = smul.u32 128, %s27 (stack66)
        %v29876 = vlaneseq (stack67)
        %v29877 = vand.u32 %v29876, 127 (stack68)
        %v29878 = vstv %s29875 (stack69)
        %v29879 = vadd.s32 %v29877, %v29878 (stack70)
        %v29883 = vadd.s32 %v29873, %v29879 (stack65)
        %vm29887 = vcmp.lt.u32.totalorder %v29883, %v29873 (stack71)
        %vm29892 = vcmp.lt.u32.totalorder %v29873, %v3816 (stack71)
        %v29897 = vadd.s32 %v3803, %v26626 (stack65)
        %v29901 = vadd.s32 %v29897, 1 (stack65)
        %v29905 = vsel /*vm=*/%vm29892, /*on_true_vy=*/%v29901, /*on_false_vx=*/%v29897 (stack72)
        %v29909 = vadd.s32 %v29905, 1 (stack65)
        %v29913 = vsel /*vm=*/%vm29887, /*on_true_vy=*/%v29909, /*on_false_vx=*/%v29905 (stack72)
        %v29918 = vadd.s32 %v29913, %v10 (stack65)
        %v29922 = vadd.s32 %v29883, %v9 (stack65)
        %v29926 = vadd.s32 %v29918, %v29922 (stack65)
        %v29928 = vshll.u32 %v29922, 13 (stack73)
        %v29929 = vshrl.u32 %v29922, 19 (stack74)
        %v29930 = vor.u32 %v29928, %v29929 (stack75)
        %v29931 = vxor.u32 %v29926, %v29930 (stack76)
        %v29934 = vadd.s32 %v29926, %v29931 (stack65)
        %v29936 = vshll.u32 %v29931, 15 (stack73)
        %v29937 = vshrl.u32 %v29931, 17 (stack74)
        %v29938 = vor.u32 %v29936, %v29937 (stack75)
        %v29939 = vxor.u32 %v29934, %v29938 (stack76)
        %v29942 = vadd.s32 %v29934, %v29939 (stack65)
        %v29944 = vshll.u32 %v29939, 26 (stack73)
        %v29945 = vshrl.u32 %v29939, 6 (stack74)
        %v29946 = vor.u32 %v29944, %v29945 (stack75)
        %v29947 = vxor.u32 %v29942, %v29946 (stack76)
        %v29950 = vadd.s32 %v29942, %v29947 (stack65)
        %v29954 = vadd.s32 %v29950, %v9 (stack65)
        %v29956 = vshll.u32 %v29947, 6 (stack73)
        %v29957 = vshrl.u32 %v29947, 26 (stack74)
        %v29958 = vor.u32 %v29956, %v29957 (stack75)
        %v29959 = vxor.u32 %v29950, %v29958 (stack76)
        %v29962 = vadd.s32 %v29959, %v8 (stack65)
        %v29966 = vadd.s32 %v29962, 1 (stack65)
        %v29970 = vadd.s32 %v29954, %v29966 (stack65)
        %v29972 = vshll.u32 %v29966, 17 (stack73)
        %v29973 = vshrl.u32 %v29966, 15 (stack74)
        %v29974 = vor.u32 %v29972, %v29973 (stack75)
        %v29975 = vxor.u32 %v29970, %v29974 (stack76)
        %v29978 = vadd.s32 %v29970, %v29975 (stack65)
        %v29980 = vshll.u32 %v29975, 29 (stack73)
        %v29981 = vshrl.u32 %v29975, 3 (stack74)
        %v29982 = vor.u32 %v29980, %v29981 (stack75)
        %v29983 = vxor.u32 %v29978, %v29982 (stack76)
        %v29986 = vadd.s32 %v29978, %v29983 (stack65)
        %v29988 = vshll.u32 %v29983, 16 (stack73)
        %v29989 = vshrl.u32 %v29983, 16 (stack74)
        %v29990 = vor.u32 %v29988, %v29989 (stack75)
        %v29991 = vxor.u32 %v29986, %v29990 (stack76)
        %v29994 = vadd.s32 %v29986, %v29991 (stack65)
        %v29998 = vadd.s32 %v29994, %v8 (stack65)
        %v30000 = vshll.u32 %v29991, 24 (stack73)
        %v30001 = vshrl.u32 %v29991, 8 (stack74)
        %v30002 = vor.u32 %v30000, %v30001 (stack75)
        %v30003 = vxor.u32 %v29994, %v30002 (stack76)
        %v30006 = vadd.s32 %v30003, %v10 (stack65)
        %v30010 = vadd.s32 %v30006, 2 (stack65)
        %v30014 = vadd.s32 %v29998, %v30010 (stack65)
        %v30016 = vshll.u32 %v30010, 13 (stack73)
        %v30017 = vshrl.u32 %v30010, 19 (stack74)
        %v30018 = vor.u32 %v30016, %v30017 (stack75)
        %v30019 = vxor.u32 %v30014, %v30018 (stack76)
        %v30022 = vadd.s32 %v30014, %v30019 (stack65)
        %v30024 = vshll.u32 %v30019, 15 (stack73)
        %v30025 = vshrl.u32 %v30019, 17 (stack74)
        %v30026 = vor.u32 %v30024, %v30025 (stack75)
        %v30027 = vxor.u32 %v30022, %v30026 (stack76)
        %v30030 = vadd.s32 %v30022, %v30027 (stack65)
        %v30032 = vshll.u32 %v30027, 26 (stack73)
        %v30033 = vshrl.u32 %v30027, 6 (stack74)
        %v30034 = vor.u32 %v30032, %v30033 (stack75)
        %v30035 = vxor.u32 %v30030, %v30034 (stack76)
        %v30038 = vadd.s32 %v30030, %v30035 (stack65)
        %v30042 = vadd.s32 %v30038, %v10 (stack65)
        %v30044 = vshll.u32 %v30035, 6 (stack73)
        %v30045 = vshrl.u32 %v30035, 26 (stack74)
        %v30046 = vor.u32 %v30044, %v30045 (stack75)
        %v30047 = vxor.u32 %v30038, %v30046 (stack76)
        %v30050 = vadd.s32 %v30047, %v9 (stack65)
        %v30054 = vadd.s32 %v30050, 3 (stack65)
        %v30058 = vadd.s32 %v30042, %v30054 (stack65)
        %v30060 = vshll.u32 %v30054, 17 (stack73)
        %v30061 = vshrl.u32 %v30054, 15 (stack74)
        %v30062 = vor.u32 %v30060, %v30061 (stack75)
        %v30063 = vxor.u32 %v30058, %v30062 (stack76)
        %v30066 = vadd.s32 %v30058, %v30063 (stack65)
        %v30068 = vshll.u32 %v30063, 29 (stack73)
        %v30069 = vshrl.u32 %v30063, 3 (stack74)
        %v30070 = vor.u32 %v30068, %v30069 (stack75)
        %v30071 = vxor.u32 %v30066, %v30070 (stack76)
        %v30074 = vadd.s32 %v30066, %v30071 (stack65)
        %v30076 = vshll.u32 %v30071, 16 (stack73)
        %v30077 = vshrl.u32 %v30071, 16 (stack74)
        %v30078 = vor.u32 %v30076, %v30077 (stack75)
        %v30079 = vxor.u32 %v30074, %v30078 (stack76)
        %v30082 = vadd.s32 %v30074, %v30079 (stack65)
        %v30086 = vadd.s32 %v30082, %v9 (stack65)
        %v30088 = vshll.u32 %v30079, 24 (stack73)
        %v30089 = vshrl.u32 %v30079, 8 (stack74)
        %v30090 = vor.u32 %v30088, %v30089 (stack75)
        %v30091 = vxor.u32 %v30082, %v30090 (stack76)
        %v30094 = vadd.s32 %v30091, %v8 (stack65)
        %v30098 = vadd.s32 %v30094, 4 (stack65)
        %v30102 = vadd.s32 %v30086, %v30098 (stack65)
        %v30104 = vshll.u32 %v30098, 13 (stack73)
        %v30105 = vshrl.u32 %v30098, 19 (stack74)
        %v30106 = vor.u32 %v30104, %v30105 (stack75)
        %v30107 = vxor.u32 %v30102, %v30106 (stack76)
        %v30110 = vadd.s32 %v30102, %v30107 (stack65)
        %v30112 = vshll.u32 %v30107, 15 (stack73)
        %v30113 = vshrl.u32 %v30107, 17 (stack74)
        %v30114 = vor.u32 %v30112, %v30113 (stack75)
        %v30115 = vxor.u32 %v30110, %v30114 (stack76)
        %v30118 = vadd.s32 %v30110, %v30115 (stack65)
        %v30120 = vshll.u32 %v30115, 26 (stack73)
        %v30121 = vshrl.u32 %v30115, 6 (stack74)
        %v30122 = vor.u32 %v30120, %v30121 (stack75)
        %v30123 = vxor.u32 %v30118, %v30122 (stack76)
        %v30126 = vadd.s32 %v30118, %v30123 (stack65)
        %v30130 = vadd.s32 %v30126, %v8 (stack65)
        %v30132 = vshll.u32 %v30123, 6 (stack73)
        %v30133 = vshrl.u32 %v30123, 26 (stack74)
        %v30134 = vor.u32 %v30132, %v30133 (stack75)
        %v30135 = vxor.u32 %v30126, %v30134 (stack76)
        %v30138 = vadd.s32 %v30135, %v10 (stack65)
        %v30142 = vadd.s32 %v30138, 5 (stack65)
        %v30144 = vxor.u32 %v30130, %v30142 (stack76)
        %v30145 = vand.u32.u8 %v30144, 255 (stack77)
        %v30146 = vand.u32 %v30145, 65535 (stack78)
        %v30147 = vshrl.u32 %v30146, 1 (stack79)
        %v30148 = vor.u32 %v30147, 16256 (stack75)
        %v30149 = vand.u32.u16 %v30148, 65535 (stack80)
        %v30150 = vunpack.i.l.bf16 %v30149 (stack81)
        %v30154 = vadd.f32 %v30150, -1.0 (stack82)
        %v30158 = vmul.f32 %v30154, 2.0 (stack83)
        %v30162 = vadd.f32 %v30158, -0.99609375 (stack82)
        %v30166 = vmax.f32 -0.99609375, %v30162 (stack84)
        %v30168 = vand.u32 2147483647, %v30166 (stack85)
        %vm30171 = vcmp.eq.f32.partialorder %v30168, 1.0 (stack86)
        %v30176 = vmul.f32 %v30166, inf (stack83)
        %v30178 = vxor.u32 %v30166, 2147483648 (stack87)
        %v30181 = vmul.f32 %v30166, %v30178 (stack83)
        %v30183 = vadd.f32 %v30181, 1.0 (stack88)
        %v30184 = vlog2.pop %v30183 (stack89)
        %v30185 = vmul.f32 %v30184, 0.6931472 (stack90)
        %v30186 = vmul.f32 -0.5, %v30181 (stack91)
        %v30187 = vadd.f32 %v30186, 1.0 (stack92)
        %v30188 = vmul.f32 %v30187, %v30181 (stack93)
        %v30189 = vand.u32 2147483647, %v30181 (stack94)
        %vm30190 = vcmp.lt.f32.partialorder %v30189, 0.0004427343 (stack95)
        %v30191 = vsel /*vm=*/%vm30190, /*on_true_vy=*/%v30188, /*on_false_vx=*/%v30185 (stack96)
        %v30192 = vxor.u32 %v30191, 2147483648 (stack87)
        %vm30195 = vcmp.lt.f32.partialorder %v30192, 5.0 (stack86)
        %v30200 = vsel /*vm=*/%vm30195, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v30204 = vsel /*vm=*/%vm30195, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v30208 = vsel /*vm=*/%vm30195, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v30212 = vsel /*vm=*/%vm30195, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v30216 = vsel /*vm=*/%vm30195, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v30220 = vsel /*vm=*/%vm30195, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v30224 = vsel /*vm=*/%vm30195, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v30228 = vsel /*vm=*/%vm30195, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v30232 = vsel /*vm=*/%vm30195, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v30236 = vadd.f32 %v30192, -2.5 (stack82)
        %v30238 = vrsqrt.pop %v30192 (stack97)
        %v30239 = vmul.f32 %v30192, %v30238 (stack98)
        %vm30240 = vcmp.eq.f32.partialorder %v30192, inf (stack99)
        %v30241 = vsel /*vm=*/%vm30240, /*on_true_vy=*/%v30192, /*on_false_vx=*/%v30239 (stack100)
        %vm30242 = vcmp.eq.f32.partialorder %v30192, 0.0 (stack101)
        %v30243 = vand.u32 %v30192, 2147483648 (stack102)
        %v30244 = vsel /*vm=*/%vm30242, /*on_true_vy=*/%v30243, /*on_false_vx=*/%v30241 (stack103)
        %v30247 = vadd.f32 %v30244, -3.0 (stack82)
        %v30251 = vsel /*vm=*/%vm30195, /*on_true_vy=*/%v30236, /*on_false_vx=*/%v30247 (stack72)
        %v30255 = vmul.f32 %v30232, %v30251 (stack83)
        %v30259 = vadd.f32 %v30228, %v30255 (stack82)
        %v30263 = vmul.f32 %v30259, %v30251 (stack83)
        %v30267 = vadd.f32 %v30224, %v30263 (stack82)
        %v30271 = vmul.f32 %v30267, %v30251 (stack83)
        %v30275 = vadd.f32 %v30220, %v30271 (stack82)
        %v30279 = vmul.f32 %v30275, %v30251 (stack83)
        %v30283 = vadd.f32 %v30216, %v30279 (stack82)
        %v30287 = vmul.f32 %v30283, %v30251 (stack83)
        %v30291 = vadd.f32 %v30212, %v30287 (stack82)
        %v30295 = vmul.f32 %v30291, %v30251 (stack83)
        %v30299 = vadd.f32 %v30208, %v30295 (stack82)
        %v30303 = vmul.f32 %v30299, %v30251 (stack83)
        %v30307 = vadd.f32 %v30204, %v30303 (stack82)
        %v30311 = vmul.f32 %v30307, %v30251 (stack83)
        %v30315 = vadd.f32 %v30200, %v30311 (stack82)
        %v30319 = vmul.f32 %v30315, %v30166 (stack83)
        %v30323 = vsel /*vm=*/%vm30171, /*on_true_vy=*/%v30176, /*on_false_vx=*/%v30319 (stack72)
        %v30327 = vmul.f32 %v30323, 1.4140625 (stack83)
        %s30329 = scalar_lea.vmem %s280, 924 [#allocation0] (stack107)
        %v30330 = vpack.c.bf16 0.0, %v30327 (stack104)
        %30331 = vst [vmem:[%s30329] sm:$0xf] /*vst_source=*/%v30330 (stack105)
        %s30332 = sadd.s32 %s339, 64 (stack106)
        %s30333 = sshrl.u32 %s30332, 10 (stack49)
        %p30334 = scmp.lt.s32.totalorder 1, %s30333 (stack50)
        %s30335 = scalar_select /*predicate=*/%p30334, /*on_true=*/1, /*on_false=*/%s30333 (stack51)
        %s30336 = sand.u32 %s30332, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s30337 = sshrl.u32 %s30336, 7 (stack53)
        %s30338 = sand.u32 %s30336, 127 /* smod.u32 w/div 128 */ (stack54)
        %s30339 = smul.addr %s30335, 8 (stack55)
        %s30340 = scalar_lea.vmem %s3, %s30339 (stack56)
        %s30342 = scalar_lea.vmem %s30340, %s30337 (stack57)
        %v30343 = vld [vmem:[%s30342] ss:$0 sm:$0xff] (stack58)
        %s30344 = sand.u32 %s30338, 255 (stack59)
        %s30346 = sor.u32 256, %s30344 (stack60)
        %30347 = vbcast.lane.b32.xlu0 %v30343, %s30346 (stack61)
        %v30348 = vpop.permute.xlu0 %30347 (stack62)
        %s30349 = sadd.s32 %s347, 64 (stack106)
        %s30350 = sshrl.u32 %s30349, 10 (stack49)
        %p30351 = scmp.lt.s32.totalorder 1, %s30350 (stack50)
        %s30352 = scalar_select /*predicate=*/%p30351, /*on_true=*/1, /*on_false=*/%s30350 (stack51)
        %s30353 = sand.u32 %s30349, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s30354 = sshrl.u32 %s30353, 7 (stack53)
        %s30355 = sand.u32 %s30353, 127 /* smod.u32 w/div 128 */ (stack54)
        %s30356 = smul.addr %s30352, 8 (stack55)
        %s30357 = scalar_lea.vmem %s5, %s30356 (stack56)
        %s30359 = scalar_lea.vmem %s30357, %s30354 (stack57)
        %v30360 = vld [vmem:[%s30359] ss:$0 sm:$0xff] (stack58)
        %s30361 = sand.u32 %s30355, 255 (stack59)
        %s30363 = sor.u32 256, %s30361 (stack60)
        %30364 = vbcast.lane.b32.xlu0 %v30360, %s30363 (stack61)
        %v30365 = vpop.permute.xlu0 %30364 (stack62)
        %v30368 = vadd.s32 %v408, %v30365 (stack65)
        %s30370 = smul.u32 128, %s27 (stack66)
        %v30371 = vlaneseq (stack67)
        %v30372 = vand.u32 %v30371, 127 (stack68)
        %v30373 = vstv %s30370 (stack69)
        %v30374 = vadd.s32 %v30372, %v30373 (stack70)
        %v30378 = vadd.s32 %v30368, %v30374 (stack65)
        %vm30382 = vcmp.lt.u32.totalorder %v30378, %v30368 (stack71)
        %vm30387 = vcmp.lt.u32.totalorder %v30368, %v408 (stack71)
        %v30392 = vadd.s32 %v380, %v30348 (stack65)
        %v30396 = vadd.s32 %v30392, 1 (stack65)
        %v30400 = vsel /*vm=*/%vm30387, /*on_true_vy=*/%v30396, /*on_false_vx=*/%v30392 (stack72)
        %v30404 = vadd.s32 %v30400, 1 (stack65)
        %v30408 = vsel /*vm=*/%vm30382, /*on_true_vy=*/%v30404, /*on_false_vx=*/%v30400 (stack72)
        %v30413 = vadd.s32 %v30408, %v10 (stack65)
        %v30417 = vadd.s32 %v30378, %v9 (stack65)
        %v30421 = vadd.s32 %v30413, %v30417 (stack65)
        %v30423 = vshll.u32 %v30417, 13 (stack73)
        %v30424 = vshrl.u32 %v30417, 19 (stack74)
        %v30425 = vor.u32 %v30423, %v30424 (stack75)
        %v30426 = vxor.u32 %v30421, %v30425 (stack76)
        %v30429 = vadd.s32 %v30421, %v30426 (stack65)
        %v30431 = vshll.u32 %v30426, 15 (stack73)
        %v30432 = vshrl.u32 %v30426, 17 (stack74)
        %v30433 = vor.u32 %v30431, %v30432 (stack75)
        %v30434 = vxor.u32 %v30429, %v30433 (stack76)
        %v30437 = vadd.s32 %v30429, %v30434 (stack65)
        %v30439 = vshll.u32 %v30434, 26 (stack73)
        %v30440 = vshrl.u32 %v30434, 6 (stack74)
        %v30441 = vor.u32 %v30439, %v30440 (stack75)
        %v30442 = vxor.u32 %v30437, %v30441 (stack76)
        %v30445 = vadd.s32 %v30437, %v30442 (stack65)
        %v30449 = vadd.s32 %v30445, %v9 (stack65)
        %v30451 = vshll.u32 %v30442, 6 (stack73)
        %v30452 = vshrl.u32 %v30442, 26 (stack74)
        %v30453 = vor.u32 %v30451, %v30452 (stack75)
        %v30454 = vxor.u32 %v30445, %v30453 (stack76)
        %v30457 = vadd.s32 %v30454, %v8 (stack65)
        %v30461 = vadd.s32 %v30457, 1 (stack65)
        %v30465 = vadd.s32 %v30449, %v30461 (stack65)
        %v30467 = vshll.u32 %v30461, 17 (stack73)
        %v30468 = vshrl.u32 %v30461, 15 (stack74)
        %v30469 = vor.u32 %v30467, %v30468 (stack75)
        %v30470 = vxor.u32 %v30465, %v30469 (stack76)
        %v30473 = vadd.s32 %v30465, %v30470 (stack65)
        %v30475 = vshll.u32 %v30470, 29 (stack73)
        %v30476 = vshrl.u32 %v30470, 3 (stack74)
        %v30477 = vor.u32 %v30475, %v30476 (stack75)
        %v30478 = vxor.u32 %v30473, %v30477 (stack76)
        %v30481 = vadd.s32 %v30473, %v30478 (stack65)
        %v30483 = vshll.u32 %v30478, 16 (stack73)
        %v30484 = vshrl.u32 %v30478, 16 (stack74)
        %v30485 = vor.u32 %v30483, %v30484 (stack75)
        %v30486 = vxor.u32 %v30481, %v30485 (stack76)
        %v30489 = vadd.s32 %v30481, %v30486 (stack65)
        %v30493 = vadd.s32 %v30489, %v8 (stack65)
        %v30495 = vshll.u32 %v30486, 24 (stack73)
        %v30496 = vshrl.u32 %v30486, 8 (stack74)
        %v30497 = vor.u32 %v30495, %v30496 (stack75)
        %v30498 = vxor.u32 %v30489, %v30497 (stack76)
        %v30501 = vadd.s32 %v30498, %v10 (stack65)
        %v30505 = vadd.s32 %v30501, 2 (stack65)
        %v30509 = vadd.s32 %v30493, %v30505 (stack65)
        %v30511 = vshll.u32 %v30505, 13 (stack73)
        %v30512 = vshrl.u32 %v30505, 19 (stack74)
        %v30513 = vor.u32 %v30511, %v30512 (stack75)
        %v30514 = vxor.u32 %v30509, %v30513 (stack76)
        %v30517 = vadd.s32 %v30509, %v30514 (stack65)
        %v30519 = vshll.u32 %v30514, 15 (stack73)
        %v30520 = vshrl.u32 %v30514, 17 (stack74)
        %v30521 = vor.u32 %v30519, %v30520 (stack75)
        %v30522 = vxor.u32 %v30517, %v30521 (stack76)
        %v30525 = vadd.s32 %v30517, %v30522 (stack65)
        %v30527 = vshll.u32 %v30522, 26 (stack73)
        %v30528 = vshrl.u32 %v30522, 6 (stack74)
        %v30529 = vor.u32 %v30527, %v30528 (stack75)
        %v30530 = vxor.u32 %v30525, %v30529 (stack76)
        %v30533 = vadd.s32 %v30525, %v30530 (stack65)
        %v30537 = vadd.s32 %v30533, %v10 (stack65)
        %v30539 = vshll.u32 %v30530, 6 (stack73)
        %v30540 = vshrl.u32 %v30530, 26 (stack74)
        %v30541 = vor.u32 %v30539, %v30540 (stack75)
        %v30542 = vxor.u32 %v30533, %v30541 (stack76)
        %v30545 = vadd.s32 %v30542, %v9 (stack65)
        %v30549 = vadd.s32 %v30545, 3 (stack65)
        %v30553 = vadd.s32 %v30537, %v30549 (stack65)
        %v30555 = vshll.u32 %v30549, 17 (stack73)
        %v30556 = vshrl.u32 %v30549, 15 (stack74)
        %v30557 = vor.u32 %v30555, %v30556 (stack75)
        %v30558 = vxor.u32 %v30553, %v30557 (stack76)
        %v30561 = vadd.s32 %v30553, %v30558 (stack65)
        %v30563 = vshll.u32 %v30558, 29 (stack73)
        %v30564 = vshrl.u32 %v30558, 3 (stack74)
        %v30565 = vor.u32 %v30563, %v30564 (stack75)
        %v30566 = vxor.u32 %v30561, %v30565 (stack76)
        %v30569 = vadd.s32 %v30561, %v30566 (stack65)
        %v30571 = vshll.u32 %v30566, 16 (stack73)
        %v30572 = vshrl.u32 %v30566, 16 (stack74)
        %v30573 = vor.u32 %v30571, %v30572 (stack75)
        %v30574 = vxor.u32 %v30569, %v30573 (stack76)
        %v30577 = vadd.s32 %v30569, %v30574 (stack65)
        %v30581 = vadd.s32 %v30577, %v9 (stack65)
        %v30583 = vshll.u32 %v30574, 24 (stack73)
        %v30584 = vshrl.u32 %v30574, 8 (stack74)
        %v30585 = vor.u32 %v30583, %v30584 (stack75)
        %v30586 = vxor.u32 %v30577, %v30585 (stack76)
        %v30589 = vadd.s32 %v30586, %v8 (stack65)
        %v30593 = vadd.s32 %v30589, 4 (stack65)
        %v30597 = vadd.s32 %v30581, %v30593 (stack65)
        %v30599 = vshll.u32 %v30593, 13 (stack73)
        %v30600 = vshrl.u32 %v30593, 19 (stack74)
        %v30601 = vor.u32 %v30599, %v30600 (stack75)
        %v30602 = vxor.u32 %v30597, %v30601 (stack76)
        %v30605 = vadd.s32 %v30597, %v30602 (stack65)
        %v30607 = vshll.u32 %v30602, 15 (stack73)
        %v30608 = vshrl.u32 %v30602, 17 (stack74)
        %v30609 = vor.u32 %v30607, %v30608 (stack75)
        %v30610 = vxor.u32 %v30605, %v30609 (stack76)
        %v30613 = vadd.s32 %v30605, %v30610 (stack65)
        %v30615 = vshll.u32 %v30610, 26 (stack73)
        %v30616 = vshrl.u32 %v30610, 6 (stack74)
        %v30617 = vor.u32 %v30615, %v30616 (stack75)
        %v30618 = vxor.u32 %v30613, %v30617 (stack76)
        %v30621 = vadd.s32 %v30613, %v30618 (stack65)
        %v30625 = vadd.s32 %v30621, %v8 (stack65)
        %v30627 = vshll.u32 %v30618, 6 (stack73)
        %v30628 = vshrl.u32 %v30618, 26 (stack74)
        %v30629 = vor.u32 %v30627, %v30628 (stack75)
        %v30630 = vxor.u32 %v30621, %v30629 (stack76)
        %v30633 = vadd.s32 %v30630, %v10 (stack65)
        %v30637 = vadd.s32 %v30633, 5 (stack65)
        %v30639 = vxor.u32 %v30625, %v30637 (stack76)
        %v30640 = vand.u32.u8 %v30639, 255 (stack77)
        %v30641 = vand.u32 %v30640, 65535 (stack78)
        %v30642 = vshrl.u32 %v30641, 1 (stack79)
        %v30643 = vor.u32 %v30642, 16256 (stack75)
        %v30644 = vand.u32.u16 %v30643, 65535 (stack80)
        %v30645 = vunpack.i.l.bf16 %v30644 (stack81)
        %v30649 = vadd.f32 %v30645, -1.0 (stack82)
        %v30653 = vmul.f32 %v30649, 2.0 (stack83)
        %v30657 = vadd.f32 %v30653, -0.99609375 (stack82)
        %v30661 = vmax.f32 -0.99609375, %v30657 (stack84)
        %v30663 = vand.u32 2147483647, %v30661 (stack85)
        %vm30666 = vcmp.eq.f32.partialorder %v30663, 1.0 (stack86)
        %v30671 = vmul.f32 %v30661, inf (stack83)
        %v30673 = vxor.u32 %v30661, 2147483648 (stack87)
        %v30676 = vmul.f32 %v30661, %v30673 (stack83)
        %v30678 = vadd.f32 %v30676, 1.0 (stack88)
        %v30679 = vlog2.pop %v30678 (stack89)
        %v30680 = vmul.f32 %v30679, 0.6931472 (stack90)
        %v30681 = vmul.f32 -0.5, %v30676 (stack91)
        %v30682 = vadd.f32 %v30681, 1.0 (stack92)
        %v30683 = vmul.f32 %v30682, %v30676 (stack93)
        %v30684 = vand.u32 2147483647, %v30676 (stack94)
        %vm30685 = vcmp.lt.f32.partialorder %v30684, 0.0004427343 (stack95)
        %v30686 = vsel /*vm=*/%vm30685, /*on_true_vy=*/%v30683, /*on_false_vx=*/%v30680 (stack96)
        %v30687 = vxor.u32 %v30686, 2147483648 (stack87)
        %vm30690 = vcmp.lt.f32.partialorder %v30687, 5.0 (stack86)
        %v30695 = vsel /*vm=*/%vm30690, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v30699 = vsel /*vm=*/%vm30690, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v30703 = vsel /*vm=*/%vm30690, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v30707 = vsel /*vm=*/%vm30690, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v30711 = vsel /*vm=*/%vm30690, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v30715 = vsel /*vm=*/%vm30690, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v30719 = vsel /*vm=*/%vm30690, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v30723 = vsel /*vm=*/%vm30690, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v30727 = vsel /*vm=*/%vm30690, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v30731 = vadd.f32 %v30687, -2.5 (stack82)
        %v30733 = vrsqrt.pop %v30687 (stack97)
        %v30734 = vmul.f32 %v30687, %v30733 (stack98)
        %vm30735 = vcmp.eq.f32.partialorder %v30687, inf (stack99)
        %v30736 = vsel /*vm=*/%vm30735, /*on_true_vy=*/%v30687, /*on_false_vx=*/%v30734 (stack100)
        %vm30737 = vcmp.eq.f32.partialorder %v30687, 0.0 (stack101)
        %v30738 = vand.u32 %v30687, 2147483648 (stack102)
        %v30739 = vsel /*vm=*/%vm30737, /*on_true_vy=*/%v30738, /*on_false_vx=*/%v30736 (stack103)
        %v30742 = vadd.f32 %v30739, -3.0 (stack82)
        %v30746 = vsel /*vm=*/%vm30690, /*on_true_vy=*/%v30731, /*on_false_vx=*/%v30742 (stack72)
        %v30750 = vmul.f32 %v30727, %v30746 (stack83)
        %v30754 = vadd.f32 %v30723, %v30750 (stack82)
        %v30758 = vmul.f32 %v30754, %v30746 (stack83)
        %v30762 = vadd.f32 %v30719, %v30758 (stack82)
        %v30766 = vmul.f32 %v30762, %v30746 (stack83)
        %v30770 = vadd.f32 %v30715, %v30766 (stack82)
        %v30774 = vmul.f32 %v30770, %v30746 (stack83)
        %v30778 = vadd.f32 %v30711, %v30774 (stack82)
        %v30782 = vmul.f32 %v30778, %v30746 (stack83)
        %v30786 = vadd.f32 %v30707, %v30782 (stack82)
        %v30790 = vmul.f32 %v30786, %v30746 (stack83)
        %v30794 = vadd.f32 %v30703, %v30790 (stack82)
        %v30798 = vmul.f32 %v30794, %v30746 (stack83)
        %v30802 = vadd.f32 %v30699, %v30798 (stack82)
        %v30806 = vmul.f32 %v30802, %v30746 (stack83)
        %v30810 = vadd.f32 %v30695, %v30806 (stack82)
        %v30814 = vmul.f32 %v30810, %v30661 (stack83)
        %v30818 = vsel /*vm=*/%vm30666, /*on_true_vy=*/%v30671, /*on_false_vx=*/%v30814 (stack72)
        %v30822 = vmul.f32 %v30818, 1.4140625 (stack83)
        %s30824 = scalar_lea.vmem %s280, 32 [#allocation0] (stack107)
        %v30825 = vpack.c.bf16 0.0, %v30822 (stack104)
        %30826 = vst [vmem:[%s30824] sm:$0xf] /*vst_source=*/%v30825 (stack105)
        %v30829 = vadd.s32 %v894, %v30365 (stack65)
        %s30831 = smul.u32 128, %s27 (stack66)
        %v30832 = vlaneseq (stack67)
        %v30833 = vand.u32 %v30832, 127 (stack68)
        %v30834 = vstv %s30831 (stack69)
        %v30835 = vadd.s32 %v30833, %v30834 (stack70)
        %v30839 = vadd.s32 %v30829, %v30835 (stack65)
        %vm30843 = vcmp.lt.u32.totalorder %v30839, %v30829 (stack71)
        %vm30848 = vcmp.lt.u32.totalorder %v30829, %v894 (stack71)
        %v30853 = vadd.s32 %v881, %v30348 (stack65)
        %v30857 = vadd.s32 %v30853, 1 (stack65)
        %v30861 = vsel /*vm=*/%vm30848, /*on_true_vy=*/%v30857, /*on_false_vx=*/%v30853 (stack72)
        %v30865 = vadd.s32 %v30861, 1 (stack65)
        %v30869 = vsel /*vm=*/%vm30843, /*on_true_vy=*/%v30865, /*on_false_vx=*/%v30861 (stack72)
        %v30874 = vadd.s32 %v30869, %v10 (stack65)
        %v30878 = vadd.s32 %v30839, %v9 (stack65)
        %v30882 = vadd.s32 %v30874, %v30878 (stack65)
        %v30884 = vshll.u32 %v30878, 13 (stack73)
        %v30885 = vshrl.u32 %v30878, 19 (stack74)
        %v30886 = vor.u32 %v30884, %v30885 (stack75)
        %v30887 = vxor.u32 %v30882, %v30886 (stack76)
        %v30890 = vadd.s32 %v30882, %v30887 (stack65)
        %v30892 = vshll.u32 %v30887, 15 (stack73)
        %v30893 = vshrl.u32 %v30887, 17 (stack74)
        %v30894 = vor.u32 %v30892, %v30893 (stack75)
        %v30895 = vxor.u32 %v30890, %v30894 (stack76)
        %v30898 = vadd.s32 %v30890, %v30895 (stack65)
        %v30900 = vshll.u32 %v30895, 26 (stack73)
        %v30901 = vshrl.u32 %v30895, 6 (stack74)
        %v30902 = vor.u32 %v30900, %v30901 (stack75)
        %v30903 = vxor.u32 %v30898, %v30902 (stack76)
        %v30906 = vadd.s32 %v30898, %v30903 (stack65)
        %v30910 = vadd.s32 %v30906, %v9 (stack65)
        %v30912 = vshll.u32 %v30903, 6 (stack73)
        %v30913 = vshrl.u32 %v30903, 26 (stack74)
        %v30914 = vor.u32 %v30912, %v30913 (stack75)
        %v30915 = vxor.u32 %v30906, %v30914 (stack76)
        %v30918 = vadd.s32 %v30915, %v8 (stack65)
        %v30922 = vadd.s32 %v30918, 1 (stack65)
        %v30926 = vadd.s32 %v30910, %v30922 (stack65)
        %v30928 = vshll.u32 %v30922, 17 (stack73)
        %v30929 = vshrl.u32 %v30922, 15 (stack74)
        %v30930 = vor.u32 %v30928, %v30929 (stack75)
        %v30931 = vxor.u32 %v30926, %v30930 (stack76)
        %v30934 = vadd.s32 %v30926, %v30931 (stack65)
        %v30936 = vshll.u32 %v30931, 29 (stack73)
        %v30937 = vshrl.u32 %v30931, 3 (stack74)
        %v30938 = vor.u32 %v30936, %v30937 (stack75)
        %v30939 = vxor.u32 %v30934, %v30938 (stack76)
        %v30942 = vadd.s32 %v30934, %v30939 (stack65)
        %v30944 = vshll.u32 %v30939, 16 (stack73)
        %v30945 = vshrl.u32 %v30939, 16 (stack74)
        %v30946 = vor.u32 %v30944, %v30945 (stack75)
        %v30947 = vxor.u32 %v30942, %v30946 (stack76)
        %v30950 = vadd.s32 %v30942, %v30947 (stack65)
        %v30954 = vadd.s32 %v30950, %v8 (stack65)
        %v30956 = vshll.u32 %v30947, 24 (stack73)
        %v30957 = vshrl.u32 %v30947, 8 (stack74)
        %v30958 = vor.u32 %v30956, %v30957 (stack75)
        %v30959 = vxor.u32 %v30950, %v30958 (stack76)
        %v30962 = vadd.s32 %v30959, %v10 (stack65)
        %v30966 = vadd.s32 %v30962, 2 (stack65)
        %v30970 = vadd.s32 %v30954, %v30966 (stack65)
        %v30972 = vshll.u32 %v30966, 13 (stack73)
        %v30973 = vshrl.u32 %v30966, 19 (stack74)
        %v30974 = vor.u32 %v30972, %v30973 (stack75)
        %v30975 = vxor.u32 %v30970, %v30974 (stack76)
        %v30978 = vadd.s32 %v30970, %v30975 (stack65)
        %v30980 = vshll.u32 %v30975, 15 (stack73)
        %v30981 = vshrl.u32 %v30975, 17 (stack74)
        %v30982 = vor.u32 %v30980, %v30981 (stack75)
        %v30983 = vxor.u32 %v30978, %v30982 (stack76)
        %v30986 = vadd.s32 %v30978, %v30983 (stack65)
        %v30988 = vshll.u32 %v30983, 26 (stack73)
        %v30989 = vshrl.u32 %v30983, 6 (stack74)
        %v30990 = vor.u32 %v30988, %v30989 (stack75)
        %v30991 = vxor.u32 %v30986, %v30990 (stack76)
        %v30994 = vadd.s32 %v30986, %v30991 (stack65)
        %v30998 = vadd.s32 %v30994, %v10 (stack65)
        %v31000 = vshll.u32 %v30991, 6 (stack73)
        %v31001 = vshrl.u32 %v30991, 26 (stack74)
        %v31002 = vor.u32 %v31000, %v31001 (stack75)
        %v31003 = vxor.u32 %v30994, %v31002 (stack76)
        %v31006 = vadd.s32 %v31003, %v9 (stack65)
        %v31010 = vadd.s32 %v31006, 3 (stack65)
        %v31014 = vadd.s32 %v30998, %v31010 (stack65)
        %v31016 = vshll.u32 %v31010, 17 (stack73)
        %v31017 = vshrl.u32 %v31010, 15 (stack74)
        %v31018 = vor.u32 %v31016, %v31017 (stack75)
        %v31019 = vxor.u32 %v31014, %v31018 (stack76)
        %v31022 = vadd.s32 %v31014, %v31019 (stack65)
        %v31024 = vshll.u32 %v31019, 29 (stack73)
        %v31025 = vshrl.u32 %v31019, 3 (stack74)
        %v31026 = vor.u32 %v31024, %v31025 (stack75)
        %v31027 = vxor.u32 %v31022, %v31026 (stack76)
        %v31030 = vadd.s32 %v31022, %v31027 (stack65)
        %v31032 = vshll.u32 %v31027, 16 (stack73)
        %v31033 = vshrl.u32 %v31027, 16 (stack74)
        %v31034 = vor.u32 %v31032, %v31033 (stack75)
        %v31035 = vxor.u32 %v31030, %v31034 (stack76)
        %v31038 = vadd.s32 %v31030, %v31035 (stack65)
        %v31042 = vadd.s32 %v31038, %v9 (stack65)
        %v31044 = vshll.u32 %v31035, 24 (stack73)
        %v31045 = vshrl.u32 %v31035, 8 (stack74)
        %v31046 = vor.u32 %v31044, %v31045 (stack75)
        %v31047 = vxor.u32 %v31038, %v31046 (stack76)
        %v31050 = vadd.s32 %v31047, %v8 (stack65)
        %v31054 = vadd.s32 %v31050, 4 (stack65)
        %v31058 = vadd.s32 %v31042, %v31054 (stack65)
        %v31060 = vshll.u32 %v31054, 13 (stack73)
        %v31061 = vshrl.u32 %v31054, 19 (stack74)
        %v31062 = vor.u32 %v31060, %v31061 (stack75)
        %v31063 = vxor.u32 %v31058, %v31062 (stack76)
        %v31066 = vadd.s32 %v31058, %v31063 (stack65)
        %v31068 = vshll.u32 %v31063, 15 (stack73)
        %v31069 = vshrl.u32 %v31063, 17 (stack74)
        %v31070 = vor.u32 %v31068, %v31069 (stack75)
        %v31071 = vxor.u32 %v31066, %v31070 (stack76)
        %v31074 = vadd.s32 %v31066, %v31071 (stack65)
        %v31076 = vshll.u32 %v31071, 26 (stack73)
        %v31077 = vshrl.u32 %v31071, 6 (stack74)
        %v31078 = vor.u32 %v31076, %v31077 (stack75)
        %v31079 = vxor.u32 %v31074, %v31078 (stack76)
        %v31082 = vadd.s32 %v31074, %v31079 (stack65)
        %v31086 = vadd.s32 %v31082, %v8 (stack65)
        %v31088 = vshll.u32 %v31079, 6 (stack73)
        %v31089 = vshrl.u32 %v31079, 26 (stack74)
        %v31090 = vor.u32 %v31088, %v31089 (stack75)
        %v31091 = vxor.u32 %v31082, %v31090 (stack76)
        %v31094 = vadd.s32 %v31091, %v10 (stack65)
        %v31098 = vadd.s32 %v31094, 5 (stack65)
        %v31100 = vxor.u32 %v31086, %v31098 (stack76)
        %v31101 = vand.u32.u8 %v31100, 255 (stack77)
        %v31102 = vand.u32 %v31101, 65535 (stack78)
        %v31103 = vshrl.u32 %v31102, 1 (stack79)
        %v31104 = vor.u32 %v31103, 16256 (stack75)
        %v31105 = vand.u32.u16 %v31104, 65535 (stack80)
        %v31106 = vunpack.i.l.bf16 %v31105 (stack81)
        %v31110 = vadd.f32 %v31106, -1.0 (stack82)
        %v31114 = vmul.f32 %v31110, 2.0 (stack83)
        %v31118 = vadd.f32 %v31114, -0.99609375 (stack82)
        %v31122 = vmax.f32 -0.99609375, %v31118 (stack84)
        %v31124 = vand.u32 2147483647, %v31122 (stack85)
        %vm31127 = vcmp.eq.f32.partialorder %v31124, 1.0 (stack86)
        %v31132 = vmul.f32 %v31122, inf (stack83)
        %v31134 = vxor.u32 %v31122, 2147483648 (stack87)
        %v31137 = vmul.f32 %v31122, %v31134 (stack83)
        %v31139 = vadd.f32 %v31137, 1.0 (stack88)
        %v31140 = vlog2.pop %v31139 (stack89)
        %v31141 = vmul.f32 %v31140, 0.6931472 (stack90)
        %v31142 = vmul.f32 -0.5, %v31137 (stack91)
        %v31143 = vadd.f32 %v31142, 1.0 (stack92)
        %v31144 = vmul.f32 %v31143, %v31137 (stack93)
        %v31145 = vand.u32 2147483647, %v31137 (stack94)
        %vm31146 = vcmp.lt.f32.partialorder %v31145, 0.0004427343 (stack95)
        %v31147 = vsel /*vm=*/%vm31146, /*on_true_vy=*/%v31144, /*on_false_vx=*/%v31141 (stack96)
        %v31148 = vxor.u32 %v31147, 2147483648 (stack87)
        %vm31151 = vcmp.lt.f32.partialorder %v31148, 5.0 (stack86)
        %v31156 = vsel /*vm=*/%vm31151, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v31160 = vsel /*vm=*/%vm31151, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v31164 = vsel /*vm=*/%vm31151, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v31168 = vsel /*vm=*/%vm31151, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v31172 = vsel /*vm=*/%vm31151, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v31176 = vsel /*vm=*/%vm31151, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v31180 = vsel /*vm=*/%vm31151, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v31184 = vsel /*vm=*/%vm31151, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v31188 = vsel /*vm=*/%vm31151, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v31192 = vadd.f32 %v31148, -2.5 (stack82)
        %v31194 = vrsqrt.pop %v31148 (stack97)
        %v31195 = vmul.f32 %v31148, %v31194 (stack98)
        %vm31196 = vcmp.eq.f32.partialorder %v31148, inf (stack99)
        %v31197 = vsel /*vm=*/%vm31196, /*on_true_vy=*/%v31148, /*on_false_vx=*/%v31195 (stack100)
        %vm31198 = vcmp.eq.f32.partialorder %v31148, 0.0 (stack101)
        %v31199 = vand.u32 %v31148, 2147483648 (stack102)
        %v31200 = vsel /*vm=*/%vm31198, /*on_true_vy=*/%v31199, /*on_false_vx=*/%v31197 (stack103)
        %v31203 = vadd.f32 %v31200, -3.0 (stack82)
        %v31207 = vsel /*vm=*/%vm31151, /*on_true_vy=*/%v31192, /*on_false_vx=*/%v31203 (stack72)
        %v31211 = vmul.f32 %v31188, %v31207 (stack83)
        %v31215 = vadd.f32 %v31184, %v31211 (stack82)
        %v31219 = vmul.f32 %v31215, %v31207 (stack83)
        %v31223 = vadd.f32 %v31180, %v31219 (stack82)
        %v31227 = vmul.f32 %v31223, %v31207 (stack83)
        %v31231 = vadd.f32 %v31176, %v31227 (stack82)
        %v31235 = vmul.f32 %v31231, %v31207 (stack83)
        %v31239 = vadd.f32 %v31172, %v31235 (stack82)
        %v31243 = vmul.f32 %v31239, %v31207 (stack83)
        %v31247 = vadd.f32 %v31168, %v31243 (stack82)
        %v31251 = vmul.f32 %v31247, %v31207 (stack83)
        %v31255 = vadd.f32 %v31164, %v31251 (stack82)
        %v31259 = vmul.f32 %v31255, %v31207 (stack83)
        %v31263 = vadd.f32 %v31160, %v31259 (stack82)
        %v31267 = vmul.f32 %v31263, %v31207 (stack83)
        %v31271 = vadd.f32 %v31156, %v31267 (stack82)
        %v31275 = vmul.f32 %v31271, %v31122 (stack83)
        %v31279 = vsel /*vm=*/%vm31127, /*on_true_vy=*/%v31132, /*on_false_vx=*/%v31275 (stack72)
        %v31283 = vmul.f32 %v31279, 1.4140625 (stack83)
        %s31285 = scalar_lea.vmem %s280, 160 [#allocation0] (stack107)
        %v31286 = vpack.c.bf16 0.0, %v31283 (stack104)
        %31287 = vst [vmem:[%s31285] sm:$0xf] /*vst_source=*/%v31286 (stack105)
        %v31290 = vadd.s32 %v1381, %v30365 (stack65)
        %s31292 = smul.u32 128, %s27 (stack66)
        %v31293 = vlaneseq (stack67)
        %v31294 = vand.u32 %v31293, 127 (stack68)
        %v31295 = vstv %s31292 (stack69)
        %v31296 = vadd.s32 %v31294, %v31295 (stack70)
        %v31300 = vadd.s32 %v31290, %v31296 (stack65)
        %vm31304 = vcmp.lt.u32.totalorder %v31300, %v31290 (stack71)
        %vm31309 = vcmp.lt.u32.totalorder %v31290, %v1381 (stack71)
        %v31314 = vadd.s32 %v1368, %v30348 (stack65)
        %v31318 = vadd.s32 %v31314, 1 (stack65)
        %v31322 = vsel /*vm=*/%vm31309, /*on_true_vy=*/%v31318, /*on_false_vx=*/%v31314 (stack72)
        %v31326 = vadd.s32 %v31322, 1 (stack65)
        %v31330 = vsel /*vm=*/%vm31304, /*on_true_vy=*/%v31326, /*on_false_vx=*/%v31322 (stack72)
        %v31335 = vadd.s32 %v31330, %v10 (stack65)
        %v31339 = vadd.s32 %v31300, %v9 (stack65)
        %v31343 = vadd.s32 %v31335, %v31339 (stack65)
        %v31345 = vshll.u32 %v31339, 13 (stack73)
        %v31346 = vshrl.u32 %v31339, 19 (stack74)
        %v31347 = vor.u32 %v31345, %v31346 (stack75)
        %v31348 = vxor.u32 %v31343, %v31347 (stack76)
        %v31351 = vadd.s32 %v31343, %v31348 (stack65)
        %v31353 = vshll.u32 %v31348, 15 (stack73)
        %v31354 = vshrl.u32 %v31348, 17 (stack74)
        %v31355 = vor.u32 %v31353, %v31354 (stack75)
        %v31356 = vxor.u32 %v31351, %v31355 (stack76)
        %v31359 = vadd.s32 %v31351, %v31356 (stack65)
        %v31361 = vshll.u32 %v31356, 26 (stack73)
        %v31362 = vshrl.u32 %v31356, 6 (stack74)
        %v31363 = vor.u32 %v31361, %v31362 (stack75)
        %v31364 = vxor.u32 %v31359, %v31363 (stack76)
        %v31367 = vadd.s32 %v31359, %v31364 (stack65)
        %v31371 = vadd.s32 %v31367, %v9 (stack65)
        %v31373 = vshll.u32 %v31364, 6 (stack73)
        %v31374 = vshrl.u32 %v31364, 26 (stack74)
        %v31375 = vor.u32 %v31373, %v31374 (stack75)
        %v31376 = vxor.u32 %v31367, %v31375 (stack76)
        %v31379 = vadd.s32 %v31376, %v8 (stack65)
        %v31383 = vadd.s32 %v31379, 1 (stack65)
        %v31387 = vadd.s32 %v31371, %v31383 (stack65)
        %v31389 = vshll.u32 %v31383, 17 (stack73)
        %v31390 = vshrl.u32 %v31383, 15 (stack74)
        %v31391 = vor.u32 %v31389, %v31390 (stack75)
        %v31392 = vxor.u32 %v31387, %v31391 (stack76)
        %v31395 = vadd.s32 %v31387, %v31392 (stack65)
        %v31397 = vshll.u32 %v31392, 29 (stack73)
        %v31398 = vshrl.u32 %v31392, 3 (stack74)
        %v31399 = vor.u32 %v31397, %v31398 (stack75)
        %v31400 = vxor.u32 %v31395, %v31399 (stack76)
        %v31403 = vadd.s32 %v31395, %v31400 (stack65)
        %v31405 = vshll.u32 %v31400, 16 (stack73)
        %v31406 = vshrl.u32 %v31400, 16 (stack74)
        %v31407 = vor.u32 %v31405, %v31406 (stack75)
        %v31408 = vxor.u32 %v31403, %v31407 (stack76)
        %v31411 = vadd.s32 %v31403, %v31408 (stack65)
        %v31415 = vadd.s32 %v31411, %v8 (stack65)
        %v31417 = vshll.u32 %v31408, 24 (stack73)
        %v31418 = vshrl.u32 %v31408, 8 (stack74)
        %v31419 = vor.u32 %v31417, %v31418 (stack75)
        %v31420 = vxor.u32 %v31411, %v31419 (stack76)
        %v31423 = vadd.s32 %v31420, %v10 (stack65)
        %v31427 = vadd.s32 %v31423, 2 (stack65)
        %v31431 = vadd.s32 %v31415, %v31427 (stack65)
        %v31433 = vshll.u32 %v31427, 13 (stack73)
        %v31434 = vshrl.u32 %v31427, 19 (stack74)
        %v31435 = vor.u32 %v31433, %v31434 (stack75)
        %v31436 = vxor.u32 %v31431, %v31435 (stack76)
        %v31439 = vadd.s32 %v31431, %v31436 (stack65)
        %v31441 = vshll.u32 %v31436, 15 (stack73)
        %v31442 = vshrl.u32 %v31436, 17 (stack74)
        %v31443 = vor.u32 %v31441, %v31442 (stack75)
        %v31444 = vxor.u32 %v31439, %v31443 (stack76)
        %v31447 = vadd.s32 %v31439, %v31444 (stack65)
        %v31449 = vshll.u32 %v31444, 26 (stack73)
        %v31450 = vshrl.u32 %v31444, 6 (stack74)
        %v31451 = vor.u32 %v31449, %v31450 (stack75)
        %v31452 = vxor.u32 %v31447, %v31451 (stack76)
        %v31455 = vadd.s32 %v31447, %v31452 (stack65)
        %v31459 = vadd.s32 %v31455, %v10 (stack65)
        %v31461 = vshll.u32 %v31452, 6 (stack73)
        %v31462 = vshrl.u32 %v31452, 26 (stack74)
        %v31463 = vor.u32 %v31461, %v31462 (stack75)
        %v31464 = vxor.u32 %v31455, %v31463 (stack76)
        %v31467 = vadd.s32 %v31464, %v9 (stack65)
        %v31471 = vadd.s32 %v31467, 3 (stack65)
        %v31475 = vadd.s32 %v31459, %v31471 (stack65)
        %v31477 = vshll.u32 %v31471, 17 (stack73)
        %v31478 = vshrl.u32 %v31471, 15 (stack74)
        %v31479 = vor.u32 %v31477, %v31478 (stack75)
        %v31480 = vxor.u32 %v31475, %v31479 (stack76)
        %v31483 = vadd.s32 %v31475, %v31480 (stack65)
        %v31485 = vshll.u32 %v31480, 29 (stack73)
        %v31486 = vshrl.u32 %v31480, 3 (stack74)
        %v31487 = vor.u32 %v31485, %v31486 (stack75)
        %v31488 = vxor.u32 %v31483, %v31487 (stack76)
        %v31491 = vadd.s32 %v31483, %v31488 (stack65)
        %v31493 = vshll.u32 %v31488, 16 (stack73)
        %v31494 = vshrl.u32 %v31488, 16 (stack74)
        %v31495 = vor.u32 %v31493, %v31494 (stack75)
        %v31496 = vxor.u32 %v31491, %v31495 (stack76)
        %v31499 = vadd.s32 %v31491, %v31496 (stack65)
        %v31503 = vadd.s32 %v31499, %v9 (stack65)
        %v31505 = vshll.u32 %v31496, 24 (stack73)
        %v31506 = vshrl.u32 %v31496, 8 (stack74)
        %v31507 = vor.u32 %v31505, %v31506 (stack75)
        %v31508 = vxor.u32 %v31499, %v31507 (stack76)
        %v31511 = vadd.s32 %v31508, %v8 (stack65)
        %v31515 = vadd.s32 %v31511, 4 (stack65)
        %v31519 = vadd.s32 %v31503, %v31515 (stack65)
        %v31521 = vshll.u32 %v31515, 13 (stack73)
        %v31522 = vshrl.u32 %v31515, 19 (stack74)
        %v31523 = vor.u32 %v31521, %v31522 (stack75)
        %v31524 = vxor.u32 %v31519, %v31523 (stack76)
        %v31527 = vadd.s32 %v31519, %v31524 (stack65)
        %v31529 = vshll.u32 %v31524, 15 (stack73)
        %v31530 = vshrl.u32 %v31524, 17 (stack74)
        %v31531 = vor.u32 %v31529, %v31530 (stack75)
        %v31532 = vxor.u32 %v31527, %v31531 (stack76)
        %v31535 = vadd.s32 %v31527, %v31532 (stack65)
        %v31537 = vshll.u32 %v31532, 26 (stack73)
        %v31538 = vshrl.u32 %v31532, 6 (stack74)
        %v31539 = vor.u32 %v31537, %v31538 (stack75)
        %v31540 = vxor.u32 %v31535, %v31539 (stack76)
        %v31543 = vadd.s32 %v31535, %v31540 (stack65)
        %v31547 = vadd.s32 %v31543, %v8 (stack65)
        %v31549 = vshll.u32 %v31540, 6 (stack73)
        %v31550 = vshrl.u32 %v31540, 26 (stack74)
        %v31551 = vor.u32 %v31549, %v31550 (stack75)
        %v31552 = vxor.u32 %v31543, %v31551 (stack76)
        %v31555 = vadd.s32 %v31552, %v10 (stack65)
        %v31559 = vadd.s32 %v31555, 5 (stack65)
        %v31561 = vxor.u32 %v31547, %v31559 (stack76)
        %v31562 = vand.u32.u8 %v31561, 255 (stack77)
        %v31563 = vand.u32 %v31562, 65535 (stack78)
        %v31564 = vshrl.u32 %v31563, 1 (stack79)
        %v31565 = vor.u32 %v31564, 16256 (stack75)
        %v31566 = vand.u32.u16 %v31565, 65535 (stack80)
        %v31567 = vunpack.i.l.bf16 %v31566 (stack81)
        %v31571 = vadd.f32 %v31567, -1.0 (stack82)
        %v31575 = vmul.f32 %v31571, 2.0 (stack83)
        %v31579 = vadd.f32 %v31575, -0.99609375 (stack82)
        %v31583 = vmax.f32 -0.99609375, %v31579 (stack84)
        %v31585 = vand.u32 2147483647, %v31583 (stack85)
        %vm31588 = vcmp.eq.f32.partialorder %v31585, 1.0 (stack86)
        %v31593 = vmul.f32 %v31583, inf (stack83)
        %v31595 = vxor.u32 %v31583, 2147483648 (stack87)
        %v31598 = vmul.f32 %v31583, %v31595 (stack83)
        %v31600 = vadd.f32 %v31598, 1.0 (stack88)
        %v31601 = vlog2.pop %v31600 (stack89)
        %v31602 = vmul.f32 %v31601, 0.6931472 (stack90)
        %v31603 = vmul.f32 -0.5, %v31598 (stack91)
        %v31604 = vadd.f32 %v31603, 1.0 (stack92)
        %v31605 = vmul.f32 %v31604, %v31598 (stack93)
        %v31606 = vand.u32 2147483647, %v31598 (stack94)
        %vm31607 = vcmp.lt.f32.partialorder %v31606, 0.0004427343 (stack95)
        %v31608 = vsel /*vm=*/%vm31607, /*on_true_vy=*/%v31605, /*on_false_vx=*/%v31602 (stack96)
        %v31609 = vxor.u32 %v31608, 2147483648 (stack87)
        %vm31612 = vcmp.lt.f32.partialorder %v31609, 5.0 (stack86)
        %v31617 = vsel /*vm=*/%vm31612, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v31621 = vsel /*vm=*/%vm31612, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v31625 = vsel /*vm=*/%vm31612, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v31629 = vsel /*vm=*/%vm31612, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v31633 = vsel /*vm=*/%vm31612, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v31637 = vsel /*vm=*/%vm31612, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v31641 = vsel /*vm=*/%vm31612, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v31645 = vsel /*vm=*/%vm31612, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v31649 = vsel /*vm=*/%vm31612, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v31653 = vadd.f32 %v31609, -2.5 (stack82)
        %v31655 = vrsqrt.pop %v31609 (stack97)
        %v31656 = vmul.f32 %v31609, %v31655 (stack98)
        %vm31657 = vcmp.eq.f32.partialorder %v31609, inf (stack99)
        %v31658 = vsel /*vm=*/%vm31657, /*on_true_vy=*/%v31609, /*on_false_vx=*/%v31656 (stack100)
        %vm31659 = vcmp.eq.f32.partialorder %v31609, 0.0 (stack101)
        %v31660 = vand.u32 %v31609, 2147483648 (stack102)
        %v31661 = vsel /*vm=*/%vm31659, /*on_true_vy=*/%v31660, /*on_false_vx=*/%v31658 (stack103)
        %v31664 = vadd.f32 %v31661, -3.0 (stack82)
        %v31668 = vsel /*vm=*/%vm31612, /*on_true_vy=*/%v31653, /*on_false_vx=*/%v31664 (stack72)
        %v31672 = vmul.f32 %v31649, %v31668 (stack83)
        %v31676 = vadd.f32 %v31645, %v31672 (stack82)
        %v31680 = vmul.f32 %v31676, %v31668 (stack83)
        %v31684 = vadd.f32 %v31641, %v31680 (stack82)
        %v31688 = vmul.f32 %v31684, %v31668 (stack83)
        %v31692 = vadd.f32 %v31637, %v31688 (stack82)
        %v31696 = vmul.f32 %v31692, %v31668 (stack83)
        %v31700 = vadd.f32 %v31633, %v31696 (stack82)
        %v31704 = vmul.f32 %v31700, %v31668 (stack83)
        %v31708 = vadd.f32 %v31629, %v31704 (stack82)
        %v31712 = vmul.f32 %v31708, %v31668 (stack83)
        %v31716 = vadd.f32 %v31625, %v31712 (stack82)
        %v31720 = vmul.f32 %v31716, %v31668 (stack83)
        %v31724 = vadd.f32 %v31621, %v31720 (stack82)
        %v31728 = vmul.f32 %v31724, %v31668 (stack83)
        %v31732 = vadd.f32 %v31617, %v31728 (stack82)
        %v31736 = vmul.f32 %v31732, %v31583 (stack83)
        %v31740 = vsel /*vm=*/%vm31588, /*on_true_vy=*/%v31593, /*on_false_vx=*/%v31736 (stack72)
        %v31744 = vmul.f32 %v31740, 1.4140625 (stack83)
        %s31746 = scalar_lea.vmem %s280, 288 [#allocation0] (stack107)
        %v31747 = vpack.c.bf16 0.0, %v31744 (stack104)
        %31748 = vst [vmem:[%s31746] sm:$0xf] /*vst_source=*/%v31747 (stack105)
        %v31751 = vadd.s32 %v1868, %v30365 (stack65)
        %s31753 = smul.u32 128, %s27 (stack66)
        %v31754 = vlaneseq (stack67)
        %v31755 = vand.u32 %v31754, 127 (stack68)
        %v31756 = vstv %s31753 (stack69)
        %v31757 = vadd.s32 %v31755, %v31756 (stack70)
        %v31761 = vadd.s32 %v31751, %v31757 (stack65)
        %vm31765 = vcmp.lt.u32.totalorder %v31761, %v31751 (stack71)
        %vm31770 = vcmp.lt.u32.totalorder %v31751, %v1868 (stack71)
        %v31775 = vadd.s32 %v1855, %v30348 (stack65)
        %v31779 = vadd.s32 %v31775, 1 (stack65)
        %v31783 = vsel /*vm=*/%vm31770, /*on_true_vy=*/%v31779, /*on_false_vx=*/%v31775 (stack72)
        %v31787 = vadd.s32 %v31783, 1 (stack65)
        %v31791 = vsel /*vm=*/%vm31765, /*on_true_vy=*/%v31787, /*on_false_vx=*/%v31783 (stack72)
        %v31796 = vadd.s32 %v31791, %v10 (stack65)
        %v31800 = vadd.s32 %v31761, %v9 (stack65)
        %v31804 = vadd.s32 %v31796, %v31800 (stack65)
        %v31806 = vshll.u32 %v31800, 13 (stack73)
        %v31807 = vshrl.u32 %v31800, 19 (stack74)
        %v31808 = vor.u32 %v31806, %v31807 (stack75)
        %v31809 = vxor.u32 %v31804, %v31808 (stack76)
        %v31812 = vadd.s32 %v31804, %v31809 (stack65)
        %v31814 = vshll.u32 %v31809, 15 (stack73)
        %v31815 = vshrl.u32 %v31809, 17 (stack74)
        %v31816 = vor.u32 %v31814, %v31815 (stack75)
        %v31817 = vxor.u32 %v31812, %v31816 (stack76)
        %v31820 = vadd.s32 %v31812, %v31817 (stack65)
        %v31822 = vshll.u32 %v31817, 26 (stack73)
        %v31823 = vshrl.u32 %v31817, 6 (stack74)
        %v31824 = vor.u32 %v31822, %v31823 (stack75)
        %v31825 = vxor.u32 %v31820, %v31824 (stack76)
        %v31828 = vadd.s32 %v31820, %v31825 (stack65)
        %v31832 = vadd.s32 %v31828, %v9 (stack65)
        %v31834 = vshll.u32 %v31825, 6 (stack73)
        %v31835 = vshrl.u32 %v31825, 26 (stack74)
        %v31836 = vor.u32 %v31834, %v31835 (stack75)
        %v31837 = vxor.u32 %v31828, %v31836 (stack76)
        %v31840 = vadd.s32 %v31837, %v8 (stack65)
        %v31844 = vadd.s32 %v31840, 1 (stack65)
        %v31848 = vadd.s32 %v31832, %v31844 (stack65)
        %v31850 = vshll.u32 %v31844, 17 (stack73)
        %v31851 = vshrl.u32 %v31844, 15 (stack74)
        %v31852 = vor.u32 %v31850, %v31851 (stack75)
        %v31853 = vxor.u32 %v31848, %v31852 (stack76)
        %v31856 = vadd.s32 %v31848, %v31853 (stack65)
        %v31858 = vshll.u32 %v31853, 29 (stack73)
        %v31859 = vshrl.u32 %v31853, 3 (stack74)
        %v31860 = vor.u32 %v31858, %v31859 (stack75)
        %v31861 = vxor.u32 %v31856, %v31860 (stack76)
        %v31864 = vadd.s32 %v31856, %v31861 (stack65)
        %v31866 = vshll.u32 %v31861, 16 (stack73)
        %v31867 = vshrl.u32 %v31861, 16 (stack74)
        %v31868 = vor.u32 %v31866, %v31867 (stack75)
        %v31869 = vxor.u32 %v31864, %v31868 (stack76)
        %v31872 = vadd.s32 %v31864, %v31869 (stack65)
        %v31876 = vadd.s32 %v31872, %v8 (stack65)
        %v31878 = vshll.u32 %v31869, 24 (stack73)
        %v31879 = vshrl.u32 %v31869, 8 (stack74)
        %v31880 = vor.u32 %v31878, %v31879 (stack75)
        %v31881 = vxor.u32 %v31872, %v31880 (stack76)
        %v31884 = vadd.s32 %v31881, %v10 (stack65)
        %v31888 = vadd.s32 %v31884, 2 (stack65)
        %v31892 = vadd.s32 %v31876, %v31888 (stack65)
        %v31894 = vshll.u32 %v31888, 13 (stack73)
        %v31895 = vshrl.u32 %v31888, 19 (stack74)
        %v31896 = vor.u32 %v31894, %v31895 (stack75)
        %v31897 = vxor.u32 %v31892, %v31896 (stack76)
        %v31900 = vadd.s32 %v31892, %v31897 (stack65)
        %v31902 = vshll.u32 %v31897, 15 (stack73)
        %v31903 = vshrl.u32 %v31897, 17 (stack74)
        %v31904 = vor.u32 %v31902, %v31903 (stack75)
        %v31905 = vxor.u32 %v31900, %v31904 (stack76)
        %v31908 = vadd.s32 %v31900, %v31905 (stack65)
        %v31910 = vshll.u32 %v31905, 26 (stack73)
        %v31911 = vshrl.u32 %v31905, 6 (stack74)
        %v31912 = vor.u32 %v31910, %v31911 (stack75)
        %v31913 = vxor.u32 %v31908, %v31912 (stack76)
        %v31916 = vadd.s32 %v31908, %v31913 (stack65)
        %v31920 = vadd.s32 %v31916, %v10 (stack65)
        %v31922 = vshll.u32 %v31913, 6 (stack73)
        %v31923 = vshrl.u32 %v31913, 26 (stack74)
        %v31924 = vor.u32 %v31922, %v31923 (stack75)
        %v31925 = vxor.u32 %v31916, %v31924 (stack76)
        %v31928 = vadd.s32 %v31925, %v9 (stack65)
        %v31932 = vadd.s32 %v31928, 3 (stack65)
        %v31936 = vadd.s32 %v31920, %v31932 (stack65)
        %v31938 = vshll.u32 %v31932, 17 (stack73)
        %v31939 = vshrl.u32 %v31932, 15 (stack74)
        %v31940 = vor.u32 %v31938, %v31939 (stack75)
        %v31941 = vxor.u32 %v31936, %v31940 (stack76)
        %v31944 = vadd.s32 %v31936, %v31941 (stack65)
        %v31946 = vshll.u32 %v31941, 29 (stack73)
        %v31947 = vshrl.u32 %v31941, 3 (stack74)
        %v31948 = vor.u32 %v31946, %v31947 (stack75)
        %v31949 = vxor.u32 %v31944, %v31948 (stack76)
        %v31952 = vadd.s32 %v31944, %v31949 (stack65)
        %v31954 = vshll.u32 %v31949, 16 (stack73)
        %v31955 = vshrl.u32 %v31949, 16 (stack74)
        %v31956 = vor.u32 %v31954, %v31955 (stack75)
        %v31957 = vxor.u32 %v31952, %v31956 (stack76)
        %v31960 = vadd.s32 %v31952, %v31957 (stack65)
        %v31964 = vadd.s32 %v31960, %v9 (stack65)
        %v31966 = vshll.u32 %v31957, 24 (stack73)
        %v31967 = vshrl.u32 %v31957, 8 (stack74)
        %v31968 = vor.u32 %v31966, %v31967 (stack75)
        %v31969 = vxor.u32 %v31960, %v31968 (stack76)
        %v31972 = vadd.s32 %v31969, %v8 (stack65)
        %v31976 = vadd.s32 %v31972, 4 (stack65)
        %v31980 = vadd.s32 %v31964, %v31976 (stack65)
        %v31982 = vshll.u32 %v31976, 13 (stack73)
        %v31983 = vshrl.u32 %v31976, 19 (stack74)
        %v31984 = vor.u32 %v31982, %v31983 (stack75)
        %v31985 = vxor.u32 %v31980, %v31984 (stack76)
        %v31988 = vadd.s32 %v31980, %v31985 (stack65)
        %v31990 = vshll.u32 %v31985, 15 (stack73)
        %v31991 = vshrl.u32 %v31985, 17 (stack74)
        %v31992 = vor.u32 %v31990, %v31991 (stack75)
        %v31993 = vxor.u32 %v31988, %v31992 (stack76)
        %v31996 = vadd.s32 %v31988, %v31993 (stack65)
        %v31998 = vshll.u32 %v31993, 26 (stack73)
        %v31999 = vshrl.u32 %v31993, 6 (stack74)
        %v32000 = vor.u32 %v31998, %v31999 (stack75)
        %v32001 = vxor.u32 %v31996, %v32000 (stack76)
        %v32004 = vadd.s32 %v31996, %v32001 (stack65)
        %v32008 = vadd.s32 %v32004, %v8 (stack65)
        %v32010 = vshll.u32 %v32001, 6 (stack73)
        %v32011 = vshrl.u32 %v32001, 26 (stack74)
        %v32012 = vor.u32 %v32010, %v32011 (stack75)
        %v32013 = vxor.u32 %v32004, %v32012 (stack76)
        %v32016 = vadd.s32 %v32013, %v10 (stack65)
        %v32020 = vadd.s32 %v32016, 5 (stack65)
        %v32022 = vxor.u32 %v32008, %v32020 (stack76)
        %v32023 = vand.u32.u8 %v32022, 255 (stack77)
        %v32024 = vand.u32 %v32023, 65535 (stack78)
        %v32025 = vshrl.u32 %v32024, 1 (stack79)
        %v32026 = vor.u32 %v32025, 16256 (stack75)
        %v32027 = vand.u32.u16 %v32026, 65535 (stack80)
        %v32028 = vunpack.i.l.bf16 %v32027 (stack81)
        %v32032 = vadd.f32 %v32028, -1.0 (stack82)
        %v32036 = vmul.f32 %v32032, 2.0 (stack83)
        %v32040 = vadd.f32 %v32036, -0.99609375 (stack82)
        %v32044 = vmax.f32 -0.99609375, %v32040 (stack84)
        %v32046 = vand.u32 2147483647, %v32044 (stack85)
        %vm32049 = vcmp.eq.f32.partialorder %v32046, 1.0 (stack86)
        %v32054 = vmul.f32 %v32044, inf (stack83)
        %v32056 = vxor.u32 %v32044, 2147483648 (stack87)
        %v32059 = vmul.f32 %v32044, %v32056 (stack83)
        %v32061 = vadd.f32 %v32059, 1.0 (stack88)
        %v32062 = vlog2.pop %v32061 (stack89)
        %v32063 = vmul.f32 %v32062, 0.6931472 (stack90)
        %v32064 = vmul.f32 -0.5, %v32059 (stack91)
        %v32065 = vadd.f32 %v32064, 1.0 (stack92)
        %v32066 = vmul.f32 %v32065, %v32059 (stack93)
        %v32067 = vand.u32 2147483647, %v32059 (stack94)
        %vm32068 = vcmp.lt.f32.partialorder %v32067, 0.0004427343 (stack95)
        %v32069 = vsel /*vm=*/%vm32068, /*on_true_vy=*/%v32066, /*on_false_vx=*/%v32063 (stack96)
        %v32070 = vxor.u32 %v32069, 2147483648 (stack87)
        %vm32073 = vcmp.lt.f32.partialorder %v32070, 5.0 (stack86)
        %v32078 = vsel /*vm=*/%vm32073, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v32082 = vsel /*vm=*/%vm32073, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v32086 = vsel /*vm=*/%vm32073, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v32090 = vsel /*vm=*/%vm32073, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v32094 = vsel /*vm=*/%vm32073, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v32098 = vsel /*vm=*/%vm32073, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v32102 = vsel /*vm=*/%vm32073, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v32106 = vsel /*vm=*/%vm32073, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v32110 = vsel /*vm=*/%vm32073, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v32114 = vadd.f32 %v32070, -2.5 (stack82)
        %v32116 = vrsqrt.pop %v32070 (stack97)
        %v32117 = vmul.f32 %v32070, %v32116 (stack98)
        %vm32118 = vcmp.eq.f32.partialorder %v32070, inf (stack99)
        %v32119 = vsel /*vm=*/%vm32118, /*on_true_vy=*/%v32070, /*on_false_vx=*/%v32117 (stack100)
        %vm32120 = vcmp.eq.f32.partialorder %v32070, 0.0 (stack101)
        %v32121 = vand.u32 %v32070, 2147483648 (stack102)
        %v32122 = vsel /*vm=*/%vm32120, /*on_true_vy=*/%v32121, /*on_false_vx=*/%v32119 (stack103)
        %v32125 = vadd.f32 %v32122, -3.0 (stack82)
        %v32129 = vsel /*vm=*/%vm32073, /*on_true_vy=*/%v32114, /*on_false_vx=*/%v32125 (stack72)
        %v32133 = vmul.f32 %v32110, %v32129 (stack83)
        %v32137 = vadd.f32 %v32106, %v32133 (stack82)
        %v32141 = vmul.f32 %v32137, %v32129 (stack83)
        %v32145 = vadd.f32 %v32102, %v32141 (stack82)
        %v32149 = vmul.f32 %v32145, %v32129 (stack83)
        %v32153 = vadd.f32 %v32098, %v32149 (stack82)
        %v32157 = vmul.f32 %v32153, %v32129 (stack83)
        %v32161 = vadd.f32 %v32094, %v32157 (stack82)
        %v32165 = vmul.f32 %v32161, %v32129 (stack83)
        %v32169 = vadd.f32 %v32090, %v32165 (stack82)
        %v32173 = vmul.f32 %v32169, %v32129 (stack83)
        %v32177 = vadd.f32 %v32086, %v32173 (stack82)
        %v32181 = vmul.f32 %v32177, %v32129 (stack83)
        %v32185 = vadd.f32 %v32082, %v32181 (stack82)
        %v32189 = vmul.f32 %v32185, %v32129 (stack83)
        %v32193 = vadd.f32 %v32078, %v32189 (stack82)
        %v32197 = vmul.f32 %v32193, %v32044 (stack83)
        %v32201 = vsel /*vm=*/%vm32049, /*on_true_vy=*/%v32054, /*on_false_vx=*/%v32197 (stack72)
        %v32205 = vmul.f32 %v32201, 1.4140625 (stack83)
        %s32207 = scalar_lea.vmem %s280, 416 [#allocation0] (stack107)
        %v32208 = vpack.c.bf16 0.0, %v32205 (stack104)
        %32209 = vst [vmem:[%s32207] sm:$0xf] /*vst_source=*/%v32208 (stack105)
        %v32212 = vadd.s32 %v2355, %v30365 (stack65)
        %s32214 = smul.u32 128, %s27 (stack66)
        %v32215 = vlaneseq (stack67)
        %v32216 = vand.u32 %v32215, 127 (stack68)
        %v32217 = vstv %s32214 (stack69)
        %v32218 = vadd.s32 %v32216, %v32217 (stack70)
        %v32222 = vadd.s32 %v32212, %v32218 (stack65)
        %vm32226 = vcmp.lt.u32.totalorder %v32222, %v32212 (stack71)
        %vm32231 = vcmp.lt.u32.totalorder %v32212, %v2355 (stack71)
        %v32236 = vadd.s32 %v2342, %v30348 (stack65)
        %v32240 = vadd.s32 %v32236, 1 (stack65)
        %v32244 = vsel /*vm=*/%vm32231, /*on_true_vy=*/%v32240, /*on_false_vx=*/%v32236 (stack72)
        %v32248 = vadd.s32 %v32244, 1 (stack65)
        %v32252 = vsel /*vm=*/%vm32226, /*on_true_vy=*/%v32248, /*on_false_vx=*/%v32244 (stack72)
        %v32257 = vadd.s32 %v32252, %v10 (stack65)
        %v32261 = vadd.s32 %v32222, %v9 (stack65)
        %v32265 = vadd.s32 %v32257, %v32261 (stack65)
        %v32267 = vshll.u32 %v32261, 13 (stack73)
        %v32268 = vshrl.u32 %v32261, 19 (stack74)
        %v32269 = vor.u32 %v32267, %v32268 (stack75)
        %v32270 = vxor.u32 %v32265, %v32269 (stack76)
        %v32273 = vadd.s32 %v32265, %v32270 (stack65)
        %v32275 = vshll.u32 %v32270, 15 (stack73)
        %v32276 = vshrl.u32 %v32270, 17 (stack74)
        %v32277 = vor.u32 %v32275, %v32276 (stack75)
        %v32278 = vxor.u32 %v32273, %v32277 (stack76)
        %v32281 = vadd.s32 %v32273, %v32278 (stack65)
        %v32283 = vshll.u32 %v32278, 26 (stack73)
        %v32284 = vshrl.u32 %v32278, 6 (stack74)
        %v32285 = vor.u32 %v32283, %v32284 (stack75)
        %v32286 = vxor.u32 %v32281, %v32285 (stack76)
        %v32289 = vadd.s32 %v32281, %v32286 (stack65)
        %v32293 = vadd.s32 %v32289, %v9 (stack65)
        %v32295 = vshll.u32 %v32286, 6 (stack73)
        %v32296 = vshrl.u32 %v32286, 26 (stack74)
        %v32297 = vor.u32 %v32295, %v32296 (stack75)
        %v32298 = vxor.u32 %v32289, %v32297 (stack76)
        %v32301 = vadd.s32 %v32298, %v8 (stack65)
        %v32305 = vadd.s32 %v32301, 1 (stack65)
        %v32309 = vadd.s32 %v32293, %v32305 (stack65)
        %v32311 = vshll.u32 %v32305, 17 (stack73)
        %v32312 = vshrl.u32 %v32305, 15 (stack74)
        %v32313 = vor.u32 %v32311, %v32312 (stack75)
        %v32314 = vxor.u32 %v32309, %v32313 (stack76)
        %v32317 = vadd.s32 %v32309, %v32314 (stack65)
        %v32319 = vshll.u32 %v32314, 29 (stack73)
        %v32320 = vshrl.u32 %v32314, 3 (stack74)
        %v32321 = vor.u32 %v32319, %v32320 (stack75)
        %v32322 = vxor.u32 %v32317, %v32321 (stack76)
        %v32325 = vadd.s32 %v32317, %v32322 (stack65)
        %v32327 = vshll.u32 %v32322, 16 (stack73)
        %v32328 = vshrl.u32 %v32322, 16 (stack74)
        %v32329 = vor.u32 %v32327, %v32328 (stack75)
        %v32330 = vxor.u32 %v32325, %v32329 (stack76)
        %v32333 = vadd.s32 %v32325, %v32330 (stack65)
        %v32337 = vadd.s32 %v32333, %v8 (stack65)
        %v32339 = vshll.u32 %v32330, 24 (stack73)
        %v32340 = vshrl.u32 %v32330, 8 (stack74)
        %v32341 = vor.u32 %v32339, %v32340 (stack75)
        %v32342 = vxor.u32 %v32333, %v32341 (stack76)
        %v32345 = vadd.s32 %v32342, %v10 (stack65)
        %v32349 = vadd.s32 %v32345, 2 (stack65)
        %v32353 = vadd.s32 %v32337, %v32349 (stack65)
        %v32355 = vshll.u32 %v32349, 13 (stack73)
        %v32356 = vshrl.u32 %v32349, 19 (stack74)
        %v32357 = vor.u32 %v32355, %v32356 (stack75)
        %v32358 = vxor.u32 %v32353, %v32357 (stack76)
        %v32361 = vadd.s32 %v32353, %v32358 (stack65)
        %v32363 = vshll.u32 %v32358, 15 (stack73)
        %v32364 = vshrl.u32 %v32358, 17 (stack74)
        %v32365 = vor.u32 %v32363, %v32364 (stack75)
        %v32366 = vxor.u32 %v32361, %v32365 (stack76)
        %v32369 = vadd.s32 %v32361, %v32366 (stack65)
        %v32371 = vshll.u32 %v32366, 26 (stack73)
        %v32372 = vshrl.u32 %v32366, 6 (stack74)
        %v32373 = vor.u32 %v32371, %v32372 (stack75)
        %v32374 = vxor.u32 %v32369, %v32373 (stack76)
        %v32377 = vadd.s32 %v32369, %v32374 (stack65)
        %v32381 = vadd.s32 %v32377, %v10 (stack65)
        %v32383 = vshll.u32 %v32374, 6 (stack73)
        %v32384 = vshrl.u32 %v32374, 26 (stack74)
        %v32385 = vor.u32 %v32383, %v32384 (stack75)
        %v32386 = vxor.u32 %v32377, %v32385 (stack76)
        %v32389 = vadd.s32 %v32386, %v9 (stack65)
        %v32393 = vadd.s32 %v32389, 3 (stack65)
        %v32397 = vadd.s32 %v32381, %v32393 (stack65)
        %v32399 = vshll.u32 %v32393, 17 (stack73)
        %v32400 = vshrl.u32 %v32393, 15 (stack74)
        %v32401 = vor.u32 %v32399, %v32400 (stack75)
        %v32402 = vxor.u32 %v32397, %v32401 (stack76)
        %v32405 = vadd.s32 %v32397, %v32402 (stack65)
        %v32407 = vshll.u32 %v32402, 29 (stack73)
        %v32408 = vshrl.u32 %v32402, 3 (stack74)
        %v32409 = vor.u32 %v32407, %v32408 (stack75)
        %v32410 = vxor.u32 %v32405, %v32409 (stack76)
        %v32413 = vadd.s32 %v32405, %v32410 (stack65)
        %v32415 = vshll.u32 %v32410, 16 (stack73)
        %v32416 = vshrl.u32 %v32410, 16 (stack74)
        %v32417 = vor.u32 %v32415, %v32416 (stack75)
        %v32418 = vxor.u32 %v32413, %v32417 (stack76)
        %v32421 = vadd.s32 %v32413, %v32418 (stack65)
        %v32425 = vadd.s32 %v32421, %v9 (stack65)
        %v32427 = vshll.u32 %v32418, 24 (stack73)
        %v32428 = vshrl.u32 %v32418, 8 (stack74)
        %v32429 = vor.u32 %v32427, %v32428 (stack75)
        %v32430 = vxor.u32 %v32421, %v32429 (stack76)
        %v32433 = vadd.s32 %v32430, %v8 (stack65)
        %v32437 = vadd.s32 %v32433, 4 (stack65)
        %v32441 = vadd.s32 %v32425, %v32437 (stack65)
        %v32443 = vshll.u32 %v32437, 13 (stack73)
        %v32444 = vshrl.u32 %v32437, 19 (stack74)
        %v32445 = vor.u32 %v32443, %v32444 (stack75)
        %v32446 = vxor.u32 %v32441, %v32445 (stack76)
        %v32449 = vadd.s32 %v32441, %v32446 (stack65)
        %v32451 = vshll.u32 %v32446, 15 (stack73)
        %v32452 = vshrl.u32 %v32446, 17 (stack74)
        %v32453 = vor.u32 %v32451, %v32452 (stack75)
        %v32454 = vxor.u32 %v32449, %v32453 (stack76)
        %v32457 = vadd.s32 %v32449, %v32454 (stack65)
        %v32459 = vshll.u32 %v32454, 26 (stack73)
        %v32460 = vshrl.u32 %v32454, 6 (stack74)
        %v32461 = vor.u32 %v32459, %v32460 (stack75)
        %v32462 = vxor.u32 %v32457, %v32461 (stack76)
        %v32465 = vadd.s32 %v32457, %v32462 (stack65)
        %v32469 = vadd.s32 %v32465, %v8 (stack65)
        %v32471 = vshll.u32 %v32462, 6 (stack73)
        %v32472 = vshrl.u32 %v32462, 26 (stack74)
        %v32473 = vor.u32 %v32471, %v32472 (stack75)
        %v32474 = vxor.u32 %v32465, %v32473 (stack76)
        %v32477 = vadd.s32 %v32474, %v10 (stack65)
        %v32481 = vadd.s32 %v32477, 5 (stack65)
        %v32483 = vxor.u32 %v32469, %v32481 (stack76)
        %v32484 = vand.u32.u8 %v32483, 255 (stack77)
        %v32485 = vand.u32 %v32484, 65535 (stack78)
        %v32486 = vshrl.u32 %v32485, 1 (stack79)
        %v32487 = vor.u32 %v32486, 16256 (stack75)
        %v32488 = vand.u32.u16 %v32487, 65535 (stack80)
        %v32489 = vunpack.i.l.bf16 %v32488 (stack81)
        %v32493 = vadd.f32 %v32489, -1.0 (stack82)
        %v32497 = vmul.f32 %v32493, 2.0 (stack83)
        %v32501 = vadd.f32 %v32497, -0.99609375 (stack82)
        %v32505 = vmax.f32 -0.99609375, %v32501 (stack84)
        %v32507 = vand.u32 2147483647, %v32505 (stack85)
        %vm32510 = vcmp.eq.f32.partialorder %v32507, 1.0 (stack86)
        %v32515 = vmul.f32 %v32505, inf (stack83)
        %v32517 = vxor.u32 %v32505, 2147483648 (stack87)
        %v32520 = vmul.f32 %v32505, %v32517 (stack83)
        %v32522 = vadd.f32 %v32520, 1.0 (stack88)
        %v32523 = vlog2.pop %v32522 (stack89)
        %v32524 = vmul.f32 %v32523, 0.6931472 (stack90)
        %v32525 = vmul.f32 -0.5, %v32520 (stack91)
        %v32526 = vadd.f32 %v32525, 1.0 (stack92)
        %v32527 = vmul.f32 %v32526, %v32520 (stack93)
        %v32528 = vand.u32 2147483647, %v32520 (stack94)
        %vm32529 = vcmp.lt.f32.partialorder %v32528, 0.0004427343 (stack95)
        %v32530 = vsel /*vm=*/%vm32529, /*on_true_vy=*/%v32527, /*on_false_vx=*/%v32524 (stack96)
        %v32531 = vxor.u32 %v32530, 2147483648 (stack87)
        %vm32534 = vcmp.lt.f32.partialorder %v32531, 5.0 (stack86)
        %v32539 = vsel /*vm=*/%vm32534, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v32543 = vsel /*vm=*/%vm32534, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v32547 = vsel /*vm=*/%vm32534, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v32551 = vsel /*vm=*/%vm32534, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v32555 = vsel /*vm=*/%vm32534, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v32559 = vsel /*vm=*/%vm32534, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v32563 = vsel /*vm=*/%vm32534, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v32567 = vsel /*vm=*/%vm32534, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v32571 = vsel /*vm=*/%vm32534, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v32575 = vadd.f32 %v32531, -2.5 (stack82)
        %v32577 = vrsqrt.pop %v32531 (stack97)
        %v32578 = vmul.f32 %v32531, %v32577 (stack98)
        %vm32579 = vcmp.eq.f32.partialorder %v32531, inf (stack99)
        %v32580 = vsel /*vm=*/%vm32579, /*on_true_vy=*/%v32531, /*on_false_vx=*/%v32578 (stack100)
        %vm32581 = vcmp.eq.f32.partialorder %v32531, 0.0 (stack101)
        %v32582 = vand.u32 %v32531, 2147483648 (stack102)
        %v32583 = vsel /*vm=*/%vm32581, /*on_true_vy=*/%v32582, /*on_false_vx=*/%v32580 (stack103)
        %v32586 = vadd.f32 %v32583, -3.0 (stack82)
        %v32590 = vsel /*vm=*/%vm32534, /*on_true_vy=*/%v32575, /*on_false_vx=*/%v32586 (stack72)
        %v32594 = vmul.f32 %v32571, %v32590 (stack83)
        %v32598 = vadd.f32 %v32567, %v32594 (stack82)
        %v32602 = vmul.f32 %v32598, %v32590 (stack83)
        %v32606 = vadd.f32 %v32563, %v32602 (stack82)
        %v32610 = vmul.f32 %v32606, %v32590 (stack83)
        %v32614 = vadd.f32 %v32559, %v32610 (stack82)
        %v32618 = vmul.f32 %v32614, %v32590 (stack83)
        %v32622 = vadd.f32 %v32555, %v32618 (stack82)
        %v32626 = vmul.f32 %v32622, %v32590 (stack83)
        %v32630 = vadd.f32 %v32551, %v32626 (stack82)
        %v32634 = vmul.f32 %v32630, %v32590 (stack83)
        %v32638 = vadd.f32 %v32547, %v32634 (stack82)
        %v32642 = vmul.f32 %v32638, %v32590 (stack83)
        %v32646 = vadd.f32 %v32543, %v32642 (stack82)
        %v32650 = vmul.f32 %v32646, %v32590 (stack83)
        %v32654 = vadd.f32 %v32539, %v32650 (stack82)
        %v32658 = vmul.f32 %v32654, %v32505 (stack83)
        %v32662 = vsel /*vm=*/%vm32510, /*on_true_vy=*/%v32515, /*on_false_vx=*/%v32658 (stack72)
        %v32666 = vmul.f32 %v32662, 1.4140625 (stack83)
        %s32668 = scalar_lea.vmem %s280, 544 [#allocation0] (stack107)
        %v32669 = vpack.c.bf16 0.0, %v32666 (stack104)
        %32670 = vst [vmem:[%s32668] sm:$0xf] /*vst_source=*/%v32669 (stack105)
        %v32673 = vadd.s32 %v2842, %v30365 (stack65)
        %s32675 = smul.u32 128, %s27 (stack66)
        %v32676 = vlaneseq (stack67)
        %v32677 = vand.u32 %v32676, 127 (stack68)
        %v32678 = vstv %s32675 (stack69)
        %v32679 = vadd.s32 %v32677, %v32678 (stack70)
        %v32683 = vadd.s32 %v32673, %v32679 (stack65)
        %vm32687 = vcmp.lt.u32.totalorder %v32683, %v32673 (stack71)
        %vm32692 = vcmp.lt.u32.totalorder %v32673, %v2842 (stack71)
        %v32697 = vadd.s32 %v2829, %v30348 (stack65)
        %v32701 = vadd.s32 %v32697, 1 (stack65)
        %v32705 = vsel /*vm=*/%vm32692, /*on_true_vy=*/%v32701, /*on_false_vx=*/%v32697 (stack72)
        %v32709 = vadd.s32 %v32705, 1 (stack65)
        %v32713 = vsel /*vm=*/%vm32687, /*on_true_vy=*/%v32709, /*on_false_vx=*/%v32705 (stack72)
        %v32718 = vadd.s32 %v32713, %v10 (stack65)
        %v32722 = vadd.s32 %v32683, %v9 (stack65)
        %v32726 = vadd.s32 %v32718, %v32722 (stack65)
        %v32728 = vshll.u32 %v32722, 13 (stack73)
        %v32729 = vshrl.u32 %v32722, 19 (stack74)
        %v32730 = vor.u32 %v32728, %v32729 (stack75)
        %v32731 = vxor.u32 %v32726, %v32730 (stack76)
        %v32734 = vadd.s32 %v32726, %v32731 (stack65)
        %v32736 = vshll.u32 %v32731, 15 (stack73)
        %v32737 = vshrl.u32 %v32731, 17 (stack74)
        %v32738 = vor.u32 %v32736, %v32737 (stack75)
        %v32739 = vxor.u32 %v32734, %v32738 (stack76)
        %v32742 = vadd.s32 %v32734, %v32739 (stack65)
        %v32744 = vshll.u32 %v32739, 26 (stack73)
        %v32745 = vshrl.u32 %v32739, 6 (stack74)
        %v32746 = vor.u32 %v32744, %v32745 (stack75)
        %v32747 = vxor.u32 %v32742, %v32746 (stack76)
        %v32750 = vadd.s32 %v32742, %v32747 (stack65)
        %v32754 = vadd.s32 %v32750, %v9 (stack65)
        %v32756 = vshll.u32 %v32747, 6 (stack73)
        %v32757 = vshrl.u32 %v32747, 26 (stack74)
        %v32758 = vor.u32 %v32756, %v32757 (stack75)
        %v32759 = vxor.u32 %v32750, %v32758 (stack76)
        %v32762 = vadd.s32 %v32759, %v8 (stack65)
        %v32766 = vadd.s32 %v32762, 1 (stack65)
        %v32770 = vadd.s32 %v32754, %v32766 (stack65)
        %v32772 = vshll.u32 %v32766, 17 (stack73)
        %v32773 = vshrl.u32 %v32766, 15 (stack74)
        %v32774 = vor.u32 %v32772, %v32773 (stack75)
        %v32775 = vxor.u32 %v32770, %v32774 (stack76)
        %v32778 = vadd.s32 %v32770, %v32775 (stack65)
        %v32780 = vshll.u32 %v32775, 29 (stack73)
        %v32781 = vshrl.u32 %v32775, 3 (stack74)
        %v32782 = vor.u32 %v32780, %v32781 (stack75)
        %v32783 = vxor.u32 %v32778, %v32782 (stack76)
        %v32786 = vadd.s32 %v32778, %v32783 (stack65)
        %v32788 = vshll.u32 %v32783, 16 (stack73)
        %v32789 = vshrl.u32 %v32783, 16 (stack74)
        %v32790 = vor.u32 %v32788, %v32789 (stack75)
        %v32791 = vxor.u32 %v32786, %v32790 (stack76)
        %v32794 = vadd.s32 %v32786, %v32791 (stack65)
        %v32798 = vadd.s32 %v32794, %v8 (stack65)
        %v32800 = vshll.u32 %v32791, 24 (stack73)
        %v32801 = vshrl.u32 %v32791, 8 (stack74)
        %v32802 = vor.u32 %v32800, %v32801 (stack75)
        %v32803 = vxor.u32 %v32794, %v32802 (stack76)
        %v32806 = vadd.s32 %v32803, %v10 (stack65)
        %v32810 = vadd.s32 %v32806, 2 (stack65)
        %v32814 = vadd.s32 %v32798, %v32810 (stack65)
        %v32816 = vshll.u32 %v32810, 13 (stack73)
        %v32817 = vshrl.u32 %v32810, 19 (stack74)
        %v32818 = vor.u32 %v32816, %v32817 (stack75)
        %v32819 = vxor.u32 %v32814, %v32818 (stack76)
        %v32822 = vadd.s32 %v32814, %v32819 (stack65)
        %v32824 = vshll.u32 %v32819, 15 (stack73)
        %v32825 = vshrl.u32 %v32819, 17 (stack74)
        %v32826 = vor.u32 %v32824, %v32825 (stack75)
        %v32827 = vxor.u32 %v32822, %v32826 (stack76)
        %v32830 = vadd.s32 %v32822, %v32827 (stack65)
        %v32832 = vshll.u32 %v32827, 26 (stack73)
        %v32833 = vshrl.u32 %v32827, 6 (stack74)
        %v32834 = vor.u32 %v32832, %v32833 (stack75)
        %v32835 = vxor.u32 %v32830, %v32834 (stack76)
        %v32838 = vadd.s32 %v32830, %v32835 (stack65)
        %v32842 = vadd.s32 %v32838, %v10 (stack65)
        %v32844 = vshll.u32 %v32835, 6 (stack73)
        %v32845 = vshrl.u32 %v32835, 26 (stack74)
        %v32846 = vor.u32 %v32844, %v32845 (stack75)
        %v32847 = vxor.u32 %v32838, %v32846 (stack76)
        %v32850 = vadd.s32 %v32847, %v9 (stack65)
        %v32854 = vadd.s32 %v32850, 3 (stack65)
        %v32858 = vadd.s32 %v32842, %v32854 (stack65)
        %v32860 = vshll.u32 %v32854, 17 (stack73)
        %v32861 = vshrl.u32 %v32854, 15 (stack74)
        %v32862 = vor.u32 %v32860, %v32861 (stack75)
        %v32863 = vxor.u32 %v32858, %v32862 (stack76)
        %v32866 = vadd.s32 %v32858, %v32863 (stack65)
        %v32868 = vshll.u32 %v32863, 29 (stack73)
        %v32869 = vshrl.u32 %v32863, 3 (stack74)
        %v32870 = vor.u32 %v32868, %v32869 (stack75)
        %v32871 = vxor.u32 %v32866, %v32870 (stack76)
        %v32874 = vadd.s32 %v32866, %v32871 (stack65)
        %v32876 = vshll.u32 %v32871, 16 (stack73)
        %v32877 = vshrl.u32 %v32871, 16 (stack74)
        %v32878 = vor.u32 %v32876, %v32877 (stack75)
        %v32879 = vxor.u32 %v32874, %v32878 (stack76)
        %v32882 = vadd.s32 %v32874, %v32879 (stack65)
        %v32886 = vadd.s32 %v32882, %v9 (stack65)
        %v32888 = vshll.u32 %v32879, 24 (stack73)
        %v32889 = vshrl.u32 %v32879, 8 (stack74)
        %v32890 = vor.u32 %v32888, %v32889 (stack75)
        %v32891 = vxor.u32 %v32882, %v32890 (stack76)
        %v32894 = vadd.s32 %v32891, %v8 (stack65)
        %v32898 = vadd.s32 %v32894, 4 (stack65)
        %v32902 = vadd.s32 %v32886, %v32898 (stack65)
        %v32904 = vshll.u32 %v32898, 13 (stack73)
        %v32905 = vshrl.u32 %v32898, 19 (stack74)
        %v32906 = vor.u32 %v32904, %v32905 (stack75)
        %v32907 = vxor.u32 %v32902, %v32906 (stack76)
        %v32910 = vadd.s32 %v32902, %v32907 (stack65)
        %v32912 = vshll.u32 %v32907, 15 (stack73)
        %v32913 = vshrl.u32 %v32907, 17 (stack74)
        %v32914 = vor.u32 %v32912, %v32913 (stack75)
        %v32915 = vxor.u32 %v32910, %v32914 (stack76)
        %v32918 = vadd.s32 %v32910, %v32915 (stack65)
        %v32920 = vshll.u32 %v32915, 26 (stack73)
        %v32921 = vshrl.u32 %v32915, 6 (stack74)
        %v32922 = vor.u32 %v32920, %v32921 (stack75)
        %v32923 = vxor.u32 %v32918, %v32922 (stack76)
        %v32926 = vadd.s32 %v32918, %v32923 (stack65)
        %v32930 = vadd.s32 %v32926, %v8 (stack65)
        %v32932 = vshll.u32 %v32923, 6 (stack73)
        %v32933 = vshrl.u32 %v32923, 26 (stack74)
        %v32934 = vor.u32 %v32932, %v32933 (stack75)
        %v32935 = vxor.u32 %v32926, %v32934 (stack76)
        %v32938 = vadd.s32 %v32935, %v10 (stack65)
        %v32942 = vadd.s32 %v32938, 5 (stack65)
        %v32944 = vxor.u32 %v32930, %v32942 (stack76)
        %v32945 = vand.u32.u8 %v32944, 255 (stack77)
        %v32946 = vand.u32 %v32945, 65535 (stack78)
        %v32947 = vshrl.u32 %v32946, 1 (stack79)
        %v32948 = vor.u32 %v32947, 16256 (stack75)
        %v32949 = vand.u32.u16 %v32948, 65535 (stack80)
        %v32950 = vunpack.i.l.bf16 %v32949 (stack81)
        %v32954 = vadd.f32 %v32950, -1.0 (stack82)
        %v32958 = vmul.f32 %v32954, 2.0 (stack83)
        %v32962 = vadd.f32 %v32958, -0.99609375 (stack82)
        %v32966 = vmax.f32 -0.99609375, %v32962 (stack84)
        %v32968 = vand.u32 2147483647, %v32966 (stack85)
        %vm32971 = vcmp.eq.f32.partialorder %v32968, 1.0 (stack86)
        %v32976 = vmul.f32 %v32966, inf (stack83)
        %v32978 = vxor.u32 %v32966, 2147483648 (stack87)
        %v32981 = vmul.f32 %v32966, %v32978 (stack83)
        %v32983 = vadd.f32 %v32981, 1.0 (stack88)
        %v32984 = vlog2.pop %v32983 (stack89)
        %v32985 = vmul.f32 %v32984, 0.6931472 (stack90)
        %v32986 = vmul.f32 -0.5, %v32981 (stack91)
        %v32987 = vadd.f32 %v32986, 1.0 (stack92)
        %v32988 = vmul.f32 %v32987, %v32981 (stack93)
        %v32989 = vand.u32 2147483647, %v32981 (stack94)
        %vm32990 = vcmp.lt.f32.partialorder %v32989, 0.0004427343 (stack95)
        %v32991 = vsel /*vm=*/%vm32990, /*on_true_vy=*/%v32988, /*on_false_vx=*/%v32985 (stack96)
        %v32992 = vxor.u32 %v32991, 2147483648 (stack87)
        %vm32995 = vcmp.lt.f32.partialorder %v32992, 5.0 (stack86)
        %v33000 = vsel /*vm=*/%vm32995, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v33004 = vsel /*vm=*/%vm32995, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v33008 = vsel /*vm=*/%vm32995, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v33012 = vsel /*vm=*/%vm32995, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v33016 = vsel /*vm=*/%vm32995, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v33020 = vsel /*vm=*/%vm32995, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v33024 = vsel /*vm=*/%vm32995, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v33028 = vsel /*vm=*/%vm32995, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v33032 = vsel /*vm=*/%vm32995, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v33036 = vadd.f32 %v32992, -2.5 (stack82)
        %v33038 = vrsqrt.pop %v32992 (stack97)
        %v33039 = vmul.f32 %v32992, %v33038 (stack98)
        %vm33040 = vcmp.eq.f32.partialorder %v32992, inf (stack99)
        %v33041 = vsel /*vm=*/%vm33040, /*on_true_vy=*/%v32992, /*on_false_vx=*/%v33039 (stack100)
        %vm33042 = vcmp.eq.f32.partialorder %v32992, 0.0 (stack101)
        %v33043 = vand.u32 %v32992, 2147483648 (stack102)
        %v33044 = vsel /*vm=*/%vm33042, /*on_true_vy=*/%v33043, /*on_false_vx=*/%v33041 (stack103)
        %v33047 = vadd.f32 %v33044, -3.0 (stack82)
        %v33051 = vsel /*vm=*/%vm32995, /*on_true_vy=*/%v33036, /*on_false_vx=*/%v33047 (stack72)
        %v33055 = vmul.f32 %v33032, %v33051 (stack83)
        %v33059 = vadd.f32 %v33028, %v33055 (stack82)
        %v33063 = vmul.f32 %v33059, %v33051 (stack83)
        %v33067 = vadd.f32 %v33024, %v33063 (stack82)
        %v33071 = vmul.f32 %v33067, %v33051 (stack83)
        %v33075 = vadd.f32 %v33020, %v33071 (stack82)
        %v33079 = vmul.f32 %v33075, %v33051 (stack83)
        %v33083 = vadd.f32 %v33016, %v33079 (stack82)
        %v33087 = vmul.f32 %v33083, %v33051 (stack83)
        %v33091 = vadd.f32 %v33012, %v33087 (stack82)
        %v33095 = vmul.f32 %v33091, %v33051 (stack83)
        %v33099 = vadd.f32 %v33008, %v33095 (stack82)
        %v33103 = vmul.f32 %v33099, %v33051 (stack83)
        %v33107 = vadd.f32 %v33004, %v33103 (stack82)
        %v33111 = vmul.f32 %v33107, %v33051 (stack83)
        %v33115 = vadd.f32 %v33000, %v33111 (stack82)
        %v33119 = vmul.f32 %v33115, %v32966 (stack83)
        %v33123 = vsel /*vm=*/%vm32971, /*on_true_vy=*/%v32976, /*on_false_vx=*/%v33119 (stack72)
        %v33127 = vmul.f32 %v33123, 1.4140625 (stack83)
        %s33129 = scalar_lea.vmem %s280, 672 [#allocation0] (stack107)
        %v33130 = vpack.c.bf16 0.0, %v33127 (stack104)
        %33131 = vst [vmem:[%s33129] sm:$0xf] /*vst_source=*/%v33130 (stack105)
        %v33134 = vadd.s32 %v3329, %v30365 (stack65)
        %s33136 = smul.u32 128, %s27 (stack66)
        %v33137 = vlaneseq (stack67)
        %v33138 = vand.u32 %v33137, 127 (stack68)
        %v33139 = vstv %s33136 (stack69)
        %v33140 = vadd.s32 %v33138, %v33139 (stack70)
        %v33144 = vadd.s32 %v33134, %v33140 (stack65)
        %vm33148 = vcmp.lt.u32.totalorder %v33144, %v33134 (stack71)
        %vm33153 = vcmp.lt.u32.totalorder %v33134, %v3329 (stack71)
        %v33158 = vadd.s32 %v3316, %v30348 (stack65)
        %v33162 = vadd.s32 %v33158, 1 (stack65)
        %v33166 = vsel /*vm=*/%vm33153, /*on_true_vy=*/%v33162, /*on_false_vx=*/%v33158 (stack72)
        %v33170 = vadd.s32 %v33166, 1 (stack65)
        %v33174 = vsel /*vm=*/%vm33148, /*on_true_vy=*/%v33170, /*on_false_vx=*/%v33166 (stack72)
        %v33179 = vadd.s32 %v33174, %v10 (stack65)
        %v33183 = vadd.s32 %v33144, %v9 (stack65)
        %v33187 = vadd.s32 %v33179, %v33183 (stack65)
        %v33189 = vshll.u32 %v33183, 13 (stack73)
        %v33190 = vshrl.u32 %v33183, 19 (stack74)
        %v33191 = vor.u32 %v33189, %v33190 (stack75)
        %v33192 = vxor.u32 %v33187, %v33191 (stack76)
        %v33195 = vadd.s32 %v33187, %v33192 (stack65)
        %v33197 = vshll.u32 %v33192, 15 (stack73)
        %v33198 = vshrl.u32 %v33192, 17 (stack74)
        %v33199 = vor.u32 %v33197, %v33198 (stack75)
        %v33200 = vxor.u32 %v33195, %v33199 (stack76)
        %v33203 = vadd.s32 %v33195, %v33200 (stack65)
        %v33205 = vshll.u32 %v33200, 26 (stack73)
        %v33206 = vshrl.u32 %v33200, 6 (stack74)
        %v33207 = vor.u32 %v33205, %v33206 (stack75)
        %v33208 = vxor.u32 %v33203, %v33207 (stack76)
        %v33211 = vadd.s32 %v33203, %v33208 (stack65)
        %v33215 = vadd.s32 %v33211, %v9 (stack65)
        %v33217 = vshll.u32 %v33208, 6 (stack73)
        %v33218 = vshrl.u32 %v33208, 26 (stack74)
        %v33219 = vor.u32 %v33217, %v33218 (stack75)
        %v33220 = vxor.u32 %v33211, %v33219 (stack76)
        %v33223 = vadd.s32 %v33220, %v8 (stack65)
        %v33227 = vadd.s32 %v33223, 1 (stack65)
        %v33231 = vadd.s32 %v33215, %v33227 (stack65)
        %v33233 = vshll.u32 %v33227, 17 (stack73)
        %v33234 = vshrl.u32 %v33227, 15 (stack74)
        %v33235 = vor.u32 %v33233, %v33234 (stack75)
        %v33236 = vxor.u32 %v33231, %v33235 (stack76)
        %v33239 = vadd.s32 %v33231, %v33236 (stack65)
        %v33241 = vshll.u32 %v33236, 29 (stack73)
        %v33242 = vshrl.u32 %v33236, 3 (stack74)
        %v33243 = vor.u32 %v33241, %v33242 (stack75)
        %v33244 = vxor.u32 %v33239, %v33243 (stack76)
        %v33247 = vadd.s32 %v33239, %v33244 (stack65)
        %v33249 = vshll.u32 %v33244, 16 (stack73)
        %v33250 = vshrl.u32 %v33244, 16 (stack74)
        %v33251 = vor.u32 %v33249, %v33250 (stack75)
        %v33252 = vxor.u32 %v33247, %v33251 (stack76)
        %v33255 = vadd.s32 %v33247, %v33252 (stack65)
        %v33259 = vadd.s32 %v33255, %v8 (stack65)
        %v33261 = vshll.u32 %v33252, 24 (stack73)
        %v33262 = vshrl.u32 %v33252, 8 (stack74)
        %v33263 = vor.u32 %v33261, %v33262 (stack75)
        %v33264 = vxor.u32 %v33255, %v33263 (stack76)
        %v33267 = vadd.s32 %v33264, %v10 (stack65)
        %v33271 = vadd.s32 %v33267, 2 (stack65)
        %v33275 = vadd.s32 %v33259, %v33271 (stack65)
        %v33277 = vshll.u32 %v33271, 13 (stack73)
        %v33278 = vshrl.u32 %v33271, 19 (stack74)
        %v33279 = vor.u32 %v33277, %v33278 (stack75)
        %v33280 = vxor.u32 %v33275, %v33279 (stack76)
        %v33283 = vadd.s32 %v33275, %v33280 (stack65)
        %v33285 = vshll.u32 %v33280, 15 (stack73)
        %v33286 = vshrl.u32 %v33280, 17 (stack74)
        %v33287 = vor.u32 %v33285, %v33286 (stack75)
        %v33288 = vxor.u32 %v33283, %v33287 (stack76)
        %v33291 = vadd.s32 %v33283, %v33288 (stack65)
        %v33293 = vshll.u32 %v33288, 26 (stack73)
        %v33294 = vshrl.u32 %v33288, 6 (stack74)
        %v33295 = vor.u32 %v33293, %v33294 (stack75)
        %v33296 = vxor.u32 %v33291, %v33295 (stack76)
        %v33299 = vadd.s32 %v33291, %v33296 (stack65)
        %v33303 = vadd.s32 %v33299, %v10 (stack65)
        %v33305 = vshll.u32 %v33296, 6 (stack73)
        %v33306 = vshrl.u32 %v33296, 26 (stack74)
        %v33307 = vor.u32 %v33305, %v33306 (stack75)
        %v33308 = vxor.u32 %v33299, %v33307 (stack76)
        %v33311 = vadd.s32 %v33308, %v9 (stack65)
        %v33315 = vadd.s32 %v33311, 3 (stack65)
        %v33319 = vadd.s32 %v33303, %v33315 (stack65)
        %v33321 = vshll.u32 %v33315, 17 (stack73)
        %v33322 = vshrl.u32 %v33315, 15 (stack74)
        %v33323 = vor.u32 %v33321, %v33322 (stack75)
        %v33324 = vxor.u32 %v33319, %v33323 (stack76)
        %v33327 = vadd.s32 %v33319, %v33324 (stack65)
        %v33329 = vshll.u32 %v33324, 29 (stack73)
        %v33330 = vshrl.u32 %v33324, 3 (stack74)
        %v33331 = vor.u32 %v33329, %v33330 (stack75)
        %v33332 = vxor.u32 %v33327, %v33331 (stack76)
        %v33335 = vadd.s32 %v33327, %v33332 (stack65)
        %v33337 = vshll.u32 %v33332, 16 (stack73)
        %v33338 = vshrl.u32 %v33332, 16 (stack74)
        %v33339 = vor.u32 %v33337, %v33338 (stack75)
        %v33340 = vxor.u32 %v33335, %v33339 (stack76)
        %v33343 = vadd.s32 %v33335, %v33340 (stack65)
        %v33347 = vadd.s32 %v33343, %v9 (stack65)
        %v33349 = vshll.u32 %v33340, 24 (stack73)
        %v33350 = vshrl.u32 %v33340, 8 (stack74)
        %v33351 = vor.u32 %v33349, %v33350 (stack75)
        %v33352 = vxor.u32 %v33343, %v33351 (stack76)
        %v33355 = vadd.s32 %v33352, %v8 (stack65)
        %v33359 = vadd.s32 %v33355, 4 (stack65)
        %v33363 = vadd.s32 %v33347, %v33359 (stack65)
        %v33365 = vshll.u32 %v33359, 13 (stack73)
        %v33366 = vshrl.u32 %v33359, 19 (stack74)
        %v33367 = vor.u32 %v33365, %v33366 (stack75)
        %v33368 = vxor.u32 %v33363, %v33367 (stack76)
        %v33371 = vadd.s32 %v33363, %v33368 (stack65)
        %v33373 = vshll.u32 %v33368, 15 (stack73)
        %v33374 = vshrl.u32 %v33368, 17 (stack74)
        %v33375 = vor.u32 %v33373, %v33374 (stack75)
        %v33376 = vxor.u32 %v33371, %v33375 (stack76)
        %v33379 = vadd.s32 %v33371, %v33376 (stack65)
        %v33381 = vshll.u32 %v33376, 26 (stack73)
        %v33382 = vshrl.u32 %v33376, 6 (stack74)
        %v33383 = vor.u32 %v33381, %v33382 (stack75)
        %v33384 = vxor.u32 %v33379, %v33383 (stack76)
        %v33387 = vadd.s32 %v33379, %v33384 (stack65)
        %v33391 = vadd.s32 %v33387, %v8 (stack65)
        %v33393 = vshll.u32 %v33384, 6 (stack73)
        %v33394 = vshrl.u32 %v33384, 26 (stack74)
        %v33395 = vor.u32 %v33393, %v33394 (stack75)
        %v33396 = vxor.u32 %v33387, %v33395 (stack76)
        %v33399 = vadd.s32 %v33396, %v10 (stack65)
        %v33403 = vadd.s32 %v33399, 5 (stack65)
        %v33405 = vxor.u32 %v33391, %v33403 (stack76)
        %v33406 = vand.u32.u8 %v33405, 255 (stack77)
        %v33407 = vand.u32 %v33406, 65535 (stack78)
        %v33408 = vshrl.u32 %v33407, 1 (stack79)
        %v33409 = vor.u32 %v33408, 16256 (stack75)
        %v33410 = vand.u32.u16 %v33409, 65535 (stack80)
        %v33411 = vunpack.i.l.bf16 %v33410 (stack81)
        %v33415 = vadd.f32 %v33411, -1.0 (stack82)
        %v33419 = vmul.f32 %v33415, 2.0 (stack83)
        %v33423 = vadd.f32 %v33419, -0.99609375 (stack82)
        %v33427 = vmax.f32 -0.99609375, %v33423 (stack84)
        %v33429 = vand.u32 2147483647, %v33427 (stack85)
        %vm33432 = vcmp.eq.f32.partialorder %v33429, 1.0 (stack86)
        %v33437 = vmul.f32 %v33427, inf (stack83)
        %v33439 = vxor.u32 %v33427, 2147483648 (stack87)
        %v33442 = vmul.f32 %v33427, %v33439 (stack83)
        %v33444 = vadd.f32 %v33442, 1.0 (stack88)
        %v33445 = vlog2.pop %v33444 (stack89)
        %v33446 = vmul.f32 %v33445, 0.6931472 (stack90)
        %v33447 = vmul.f32 -0.5, %v33442 (stack91)
        %v33448 = vadd.f32 %v33447, 1.0 (stack92)
        %v33449 = vmul.f32 %v33448, %v33442 (stack93)
        %v33450 = vand.u32 2147483647, %v33442 (stack94)
        %vm33451 = vcmp.lt.f32.partialorder %v33450, 0.0004427343 (stack95)
        %v33452 = vsel /*vm=*/%vm33451, /*on_true_vy=*/%v33449, /*on_false_vx=*/%v33446 (stack96)
        %v33453 = vxor.u32 %v33452, 2147483648 (stack87)
        %vm33456 = vcmp.lt.f32.partialorder %v33453, 5.0 (stack86)
        %v33461 = vsel /*vm=*/%vm33456, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v33465 = vsel /*vm=*/%vm33456, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v33469 = vsel /*vm=*/%vm33456, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v33473 = vsel /*vm=*/%vm33456, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v33477 = vsel /*vm=*/%vm33456, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v33481 = vsel /*vm=*/%vm33456, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v33485 = vsel /*vm=*/%vm33456, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v33489 = vsel /*vm=*/%vm33456, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v33493 = vsel /*vm=*/%vm33456, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v33497 = vadd.f32 %v33453, -2.5 (stack82)
        %v33499 = vrsqrt.pop %v33453 (stack97)
        %v33500 = vmul.f32 %v33453, %v33499 (stack98)
        %vm33501 = vcmp.eq.f32.partialorder %v33453, inf (stack99)
        %v33502 = vsel /*vm=*/%vm33501, /*on_true_vy=*/%v33453, /*on_false_vx=*/%v33500 (stack100)
        %vm33503 = vcmp.eq.f32.partialorder %v33453, 0.0 (stack101)
        %v33504 = vand.u32 %v33453, 2147483648 (stack102)
        %v33505 = vsel /*vm=*/%vm33503, /*on_true_vy=*/%v33504, /*on_false_vx=*/%v33502 (stack103)
        %v33508 = vadd.f32 %v33505, -3.0 (stack82)
        %v33512 = vsel /*vm=*/%vm33456, /*on_true_vy=*/%v33497, /*on_false_vx=*/%v33508 (stack72)
        %v33516 = vmul.f32 %v33493, %v33512 (stack83)
        %v33520 = vadd.f32 %v33489, %v33516 (stack82)
        %v33524 = vmul.f32 %v33520, %v33512 (stack83)
        %v33528 = vadd.f32 %v33485, %v33524 (stack82)
        %v33532 = vmul.f32 %v33528, %v33512 (stack83)
        %v33536 = vadd.f32 %v33481, %v33532 (stack82)
        %v33540 = vmul.f32 %v33536, %v33512 (stack83)
        %v33544 = vadd.f32 %v33477, %v33540 (stack82)
        %v33548 = vmul.f32 %v33544, %v33512 (stack83)
        %v33552 = vadd.f32 %v33473, %v33548 (stack82)
        %v33556 = vmul.f32 %v33552, %v33512 (stack83)
        %v33560 = vadd.f32 %v33469, %v33556 (stack82)
        %v33564 = vmul.f32 %v33560, %v33512 (stack83)
        %v33568 = vadd.f32 %v33465, %v33564 (stack82)
        %v33572 = vmul.f32 %v33568, %v33512 (stack83)
        %v33576 = vadd.f32 %v33461, %v33572 (stack82)
        %v33580 = vmul.f32 %v33576, %v33427 (stack83)
        %v33584 = vsel /*vm=*/%vm33432, /*on_true_vy=*/%v33437, /*on_false_vx=*/%v33580 (stack72)
        %v33588 = vmul.f32 %v33584, 1.4140625 (stack83)
        %s33590 = scalar_lea.vmem %s280, 800 [#allocation0] (stack107)
        %v33591 = vpack.c.bf16 0.0, %v33588 (stack104)
        %33592 = vst [vmem:[%s33590] sm:$0xf] /*vst_source=*/%v33591 (stack105)
        %v33595 = vadd.s32 %v3816, %v30365 (stack65)
        %s33597 = smul.u32 128, %s27 (stack66)
        %v33598 = vlaneseq (stack67)
        %v33599 = vand.u32 %v33598, 127 (stack68)
        %v33600 = vstv %s33597 (stack69)
        %v33601 = vadd.s32 %v33599, %v33600 (stack70)
        %v33605 = vadd.s32 %v33595, %v33601 (stack65)
        %vm33609 = vcmp.lt.u32.totalorder %v33605, %v33595 (stack71)
        %vm33614 = vcmp.lt.u32.totalorder %v33595, %v3816 (stack71)
        %v33619 = vadd.s32 %v3803, %v30348 (stack65)
        %v33623 = vadd.s32 %v33619, 1 (stack65)
        %v33627 = vsel /*vm=*/%vm33614, /*on_true_vy=*/%v33623, /*on_false_vx=*/%v33619 (stack72)
        %v33631 = vadd.s32 %v33627, 1 (stack65)
        %v33635 = vsel /*vm=*/%vm33609, /*on_true_vy=*/%v33631, /*on_false_vx=*/%v33627 (stack72)
        %v33640 = vadd.s32 %v33635, %v10 (stack65)
        %v33644 = vadd.s32 %v33605, %v9 (stack65)
        %v33648 = vadd.s32 %v33640, %v33644 (stack65)
        %v33650 = vshll.u32 %v33644, 13 (stack73)
        %v33651 = vshrl.u32 %v33644, 19 (stack74)
        %v33652 = vor.u32 %v33650, %v33651 (stack75)
        %v33653 = vxor.u32 %v33648, %v33652 (stack76)
        %v33656 = vadd.s32 %v33648, %v33653 (stack65)
        %v33658 = vshll.u32 %v33653, 15 (stack73)
        %v33659 = vshrl.u32 %v33653, 17 (stack74)
        %v33660 = vor.u32 %v33658, %v33659 (stack75)
        %v33661 = vxor.u32 %v33656, %v33660 (stack76)
        %v33664 = vadd.s32 %v33656, %v33661 (stack65)
        %v33666 = vshll.u32 %v33661, 26 (stack73)
        %v33667 = vshrl.u32 %v33661, 6 (stack74)
        %v33668 = vor.u32 %v33666, %v33667 (stack75)
        %v33669 = vxor.u32 %v33664, %v33668 (stack76)
        %v33672 = vadd.s32 %v33664, %v33669 (stack65)
        %v33676 = vadd.s32 %v33672, %v9 (stack65)
        %v33678 = vshll.u32 %v33669, 6 (stack73)
        %v33679 = vshrl.u32 %v33669, 26 (stack74)
        %v33680 = vor.u32 %v33678, %v33679 (stack75)
        %v33681 = vxor.u32 %v33672, %v33680 (stack76)
        %v33684 = vadd.s32 %v33681, %v8 (stack65)
        %v33688 = vadd.s32 %v33684, 1 (stack65)
        %v33692 = vadd.s32 %v33676, %v33688 (stack65)
        %v33694 = vshll.u32 %v33688, 17 (stack73)
        %v33695 = vshrl.u32 %v33688, 15 (stack74)
        %v33696 = vor.u32 %v33694, %v33695 (stack75)
        %v33697 = vxor.u32 %v33692, %v33696 (stack76)
        %v33700 = vadd.s32 %v33692, %v33697 (stack65)
        %v33702 = vshll.u32 %v33697, 29 (stack73)
        %v33703 = vshrl.u32 %v33697, 3 (stack74)
        %v33704 = vor.u32 %v33702, %v33703 (stack75)
        %v33705 = vxor.u32 %v33700, %v33704 (stack76)
        %v33708 = vadd.s32 %v33700, %v33705 (stack65)
        %v33710 = vshll.u32 %v33705, 16 (stack73)
        %v33711 = vshrl.u32 %v33705, 16 (stack74)
        %v33712 = vor.u32 %v33710, %v33711 (stack75)
        %v33713 = vxor.u32 %v33708, %v33712 (stack76)
        %v33716 = vadd.s32 %v33708, %v33713 (stack65)
        %v33720 = vadd.s32 %v33716, %v8 (stack65)
        %v33722 = vshll.u32 %v33713, 24 (stack73)
        %v33723 = vshrl.u32 %v33713, 8 (stack74)
        %v33724 = vor.u32 %v33722, %v33723 (stack75)
        %v33725 = vxor.u32 %v33716, %v33724 (stack76)
        %v33728 = vadd.s32 %v33725, %v10 (stack65)
        %v33732 = vadd.s32 %v33728, 2 (stack65)
        %v33736 = vadd.s32 %v33720, %v33732 (stack65)
        %v33738 = vshll.u32 %v33732, 13 (stack73)
        %v33739 = vshrl.u32 %v33732, 19 (stack74)
        %v33740 = vor.u32 %v33738, %v33739 (stack75)
        %v33741 = vxor.u32 %v33736, %v33740 (stack76)
        %v33744 = vadd.s32 %v33736, %v33741 (stack65)
        %v33746 = vshll.u32 %v33741, 15 (stack73)
        %v33747 = vshrl.u32 %v33741, 17 (stack74)
        %v33748 = vor.u32 %v33746, %v33747 (stack75)
        %v33749 = vxor.u32 %v33744, %v33748 (stack76)
        %v33752 = vadd.s32 %v33744, %v33749 (stack65)
        %v33754 = vshll.u32 %v33749, 26 (stack73)
        %v33755 = vshrl.u32 %v33749, 6 (stack74)
        %v33756 = vor.u32 %v33754, %v33755 (stack75)
        %v33757 = vxor.u32 %v33752, %v33756 (stack76)
        %v33760 = vadd.s32 %v33752, %v33757 (stack65)
        %v33764 = vadd.s32 %v33760, %v10 (stack65)
        %v33766 = vshll.u32 %v33757, 6 (stack73)
        %v33767 = vshrl.u32 %v33757, 26 (stack74)
        %v33768 = vor.u32 %v33766, %v33767 (stack75)
        %v33769 = vxor.u32 %v33760, %v33768 (stack76)
        %v33772 = vadd.s32 %v33769, %v9 (stack65)
        %v33776 = vadd.s32 %v33772, 3 (stack65)
        %v33780 = vadd.s32 %v33764, %v33776 (stack65)
        %v33782 = vshll.u32 %v33776, 17 (stack73)
        %v33783 = vshrl.u32 %v33776, 15 (stack74)
        %v33784 = vor.u32 %v33782, %v33783 (stack75)
        %v33785 = vxor.u32 %v33780, %v33784 (stack76)
        %v33788 = vadd.s32 %v33780, %v33785 (stack65)
        %v33790 = vshll.u32 %v33785, 29 (stack73)
        %v33791 = vshrl.u32 %v33785, 3 (stack74)
        %v33792 = vor.u32 %v33790, %v33791 (stack75)
        %v33793 = vxor.u32 %v33788, %v33792 (stack76)
        %v33796 = vadd.s32 %v33788, %v33793 (stack65)
        %v33798 = vshll.u32 %v33793, 16 (stack73)
        %v33799 = vshrl.u32 %v33793, 16 (stack74)
        %v33800 = vor.u32 %v33798, %v33799 (stack75)
        %v33801 = vxor.u32 %v33796, %v33800 (stack76)
        %v33804 = vadd.s32 %v33796, %v33801 (stack65)
        %v33808 = vadd.s32 %v33804, %v9 (stack65)
        %v33810 = vshll.u32 %v33801, 24 (stack73)
        %v33811 = vshrl.u32 %v33801, 8 (stack74)
        %v33812 = vor.u32 %v33810, %v33811 (stack75)
        %v33813 = vxor.u32 %v33804, %v33812 (stack76)
        %v33816 = vadd.s32 %v33813, %v8 (stack65)
        %v33820 = vadd.s32 %v33816, 4 (stack65)
        %v33824 = vadd.s32 %v33808, %v33820 (stack65)
        %v33826 = vshll.u32 %v33820, 13 (stack73)
        %v33827 = vshrl.u32 %v33820, 19 (stack74)
        %v33828 = vor.u32 %v33826, %v33827 (stack75)
        %v33829 = vxor.u32 %v33824, %v33828 (stack76)
        %v33832 = vadd.s32 %v33824, %v33829 (stack65)
        %v33834 = vshll.u32 %v33829, 15 (stack73)
        %v33835 = vshrl.u32 %v33829, 17 (stack74)
        %v33836 = vor.u32 %v33834, %v33835 (stack75)
        %v33837 = vxor.u32 %v33832, %v33836 (stack76)
        %v33840 = vadd.s32 %v33832, %v33837 (stack65)
        %v33842 = vshll.u32 %v33837, 26 (stack73)
        %v33843 = vshrl.u32 %v33837, 6 (stack74)
        %v33844 = vor.u32 %v33842, %v33843 (stack75)
        %v33845 = vxor.u32 %v33840, %v33844 (stack76)
        %v33848 = vadd.s32 %v33840, %v33845 (stack65)
        %v33852 = vadd.s32 %v33848, %v8 (stack65)
        %v33854 = vshll.u32 %v33845, 6 (stack73)
        %v33855 = vshrl.u32 %v33845, 26 (stack74)
        %v33856 = vor.u32 %v33854, %v33855 (stack75)
        %v33857 = vxor.u32 %v33848, %v33856 (stack76)
        %v33860 = vadd.s32 %v33857, %v10 (stack65)
        %v33864 = vadd.s32 %v33860, 5 (stack65)
        %v33866 = vxor.u32 %v33852, %v33864 (stack76)
        %v33867 = vand.u32.u8 %v33866, 255 (stack77)
        %v33868 = vand.u32 %v33867, 65535 (stack78)
        %v33869 = vshrl.u32 %v33868, 1 (stack79)
        %v33870 = vor.u32 %v33869, 16256 (stack75)
        %v33871 = vand.u32.u16 %v33870, 65535 (stack80)
        %v33872 = vunpack.i.l.bf16 %v33871 (stack81)
        %v33876 = vadd.f32 %v33872, -1.0 (stack82)
        %v33880 = vmul.f32 %v33876, 2.0 (stack83)
        %v33884 = vadd.f32 %v33880, -0.99609375 (stack82)
        %v33888 = vmax.f32 -0.99609375, %v33884 (stack84)
        %v33890 = vand.u32 2147483647, %v33888 (stack85)
        %vm33893 = vcmp.eq.f32.partialorder %v33890, 1.0 (stack86)
        %v33898 = vmul.f32 %v33888, inf (stack83)
        %v33900 = vxor.u32 %v33888, 2147483648 (stack87)
        %v33903 = vmul.f32 %v33888, %v33900 (stack83)
        %v33905 = vadd.f32 %v33903, 1.0 (stack88)
        %v33906 = vlog2.pop %v33905 (stack89)
        %v33907 = vmul.f32 %v33906, 0.6931472 (stack90)
        %v33908 = vmul.f32 -0.5, %v33903 (stack91)
        %v33909 = vadd.f32 %v33908, 1.0 (stack92)
        %v33910 = vmul.f32 %v33909, %v33903 (stack93)
        %v33911 = vand.u32 2147483647, %v33903 (stack94)
        %vm33912 = vcmp.lt.f32.partialorder %v33911, 0.0004427343 (stack95)
        %v33913 = vsel /*vm=*/%vm33912, /*on_true_vy=*/%v33910, /*on_false_vx=*/%v33907 (stack96)
        %v33914 = vxor.u32 %v33913, 2147483648 (stack87)
        %vm33917 = vcmp.lt.f32.partialorder %v33914, 5.0 (stack86)
        %v33922 = vsel /*vm=*/%vm33917, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v33926 = vsel /*vm=*/%vm33917, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v33930 = vsel /*vm=*/%vm33917, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v33934 = vsel /*vm=*/%vm33917, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v33938 = vsel /*vm=*/%vm33917, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v33942 = vsel /*vm=*/%vm33917, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v33946 = vsel /*vm=*/%vm33917, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v33950 = vsel /*vm=*/%vm33917, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v33954 = vsel /*vm=*/%vm33917, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v33958 = vadd.f32 %v33914, -2.5 (stack82)
        %v33960 = vrsqrt.pop %v33914 (stack97)
        %v33961 = vmul.f32 %v33914, %v33960 (stack98)
        %vm33962 = vcmp.eq.f32.partialorder %v33914, inf (stack99)
        %v33963 = vsel /*vm=*/%vm33962, /*on_true_vy=*/%v33914, /*on_false_vx=*/%v33961 (stack100)
        %vm33964 = vcmp.eq.f32.partialorder %v33914, 0.0 (stack101)
        %v33965 = vand.u32 %v33914, 2147483648 (stack102)
        %v33966 = vsel /*vm=*/%vm33964, /*on_true_vy=*/%v33965, /*on_false_vx=*/%v33963 (stack103)
        %v33969 = vadd.f32 %v33966, -3.0 (stack82)
        %v33973 = vsel /*vm=*/%vm33917, /*on_true_vy=*/%v33958, /*on_false_vx=*/%v33969 (stack72)
        %v33977 = vmul.f32 %v33954, %v33973 (stack83)
        %v33981 = vadd.f32 %v33950, %v33977 (stack82)
        %v33985 = vmul.f32 %v33981, %v33973 (stack83)
        %v33989 = vadd.f32 %v33946, %v33985 (stack82)
        %v33993 = vmul.f32 %v33989, %v33973 (stack83)
        %v33997 = vadd.f32 %v33942, %v33993 (stack82)
        %v34001 = vmul.f32 %v33997, %v33973 (stack83)
        %v34005 = vadd.f32 %v33938, %v34001 (stack82)
        %v34009 = vmul.f32 %v34005, %v33973 (stack83)
        %v34013 = vadd.f32 %v33934, %v34009 (stack82)
        %v34017 = vmul.f32 %v34013, %v33973 (stack83)
        %v34021 = vadd.f32 %v33930, %v34017 (stack82)
        %v34025 = vmul.f32 %v34021, %v33973 (stack83)
        %v34029 = vadd.f32 %v33926, %v34025 (stack82)
        %v34033 = vmul.f32 %v34029, %v33973 (stack83)
        %v34037 = vadd.f32 %v33922, %v34033 (stack82)
        %v34041 = vmul.f32 %v34037, %v33888 (stack83)
        %v34045 = vsel /*vm=*/%vm33893, /*on_true_vy=*/%v33898, /*on_false_vx=*/%v34041 (stack72)
        %v34049 = vmul.f32 %v34045, 1.4140625 (stack83)
        %s34051 = scalar_lea.vmem %s280, 928 [#allocation0] (stack107)
        %v34052 = vpack.c.bf16 0.0, %v34049 (stack104)
        %34053 = vst [vmem:[%s34051] sm:$0xf] /*vst_source=*/%v34052 (stack105)
        %s34054 = sadd.s32 %s339, 72 (stack106)
        %s34055 = sshrl.u32 %s34054, 10 (stack49)
        %p34056 = scmp.lt.s32.totalorder 1, %s34055 (stack50)
        %s34057 = scalar_select /*predicate=*/%p34056, /*on_true=*/1, /*on_false=*/%s34055 (stack51)
        %s34058 = sand.u32 %s34054, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s34059 = sshrl.u32 %s34058, 7 (stack53)
        %s34060 = sand.u32 %s34058, 127 /* smod.u32 w/div 128 */ (stack54)
        %s34061 = smul.addr %s34057, 8 (stack55)
        %s34062 = scalar_lea.vmem %s3, %s34061 (stack56)
        %s34064 = scalar_lea.vmem %s34062, %s34059 (stack57)
        %v34065 = vld [vmem:[%s34064] ss:$0 sm:$0xff] (stack58)
        %s34066 = sand.u32 %s34060, 255 (stack59)
        %s34068 = sor.u32 256, %s34066 (stack60)
        %34069 = vbcast.lane.b32.xlu0 %v34065, %s34068 (stack61)
        %v34070 = vpop.permute.xlu0 %34069 (stack62)
        %s34071 = sadd.s32 %s347, 72 (stack106)
        %s34072 = sshrl.u32 %s34071, 10 (stack49)
        %p34073 = scmp.lt.s32.totalorder 1, %s34072 (stack50)
        %s34074 = scalar_select /*predicate=*/%p34073, /*on_true=*/1, /*on_false=*/%s34072 (stack51)
        %s34075 = sand.u32 %s34071, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s34076 = sshrl.u32 %s34075, 7 (stack53)
        %s34077 = sand.u32 %s34075, 127 /* smod.u32 w/div 128 */ (stack54)
        %s34078 = smul.addr %s34074, 8 (stack55)
        %s34079 = scalar_lea.vmem %s5, %s34078 (stack56)
        %s34081 = scalar_lea.vmem %s34079, %s34076 (stack57)
        %v34082 = vld [vmem:[%s34081] ss:$0 sm:$0xff] (stack58)
        %s34083 = sand.u32 %s34077, 255 (stack59)
        %s34085 = sor.u32 256, %s34083 (stack60)
        %34086 = vbcast.lane.b32.xlu0 %v34082, %s34085 (stack61)
        %v34087 = vpop.permute.xlu0 %34086 (stack62)
        %v34090 = vadd.s32 %v408, %v34087 (stack65)
        %s34092 = smul.u32 128, %s27 (stack66)
        %v34093 = vlaneseq (stack67)
        %v34094 = vand.u32 %v34093, 127 (stack68)
        %v34095 = vstv %s34092 (stack69)
        %v34096 = vadd.s32 %v34094, %v34095 (stack70)
        %v34100 = vadd.s32 %v34090, %v34096 (stack65)
        %vm34104 = vcmp.lt.u32.totalorder %v34100, %v34090 (stack71)
        %vm34109 = vcmp.lt.u32.totalorder %v34090, %v408 (stack71)
        %v34114 = vadd.s32 %v380, %v34070 (stack65)
        %v34118 = vadd.s32 %v34114, 1 (stack65)
        %v34122 = vsel /*vm=*/%vm34109, /*on_true_vy=*/%v34118, /*on_false_vx=*/%v34114 (stack72)
        %v34126 = vadd.s32 %v34122, 1 (stack65)
        %v34130 = vsel /*vm=*/%vm34104, /*on_true_vy=*/%v34126, /*on_false_vx=*/%v34122 (stack72)
        %v34135 = vadd.s32 %v34130, %v10 (stack65)
        %v34139 = vadd.s32 %v34100, %v9 (stack65)
        %v34143 = vadd.s32 %v34135, %v34139 (stack65)
        %v34145 = vshll.u32 %v34139, 13 (stack73)
        %v34146 = vshrl.u32 %v34139, 19 (stack74)
        %v34147 = vor.u32 %v34145, %v34146 (stack75)
        %v34148 = vxor.u32 %v34143, %v34147 (stack76)
        %v34151 = vadd.s32 %v34143, %v34148 (stack65)
        %v34153 = vshll.u32 %v34148, 15 (stack73)
        %v34154 = vshrl.u32 %v34148, 17 (stack74)
        %v34155 = vor.u32 %v34153, %v34154 (stack75)
        %v34156 = vxor.u32 %v34151, %v34155 (stack76)
        %v34159 = vadd.s32 %v34151, %v34156 (stack65)
        %v34161 = vshll.u32 %v34156, 26 (stack73)
        %v34162 = vshrl.u32 %v34156, 6 (stack74)
        %v34163 = vor.u32 %v34161, %v34162 (stack75)
        %v34164 = vxor.u32 %v34159, %v34163 (stack76)
        %v34167 = vadd.s32 %v34159, %v34164 (stack65)
        %v34171 = vadd.s32 %v34167, %v9 (stack65)
        %v34173 = vshll.u32 %v34164, 6 (stack73)
        %v34174 = vshrl.u32 %v34164, 26 (stack74)
        %v34175 = vor.u32 %v34173, %v34174 (stack75)
        %v34176 = vxor.u32 %v34167, %v34175 (stack76)
        %v34179 = vadd.s32 %v34176, %v8 (stack65)
        %v34183 = vadd.s32 %v34179, 1 (stack65)
        %v34187 = vadd.s32 %v34171, %v34183 (stack65)
        %v34189 = vshll.u32 %v34183, 17 (stack73)
        %v34190 = vshrl.u32 %v34183, 15 (stack74)
        %v34191 = vor.u32 %v34189, %v34190 (stack75)
        %v34192 = vxor.u32 %v34187, %v34191 (stack76)
        %v34195 = vadd.s32 %v34187, %v34192 (stack65)
        %v34197 = vshll.u32 %v34192, 29 (stack73)
        %v34198 = vshrl.u32 %v34192, 3 (stack74)
        %v34199 = vor.u32 %v34197, %v34198 (stack75)
        %v34200 = vxor.u32 %v34195, %v34199 (stack76)
        %v34203 = vadd.s32 %v34195, %v34200 (stack65)
        %v34205 = vshll.u32 %v34200, 16 (stack73)
        %v34206 = vshrl.u32 %v34200, 16 (stack74)
        %v34207 = vor.u32 %v34205, %v34206 (stack75)
        %v34208 = vxor.u32 %v34203, %v34207 (stack76)
        %v34211 = vadd.s32 %v34203, %v34208 (stack65)
        %v34215 = vadd.s32 %v34211, %v8 (stack65)
        %v34217 = vshll.u32 %v34208, 24 (stack73)
        %v34218 = vshrl.u32 %v34208, 8 (stack74)
        %v34219 = vor.u32 %v34217, %v34218 (stack75)
        %v34220 = vxor.u32 %v34211, %v34219 (stack76)
        %v34223 = vadd.s32 %v34220, %v10 (stack65)
        %v34227 = vadd.s32 %v34223, 2 (stack65)
        %v34231 = vadd.s32 %v34215, %v34227 (stack65)
        %v34233 = vshll.u32 %v34227, 13 (stack73)
        %v34234 = vshrl.u32 %v34227, 19 (stack74)
        %v34235 = vor.u32 %v34233, %v34234 (stack75)
        %v34236 = vxor.u32 %v34231, %v34235 (stack76)
        %v34239 = vadd.s32 %v34231, %v34236 (stack65)
        %v34241 = vshll.u32 %v34236, 15 (stack73)
        %v34242 = vshrl.u32 %v34236, 17 (stack74)
        %v34243 = vor.u32 %v34241, %v34242 (stack75)
        %v34244 = vxor.u32 %v34239, %v34243 (stack76)
        %v34247 = vadd.s32 %v34239, %v34244 (stack65)
        %v34249 = vshll.u32 %v34244, 26 (stack73)
        %v34250 = vshrl.u32 %v34244, 6 (stack74)
        %v34251 = vor.u32 %v34249, %v34250 (stack75)
        %v34252 = vxor.u32 %v34247, %v34251 (stack76)
        %v34255 = vadd.s32 %v34247, %v34252 (stack65)
        %v34259 = vadd.s32 %v34255, %v10 (stack65)
        %v34261 = vshll.u32 %v34252, 6 (stack73)
        %v34262 = vshrl.u32 %v34252, 26 (stack74)
        %v34263 = vor.u32 %v34261, %v34262 (stack75)
        %v34264 = vxor.u32 %v34255, %v34263 (stack76)
        %v34267 = vadd.s32 %v34264, %v9 (stack65)
        %v34271 = vadd.s32 %v34267, 3 (stack65)
        %v34275 = vadd.s32 %v34259, %v34271 (stack65)
        %v34277 = vshll.u32 %v34271, 17 (stack73)
        %v34278 = vshrl.u32 %v34271, 15 (stack74)
        %v34279 = vor.u32 %v34277, %v34278 (stack75)
        %v34280 = vxor.u32 %v34275, %v34279 (stack76)
        %v34283 = vadd.s32 %v34275, %v34280 (stack65)
        %v34285 = vshll.u32 %v34280, 29 (stack73)
        %v34286 = vshrl.u32 %v34280, 3 (stack74)
        %v34287 = vor.u32 %v34285, %v34286 (stack75)
        %v34288 = vxor.u32 %v34283, %v34287 (stack76)
        %v34291 = vadd.s32 %v34283, %v34288 (stack65)
        %v34293 = vshll.u32 %v34288, 16 (stack73)
        %v34294 = vshrl.u32 %v34288, 16 (stack74)
        %v34295 = vor.u32 %v34293, %v34294 (stack75)
        %v34296 = vxor.u32 %v34291, %v34295 (stack76)
        %v34299 = vadd.s32 %v34291, %v34296 (stack65)
        %v34303 = vadd.s32 %v34299, %v9 (stack65)
        %v34305 = vshll.u32 %v34296, 24 (stack73)
        %v34306 = vshrl.u32 %v34296, 8 (stack74)
        %v34307 = vor.u32 %v34305, %v34306 (stack75)
        %v34308 = vxor.u32 %v34299, %v34307 (stack76)
        %v34311 = vadd.s32 %v34308, %v8 (stack65)
        %v34315 = vadd.s32 %v34311, 4 (stack65)
        %v34319 = vadd.s32 %v34303, %v34315 (stack65)
        %v34321 = vshll.u32 %v34315, 13 (stack73)
        %v34322 = vshrl.u32 %v34315, 19 (stack74)
        %v34323 = vor.u32 %v34321, %v34322 (stack75)
        %v34324 = vxor.u32 %v34319, %v34323 (stack76)
        %v34327 = vadd.s32 %v34319, %v34324 (stack65)
        %v34329 = vshll.u32 %v34324, 15 (stack73)
        %v34330 = vshrl.u32 %v34324, 17 (stack74)
        %v34331 = vor.u32 %v34329, %v34330 (stack75)
        %v34332 = vxor.u32 %v34327, %v34331 (stack76)
        %v34335 = vadd.s32 %v34327, %v34332 (stack65)
        %v34337 = vshll.u32 %v34332, 26 (stack73)
        %v34338 = vshrl.u32 %v34332, 6 (stack74)
        %v34339 = vor.u32 %v34337, %v34338 (stack75)
        %v34340 = vxor.u32 %v34335, %v34339 (stack76)
        %v34343 = vadd.s32 %v34335, %v34340 (stack65)
        %v34347 = vadd.s32 %v34343, %v8 (stack65)
        %v34349 = vshll.u32 %v34340, 6 (stack73)
        %v34350 = vshrl.u32 %v34340, 26 (stack74)
        %v34351 = vor.u32 %v34349, %v34350 (stack75)
        %v34352 = vxor.u32 %v34343, %v34351 (stack76)
        %v34355 = vadd.s32 %v34352, %v10 (stack65)
        %v34359 = vadd.s32 %v34355, 5 (stack65)
        %v34361 = vxor.u32 %v34347, %v34359 (stack76)
        %v34362 = vand.u32.u8 %v34361, 255 (stack77)
        %v34363 = vand.u32 %v34362, 65535 (stack78)
        %v34364 = vshrl.u32 %v34363, 1 (stack79)
        %v34365 = vor.u32 %v34364, 16256 (stack75)
        %v34366 = vand.u32.u16 %v34365, 65535 (stack80)
        %v34367 = vunpack.i.l.bf16 %v34366 (stack81)
        %v34371 = vadd.f32 %v34367, -1.0 (stack82)
        %v34375 = vmul.f32 %v34371, 2.0 (stack83)
        %v34379 = vadd.f32 %v34375, -0.99609375 (stack82)
        %v34383 = vmax.f32 -0.99609375, %v34379 (stack84)
        %v34385 = vand.u32 2147483647, %v34383 (stack85)
        %vm34388 = vcmp.eq.f32.partialorder %v34385, 1.0 (stack86)
        %v34393 = vmul.f32 %v34383, inf (stack83)
        %v34395 = vxor.u32 %v34383, 2147483648 (stack87)
        %v34398 = vmul.f32 %v34383, %v34395 (stack83)
        %v34400 = vadd.f32 %v34398, 1.0 (stack88)
        %v34401 = vlog2.pop %v34400 (stack89)
        %v34402 = vmul.f32 %v34401, 0.6931472 (stack90)
        %v34403 = vmul.f32 -0.5, %v34398 (stack91)
        %v34404 = vadd.f32 %v34403, 1.0 (stack92)
        %v34405 = vmul.f32 %v34404, %v34398 (stack93)
        %v34406 = vand.u32 2147483647, %v34398 (stack94)
        %vm34407 = vcmp.lt.f32.partialorder %v34406, 0.0004427343 (stack95)
        %v34408 = vsel /*vm=*/%vm34407, /*on_true_vy=*/%v34405, /*on_false_vx=*/%v34402 (stack96)
        %v34409 = vxor.u32 %v34408, 2147483648 (stack87)
        %vm34412 = vcmp.lt.f32.partialorder %v34409, 5.0 (stack86)
        %v34417 = vsel /*vm=*/%vm34412, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v34421 = vsel /*vm=*/%vm34412, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v34425 = vsel /*vm=*/%vm34412, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v34429 = vsel /*vm=*/%vm34412, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v34433 = vsel /*vm=*/%vm34412, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v34437 = vsel /*vm=*/%vm34412, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v34441 = vsel /*vm=*/%vm34412, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v34445 = vsel /*vm=*/%vm34412, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v34449 = vsel /*vm=*/%vm34412, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v34453 = vadd.f32 %v34409, -2.5 (stack82)
        %v34455 = vrsqrt.pop %v34409 (stack97)
        %v34456 = vmul.f32 %v34409, %v34455 (stack98)
        %vm34457 = vcmp.eq.f32.partialorder %v34409, inf (stack99)
        %v34458 = vsel /*vm=*/%vm34457, /*on_true_vy=*/%v34409, /*on_false_vx=*/%v34456 (stack100)
        %vm34459 = vcmp.eq.f32.partialorder %v34409, 0.0 (stack101)
        %v34460 = vand.u32 %v34409, 2147483648 (stack102)
        %v34461 = vsel /*vm=*/%vm34459, /*on_true_vy=*/%v34460, /*on_false_vx=*/%v34458 (stack103)
        %v34464 = vadd.f32 %v34461, -3.0 (stack82)
        %v34468 = vsel /*vm=*/%vm34412, /*on_true_vy=*/%v34453, /*on_false_vx=*/%v34464 (stack72)
        %v34472 = vmul.f32 %v34449, %v34468 (stack83)
        %v34476 = vadd.f32 %v34445, %v34472 (stack82)
        %v34480 = vmul.f32 %v34476, %v34468 (stack83)
        %v34484 = vadd.f32 %v34441, %v34480 (stack82)
        %v34488 = vmul.f32 %v34484, %v34468 (stack83)
        %v34492 = vadd.f32 %v34437, %v34488 (stack82)
        %v34496 = vmul.f32 %v34492, %v34468 (stack83)
        %v34500 = vadd.f32 %v34433, %v34496 (stack82)
        %v34504 = vmul.f32 %v34500, %v34468 (stack83)
        %v34508 = vadd.f32 %v34429, %v34504 (stack82)
        %v34512 = vmul.f32 %v34508, %v34468 (stack83)
        %v34516 = vadd.f32 %v34425, %v34512 (stack82)
        %v34520 = vmul.f32 %v34516, %v34468 (stack83)
        %v34524 = vadd.f32 %v34421, %v34520 (stack82)
        %v34528 = vmul.f32 %v34524, %v34468 (stack83)
        %v34532 = vadd.f32 %v34417, %v34528 (stack82)
        %v34536 = vmul.f32 %v34532, %v34383 (stack83)
        %v34540 = vsel /*vm=*/%vm34388, /*on_true_vy=*/%v34393, /*on_false_vx=*/%v34536 (stack72)
        %v34544 = vmul.f32 %v34540, 1.4140625 (stack83)
        %s34546 = scalar_lea.vmem %s280, 36 [#allocation0] (stack107)
        %v34547 = vpack.c.bf16 0.0, %v34544 (stack104)
        %34548 = vst [vmem:[%s34546] sm:$0xf] /*vst_source=*/%v34547 (stack105)
        %v34551 = vadd.s32 %v894, %v34087 (stack65)
        %s34553 = smul.u32 128, %s27 (stack66)
        %v34554 = vlaneseq (stack67)
        %v34555 = vand.u32 %v34554, 127 (stack68)
        %v34556 = vstv %s34553 (stack69)
        %v34557 = vadd.s32 %v34555, %v34556 (stack70)
        %v34561 = vadd.s32 %v34551, %v34557 (stack65)
        %vm34565 = vcmp.lt.u32.totalorder %v34561, %v34551 (stack71)
        %vm34570 = vcmp.lt.u32.totalorder %v34551, %v894 (stack71)
        %v34575 = vadd.s32 %v881, %v34070 (stack65)
        %v34579 = vadd.s32 %v34575, 1 (stack65)
        %v34583 = vsel /*vm=*/%vm34570, /*on_true_vy=*/%v34579, /*on_false_vx=*/%v34575 (stack72)
        %v34587 = vadd.s32 %v34583, 1 (stack65)
        %v34591 = vsel /*vm=*/%vm34565, /*on_true_vy=*/%v34587, /*on_false_vx=*/%v34583 (stack72)
        %v34596 = vadd.s32 %v34591, %v10 (stack65)
        %v34600 = vadd.s32 %v34561, %v9 (stack65)
        %v34604 = vadd.s32 %v34596, %v34600 (stack65)
        %v34606 = vshll.u32 %v34600, 13 (stack73)
        %v34607 = vshrl.u32 %v34600, 19 (stack74)
        %v34608 = vor.u32 %v34606, %v34607 (stack75)
        %v34609 = vxor.u32 %v34604, %v34608 (stack76)
        %v34612 = vadd.s32 %v34604, %v34609 (stack65)
        %v34614 = vshll.u32 %v34609, 15 (stack73)
        %v34615 = vshrl.u32 %v34609, 17 (stack74)
        %v34616 = vor.u32 %v34614, %v34615 (stack75)
        %v34617 = vxor.u32 %v34612, %v34616 (stack76)
        %v34620 = vadd.s32 %v34612, %v34617 (stack65)
        %v34622 = vshll.u32 %v34617, 26 (stack73)
        %v34623 = vshrl.u32 %v34617, 6 (stack74)
        %v34624 = vor.u32 %v34622, %v34623 (stack75)
        %v34625 = vxor.u32 %v34620, %v34624 (stack76)
        %v34628 = vadd.s32 %v34620, %v34625 (stack65)
        %v34632 = vadd.s32 %v34628, %v9 (stack65)
        %v34634 = vshll.u32 %v34625, 6 (stack73)
        %v34635 = vshrl.u32 %v34625, 26 (stack74)
        %v34636 = vor.u32 %v34634, %v34635 (stack75)
        %v34637 = vxor.u32 %v34628, %v34636 (stack76)
        %v34640 = vadd.s32 %v34637, %v8 (stack65)
        %v34644 = vadd.s32 %v34640, 1 (stack65)
        %v34648 = vadd.s32 %v34632, %v34644 (stack65)
        %v34650 = vshll.u32 %v34644, 17 (stack73)
        %v34651 = vshrl.u32 %v34644, 15 (stack74)
        %v34652 = vor.u32 %v34650, %v34651 (stack75)
        %v34653 = vxor.u32 %v34648, %v34652 (stack76)
        %v34656 = vadd.s32 %v34648, %v34653 (stack65)
        %v34658 = vshll.u32 %v34653, 29 (stack73)
        %v34659 = vshrl.u32 %v34653, 3 (stack74)
        %v34660 = vor.u32 %v34658, %v34659 (stack75)
        %v34661 = vxor.u32 %v34656, %v34660 (stack76)
        %v34664 = vadd.s32 %v34656, %v34661 (stack65)
        %v34666 = vshll.u32 %v34661, 16 (stack73)
        %v34667 = vshrl.u32 %v34661, 16 (stack74)
        %v34668 = vor.u32 %v34666, %v34667 (stack75)
        %v34669 = vxor.u32 %v34664, %v34668 (stack76)
        %v34672 = vadd.s32 %v34664, %v34669 (stack65)
        %v34676 = vadd.s32 %v34672, %v8 (stack65)
        %v34678 = vshll.u32 %v34669, 24 (stack73)
        %v34679 = vshrl.u32 %v34669, 8 (stack74)
        %v34680 = vor.u32 %v34678, %v34679 (stack75)
        %v34681 = vxor.u32 %v34672, %v34680 (stack76)
        %v34684 = vadd.s32 %v34681, %v10 (stack65)
        %v34688 = vadd.s32 %v34684, 2 (stack65)
        %v34692 = vadd.s32 %v34676, %v34688 (stack65)
        %v34694 = vshll.u32 %v34688, 13 (stack73)
        %v34695 = vshrl.u32 %v34688, 19 (stack74)
        %v34696 = vor.u32 %v34694, %v34695 (stack75)
        %v34697 = vxor.u32 %v34692, %v34696 (stack76)
        %v34700 = vadd.s32 %v34692, %v34697 (stack65)
        %v34702 = vshll.u32 %v34697, 15 (stack73)
        %v34703 = vshrl.u32 %v34697, 17 (stack74)
        %v34704 = vor.u32 %v34702, %v34703 (stack75)
        %v34705 = vxor.u32 %v34700, %v34704 (stack76)
        %v34708 = vadd.s32 %v34700, %v34705 (stack65)
        %v34710 = vshll.u32 %v34705, 26 (stack73)
        %v34711 = vshrl.u32 %v34705, 6 (stack74)
        %v34712 = vor.u32 %v34710, %v34711 (stack75)
        %v34713 = vxor.u32 %v34708, %v34712 (stack76)
        %v34716 = vadd.s32 %v34708, %v34713 (stack65)
        %v34720 = vadd.s32 %v34716, %v10 (stack65)
        %v34722 = vshll.u32 %v34713, 6 (stack73)
        %v34723 = vshrl.u32 %v34713, 26 (stack74)
        %v34724 = vor.u32 %v34722, %v34723 (stack75)
        %v34725 = vxor.u32 %v34716, %v34724 (stack76)
        %v34728 = vadd.s32 %v34725, %v9 (stack65)
        %v34732 = vadd.s32 %v34728, 3 (stack65)
        %v34736 = vadd.s32 %v34720, %v34732 (stack65)
        %v34738 = vshll.u32 %v34732, 17 (stack73)
        %v34739 = vshrl.u32 %v34732, 15 (stack74)
        %v34740 = vor.u32 %v34738, %v34739 (stack75)
        %v34741 = vxor.u32 %v34736, %v34740 (stack76)
        %v34744 = vadd.s32 %v34736, %v34741 (stack65)
        %v34746 = vshll.u32 %v34741, 29 (stack73)
        %v34747 = vshrl.u32 %v34741, 3 (stack74)
        %v34748 = vor.u32 %v34746, %v34747 (stack75)
        %v34749 = vxor.u32 %v34744, %v34748 (stack76)
        %v34752 = vadd.s32 %v34744, %v34749 (stack65)
        %v34754 = vshll.u32 %v34749, 16 (stack73)
        %v34755 = vshrl.u32 %v34749, 16 (stack74)
        %v34756 = vor.u32 %v34754, %v34755 (stack75)
        %v34757 = vxor.u32 %v34752, %v34756 (stack76)
        %v34760 = vadd.s32 %v34752, %v34757 (stack65)
        %v34764 = vadd.s32 %v34760, %v9 (stack65)
        %v34766 = vshll.u32 %v34757, 24 (stack73)
        %v34767 = vshrl.u32 %v34757, 8 (stack74)
        %v34768 = vor.u32 %v34766, %v34767 (stack75)
        %v34769 = vxor.u32 %v34760, %v34768 (stack76)
        %v34772 = vadd.s32 %v34769, %v8 (stack65)
        %v34776 = vadd.s32 %v34772, 4 (stack65)
        %v34780 = vadd.s32 %v34764, %v34776 (stack65)
        %v34782 = vshll.u32 %v34776, 13 (stack73)
        %v34783 = vshrl.u32 %v34776, 19 (stack74)
        %v34784 = vor.u32 %v34782, %v34783 (stack75)
        %v34785 = vxor.u32 %v34780, %v34784 (stack76)
        %v34788 = vadd.s32 %v34780, %v34785 (stack65)
        %v34790 = vshll.u32 %v34785, 15 (stack73)
        %v34791 = vshrl.u32 %v34785, 17 (stack74)
        %v34792 = vor.u32 %v34790, %v34791 (stack75)
        %v34793 = vxor.u32 %v34788, %v34792 (stack76)
        %v34796 = vadd.s32 %v34788, %v34793 (stack65)
        %v34798 = vshll.u32 %v34793, 26 (stack73)
        %v34799 = vshrl.u32 %v34793, 6 (stack74)
        %v34800 = vor.u32 %v34798, %v34799 (stack75)
        %v34801 = vxor.u32 %v34796, %v34800 (stack76)
        %v34804 = vadd.s32 %v34796, %v34801 (stack65)
        %v34808 = vadd.s32 %v34804, %v8 (stack65)
        %v34810 = vshll.u32 %v34801, 6 (stack73)
        %v34811 = vshrl.u32 %v34801, 26 (stack74)
        %v34812 = vor.u32 %v34810, %v34811 (stack75)
        %v34813 = vxor.u32 %v34804, %v34812 (stack76)
        %v34816 = vadd.s32 %v34813, %v10 (stack65)
        %v34820 = vadd.s32 %v34816, 5 (stack65)
        %v34822 = vxor.u32 %v34808, %v34820 (stack76)
        %v34823 = vand.u32.u8 %v34822, 255 (stack77)
        %v34824 = vand.u32 %v34823, 65535 (stack78)
        %v34825 = vshrl.u32 %v34824, 1 (stack79)
        %v34826 = vor.u32 %v34825, 16256 (stack75)
        %v34827 = vand.u32.u16 %v34826, 65535 (stack80)
        %v34828 = vunpack.i.l.bf16 %v34827 (stack81)
        %v34832 = vadd.f32 %v34828, -1.0 (stack82)
        %v34836 = vmul.f32 %v34832, 2.0 (stack83)
        %v34840 = vadd.f32 %v34836, -0.99609375 (stack82)
        %v34844 = vmax.f32 -0.99609375, %v34840 (stack84)
        %v34846 = vand.u32 2147483647, %v34844 (stack85)
        %vm34849 = vcmp.eq.f32.partialorder %v34846, 1.0 (stack86)
        %v34854 = vmul.f32 %v34844, inf (stack83)
        %v34856 = vxor.u32 %v34844, 2147483648 (stack87)
        %v34859 = vmul.f32 %v34844, %v34856 (stack83)
        %v34861 = vadd.f32 %v34859, 1.0 (stack88)
        %v34862 = vlog2.pop %v34861 (stack89)
        %v34863 = vmul.f32 %v34862, 0.6931472 (stack90)
        %v34864 = vmul.f32 -0.5, %v34859 (stack91)
        %v34865 = vadd.f32 %v34864, 1.0 (stack92)
        %v34866 = vmul.f32 %v34865, %v34859 (stack93)
        %v34867 = vand.u32 2147483647, %v34859 (stack94)
        %vm34868 = vcmp.lt.f32.partialorder %v34867, 0.0004427343 (stack95)
        %v34869 = vsel /*vm=*/%vm34868, /*on_true_vy=*/%v34866, /*on_false_vx=*/%v34863 (stack96)
        %v34870 = vxor.u32 %v34869, 2147483648 (stack87)
        %vm34873 = vcmp.lt.f32.partialorder %v34870, 5.0 (stack86)
        %v34878 = vsel /*vm=*/%vm34873, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v34882 = vsel /*vm=*/%vm34873, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v34886 = vsel /*vm=*/%vm34873, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v34890 = vsel /*vm=*/%vm34873, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v34894 = vsel /*vm=*/%vm34873, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v34898 = vsel /*vm=*/%vm34873, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v34902 = vsel /*vm=*/%vm34873, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v34906 = vsel /*vm=*/%vm34873, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v34910 = vsel /*vm=*/%vm34873, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v34914 = vadd.f32 %v34870, -2.5 (stack82)
        %v34916 = vrsqrt.pop %v34870 (stack97)
        %v34917 = vmul.f32 %v34870, %v34916 (stack98)
        %vm34918 = vcmp.eq.f32.partialorder %v34870, inf (stack99)
        %v34919 = vsel /*vm=*/%vm34918, /*on_true_vy=*/%v34870, /*on_false_vx=*/%v34917 (stack100)
        %vm34920 = vcmp.eq.f32.partialorder %v34870, 0.0 (stack101)
        %v34921 = vand.u32 %v34870, 2147483648 (stack102)
        %v34922 = vsel /*vm=*/%vm34920, /*on_true_vy=*/%v34921, /*on_false_vx=*/%v34919 (stack103)
        %v34925 = vadd.f32 %v34922, -3.0 (stack82)
        %v34929 = vsel /*vm=*/%vm34873, /*on_true_vy=*/%v34914, /*on_false_vx=*/%v34925 (stack72)
        %v34933 = vmul.f32 %v34910, %v34929 (stack83)
        %v34937 = vadd.f32 %v34906, %v34933 (stack82)
        %v34941 = vmul.f32 %v34937, %v34929 (stack83)
        %v34945 = vadd.f32 %v34902, %v34941 (stack82)
        %v34949 = vmul.f32 %v34945, %v34929 (stack83)
        %v34953 = vadd.f32 %v34898, %v34949 (stack82)
        %v34957 = vmul.f32 %v34953, %v34929 (stack83)
        %v34961 = vadd.f32 %v34894, %v34957 (stack82)
        %v34965 = vmul.f32 %v34961, %v34929 (stack83)
        %v34969 = vadd.f32 %v34890, %v34965 (stack82)
        %v34973 = vmul.f32 %v34969, %v34929 (stack83)
        %v34977 = vadd.f32 %v34886, %v34973 (stack82)
        %v34981 = vmul.f32 %v34977, %v34929 (stack83)
        %v34985 = vadd.f32 %v34882, %v34981 (stack82)
        %v34989 = vmul.f32 %v34985, %v34929 (stack83)
        %v34993 = vadd.f32 %v34878, %v34989 (stack82)
        %v34997 = vmul.f32 %v34993, %v34844 (stack83)
        %v35001 = vsel /*vm=*/%vm34849, /*on_true_vy=*/%v34854, /*on_false_vx=*/%v34997 (stack72)
        %v35005 = vmul.f32 %v35001, 1.4140625 (stack83)
        %s35007 = scalar_lea.vmem %s280, 164 [#allocation0] (stack107)
        %v35008 = vpack.c.bf16 0.0, %v35005 (stack104)
        %35009 = vst [vmem:[%s35007] sm:$0xf] /*vst_source=*/%v35008 (stack105)
        %v35012 = vadd.s32 %v1381, %v34087 (stack65)
        %s35014 = smul.u32 128, %s27 (stack66)
        %v35015 = vlaneseq (stack67)
        %v35016 = vand.u32 %v35015, 127 (stack68)
        %v35017 = vstv %s35014 (stack69)
        %v35018 = vadd.s32 %v35016, %v35017 (stack70)
        %v35022 = vadd.s32 %v35012, %v35018 (stack65)
        %vm35026 = vcmp.lt.u32.totalorder %v35022, %v35012 (stack71)
        %vm35031 = vcmp.lt.u32.totalorder %v35012, %v1381 (stack71)
        %v35036 = vadd.s32 %v1368, %v34070 (stack65)
        %v35040 = vadd.s32 %v35036, 1 (stack65)
        %v35044 = vsel /*vm=*/%vm35031, /*on_true_vy=*/%v35040, /*on_false_vx=*/%v35036 (stack72)
        %v35048 = vadd.s32 %v35044, 1 (stack65)
        %v35052 = vsel /*vm=*/%vm35026, /*on_true_vy=*/%v35048, /*on_false_vx=*/%v35044 (stack72)
        %v35057 = vadd.s32 %v35052, %v10 (stack65)
        %v35061 = vadd.s32 %v35022, %v9 (stack65)
        %v35065 = vadd.s32 %v35057, %v35061 (stack65)
        %v35067 = vshll.u32 %v35061, 13 (stack73)
        %v35068 = vshrl.u32 %v35061, 19 (stack74)
        %v35069 = vor.u32 %v35067, %v35068 (stack75)
        %v35070 = vxor.u32 %v35065, %v35069 (stack76)
        %v35073 = vadd.s32 %v35065, %v35070 (stack65)
        %v35075 = vshll.u32 %v35070, 15 (stack73)
        %v35076 = vshrl.u32 %v35070, 17 (stack74)
        %v35077 = vor.u32 %v35075, %v35076 (stack75)
        %v35078 = vxor.u32 %v35073, %v35077 (stack76)
        %v35081 = vadd.s32 %v35073, %v35078 (stack65)
        %v35083 = vshll.u32 %v35078, 26 (stack73)
        %v35084 = vshrl.u32 %v35078, 6 (stack74)
        %v35085 = vor.u32 %v35083, %v35084 (stack75)
        %v35086 = vxor.u32 %v35081, %v35085 (stack76)
        %v35089 = vadd.s32 %v35081, %v35086 (stack65)
        %v35093 = vadd.s32 %v35089, %v9 (stack65)
        %v35095 = vshll.u32 %v35086, 6 (stack73)
        %v35096 = vshrl.u32 %v35086, 26 (stack74)
        %v35097 = vor.u32 %v35095, %v35096 (stack75)
        %v35098 = vxor.u32 %v35089, %v35097 (stack76)
        %v35101 = vadd.s32 %v35098, %v8 (stack65)
        %v35105 = vadd.s32 %v35101, 1 (stack65)
        %v35109 = vadd.s32 %v35093, %v35105 (stack65)
        %v35111 = vshll.u32 %v35105, 17 (stack73)
        %v35112 = vshrl.u32 %v35105, 15 (stack74)
        %v35113 = vor.u32 %v35111, %v35112 (stack75)
        %v35114 = vxor.u32 %v35109, %v35113 (stack76)
        %v35117 = vadd.s32 %v35109, %v35114 (stack65)
        %v35119 = vshll.u32 %v35114, 29 (stack73)
        %v35120 = vshrl.u32 %v35114, 3 (stack74)
        %v35121 = vor.u32 %v35119, %v35120 (stack75)
        %v35122 = vxor.u32 %v35117, %v35121 (stack76)
        %v35125 = vadd.s32 %v35117, %v35122 (stack65)
        %v35127 = vshll.u32 %v35122, 16 (stack73)
        %v35128 = vshrl.u32 %v35122, 16 (stack74)
        %v35129 = vor.u32 %v35127, %v35128 (stack75)
        %v35130 = vxor.u32 %v35125, %v35129 (stack76)
        %v35133 = vadd.s32 %v35125, %v35130 (stack65)
        %v35137 = vadd.s32 %v35133, %v8 (stack65)
        %v35139 = vshll.u32 %v35130, 24 (stack73)
        %v35140 = vshrl.u32 %v35130, 8 (stack74)
        %v35141 = vor.u32 %v35139, %v35140 (stack75)
        %v35142 = vxor.u32 %v35133, %v35141 (stack76)
        %v35145 = vadd.s32 %v35142, %v10 (stack65)
        %v35149 = vadd.s32 %v35145, 2 (stack65)
        %v35153 = vadd.s32 %v35137, %v35149 (stack65)
        %v35155 = vshll.u32 %v35149, 13 (stack73)
        %v35156 = vshrl.u32 %v35149, 19 (stack74)
        %v35157 = vor.u32 %v35155, %v35156 (stack75)
        %v35158 = vxor.u32 %v35153, %v35157 (stack76)
        %v35161 = vadd.s32 %v35153, %v35158 (stack65)
        %v35163 = vshll.u32 %v35158, 15 (stack73)
        %v35164 = vshrl.u32 %v35158, 17 (stack74)
        %v35165 = vor.u32 %v35163, %v35164 (stack75)
        %v35166 = vxor.u32 %v35161, %v35165 (stack76)
        %v35169 = vadd.s32 %v35161, %v35166 (stack65)
        %v35171 = vshll.u32 %v35166, 26 (stack73)
        %v35172 = vshrl.u32 %v35166, 6 (stack74)
        %v35173 = vor.u32 %v35171, %v35172 (stack75)
        %v35174 = vxor.u32 %v35169, %v35173 (stack76)
        %v35177 = vadd.s32 %v35169, %v35174 (stack65)
        %v35181 = vadd.s32 %v35177, %v10 (stack65)
        %v35183 = vshll.u32 %v35174, 6 (stack73)
        %v35184 = vshrl.u32 %v35174, 26 (stack74)
        %v35185 = vor.u32 %v35183, %v35184 (stack75)
        %v35186 = vxor.u32 %v35177, %v35185 (stack76)
        %v35189 = vadd.s32 %v35186, %v9 (stack65)
        %v35193 = vadd.s32 %v35189, 3 (stack65)
        %v35197 = vadd.s32 %v35181, %v35193 (stack65)
        %v35199 = vshll.u32 %v35193, 17 (stack73)
        %v35200 = vshrl.u32 %v35193, 15 (stack74)
        %v35201 = vor.u32 %v35199, %v35200 (stack75)
        %v35202 = vxor.u32 %v35197, %v35201 (stack76)
        %v35205 = vadd.s32 %v35197, %v35202 (stack65)
        %v35207 = vshll.u32 %v35202, 29 (stack73)
        %v35208 = vshrl.u32 %v35202, 3 (stack74)
        %v35209 = vor.u32 %v35207, %v35208 (stack75)
        %v35210 = vxor.u32 %v35205, %v35209 (stack76)
        %v35213 = vadd.s32 %v35205, %v35210 (stack65)
        %v35215 = vshll.u32 %v35210, 16 (stack73)
        %v35216 = vshrl.u32 %v35210, 16 (stack74)
        %v35217 = vor.u32 %v35215, %v35216 (stack75)
        %v35218 = vxor.u32 %v35213, %v35217 (stack76)
        %v35221 = vadd.s32 %v35213, %v35218 (stack65)
        %v35225 = vadd.s32 %v35221, %v9 (stack65)
        %v35227 = vshll.u32 %v35218, 24 (stack73)
        %v35228 = vshrl.u32 %v35218, 8 (stack74)
        %v35229 = vor.u32 %v35227, %v35228 (stack75)
        %v35230 = vxor.u32 %v35221, %v35229 (stack76)
        %v35233 = vadd.s32 %v35230, %v8 (stack65)
        %v35237 = vadd.s32 %v35233, 4 (stack65)
        %v35241 = vadd.s32 %v35225, %v35237 (stack65)
        %v35243 = vshll.u32 %v35237, 13 (stack73)
        %v35244 = vshrl.u32 %v35237, 19 (stack74)
        %v35245 = vor.u32 %v35243, %v35244 (stack75)
        %v35246 = vxor.u32 %v35241, %v35245 (stack76)
        %v35249 = vadd.s32 %v35241, %v35246 (stack65)
        %v35251 = vshll.u32 %v35246, 15 (stack73)
        %v35252 = vshrl.u32 %v35246, 17 (stack74)
        %v35253 = vor.u32 %v35251, %v35252 (stack75)
        %v35254 = vxor.u32 %v35249, %v35253 (stack76)
        %v35257 = vadd.s32 %v35249, %v35254 (stack65)
        %v35259 = vshll.u32 %v35254, 26 (stack73)
        %v35260 = vshrl.u32 %v35254, 6 (stack74)
        %v35261 = vor.u32 %v35259, %v35260 (stack75)
        %v35262 = vxor.u32 %v35257, %v35261 (stack76)
        %v35265 = vadd.s32 %v35257, %v35262 (stack65)
        %v35269 = vadd.s32 %v35265, %v8 (stack65)
        %v35271 = vshll.u32 %v35262, 6 (stack73)
        %v35272 = vshrl.u32 %v35262, 26 (stack74)
        %v35273 = vor.u32 %v35271, %v35272 (stack75)
        %v35274 = vxor.u32 %v35265, %v35273 (stack76)
        %v35277 = vadd.s32 %v35274, %v10 (stack65)
        %v35281 = vadd.s32 %v35277, 5 (stack65)
        %v35283 = vxor.u32 %v35269, %v35281 (stack76)
        %v35284 = vand.u32.u8 %v35283, 255 (stack77)
        %v35285 = vand.u32 %v35284, 65535 (stack78)
        %v35286 = vshrl.u32 %v35285, 1 (stack79)
        %v35287 = vor.u32 %v35286, 16256 (stack75)
        %v35288 = vand.u32.u16 %v35287, 65535 (stack80)
        %v35289 = vunpack.i.l.bf16 %v35288 (stack81)
        %v35293 = vadd.f32 %v35289, -1.0 (stack82)
        %v35297 = vmul.f32 %v35293, 2.0 (stack83)
        %v35301 = vadd.f32 %v35297, -0.99609375 (stack82)
        %v35305 = vmax.f32 -0.99609375, %v35301 (stack84)
        %v35307 = vand.u32 2147483647, %v35305 (stack85)
        %vm35310 = vcmp.eq.f32.partialorder %v35307, 1.0 (stack86)
        %v35315 = vmul.f32 %v35305, inf (stack83)
        %v35317 = vxor.u32 %v35305, 2147483648 (stack87)
        %v35320 = vmul.f32 %v35305, %v35317 (stack83)
        %v35322 = vadd.f32 %v35320, 1.0 (stack88)
        %v35323 = vlog2.pop %v35322 (stack89)
        %v35324 = vmul.f32 %v35323, 0.6931472 (stack90)
        %v35325 = vmul.f32 -0.5, %v35320 (stack91)
        %v35326 = vadd.f32 %v35325, 1.0 (stack92)
        %v35327 = vmul.f32 %v35326, %v35320 (stack93)
        %v35328 = vand.u32 2147483647, %v35320 (stack94)
        %vm35329 = vcmp.lt.f32.partialorder %v35328, 0.0004427343 (stack95)
        %v35330 = vsel /*vm=*/%vm35329, /*on_true_vy=*/%v35327, /*on_false_vx=*/%v35324 (stack96)
        %v35331 = vxor.u32 %v35330, 2147483648 (stack87)
        %vm35334 = vcmp.lt.f32.partialorder %v35331, 5.0 (stack86)
        %v35339 = vsel /*vm=*/%vm35334, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v35343 = vsel /*vm=*/%vm35334, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v35347 = vsel /*vm=*/%vm35334, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v35351 = vsel /*vm=*/%vm35334, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v35355 = vsel /*vm=*/%vm35334, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v35359 = vsel /*vm=*/%vm35334, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v35363 = vsel /*vm=*/%vm35334, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v35367 = vsel /*vm=*/%vm35334, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v35371 = vsel /*vm=*/%vm35334, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v35375 = vadd.f32 %v35331, -2.5 (stack82)
        %v35377 = vrsqrt.pop %v35331 (stack97)
        %v35378 = vmul.f32 %v35331, %v35377 (stack98)
        %vm35379 = vcmp.eq.f32.partialorder %v35331, inf (stack99)
        %v35380 = vsel /*vm=*/%vm35379, /*on_true_vy=*/%v35331, /*on_false_vx=*/%v35378 (stack100)
        %vm35381 = vcmp.eq.f32.partialorder %v35331, 0.0 (stack101)
        %v35382 = vand.u32 %v35331, 2147483648 (stack102)
        %v35383 = vsel /*vm=*/%vm35381, /*on_true_vy=*/%v35382, /*on_false_vx=*/%v35380 (stack103)
        %v35386 = vadd.f32 %v35383, -3.0 (stack82)
        %v35390 = vsel /*vm=*/%vm35334, /*on_true_vy=*/%v35375, /*on_false_vx=*/%v35386 (stack72)
        %v35394 = vmul.f32 %v35371, %v35390 (stack83)
        %v35398 = vadd.f32 %v35367, %v35394 (stack82)
        %v35402 = vmul.f32 %v35398, %v35390 (stack83)
        %v35406 = vadd.f32 %v35363, %v35402 (stack82)
        %v35410 = vmul.f32 %v35406, %v35390 (stack83)
        %v35414 = vadd.f32 %v35359, %v35410 (stack82)
        %v35418 = vmul.f32 %v35414, %v35390 (stack83)
        %v35422 = vadd.f32 %v35355, %v35418 (stack82)
        %v35426 = vmul.f32 %v35422, %v35390 (stack83)
        %v35430 = vadd.f32 %v35351, %v35426 (stack82)
        %v35434 = vmul.f32 %v35430, %v35390 (stack83)
        %v35438 = vadd.f32 %v35347, %v35434 (stack82)
        %v35442 = vmul.f32 %v35438, %v35390 (stack83)
        %v35446 = vadd.f32 %v35343, %v35442 (stack82)
        %v35450 = vmul.f32 %v35446, %v35390 (stack83)
        %v35454 = vadd.f32 %v35339, %v35450 (stack82)
        %v35458 = vmul.f32 %v35454, %v35305 (stack83)
        %v35462 = vsel /*vm=*/%vm35310, /*on_true_vy=*/%v35315, /*on_false_vx=*/%v35458 (stack72)
        %v35466 = vmul.f32 %v35462, 1.4140625 (stack83)
        %s35468 = scalar_lea.vmem %s280, 292 [#allocation0] (stack107)
        %v35469 = vpack.c.bf16 0.0, %v35466 (stack104)
        %35470 = vst [vmem:[%s35468] sm:$0xf] /*vst_source=*/%v35469 (stack105)
        %v35473 = vadd.s32 %v1868, %v34087 (stack65)
        %s35475 = smul.u32 128, %s27 (stack66)
        %v35476 = vlaneseq (stack67)
        %v35477 = vand.u32 %v35476, 127 (stack68)
        %v35478 = vstv %s35475 (stack69)
        %v35479 = vadd.s32 %v35477, %v35478 (stack70)
        %v35483 = vadd.s32 %v35473, %v35479 (stack65)
        %vm35487 = vcmp.lt.u32.totalorder %v35483, %v35473 (stack71)
        %vm35492 = vcmp.lt.u32.totalorder %v35473, %v1868 (stack71)
        %v35497 = vadd.s32 %v1855, %v34070 (stack65)
        %v35501 = vadd.s32 %v35497, 1 (stack65)
        %v35505 = vsel /*vm=*/%vm35492, /*on_true_vy=*/%v35501, /*on_false_vx=*/%v35497 (stack72)
        %v35509 = vadd.s32 %v35505, 1 (stack65)
        %v35513 = vsel /*vm=*/%vm35487, /*on_true_vy=*/%v35509, /*on_false_vx=*/%v35505 (stack72)
        %v35518 = vadd.s32 %v35513, %v10 (stack65)
        %v35522 = vadd.s32 %v35483, %v9 (stack65)
        %v35526 = vadd.s32 %v35518, %v35522 (stack65)
        %v35528 = vshll.u32 %v35522, 13 (stack73)
        %v35529 = vshrl.u32 %v35522, 19 (stack74)
        %v35530 = vor.u32 %v35528, %v35529 (stack75)
        %v35531 = vxor.u32 %v35526, %v35530 (stack76)
        %v35534 = vadd.s32 %v35526, %v35531 (stack65)
        %v35536 = vshll.u32 %v35531, 15 (stack73)
        %v35537 = vshrl.u32 %v35531, 17 (stack74)
        %v35538 = vor.u32 %v35536, %v35537 (stack75)
        %v35539 = vxor.u32 %v35534, %v35538 (stack76)
        %v35542 = vadd.s32 %v35534, %v35539 (stack65)
        %v35544 = vshll.u32 %v35539, 26 (stack73)
        %v35545 = vshrl.u32 %v35539, 6 (stack74)
        %v35546 = vor.u32 %v35544, %v35545 (stack75)
        %v35547 = vxor.u32 %v35542, %v35546 (stack76)
        %v35550 = vadd.s32 %v35542, %v35547 (stack65)
        %v35554 = vadd.s32 %v35550, %v9 (stack65)
        %v35556 = vshll.u32 %v35547, 6 (stack73)
        %v35557 = vshrl.u32 %v35547, 26 (stack74)
        %v35558 = vor.u32 %v35556, %v35557 (stack75)
        %v35559 = vxor.u32 %v35550, %v35558 (stack76)
        %v35562 = vadd.s32 %v35559, %v8 (stack65)
        %v35566 = vadd.s32 %v35562, 1 (stack65)
        %v35570 = vadd.s32 %v35554, %v35566 (stack65)
        %v35572 = vshll.u32 %v35566, 17 (stack73)
        %v35573 = vshrl.u32 %v35566, 15 (stack74)
        %v35574 = vor.u32 %v35572, %v35573 (stack75)
        %v35575 = vxor.u32 %v35570, %v35574 (stack76)
        %v35578 = vadd.s32 %v35570, %v35575 (stack65)
        %v35580 = vshll.u32 %v35575, 29 (stack73)
        %v35581 = vshrl.u32 %v35575, 3 (stack74)
        %v35582 = vor.u32 %v35580, %v35581 (stack75)
        %v35583 = vxor.u32 %v35578, %v35582 (stack76)
        %v35586 = vadd.s32 %v35578, %v35583 (stack65)
        %v35588 = vshll.u32 %v35583, 16 (stack73)
        %v35589 = vshrl.u32 %v35583, 16 (stack74)
        %v35590 = vor.u32 %v35588, %v35589 (stack75)
        %v35591 = vxor.u32 %v35586, %v35590 (stack76)
        %v35594 = vadd.s32 %v35586, %v35591 (stack65)
        %v35598 = vadd.s32 %v35594, %v8 (stack65)
        %v35600 = vshll.u32 %v35591, 24 (stack73)
        %v35601 = vshrl.u32 %v35591, 8 (stack74)
        %v35602 = vor.u32 %v35600, %v35601 (stack75)
        %v35603 = vxor.u32 %v35594, %v35602 (stack76)
        %v35606 = vadd.s32 %v35603, %v10 (stack65)
        %v35610 = vadd.s32 %v35606, 2 (stack65)
        %v35614 = vadd.s32 %v35598, %v35610 (stack65)
        %v35616 = vshll.u32 %v35610, 13 (stack73)
        %v35617 = vshrl.u32 %v35610, 19 (stack74)
        %v35618 = vor.u32 %v35616, %v35617 (stack75)
        %v35619 = vxor.u32 %v35614, %v35618 (stack76)
        %v35622 = vadd.s32 %v35614, %v35619 (stack65)
        %v35624 = vshll.u32 %v35619, 15 (stack73)
        %v35625 = vshrl.u32 %v35619, 17 (stack74)
        %v35626 = vor.u32 %v35624, %v35625 (stack75)
        %v35627 = vxor.u32 %v35622, %v35626 (stack76)
        %v35630 = vadd.s32 %v35622, %v35627 (stack65)
        %v35632 = vshll.u32 %v35627, 26 (stack73)
        %v35633 = vshrl.u32 %v35627, 6 (stack74)
        %v35634 = vor.u32 %v35632, %v35633 (stack75)
        %v35635 = vxor.u32 %v35630, %v35634 (stack76)
        %v35638 = vadd.s32 %v35630, %v35635 (stack65)
        %v35642 = vadd.s32 %v35638, %v10 (stack65)
        %v35644 = vshll.u32 %v35635, 6 (stack73)
        %v35645 = vshrl.u32 %v35635, 26 (stack74)
        %v35646 = vor.u32 %v35644, %v35645 (stack75)
        %v35647 = vxor.u32 %v35638, %v35646 (stack76)
        %v35650 = vadd.s32 %v35647, %v9 (stack65)
        %v35654 = vadd.s32 %v35650, 3 (stack65)
        %v35658 = vadd.s32 %v35642, %v35654 (stack65)
        %v35660 = vshll.u32 %v35654, 17 (stack73)
        %v35661 = vshrl.u32 %v35654, 15 (stack74)
        %v35662 = vor.u32 %v35660, %v35661 (stack75)
        %v35663 = vxor.u32 %v35658, %v35662 (stack76)
        %v35666 = vadd.s32 %v35658, %v35663 (stack65)
        %v35668 = vshll.u32 %v35663, 29 (stack73)
        %v35669 = vshrl.u32 %v35663, 3 (stack74)
        %v35670 = vor.u32 %v35668, %v35669 (stack75)
        %v35671 = vxor.u32 %v35666, %v35670 (stack76)
        %v35674 = vadd.s32 %v35666, %v35671 (stack65)
        %v35676 = vshll.u32 %v35671, 16 (stack73)
        %v35677 = vshrl.u32 %v35671, 16 (stack74)
        %v35678 = vor.u32 %v35676, %v35677 (stack75)
        %v35679 = vxor.u32 %v35674, %v35678 (stack76)
        %v35682 = vadd.s32 %v35674, %v35679 (stack65)
        %v35686 = vadd.s32 %v35682, %v9 (stack65)
        %v35688 = vshll.u32 %v35679, 24 (stack73)
        %v35689 = vshrl.u32 %v35679, 8 (stack74)
        %v35690 = vor.u32 %v35688, %v35689 (stack75)
        %v35691 = vxor.u32 %v35682, %v35690 (stack76)
        %v35694 = vadd.s32 %v35691, %v8 (stack65)
        %v35698 = vadd.s32 %v35694, 4 (stack65)
        %v35702 = vadd.s32 %v35686, %v35698 (stack65)
        %v35704 = vshll.u32 %v35698, 13 (stack73)
        %v35705 = vshrl.u32 %v35698, 19 (stack74)
        %v35706 = vor.u32 %v35704, %v35705 (stack75)
        %v35707 = vxor.u32 %v35702, %v35706 (stack76)
        %v35710 = vadd.s32 %v35702, %v35707 (stack65)
        %v35712 = vshll.u32 %v35707, 15 (stack73)
        %v35713 = vshrl.u32 %v35707, 17 (stack74)
        %v35714 = vor.u32 %v35712, %v35713 (stack75)
        %v35715 = vxor.u32 %v35710, %v35714 (stack76)
        %v35718 = vadd.s32 %v35710, %v35715 (stack65)
        %v35720 = vshll.u32 %v35715, 26 (stack73)
        %v35721 = vshrl.u32 %v35715, 6 (stack74)
        %v35722 = vor.u32 %v35720, %v35721 (stack75)
        %v35723 = vxor.u32 %v35718, %v35722 (stack76)
        %v35726 = vadd.s32 %v35718, %v35723 (stack65)
        %v35730 = vadd.s32 %v35726, %v8 (stack65)
        %v35732 = vshll.u32 %v35723, 6 (stack73)
        %v35733 = vshrl.u32 %v35723, 26 (stack74)
        %v35734 = vor.u32 %v35732, %v35733 (stack75)
        %v35735 = vxor.u32 %v35726, %v35734 (stack76)
        %v35738 = vadd.s32 %v35735, %v10 (stack65)
        %v35742 = vadd.s32 %v35738, 5 (stack65)
        %v35744 = vxor.u32 %v35730, %v35742 (stack76)
        %v35745 = vand.u32.u8 %v35744, 255 (stack77)
        %v35746 = vand.u32 %v35745, 65535 (stack78)
        %v35747 = vshrl.u32 %v35746, 1 (stack79)
        %v35748 = vor.u32 %v35747, 16256 (stack75)
        %v35749 = vand.u32.u16 %v35748, 65535 (stack80)
        %v35750 = vunpack.i.l.bf16 %v35749 (stack81)
        %v35754 = vadd.f32 %v35750, -1.0 (stack82)
        %v35758 = vmul.f32 %v35754, 2.0 (stack83)
        %v35762 = vadd.f32 %v35758, -0.99609375 (stack82)
        %v35766 = vmax.f32 -0.99609375, %v35762 (stack84)
        %v35768 = vand.u32 2147483647, %v35766 (stack85)
        %vm35771 = vcmp.eq.f32.partialorder %v35768, 1.0 (stack86)
        %v35776 = vmul.f32 %v35766, inf (stack83)
        %v35778 = vxor.u32 %v35766, 2147483648 (stack87)
        %v35781 = vmul.f32 %v35766, %v35778 (stack83)
        %v35783 = vadd.f32 %v35781, 1.0 (stack88)
        %v35784 = vlog2.pop %v35783 (stack89)
        %v35785 = vmul.f32 %v35784, 0.6931472 (stack90)
        %v35786 = vmul.f32 -0.5, %v35781 (stack91)
        %v35787 = vadd.f32 %v35786, 1.0 (stack92)
        %v35788 = vmul.f32 %v35787, %v35781 (stack93)
        %v35789 = vand.u32 2147483647, %v35781 (stack94)
        %vm35790 = vcmp.lt.f32.partialorder %v35789, 0.0004427343 (stack95)
        %v35791 = vsel /*vm=*/%vm35790, /*on_true_vy=*/%v35788, /*on_false_vx=*/%v35785 (stack96)
        %v35792 = vxor.u32 %v35791, 2147483648 (stack87)
        %vm35795 = vcmp.lt.f32.partialorder %v35792, 5.0 (stack86)
        %v35800 = vsel /*vm=*/%vm35795, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v35804 = vsel /*vm=*/%vm35795, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v35808 = vsel /*vm=*/%vm35795, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v35812 = vsel /*vm=*/%vm35795, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v35816 = vsel /*vm=*/%vm35795, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v35820 = vsel /*vm=*/%vm35795, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v35824 = vsel /*vm=*/%vm35795, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v35828 = vsel /*vm=*/%vm35795, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v35832 = vsel /*vm=*/%vm35795, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v35836 = vadd.f32 %v35792, -2.5 (stack82)
        %v35838 = vrsqrt.pop %v35792 (stack97)
        %v35839 = vmul.f32 %v35792, %v35838 (stack98)
        %vm35840 = vcmp.eq.f32.partialorder %v35792, inf (stack99)
        %v35841 = vsel /*vm=*/%vm35840, /*on_true_vy=*/%v35792, /*on_false_vx=*/%v35839 (stack100)
        %vm35842 = vcmp.eq.f32.partialorder %v35792, 0.0 (stack101)
        %v35843 = vand.u32 %v35792, 2147483648 (stack102)
        %v35844 = vsel /*vm=*/%vm35842, /*on_true_vy=*/%v35843, /*on_false_vx=*/%v35841 (stack103)
        %v35847 = vadd.f32 %v35844, -3.0 (stack82)
        %v35851 = vsel /*vm=*/%vm35795, /*on_true_vy=*/%v35836, /*on_false_vx=*/%v35847 (stack72)
        %v35855 = vmul.f32 %v35832, %v35851 (stack83)
        %v35859 = vadd.f32 %v35828, %v35855 (stack82)
        %v35863 = vmul.f32 %v35859, %v35851 (stack83)
        %v35867 = vadd.f32 %v35824, %v35863 (stack82)
        %v35871 = vmul.f32 %v35867, %v35851 (stack83)
        %v35875 = vadd.f32 %v35820, %v35871 (stack82)
        %v35879 = vmul.f32 %v35875, %v35851 (stack83)
        %v35883 = vadd.f32 %v35816, %v35879 (stack82)
        %v35887 = vmul.f32 %v35883, %v35851 (stack83)
        %v35891 = vadd.f32 %v35812, %v35887 (stack82)
        %v35895 = vmul.f32 %v35891, %v35851 (stack83)
        %v35899 = vadd.f32 %v35808, %v35895 (stack82)
        %v35903 = vmul.f32 %v35899, %v35851 (stack83)
        %v35907 = vadd.f32 %v35804, %v35903 (stack82)
        %v35911 = vmul.f32 %v35907, %v35851 (stack83)
        %v35915 = vadd.f32 %v35800, %v35911 (stack82)
        %v35919 = vmul.f32 %v35915, %v35766 (stack83)
        %v35923 = vsel /*vm=*/%vm35771, /*on_true_vy=*/%v35776, /*on_false_vx=*/%v35919 (stack72)
        %v35927 = vmul.f32 %v35923, 1.4140625 (stack83)
        %s35929 = scalar_lea.vmem %s280, 420 [#allocation0] (stack107)
        %v35930 = vpack.c.bf16 0.0, %v35927 (stack104)
        %35931 = vst [vmem:[%s35929] sm:$0xf] /*vst_source=*/%v35930 (stack105)
        %v35934 = vadd.s32 %v2355, %v34087 (stack65)
        %s35936 = smul.u32 128, %s27 (stack66)
        %v35937 = vlaneseq (stack67)
        %v35938 = vand.u32 %v35937, 127 (stack68)
        %v35939 = vstv %s35936 (stack69)
        %v35940 = vadd.s32 %v35938, %v35939 (stack70)
        %v35944 = vadd.s32 %v35934, %v35940 (stack65)
        %vm35948 = vcmp.lt.u32.totalorder %v35944, %v35934 (stack71)
        %vm35953 = vcmp.lt.u32.totalorder %v35934, %v2355 (stack71)
        %v35958 = vadd.s32 %v2342, %v34070 (stack65)
        %v35962 = vadd.s32 %v35958, 1 (stack65)
        %v35966 = vsel /*vm=*/%vm35953, /*on_true_vy=*/%v35962, /*on_false_vx=*/%v35958 (stack72)
        %v35970 = vadd.s32 %v35966, 1 (stack65)
        %v35974 = vsel /*vm=*/%vm35948, /*on_true_vy=*/%v35970, /*on_false_vx=*/%v35966 (stack72)
        %v35979 = vadd.s32 %v35974, %v10 (stack65)
        %v35983 = vadd.s32 %v35944, %v9 (stack65)
        %v35987 = vadd.s32 %v35979, %v35983 (stack65)
        %v35989 = vshll.u32 %v35983, 13 (stack73)
        %v35990 = vshrl.u32 %v35983, 19 (stack74)
        %v35991 = vor.u32 %v35989, %v35990 (stack75)
        %v35992 = vxor.u32 %v35987, %v35991 (stack76)
        %v35995 = vadd.s32 %v35987, %v35992 (stack65)
        %v35997 = vshll.u32 %v35992, 15 (stack73)
        %v35998 = vshrl.u32 %v35992, 17 (stack74)
        %v35999 = vor.u32 %v35997, %v35998 (stack75)
        %v36000 = vxor.u32 %v35995, %v35999 (stack76)
        %v36003 = vadd.s32 %v35995, %v36000 (stack65)
        %v36005 = vshll.u32 %v36000, 26 (stack73)
        %v36006 = vshrl.u32 %v36000, 6 (stack74)
        %v36007 = vor.u32 %v36005, %v36006 (stack75)
        %v36008 = vxor.u32 %v36003, %v36007 (stack76)
        %v36011 = vadd.s32 %v36003, %v36008 (stack65)
        %v36015 = vadd.s32 %v36011, %v9 (stack65)
        %v36017 = vshll.u32 %v36008, 6 (stack73)
        %v36018 = vshrl.u32 %v36008, 26 (stack74)
        %v36019 = vor.u32 %v36017, %v36018 (stack75)
        %v36020 = vxor.u32 %v36011, %v36019 (stack76)
        %v36023 = vadd.s32 %v36020, %v8 (stack65)
        %v36027 = vadd.s32 %v36023, 1 (stack65)
        %v36031 = vadd.s32 %v36015, %v36027 (stack65)
        %v36033 = vshll.u32 %v36027, 17 (stack73)
        %v36034 = vshrl.u32 %v36027, 15 (stack74)
        %v36035 = vor.u32 %v36033, %v36034 (stack75)
        %v36036 = vxor.u32 %v36031, %v36035 (stack76)
        %v36039 = vadd.s32 %v36031, %v36036 (stack65)
        %v36041 = vshll.u32 %v36036, 29 (stack73)
        %v36042 = vshrl.u32 %v36036, 3 (stack74)
        %v36043 = vor.u32 %v36041, %v36042 (stack75)
        %v36044 = vxor.u32 %v36039, %v36043 (stack76)
        %v36047 = vadd.s32 %v36039, %v36044 (stack65)
        %v36049 = vshll.u32 %v36044, 16 (stack73)
        %v36050 = vshrl.u32 %v36044, 16 (stack74)
        %v36051 = vor.u32 %v36049, %v36050 (stack75)
        %v36052 = vxor.u32 %v36047, %v36051 (stack76)
        %v36055 = vadd.s32 %v36047, %v36052 (stack65)
        %v36059 = vadd.s32 %v36055, %v8 (stack65)
        %v36061 = vshll.u32 %v36052, 24 (stack73)
        %v36062 = vshrl.u32 %v36052, 8 (stack74)
        %v36063 = vor.u32 %v36061, %v36062 (stack75)
        %v36064 = vxor.u32 %v36055, %v36063 (stack76)
        %v36067 = vadd.s32 %v36064, %v10 (stack65)
        %v36071 = vadd.s32 %v36067, 2 (stack65)
        %v36075 = vadd.s32 %v36059, %v36071 (stack65)
        %v36077 = vshll.u32 %v36071, 13 (stack73)
        %v36078 = vshrl.u32 %v36071, 19 (stack74)
        %v36079 = vor.u32 %v36077, %v36078 (stack75)
        %v36080 = vxor.u32 %v36075, %v36079 (stack76)
        %v36083 = vadd.s32 %v36075, %v36080 (stack65)
        %v36085 = vshll.u32 %v36080, 15 (stack73)
        %v36086 = vshrl.u32 %v36080, 17 (stack74)
        %v36087 = vor.u32 %v36085, %v36086 (stack75)
        %v36088 = vxor.u32 %v36083, %v36087 (stack76)
        %v36091 = vadd.s32 %v36083, %v36088 (stack65)
        %v36093 = vshll.u32 %v36088, 26 (stack73)
        %v36094 = vshrl.u32 %v36088, 6 (stack74)
        %v36095 = vor.u32 %v36093, %v36094 (stack75)
        %v36096 = vxor.u32 %v36091, %v36095 (stack76)
        %v36099 = vadd.s32 %v36091, %v36096 (stack65)
        %v36103 = vadd.s32 %v36099, %v10 (stack65)
        %v36105 = vshll.u32 %v36096, 6 (stack73)
        %v36106 = vshrl.u32 %v36096, 26 (stack74)
        %v36107 = vor.u32 %v36105, %v36106 (stack75)
        %v36108 = vxor.u32 %v36099, %v36107 (stack76)
        %v36111 = vadd.s32 %v36108, %v9 (stack65)
        %v36115 = vadd.s32 %v36111, 3 (stack65)
        %v36119 = vadd.s32 %v36103, %v36115 (stack65)
        %v36121 = vshll.u32 %v36115, 17 (stack73)
        %v36122 = vshrl.u32 %v36115, 15 (stack74)
        %v36123 = vor.u32 %v36121, %v36122 (stack75)
        %v36124 = vxor.u32 %v36119, %v36123 (stack76)
        %v36127 = vadd.s32 %v36119, %v36124 (stack65)
        %v36129 = vshll.u32 %v36124, 29 (stack73)
        %v36130 = vshrl.u32 %v36124, 3 (stack74)
        %v36131 = vor.u32 %v36129, %v36130 (stack75)
        %v36132 = vxor.u32 %v36127, %v36131 (stack76)
        %v36135 = vadd.s32 %v36127, %v36132 (stack65)
        %v36137 = vshll.u32 %v36132, 16 (stack73)
        %v36138 = vshrl.u32 %v36132, 16 (stack74)
        %v36139 = vor.u32 %v36137, %v36138 (stack75)
        %v36140 = vxor.u32 %v36135, %v36139 (stack76)
        %v36143 = vadd.s32 %v36135, %v36140 (stack65)
        %v36147 = vadd.s32 %v36143, %v9 (stack65)
        %v36149 = vshll.u32 %v36140, 24 (stack73)
        %v36150 = vshrl.u32 %v36140, 8 (stack74)
        %v36151 = vor.u32 %v36149, %v36150 (stack75)
        %v36152 = vxor.u32 %v36143, %v36151 (stack76)
        %v36155 = vadd.s32 %v36152, %v8 (stack65)
        %v36159 = vadd.s32 %v36155, 4 (stack65)
        %v36163 = vadd.s32 %v36147, %v36159 (stack65)
        %v36165 = vshll.u32 %v36159, 13 (stack73)
        %v36166 = vshrl.u32 %v36159, 19 (stack74)
        %v36167 = vor.u32 %v36165, %v36166 (stack75)
        %v36168 = vxor.u32 %v36163, %v36167 (stack76)
        %v36171 = vadd.s32 %v36163, %v36168 (stack65)
        %v36173 = vshll.u32 %v36168, 15 (stack73)
        %v36174 = vshrl.u32 %v36168, 17 (stack74)
        %v36175 = vor.u32 %v36173, %v36174 (stack75)
        %v36176 = vxor.u32 %v36171, %v36175 (stack76)
        %v36179 = vadd.s32 %v36171, %v36176 (stack65)
        %v36181 = vshll.u32 %v36176, 26 (stack73)
        %v36182 = vshrl.u32 %v36176, 6 (stack74)
        %v36183 = vor.u32 %v36181, %v36182 (stack75)
        %v36184 = vxor.u32 %v36179, %v36183 (stack76)
        %v36187 = vadd.s32 %v36179, %v36184 (stack65)
        %v36191 = vadd.s32 %v36187, %v8 (stack65)
        %v36193 = vshll.u32 %v36184, 6 (stack73)
        %v36194 = vshrl.u32 %v36184, 26 (stack74)
        %v36195 = vor.u32 %v36193, %v36194 (stack75)
        %v36196 = vxor.u32 %v36187, %v36195 (stack76)
        %v36199 = vadd.s32 %v36196, %v10 (stack65)
        %v36203 = vadd.s32 %v36199, 5 (stack65)
        %v36205 = vxor.u32 %v36191, %v36203 (stack76)
        %v36206 = vand.u32.u8 %v36205, 255 (stack77)
        %v36207 = vand.u32 %v36206, 65535 (stack78)
        %v36208 = vshrl.u32 %v36207, 1 (stack79)
        %v36209 = vor.u32 %v36208, 16256 (stack75)
        %v36210 = vand.u32.u16 %v36209, 65535 (stack80)
        %v36211 = vunpack.i.l.bf16 %v36210 (stack81)
        %v36215 = vadd.f32 %v36211, -1.0 (stack82)
        %v36219 = vmul.f32 %v36215, 2.0 (stack83)
        %v36223 = vadd.f32 %v36219, -0.99609375 (stack82)
        %v36227 = vmax.f32 -0.99609375, %v36223 (stack84)
        %v36229 = vand.u32 2147483647, %v36227 (stack85)
        %vm36232 = vcmp.eq.f32.partialorder %v36229, 1.0 (stack86)
        %v36237 = vmul.f32 %v36227, inf (stack83)
        %v36239 = vxor.u32 %v36227, 2147483648 (stack87)
        %v36242 = vmul.f32 %v36227, %v36239 (stack83)
        %v36244 = vadd.f32 %v36242, 1.0 (stack88)
        %v36245 = vlog2.pop %v36244 (stack89)
        %v36246 = vmul.f32 %v36245, 0.6931472 (stack90)
        %v36247 = vmul.f32 -0.5, %v36242 (stack91)
        %v36248 = vadd.f32 %v36247, 1.0 (stack92)
        %v36249 = vmul.f32 %v36248, %v36242 (stack93)
        %v36250 = vand.u32 2147483647, %v36242 (stack94)
        %vm36251 = vcmp.lt.f32.partialorder %v36250, 0.0004427343 (stack95)
        %v36252 = vsel /*vm=*/%vm36251, /*on_true_vy=*/%v36249, /*on_false_vx=*/%v36246 (stack96)
        %v36253 = vxor.u32 %v36252, 2147483648 (stack87)
        %vm36256 = vcmp.lt.f32.partialorder %v36253, 5.0 (stack86)
        %v36261 = vsel /*vm=*/%vm36256, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v36265 = vsel /*vm=*/%vm36256, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v36269 = vsel /*vm=*/%vm36256, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v36273 = vsel /*vm=*/%vm36256, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v36277 = vsel /*vm=*/%vm36256, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v36281 = vsel /*vm=*/%vm36256, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v36285 = vsel /*vm=*/%vm36256, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v36289 = vsel /*vm=*/%vm36256, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v36293 = vsel /*vm=*/%vm36256, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v36297 = vadd.f32 %v36253, -2.5 (stack82)
        %v36299 = vrsqrt.pop %v36253 (stack97)
        %v36300 = vmul.f32 %v36253, %v36299 (stack98)
        %vm36301 = vcmp.eq.f32.partialorder %v36253, inf (stack99)
        %v36302 = vsel /*vm=*/%vm36301, /*on_true_vy=*/%v36253, /*on_false_vx=*/%v36300 (stack100)
        %vm36303 = vcmp.eq.f32.partialorder %v36253, 0.0 (stack101)
        %v36304 = vand.u32 %v36253, 2147483648 (stack102)
        %v36305 = vsel /*vm=*/%vm36303, /*on_true_vy=*/%v36304, /*on_false_vx=*/%v36302 (stack103)
        %v36308 = vadd.f32 %v36305, -3.0 (stack82)
        %v36312 = vsel /*vm=*/%vm36256, /*on_true_vy=*/%v36297, /*on_false_vx=*/%v36308 (stack72)
        %v36316 = vmul.f32 %v36293, %v36312 (stack83)
        %v36320 = vadd.f32 %v36289, %v36316 (stack82)
        %v36324 = vmul.f32 %v36320, %v36312 (stack83)
        %v36328 = vadd.f32 %v36285, %v36324 (stack82)
        %v36332 = vmul.f32 %v36328, %v36312 (stack83)
        %v36336 = vadd.f32 %v36281, %v36332 (stack82)
        %v36340 = vmul.f32 %v36336, %v36312 (stack83)
        %v36344 = vadd.f32 %v36277, %v36340 (stack82)
        %v36348 = vmul.f32 %v36344, %v36312 (stack83)
        %v36352 = vadd.f32 %v36273, %v36348 (stack82)
        %v36356 = vmul.f32 %v36352, %v36312 (stack83)
        %v36360 = vadd.f32 %v36269, %v36356 (stack82)
        %v36364 = vmul.f32 %v36360, %v36312 (stack83)
        %v36368 = vadd.f32 %v36265, %v36364 (stack82)
        %v36372 = vmul.f32 %v36368, %v36312 (stack83)
        %v36376 = vadd.f32 %v36261, %v36372 (stack82)
        %v36380 = vmul.f32 %v36376, %v36227 (stack83)
        %v36384 = vsel /*vm=*/%vm36232, /*on_true_vy=*/%v36237, /*on_false_vx=*/%v36380 (stack72)
        %v36388 = vmul.f32 %v36384, 1.4140625 (stack83)
        %s36390 = scalar_lea.vmem %s280, 548 [#allocation0] (stack107)
        %v36391 = vpack.c.bf16 0.0, %v36388 (stack104)
        %36392 = vst [vmem:[%s36390] sm:$0xf] /*vst_source=*/%v36391 (stack105)
        %v36395 = vadd.s32 %v2842, %v34087 (stack65)
        %s36397 = smul.u32 128, %s27 (stack66)
        %v36398 = vlaneseq (stack67)
        %v36399 = vand.u32 %v36398, 127 (stack68)
        %v36400 = vstv %s36397 (stack69)
        %v36401 = vadd.s32 %v36399, %v36400 (stack70)
        %v36405 = vadd.s32 %v36395, %v36401 (stack65)
        %vm36409 = vcmp.lt.u32.totalorder %v36405, %v36395 (stack71)
        %vm36414 = vcmp.lt.u32.totalorder %v36395, %v2842 (stack71)
        %v36419 = vadd.s32 %v2829, %v34070 (stack65)
        %v36423 = vadd.s32 %v36419, 1 (stack65)
        %v36427 = vsel /*vm=*/%vm36414, /*on_true_vy=*/%v36423, /*on_false_vx=*/%v36419 (stack72)
        %v36431 = vadd.s32 %v36427, 1 (stack65)
        %v36435 = vsel /*vm=*/%vm36409, /*on_true_vy=*/%v36431, /*on_false_vx=*/%v36427 (stack72)
        %v36440 = vadd.s32 %v36435, %v10 (stack65)
        %v36444 = vadd.s32 %v36405, %v9 (stack65)
        %v36448 = vadd.s32 %v36440, %v36444 (stack65)
        %v36450 = vshll.u32 %v36444, 13 (stack73)
        %v36451 = vshrl.u32 %v36444, 19 (stack74)
        %v36452 = vor.u32 %v36450, %v36451 (stack75)
        %v36453 = vxor.u32 %v36448, %v36452 (stack76)
        %v36456 = vadd.s32 %v36448, %v36453 (stack65)
        %v36458 = vshll.u32 %v36453, 15 (stack73)
        %v36459 = vshrl.u32 %v36453, 17 (stack74)
        %v36460 = vor.u32 %v36458, %v36459 (stack75)
        %v36461 = vxor.u32 %v36456, %v36460 (stack76)
        %v36464 = vadd.s32 %v36456, %v36461 (stack65)
        %v36466 = vshll.u32 %v36461, 26 (stack73)
        %v36467 = vshrl.u32 %v36461, 6 (stack74)
        %v36468 = vor.u32 %v36466, %v36467 (stack75)
        %v36469 = vxor.u32 %v36464, %v36468 (stack76)
        %v36472 = vadd.s32 %v36464, %v36469 (stack65)
        %v36476 = vadd.s32 %v36472, %v9 (stack65)
        %v36478 = vshll.u32 %v36469, 6 (stack73)
        %v36479 = vshrl.u32 %v36469, 26 (stack74)
        %v36480 = vor.u32 %v36478, %v36479 (stack75)
        %v36481 = vxor.u32 %v36472, %v36480 (stack76)
        %v36484 = vadd.s32 %v36481, %v8 (stack65)
        %v36488 = vadd.s32 %v36484, 1 (stack65)
        %v36492 = vadd.s32 %v36476, %v36488 (stack65)
        %v36494 = vshll.u32 %v36488, 17 (stack73)
        %v36495 = vshrl.u32 %v36488, 15 (stack74)
        %v36496 = vor.u32 %v36494, %v36495 (stack75)
        %v36497 = vxor.u32 %v36492, %v36496 (stack76)
        %v36500 = vadd.s32 %v36492, %v36497 (stack65)
        %v36502 = vshll.u32 %v36497, 29 (stack73)
        %v36503 = vshrl.u32 %v36497, 3 (stack74)
        %v36504 = vor.u32 %v36502, %v36503 (stack75)
        %v36505 = vxor.u32 %v36500, %v36504 (stack76)
        %v36508 = vadd.s32 %v36500, %v36505 (stack65)
        %v36510 = vshll.u32 %v36505, 16 (stack73)
        %v36511 = vshrl.u32 %v36505, 16 (stack74)
        %v36512 = vor.u32 %v36510, %v36511 (stack75)
        %v36513 = vxor.u32 %v36508, %v36512 (stack76)
        %v36516 = vadd.s32 %v36508, %v36513 (stack65)
        %v36520 = vadd.s32 %v36516, %v8 (stack65)
        %v36522 = vshll.u32 %v36513, 24 (stack73)
        %v36523 = vshrl.u32 %v36513, 8 (stack74)
        %v36524 = vor.u32 %v36522, %v36523 (stack75)
        %v36525 = vxor.u32 %v36516, %v36524 (stack76)
        %v36528 = vadd.s32 %v36525, %v10 (stack65)
        %v36532 = vadd.s32 %v36528, 2 (stack65)
        %v36536 = vadd.s32 %v36520, %v36532 (stack65)
        %v36538 = vshll.u32 %v36532, 13 (stack73)
        %v36539 = vshrl.u32 %v36532, 19 (stack74)
        %v36540 = vor.u32 %v36538, %v36539 (stack75)
        %v36541 = vxor.u32 %v36536, %v36540 (stack76)
        %v36544 = vadd.s32 %v36536, %v36541 (stack65)
        %v36546 = vshll.u32 %v36541, 15 (stack73)
        %v36547 = vshrl.u32 %v36541, 17 (stack74)
        %v36548 = vor.u32 %v36546, %v36547 (stack75)
        %v36549 = vxor.u32 %v36544, %v36548 (stack76)
        %v36552 = vadd.s32 %v36544, %v36549 (stack65)
        %v36554 = vshll.u32 %v36549, 26 (stack73)
        %v36555 = vshrl.u32 %v36549, 6 (stack74)
        %v36556 = vor.u32 %v36554, %v36555 (stack75)
        %v36557 = vxor.u32 %v36552, %v36556 (stack76)
        %v36560 = vadd.s32 %v36552, %v36557 (stack65)
        %v36564 = vadd.s32 %v36560, %v10 (stack65)
        %v36566 = vshll.u32 %v36557, 6 (stack73)
        %v36567 = vshrl.u32 %v36557, 26 (stack74)
        %v36568 = vor.u32 %v36566, %v36567 (stack75)
        %v36569 = vxor.u32 %v36560, %v36568 (stack76)
        %v36572 = vadd.s32 %v36569, %v9 (stack65)
        %v36576 = vadd.s32 %v36572, 3 (stack65)
        %v36580 = vadd.s32 %v36564, %v36576 (stack65)
        %v36582 = vshll.u32 %v36576, 17 (stack73)
        %v36583 = vshrl.u32 %v36576, 15 (stack74)
        %v36584 = vor.u32 %v36582, %v36583 (stack75)
        %v36585 = vxor.u32 %v36580, %v36584 (stack76)
        %v36588 = vadd.s32 %v36580, %v36585 (stack65)
        %v36590 = vshll.u32 %v36585, 29 (stack73)
        %v36591 = vshrl.u32 %v36585, 3 (stack74)
        %v36592 = vor.u32 %v36590, %v36591 (stack75)
        %v36593 = vxor.u32 %v36588, %v36592 (stack76)
        %v36596 = vadd.s32 %v36588, %v36593 (stack65)
        %v36598 = vshll.u32 %v36593, 16 (stack73)
        %v36599 = vshrl.u32 %v36593, 16 (stack74)
        %v36600 = vor.u32 %v36598, %v36599 (stack75)
        %v36601 = vxor.u32 %v36596, %v36600 (stack76)
        %v36604 = vadd.s32 %v36596, %v36601 (stack65)
        %v36608 = vadd.s32 %v36604, %v9 (stack65)
        %v36610 = vshll.u32 %v36601, 24 (stack73)
        %v36611 = vshrl.u32 %v36601, 8 (stack74)
        %v36612 = vor.u32 %v36610, %v36611 (stack75)
        %v36613 = vxor.u32 %v36604, %v36612 (stack76)
        %v36616 = vadd.s32 %v36613, %v8 (stack65)
        %v36620 = vadd.s32 %v36616, 4 (stack65)
        %v36624 = vadd.s32 %v36608, %v36620 (stack65)
        %v36626 = vshll.u32 %v36620, 13 (stack73)
        %v36627 = vshrl.u32 %v36620, 19 (stack74)
        %v36628 = vor.u32 %v36626, %v36627 (stack75)
        %v36629 = vxor.u32 %v36624, %v36628 (stack76)
        %v36632 = vadd.s32 %v36624, %v36629 (stack65)
        %v36634 = vshll.u32 %v36629, 15 (stack73)
        %v36635 = vshrl.u32 %v36629, 17 (stack74)
        %v36636 = vor.u32 %v36634, %v36635 (stack75)
        %v36637 = vxor.u32 %v36632, %v36636 (stack76)
        %v36640 = vadd.s32 %v36632, %v36637 (stack65)
        %v36642 = vshll.u32 %v36637, 26 (stack73)
        %v36643 = vshrl.u32 %v36637, 6 (stack74)
        %v36644 = vor.u32 %v36642, %v36643 (stack75)
        %v36645 = vxor.u32 %v36640, %v36644 (stack76)
        %v36648 = vadd.s32 %v36640, %v36645 (stack65)
        %v36652 = vadd.s32 %v36648, %v8 (stack65)
        %v36654 = vshll.u32 %v36645, 6 (stack73)
        %v36655 = vshrl.u32 %v36645, 26 (stack74)
        %v36656 = vor.u32 %v36654, %v36655 (stack75)
        %v36657 = vxor.u32 %v36648, %v36656 (stack76)
        %v36660 = vadd.s32 %v36657, %v10 (stack65)
        %v36664 = vadd.s32 %v36660, 5 (stack65)
        %v36666 = vxor.u32 %v36652, %v36664 (stack76)
        %v36667 = vand.u32.u8 %v36666, 255 (stack77)
        %v36668 = vand.u32 %v36667, 65535 (stack78)
        %v36669 = vshrl.u32 %v36668, 1 (stack79)
        %v36670 = vor.u32 %v36669, 16256 (stack75)
        %v36671 = vand.u32.u16 %v36670, 65535 (stack80)
        %v36672 = vunpack.i.l.bf16 %v36671 (stack81)
        %v36676 = vadd.f32 %v36672, -1.0 (stack82)
        %v36680 = vmul.f32 %v36676, 2.0 (stack83)
        %v36684 = vadd.f32 %v36680, -0.99609375 (stack82)
        %v36688 = vmax.f32 -0.99609375, %v36684 (stack84)
        %v36690 = vand.u32 2147483647, %v36688 (stack85)
        %vm36693 = vcmp.eq.f32.partialorder %v36690, 1.0 (stack86)
        %v36698 = vmul.f32 %v36688, inf (stack83)
        %v36700 = vxor.u32 %v36688, 2147483648 (stack87)
        %v36703 = vmul.f32 %v36688, %v36700 (stack83)
        %v36705 = vadd.f32 %v36703, 1.0 (stack88)
        %v36706 = vlog2.pop %v36705 (stack89)
        %v36707 = vmul.f32 %v36706, 0.6931472 (stack90)
        %v36708 = vmul.f32 -0.5, %v36703 (stack91)
        %v36709 = vadd.f32 %v36708, 1.0 (stack92)
        %v36710 = vmul.f32 %v36709, %v36703 (stack93)
        %v36711 = vand.u32 2147483647, %v36703 (stack94)
        %vm36712 = vcmp.lt.f32.partialorder %v36711, 0.0004427343 (stack95)
        %v36713 = vsel /*vm=*/%vm36712, /*on_true_vy=*/%v36710, /*on_false_vx=*/%v36707 (stack96)
        %v36714 = vxor.u32 %v36713, 2147483648 (stack87)
        %vm36717 = vcmp.lt.f32.partialorder %v36714, 5.0 (stack86)
        %v36722 = vsel /*vm=*/%vm36717, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v36726 = vsel /*vm=*/%vm36717, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v36730 = vsel /*vm=*/%vm36717, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v36734 = vsel /*vm=*/%vm36717, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v36738 = vsel /*vm=*/%vm36717, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v36742 = vsel /*vm=*/%vm36717, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v36746 = vsel /*vm=*/%vm36717, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v36750 = vsel /*vm=*/%vm36717, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v36754 = vsel /*vm=*/%vm36717, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v36758 = vadd.f32 %v36714, -2.5 (stack82)
        %v36760 = vrsqrt.pop %v36714 (stack97)
        %v36761 = vmul.f32 %v36714, %v36760 (stack98)
        %vm36762 = vcmp.eq.f32.partialorder %v36714, inf (stack99)
        %v36763 = vsel /*vm=*/%vm36762, /*on_true_vy=*/%v36714, /*on_false_vx=*/%v36761 (stack100)
        %vm36764 = vcmp.eq.f32.partialorder %v36714, 0.0 (stack101)
        %v36765 = vand.u32 %v36714, 2147483648 (stack102)
        %v36766 = vsel /*vm=*/%vm36764, /*on_true_vy=*/%v36765, /*on_false_vx=*/%v36763 (stack103)
        %v36769 = vadd.f32 %v36766, -3.0 (stack82)
        %v36773 = vsel /*vm=*/%vm36717, /*on_true_vy=*/%v36758, /*on_false_vx=*/%v36769 (stack72)
        %v36777 = vmul.f32 %v36754, %v36773 (stack83)
        %v36781 = vadd.f32 %v36750, %v36777 (stack82)
        %v36785 = vmul.f32 %v36781, %v36773 (stack83)
        %v36789 = vadd.f32 %v36746, %v36785 (stack82)
        %v36793 = vmul.f32 %v36789, %v36773 (stack83)
        %v36797 = vadd.f32 %v36742, %v36793 (stack82)
        %v36801 = vmul.f32 %v36797, %v36773 (stack83)
        %v36805 = vadd.f32 %v36738, %v36801 (stack82)
        %v36809 = vmul.f32 %v36805, %v36773 (stack83)
        %v36813 = vadd.f32 %v36734, %v36809 (stack82)
        %v36817 = vmul.f32 %v36813, %v36773 (stack83)
        %v36821 = vadd.f32 %v36730, %v36817 (stack82)
        %v36825 = vmul.f32 %v36821, %v36773 (stack83)
        %v36829 = vadd.f32 %v36726, %v36825 (stack82)
        %v36833 = vmul.f32 %v36829, %v36773 (stack83)
        %v36837 = vadd.f32 %v36722, %v36833 (stack82)
        %v36841 = vmul.f32 %v36837, %v36688 (stack83)
        %v36845 = vsel /*vm=*/%vm36693, /*on_true_vy=*/%v36698, /*on_false_vx=*/%v36841 (stack72)
        %v36849 = vmul.f32 %v36845, 1.4140625 (stack83)
        %s36851 = scalar_lea.vmem %s280, 676 [#allocation0] (stack107)
        %v36852 = vpack.c.bf16 0.0, %v36849 (stack104)
        %36853 = vst [vmem:[%s36851] sm:$0xf] /*vst_source=*/%v36852 (stack105)
        %v36856 = vadd.s32 %v3329, %v34087 (stack65)
        %s36858 = smul.u32 128, %s27 (stack66)
        %v36859 = vlaneseq (stack67)
        %v36860 = vand.u32 %v36859, 127 (stack68)
        %v36861 = vstv %s36858 (stack69)
        %v36862 = vadd.s32 %v36860, %v36861 (stack70)
        %v36866 = vadd.s32 %v36856, %v36862 (stack65)
        %vm36870 = vcmp.lt.u32.totalorder %v36866, %v36856 (stack71)
        %vm36875 = vcmp.lt.u32.totalorder %v36856, %v3329 (stack71)
        %v36880 = vadd.s32 %v3316, %v34070 (stack65)
        %v36884 = vadd.s32 %v36880, 1 (stack65)
        %v36888 = vsel /*vm=*/%vm36875, /*on_true_vy=*/%v36884, /*on_false_vx=*/%v36880 (stack72)
        %v36892 = vadd.s32 %v36888, 1 (stack65)
        %v36896 = vsel /*vm=*/%vm36870, /*on_true_vy=*/%v36892, /*on_false_vx=*/%v36888 (stack72)
        %v36901 = vadd.s32 %v36896, %v10 (stack65)
        %v36905 = vadd.s32 %v36866, %v9 (stack65)
        %v36909 = vadd.s32 %v36901, %v36905 (stack65)
        %v36911 = vshll.u32 %v36905, 13 (stack73)
        %v36912 = vshrl.u32 %v36905, 19 (stack74)
        %v36913 = vor.u32 %v36911, %v36912 (stack75)
        %v36914 = vxor.u32 %v36909, %v36913 (stack76)
        %v36917 = vadd.s32 %v36909, %v36914 (stack65)
        %v36919 = vshll.u32 %v36914, 15 (stack73)
        %v36920 = vshrl.u32 %v36914, 17 (stack74)
        %v36921 = vor.u32 %v36919, %v36920 (stack75)
        %v36922 = vxor.u32 %v36917, %v36921 (stack76)
        %v36925 = vadd.s32 %v36917, %v36922 (stack65)
        %v36927 = vshll.u32 %v36922, 26 (stack73)
        %v36928 = vshrl.u32 %v36922, 6 (stack74)
        %v36929 = vor.u32 %v36927, %v36928 (stack75)
        %v36930 = vxor.u32 %v36925, %v36929 (stack76)
        %v36933 = vadd.s32 %v36925, %v36930 (stack65)
        %v36937 = vadd.s32 %v36933, %v9 (stack65)
        %v36939 = vshll.u32 %v36930, 6 (stack73)
        %v36940 = vshrl.u32 %v36930, 26 (stack74)
        %v36941 = vor.u32 %v36939, %v36940 (stack75)
        %v36942 = vxor.u32 %v36933, %v36941 (stack76)
        %v36945 = vadd.s32 %v36942, %v8 (stack65)
        %v36949 = vadd.s32 %v36945, 1 (stack65)
        %v36953 = vadd.s32 %v36937, %v36949 (stack65)
        %v36955 = vshll.u32 %v36949, 17 (stack73)
        %v36956 = vshrl.u32 %v36949, 15 (stack74)
        %v36957 = vor.u32 %v36955, %v36956 (stack75)
        %v36958 = vxor.u32 %v36953, %v36957 (stack76)
        %v36961 = vadd.s32 %v36953, %v36958 (stack65)
        %v36963 = vshll.u32 %v36958, 29 (stack73)
        %v36964 = vshrl.u32 %v36958, 3 (stack74)
        %v36965 = vor.u32 %v36963, %v36964 (stack75)
        %v36966 = vxor.u32 %v36961, %v36965 (stack76)
        %v36969 = vadd.s32 %v36961, %v36966 (stack65)
        %v36971 = vshll.u32 %v36966, 16 (stack73)
        %v36972 = vshrl.u32 %v36966, 16 (stack74)
        %v36973 = vor.u32 %v36971, %v36972 (stack75)
        %v36974 = vxor.u32 %v36969, %v36973 (stack76)
        %v36977 = vadd.s32 %v36969, %v36974 (stack65)
        %v36981 = vadd.s32 %v36977, %v8 (stack65)
        %v36983 = vshll.u32 %v36974, 24 (stack73)
        %v36984 = vshrl.u32 %v36974, 8 (stack74)
        %v36985 = vor.u32 %v36983, %v36984 (stack75)
        %v36986 = vxor.u32 %v36977, %v36985 (stack76)
        %v36989 = vadd.s32 %v36986, %v10 (stack65)
        %v36993 = vadd.s32 %v36989, 2 (stack65)
        %v36997 = vadd.s32 %v36981, %v36993 (stack65)
        %v36999 = vshll.u32 %v36993, 13 (stack73)
        %v37000 = vshrl.u32 %v36993, 19 (stack74)
        %v37001 = vor.u32 %v36999, %v37000 (stack75)
        %v37002 = vxor.u32 %v36997, %v37001 (stack76)
        %v37005 = vadd.s32 %v36997, %v37002 (stack65)
        %v37007 = vshll.u32 %v37002, 15 (stack73)
        %v37008 = vshrl.u32 %v37002, 17 (stack74)
        %v37009 = vor.u32 %v37007, %v37008 (stack75)
        %v37010 = vxor.u32 %v37005, %v37009 (stack76)
        %v37013 = vadd.s32 %v37005, %v37010 (stack65)
        %v37015 = vshll.u32 %v37010, 26 (stack73)
        %v37016 = vshrl.u32 %v37010, 6 (stack74)
        %v37017 = vor.u32 %v37015, %v37016 (stack75)
        %v37018 = vxor.u32 %v37013, %v37017 (stack76)
        %v37021 = vadd.s32 %v37013, %v37018 (stack65)
        %v37025 = vadd.s32 %v37021, %v10 (stack65)
        %v37027 = vshll.u32 %v37018, 6 (stack73)
        %v37028 = vshrl.u32 %v37018, 26 (stack74)
        %v37029 = vor.u32 %v37027, %v37028 (stack75)
        %v37030 = vxor.u32 %v37021, %v37029 (stack76)
        %v37033 = vadd.s32 %v37030, %v9 (stack65)
        %v37037 = vadd.s32 %v37033, 3 (stack65)
        %v37041 = vadd.s32 %v37025, %v37037 (stack65)
        %v37043 = vshll.u32 %v37037, 17 (stack73)
        %v37044 = vshrl.u32 %v37037, 15 (stack74)
        %v37045 = vor.u32 %v37043, %v37044 (stack75)
        %v37046 = vxor.u32 %v37041, %v37045 (stack76)
        %v37049 = vadd.s32 %v37041, %v37046 (stack65)
        %v37051 = vshll.u32 %v37046, 29 (stack73)
        %v37052 = vshrl.u32 %v37046, 3 (stack74)
        %v37053 = vor.u32 %v37051, %v37052 (stack75)
        %v37054 = vxor.u32 %v37049, %v37053 (stack76)
        %v37057 = vadd.s32 %v37049, %v37054 (stack65)
        %v37059 = vshll.u32 %v37054, 16 (stack73)
        %v37060 = vshrl.u32 %v37054, 16 (stack74)
        %v37061 = vor.u32 %v37059, %v37060 (stack75)
        %v37062 = vxor.u32 %v37057, %v37061 (stack76)
        %v37065 = vadd.s32 %v37057, %v37062 (stack65)
        %v37069 = vadd.s32 %v37065, %v9 (stack65)
        %v37071 = vshll.u32 %v37062, 24 (stack73)
        %v37072 = vshrl.u32 %v37062, 8 (stack74)
        %v37073 = vor.u32 %v37071, %v37072 (stack75)
        %v37074 = vxor.u32 %v37065, %v37073 (stack76)
        %v37077 = vadd.s32 %v37074, %v8 (stack65)
        %v37081 = vadd.s32 %v37077, 4 (stack65)
        %v37085 = vadd.s32 %v37069, %v37081 (stack65)
        %v37087 = vshll.u32 %v37081, 13 (stack73)
        %v37088 = vshrl.u32 %v37081, 19 (stack74)
        %v37089 = vor.u32 %v37087, %v37088 (stack75)
        %v37090 = vxor.u32 %v37085, %v37089 (stack76)
        %v37093 = vadd.s32 %v37085, %v37090 (stack65)
        %v37095 = vshll.u32 %v37090, 15 (stack73)
        %v37096 = vshrl.u32 %v37090, 17 (stack74)
        %v37097 = vor.u32 %v37095, %v37096 (stack75)
        %v37098 = vxor.u32 %v37093, %v37097 (stack76)
        %v37101 = vadd.s32 %v37093, %v37098 (stack65)
        %v37103 = vshll.u32 %v37098, 26 (stack73)
        %v37104 = vshrl.u32 %v37098, 6 (stack74)
        %v37105 = vor.u32 %v37103, %v37104 (stack75)
        %v37106 = vxor.u32 %v37101, %v37105 (stack76)
        %v37109 = vadd.s32 %v37101, %v37106 (stack65)
        %v37113 = vadd.s32 %v37109, %v8 (stack65)
        %v37115 = vshll.u32 %v37106, 6 (stack73)
        %v37116 = vshrl.u32 %v37106, 26 (stack74)
        %v37117 = vor.u32 %v37115, %v37116 (stack75)
        %v37118 = vxor.u32 %v37109, %v37117 (stack76)
        %v37121 = vadd.s32 %v37118, %v10 (stack65)
        %v37125 = vadd.s32 %v37121, 5 (stack65)
        %v37127 = vxor.u32 %v37113, %v37125 (stack76)
        %v37128 = vand.u32.u8 %v37127, 255 (stack77)
        %v37129 = vand.u32 %v37128, 65535 (stack78)
        %v37130 = vshrl.u32 %v37129, 1 (stack79)
        %v37131 = vor.u32 %v37130, 16256 (stack75)
        %v37132 = vand.u32.u16 %v37131, 65535 (stack80)
        %v37133 = vunpack.i.l.bf16 %v37132 (stack81)
        %v37137 = vadd.f32 %v37133, -1.0 (stack82)
        %v37141 = vmul.f32 %v37137, 2.0 (stack83)
        %v37145 = vadd.f32 %v37141, -0.99609375 (stack82)
        %v37149 = vmax.f32 -0.99609375, %v37145 (stack84)
        %v37151 = vand.u32 2147483647, %v37149 (stack85)
        %vm37154 = vcmp.eq.f32.partialorder %v37151, 1.0 (stack86)
        %v37159 = vmul.f32 %v37149, inf (stack83)
        %v37161 = vxor.u32 %v37149, 2147483648 (stack87)
        %v37164 = vmul.f32 %v37149, %v37161 (stack83)
        %v37166 = vadd.f32 %v37164, 1.0 (stack88)
        %v37167 = vlog2.pop %v37166 (stack89)
        %v37168 = vmul.f32 %v37167, 0.6931472 (stack90)
        %v37169 = vmul.f32 -0.5, %v37164 (stack91)
        %v37170 = vadd.f32 %v37169, 1.0 (stack92)
        %v37171 = vmul.f32 %v37170, %v37164 (stack93)
        %v37172 = vand.u32 2147483647, %v37164 (stack94)
        %vm37173 = vcmp.lt.f32.partialorder %v37172, 0.0004427343 (stack95)
        %v37174 = vsel /*vm=*/%vm37173, /*on_true_vy=*/%v37171, /*on_false_vx=*/%v37168 (stack96)
        %v37175 = vxor.u32 %v37174, 2147483648 (stack87)
        %vm37178 = vcmp.lt.f32.partialorder %v37175, 5.0 (stack86)
        %v37183 = vsel /*vm=*/%vm37178, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v37187 = vsel /*vm=*/%vm37178, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v37191 = vsel /*vm=*/%vm37178, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v37195 = vsel /*vm=*/%vm37178, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v37199 = vsel /*vm=*/%vm37178, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v37203 = vsel /*vm=*/%vm37178, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v37207 = vsel /*vm=*/%vm37178, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v37211 = vsel /*vm=*/%vm37178, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v37215 = vsel /*vm=*/%vm37178, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v37219 = vadd.f32 %v37175, -2.5 (stack82)
        %v37221 = vrsqrt.pop %v37175 (stack97)
        %v37222 = vmul.f32 %v37175, %v37221 (stack98)
        %vm37223 = vcmp.eq.f32.partialorder %v37175, inf (stack99)
        %v37224 = vsel /*vm=*/%vm37223, /*on_true_vy=*/%v37175, /*on_false_vx=*/%v37222 (stack100)
        %vm37225 = vcmp.eq.f32.partialorder %v37175, 0.0 (stack101)
        %v37226 = vand.u32 %v37175, 2147483648 (stack102)
        %v37227 = vsel /*vm=*/%vm37225, /*on_true_vy=*/%v37226, /*on_false_vx=*/%v37224 (stack103)
        %v37230 = vadd.f32 %v37227, -3.0 (stack82)
        %v37234 = vsel /*vm=*/%vm37178, /*on_true_vy=*/%v37219, /*on_false_vx=*/%v37230 (stack72)
        %v37238 = vmul.f32 %v37215, %v37234 (stack83)
        %v37242 = vadd.f32 %v37211, %v37238 (stack82)
        %v37246 = vmul.f32 %v37242, %v37234 (stack83)
        %v37250 = vadd.f32 %v37207, %v37246 (stack82)
        %v37254 = vmul.f32 %v37250, %v37234 (stack83)
        %v37258 = vadd.f32 %v37203, %v37254 (stack82)
        %v37262 = vmul.f32 %v37258, %v37234 (stack83)
        %v37266 = vadd.f32 %v37199, %v37262 (stack82)
        %v37270 = vmul.f32 %v37266, %v37234 (stack83)
        %v37274 = vadd.f32 %v37195, %v37270 (stack82)
        %v37278 = vmul.f32 %v37274, %v37234 (stack83)
        %v37282 = vadd.f32 %v37191, %v37278 (stack82)
        %v37286 = vmul.f32 %v37282, %v37234 (stack83)
        %v37290 = vadd.f32 %v37187, %v37286 (stack82)
        %v37294 = vmul.f32 %v37290, %v37234 (stack83)
        %v37298 = vadd.f32 %v37183, %v37294 (stack82)
        %v37302 = vmul.f32 %v37298, %v37149 (stack83)
        %v37306 = vsel /*vm=*/%vm37154, /*on_true_vy=*/%v37159, /*on_false_vx=*/%v37302 (stack72)
        %v37310 = vmul.f32 %v37306, 1.4140625 (stack83)
        %s37312 = scalar_lea.vmem %s280, 804 [#allocation0] (stack107)
        %v37313 = vpack.c.bf16 0.0, %v37310 (stack104)
        %37314 = vst [vmem:[%s37312] sm:$0xf] /*vst_source=*/%v37313 (stack105)
        %v37317 = vadd.s32 %v3816, %v34087 (stack65)
        %s37319 = smul.u32 128, %s27 (stack66)
        %v37320 = vlaneseq (stack67)
        %v37321 = vand.u32 %v37320, 127 (stack68)
        %v37322 = vstv %s37319 (stack69)
        %v37323 = vadd.s32 %v37321, %v37322 (stack70)
        %v37327 = vadd.s32 %v37317, %v37323 (stack65)
        %vm37331 = vcmp.lt.u32.totalorder %v37327, %v37317 (stack71)
        %vm37336 = vcmp.lt.u32.totalorder %v37317, %v3816 (stack71)
        %v37341 = vadd.s32 %v3803, %v34070 (stack65)
        %v37345 = vadd.s32 %v37341, 1 (stack65)
        %v37349 = vsel /*vm=*/%vm37336, /*on_true_vy=*/%v37345, /*on_false_vx=*/%v37341 (stack72)
        %v37353 = vadd.s32 %v37349, 1 (stack65)
        %v37357 = vsel /*vm=*/%vm37331, /*on_true_vy=*/%v37353, /*on_false_vx=*/%v37349 (stack72)
        %v37362 = vadd.s32 %v37357, %v10 (stack65)
        %v37366 = vadd.s32 %v37327, %v9 (stack65)
        %v37370 = vadd.s32 %v37362, %v37366 (stack65)
        %v37372 = vshll.u32 %v37366, 13 (stack73)
        %v37373 = vshrl.u32 %v37366, 19 (stack74)
        %v37374 = vor.u32 %v37372, %v37373 (stack75)
        %v37375 = vxor.u32 %v37370, %v37374 (stack76)
        %v37378 = vadd.s32 %v37370, %v37375 (stack65)
        %v37380 = vshll.u32 %v37375, 15 (stack73)
        %v37381 = vshrl.u32 %v37375, 17 (stack74)
        %v37382 = vor.u32 %v37380, %v37381 (stack75)
        %v37383 = vxor.u32 %v37378, %v37382 (stack76)
        %v37386 = vadd.s32 %v37378, %v37383 (stack65)
        %v37388 = vshll.u32 %v37383, 26 (stack73)
        %v37389 = vshrl.u32 %v37383, 6 (stack74)
        %v37390 = vor.u32 %v37388, %v37389 (stack75)
        %v37391 = vxor.u32 %v37386, %v37390 (stack76)
        %v37394 = vadd.s32 %v37386, %v37391 (stack65)
        %v37398 = vadd.s32 %v37394, %v9 (stack65)
        %v37400 = vshll.u32 %v37391, 6 (stack73)
        %v37401 = vshrl.u32 %v37391, 26 (stack74)
        %v37402 = vor.u32 %v37400, %v37401 (stack75)
        %v37403 = vxor.u32 %v37394, %v37402 (stack76)
        %v37406 = vadd.s32 %v37403, %v8 (stack65)
        %v37410 = vadd.s32 %v37406, 1 (stack65)
        %v37414 = vadd.s32 %v37398, %v37410 (stack65)
        %v37416 = vshll.u32 %v37410, 17 (stack73)
        %v37417 = vshrl.u32 %v37410, 15 (stack74)
        %v37418 = vor.u32 %v37416, %v37417 (stack75)
        %v37419 = vxor.u32 %v37414, %v37418 (stack76)
        %v37422 = vadd.s32 %v37414, %v37419 (stack65)
        %v37424 = vshll.u32 %v37419, 29 (stack73)
        %v37425 = vshrl.u32 %v37419, 3 (stack74)
        %v37426 = vor.u32 %v37424, %v37425 (stack75)
        %v37427 = vxor.u32 %v37422, %v37426 (stack76)
        %v37430 = vadd.s32 %v37422, %v37427 (stack65)
        %v37432 = vshll.u32 %v37427, 16 (stack73)
        %v37433 = vshrl.u32 %v37427, 16 (stack74)
        %v37434 = vor.u32 %v37432, %v37433 (stack75)
        %v37435 = vxor.u32 %v37430, %v37434 (stack76)
        %v37438 = vadd.s32 %v37430, %v37435 (stack65)
        %v37442 = vadd.s32 %v37438, %v8 (stack65)
        %v37444 = vshll.u32 %v37435, 24 (stack73)
        %v37445 = vshrl.u32 %v37435, 8 (stack74)
        %v37446 = vor.u32 %v37444, %v37445 (stack75)
        %v37447 = vxor.u32 %v37438, %v37446 (stack76)
        %v37450 = vadd.s32 %v37447, %v10 (stack65)
        %v37454 = vadd.s32 %v37450, 2 (stack65)
        %v37458 = vadd.s32 %v37442, %v37454 (stack65)
        %v37460 = vshll.u32 %v37454, 13 (stack73)
        %v37461 = vshrl.u32 %v37454, 19 (stack74)
        %v37462 = vor.u32 %v37460, %v37461 (stack75)
        %v37463 = vxor.u32 %v37458, %v37462 (stack76)
        %v37466 = vadd.s32 %v37458, %v37463 (stack65)
        %v37468 = vshll.u32 %v37463, 15 (stack73)
        %v37469 = vshrl.u32 %v37463, 17 (stack74)
        %v37470 = vor.u32 %v37468, %v37469 (stack75)
        %v37471 = vxor.u32 %v37466, %v37470 (stack76)
        %v37474 = vadd.s32 %v37466, %v37471 (stack65)
        %v37476 = vshll.u32 %v37471, 26 (stack73)
        %v37477 = vshrl.u32 %v37471, 6 (stack74)
        %v37478 = vor.u32 %v37476, %v37477 (stack75)
        %v37479 = vxor.u32 %v37474, %v37478 (stack76)
        %v37482 = vadd.s32 %v37474, %v37479 (stack65)
        %v37486 = vadd.s32 %v37482, %v10 (stack65)
        %v37488 = vshll.u32 %v37479, 6 (stack73)
        %v37489 = vshrl.u32 %v37479, 26 (stack74)
        %v37490 = vor.u32 %v37488, %v37489 (stack75)
        %v37491 = vxor.u32 %v37482, %v37490 (stack76)
        %v37494 = vadd.s32 %v37491, %v9 (stack65)
        %v37498 = vadd.s32 %v37494, 3 (stack65)
        %v37502 = vadd.s32 %v37486, %v37498 (stack65)
        %v37504 = vshll.u32 %v37498, 17 (stack73)
        %v37505 = vshrl.u32 %v37498, 15 (stack74)
        %v37506 = vor.u32 %v37504, %v37505 (stack75)
        %v37507 = vxor.u32 %v37502, %v37506 (stack76)
        %v37510 = vadd.s32 %v37502, %v37507 (stack65)
        %v37512 = vshll.u32 %v37507, 29 (stack73)
        %v37513 = vshrl.u32 %v37507, 3 (stack74)
        %v37514 = vor.u32 %v37512, %v37513 (stack75)
        %v37515 = vxor.u32 %v37510, %v37514 (stack76)
        %v37518 = vadd.s32 %v37510, %v37515 (stack65)
        %v37520 = vshll.u32 %v37515, 16 (stack73)
        %v37521 = vshrl.u32 %v37515, 16 (stack74)
        %v37522 = vor.u32 %v37520, %v37521 (stack75)
        %v37523 = vxor.u32 %v37518, %v37522 (stack76)
        %v37526 = vadd.s32 %v37518, %v37523 (stack65)
        %v37530 = vadd.s32 %v37526, %v9 (stack65)
        %v37532 = vshll.u32 %v37523, 24 (stack73)
        %v37533 = vshrl.u32 %v37523, 8 (stack74)
        %v37534 = vor.u32 %v37532, %v37533 (stack75)
        %v37535 = vxor.u32 %v37526, %v37534 (stack76)
        %v37538 = vadd.s32 %v37535, %v8 (stack65)
        %v37542 = vadd.s32 %v37538, 4 (stack65)
        %v37546 = vadd.s32 %v37530, %v37542 (stack65)
        %v37548 = vshll.u32 %v37542, 13 (stack73)
        %v37549 = vshrl.u32 %v37542, 19 (stack74)
        %v37550 = vor.u32 %v37548, %v37549 (stack75)
        %v37551 = vxor.u32 %v37546, %v37550 (stack76)
        %v37554 = vadd.s32 %v37546, %v37551 (stack65)
        %v37556 = vshll.u32 %v37551, 15 (stack73)
        %v37557 = vshrl.u32 %v37551, 17 (stack74)
        %v37558 = vor.u32 %v37556, %v37557 (stack75)
        %v37559 = vxor.u32 %v37554, %v37558 (stack76)
        %v37562 = vadd.s32 %v37554, %v37559 (stack65)
        %v37564 = vshll.u32 %v37559, 26 (stack73)
        %v37565 = vshrl.u32 %v37559, 6 (stack74)
        %v37566 = vor.u32 %v37564, %v37565 (stack75)
        %v37567 = vxor.u32 %v37562, %v37566 (stack76)
        %v37570 = vadd.s32 %v37562, %v37567 (stack65)
        %v37574 = vadd.s32 %v37570, %v8 (stack65)
        %v37576 = vshll.u32 %v37567, 6 (stack73)
        %v37577 = vshrl.u32 %v37567, 26 (stack74)
        %v37578 = vor.u32 %v37576, %v37577 (stack75)
        %v37579 = vxor.u32 %v37570, %v37578 (stack76)
        %v37582 = vadd.s32 %v37579, %v10 (stack65)
        %v37586 = vadd.s32 %v37582, 5 (stack65)
        %v37588 = vxor.u32 %v37574, %v37586 (stack76)
        %v37589 = vand.u32.u8 %v37588, 255 (stack77)
        %v37590 = vand.u32 %v37589, 65535 (stack78)
        %v37591 = vshrl.u32 %v37590, 1 (stack79)
        %v37592 = vor.u32 %v37591, 16256 (stack75)
        %v37593 = vand.u32.u16 %v37592, 65535 (stack80)
        %v37594 = vunpack.i.l.bf16 %v37593 (stack81)
        %v37598 = vadd.f32 %v37594, -1.0 (stack82)
        %v37602 = vmul.f32 %v37598, 2.0 (stack83)
        %v37606 = vadd.f32 %v37602, -0.99609375 (stack82)
        %v37610 = vmax.f32 -0.99609375, %v37606 (stack84)
        %v37612 = vand.u32 2147483647, %v37610 (stack85)
        %vm37615 = vcmp.eq.f32.partialorder %v37612, 1.0 (stack86)
        %v37620 = vmul.f32 %v37610, inf (stack83)
        %v37622 = vxor.u32 %v37610, 2147483648 (stack87)
        %v37625 = vmul.f32 %v37610, %v37622 (stack83)
        %v37627 = vadd.f32 %v37625, 1.0 (stack88)
        %v37628 = vlog2.pop %v37627 (stack89)
        %v37629 = vmul.f32 %v37628, 0.6931472 (stack90)
        %v37630 = vmul.f32 -0.5, %v37625 (stack91)
        %v37631 = vadd.f32 %v37630, 1.0 (stack92)
        %v37632 = vmul.f32 %v37631, %v37625 (stack93)
        %v37633 = vand.u32 2147483647, %v37625 (stack94)
        %vm37634 = vcmp.lt.f32.partialorder %v37633, 0.0004427343 (stack95)
        %v37635 = vsel /*vm=*/%vm37634, /*on_true_vy=*/%v37632, /*on_false_vx=*/%v37629 (stack96)
        %v37636 = vxor.u32 %v37635, 2147483648 (stack87)
        %vm37639 = vcmp.lt.f32.partialorder %v37636, 5.0 (stack86)
        %v37644 = vsel /*vm=*/%vm37639, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v37648 = vsel /*vm=*/%vm37639, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v37652 = vsel /*vm=*/%vm37639, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v37656 = vsel /*vm=*/%vm37639, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v37660 = vsel /*vm=*/%vm37639, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v37664 = vsel /*vm=*/%vm37639, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v37668 = vsel /*vm=*/%vm37639, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v37672 = vsel /*vm=*/%vm37639, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v37676 = vsel /*vm=*/%vm37639, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v37680 = vadd.f32 %v37636, -2.5 (stack82)
        %v37682 = vrsqrt.pop %v37636 (stack97)
        %v37683 = vmul.f32 %v37636, %v37682 (stack98)
        %vm37684 = vcmp.eq.f32.partialorder %v37636, inf (stack99)
        %v37685 = vsel /*vm=*/%vm37684, /*on_true_vy=*/%v37636, /*on_false_vx=*/%v37683 (stack100)
        %vm37686 = vcmp.eq.f32.partialorder %v37636, 0.0 (stack101)
        %v37687 = vand.u32 %v37636, 2147483648 (stack102)
        %v37688 = vsel /*vm=*/%vm37686, /*on_true_vy=*/%v37687, /*on_false_vx=*/%v37685 (stack103)
        %v37691 = vadd.f32 %v37688, -3.0 (stack82)
        %v37695 = vsel /*vm=*/%vm37639, /*on_true_vy=*/%v37680, /*on_false_vx=*/%v37691 (stack72)
        %v37699 = vmul.f32 %v37676, %v37695 (stack83)
        %v37703 = vadd.f32 %v37672, %v37699 (stack82)
        %v37707 = vmul.f32 %v37703, %v37695 (stack83)
        %v37711 = vadd.f32 %v37668, %v37707 (stack82)
        %v37715 = vmul.f32 %v37711, %v37695 (stack83)
        %v37719 = vadd.f32 %v37664, %v37715 (stack82)
        %v37723 = vmul.f32 %v37719, %v37695 (stack83)
        %v37727 = vadd.f32 %v37660, %v37723 (stack82)
        %v37731 = vmul.f32 %v37727, %v37695 (stack83)
        %v37735 = vadd.f32 %v37656, %v37731 (stack82)
        %v37739 = vmul.f32 %v37735, %v37695 (stack83)
        %v37743 = vadd.f32 %v37652, %v37739 (stack82)
        %v37747 = vmul.f32 %v37743, %v37695 (stack83)
        %v37751 = vadd.f32 %v37648, %v37747 (stack82)
        %v37755 = vmul.f32 %v37751, %v37695 (stack83)
        %v37759 = vadd.f32 %v37644, %v37755 (stack82)
        %v37763 = vmul.f32 %v37759, %v37610 (stack83)
        %v37767 = vsel /*vm=*/%vm37615, /*on_true_vy=*/%v37620, /*on_false_vx=*/%v37763 (stack72)
        %v37771 = vmul.f32 %v37767, 1.4140625 (stack83)
        %s37773 = scalar_lea.vmem %s280, 932 [#allocation0] (stack107)
        %v37774 = vpack.c.bf16 0.0, %v37771 (stack104)
        %37775 = vst [vmem:[%s37773] sm:$0xf] /*vst_source=*/%v37774 (stack105)
        %s37776 = sadd.s32 %s339, 80 (stack106)
        %s37777 = sshrl.u32 %s37776, 10 (stack49)
        %p37778 = scmp.lt.s32.totalorder 1, %s37777 (stack50)
        %s37779 = scalar_select /*predicate=*/%p37778, /*on_true=*/1, /*on_false=*/%s37777 (stack51)
        %s37780 = sand.u32 %s37776, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s37781 = sshrl.u32 %s37780, 7 (stack53)
        %s37782 = sand.u32 %s37780, 127 /* smod.u32 w/div 128 */ (stack54)
        %s37783 = smul.addr %s37779, 8 (stack55)
        %s37784 = scalar_lea.vmem %s3, %s37783 (stack56)
        %s37786 = scalar_lea.vmem %s37784, %s37781 (stack57)
        %v37787 = vld [vmem:[%s37786] ss:$0 sm:$0xff] (stack58)
        %s37788 = sand.u32 %s37782, 255 (stack59)
        %s37790 = sor.u32 256, %s37788 (stack60)
        %37791 = vbcast.lane.b32.xlu0 %v37787, %s37790 (stack61)
        %v37792 = vpop.permute.xlu0 %37791 (stack62)
        %s37793 = sadd.s32 %s347, 80 (stack106)
        %s37794 = sshrl.u32 %s37793, 10 (stack49)
        %p37795 = scmp.lt.s32.totalorder 1, %s37794 (stack50)
        %s37796 = scalar_select /*predicate=*/%p37795, /*on_true=*/1, /*on_false=*/%s37794 (stack51)
        %s37797 = sand.u32 %s37793, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s37798 = sshrl.u32 %s37797, 7 (stack53)
        %s37799 = sand.u32 %s37797, 127 /* smod.u32 w/div 128 */ (stack54)
        %s37800 = smul.addr %s37796, 8 (stack55)
        %s37801 = scalar_lea.vmem %s5, %s37800 (stack56)
        %s37803 = scalar_lea.vmem %s37801, %s37798 (stack57)
        %v37804 = vld [vmem:[%s37803] ss:$0 sm:$0xff] (stack58)
        %s37805 = sand.u32 %s37799, 255 (stack59)
        %s37807 = sor.u32 256, %s37805 (stack60)
        %37808 = vbcast.lane.b32.xlu0 %v37804, %s37807 (stack61)
        %v37809 = vpop.permute.xlu0 %37808 (stack62)
        %v37812 = vadd.s32 %v408, %v37809 (stack65)
        %s37814 = smul.u32 128, %s27 (stack66)
        %v37815 = vlaneseq (stack67)
        %v37816 = vand.u32 %v37815, 127 (stack68)
        %v37817 = vstv %s37814 (stack69)
        %v37818 = vadd.s32 %v37816, %v37817 (stack70)
        %v37822 = vadd.s32 %v37812, %v37818 (stack65)
        %vm37826 = vcmp.lt.u32.totalorder %v37822, %v37812 (stack71)
        %vm37831 = vcmp.lt.u32.totalorder %v37812, %v408 (stack71)
        %v37836 = vadd.s32 %v380, %v37792 (stack65)
        %v37840 = vadd.s32 %v37836, 1 (stack65)
        %v37844 = vsel /*vm=*/%vm37831, /*on_true_vy=*/%v37840, /*on_false_vx=*/%v37836 (stack72)
        %v37848 = vadd.s32 %v37844, 1 (stack65)
        %v37852 = vsel /*vm=*/%vm37826, /*on_true_vy=*/%v37848, /*on_false_vx=*/%v37844 (stack72)
        %v37857 = vadd.s32 %v37852, %v10 (stack65)
        %v37861 = vadd.s32 %v37822, %v9 (stack65)
        %v37865 = vadd.s32 %v37857, %v37861 (stack65)
        %v37867 = vshll.u32 %v37861, 13 (stack73)
        %v37868 = vshrl.u32 %v37861, 19 (stack74)
        %v37869 = vor.u32 %v37867, %v37868 (stack75)
        %v37870 = vxor.u32 %v37865, %v37869 (stack76)
        %v37873 = vadd.s32 %v37865, %v37870 (stack65)
        %v37875 = vshll.u32 %v37870, 15 (stack73)
        %v37876 = vshrl.u32 %v37870, 17 (stack74)
        %v37877 = vor.u32 %v37875, %v37876 (stack75)
        %v37878 = vxor.u32 %v37873, %v37877 (stack76)
        %v37881 = vadd.s32 %v37873, %v37878 (stack65)
        %v37883 = vshll.u32 %v37878, 26 (stack73)
        %v37884 = vshrl.u32 %v37878, 6 (stack74)
        %v37885 = vor.u32 %v37883, %v37884 (stack75)
        %v37886 = vxor.u32 %v37881, %v37885 (stack76)
        %v37889 = vadd.s32 %v37881, %v37886 (stack65)
        %v37893 = vadd.s32 %v37889, %v9 (stack65)
        %v37895 = vshll.u32 %v37886, 6 (stack73)
        %v37896 = vshrl.u32 %v37886, 26 (stack74)
        %v37897 = vor.u32 %v37895, %v37896 (stack75)
        %v37898 = vxor.u32 %v37889, %v37897 (stack76)
        %v37901 = vadd.s32 %v37898, %v8 (stack65)
        %v37905 = vadd.s32 %v37901, 1 (stack65)
        %v37909 = vadd.s32 %v37893, %v37905 (stack65)
        %v37911 = vshll.u32 %v37905, 17 (stack73)
        %v37912 = vshrl.u32 %v37905, 15 (stack74)
        %v37913 = vor.u32 %v37911, %v37912 (stack75)
        %v37914 = vxor.u32 %v37909, %v37913 (stack76)
        %v37917 = vadd.s32 %v37909, %v37914 (stack65)
        %v37919 = vshll.u32 %v37914, 29 (stack73)
        %v37920 = vshrl.u32 %v37914, 3 (stack74)
        %v37921 = vor.u32 %v37919, %v37920 (stack75)
        %v37922 = vxor.u32 %v37917, %v37921 (stack76)
        %v37925 = vadd.s32 %v37917, %v37922 (stack65)
        %v37927 = vshll.u32 %v37922, 16 (stack73)
        %v37928 = vshrl.u32 %v37922, 16 (stack74)
        %v37929 = vor.u32 %v37927, %v37928 (stack75)
        %v37930 = vxor.u32 %v37925, %v37929 (stack76)
        %v37933 = vadd.s32 %v37925, %v37930 (stack65)
        %v37937 = vadd.s32 %v37933, %v8 (stack65)
        %v37939 = vshll.u32 %v37930, 24 (stack73)
        %v37940 = vshrl.u32 %v37930, 8 (stack74)
        %v37941 = vor.u32 %v37939, %v37940 (stack75)
        %v37942 = vxor.u32 %v37933, %v37941 (stack76)
        %v37945 = vadd.s32 %v37942, %v10 (stack65)
        %v37949 = vadd.s32 %v37945, 2 (stack65)
        %v37953 = vadd.s32 %v37937, %v37949 (stack65)
        %v37955 = vshll.u32 %v37949, 13 (stack73)
        %v37956 = vshrl.u32 %v37949, 19 (stack74)
        %v37957 = vor.u32 %v37955, %v37956 (stack75)
        %v37958 = vxor.u32 %v37953, %v37957 (stack76)
        %v37961 = vadd.s32 %v37953, %v37958 (stack65)
        %v37963 = vshll.u32 %v37958, 15 (stack73)
        %v37964 = vshrl.u32 %v37958, 17 (stack74)
        %v37965 = vor.u32 %v37963, %v37964 (stack75)
        %v37966 = vxor.u32 %v37961, %v37965 (stack76)
        %v37969 = vadd.s32 %v37961, %v37966 (stack65)
        %v37971 = vshll.u32 %v37966, 26 (stack73)
        %v37972 = vshrl.u32 %v37966, 6 (stack74)
        %v37973 = vor.u32 %v37971, %v37972 (stack75)
        %v37974 = vxor.u32 %v37969, %v37973 (stack76)
        %v37977 = vadd.s32 %v37969, %v37974 (stack65)
        %v37981 = vadd.s32 %v37977, %v10 (stack65)
        %v37983 = vshll.u32 %v37974, 6 (stack73)
        %v37984 = vshrl.u32 %v37974, 26 (stack74)
        %v37985 = vor.u32 %v37983, %v37984 (stack75)
        %v37986 = vxor.u32 %v37977, %v37985 (stack76)
        %v37989 = vadd.s32 %v37986, %v9 (stack65)
        %v37993 = vadd.s32 %v37989, 3 (stack65)
        %v37997 = vadd.s32 %v37981, %v37993 (stack65)
        %v37999 = vshll.u32 %v37993, 17 (stack73)
        %v38000 = vshrl.u32 %v37993, 15 (stack74)
        %v38001 = vor.u32 %v37999, %v38000 (stack75)
        %v38002 = vxor.u32 %v37997, %v38001 (stack76)
        %v38005 = vadd.s32 %v37997, %v38002 (stack65)
        %v38007 = vshll.u32 %v38002, 29 (stack73)
        %v38008 = vshrl.u32 %v38002, 3 (stack74)
        %v38009 = vor.u32 %v38007, %v38008 (stack75)
        %v38010 = vxor.u32 %v38005, %v38009 (stack76)
        %v38013 = vadd.s32 %v38005, %v38010 (stack65)
        %v38015 = vshll.u32 %v38010, 16 (stack73)
        %v38016 = vshrl.u32 %v38010, 16 (stack74)
        %v38017 = vor.u32 %v38015, %v38016 (stack75)
        %v38018 = vxor.u32 %v38013, %v38017 (stack76)
        %v38021 = vadd.s32 %v38013, %v38018 (stack65)
        %v38025 = vadd.s32 %v38021, %v9 (stack65)
        %v38027 = vshll.u32 %v38018, 24 (stack73)
        %v38028 = vshrl.u32 %v38018, 8 (stack74)
        %v38029 = vor.u32 %v38027, %v38028 (stack75)
        %v38030 = vxor.u32 %v38021, %v38029 (stack76)
        %v38033 = vadd.s32 %v38030, %v8 (stack65)
        %v38037 = vadd.s32 %v38033, 4 (stack65)
        %v38041 = vadd.s32 %v38025, %v38037 (stack65)
        %v38043 = vshll.u32 %v38037, 13 (stack73)
        %v38044 = vshrl.u32 %v38037, 19 (stack74)
        %v38045 = vor.u32 %v38043, %v38044 (stack75)
        %v38046 = vxor.u32 %v38041, %v38045 (stack76)
        %v38049 = vadd.s32 %v38041, %v38046 (stack65)
        %v38051 = vshll.u32 %v38046, 15 (stack73)
        %v38052 = vshrl.u32 %v38046, 17 (stack74)
        %v38053 = vor.u32 %v38051, %v38052 (stack75)
        %v38054 = vxor.u32 %v38049, %v38053 (stack76)
        %v38057 = vadd.s32 %v38049, %v38054 (stack65)
        %v38059 = vshll.u32 %v38054, 26 (stack73)
        %v38060 = vshrl.u32 %v38054, 6 (stack74)
        %v38061 = vor.u32 %v38059, %v38060 (stack75)
        %v38062 = vxor.u32 %v38057, %v38061 (stack76)
        %v38065 = vadd.s32 %v38057, %v38062 (stack65)
        %v38069 = vadd.s32 %v38065, %v8 (stack65)
        %v38071 = vshll.u32 %v38062, 6 (stack73)
        %v38072 = vshrl.u32 %v38062, 26 (stack74)
        %v38073 = vor.u32 %v38071, %v38072 (stack75)
        %v38074 = vxor.u32 %v38065, %v38073 (stack76)
        %v38077 = vadd.s32 %v38074, %v10 (stack65)
        %v38081 = vadd.s32 %v38077, 5 (stack65)
        %v38083 = vxor.u32 %v38069, %v38081 (stack76)
        %v38084 = vand.u32.u8 %v38083, 255 (stack77)
        %v38085 = vand.u32 %v38084, 65535 (stack78)
        %v38086 = vshrl.u32 %v38085, 1 (stack79)
        %v38087 = vor.u32 %v38086, 16256 (stack75)
        %v38088 = vand.u32.u16 %v38087, 65535 (stack80)
        %v38089 = vunpack.i.l.bf16 %v38088 (stack81)
        %v38093 = vadd.f32 %v38089, -1.0 (stack82)
        %v38097 = vmul.f32 %v38093, 2.0 (stack83)
        %v38101 = vadd.f32 %v38097, -0.99609375 (stack82)
        %v38105 = vmax.f32 -0.99609375, %v38101 (stack84)
        %v38107 = vand.u32 2147483647, %v38105 (stack85)
        %vm38110 = vcmp.eq.f32.partialorder %v38107, 1.0 (stack86)
        %v38115 = vmul.f32 %v38105, inf (stack83)
        %v38117 = vxor.u32 %v38105, 2147483648 (stack87)
        %v38120 = vmul.f32 %v38105, %v38117 (stack83)
        %v38122 = vadd.f32 %v38120, 1.0 (stack88)
        %v38123 = vlog2.pop %v38122 (stack89)
        %v38124 = vmul.f32 %v38123, 0.6931472 (stack90)
        %v38125 = vmul.f32 -0.5, %v38120 (stack91)
        %v38126 = vadd.f32 %v38125, 1.0 (stack92)
        %v38127 = vmul.f32 %v38126, %v38120 (stack93)
        %v38128 = vand.u32 2147483647, %v38120 (stack94)
        %vm38129 = vcmp.lt.f32.partialorder %v38128, 0.0004427343 (stack95)
        %v38130 = vsel /*vm=*/%vm38129, /*on_true_vy=*/%v38127, /*on_false_vx=*/%v38124 (stack96)
        %v38131 = vxor.u32 %v38130, 2147483648 (stack87)
        %vm38134 = vcmp.lt.f32.partialorder %v38131, 5.0 (stack86)
        %v38139 = vsel /*vm=*/%vm38134, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v38143 = vsel /*vm=*/%vm38134, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v38147 = vsel /*vm=*/%vm38134, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v38151 = vsel /*vm=*/%vm38134, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v38155 = vsel /*vm=*/%vm38134, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v38159 = vsel /*vm=*/%vm38134, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v38163 = vsel /*vm=*/%vm38134, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v38167 = vsel /*vm=*/%vm38134, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v38171 = vsel /*vm=*/%vm38134, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v38175 = vadd.f32 %v38131, -2.5 (stack82)
        %v38177 = vrsqrt.pop %v38131 (stack97)
        %v38178 = vmul.f32 %v38131, %v38177 (stack98)
        %vm38179 = vcmp.eq.f32.partialorder %v38131, inf (stack99)
        %v38180 = vsel /*vm=*/%vm38179, /*on_true_vy=*/%v38131, /*on_false_vx=*/%v38178 (stack100)
        %vm38181 = vcmp.eq.f32.partialorder %v38131, 0.0 (stack101)
        %v38182 = vand.u32 %v38131, 2147483648 (stack102)
        %v38183 = vsel /*vm=*/%vm38181, /*on_true_vy=*/%v38182, /*on_false_vx=*/%v38180 (stack103)
        %v38186 = vadd.f32 %v38183, -3.0 (stack82)
        %v38190 = vsel /*vm=*/%vm38134, /*on_true_vy=*/%v38175, /*on_false_vx=*/%v38186 (stack72)
        %v38194 = vmul.f32 %v38171, %v38190 (stack83)
        %v38198 = vadd.f32 %v38167, %v38194 (stack82)
        %v38202 = vmul.f32 %v38198, %v38190 (stack83)
        %v38206 = vadd.f32 %v38163, %v38202 (stack82)
        %v38210 = vmul.f32 %v38206, %v38190 (stack83)
        %v38214 = vadd.f32 %v38159, %v38210 (stack82)
        %v38218 = vmul.f32 %v38214, %v38190 (stack83)
        %v38222 = vadd.f32 %v38155, %v38218 (stack82)
        %v38226 = vmul.f32 %v38222, %v38190 (stack83)
        %v38230 = vadd.f32 %v38151, %v38226 (stack82)
        %v38234 = vmul.f32 %v38230, %v38190 (stack83)
        %v38238 = vadd.f32 %v38147, %v38234 (stack82)
        %v38242 = vmul.f32 %v38238, %v38190 (stack83)
        %v38246 = vadd.f32 %v38143, %v38242 (stack82)
        %v38250 = vmul.f32 %v38246, %v38190 (stack83)
        %v38254 = vadd.f32 %v38139, %v38250 (stack82)
        %v38258 = vmul.f32 %v38254, %v38105 (stack83)
        %v38262 = vsel /*vm=*/%vm38110, /*on_true_vy=*/%v38115, /*on_false_vx=*/%v38258 (stack72)
        %v38266 = vmul.f32 %v38262, 1.4140625 (stack83)
        %s38268 = scalar_lea.vmem %s280, 40 [#allocation0] (stack107)
        %v38269 = vpack.c.bf16 0.0, %v38266 (stack104)
        %38270 = vst [vmem:[%s38268] sm:$0xf] /*vst_source=*/%v38269 (stack105)
        %v38273 = vadd.s32 %v894, %v37809 (stack65)
        %s38275 = smul.u32 128, %s27 (stack66)
        %v38276 = vlaneseq (stack67)
        %v38277 = vand.u32 %v38276, 127 (stack68)
        %v38278 = vstv %s38275 (stack69)
        %v38279 = vadd.s32 %v38277, %v38278 (stack70)
        %v38283 = vadd.s32 %v38273, %v38279 (stack65)
        %vm38287 = vcmp.lt.u32.totalorder %v38283, %v38273 (stack71)
        %vm38292 = vcmp.lt.u32.totalorder %v38273, %v894 (stack71)
        %v38297 = vadd.s32 %v881, %v37792 (stack65)
        %v38301 = vadd.s32 %v38297, 1 (stack65)
        %v38305 = vsel /*vm=*/%vm38292, /*on_true_vy=*/%v38301, /*on_false_vx=*/%v38297 (stack72)
        %v38309 = vadd.s32 %v38305, 1 (stack65)
        %v38313 = vsel /*vm=*/%vm38287, /*on_true_vy=*/%v38309, /*on_false_vx=*/%v38305 (stack72)
        %v38318 = vadd.s32 %v38313, %v10 (stack65)
        %v38322 = vadd.s32 %v38283, %v9 (stack65)
        %v38326 = vadd.s32 %v38318, %v38322 (stack65)
        %v38328 = vshll.u32 %v38322, 13 (stack73)
        %v38329 = vshrl.u32 %v38322, 19 (stack74)
        %v38330 = vor.u32 %v38328, %v38329 (stack75)
        %v38331 = vxor.u32 %v38326, %v38330 (stack76)
        %v38334 = vadd.s32 %v38326, %v38331 (stack65)
        %v38336 = vshll.u32 %v38331, 15 (stack73)
        %v38337 = vshrl.u32 %v38331, 17 (stack74)
        %v38338 = vor.u32 %v38336, %v38337 (stack75)
        %v38339 = vxor.u32 %v38334, %v38338 (stack76)
        %v38342 = vadd.s32 %v38334, %v38339 (stack65)
        %v38344 = vshll.u32 %v38339, 26 (stack73)
        %v38345 = vshrl.u32 %v38339, 6 (stack74)
        %v38346 = vor.u32 %v38344, %v38345 (stack75)
        %v38347 = vxor.u32 %v38342, %v38346 (stack76)
        %v38350 = vadd.s32 %v38342, %v38347 (stack65)
        %v38354 = vadd.s32 %v38350, %v9 (stack65)
        %v38356 = vshll.u32 %v38347, 6 (stack73)
        %v38357 = vshrl.u32 %v38347, 26 (stack74)
        %v38358 = vor.u32 %v38356, %v38357 (stack75)
        %v38359 = vxor.u32 %v38350, %v38358 (stack76)
        %v38362 = vadd.s32 %v38359, %v8 (stack65)
        %v38366 = vadd.s32 %v38362, 1 (stack65)
        %v38370 = vadd.s32 %v38354, %v38366 (stack65)
        %v38372 = vshll.u32 %v38366, 17 (stack73)
        %v38373 = vshrl.u32 %v38366, 15 (stack74)
        %v38374 = vor.u32 %v38372, %v38373 (stack75)
        %v38375 = vxor.u32 %v38370, %v38374 (stack76)
        %v38378 = vadd.s32 %v38370, %v38375 (stack65)
        %v38380 = vshll.u32 %v38375, 29 (stack73)
        %v38381 = vshrl.u32 %v38375, 3 (stack74)
        %v38382 = vor.u32 %v38380, %v38381 (stack75)
        %v38383 = vxor.u32 %v38378, %v38382 (stack76)
        %v38386 = vadd.s32 %v38378, %v38383 (stack65)
        %v38388 = vshll.u32 %v38383, 16 (stack73)
        %v38389 = vshrl.u32 %v38383, 16 (stack74)
        %v38390 = vor.u32 %v38388, %v38389 (stack75)
        %v38391 = vxor.u32 %v38386, %v38390 (stack76)
        %v38394 = vadd.s32 %v38386, %v38391 (stack65)
        %v38398 = vadd.s32 %v38394, %v8 (stack65)
        %v38400 = vshll.u32 %v38391, 24 (stack73)
        %v38401 = vshrl.u32 %v38391, 8 (stack74)
        %v38402 = vor.u32 %v38400, %v38401 (stack75)
        %v38403 = vxor.u32 %v38394, %v38402 (stack76)
        %v38406 = vadd.s32 %v38403, %v10 (stack65)
        %v38410 = vadd.s32 %v38406, 2 (stack65)
        %v38414 = vadd.s32 %v38398, %v38410 (stack65)
        %v38416 = vshll.u32 %v38410, 13 (stack73)
        %v38417 = vshrl.u32 %v38410, 19 (stack74)
        %v38418 = vor.u32 %v38416, %v38417 (stack75)
        %v38419 = vxor.u32 %v38414, %v38418 (stack76)
        %v38422 = vadd.s32 %v38414, %v38419 (stack65)
        %v38424 = vshll.u32 %v38419, 15 (stack73)
        %v38425 = vshrl.u32 %v38419, 17 (stack74)
        %v38426 = vor.u32 %v38424, %v38425 (stack75)
        %v38427 = vxor.u32 %v38422, %v38426 (stack76)
        %v38430 = vadd.s32 %v38422, %v38427 (stack65)
        %v38432 = vshll.u32 %v38427, 26 (stack73)
        %v38433 = vshrl.u32 %v38427, 6 (stack74)
        %v38434 = vor.u32 %v38432, %v38433 (stack75)
        %v38435 = vxor.u32 %v38430, %v38434 (stack76)
        %v38438 = vadd.s32 %v38430, %v38435 (stack65)
        %v38442 = vadd.s32 %v38438, %v10 (stack65)
        %v38444 = vshll.u32 %v38435, 6 (stack73)
        %v38445 = vshrl.u32 %v38435, 26 (stack74)
        %v38446 = vor.u32 %v38444, %v38445 (stack75)
        %v38447 = vxor.u32 %v38438, %v38446 (stack76)
        %v38450 = vadd.s32 %v38447, %v9 (stack65)
        %v38454 = vadd.s32 %v38450, 3 (stack65)
        %v38458 = vadd.s32 %v38442, %v38454 (stack65)
        %v38460 = vshll.u32 %v38454, 17 (stack73)
        %v38461 = vshrl.u32 %v38454, 15 (stack74)
        %v38462 = vor.u32 %v38460, %v38461 (stack75)
        %v38463 = vxor.u32 %v38458, %v38462 (stack76)
        %v38466 = vadd.s32 %v38458, %v38463 (stack65)
        %v38468 = vshll.u32 %v38463, 29 (stack73)
        %v38469 = vshrl.u32 %v38463, 3 (stack74)
        %v38470 = vor.u32 %v38468, %v38469 (stack75)
        %v38471 = vxor.u32 %v38466, %v38470 (stack76)
        %v38474 = vadd.s32 %v38466, %v38471 (stack65)
        %v38476 = vshll.u32 %v38471, 16 (stack73)
        %v38477 = vshrl.u32 %v38471, 16 (stack74)
        %v38478 = vor.u32 %v38476, %v38477 (stack75)
        %v38479 = vxor.u32 %v38474, %v38478 (stack76)
        %v38482 = vadd.s32 %v38474, %v38479 (stack65)
        %v38486 = vadd.s32 %v38482, %v9 (stack65)
        %v38488 = vshll.u32 %v38479, 24 (stack73)
        %v38489 = vshrl.u32 %v38479, 8 (stack74)
        %v38490 = vor.u32 %v38488, %v38489 (stack75)
        %v38491 = vxor.u32 %v38482, %v38490 (stack76)
        %v38494 = vadd.s32 %v38491, %v8 (stack65)
        %v38498 = vadd.s32 %v38494, 4 (stack65)
        %v38502 = vadd.s32 %v38486, %v38498 (stack65)
        %v38504 = vshll.u32 %v38498, 13 (stack73)
        %v38505 = vshrl.u32 %v38498, 19 (stack74)
        %v38506 = vor.u32 %v38504, %v38505 (stack75)
        %v38507 = vxor.u32 %v38502, %v38506 (stack76)
        %v38510 = vadd.s32 %v38502, %v38507 (stack65)
        %v38512 = vshll.u32 %v38507, 15 (stack73)
        %v38513 = vshrl.u32 %v38507, 17 (stack74)
        %v38514 = vor.u32 %v38512, %v38513 (stack75)
        %v38515 = vxor.u32 %v38510, %v38514 (stack76)
        %v38518 = vadd.s32 %v38510, %v38515 (stack65)
        %v38520 = vshll.u32 %v38515, 26 (stack73)
        %v38521 = vshrl.u32 %v38515, 6 (stack74)
        %v38522 = vor.u32 %v38520, %v38521 (stack75)
        %v38523 = vxor.u32 %v38518, %v38522 (stack76)
        %v38526 = vadd.s32 %v38518, %v38523 (stack65)
        %v38530 = vadd.s32 %v38526, %v8 (stack65)
        %v38532 = vshll.u32 %v38523, 6 (stack73)
        %v38533 = vshrl.u32 %v38523, 26 (stack74)
        %v38534 = vor.u32 %v38532, %v38533 (stack75)
        %v38535 = vxor.u32 %v38526, %v38534 (stack76)
        %v38538 = vadd.s32 %v38535, %v10 (stack65)
        %v38542 = vadd.s32 %v38538, 5 (stack65)
        %v38544 = vxor.u32 %v38530, %v38542 (stack76)
        %v38545 = vand.u32.u8 %v38544, 255 (stack77)
        %v38546 = vand.u32 %v38545, 65535 (stack78)
        %v38547 = vshrl.u32 %v38546, 1 (stack79)
        %v38548 = vor.u32 %v38547, 16256 (stack75)
        %v38549 = vand.u32.u16 %v38548, 65535 (stack80)
        %v38550 = vunpack.i.l.bf16 %v38549 (stack81)
        %v38554 = vadd.f32 %v38550, -1.0 (stack82)
        %v38558 = vmul.f32 %v38554, 2.0 (stack83)
        %v38562 = vadd.f32 %v38558, -0.99609375 (stack82)
        %v38566 = vmax.f32 -0.99609375, %v38562 (stack84)
        %v38568 = vand.u32 2147483647, %v38566 (stack85)
        %vm38571 = vcmp.eq.f32.partialorder %v38568, 1.0 (stack86)
        %v38576 = vmul.f32 %v38566, inf (stack83)
        %v38578 = vxor.u32 %v38566, 2147483648 (stack87)
        %v38581 = vmul.f32 %v38566, %v38578 (stack83)
        %v38583 = vadd.f32 %v38581, 1.0 (stack88)
        %v38584 = vlog2.pop %v38583 (stack89)
        %v38585 = vmul.f32 %v38584, 0.6931472 (stack90)
        %v38586 = vmul.f32 -0.5, %v38581 (stack91)
        %v38587 = vadd.f32 %v38586, 1.0 (stack92)
        %v38588 = vmul.f32 %v38587, %v38581 (stack93)
        %v38589 = vand.u32 2147483647, %v38581 (stack94)
        %vm38590 = vcmp.lt.f32.partialorder %v38589, 0.0004427343 (stack95)
        %v38591 = vsel /*vm=*/%vm38590, /*on_true_vy=*/%v38588, /*on_false_vx=*/%v38585 (stack96)
        %v38592 = vxor.u32 %v38591, 2147483648 (stack87)
        %vm38595 = vcmp.lt.f32.partialorder %v38592, 5.0 (stack86)
        %v38600 = vsel /*vm=*/%vm38595, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v38604 = vsel /*vm=*/%vm38595, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v38608 = vsel /*vm=*/%vm38595, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v38612 = vsel /*vm=*/%vm38595, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v38616 = vsel /*vm=*/%vm38595, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v38620 = vsel /*vm=*/%vm38595, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v38624 = vsel /*vm=*/%vm38595, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v38628 = vsel /*vm=*/%vm38595, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v38632 = vsel /*vm=*/%vm38595, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v38636 = vadd.f32 %v38592, -2.5 (stack82)
        %v38638 = vrsqrt.pop %v38592 (stack97)
        %v38639 = vmul.f32 %v38592, %v38638 (stack98)
        %vm38640 = vcmp.eq.f32.partialorder %v38592, inf (stack99)
        %v38641 = vsel /*vm=*/%vm38640, /*on_true_vy=*/%v38592, /*on_false_vx=*/%v38639 (stack100)
        %vm38642 = vcmp.eq.f32.partialorder %v38592, 0.0 (stack101)
        %v38643 = vand.u32 %v38592, 2147483648 (stack102)
        %v38644 = vsel /*vm=*/%vm38642, /*on_true_vy=*/%v38643, /*on_false_vx=*/%v38641 (stack103)
        %v38647 = vadd.f32 %v38644, -3.0 (stack82)
        %v38651 = vsel /*vm=*/%vm38595, /*on_true_vy=*/%v38636, /*on_false_vx=*/%v38647 (stack72)
        %v38655 = vmul.f32 %v38632, %v38651 (stack83)
        %v38659 = vadd.f32 %v38628, %v38655 (stack82)
        %v38663 = vmul.f32 %v38659, %v38651 (stack83)
        %v38667 = vadd.f32 %v38624, %v38663 (stack82)
        %v38671 = vmul.f32 %v38667, %v38651 (stack83)
        %v38675 = vadd.f32 %v38620, %v38671 (stack82)
        %v38679 = vmul.f32 %v38675, %v38651 (stack83)
        %v38683 = vadd.f32 %v38616, %v38679 (stack82)
        %v38687 = vmul.f32 %v38683, %v38651 (stack83)
        %v38691 = vadd.f32 %v38612, %v38687 (stack82)
        %v38695 = vmul.f32 %v38691, %v38651 (stack83)
        %v38699 = vadd.f32 %v38608, %v38695 (stack82)
        %v38703 = vmul.f32 %v38699, %v38651 (stack83)
        %v38707 = vadd.f32 %v38604, %v38703 (stack82)
        %v38711 = vmul.f32 %v38707, %v38651 (stack83)
        %v38715 = vadd.f32 %v38600, %v38711 (stack82)
        %v38719 = vmul.f32 %v38715, %v38566 (stack83)
        %v38723 = vsel /*vm=*/%vm38571, /*on_true_vy=*/%v38576, /*on_false_vx=*/%v38719 (stack72)
        %v38727 = vmul.f32 %v38723, 1.4140625 (stack83)
        %s38729 = scalar_lea.vmem %s280, 168 [#allocation0] (stack107)
        %v38730 = vpack.c.bf16 0.0, %v38727 (stack104)
        %38731 = vst [vmem:[%s38729] sm:$0xf] /*vst_source=*/%v38730 (stack105)
        %v38734 = vadd.s32 %v1381, %v37809 (stack65)
        %s38736 = smul.u32 128, %s27 (stack66)
        %v38737 = vlaneseq (stack67)
        %v38738 = vand.u32 %v38737, 127 (stack68)
        %v38739 = vstv %s38736 (stack69)
        %v38740 = vadd.s32 %v38738, %v38739 (stack70)
        %v38744 = vadd.s32 %v38734, %v38740 (stack65)
        %vm38748 = vcmp.lt.u32.totalorder %v38744, %v38734 (stack71)
        %vm38753 = vcmp.lt.u32.totalorder %v38734, %v1381 (stack71)
        %v38758 = vadd.s32 %v1368, %v37792 (stack65)
        %v38762 = vadd.s32 %v38758, 1 (stack65)
        %v38766 = vsel /*vm=*/%vm38753, /*on_true_vy=*/%v38762, /*on_false_vx=*/%v38758 (stack72)
        %v38770 = vadd.s32 %v38766, 1 (stack65)
        %v38774 = vsel /*vm=*/%vm38748, /*on_true_vy=*/%v38770, /*on_false_vx=*/%v38766 (stack72)
        %v38779 = vadd.s32 %v38774, %v10 (stack65)
        %v38783 = vadd.s32 %v38744, %v9 (stack65)
        %v38787 = vadd.s32 %v38779, %v38783 (stack65)
        %v38789 = vshll.u32 %v38783, 13 (stack73)
        %v38790 = vshrl.u32 %v38783, 19 (stack74)
        %v38791 = vor.u32 %v38789, %v38790 (stack75)
        %v38792 = vxor.u32 %v38787, %v38791 (stack76)
        %v38795 = vadd.s32 %v38787, %v38792 (stack65)
        %v38797 = vshll.u32 %v38792, 15 (stack73)
        %v38798 = vshrl.u32 %v38792, 17 (stack74)
        %v38799 = vor.u32 %v38797, %v38798 (stack75)
        %v38800 = vxor.u32 %v38795, %v38799 (stack76)
        %v38803 = vadd.s32 %v38795, %v38800 (stack65)
        %v38805 = vshll.u32 %v38800, 26 (stack73)
        %v38806 = vshrl.u32 %v38800, 6 (stack74)
        %v38807 = vor.u32 %v38805, %v38806 (stack75)
        %v38808 = vxor.u32 %v38803, %v38807 (stack76)
        %v38811 = vadd.s32 %v38803, %v38808 (stack65)
        %v38815 = vadd.s32 %v38811, %v9 (stack65)
        %v38817 = vshll.u32 %v38808, 6 (stack73)
        %v38818 = vshrl.u32 %v38808, 26 (stack74)
        %v38819 = vor.u32 %v38817, %v38818 (stack75)
        %v38820 = vxor.u32 %v38811, %v38819 (stack76)
        %v38823 = vadd.s32 %v38820, %v8 (stack65)
        %v38827 = vadd.s32 %v38823, 1 (stack65)
        %v38831 = vadd.s32 %v38815, %v38827 (stack65)
        %v38833 = vshll.u32 %v38827, 17 (stack73)
        %v38834 = vshrl.u32 %v38827, 15 (stack74)
        %v38835 = vor.u32 %v38833, %v38834 (stack75)
        %v38836 = vxor.u32 %v38831, %v38835 (stack76)
        %v38839 = vadd.s32 %v38831, %v38836 (stack65)
        %v38841 = vshll.u32 %v38836, 29 (stack73)
        %v38842 = vshrl.u32 %v38836, 3 (stack74)
        %v38843 = vor.u32 %v38841, %v38842 (stack75)
        %v38844 = vxor.u32 %v38839, %v38843 (stack76)
        %v38847 = vadd.s32 %v38839, %v38844 (stack65)
        %v38849 = vshll.u32 %v38844, 16 (stack73)
        %v38850 = vshrl.u32 %v38844, 16 (stack74)
        %v38851 = vor.u32 %v38849, %v38850 (stack75)
        %v38852 = vxor.u32 %v38847, %v38851 (stack76)
        %v38855 = vadd.s32 %v38847, %v38852 (stack65)
        %v38859 = vadd.s32 %v38855, %v8 (stack65)
        %v38861 = vshll.u32 %v38852, 24 (stack73)
        %v38862 = vshrl.u32 %v38852, 8 (stack74)
        %v38863 = vor.u32 %v38861, %v38862 (stack75)
        %v38864 = vxor.u32 %v38855, %v38863 (stack76)
        %v38867 = vadd.s32 %v38864, %v10 (stack65)
        %v38871 = vadd.s32 %v38867, 2 (stack65)
        %v38875 = vadd.s32 %v38859, %v38871 (stack65)
        %v38877 = vshll.u32 %v38871, 13 (stack73)
        %v38878 = vshrl.u32 %v38871, 19 (stack74)
        %v38879 = vor.u32 %v38877, %v38878 (stack75)
        %v38880 = vxor.u32 %v38875, %v38879 (stack76)
        %v38883 = vadd.s32 %v38875, %v38880 (stack65)
        %v38885 = vshll.u32 %v38880, 15 (stack73)
        %v38886 = vshrl.u32 %v38880, 17 (stack74)
        %v38887 = vor.u32 %v38885, %v38886 (stack75)
        %v38888 = vxor.u32 %v38883, %v38887 (stack76)
        %v38891 = vadd.s32 %v38883, %v38888 (stack65)
        %v38893 = vshll.u32 %v38888, 26 (stack73)
        %v38894 = vshrl.u32 %v38888, 6 (stack74)
        %v38895 = vor.u32 %v38893, %v38894 (stack75)
        %v38896 = vxor.u32 %v38891, %v38895 (stack76)
        %v38899 = vadd.s32 %v38891, %v38896 (stack65)
        %v38903 = vadd.s32 %v38899, %v10 (stack65)
        %v38905 = vshll.u32 %v38896, 6 (stack73)
        %v38906 = vshrl.u32 %v38896, 26 (stack74)
        %v38907 = vor.u32 %v38905, %v38906 (stack75)
        %v38908 = vxor.u32 %v38899, %v38907 (stack76)
        %v38911 = vadd.s32 %v38908, %v9 (stack65)
        %v38915 = vadd.s32 %v38911, 3 (stack65)
        %v38919 = vadd.s32 %v38903, %v38915 (stack65)
        %v38921 = vshll.u32 %v38915, 17 (stack73)
        %v38922 = vshrl.u32 %v38915, 15 (stack74)
        %v38923 = vor.u32 %v38921, %v38922 (stack75)
        %v38924 = vxor.u32 %v38919, %v38923 (stack76)
        %v38927 = vadd.s32 %v38919, %v38924 (stack65)
        %v38929 = vshll.u32 %v38924, 29 (stack73)
        %v38930 = vshrl.u32 %v38924, 3 (stack74)
        %v38931 = vor.u32 %v38929, %v38930 (stack75)
        %v38932 = vxor.u32 %v38927, %v38931 (stack76)
        %v38935 = vadd.s32 %v38927, %v38932 (stack65)
        %v38937 = vshll.u32 %v38932, 16 (stack73)
        %v38938 = vshrl.u32 %v38932, 16 (stack74)
        %v38939 = vor.u32 %v38937, %v38938 (stack75)
        %v38940 = vxor.u32 %v38935, %v38939 (stack76)
        %v38943 = vadd.s32 %v38935, %v38940 (stack65)
        %v38947 = vadd.s32 %v38943, %v9 (stack65)
        %v38949 = vshll.u32 %v38940, 24 (stack73)
        %v38950 = vshrl.u32 %v38940, 8 (stack74)
        %v38951 = vor.u32 %v38949, %v38950 (stack75)
        %v38952 = vxor.u32 %v38943, %v38951 (stack76)
        %v38955 = vadd.s32 %v38952, %v8 (stack65)
        %v38959 = vadd.s32 %v38955, 4 (stack65)
        %v38963 = vadd.s32 %v38947, %v38959 (stack65)
        %v38965 = vshll.u32 %v38959, 13 (stack73)
        %v38966 = vshrl.u32 %v38959, 19 (stack74)
        %v38967 = vor.u32 %v38965, %v38966 (stack75)
        %v38968 = vxor.u32 %v38963, %v38967 (stack76)
        %v38971 = vadd.s32 %v38963, %v38968 (stack65)
        %v38973 = vshll.u32 %v38968, 15 (stack73)
        %v38974 = vshrl.u32 %v38968, 17 (stack74)
        %v38975 = vor.u32 %v38973, %v38974 (stack75)
        %v38976 = vxor.u32 %v38971, %v38975 (stack76)
        %v38979 = vadd.s32 %v38971, %v38976 (stack65)
        %v38981 = vshll.u32 %v38976, 26 (stack73)
        %v38982 = vshrl.u32 %v38976, 6 (stack74)
        %v38983 = vor.u32 %v38981, %v38982 (stack75)
        %v38984 = vxor.u32 %v38979, %v38983 (stack76)
        %v38987 = vadd.s32 %v38979, %v38984 (stack65)
        %v38991 = vadd.s32 %v38987, %v8 (stack65)
        %v38993 = vshll.u32 %v38984, 6 (stack73)
        %v38994 = vshrl.u32 %v38984, 26 (stack74)
        %v38995 = vor.u32 %v38993, %v38994 (stack75)
        %v38996 = vxor.u32 %v38987, %v38995 (stack76)
        %v38999 = vadd.s32 %v38996, %v10 (stack65)
        %v39003 = vadd.s32 %v38999, 5 (stack65)
        %v39005 = vxor.u32 %v38991, %v39003 (stack76)
        %v39006 = vand.u32.u8 %v39005, 255 (stack77)
        %v39007 = vand.u32 %v39006, 65535 (stack78)
        %v39008 = vshrl.u32 %v39007, 1 (stack79)
        %v39009 = vor.u32 %v39008, 16256 (stack75)
        %v39010 = vand.u32.u16 %v39009, 65535 (stack80)
        %v39011 = vunpack.i.l.bf16 %v39010 (stack81)
        %v39015 = vadd.f32 %v39011, -1.0 (stack82)
        %v39019 = vmul.f32 %v39015, 2.0 (stack83)
        %v39023 = vadd.f32 %v39019, -0.99609375 (stack82)
        %v39027 = vmax.f32 -0.99609375, %v39023 (stack84)
        %v39029 = vand.u32 2147483647, %v39027 (stack85)
        %vm39032 = vcmp.eq.f32.partialorder %v39029, 1.0 (stack86)
        %v39037 = vmul.f32 %v39027, inf (stack83)
        %v39039 = vxor.u32 %v39027, 2147483648 (stack87)
        %v39042 = vmul.f32 %v39027, %v39039 (stack83)
        %v39044 = vadd.f32 %v39042, 1.0 (stack88)
        %v39045 = vlog2.pop %v39044 (stack89)
        %v39046 = vmul.f32 %v39045, 0.6931472 (stack90)
        %v39047 = vmul.f32 -0.5, %v39042 (stack91)
        %v39048 = vadd.f32 %v39047, 1.0 (stack92)
        %v39049 = vmul.f32 %v39048, %v39042 (stack93)
        %v39050 = vand.u32 2147483647, %v39042 (stack94)
        %vm39051 = vcmp.lt.f32.partialorder %v39050, 0.0004427343 (stack95)
        %v39052 = vsel /*vm=*/%vm39051, /*on_true_vy=*/%v39049, /*on_false_vx=*/%v39046 (stack96)
        %v39053 = vxor.u32 %v39052, 2147483648 (stack87)
        %vm39056 = vcmp.lt.f32.partialorder %v39053, 5.0 (stack86)
        %v39061 = vsel /*vm=*/%vm39056, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v39065 = vsel /*vm=*/%vm39056, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v39069 = vsel /*vm=*/%vm39056, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v39073 = vsel /*vm=*/%vm39056, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v39077 = vsel /*vm=*/%vm39056, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v39081 = vsel /*vm=*/%vm39056, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v39085 = vsel /*vm=*/%vm39056, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v39089 = vsel /*vm=*/%vm39056, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v39093 = vsel /*vm=*/%vm39056, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v39097 = vadd.f32 %v39053, -2.5 (stack82)
        %v39099 = vrsqrt.pop %v39053 (stack97)
        %v39100 = vmul.f32 %v39053, %v39099 (stack98)
        %vm39101 = vcmp.eq.f32.partialorder %v39053, inf (stack99)
        %v39102 = vsel /*vm=*/%vm39101, /*on_true_vy=*/%v39053, /*on_false_vx=*/%v39100 (stack100)
        %vm39103 = vcmp.eq.f32.partialorder %v39053, 0.0 (stack101)
        %v39104 = vand.u32 %v39053, 2147483648 (stack102)
        %v39105 = vsel /*vm=*/%vm39103, /*on_true_vy=*/%v39104, /*on_false_vx=*/%v39102 (stack103)
        %v39108 = vadd.f32 %v39105, -3.0 (stack82)
        %v39112 = vsel /*vm=*/%vm39056, /*on_true_vy=*/%v39097, /*on_false_vx=*/%v39108 (stack72)
        %v39116 = vmul.f32 %v39093, %v39112 (stack83)
        %v39120 = vadd.f32 %v39089, %v39116 (stack82)
        %v39124 = vmul.f32 %v39120, %v39112 (stack83)
        %v39128 = vadd.f32 %v39085, %v39124 (stack82)
        %v39132 = vmul.f32 %v39128, %v39112 (stack83)
        %v39136 = vadd.f32 %v39081, %v39132 (stack82)
        %v39140 = vmul.f32 %v39136, %v39112 (stack83)
        %v39144 = vadd.f32 %v39077, %v39140 (stack82)
        %v39148 = vmul.f32 %v39144, %v39112 (stack83)
        %v39152 = vadd.f32 %v39073, %v39148 (stack82)
        %v39156 = vmul.f32 %v39152, %v39112 (stack83)
        %v39160 = vadd.f32 %v39069, %v39156 (stack82)
        %v39164 = vmul.f32 %v39160, %v39112 (stack83)
        %v39168 = vadd.f32 %v39065, %v39164 (stack82)
        %v39172 = vmul.f32 %v39168, %v39112 (stack83)
        %v39176 = vadd.f32 %v39061, %v39172 (stack82)
        %v39180 = vmul.f32 %v39176, %v39027 (stack83)
        %v39184 = vsel /*vm=*/%vm39032, /*on_true_vy=*/%v39037, /*on_false_vx=*/%v39180 (stack72)
        %v39188 = vmul.f32 %v39184, 1.4140625 (stack83)
        %s39190 = scalar_lea.vmem %s280, 296 [#allocation0] (stack107)
        %v39191 = vpack.c.bf16 0.0, %v39188 (stack104)
        %39192 = vst [vmem:[%s39190] sm:$0xf] /*vst_source=*/%v39191 (stack105)
        %v39195 = vadd.s32 %v1868, %v37809 (stack65)
        %s39197 = smul.u32 128, %s27 (stack66)
        %v39198 = vlaneseq (stack67)
        %v39199 = vand.u32 %v39198, 127 (stack68)
        %v39200 = vstv %s39197 (stack69)
        %v39201 = vadd.s32 %v39199, %v39200 (stack70)
        %v39205 = vadd.s32 %v39195, %v39201 (stack65)
        %vm39209 = vcmp.lt.u32.totalorder %v39205, %v39195 (stack71)
        %vm39214 = vcmp.lt.u32.totalorder %v39195, %v1868 (stack71)
        %v39219 = vadd.s32 %v1855, %v37792 (stack65)
        %v39223 = vadd.s32 %v39219, 1 (stack65)
        %v39227 = vsel /*vm=*/%vm39214, /*on_true_vy=*/%v39223, /*on_false_vx=*/%v39219 (stack72)
        %v39231 = vadd.s32 %v39227, 1 (stack65)
        %v39235 = vsel /*vm=*/%vm39209, /*on_true_vy=*/%v39231, /*on_false_vx=*/%v39227 (stack72)
        %v39240 = vadd.s32 %v39235, %v10 (stack65)
        %v39244 = vadd.s32 %v39205, %v9 (stack65)
        %v39248 = vadd.s32 %v39240, %v39244 (stack65)
        %v39250 = vshll.u32 %v39244, 13 (stack73)
        %v39251 = vshrl.u32 %v39244, 19 (stack74)
        %v39252 = vor.u32 %v39250, %v39251 (stack75)
        %v39253 = vxor.u32 %v39248, %v39252 (stack76)
        %v39256 = vadd.s32 %v39248, %v39253 (stack65)
        %v39258 = vshll.u32 %v39253, 15 (stack73)
        %v39259 = vshrl.u32 %v39253, 17 (stack74)
        %v39260 = vor.u32 %v39258, %v39259 (stack75)
        %v39261 = vxor.u32 %v39256, %v39260 (stack76)
        %v39264 = vadd.s32 %v39256, %v39261 (stack65)
        %v39266 = vshll.u32 %v39261, 26 (stack73)
        %v39267 = vshrl.u32 %v39261, 6 (stack74)
        %v39268 = vor.u32 %v39266, %v39267 (stack75)
        %v39269 = vxor.u32 %v39264, %v39268 (stack76)
        %v39272 = vadd.s32 %v39264, %v39269 (stack65)
        %v39276 = vadd.s32 %v39272, %v9 (stack65)
        %v39278 = vshll.u32 %v39269, 6 (stack73)
        %v39279 = vshrl.u32 %v39269, 26 (stack74)
        %v39280 = vor.u32 %v39278, %v39279 (stack75)
        %v39281 = vxor.u32 %v39272, %v39280 (stack76)
        %v39284 = vadd.s32 %v39281, %v8 (stack65)
        %v39288 = vadd.s32 %v39284, 1 (stack65)
        %v39292 = vadd.s32 %v39276, %v39288 (stack65)
        %v39294 = vshll.u32 %v39288, 17 (stack73)
        %v39295 = vshrl.u32 %v39288, 15 (stack74)
        %v39296 = vor.u32 %v39294, %v39295 (stack75)
        %v39297 = vxor.u32 %v39292, %v39296 (stack76)
        %v39300 = vadd.s32 %v39292, %v39297 (stack65)
        %v39302 = vshll.u32 %v39297, 29 (stack73)
        %v39303 = vshrl.u32 %v39297, 3 (stack74)
        %v39304 = vor.u32 %v39302, %v39303 (stack75)
        %v39305 = vxor.u32 %v39300, %v39304 (stack76)
        %v39308 = vadd.s32 %v39300, %v39305 (stack65)
        %v39310 = vshll.u32 %v39305, 16 (stack73)
        %v39311 = vshrl.u32 %v39305, 16 (stack74)
        %v39312 = vor.u32 %v39310, %v39311 (stack75)
        %v39313 = vxor.u32 %v39308, %v39312 (stack76)
        %v39316 = vadd.s32 %v39308, %v39313 (stack65)
        %v39320 = vadd.s32 %v39316, %v8 (stack65)
        %v39322 = vshll.u32 %v39313, 24 (stack73)
        %v39323 = vshrl.u32 %v39313, 8 (stack74)
        %v39324 = vor.u32 %v39322, %v39323 (stack75)
        %v39325 = vxor.u32 %v39316, %v39324 (stack76)
        %v39328 = vadd.s32 %v39325, %v10 (stack65)
        %v39332 = vadd.s32 %v39328, 2 (stack65)
        %v39336 = vadd.s32 %v39320, %v39332 (stack65)
        %v39338 = vshll.u32 %v39332, 13 (stack73)
        %v39339 = vshrl.u32 %v39332, 19 (stack74)
        %v39340 = vor.u32 %v39338, %v39339 (stack75)
        %v39341 = vxor.u32 %v39336, %v39340 (stack76)
        %v39344 = vadd.s32 %v39336, %v39341 (stack65)
        %v39346 = vshll.u32 %v39341, 15 (stack73)
        %v39347 = vshrl.u32 %v39341, 17 (stack74)
        %v39348 = vor.u32 %v39346, %v39347 (stack75)
        %v39349 = vxor.u32 %v39344, %v39348 (stack76)
        %v39352 = vadd.s32 %v39344, %v39349 (stack65)
        %v39354 = vshll.u32 %v39349, 26 (stack73)
        %v39355 = vshrl.u32 %v39349, 6 (stack74)
        %v39356 = vor.u32 %v39354, %v39355 (stack75)
        %v39357 = vxor.u32 %v39352, %v39356 (stack76)
        %v39360 = vadd.s32 %v39352, %v39357 (stack65)
        %v39364 = vadd.s32 %v39360, %v10 (stack65)
        %v39366 = vshll.u32 %v39357, 6 (stack73)
        %v39367 = vshrl.u32 %v39357, 26 (stack74)
        %v39368 = vor.u32 %v39366, %v39367 (stack75)
        %v39369 = vxor.u32 %v39360, %v39368 (stack76)
        %v39372 = vadd.s32 %v39369, %v9 (stack65)
        %v39376 = vadd.s32 %v39372, 3 (stack65)
        %v39380 = vadd.s32 %v39364, %v39376 (stack65)
        %v39382 = vshll.u32 %v39376, 17 (stack73)
        %v39383 = vshrl.u32 %v39376, 15 (stack74)
        %v39384 = vor.u32 %v39382, %v39383 (stack75)
        %v39385 = vxor.u32 %v39380, %v39384 (stack76)
        %v39388 = vadd.s32 %v39380, %v39385 (stack65)
        %v39390 = vshll.u32 %v39385, 29 (stack73)
        %v39391 = vshrl.u32 %v39385, 3 (stack74)
        %v39392 = vor.u32 %v39390, %v39391 (stack75)
        %v39393 = vxor.u32 %v39388, %v39392 (stack76)
        %v39396 = vadd.s32 %v39388, %v39393 (stack65)
        %v39398 = vshll.u32 %v39393, 16 (stack73)
        %v39399 = vshrl.u32 %v39393, 16 (stack74)
        %v39400 = vor.u32 %v39398, %v39399 (stack75)
        %v39401 = vxor.u32 %v39396, %v39400 (stack76)
        %v39404 = vadd.s32 %v39396, %v39401 (stack65)
        %v39408 = vadd.s32 %v39404, %v9 (stack65)
        %v39410 = vshll.u32 %v39401, 24 (stack73)
        %v39411 = vshrl.u32 %v39401, 8 (stack74)
        %v39412 = vor.u32 %v39410, %v39411 (stack75)
        %v39413 = vxor.u32 %v39404, %v39412 (stack76)
        %v39416 = vadd.s32 %v39413, %v8 (stack65)
        %v39420 = vadd.s32 %v39416, 4 (stack65)
        %v39424 = vadd.s32 %v39408, %v39420 (stack65)
        %v39426 = vshll.u32 %v39420, 13 (stack73)
        %v39427 = vshrl.u32 %v39420, 19 (stack74)
        %v39428 = vor.u32 %v39426, %v39427 (stack75)
        %v39429 = vxor.u32 %v39424, %v39428 (stack76)
        %v39432 = vadd.s32 %v39424, %v39429 (stack65)
        %v39434 = vshll.u32 %v39429, 15 (stack73)
        %v39435 = vshrl.u32 %v39429, 17 (stack74)
        %v39436 = vor.u32 %v39434, %v39435 (stack75)
        %v39437 = vxor.u32 %v39432, %v39436 (stack76)
        %v39440 = vadd.s32 %v39432, %v39437 (stack65)
        %v39442 = vshll.u32 %v39437, 26 (stack73)
        %v39443 = vshrl.u32 %v39437, 6 (stack74)
        %v39444 = vor.u32 %v39442, %v39443 (stack75)
        %v39445 = vxor.u32 %v39440, %v39444 (stack76)
        %v39448 = vadd.s32 %v39440, %v39445 (stack65)
        %v39452 = vadd.s32 %v39448, %v8 (stack65)
        %v39454 = vshll.u32 %v39445, 6 (stack73)
        %v39455 = vshrl.u32 %v39445, 26 (stack74)
        %v39456 = vor.u32 %v39454, %v39455 (stack75)
        %v39457 = vxor.u32 %v39448, %v39456 (stack76)
        %v39460 = vadd.s32 %v39457, %v10 (stack65)
        %v39464 = vadd.s32 %v39460, 5 (stack65)
        %v39466 = vxor.u32 %v39452, %v39464 (stack76)
        %v39467 = vand.u32.u8 %v39466, 255 (stack77)
        %v39468 = vand.u32 %v39467, 65535 (stack78)
        %v39469 = vshrl.u32 %v39468, 1 (stack79)
        %v39470 = vor.u32 %v39469, 16256 (stack75)
        %v39471 = vand.u32.u16 %v39470, 65535 (stack80)
        %v39472 = vunpack.i.l.bf16 %v39471 (stack81)
        %v39476 = vadd.f32 %v39472, -1.0 (stack82)
        %v39480 = vmul.f32 %v39476, 2.0 (stack83)
        %v39484 = vadd.f32 %v39480, -0.99609375 (stack82)
        %v39488 = vmax.f32 -0.99609375, %v39484 (stack84)
        %v39490 = vand.u32 2147483647, %v39488 (stack85)
        %vm39493 = vcmp.eq.f32.partialorder %v39490, 1.0 (stack86)
        %v39498 = vmul.f32 %v39488, inf (stack83)
        %v39500 = vxor.u32 %v39488, 2147483648 (stack87)
        %v39503 = vmul.f32 %v39488, %v39500 (stack83)
        %v39505 = vadd.f32 %v39503, 1.0 (stack88)
        %v39506 = vlog2.pop %v39505 (stack89)
        %v39507 = vmul.f32 %v39506, 0.6931472 (stack90)
        %v39508 = vmul.f32 -0.5, %v39503 (stack91)
        %v39509 = vadd.f32 %v39508, 1.0 (stack92)
        %v39510 = vmul.f32 %v39509, %v39503 (stack93)
        %v39511 = vand.u32 2147483647, %v39503 (stack94)
        %vm39512 = vcmp.lt.f32.partialorder %v39511, 0.0004427343 (stack95)
        %v39513 = vsel /*vm=*/%vm39512, /*on_true_vy=*/%v39510, /*on_false_vx=*/%v39507 (stack96)
        %v39514 = vxor.u32 %v39513, 2147483648 (stack87)
        %vm39517 = vcmp.lt.f32.partialorder %v39514, 5.0 (stack86)
        %v39522 = vsel /*vm=*/%vm39517, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v39526 = vsel /*vm=*/%vm39517, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v39530 = vsel /*vm=*/%vm39517, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v39534 = vsel /*vm=*/%vm39517, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v39538 = vsel /*vm=*/%vm39517, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v39542 = vsel /*vm=*/%vm39517, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v39546 = vsel /*vm=*/%vm39517, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v39550 = vsel /*vm=*/%vm39517, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v39554 = vsel /*vm=*/%vm39517, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v39558 = vadd.f32 %v39514, -2.5 (stack82)
        %v39560 = vrsqrt.pop %v39514 (stack97)
        %v39561 = vmul.f32 %v39514, %v39560 (stack98)
        %vm39562 = vcmp.eq.f32.partialorder %v39514, inf (stack99)
        %v39563 = vsel /*vm=*/%vm39562, /*on_true_vy=*/%v39514, /*on_false_vx=*/%v39561 (stack100)
        %vm39564 = vcmp.eq.f32.partialorder %v39514, 0.0 (stack101)
        %v39565 = vand.u32 %v39514, 2147483648 (stack102)
        %v39566 = vsel /*vm=*/%vm39564, /*on_true_vy=*/%v39565, /*on_false_vx=*/%v39563 (stack103)
        %v39569 = vadd.f32 %v39566, -3.0 (stack82)
        %v39573 = vsel /*vm=*/%vm39517, /*on_true_vy=*/%v39558, /*on_false_vx=*/%v39569 (stack72)
        %v39577 = vmul.f32 %v39554, %v39573 (stack83)
        %v39581 = vadd.f32 %v39550, %v39577 (stack82)
        %v39585 = vmul.f32 %v39581, %v39573 (stack83)
        %v39589 = vadd.f32 %v39546, %v39585 (stack82)
        %v39593 = vmul.f32 %v39589, %v39573 (stack83)
        %v39597 = vadd.f32 %v39542, %v39593 (stack82)
        %v39601 = vmul.f32 %v39597, %v39573 (stack83)
        %v39605 = vadd.f32 %v39538, %v39601 (stack82)
        %v39609 = vmul.f32 %v39605, %v39573 (stack83)
        %v39613 = vadd.f32 %v39534, %v39609 (stack82)
        %v39617 = vmul.f32 %v39613, %v39573 (stack83)
        %v39621 = vadd.f32 %v39530, %v39617 (stack82)
        %v39625 = vmul.f32 %v39621, %v39573 (stack83)
        %v39629 = vadd.f32 %v39526, %v39625 (stack82)
        %v39633 = vmul.f32 %v39629, %v39573 (stack83)
        %v39637 = vadd.f32 %v39522, %v39633 (stack82)
        %v39641 = vmul.f32 %v39637, %v39488 (stack83)
        %v39645 = vsel /*vm=*/%vm39493, /*on_true_vy=*/%v39498, /*on_false_vx=*/%v39641 (stack72)
        %v39649 = vmul.f32 %v39645, 1.4140625 (stack83)
        %s39651 = scalar_lea.vmem %s280, 424 [#allocation0] (stack107)
        %v39652 = vpack.c.bf16 0.0, %v39649 (stack104)
        %39653 = vst [vmem:[%s39651] sm:$0xf] /*vst_source=*/%v39652 (stack105)
        %v39656 = vadd.s32 %v2355, %v37809 (stack65)
        %s39658 = smul.u32 128, %s27 (stack66)
        %v39659 = vlaneseq (stack67)
        %v39660 = vand.u32 %v39659, 127 (stack68)
        %v39661 = vstv %s39658 (stack69)
        %v39662 = vadd.s32 %v39660, %v39661 (stack70)
        %v39666 = vadd.s32 %v39656, %v39662 (stack65)
        %vm39670 = vcmp.lt.u32.totalorder %v39666, %v39656 (stack71)
        %vm39675 = vcmp.lt.u32.totalorder %v39656, %v2355 (stack71)
        %v39680 = vadd.s32 %v2342, %v37792 (stack65)
        %v39684 = vadd.s32 %v39680, 1 (stack65)
        %v39688 = vsel /*vm=*/%vm39675, /*on_true_vy=*/%v39684, /*on_false_vx=*/%v39680 (stack72)
        %v39692 = vadd.s32 %v39688, 1 (stack65)
        %v39696 = vsel /*vm=*/%vm39670, /*on_true_vy=*/%v39692, /*on_false_vx=*/%v39688 (stack72)
        %v39701 = vadd.s32 %v39696, %v10 (stack65)
        %v39705 = vadd.s32 %v39666, %v9 (stack65)
        %v39709 = vadd.s32 %v39701, %v39705 (stack65)
        %v39711 = vshll.u32 %v39705, 13 (stack73)
        %v39712 = vshrl.u32 %v39705, 19 (stack74)
        %v39713 = vor.u32 %v39711, %v39712 (stack75)
        %v39714 = vxor.u32 %v39709, %v39713 (stack76)
        %v39717 = vadd.s32 %v39709, %v39714 (stack65)
        %v39719 = vshll.u32 %v39714, 15 (stack73)
        %v39720 = vshrl.u32 %v39714, 17 (stack74)
        %v39721 = vor.u32 %v39719, %v39720 (stack75)
        %v39722 = vxor.u32 %v39717, %v39721 (stack76)
        %v39725 = vadd.s32 %v39717, %v39722 (stack65)
        %v39727 = vshll.u32 %v39722, 26 (stack73)
        %v39728 = vshrl.u32 %v39722, 6 (stack74)
        %v39729 = vor.u32 %v39727, %v39728 (stack75)
        %v39730 = vxor.u32 %v39725, %v39729 (stack76)
        %v39733 = vadd.s32 %v39725, %v39730 (stack65)
        %v39737 = vadd.s32 %v39733, %v9 (stack65)
        %v39739 = vshll.u32 %v39730, 6 (stack73)
        %v39740 = vshrl.u32 %v39730, 26 (stack74)
        %v39741 = vor.u32 %v39739, %v39740 (stack75)
        %v39742 = vxor.u32 %v39733, %v39741 (stack76)
        %v39745 = vadd.s32 %v39742, %v8 (stack65)
        %v39749 = vadd.s32 %v39745, 1 (stack65)
        %v39753 = vadd.s32 %v39737, %v39749 (stack65)
        %v39755 = vshll.u32 %v39749, 17 (stack73)
        %v39756 = vshrl.u32 %v39749, 15 (stack74)
        %v39757 = vor.u32 %v39755, %v39756 (stack75)
        %v39758 = vxor.u32 %v39753, %v39757 (stack76)
        %v39761 = vadd.s32 %v39753, %v39758 (stack65)
        %v39763 = vshll.u32 %v39758, 29 (stack73)
        %v39764 = vshrl.u32 %v39758, 3 (stack74)
        %v39765 = vor.u32 %v39763, %v39764 (stack75)
        %v39766 = vxor.u32 %v39761, %v39765 (stack76)
        %v39769 = vadd.s32 %v39761, %v39766 (stack65)
        %v39771 = vshll.u32 %v39766, 16 (stack73)
        %v39772 = vshrl.u32 %v39766, 16 (stack74)
        %v39773 = vor.u32 %v39771, %v39772 (stack75)
        %v39774 = vxor.u32 %v39769, %v39773 (stack76)
        %v39777 = vadd.s32 %v39769, %v39774 (stack65)
        %v39781 = vadd.s32 %v39777, %v8 (stack65)
        %v39783 = vshll.u32 %v39774, 24 (stack73)
        %v39784 = vshrl.u32 %v39774, 8 (stack74)
        %v39785 = vor.u32 %v39783, %v39784 (stack75)
        %v39786 = vxor.u32 %v39777, %v39785 (stack76)
        %v39789 = vadd.s32 %v39786, %v10 (stack65)
        %v39793 = vadd.s32 %v39789, 2 (stack65)
        %v39797 = vadd.s32 %v39781, %v39793 (stack65)
        %v39799 = vshll.u32 %v39793, 13 (stack73)
        %v39800 = vshrl.u32 %v39793, 19 (stack74)
        %v39801 = vor.u32 %v39799, %v39800 (stack75)
        %v39802 = vxor.u32 %v39797, %v39801 (stack76)
        %v39805 = vadd.s32 %v39797, %v39802 (stack65)
        %v39807 = vshll.u32 %v39802, 15 (stack73)
        %v39808 = vshrl.u32 %v39802, 17 (stack74)
        %v39809 = vor.u32 %v39807, %v39808 (stack75)
        %v39810 = vxor.u32 %v39805, %v39809 (stack76)
        %v39813 = vadd.s32 %v39805, %v39810 (stack65)
        %v39815 = vshll.u32 %v39810, 26 (stack73)
        %v39816 = vshrl.u32 %v39810, 6 (stack74)
        %v39817 = vor.u32 %v39815, %v39816 (stack75)
        %v39818 = vxor.u32 %v39813, %v39817 (stack76)
        %v39821 = vadd.s32 %v39813, %v39818 (stack65)
        %v39825 = vadd.s32 %v39821, %v10 (stack65)
        %v39827 = vshll.u32 %v39818, 6 (stack73)
        %v39828 = vshrl.u32 %v39818, 26 (stack74)
        %v39829 = vor.u32 %v39827, %v39828 (stack75)
        %v39830 = vxor.u32 %v39821, %v39829 (stack76)
        %v39833 = vadd.s32 %v39830, %v9 (stack65)
        %v39837 = vadd.s32 %v39833, 3 (stack65)
        %v39841 = vadd.s32 %v39825, %v39837 (stack65)
        %v39843 = vshll.u32 %v39837, 17 (stack73)
        %v39844 = vshrl.u32 %v39837, 15 (stack74)
        %v39845 = vor.u32 %v39843, %v39844 (stack75)
        %v39846 = vxor.u32 %v39841, %v39845 (stack76)
        %v39849 = vadd.s32 %v39841, %v39846 (stack65)
        %v39851 = vshll.u32 %v39846, 29 (stack73)
        %v39852 = vshrl.u32 %v39846, 3 (stack74)
        %v39853 = vor.u32 %v39851, %v39852 (stack75)
        %v39854 = vxor.u32 %v39849, %v39853 (stack76)
        %v39857 = vadd.s32 %v39849, %v39854 (stack65)
        %v39859 = vshll.u32 %v39854, 16 (stack73)
        %v39860 = vshrl.u32 %v39854, 16 (stack74)
        %v39861 = vor.u32 %v39859, %v39860 (stack75)
        %v39862 = vxor.u32 %v39857, %v39861 (stack76)
        %v39865 = vadd.s32 %v39857, %v39862 (stack65)
        %v39869 = vadd.s32 %v39865, %v9 (stack65)
        %v39871 = vshll.u32 %v39862, 24 (stack73)
        %v39872 = vshrl.u32 %v39862, 8 (stack74)
        %v39873 = vor.u32 %v39871, %v39872 (stack75)
        %v39874 = vxor.u32 %v39865, %v39873 (stack76)
        %v39877 = vadd.s32 %v39874, %v8 (stack65)
        %v39881 = vadd.s32 %v39877, 4 (stack65)
        %v39885 = vadd.s32 %v39869, %v39881 (stack65)
        %v39887 = vshll.u32 %v39881, 13 (stack73)
        %v39888 = vshrl.u32 %v39881, 19 (stack74)
        %v39889 = vor.u32 %v39887, %v39888 (stack75)
        %v39890 = vxor.u32 %v39885, %v39889 (stack76)
        %v39893 = vadd.s32 %v39885, %v39890 (stack65)
        %v39895 = vshll.u32 %v39890, 15 (stack73)
        %v39896 = vshrl.u32 %v39890, 17 (stack74)
        %v39897 = vor.u32 %v39895, %v39896 (stack75)
        %v39898 = vxor.u32 %v39893, %v39897 (stack76)
        %v39901 = vadd.s32 %v39893, %v39898 (stack65)
        %v39903 = vshll.u32 %v39898, 26 (stack73)
        %v39904 = vshrl.u32 %v39898, 6 (stack74)
        %v39905 = vor.u32 %v39903, %v39904 (stack75)
        %v39906 = vxor.u32 %v39901, %v39905 (stack76)
        %v39909 = vadd.s32 %v39901, %v39906 (stack65)
        %v39913 = vadd.s32 %v39909, %v8 (stack65)
        %v39915 = vshll.u32 %v39906, 6 (stack73)
        %v39916 = vshrl.u32 %v39906, 26 (stack74)
        %v39917 = vor.u32 %v39915, %v39916 (stack75)
        %v39918 = vxor.u32 %v39909, %v39917 (stack76)
        %v39921 = vadd.s32 %v39918, %v10 (stack65)
        %v39925 = vadd.s32 %v39921, 5 (stack65)
        %v39927 = vxor.u32 %v39913, %v39925 (stack76)
        %v39928 = vand.u32.u8 %v39927, 255 (stack77)
        %v39929 = vand.u32 %v39928, 65535 (stack78)
        %v39930 = vshrl.u32 %v39929, 1 (stack79)
        %v39931 = vor.u32 %v39930, 16256 (stack75)
        %v39932 = vand.u32.u16 %v39931, 65535 (stack80)
        %v39933 = vunpack.i.l.bf16 %v39932 (stack81)
        %v39937 = vadd.f32 %v39933, -1.0 (stack82)
        %v39941 = vmul.f32 %v39937, 2.0 (stack83)
        %v39945 = vadd.f32 %v39941, -0.99609375 (stack82)
        %v39949 = vmax.f32 -0.99609375, %v39945 (stack84)
        %v39951 = vand.u32 2147483647, %v39949 (stack85)
        %vm39954 = vcmp.eq.f32.partialorder %v39951, 1.0 (stack86)
        %v39959 = vmul.f32 %v39949, inf (stack83)
        %v39961 = vxor.u32 %v39949, 2147483648 (stack87)
        %v39964 = vmul.f32 %v39949, %v39961 (stack83)
        %v39966 = vadd.f32 %v39964, 1.0 (stack88)
        %v39967 = vlog2.pop %v39966 (stack89)
        %v39968 = vmul.f32 %v39967, 0.6931472 (stack90)
        %v39969 = vmul.f32 -0.5, %v39964 (stack91)
        %v39970 = vadd.f32 %v39969, 1.0 (stack92)
        %v39971 = vmul.f32 %v39970, %v39964 (stack93)
        %v39972 = vand.u32 2147483647, %v39964 (stack94)
        %vm39973 = vcmp.lt.f32.partialorder %v39972, 0.0004427343 (stack95)
        %v39974 = vsel /*vm=*/%vm39973, /*on_true_vy=*/%v39971, /*on_false_vx=*/%v39968 (stack96)
        %v39975 = vxor.u32 %v39974, 2147483648 (stack87)
        %vm39978 = vcmp.lt.f32.partialorder %v39975, 5.0 (stack86)
        %v39983 = vsel /*vm=*/%vm39978, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v39987 = vsel /*vm=*/%vm39978, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v39991 = vsel /*vm=*/%vm39978, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v39995 = vsel /*vm=*/%vm39978, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v39999 = vsel /*vm=*/%vm39978, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v40003 = vsel /*vm=*/%vm39978, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v40007 = vsel /*vm=*/%vm39978, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v40011 = vsel /*vm=*/%vm39978, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v40015 = vsel /*vm=*/%vm39978, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v40019 = vadd.f32 %v39975, -2.5 (stack82)
        %v40021 = vrsqrt.pop %v39975 (stack97)
        %v40022 = vmul.f32 %v39975, %v40021 (stack98)
        %vm40023 = vcmp.eq.f32.partialorder %v39975, inf (stack99)
        %v40024 = vsel /*vm=*/%vm40023, /*on_true_vy=*/%v39975, /*on_false_vx=*/%v40022 (stack100)
        %vm40025 = vcmp.eq.f32.partialorder %v39975, 0.0 (stack101)
        %v40026 = vand.u32 %v39975, 2147483648 (stack102)
        %v40027 = vsel /*vm=*/%vm40025, /*on_true_vy=*/%v40026, /*on_false_vx=*/%v40024 (stack103)
        %v40030 = vadd.f32 %v40027, -3.0 (stack82)
        %v40034 = vsel /*vm=*/%vm39978, /*on_true_vy=*/%v40019, /*on_false_vx=*/%v40030 (stack72)
        %v40038 = vmul.f32 %v40015, %v40034 (stack83)
        %v40042 = vadd.f32 %v40011, %v40038 (stack82)
        %v40046 = vmul.f32 %v40042, %v40034 (stack83)
        %v40050 = vadd.f32 %v40007, %v40046 (stack82)
        %v40054 = vmul.f32 %v40050, %v40034 (stack83)
        %v40058 = vadd.f32 %v40003, %v40054 (stack82)
        %v40062 = vmul.f32 %v40058, %v40034 (stack83)
        %v40066 = vadd.f32 %v39999, %v40062 (stack82)
        %v40070 = vmul.f32 %v40066, %v40034 (stack83)
        %v40074 = vadd.f32 %v39995, %v40070 (stack82)
        %v40078 = vmul.f32 %v40074, %v40034 (stack83)
        %v40082 = vadd.f32 %v39991, %v40078 (stack82)
        %v40086 = vmul.f32 %v40082, %v40034 (stack83)
        %v40090 = vadd.f32 %v39987, %v40086 (stack82)
        %v40094 = vmul.f32 %v40090, %v40034 (stack83)
        %v40098 = vadd.f32 %v39983, %v40094 (stack82)
        %v40102 = vmul.f32 %v40098, %v39949 (stack83)
        %v40106 = vsel /*vm=*/%vm39954, /*on_true_vy=*/%v39959, /*on_false_vx=*/%v40102 (stack72)
        %v40110 = vmul.f32 %v40106, 1.4140625 (stack83)
        %s40112 = scalar_lea.vmem %s280, 552 [#allocation0] (stack107)
        %v40113 = vpack.c.bf16 0.0, %v40110 (stack104)
        %40114 = vst [vmem:[%s40112] sm:$0xf] /*vst_source=*/%v40113 (stack105)
        %v40117 = vadd.s32 %v2842, %v37809 (stack65)
        %s40119 = smul.u32 128, %s27 (stack66)
        %v40120 = vlaneseq (stack67)
        %v40121 = vand.u32 %v40120, 127 (stack68)
        %v40122 = vstv %s40119 (stack69)
        %v40123 = vadd.s32 %v40121, %v40122 (stack70)
        %v40127 = vadd.s32 %v40117, %v40123 (stack65)
        %vm40131 = vcmp.lt.u32.totalorder %v40127, %v40117 (stack71)
        %vm40136 = vcmp.lt.u32.totalorder %v40117, %v2842 (stack71)
        %v40141 = vadd.s32 %v2829, %v37792 (stack65)
        %v40145 = vadd.s32 %v40141, 1 (stack65)
        %v40149 = vsel /*vm=*/%vm40136, /*on_true_vy=*/%v40145, /*on_false_vx=*/%v40141 (stack72)
        %v40153 = vadd.s32 %v40149, 1 (stack65)
        %v40157 = vsel /*vm=*/%vm40131, /*on_true_vy=*/%v40153, /*on_false_vx=*/%v40149 (stack72)
        %v40162 = vadd.s32 %v40157, %v10 (stack65)
        %v40166 = vadd.s32 %v40127, %v9 (stack65)
        %v40170 = vadd.s32 %v40162, %v40166 (stack65)
        %v40172 = vshll.u32 %v40166, 13 (stack73)
        %v40173 = vshrl.u32 %v40166, 19 (stack74)
        %v40174 = vor.u32 %v40172, %v40173 (stack75)
        %v40175 = vxor.u32 %v40170, %v40174 (stack76)
        %v40178 = vadd.s32 %v40170, %v40175 (stack65)
        %v40180 = vshll.u32 %v40175, 15 (stack73)
        %v40181 = vshrl.u32 %v40175, 17 (stack74)
        %v40182 = vor.u32 %v40180, %v40181 (stack75)
        %v40183 = vxor.u32 %v40178, %v40182 (stack76)
        %v40186 = vadd.s32 %v40178, %v40183 (stack65)
        %v40188 = vshll.u32 %v40183, 26 (stack73)
        %v40189 = vshrl.u32 %v40183, 6 (stack74)
        %v40190 = vor.u32 %v40188, %v40189 (stack75)
        %v40191 = vxor.u32 %v40186, %v40190 (stack76)
        %v40194 = vadd.s32 %v40186, %v40191 (stack65)
        %v40198 = vadd.s32 %v40194, %v9 (stack65)
        %v40200 = vshll.u32 %v40191, 6 (stack73)
        %v40201 = vshrl.u32 %v40191, 26 (stack74)
        %v40202 = vor.u32 %v40200, %v40201 (stack75)
        %v40203 = vxor.u32 %v40194, %v40202 (stack76)
        %v40206 = vadd.s32 %v40203, %v8 (stack65)
        %v40210 = vadd.s32 %v40206, 1 (stack65)
        %v40214 = vadd.s32 %v40198, %v40210 (stack65)
        %v40216 = vshll.u32 %v40210, 17 (stack73)
        %v40217 = vshrl.u32 %v40210, 15 (stack74)
        %v40218 = vor.u32 %v40216, %v40217 (stack75)
        %v40219 = vxor.u32 %v40214, %v40218 (stack76)
        %v40222 = vadd.s32 %v40214, %v40219 (stack65)
        %v40224 = vshll.u32 %v40219, 29 (stack73)
        %v40225 = vshrl.u32 %v40219, 3 (stack74)
        %v40226 = vor.u32 %v40224, %v40225 (stack75)
        %v40227 = vxor.u32 %v40222, %v40226 (stack76)
        %v40230 = vadd.s32 %v40222, %v40227 (stack65)
        %v40232 = vshll.u32 %v40227, 16 (stack73)
        %v40233 = vshrl.u32 %v40227, 16 (stack74)
        %v40234 = vor.u32 %v40232, %v40233 (stack75)
        %v40235 = vxor.u32 %v40230, %v40234 (stack76)
        %v40238 = vadd.s32 %v40230, %v40235 (stack65)
        %v40242 = vadd.s32 %v40238, %v8 (stack65)
        %v40244 = vshll.u32 %v40235, 24 (stack73)
        %v40245 = vshrl.u32 %v40235, 8 (stack74)
        %v40246 = vor.u32 %v40244, %v40245 (stack75)
        %v40247 = vxor.u32 %v40238, %v40246 (stack76)
        %v40250 = vadd.s32 %v40247, %v10 (stack65)
        %v40254 = vadd.s32 %v40250, 2 (stack65)
        %v40258 = vadd.s32 %v40242, %v40254 (stack65)
        %v40260 = vshll.u32 %v40254, 13 (stack73)
        %v40261 = vshrl.u32 %v40254, 19 (stack74)
        %v40262 = vor.u32 %v40260, %v40261 (stack75)
        %v40263 = vxor.u32 %v40258, %v40262 (stack76)
        %v40266 = vadd.s32 %v40258, %v40263 (stack65)
        %v40268 = vshll.u32 %v40263, 15 (stack73)
        %v40269 = vshrl.u32 %v40263, 17 (stack74)
        %v40270 = vor.u32 %v40268, %v40269 (stack75)
        %v40271 = vxor.u32 %v40266, %v40270 (stack76)
        %v40274 = vadd.s32 %v40266, %v40271 (stack65)
        %v40276 = vshll.u32 %v40271, 26 (stack73)
        %v40277 = vshrl.u32 %v40271, 6 (stack74)
        %v40278 = vor.u32 %v40276, %v40277 (stack75)
        %v40279 = vxor.u32 %v40274, %v40278 (stack76)
        %v40282 = vadd.s32 %v40274, %v40279 (stack65)
        %v40286 = vadd.s32 %v40282, %v10 (stack65)
        %v40288 = vshll.u32 %v40279, 6 (stack73)
        %v40289 = vshrl.u32 %v40279, 26 (stack74)
        %v40290 = vor.u32 %v40288, %v40289 (stack75)
        %v40291 = vxor.u32 %v40282, %v40290 (stack76)
        %v40294 = vadd.s32 %v40291, %v9 (stack65)
        %v40298 = vadd.s32 %v40294, 3 (stack65)
        %v40302 = vadd.s32 %v40286, %v40298 (stack65)
        %v40304 = vshll.u32 %v40298, 17 (stack73)
        %v40305 = vshrl.u32 %v40298, 15 (stack74)
        %v40306 = vor.u32 %v40304, %v40305 (stack75)
        %v40307 = vxor.u32 %v40302, %v40306 (stack76)
        %v40310 = vadd.s32 %v40302, %v40307 (stack65)
        %v40312 = vshll.u32 %v40307, 29 (stack73)
        %v40313 = vshrl.u32 %v40307, 3 (stack74)
        %v40314 = vor.u32 %v40312, %v40313 (stack75)
        %v40315 = vxor.u32 %v40310, %v40314 (stack76)
        %v40318 = vadd.s32 %v40310, %v40315 (stack65)
        %v40320 = vshll.u32 %v40315, 16 (stack73)
        %v40321 = vshrl.u32 %v40315, 16 (stack74)
        %v40322 = vor.u32 %v40320, %v40321 (stack75)
        %v40323 = vxor.u32 %v40318, %v40322 (stack76)
        %v40326 = vadd.s32 %v40318, %v40323 (stack65)
        %v40330 = vadd.s32 %v40326, %v9 (stack65)
        %v40332 = vshll.u32 %v40323, 24 (stack73)
        %v40333 = vshrl.u32 %v40323, 8 (stack74)
        %v40334 = vor.u32 %v40332, %v40333 (stack75)
        %v40335 = vxor.u32 %v40326, %v40334 (stack76)
        %v40338 = vadd.s32 %v40335, %v8 (stack65)
        %v40342 = vadd.s32 %v40338, 4 (stack65)
        %v40346 = vadd.s32 %v40330, %v40342 (stack65)
        %v40348 = vshll.u32 %v40342, 13 (stack73)
        %v40349 = vshrl.u32 %v40342, 19 (stack74)
        %v40350 = vor.u32 %v40348, %v40349 (stack75)
        %v40351 = vxor.u32 %v40346, %v40350 (stack76)
        %v40354 = vadd.s32 %v40346, %v40351 (stack65)
        %v40356 = vshll.u32 %v40351, 15 (stack73)
        %v40357 = vshrl.u32 %v40351, 17 (stack74)
        %v40358 = vor.u32 %v40356, %v40357 (stack75)
        %v40359 = vxor.u32 %v40354, %v40358 (stack76)
        %v40362 = vadd.s32 %v40354, %v40359 (stack65)
        %v40364 = vshll.u32 %v40359, 26 (stack73)
        %v40365 = vshrl.u32 %v40359, 6 (stack74)
        %v40366 = vor.u32 %v40364, %v40365 (stack75)
        %v40367 = vxor.u32 %v40362, %v40366 (stack76)
        %v40370 = vadd.s32 %v40362, %v40367 (stack65)
        %v40374 = vadd.s32 %v40370, %v8 (stack65)
        %v40376 = vshll.u32 %v40367, 6 (stack73)
        %v40377 = vshrl.u32 %v40367, 26 (stack74)
        %v40378 = vor.u32 %v40376, %v40377 (stack75)
        %v40379 = vxor.u32 %v40370, %v40378 (stack76)
        %v40382 = vadd.s32 %v40379, %v10 (stack65)
        %v40386 = vadd.s32 %v40382, 5 (stack65)
        %v40388 = vxor.u32 %v40374, %v40386 (stack76)
        %v40389 = vand.u32.u8 %v40388, 255 (stack77)
        %v40390 = vand.u32 %v40389, 65535 (stack78)
        %v40391 = vshrl.u32 %v40390, 1 (stack79)
        %v40392 = vor.u32 %v40391, 16256 (stack75)
        %v40393 = vand.u32.u16 %v40392, 65535 (stack80)
        %v40394 = vunpack.i.l.bf16 %v40393 (stack81)
        %v40398 = vadd.f32 %v40394, -1.0 (stack82)
        %v40402 = vmul.f32 %v40398, 2.0 (stack83)
        %v40406 = vadd.f32 %v40402, -0.99609375 (stack82)
        %v40410 = vmax.f32 -0.99609375, %v40406 (stack84)
        %v40412 = vand.u32 2147483647, %v40410 (stack85)
        %vm40415 = vcmp.eq.f32.partialorder %v40412, 1.0 (stack86)
        %v40420 = vmul.f32 %v40410, inf (stack83)
        %v40422 = vxor.u32 %v40410, 2147483648 (stack87)
        %v40425 = vmul.f32 %v40410, %v40422 (stack83)
        %v40427 = vadd.f32 %v40425, 1.0 (stack88)
        %v40428 = vlog2.pop %v40427 (stack89)
        %v40429 = vmul.f32 %v40428, 0.6931472 (stack90)
        %v40430 = vmul.f32 -0.5, %v40425 (stack91)
        %v40431 = vadd.f32 %v40430, 1.0 (stack92)
        %v40432 = vmul.f32 %v40431, %v40425 (stack93)
        %v40433 = vand.u32 2147483647, %v40425 (stack94)
        %vm40434 = vcmp.lt.f32.partialorder %v40433, 0.0004427343 (stack95)
        %v40435 = vsel /*vm=*/%vm40434, /*on_true_vy=*/%v40432, /*on_false_vx=*/%v40429 (stack96)
        %v40436 = vxor.u32 %v40435, 2147483648 (stack87)
        %vm40439 = vcmp.lt.f32.partialorder %v40436, 5.0 (stack86)
        %v40444 = vsel /*vm=*/%vm40439, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v40448 = vsel /*vm=*/%vm40439, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v40452 = vsel /*vm=*/%vm40439, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v40456 = vsel /*vm=*/%vm40439, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v40460 = vsel /*vm=*/%vm40439, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v40464 = vsel /*vm=*/%vm40439, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v40468 = vsel /*vm=*/%vm40439, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v40472 = vsel /*vm=*/%vm40439, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v40476 = vsel /*vm=*/%vm40439, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v40480 = vadd.f32 %v40436, -2.5 (stack82)
        %v40482 = vrsqrt.pop %v40436 (stack97)
        %v40483 = vmul.f32 %v40436, %v40482 (stack98)
        %vm40484 = vcmp.eq.f32.partialorder %v40436, inf (stack99)
        %v40485 = vsel /*vm=*/%vm40484, /*on_true_vy=*/%v40436, /*on_false_vx=*/%v40483 (stack100)
        %vm40486 = vcmp.eq.f32.partialorder %v40436, 0.0 (stack101)
        %v40487 = vand.u32 %v40436, 2147483648 (stack102)
        %v40488 = vsel /*vm=*/%vm40486, /*on_true_vy=*/%v40487, /*on_false_vx=*/%v40485 (stack103)
        %v40491 = vadd.f32 %v40488, -3.0 (stack82)
        %v40495 = vsel /*vm=*/%vm40439, /*on_true_vy=*/%v40480, /*on_false_vx=*/%v40491 (stack72)
        %v40499 = vmul.f32 %v40476, %v40495 (stack83)
        %v40503 = vadd.f32 %v40472, %v40499 (stack82)
        %v40507 = vmul.f32 %v40503, %v40495 (stack83)
        %v40511 = vadd.f32 %v40468, %v40507 (stack82)
        %v40515 = vmul.f32 %v40511, %v40495 (stack83)
        %v40519 = vadd.f32 %v40464, %v40515 (stack82)
        %v40523 = vmul.f32 %v40519, %v40495 (stack83)
        %v40527 = vadd.f32 %v40460, %v40523 (stack82)
        %v40531 = vmul.f32 %v40527, %v40495 (stack83)
        %v40535 = vadd.f32 %v40456, %v40531 (stack82)
        %v40539 = vmul.f32 %v40535, %v40495 (stack83)
        %v40543 = vadd.f32 %v40452, %v40539 (stack82)
        %v40547 = vmul.f32 %v40543, %v40495 (stack83)
        %v40551 = vadd.f32 %v40448, %v40547 (stack82)
        %v40555 = vmul.f32 %v40551, %v40495 (stack83)
        %v40559 = vadd.f32 %v40444, %v40555 (stack82)
        %v40563 = vmul.f32 %v40559, %v40410 (stack83)
        %v40567 = vsel /*vm=*/%vm40415, /*on_true_vy=*/%v40420, /*on_false_vx=*/%v40563 (stack72)
        %v40571 = vmul.f32 %v40567, 1.4140625 (stack83)
        %s40573 = scalar_lea.vmem %s280, 680 [#allocation0] (stack107)
        %v40574 = vpack.c.bf16 0.0, %v40571 (stack104)
        %40575 = vst [vmem:[%s40573] sm:$0xf] /*vst_source=*/%v40574 (stack105)
        %v40578 = vadd.s32 %v3329, %v37809 (stack65)
        %s40580 = smul.u32 128, %s27 (stack66)
        %v40581 = vlaneseq (stack67)
        %v40582 = vand.u32 %v40581, 127 (stack68)
        %v40583 = vstv %s40580 (stack69)
        %v40584 = vadd.s32 %v40582, %v40583 (stack70)
        %v40588 = vadd.s32 %v40578, %v40584 (stack65)
        %vm40592 = vcmp.lt.u32.totalorder %v40588, %v40578 (stack71)
        %vm40597 = vcmp.lt.u32.totalorder %v40578, %v3329 (stack71)
        %v40602 = vadd.s32 %v3316, %v37792 (stack65)
        %v40606 = vadd.s32 %v40602, 1 (stack65)
        %v40610 = vsel /*vm=*/%vm40597, /*on_true_vy=*/%v40606, /*on_false_vx=*/%v40602 (stack72)
        %v40614 = vadd.s32 %v40610, 1 (stack65)
        %v40618 = vsel /*vm=*/%vm40592, /*on_true_vy=*/%v40614, /*on_false_vx=*/%v40610 (stack72)
        %v40623 = vadd.s32 %v40618, %v10 (stack65)
        %v40627 = vadd.s32 %v40588, %v9 (stack65)
        %v40631 = vadd.s32 %v40623, %v40627 (stack65)
        %v40633 = vshll.u32 %v40627, 13 (stack73)
        %v40634 = vshrl.u32 %v40627, 19 (stack74)
        %v40635 = vor.u32 %v40633, %v40634 (stack75)
        %v40636 = vxor.u32 %v40631, %v40635 (stack76)
        %v40639 = vadd.s32 %v40631, %v40636 (stack65)
        %v40641 = vshll.u32 %v40636, 15 (stack73)
        %v40642 = vshrl.u32 %v40636, 17 (stack74)
        %v40643 = vor.u32 %v40641, %v40642 (stack75)
        %v40644 = vxor.u32 %v40639, %v40643 (stack76)
        %v40647 = vadd.s32 %v40639, %v40644 (stack65)
        %v40649 = vshll.u32 %v40644, 26 (stack73)
        %v40650 = vshrl.u32 %v40644, 6 (stack74)
        %v40651 = vor.u32 %v40649, %v40650 (stack75)
        %v40652 = vxor.u32 %v40647, %v40651 (stack76)
        %v40655 = vadd.s32 %v40647, %v40652 (stack65)
        %v40659 = vadd.s32 %v40655, %v9 (stack65)
        %v40661 = vshll.u32 %v40652, 6 (stack73)
        %v40662 = vshrl.u32 %v40652, 26 (stack74)
        %v40663 = vor.u32 %v40661, %v40662 (stack75)
        %v40664 = vxor.u32 %v40655, %v40663 (stack76)
        %v40667 = vadd.s32 %v40664, %v8 (stack65)
        %v40671 = vadd.s32 %v40667, 1 (stack65)
        %v40675 = vadd.s32 %v40659, %v40671 (stack65)
        %v40677 = vshll.u32 %v40671, 17 (stack73)
        %v40678 = vshrl.u32 %v40671, 15 (stack74)
        %v40679 = vor.u32 %v40677, %v40678 (stack75)
        %v40680 = vxor.u32 %v40675, %v40679 (stack76)
        %v40683 = vadd.s32 %v40675, %v40680 (stack65)
        %v40685 = vshll.u32 %v40680, 29 (stack73)
        %v40686 = vshrl.u32 %v40680, 3 (stack74)
        %v40687 = vor.u32 %v40685, %v40686 (stack75)
        %v40688 = vxor.u32 %v40683, %v40687 (stack76)
        %v40691 = vadd.s32 %v40683, %v40688 (stack65)
        %v40693 = vshll.u32 %v40688, 16 (stack73)
        %v40694 = vshrl.u32 %v40688, 16 (stack74)
        %v40695 = vor.u32 %v40693, %v40694 (stack75)
        %v40696 = vxor.u32 %v40691, %v40695 (stack76)
        %v40699 = vadd.s32 %v40691, %v40696 (stack65)
        %v40703 = vadd.s32 %v40699, %v8 (stack65)
        %v40705 = vshll.u32 %v40696, 24 (stack73)
        %v40706 = vshrl.u32 %v40696, 8 (stack74)
        %v40707 = vor.u32 %v40705, %v40706 (stack75)
        %v40708 = vxor.u32 %v40699, %v40707 (stack76)
        %v40711 = vadd.s32 %v40708, %v10 (stack65)
        %v40715 = vadd.s32 %v40711, 2 (stack65)
        %v40719 = vadd.s32 %v40703, %v40715 (stack65)
        %v40721 = vshll.u32 %v40715, 13 (stack73)
        %v40722 = vshrl.u32 %v40715, 19 (stack74)
        %v40723 = vor.u32 %v40721, %v40722 (stack75)
        %v40724 = vxor.u32 %v40719, %v40723 (stack76)
        %v40727 = vadd.s32 %v40719, %v40724 (stack65)
        %v40729 = vshll.u32 %v40724, 15 (stack73)
        %v40730 = vshrl.u32 %v40724, 17 (stack74)
        %v40731 = vor.u32 %v40729, %v40730 (stack75)
        %v40732 = vxor.u32 %v40727, %v40731 (stack76)
        %v40735 = vadd.s32 %v40727, %v40732 (stack65)
        %v40737 = vshll.u32 %v40732, 26 (stack73)
        %v40738 = vshrl.u32 %v40732, 6 (stack74)
        %v40739 = vor.u32 %v40737, %v40738 (stack75)
        %v40740 = vxor.u32 %v40735, %v40739 (stack76)
        %v40743 = vadd.s32 %v40735, %v40740 (stack65)
        %v40747 = vadd.s32 %v40743, %v10 (stack65)
        %v40749 = vshll.u32 %v40740, 6 (stack73)
        %v40750 = vshrl.u32 %v40740, 26 (stack74)
        %v40751 = vor.u32 %v40749, %v40750 (stack75)
        %v40752 = vxor.u32 %v40743, %v40751 (stack76)
        %v40755 = vadd.s32 %v40752, %v9 (stack65)
        %v40759 = vadd.s32 %v40755, 3 (stack65)
        %v40763 = vadd.s32 %v40747, %v40759 (stack65)
        %v40765 = vshll.u32 %v40759, 17 (stack73)
        %v40766 = vshrl.u32 %v40759, 15 (stack74)
        %v40767 = vor.u32 %v40765, %v40766 (stack75)
        %v40768 = vxor.u32 %v40763, %v40767 (stack76)
        %v40771 = vadd.s32 %v40763, %v40768 (stack65)
        %v40773 = vshll.u32 %v40768, 29 (stack73)
        %v40774 = vshrl.u32 %v40768, 3 (stack74)
        %v40775 = vor.u32 %v40773, %v40774 (stack75)
        %v40776 = vxor.u32 %v40771, %v40775 (stack76)
        %v40779 = vadd.s32 %v40771, %v40776 (stack65)
        %v40781 = vshll.u32 %v40776, 16 (stack73)
        %v40782 = vshrl.u32 %v40776, 16 (stack74)
        %v40783 = vor.u32 %v40781, %v40782 (stack75)
        %v40784 = vxor.u32 %v40779, %v40783 (stack76)
        %v40787 = vadd.s32 %v40779, %v40784 (stack65)
        %v40791 = vadd.s32 %v40787, %v9 (stack65)
        %v40793 = vshll.u32 %v40784, 24 (stack73)
        %v40794 = vshrl.u32 %v40784, 8 (stack74)
        %v40795 = vor.u32 %v40793, %v40794 (stack75)
        %v40796 = vxor.u32 %v40787, %v40795 (stack76)
        %v40799 = vadd.s32 %v40796, %v8 (stack65)
        %v40803 = vadd.s32 %v40799, 4 (stack65)
        %v40807 = vadd.s32 %v40791, %v40803 (stack65)
        %v40809 = vshll.u32 %v40803, 13 (stack73)
        %v40810 = vshrl.u32 %v40803, 19 (stack74)
        %v40811 = vor.u32 %v40809, %v40810 (stack75)
        %v40812 = vxor.u32 %v40807, %v40811 (stack76)
        %v40815 = vadd.s32 %v40807, %v40812 (stack65)
        %v40817 = vshll.u32 %v40812, 15 (stack73)
        %v40818 = vshrl.u32 %v40812, 17 (stack74)
        %v40819 = vor.u32 %v40817, %v40818 (stack75)
        %v40820 = vxor.u32 %v40815, %v40819 (stack76)
        %v40823 = vadd.s32 %v40815, %v40820 (stack65)
        %v40825 = vshll.u32 %v40820, 26 (stack73)
        %v40826 = vshrl.u32 %v40820, 6 (stack74)
        %v40827 = vor.u32 %v40825, %v40826 (stack75)
        %v40828 = vxor.u32 %v40823, %v40827 (stack76)
        %v40831 = vadd.s32 %v40823, %v40828 (stack65)
        %v40835 = vadd.s32 %v40831, %v8 (stack65)
        %v40837 = vshll.u32 %v40828, 6 (stack73)
        %v40838 = vshrl.u32 %v40828, 26 (stack74)
        %v40839 = vor.u32 %v40837, %v40838 (stack75)
        %v40840 = vxor.u32 %v40831, %v40839 (stack76)
        %v40843 = vadd.s32 %v40840, %v10 (stack65)
        %v40847 = vadd.s32 %v40843, 5 (stack65)
        %v40849 = vxor.u32 %v40835, %v40847 (stack76)
        %v40850 = vand.u32.u8 %v40849, 255 (stack77)
        %v40851 = vand.u32 %v40850, 65535 (stack78)
        %v40852 = vshrl.u32 %v40851, 1 (stack79)
        %v40853 = vor.u32 %v40852, 16256 (stack75)
        %v40854 = vand.u32.u16 %v40853, 65535 (stack80)
        %v40855 = vunpack.i.l.bf16 %v40854 (stack81)
        %v40859 = vadd.f32 %v40855, -1.0 (stack82)
        %v40863 = vmul.f32 %v40859, 2.0 (stack83)
        %v40867 = vadd.f32 %v40863, -0.99609375 (stack82)
        %v40871 = vmax.f32 -0.99609375, %v40867 (stack84)
        %v40873 = vand.u32 2147483647, %v40871 (stack85)
        %vm40876 = vcmp.eq.f32.partialorder %v40873, 1.0 (stack86)
        %v40881 = vmul.f32 %v40871, inf (stack83)
        %v40883 = vxor.u32 %v40871, 2147483648 (stack87)
        %v40886 = vmul.f32 %v40871, %v40883 (stack83)
        %v40888 = vadd.f32 %v40886, 1.0 (stack88)
        %v40889 = vlog2.pop %v40888 (stack89)
        %v40890 = vmul.f32 %v40889, 0.6931472 (stack90)
        %v40891 = vmul.f32 -0.5, %v40886 (stack91)
        %v40892 = vadd.f32 %v40891, 1.0 (stack92)
        %v40893 = vmul.f32 %v40892, %v40886 (stack93)
        %v40894 = vand.u32 2147483647, %v40886 (stack94)
        %vm40895 = vcmp.lt.f32.partialorder %v40894, 0.0004427343 (stack95)
        %v40896 = vsel /*vm=*/%vm40895, /*on_true_vy=*/%v40893, /*on_false_vx=*/%v40890 (stack96)
        %v40897 = vxor.u32 %v40896, 2147483648 (stack87)
        %vm40900 = vcmp.lt.f32.partialorder %v40897, 5.0 (stack86)
        %v40905 = vsel /*vm=*/%vm40900, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v40909 = vsel /*vm=*/%vm40900, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v40913 = vsel /*vm=*/%vm40900, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v40917 = vsel /*vm=*/%vm40900, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v40921 = vsel /*vm=*/%vm40900, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v40925 = vsel /*vm=*/%vm40900, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v40929 = vsel /*vm=*/%vm40900, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v40933 = vsel /*vm=*/%vm40900, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v40937 = vsel /*vm=*/%vm40900, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v40941 = vadd.f32 %v40897, -2.5 (stack82)
        %v40943 = vrsqrt.pop %v40897 (stack97)
        %v40944 = vmul.f32 %v40897, %v40943 (stack98)
        %vm40945 = vcmp.eq.f32.partialorder %v40897, inf (stack99)
        %v40946 = vsel /*vm=*/%vm40945, /*on_true_vy=*/%v40897, /*on_false_vx=*/%v40944 (stack100)
        %vm40947 = vcmp.eq.f32.partialorder %v40897, 0.0 (stack101)
        %v40948 = vand.u32 %v40897, 2147483648 (stack102)
        %v40949 = vsel /*vm=*/%vm40947, /*on_true_vy=*/%v40948, /*on_false_vx=*/%v40946 (stack103)
        %v40952 = vadd.f32 %v40949, -3.0 (stack82)
        %v40956 = vsel /*vm=*/%vm40900, /*on_true_vy=*/%v40941, /*on_false_vx=*/%v40952 (stack72)
        %v40960 = vmul.f32 %v40937, %v40956 (stack83)
        %v40964 = vadd.f32 %v40933, %v40960 (stack82)
        %v40968 = vmul.f32 %v40964, %v40956 (stack83)
        %v40972 = vadd.f32 %v40929, %v40968 (stack82)
        %v40976 = vmul.f32 %v40972, %v40956 (stack83)
        %v40980 = vadd.f32 %v40925, %v40976 (stack82)
        %v40984 = vmul.f32 %v40980, %v40956 (stack83)
        %v40988 = vadd.f32 %v40921, %v40984 (stack82)
        %v40992 = vmul.f32 %v40988, %v40956 (stack83)
        %v40996 = vadd.f32 %v40917, %v40992 (stack82)
        %v41000 = vmul.f32 %v40996, %v40956 (stack83)
        %v41004 = vadd.f32 %v40913, %v41000 (stack82)
        %v41008 = vmul.f32 %v41004, %v40956 (stack83)
        %v41012 = vadd.f32 %v40909, %v41008 (stack82)
        %v41016 = vmul.f32 %v41012, %v40956 (stack83)
        %v41020 = vadd.f32 %v40905, %v41016 (stack82)
        %v41024 = vmul.f32 %v41020, %v40871 (stack83)
        %v41028 = vsel /*vm=*/%vm40876, /*on_true_vy=*/%v40881, /*on_false_vx=*/%v41024 (stack72)
        %v41032 = vmul.f32 %v41028, 1.4140625 (stack83)
        %s41034 = scalar_lea.vmem %s280, 808 [#allocation0] (stack107)
        %v41035 = vpack.c.bf16 0.0, %v41032 (stack104)
        %41036 = vst [vmem:[%s41034] sm:$0xf] /*vst_source=*/%v41035 (stack105)
        %v41039 = vadd.s32 %v3816, %v37809 (stack65)
        %s41041 = smul.u32 128, %s27 (stack66)
        %v41042 = vlaneseq (stack67)
        %v41043 = vand.u32 %v41042, 127 (stack68)
        %v41044 = vstv %s41041 (stack69)
        %v41045 = vadd.s32 %v41043, %v41044 (stack70)
        %v41049 = vadd.s32 %v41039, %v41045 (stack65)
        %vm41053 = vcmp.lt.u32.totalorder %v41049, %v41039 (stack71)
        %vm41058 = vcmp.lt.u32.totalorder %v41039, %v3816 (stack71)
        %v41063 = vadd.s32 %v3803, %v37792 (stack65)
        %v41067 = vadd.s32 %v41063, 1 (stack65)
        %v41071 = vsel /*vm=*/%vm41058, /*on_true_vy=*/%v41067, /*on_false_vx=*/%v41063 (stack72)
        %v41075 = vadd.s32 %v41071, 1 (stack65)
        %v41079 = vsel /*vm=*/%vm41053, /*on_true_vy=*/%v41075, /*on_false_vx=*/%v41071 (stack72)
        %v41084 = vadd.s32 %v41079, %v10 (stack65)
        %v41088 = vadd.s32 %v41049, %v9 (stack65)
        %v41092 = vadd.s32 %v41084, %v41088 (stack65)
        %v41094 = vshll.u32 %v41088, 13 (stack73)
        %v41095 = vshrl.u32 %v41088, 19 (stack74)
        %v41096 = vor.u32 %v41094, %v41095 (stack75)
        %v41097 = vxor.u32 %v41092, %v41096 (stack76)
        %v41100 = vadd.s32 %v41092, %v41097 (stack65)
        %v41102 = vshll.u32 %v41097, 15 (stack73)
        %v41103 = vshrl.u32 %v41097, 17 (stack74)
        %v41104 = vor.u32 %v41102, %v41103 (stack75)
        %v41105 = vxor.u32 %v41100, %v41104 (stack76)
        %v41108 = vadd.s32 %v41100, %v41105 (stack65)
        %v41110 = vshll.u32 %v41105, 26 (stack73)
        %v41111 = vshrl.u32 %v41105, 6 (stack74)
        %v41112 = vor.u32 %v41110, %v41111 (stack75)
        %v41113 = vxor.u32 %v41108, %v41112 (stack76)
        %v41116 = vadd.s32 %v41108, %v41113 (stack65)
        %v41120 = vadd.s32 %v41116, %v9 (stack65)
        %v41122 = vshll.u32 %v41113, 6 (stack73)
        %v41123 = vshrl.u32 %v41113, 26 (stack74)
        %v41124 = vor.u32 %v41122, %v41123 (stack75)
        %v41125 = vxor.u32 %v41116, %v41124 (stack76)
        %v41128 = vadd.s32 %v41125, %v8 (stack65)
        %v41132 = vadd.s32 %v41128, 1 (stack65)
        %v41136 = vadd.s32 %v41120, %v41132 (stack65)
        %v41138 = vshll.u32 %v41132, 17 (stack73)
        %v41139 = vshrl.u32 %v41132, 15 (stack74)
        %v41140 = vor.u32 %v41138, %v41139 (stack75)
        %v41141 = vxor.u32 %v41136, %v41140 (stack76)
        %v41144 = vadd.s32 %v41136, %v41141 (stack65)
        %v41146 = vshll.u32 %v41141, 29 (stack73)
        %v41147 = vshrl.u32 %v41141, 3 (stack74)
        %v41148 = vor.u32 %v41146, %v41147 (stack75)
        %v41149 = vxor.u32 %v41144, %v41148 (stack76)
        %v41152 = vadd.s32 %v41144, %v41149 (stack65)
        %v41154 = vshll.u32 %v41149, 16 (stack73)
        %v41155 = vshrl.u32 %v41149, 16 (stack74)
        %v41156 = vor.u32 %v41154, %v41155 (stack75)
        %v41157 = vxor.u32 %v41152, %v41156 (stack76)
        %v41160 = vadd.s32 %v41152, %v41157 (stack65)
        %v41164 = vadd.s32 %v41160, %v8 (stack65)
        %v41166 = vshll.u32 %v41157, 24 (stack73)
        %v41167 = vshrl.u32 %v41157, 8 (stack74)
        %v41168 = vor.u32 %v41166, %v41167 (stack75)
        %v41169 = vxor.u32 %v41160, %v41168 (stack76)
        %v41172 = vadd.s32 %v41169, %v10 (stack65)
        %v41176 = vadd.s32 %v41172, 2 (stack65)
        %v41180 = vadd.s32 %v41164, %v41176 (stack65)
        %v41182 = vshll.u32 %v41176, 13 (stack73)
        %v41183 = vshrl.u32 %v41176, 19 (stack74)
        %v41184 = vor.u32 %v41182, %v41183 (stack75)
        %v41185 = vxor.u32 %v41180, %v41184 (stack76)
        %v41188 = vadd.s32 %v41180, %v41185 (stack65)
        %v41190 = vshll.u32 %v41185, 15 (stack73)
        %v41191 = vshrl.u32 %v41185, 17 (stack74)
        %v41192 = vor.u32 %v41190, %v41191 (stack75)
        %v41193 = vxor.u32 %v41188, %v41192 (stack76)
        %v41196 = vadd.s32 %v41188, %v41193 (stack65)
        %v41198 = vshll.u32 %v41193, 26 (stack73)
        %v41199 = vshrl.u32 %v41193, 6 (stack74)
        %v41200 = vor.u32 %v41198, %v41199 (stack75)
        %v41201 = vxor.u32 %v41196, %v41200 (stack76)
        %v41204 = vadd.s32 %v41196, %v41201 (stack65)
        %v41208 = vadd.s32 %v41204, %v10 (stack65)
        %v41210 = vshll.u32 %v41201, 6 (stack73)
        %v41211 = vshrl.u32 %v41201, 26 (stack74)
        %v41212 = vor.u32 %v41210, %v41211 (stack75)
        %v41213 = vxor.u32 %v41204, %v41212 (stack76)
        %v41216 = vadd.s32 %v41213, %v9 (stack65)
        %v41220 = vadd.s32 %v41216, 3 (stack65)
        %v41224 = vadd.s32 %v41208, %v41220 (stack65)
        %v41226 = vshll.u32 %v41220, 17 (stack73)
        %v41227 = vshrl.u32 %v41220, 15 (stack74)
        %v41228 = vor.u32 %v41226, %v41227 (stack75)
        %v41229 = vxor.u32 %v41224, %v41228 (stack76)
        %v41232 = vadd.s32 %v41224, %v41229 (stack65)
        %v41234 = vshll.u32 %v41229, 29 (stack73)
        %v41235 = vshrl.u32 %v41229, 3 (stack74)
        %v41236 = vor.u32 %v41234, %v41235 (stack75)
        %v41237 = vxor.u32 %v41232, %v41236 (stack76)
        %v41240 = vadd.s32 %v41232, %v41237 (stack65)
        %v41242 = vshll.u32 %v41237, 16 (stack73)
        %v41243 = vshrl.u32 %v41237, 16 (stack74)
        %v41244 = vor.u32 %v41242, %v41243 (stack75)
        %v41245 = vxor.u32 %v41240, %v41244 (stack76)
        %v41248 = vadd.s32 %v41240, %v41245 (stack65)
        %v41252 = vadd.s32 %v41248, %v9 (stack65)
        %v41254 = vshll.u32 %v41245, 24 (stack73)
        %v41255 = vshrl.u32 %v41245, 8 (stack74)
        %v41256 = vor.u32 %v41254, %v41255 (stack75)
        %v41257 = vxor.u32 %v41248, %v41256 (stack76)
        %v41260 = vadd.s32 %v41257, %v8 (stack65)
        %v41264 = vadd.s32 %v41260, 4 (stack65)
        %v41268 = vadd.s32 %v41252, %v41264 (stack65)
        %v41270 = vshll.u32 %v41264, 13 (stack73)
        %v41271 = vshrl.u32 %v41264, 19 (stack74)
        %v41272 = vor.u32 %v41270, %v41271 (stack75)
        %v41273 = vxor.u32 %v41268, %v41272 (stack76)
        %v41276 = vadd.s32 %v41268, %v41273 (stack65)
        %v41278 = vshll.u32 %v41273, 15 (stack73)
        %v41279 = vshrl.u32 %v41273, 17 (stack74)
        %v41280 = vor.u32 %v41278, %v41279 (stack75)
        %v41281 = vxor.u32 %v41276, %v41280 (stack76)
        %v41284 = vadd.s32 %v41276, %v41281 (stack65)
        %v41286 = vshll.u32 %v41281, 26 (stack73)
        %v41287 = vshrl.u32 %v41281, 6 (stack74)
        %v41288 = vor.u32 %v41286, %v41287 (stack75)
        %v41289 = vxor.u32 %v41284, %v41288 (stack76)
        %v41292 = vadd.s32 %v41284, %v41289 (stack65)
        %v41296 = vadd.s32 %v41292, %v8 (stack65)
        %v41298 = vshll.u32 %v41289, 6 (stack73)
        %v41299 = vshrl.u32 %v41289, 26 (stack74)
        %v41300 = vor.u32 %v41298, %v41299 (stack75)
        %v41301 = vxor.u32 %v41292, %v41300 (stack76)
        %v41304 = vadd.s32 %v41301, %v10 (stack65)
        %v41308 = vadd.s32 %v41304, 5 (stack65)
        %v41310 = vxor.u32 %v41296, %v41308 (stack76)
        %v41311 = vand.u32.u8 %v41310, 255 (stack77)
        %v41312 = vand.u32 %v41311, 65535 (stack78)
        %v41313 = vshrl.u32 %v41312, 1 (stack79)
        %v41314 = vor.u32 %v41313, 16256 (stack75)
        %v41315 = vand.u32.u16 %v41314, 65535 (stack80)
        %v41316 = vunpack.i.l.bf16 %v41315 (stack81)
        %v41320 = vadd.f32 %v41316, -1.0 (stack82)
        %v41324 = vmul.f32 %v41320, 2.0 (stack83)
        %v41328 = vadd.f32 %v41324, -0.99609375 (stack82)
        %v41332 = vmax.f32 -0.99609375, %v41328 (stack84)
        %v41334 = vand.u32 2147483647, %v41332 (stack85)
        %vm41337 = vcmp.eq.f32.partialorder %v41334, 1.0 (stack86)
        %v41342 = vmul.f32 %v41332, inf (stack83)
        %v41344 = vxor.u32 %v41332, 2147483648 (stack87)
        %v41347 = vmul.f32 %v41332, %v41344 (stack83)
        %v41349 = vadd.f32 %v41347, 1.0 (stack88)
        %v41350 = vlog2.pop %v41349 (stack89)
        %v41351 = vmul.f32 %v41350, 0.6931472 (stack90)
        %v41352 = vmul.f32 -0.5, %v41347 (stack91)
        %v41353 = vadd.f32 %v41352, 1.0 (stack92)
        %v41354 = vmul.f32 %v41353, %v41347 (stack93)
        %v41355 = vand.u32 2147483647, %v41347 (stack94)
        %vm41356 = vcmp.lt.f32.partialorder %v41355, 0.0004427343 (stack95)
        %v41357 = vsel /*vm=*/%vm41356, /*on_true_vy=*/%v41354, /*on_false_vx=*/%v41351 (stack96)
        %v41358 = vxor.u32 %v41357, 2147483648 (stack87)
        %vm41361 = vcmp.lt.f32.partialorder %v41358, 5.0 (stack86)
        %v41366 = vsel /*vm=*/%vm41361, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v41370 = vsel /*vm=*/%vm41361, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v41374 = vsel /*vm=*/%vm41361, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v41378 = vsel /*vm=*/%vm41361, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v41382 = vsel /*vm=*/%vm41361, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v41386 = vsel /*vm=*/%vm41361, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v41390 = vsel /*vm=*/%vm41361, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v41394 = vsel /*vm=*/%vm41361, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v41398 = vsel /*vm=*/%vm41361, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v41402 = vadd.f32 %v41358, -2.5 (stack82)
        %v41404 = vrsqrt.pop %v41358 (stack97)
        %v41405 = vmul.f32 %v41358, %v41404 (stack98)
        %vm41406 = vcmp.eq.f32.partialorder %v41358, inf (stack99)
        %v41407 = vsel /*vm=*/%vm41406, /*on_true_vy=*/%v41358, /*on_false_vx=*/%v41405 (stack100)
        %vm41408 = vcmp.eq.f32.partialorder %v41358, 0.0 (stack101)
        %v41409 = vand.u32 %v41358, 2147483648 (stack102)
        %v41410 = vsel /*vm=*/%vm41408, /*on_true_vy=*/%v41409, /*on_false_vx=*/%v41407 (stack103)
        %v41413 = vadd.f32 %v41410, -3.0 (stack82)
        %v41417 = vsel /*vm=*/%vm41361, /*on_true_vy=*/%v41402, /*on_false_vx=*/%v41413 (stack72)
        %v41421 = vmul.f32 %v41398, %v41417 (stack83)
        %v41425 = vadd.f32 %v41394, %v41421 (stack82)
        %v41429 = vmul.f32 %v41425, %v41417 (stack83)
        %v41433 = vadd.f32 %v41390, %v41429 (stack82)
        %v41437 = vmul.f32 %v41433, %v41417 (stack83)
        %v41441 = vadd.f32 %v41386, %v41437 (stack82)
        %v41445 = vmul.f32 %v41441, %v41417 (stack83)
        %v41449 = vadd.f32 %v41382, %v41445 (stack82)
        %v41453 = vmul.f32 %v41449, %v41417 (stack83)
        %v41457 = vadd.f32 %v41378, %v41453 (stack82)
        %v41461 = vmul.f32 %v41457, %v41417 (stack83)
        %v41465 = vadd.f32 %v41374, %v41461 (stack82)
        %v41469 = vmul.f32 %v41465, %v41417 (stack83)
        %v41473 = vadd.f32 %v41370, %v41469 (stack82)
        %v41477 = vmul.f32 %v41473, %v41417 (stack83)
        %v41481 = vadd.f32 %v41366, %v41477 (stack82)
        %v41485 = vmul.f32 %v41481, %v41332 (stack83)
        %v41489 = vsel /*vm=*/%vm41337, /*on_true_vy=*/%v41342, /*on_false_vx=*/%v41485 (stack72)
        %v41493 = vmul.f32 %v41489, 1.4140625 (stack83)
        %s41495 = scalar_lea.vmem %s280, 936 [#allocation0] (stack107)
        %v41496 = vpack.c.bf16 0.0, %v41493 (stack104)
        %41497 = vst [vmem:[%s41495] sm:$0xf] /*vst_source=*/%v41496 (stack105)
        %s41498 = sadd.s32 %s339, 88 (stack106)
        %s41499 = sshrl.u32 %s41498, 10 (stack49)
        %p41500 = scmp.lt.s32.totalorder 1, %s41499 (stack50)
        %s41501 = scalar_select /*predicate=*/%p41500, /*on_true=*/1, /*on_false=*/%s41499 (stack51)
        %s41502 = sand.u32 %s41498, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s41503 = sshrl.u32 %s41502, 7 (stack53)
        %s41504 = sand.u32 %s41502, 127 /* smod.u32 w/div 128 */ (stack54)
        %s41505 = smul.addr %s41501, 8 (stack55)
        %s41506 = scalar_lea.vmem %s3, %s41505 (stack56)
        %s41508 = scalar_lea.vmem %s41506, %s41503 (stack57)
        %v41509 = vld [vmem:[%s41508] ss:$0 sm:$0xff] (stack58)
        %s41510 = sand.u32 %s41504, 255 (stack59)
        %s41512 = sor.u32 256, %s41510 (stack60)
        %41513 = vbcast.lane.b32.xlu0 %v41509, %s41512 (stack61)
        %v41514 = vpop.permute.xlu0 %41513 (stack62)
        %s41515 = sadd.s32 %s347, 88 (stack106)
        %s41516 = sshrl.u32 %s41515, 10 (stack49)
        %p41517 = scmp.lt.s32.totalorder 1, %s41516 (stack50)
        %s41518 = scalar_select /*predicate=*/%p41517, /*on_true=*/1, /*on_false=*/%s41516 (stack51)
        %s41519 = sand.u32 %s41515, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s41520 = sshrl.u32 %s41519, 7 (stack53)
        %s41521 = sand.u32 %s41519, 127 /* smod.u32 w/div 128 */ (stack54)
        %s41522 = smul.addr %s41518, 8 (stack55)
        %s41523 = scalar_lea.vmem %s5, %s41522 (stack56)
        %s41525 = scalar_lea.vmem %s41523, %s41520 (stack57)
        %v41526 = vld [vmem:[%s41525] ss:$0 sm:$0xff] (stack58)
        %s41527 = sand.u32 %s41521, 255 (stack59)
        %s41529 = sor.u32 256, %s41527 (stack60)
        %41530 = vbcast.lane.b32.xlu0 %v41526, %s41529 (stack61)
        %v41531 = vpop.permute.xlu0 %41530 (stack62)
        %v41534 = vadd.s32 %v408, %v41531 (stack65)
        %s41536 = smul.u32 128, %s27 (stack66)
        %v41537 = vlaneseq (stack67)
        %v41538 = vand.u32 %v41537, 127 (stack68)
        %v41539 = vstv %s41536 (stack69)
        %v41540 = vadd.s32 %v41538, %v41539 (stack70)
        %v41544 = vadd.s32 %v41534, %v41540 (stack65)
        %vm41548 = vcmp.lt.u32.totalorder %v41544, %v41534 (stack71)
        %vm41553 = vcmp.lt.u32.totalorder %v41534, %v408 (stack71)
        %v41558 = vadd.s32 %v380, %v41514 (stack65)
        %v41562 = vadd.s32 %v41558, 1 (stack65)
        %v41566 = vsel /*vm=*/%vm41553, /*on_true_vy=*/%v41562, /*on_false_vx=*/%v41558 (stack72)
        %v41570 = vadd.s32 %v41566, 1 (stack65)
        %v41574 = vsel /*vm=*/%vm41548, /*on_true_vy=*/%v41570, /*on_false_vx=*/%v41566 (stack72)
        %v41579 = vadd.s32 %v41574, %v10 (stack65)
        %v41583 = vadd.s32 %v41544, %v9 (stack65)
        %v41587 = vadd.s32 %v41579, %v41583 (stack65)
        %v41589 = vshll.u32 %v41583, 13 (stack73)
        %v41590 = vshrl.u32 %v41583, 19 (stack74)
        %v41591 = vor.u32 %v41589, %v41590 (stack75)
        %v41592 = vxor.u32 %v41587, %v41591 (stack76)
        %v41595 = vadd.s32 %v41587, %v41592 (stack65)
        %v41597 = vshll.u32 %v41592, 15 (stack73)
        %v41598 = vshrl.u32 %v41592, 17 (stack74)
        %v41599 = vor.u32 %v41597, %v41598 (stack75)
        %v41600 = vxor.u32 %v41595, %v41599 (stack76)
        %v41603 = vadd.s32 %v41595, %v41600 (stack65)
        %v41605 = vshll.u32 %v41600, 26 (stack73)
        %v41606 = vshrl.u32 %v41600, 6 (stack74)
        %v41607 = vor.u32 %v41605, %v41606 (stack75)
        %v41608 = vxor.u32 %v41603, %v41607 (stack76)
        %v41611 = vadd.s32 %v41603, %v41608 (stack65)
        %v41615 = vadd.s32 %v41611, %v9 (stack65)
        %v41617 = vshll.u32 %v41608, 6 (stack73)
        %v41618 = vshrl.u32 %v41608, 26 (stack74)
        %v41619 = vor.u32 %v41617, %v41618 (stack75)
        %v41620 = vxor.u32 %v41611, %v41619 (stack76)
        %v41623 = vadd.s32 %v41620, %v8 (stack65)
        %v41627 = vadd.s32 %v41623, 1 (stack65)
        %v41631 = vadd.s32 %v41615, %v41627 (stack65)
        %v41633 = vshll.u32 %v41627, 17 (stack73)
        %v41634 = vshrl.u32 %v41627, 15 (stack74)
        %v41635 = vor.u32 %v41633, %v41634 (stack75)
        %v41636 = vxor.u32 %v41631, %v41635 (stack76)
        %v41639 = vadd.s32 %v41631, %v41636 (stack65)
        %v41641 = vshll.u32 %v41636, 29 (stack73)
        %v41642 = vshrl.u32 %v41636, 3 (stack74)
        %v41643 = vor.u32 %v41641, %v41642 (stack75)
        %v41644 = vxor.u32 %v41639, %v41643 (stack76)
        %v41647 = vadd.s32 %v41639, %v41644 (stack65)
        %v41649 = vshll.u32 %v41644, 16 (stack73)
        %v41650 = vshrl.u32 %v41644, 16 (stack74)
        %v41651 = vor.u32 %v41649, %v41650 (stack75)
        %v41652 = vxor.u32 %v41647, %v41651 (stack76)
        %v41655 = vadd.s32 %v41647, %v41652 (stack65)
        %v41659 = vadd.s32 %v41655, %v8 (stack65)
        %v41661 = vshll.u32 %v41652, 24 (stack73)
        %v41662 = vshrl.u32 %v41652, 8 (stack74)
        %v41663 = vor.u32 %v41661, %v41662 (stack75)
        %v41664 = vxor.u32 %v41655, %v41663 (stack76)
        %v41667 = vadd.s32 %v41664, %v10 (stack65)
        %v41671 = vadd.s32 %v41667, 2 (stack65)
        %v41675 = vadd.s32 %v41659, %v41671 (stack65)
        %v41677 = vshll.u32 %v41671, 13 (stack73)
        %v41678 = vshrl.u32 %v41671, 19 (stack74)
        %v41679 = vor.u32 %v41677, %v41678 (stack75)
        %v41680 = vxor.u32 %v41675, %v41679 (stack76)
        %v41683 = vadd.s32 %v41675, %v41680 (stack65)
        %v41685 = vshll.u32 %v41680, 15 (stack73)
        %v41686 = vshrl.u32 %v41680, 17 (stack74)
        %v41687 = vor.u32 %v41685, %v41686 (stack75)
        %v41688 = vxor.u32 %v41683, %v41687 (stack76)
        %v41691 = vadd.s32 %v41683, %v41688 (stack65)
        %v41693 = vshll.u32 %v41688, 26 (stack73)
        %v41694 = vshrl.u32 %v41688, 6 (stack74)
        %v41695 = vor.u32 %v41693, %v41694 (stack75)
        %v41696 = vxor.u32 %v41691, %v41695 (stack76)
        %v41699 = vadd.s32 %v41691, %v41696 (stack65)
        %v41703 = vadd.s32 %v41699, %v10 (stack65)
        %v41705 = vshll.u32 %v41696, 6 (stack73)
        %v41706 = vshrl.u32 %v41696, 26 (stack74)
        %v41707 = vor.u32 %v41705, %v41706 (stack75)
        %v41708 = vxor.u32 %v41699, %v41707 (stack76)
        %v41711 = vadd.s32 %v41708, %v9 (stack65)
        %v41715 = vadd.s32 %v41711, 3 (stack65)
        %v41719 = vadd.s32 %v41703, %v41715 (stack65)
        %v41721 = vshll.u32 %v41715, 17 (stack73)
        %v41722 = vshrl.u32 %v41715, 15 (stack74)
        %v41723 = vor.u32 %v41721, %v41722 (stack75)
        %v41724 = vxor.u32 %v41719, %v41723 (stack76)
        %v41727 = vadd.s32 %v41719, %v41724 (stack65)
        %v41729 = vshll.u32 %v41724, 29 (stack73)
        %v41730 = vshrl.u32 %v41724, 3 (stack74)
        %v41731 = vor.u32 %v41729, %v41730 (stack75)
        %v41732 = vxor.u32 %v41727, %v41731 (stack76)
        %v41735 = vadd.s32 %v41727, %v41732 (stack65)
        %v41737 = vshll.u32 %v41732, 16 (stack73)
        %v41738 = vshrl.u32 %v41732, 16 (stack74)
        %v41739 = vor.u32 %v41737, %v41738 (stack75)
        %v41740 = vxor.u32 %v41735, %v41739 (stack76)
        %v41743 = vadd.s32 %v41735, %v41740 (stack65)
        %v41747 = vadd.s32 %v41743, %v9 (stack65)
        %v41749 = vshll.u32 %v41740, 24 (stack73)
        %v41750 = vshrl.u32 %v41740, 8 (stack74)
        %v41751 = vor.u32 %v41749, %v41750 (stack75)
        %v41752 = vxor.u32 %v41743, %v41751 (stack76)
        %v41755 = vadd.s32 %v41752, %v8 (stack65)
        %v41759 = vadd.s32 %v41755, 4 (stack65)
        %v41763 = vadd.s32 %v41747, %v41759 (stack65)
        %v41765 = vshll.u32 %v41759, 13 (stack73)
        %v41766 = vshrl.u32 %v41759, 19 (stack74)
        %v41767 = vor.u32 %v41765, %v41766 (stack75)
        %v41768 = vxor.u32 %v41763, %v41767 (stack76)
        %v41771 = vadd.s32 %v41763, %v41768 (stack65)
        %v41773 = vshll.u32 %v41768, 15 (stack73)
        %v41774 = vshrl.u32 %v41768, 17 (stack74)
        %v41775 = vor.u32 %v41773, %v41774 (stack75)
        %v41776 = vxor.u32 %v41771, %v41775 (stack76)
        %v41779 = vadd.s32 %v41771, %v41776 (stack65)
        %v41781 = vshll.u32 %v41776, 26 (stack73)
        %v41782 = vshrl.u32 %v41776, 6 (stack74)
        %v41783 = vor.u32 %v41781, %v41782 (stack75)
        %v41784 = vxor.u32 %v41779, %v41783 (stack76)
        %v41787 = vadd.s32 %v41779, %v41784 (stack65)
        %v41791 = vadd.s32 %v41787, %v8 (stack65)
        %v41793 = vshll.u32 %v41784, 6 (stack73)
        %v41794 = vshrl.u32 %v41784, 26 (stack74)
        %v41795 = vor.u32 %v41793, %v41794 (stack75)
        %v41796 = vxor.u32 %v41787, %v41795 (stack76)
        %v41799 = vadd.s32 %v41796, %v10 (stack65)
        %v41803 = vadd.s32 %v41799, 5 (stack65)
        %v41805 = vxor.u32 %v41791, %v41803 (stack76)
        %v41806 = vand.u32.u8 %v41805, 255 (stack77)
        %v41807 = vand.u32 %v41806, 65535 (stack78)
        %v41808 = vshrl.u32 %v41807, 1 (stack79)
        %v41809 = vor.u32 %v41808, 16256 (stack75)
        %v41810 = vand.u32.u16 %v41809, 65535 (stack80)
        %v41811 = vunpack.i.l.bf16 %v41810 (stack81)
        %v41815 = vadd.f32 %v41811, -1.0 (stack82)
        %v41819 = vmul.f32 %v41815, 2.0 (stack83)
        %v41823 = vadd.f32 %v41819, -0.99609375 (stack82)
        %v41827 = vmax.f32 -0.99609375, %v41823 (stack84)
        %v41829 = vand.u32 2147483647, %v41827 (stack85)
        %vm41832 = vcmp.eq.f32.partialorder %v41829, 1.0 (stack86)
        %v41837 = vmul.f32 %v41827, inf (stack83)
        %v41839 = vxor.u32 %v41827, 2147483648 (stack87)
        %v41842 = vmul.f32 %v41827, %v41839 (stack83)
        %v41844 = vadd.f32 %v41842, 1.0 (stack88)
        %v41845 = vlog2.pop %v41844 (stack89)
        %v41846 = vmul.f32 %v41845, 0.6931472 (stack90)
        %v41847 = vmul.f32 -0.5, %v41842 (stack91)
        %v41848 = vadd.f32 %v41847, 1.0 (stack92)
        %v41849 = vmul.f32 %v41848, %v41842 (stack93)
        %v41850 = vand.u32 2147483647, %v41842 (stack94)
        %vm41851 = vcmp.lt.f32.partialorder %v41850, 0.0004427343 (stack95)
        %v41852 = vsel /*vm=*/%vm41851, /*on_true_vy=*/%v41849, /*on_false_vx=*/%v41846 (stack96)
        %v41853 = vxor.u32 %v41852, 2147483648 (stack87)
        %vm41856 = vcmp.lt.f32.partialorder %v41853, 5.0 (stack86)
        %v41861 = vsel /*vm=*/%vm41856, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v41865 = vsel /*vm=*/%vm41856, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v41869 = vsel /*vm=*/%vm41856, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v41873 = vsel /*vm=*/%vm41856, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v41877 = vsel /*vm=*/%vm41856, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v41881 = vsel /*vm=*/%vm41856, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v41885 = vsel /*vm=*/%vm41856, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v41889 = vsel /*vm=*/%vm41856, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v41893 = vsel /*vm=*/%vm41856, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v41897 = vadd.f32 %v41853, -2.5 (stack82)
        %v41899 = vrsqrt.pop %v41853 (stack97)
        %v41900 = vmul.f32 %v41853, %v41899 (stack98)
        %vm41901 = vcmp.eq.f32.partialorder %v41853, inf (stack99)
        %v41902 = vsel /*vm=*/%vm41901, /*on_true_vy=*/%v41853, /*on_false_vx=*/%v41900 (stack100)
        %vm41903 = vcmp.eq.f32.partialorder %v41853, 0.0 (stack101)
        %v41904 = vand.u32 %v41853, 2147483648 (stack102)
        %v41905 = vsel /*vm=*/%vm41903, /*on_true_vy=*/%v41904, /*on_false_vx=*/%v41902 (stack103)
        %v41908 = vadd.f32 %v41905, -3.0 (stack82)
        %v41912 = vsel /*vm=*/%vm41856, /*on_true_vy=*/%v41897, /*on_false_vx=*/%v41908 (stack72)
        %v41916 = vmul.f32 %v41893, %v41912 (stack83)
        %v41920 = vadd.f32 %v41889, %v41916 (stack82)
        %v41924 = vmul.f32 %v41920, %v41912 (stack83)
        %v41928 = vadd.f32 %v41885, %v41924 (stack82)
        %v41932 = vmul.f32 %v41928, %v41912 (stack83)
        %v41936 = vadd.f32 %v41881, %v41932 (stack82)
        %v41940 = vmul.f32 %v41936, %v41912 (stack83)
        %v41944 = vadd.f32 %v41877, %v41940 (stack82)
        %v41948 = vmul.f32 %v41944, %v41912 (stack83)
        %v41952 = vadd.f32 %v41873, %v41948 (stack82)
        %v41956 = vmul.f32 %v41952, %v41912 (stack83)
        %v41960 = vadd.f32 %v41869, %v41956 (stack82)
        %v41964 = vmul.f32 %v41960, %v41912 (stack83)
        %v41968 = vadd.f32 %v41865, %v41964 (stack82)
        %v41972 = vmul.f32 %v41968, %v41912 (stack83)
        %v41976 = vadd.f32 %v41861, %v41972 (stack82)
        %v41980 = vmul.f32 %v41976, %v41827 (stack83)
        %v41984 = vsel /*vm=*/%vm41832, /*on_true_vy=*/%v41837, /*on_false_vx=*/%v41980 (stack72)
        %v41988 = vmul.f32 %v41984, 1.4140625 (stack83)
        %s41990 = scalar_lea.vmem %s280, 44 [#allocation0] (stack107)
        %v41991 = vpack.c.bf16 0.0, %v41988 (stack104)
        %41992 = vst [vmem:[%s41990] sm:$0xf] /*vst_source=*/%v41991 (stack105)
        %v41995 = vadd.s32 %v894, %v41531 (stack65)
        %s41997 = smul.u32 128, %s27 (stack66)
        %v41998 = vlaneseq (stack67)
        %v41999 = vand.u32 %v41998, 127 (stack68)
        %v42000 = vstv %s41997 (stack69)
        %v42001 = vadd.s32 %v41999, %v42000 (stack70)
        %v42005 = vadd.s32 %v41995, %v42001 (stack65)
        %vm42009 = vcmp.lt.u32.totalorder %v42005, %v41995 (stack71)
        %vm42014 = vcmp.lt.u32.totalorder %v41995, %v894 (stack71)
        %v42019 = vadd.s32 %v881, %v41514 (stack65)
        %v42023 = vadd.s32 %v42019, 1 (stack65)
        %v42027 = vsel /*vm=*/%vm42014, /*on_true_vy=*/%v42023, /*on_false_vx=*/%v42019 (stack72)
        %v42031 = vadd.s32 %v42027, 1 (stack65)
        %v42035 = vsel /*vm=*/%vm42009, /*on_true_vy=*/%v42031, /*on_false_vx=*/%v42027 (stack72)
        %v42040 = vadd.s32 %v42035, %v10 (stack65)
        %v42044 = vadd.s32 %v42005, %v9 (stack65)
        %v42048 = vadd.s32 %v42040, %v42044 (stack65)
        %v42050 = vshll.u32 %v42044, 13 (stack73)
        %v42051 = vshrl.u32 %v42044, 19 (stack74)
        %v42052 = vor.u32 %v42050, %v42051 (stack75)
        %v42053 = vxor.u32 %v42048, %v42052 (stack76)
        %v42056 = vadd.s32 %v42048, %v42053 (stack65)
        %v42058 = vshll.u32 %v42053, 15 (stack73)
        %v42059 = vshrl.u32 %v42053, 17 (stack74)
        %v42060 = vor.u32 %v42058, %v42059 (stack75)
        %v42061 = vxor.u32 %v42056, %v42060 (stack76)
        %v42064 = vadd.s32 %v42056, %v42061 (stack65)
        %v42066 = vshll.u32 %v42061, 26 (stack73)
        %v42067 = vshrl.u32 %v42061, 6 (stack74)
        %v42068 = vor.u32 %v42066, %v42067 (stack75)
        %v42069 = vxor.u32 %v42064, %v42068 (stack76)
        %v42072 = vadd.s32 %v42064, %v42069 (stack65)
        %v42076 = vadd.s32 %v42072, %v9 (stack65)
        %v42078 = vshll.u32 %v42069, 6 (stack73)
        %v42079 = vshrl.u32 %v42069, 26 (stack74)
        %v42080 = vor.u32 %v42078, %v42079 (stack75)
        %v42081 = vxor.u32 %v42072, %v42080 (stack76)
        %v42084 = vadd.s32 %v42081, %v8 (stack65)
        %v42088 = vadd.s32 %v42084, 1 (stack65)
        %v42092 = vadd.s32 %v42076, %v42088 (stack65)
        %v42094 = vshll.u32 %v42088, 17 (stack73)
        %v42095 = vshrl.u32 %v42088, 15 (stack74)
        %v42096 = vor.u32 %v42094, %v42095 (stack75)
        %v42097 = vxor.u32 %v42092, %v42096 (stack76)
        %v42100 = vadd.s32 %v42092, %v42097 (stack65)
        %v42102 = vshll.u32 %v42097, 29 (stack73)
        %v42103 = vshrl.u32 %v42097, 3 (stack74)
        %v42104 = vor.u32 %v42102, %v42103 (stack75)
        %v42105 = vxor.u32 %v42100, %v42104 (stack76)
        %v42108 = vadd.s32 %v42100, %v42105 (stack65)
        %v42110 = vshll.u32 %v42105, 16 (stack73)
        %v42111 = vshrl.u32 %v42105, 16 (stack74)
        %v42112 = vor.u32 %v42110, %v42111 (stack75)
        %v42113 = vxor.u32 %v42108, %v42112 (stack76)
        %v42116 = vadd.s32 %v42108, %v42113 (stack65)
        %v42120 = vadd.s32 %v42116, %v8 (stack65)
        %v42122 = vshll.u32 %v42113, 24 (stack73)
        %v42123 = vshrl.u32 %v42113, 8 (stack74)
        %v42124 = vor.u32 %v42122, %v42123 (stack75)
        %v42125 = vxor.u32 %v42116, %v42124 (stack76)
        %v42128 = vadd.s32 %v42125, %v10 (stack65)
        %v42132 = vadd.s32 %v42128, 2 (stack65)
        %v42136 = vadd.s32 %v42120, %v42132 (stack65)
        %v42138 = vshll.u32 %v42132, 13 (stack73)
        %v42139 = vshrl.u32 %v42132, 19 (stack74)
        %v42140 = vor.u32 %v42138, %v42139 (stack75)
        %v42141 = vxor.u32 %v42136, %v42140 (stack76)
        %v42144 = vadd.s32 %v42136, %v42141 (stack65)
        %v42146 = vshll.u32 %v42141, 15 (stack73)
        %v42147 = vshrl.u32 %v42141, 17 (stack74)
        %v42148 = vor.u32 %v42146, %v42147 (stack75)
        %v42149 = vxor.u32 %v42144, %v42148 (stack76)
        %v42152 = vadd.s32 %v42144, %v42149 (stack65)
        %v42154 = vshll.u32 %v42149, 26 (stack73)
        %v42155 = vshrl.u32 %v42149, 6 (stack74)
        %v42156 = vor.u32 %v42154, %v42155 (stack75)
        %v42157 = vxor.u32 %v42152, %v42156 (stack76)
        %v42160 = vadd.s32 %v42152, %v42157 (stack65)
        %v42164 = vadd.s32 %v42160, %v10 (stack65)
        %v42166 = vshll.u32 %v42157, 6 (stack73)
        %v42167 = vshrl.u32 %v42157, 26 (stack74)
        %v42168 = vor.u32 %v42166, %v42167 (stack75)
        %v42169 = vxor.u32 %v42160, %v42168 (stack76)
        %v42172 = vadd.s32 %v42169, %v9 (stack65)
        %v42176 = vadd.s32 %v42172, 3 (stack65)
        %v42180 = vadd.s32 %v42164, %v42176 (stack65)
        %v42182 = vshll.u32 %v42176, 17 (stack73)
        %v42183 = vshrl.u32 %v42176, 15 (stack74)
        %v42184 = vor.u32 %v42182, %v42183 (stack75)
        %v42185 = vxor.u32 %v42180, %v42184 (stack76)
        %v42188 = vadd.s32 %v42180, %v42185 (stack65)
        %v42190 = vshll.u32 %v42185, 29 (stack73)
        %v42191 = vshrl.u32 %v42185, 3 (stack74)
        %v42192 = vor.u32 %v42190, %v42191 (stack75)
        %v42193 = vxor.u32 %v42188, %v42192 (stack76)
        %v42196 = vadd.s32 %v42188, %v42193 (stack65)
        %v42198 = vshll.u32 %v42193, 16 (stack73)
        %v42199 = vshrl.u32 %v42193, 16 (stack74)
        %v42200 = vor.u32 %v42198, %v42199 (stack75)
        %v42201 = vxor.u32 %v42196, %v42200 (stack76)
        %v42204 = vadd.s32 %v42196, %v42201 (stack65)
        %v42208 = vadd.s32 %v42204, %v9 (stack65)
        %v42210 = vshll.u32 %v42201, 24 (stack73)
        %v42211 = vshrl.u32 %v42201, 8 (stack74)
        %v42212 = vor.u32 %v42210, %v42211 (stack75)
        %v42213 = vxor.u32 %v42204, %v42212 (stack76)
        %v42216 = vadd.s32 %v42213, %v8 (stack65)
        %v42220 = vadd.s32 %v42216, 4 (stack65)
        %v42224 = vadd.s32 %v42208, %v42220 (stack65)
        %v42226 = vshll.u32 %v42220, 13 (stack73)
        %v42227 = vshrl.u32 %v42220, 19 (stack74)
        %v42228 = vor.u32 %v42226, %v42227 (stack75)
        %v42229 = vxor.u32 %v42224, %v42228 (stack76)
        %v42232 = vadd.s32 %v42224, %v42229 (stack65)
        %v42234 = vshll.u32 %v42229, 15 (stack73)
        %v42235 = vshrl.u32 %v42229, 17 (stack74)
        %v42236 = vor.u32 %v42234, %v42235 (stack75)
        %v42237 = vxor.u32 %v42232, %v42236 (stack76)
        %v42240 = vadd.s32 %v42232, %v42237 (stack65)
        %v42242 = vshll.u32 %v42237, 26 (stack73)
        %v42243 = vshrl.u32 %v42237, 6 (stack74)
        %v42244 = vor.u32 %v42242, %v42243 (stack75)
        %v42245 = vxor.u32 %v42240, %v42244 (stack76)
        %v42248 = vadd.s32 %v42240, %v42245 (stack65)
        %v42252 = vadd.s32 %v42248, %v8 (stack65)
        %v42254 = vshll.u32 %v42245, 6 (stack73)
        %v42255 = vshrl.u32 %v42245, 26 (stack74)
        %v42256 = vor.u32 %v42254, %v42255 (stack75)
        %v42257 = vxor.u32 %v42248, %v42256 (stack76)
        %v42260 = vadd.s32 %v42257, %v10 (stack65)
        %v42264 = vadd.s32 %v42260, 5 (stack65)
        %v42266 = vxor.u32 %v42252, %v42264 (stack76)
        %v42267 = vand.u32.u8 %v42266, 255 (stack77)
        %v42268 = vand.u32 %v42267, 65535 (stack78)
        %v42269 = vshrl.u32 %v42268, 1 (stack79)
        %v42270 = vor.u32 %v42269, 16256 (stack75)
        %v42271 = vand.u32.u16 %v42270, 65535 (stack80)
        %v42272 = vunpack.i.l.bf16 %v42271 (stack81)
        %v42276 = vadd.f32 %v42272, -1.0 (stack82)
        %v42280 = vmul.f32 %v42276, 2.0 (stack83)
        %v42284 = vadd.f32 %v42280, -0.99609375 (stack82)
        %v42288 = vmax.f32 -0.99609375, %v42284 (stack84)
        %v42290 = vand.u32 2147483647, %v42288 (stack85)
        %vm42293 = vcmp.eq.f32.partialorder %v42290, 1.0 (stack86)
        %v42298 = vmul.f32 %v42288, inf (stack83)
        %v42300 = vxor.u32 %v42288, 2147483648 (stack87)
        %v42303 = vmul.f32 %v42288, %v42300 (stack83)
        %v42305 = vadd.f32 %v42303, 1.0 (stack88)
        %v42306 = vlog2.pop %v42305 (stack89)
        %v42307 = vmul.f32 %v42306, 0.6931472 (stack90)
        %v42308 = vmul.f32 -0.5, %v42303 (stack91)
        %v42309 = vadd.f32 %v42308, 1.0 (stack92)
        %v42310 = vmul.f32 %v42309, %v42303 (stack93)
        %v42311 = vand.u32 2147483647, %v42303 (stack94)
        %vm42312 = vcmp.lt.f32.partialorder %v42311, 0.0004427343 (stack95)
        %v42313 = vsel /*vm=*/%vm42312, /*on_true_vy=*/%v42310, /*on_false_vx=*/%v42307 (stack96)
        %v42314 = vxor.u32 %v42313, 2147483648 (stack87)
        %vm42317 = vcmp.lt.f32.partialorder %v42314, 5.0 (stack86)
        %v42322 = vsel /*vm=*/%vm42317, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v42326 = vsel /*vm=*/%vm42317, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v42330 = vsel /*vm=*/%vm42317, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v42334 = vsel /*vm=*/%vm42317, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v42338 = vsel /*vm=*/%vm42317, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v42342 = vsel /*vm=*/%vm42317, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v42346 = vsel /*vm=*/%vm42317, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v42350 = vsel /*vm=*/%vm42317, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v42354 = vsel /*vm=*/%vm42317, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v42358 = vadd.f32 %v42314, -2.5 (stack82)
        %v42360 = vrsqrt.pop %v42314 (stack97)
        %v42361 = vmul.f32 %v42314, %v42360 (stack98)
        %vm42362 = vcmp.eq.f32.partialorder %v42314, inf (stack99)
        %v42363 = vsel /*vm=*/%vm42362, /*on_true_vy=*/%v42314, /*on_false_vx=*/%v42361 (stack100)
        %vm42364 = vcmp.eq.f32.partialorder %v42314, 0.0 (stack101)
        %v42365 = vand.u32 %v42314, 2147483648 (stack102)
        %v42366 = vsel /*vm=*/%vm42364, /*on_true_vy=*/%v42365, /*on_false_vx=*/%v42363 (stack103)
        %v42369 = vadd.f32 %v42366, -3.0 (stack82)
        %v42373 = vsel /*vm=*/%vm42317, /*on_true_vy=*/%v42358, /*on_false_vx=*/%v42369 (stack72)
        %v42377 = vmul.f32 %v42354, %v42373 (stack83)
        %v42381 = vadd.f32 %v42350, %v42377 (stack82)
        %v42385 = vmul.f32 %v42381, %v42373 (stack83)
        %v42389 = vadd.f32 %v42346, %v42385 (stack82)
        %v42393 = vmul.f32 %v42389, %v42373 (stack83)
        %v42397 = vadd.f32 %v42342, %v42393 (stack82)
        %v42401 = vmul.f32 %v42397, %v42373 (stack83)
        %v42405 = vadd.f32 %v42338, %v42401 (stack82)
        %v42409 = vmul.f32 %v42405, %v42373 (stack83)
        %v42413 = vadd.f32 %v42334, %v42409 (stack82)
        %v42417 = vmul.f32 %v42413, %v42373 (stack83)
        %v42421 = vadd.f32 %v42330, %v42417 (stack82)
        %v42425 = vmul.f32 %v42421, %v42373 (stack83)
        %v42429 = vadd.f32 %v42326, %v42425 (stack82)
        %v42433 = vmul.f32 %v42429, %v42373 (stack83)
        %v42437 = vadd.f32 %v42322, %v42433 (stack82)
        %v42441 = vmul.f32 %v42437, %v42288 (stack83)
        %v42445 = vsel /*vm=*/%vm42293, /*on_true_vy=*/%v42298, /*on_false_vx=*/%v42441 (stack72)
        %v42449 = vmul.f32 %v42445, 1.4140625 (stack83)
        %s42451 = scalar_lea.vmem %s280, 172 [#allocation0] (stack107)
        %v42452 = vpack.c.bf16 0.0, %v42449 (stack104)
        %42453 = vst [vmem:[%s42451] sm:$0xf] /*vst_source=*/%v42452 (stack105)
        %v42456 = vadd.s32 %v1381, %v41531 (stack65)
        %s42458 = smul.u32 128, %s27 (stack66)
        %v42459 = vlaneseq (stack67)
        %v42460 = vand.u32 %v42459, 127 (stack68)
        %v42461 = vstv %s42458 (stack69)
        %v42462 = vadd.s32 %v42460, %v42461 (stack70)
        %v42466 = vadd.s32 %v42456, %v42462 (stack65)
        %vm42470 = vcmp.lt.u32.totalorder %v42466, %v42456 (stack71)
        %vm42475 = vcmp.lt.u32.totalorder %v42456, %v1381 (stack71)
        %v42480 = vadd.s32 %v1368, %v41514 (stack65)
        %v42484 = vadd.s32 %v42480, 1 (stack65)
        %v42488 = vsel /*vm=*/%vm42475, /*on_true_vy=*/%v42484, /*on_false_vx=*/%v42480 (stack72)
        %v42492 = vadd.s32 %v42488, 1 (stack65)
        %v42496 = vsel /*vm=*/%vm42470, /*on_true_vy=*/%v42492, /*on_false_vx=*/%v42488 (stack72)
        %v42501 = vadd.s32 %v42496, %v10 (stack65)
        %v42505 = vadd.s32 %v42466, %v9 (stack65)
        %v42509 = vadd.s32 %v42501, %v42505 (stack65)
        %v42511 = vshll.u32 %v42505, 13 (stack73)
        %v42512 = vshrl.u32 %v42505, 19 (stack74)
        %v42513 = vor.u32 %v42511, %v42512 (stack75)
        %v42514 = vxor.u32 %v42509, %v42513 (stack76)
        %v42517 = vadd.s32 %v42509, %v42514 (stack65)
        %v42519 = vshll.u32 %v42514, 15 (stack73)
        %v42520 = vshrl.u32 %v42514, 17 (stack74)
        %v42521 = vor.u32 %v42519, %v42520 (stack75)
        %v42522 = vxor.u32 %v42517, %v42521 (stack76)
        %v42525 = vadd.s32 %v42517, %v42522 (stack65)
        %v42527 = vshll.u32 %v42522, 26 (stack73)
        %v42528 = vshrl.u32 %v42522, 6 (stack74)
        %v42529 = vor.u32 %v42527, %v42528 (stack75)
        %v42530 = vxor.u32 %v42525, %v42529 (stack76)
        %v42533 = vadd.s32 %v42525, %v42530 (stack65)
        %v42537 = vadd.s32 %v42533, %v9 (stack65)
        %v42539 = vshll.u32 %v42530, 6 (stack73)
        %v42540 = vshrl.u32 %v42530, 26 (stack74)
        %v42541 = vor.u32 %v42539, %v42540 (stack75)
        %v42542 = vxor.u32 %v42533, %v42541 (stack76)
        %v42545 = vadd.s32 %v42542, %v8 (stack65)
        %v42549 = vadd.s32 %v42545, 1 (stack65)
        %v42553 = vadd.s32 %v42537, %v42549 (stack65)
        %v42555 = vshll.u32 %v42549, 17 (stack73)
        %v42556 = vshrl.u32 %v42549, 15 (stack74)
        %v42557 = vor.u32 %v42555, %v42556 (stack75)
        %v42558 = vxor.u32 %v42553, %v42557 (stack76)
        %v42561 = vadd.s32 %v42553, %v42558 (stack65)
        %v42563 = vshll.u32 %v42558, 29 (stack73)
        %v42564 = vshrl.u32 %v42558, 3 (stack74)
        %v42565 = vor.u32 %v42563, %v42564 (stack75)
        %v42566 = vxor.u32 %v42561, %v42565 (stack76)
        %v42569 = vadd.s32 %v42561, %v42566 (stack65)
        %v42571 = vshll.u32 %v42566, 16 (stack73)
        %v42572 = vshrl.u32 %v42566, 16 (stack74)
        %v42573 = vor.u32 %v42571, %v42572 (stack75)
        %v42574 = vxor.u32 %v42569, %v42573 (stack76)
        %v42577 = vadd.s32 %v42569, %v42574 (stack65)
        %v42581 = vadd.s32 %v42577, %v8 (stack65)
        %v42583 = vshll.u32 %v42574, 24 (stack73)
        %v42584 = vshrl.u32 %v42574, 8 (stack74)
        %v42585 = vor.u32 %v42583, %v42584 (stack75)
        %v42586 = vxor.u32 %v42577, %v42585 (stack76)
        %v42589 = vadd.s32 %v42586, %v10 (stack65)
        %v42593 = vadd.s32 %v42589, 2 (stack65)
        %v42597 = vadd.s32 %v42581, %v42593 (stack65)
        %v42599 = vshll.u32 %v42593, 13 (stack73)
        %v42600 = vshrl.u32 %v42593, 19 (stack74)
        %v42601 = vor.u32 %v42599, %v42600 (stack75)
        %v42602 = vxor.u32 %v42597, %v42601 (stack76)
        %v42605 = vadd.s32 %v42597, %v42602 (stack65)
        %v42607 = vshll.u32 %v42602, 15 (stack73)
        %v42608 = vshrl.u32 %v42602, 17 (stack74)
        %v42609 = vor.u32 %v42607, %v42608 (stack75)
        %v42610 = vxor.u32 %v42605, %v42609 (stack76)
        %v42613 = vadd.s32 %v42605, %v42610 (stack65)
        %v42615 = vshll.u32 %v42610, 26 (stack73)
        %v42616 = vshrl.u32 %v42610, 6 (stack74)
        %v42617 = vor.u32 %v42615, %v42616 (stack75)
        %v42618 = vxor.u32 %v42613, %v42617 (stack76)
        %v42621 = vadd.s32 %v42613, %v42618 (stack65)
        %v42625 = vadd.s32 %v42621, %v10 (stack65)
        %v42627 = vshll.u32 %v42618, 6 (stack73)
        %v42628 = vshrl.u32 %v42618, 26 (stack74)
        %v42629 = vor.u32 %v42627, %v42628 (stack75)
        %v42630 = vxor.u32 %v42621, %v42629 (stack76)
        %v42633 = vadd.s32 %v42630, %v9 (stack65)
        %v42637 = vadd.s32 %v42633, 3 (stack65)
        %v42641 = vadd.s32 %v42625, %v42637 (stack65)
        %v42643 = vshll.u32 %v42637, 17 (stack73)
        %v42644 = vshrl.u32 %v42637, 15 (stack74)
        %v42645 = vor.u32 %v42643, %v42644 (stack75)
        %v42646 = vxor.u32 %v42641, %v42645 (stack76)
        %v42649 = vadd.s32 %v42641, %v42646 (stack65)
        %v42651 = vshll.u32 %v42646, 29 (stack73)
        %v42652 = vshrl.u32 %v42646, 3 (stack74)
        %v42653 = vor.u32 %v42651, %v42652 (stack75)
        %v42654 = vxor.u32 %v42649, %v42653 (stack76)
        %v42657 = vadd.s32 %v42649, %v42654 (stack65)
        %v42659 = vshll.u32 %v42654, 16 (stack73)
        %v42660 = vshrl.u32 %v42654, 16 (stack74)
        %v42661 = vor.u32 %v42659, %v42660 (stack75)
        %v42662 = vxor.u32 %v42657, %v42661 (stack76)
        %v42665 = vadd.s32 %v42657, %v42662 (stack65)
        %v42669 = vadd.s32 %v42665, %v9 (stack65)
        %v42671 = vshll.u32 %v42662, 24 (stack73)
        %v42672 = vshrl.u32 %v42662, 8 (stack74)
        %v42673 = vor.u32 %v42671, %v42672 (stack75)
        %v42674 = vxor.u32 %v42665, %v42673 (stack76)
        %v42677 = vadd.s32 %v42674, %v8 (stack65)
        %v42681 = vadd.s32 %v42677, 4 (stack65)
        %v42685 = vadd.s32 %v42669, %v42681 (stack65)
        %v42687 = vshll.u32 %v42681, 13 (stack73)
        %v42688 = vshrl.u32 %v42681, 19 (stack74)
        %v42689 = vor.u32 %v42687, %v42688 (stack75)
        %v42690 = vxor.u32 %v42685, %v42689 (stack76)
        %v42693 = vadd.s32 %v42685, %v42690 (stack65)
        %v42695 = vshll.u32 %v42690, 15 (stack73)
        %v42696 = vshrl.u32 %v42690, 17 (stack74)
        %v42697 = vor.u32 %v42695, %v42696 (stack75)
        %v42698 = vxor.u32 %v42693, %v42697 (stack76)
        %v42701 = vadd.s32 %v42693, %v42698 (stack65)
        %v42703 = vshll.u32 %v42698, 26 (stack73)
        %v42704 = vshrl.u32 %v42698, 6 (stack74)
        %v42705 = vor.u32 %v42703, %v42704 (stack75)
        %v42706 = vxor.u32 %v42701, %v42705 (stack76)
        %v42709 = vadd.s32 %v42701, %v42706 (stack65)
        %v42713 = vadd.s32 %v42709, %v8 (stack65)
        %v42715 = vshll.u32 %v42706, 6 (stack73)
        %v42716 = vshrl.u32 %v42706, 26 (stack74)
        %v42717 = vor.u32 %v42715, %v42716 (stack75)
        %v42718 = vxor.u32 %v42709, %v42717 (stack76)
        %v42721 = vadd.s32 %v42718, %v10 (stack65)
        %v42725 = vadd.s32 %v42721, 5 (stack65)
        %v42727 = vxor.u32 %v42713, %v42725 (stack76)
        %v42728 = vand.u32.u8 %v42727, 255 (stack77)
        %v42729 = vand.u32 %v42728, 65535 (stack78)
        %v42730 = vshrl.u32 %v42729, 1 (stack79)
        %v42731 = vor.u32 %v42730, 16256 (stack75)
        %v42732 = vand.u32.u16 %v42731, 65535 (stack80)
        %v42733 = vunpack.i.l.bf16 %v42732 (stack81)
        %v42737 = vadd.f32 %v42733, -1.0 (stack82)
        %v42741 = vmul.f32 %v42737, 2.0 (stack83)
        %v42745 = vadd.f32 %v42741, -0.99609375 (stack82)
        %v42749 = vmax.f32 -0.99609375, %v42745 (stack84)
        %v42751 = vand.u32 2147483647, %v42749 (stack85)
        %vm42754 = vcmp.eq.f32.partialorder %v42751, 1.0 (stack86)
        %v42759 = vmul.f32 %v42749, inf (stack83)
        %v42761 = vxor.u32 %v42749, 2147483648 (stack87)
        %v42764 = vmul.f32 %v42749, %v42761 (stack83)
        %v42766 = vadd.f32 %v42764, 1.0 (stack88)
        %v42767 = vlog2.pop %v42766 (stack89)
        %v42768 = vmul.f32 %v42767, 0.6931472 (stack90)
        %v42769 = vmul.f32 -0.5, %v42764 (stack91)
        %v42770 = vadd.f32 %v42769, 1.0 (stack92)
        %v42771 = vmul.f32 %v42770, %v42764 (stack93)
        %v42772 = vand.u32 2147483647, %v42764 (stack94)
        %vm42773 = vcmp.lt.f32.partialorder %v42772, 0.0004427343 (stack95)
        %v42774 = vsel /*vm=*/%vm42773, /*on_true_vy=*/%v42771, /*on_false_vx=*/%v42768 (stack96)
        %v42775 = vxor.u32 %v42774, 2147483648 (stack87)
        %vm42778 = vcmp.lt.f32.partialorder %v42775, 5.0 (stack86)
        %v42783 = vsel /*vm=*/%vm42778, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v42787 = vsel /*vm=*/%vm42778, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v42791 = vsel /*vm=*/%vm42778, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v42795 = vsel /*vm=*/%vm42778, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v42799 = vsel /*vm=*/%vm42778, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v42803 = vsel /*vm=*/%vm42778, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v42807 = vsel /*vm=*/%vm42778, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v42811 = vsel /*vm=*/%vm42778, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v42815 = vsel /*vm=*/%vm42778, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v42819 = vadd.f32 %v42775, -2.5 (stack82)
        %v42821 = vrsqrt.pop %v42775 (stack97)
        %v42822 = vmul.f32 %v42775, %v42821 (stack98)
        %vm42823 = vcmp.eq.f32.partialorder %v42775, inf (stack99)
        %v42824 = vsel /*vm=*/%vm42823, /*on_true_vy=*/%v42775, /*on_false_vx=*/%v42822 (stack100)
        %vm42825 = vcmp.eq.f32.partialorder %v42775, 0.0 (stack101)
        %v42826 = vand.u32 %v42775, 2147483648 (stack102)
        %v42827 = vsel /*vm=*/%vm42825, /*on_true_vy=*/%v42826, /*on_false_vx=*/%v42824 (stack103)
        %v42830 = vadd.f32 %v42827, -3.0 (stack82)
        %v42834 = vsel /*vm=*/%vm42778, /*on_true_vy=*/%v42819, /*on_false_vx=*/%v42830 (stack72)
        %v42838 = vmul.f32 %v42815, %v42834 (stack83)
        %v42842 = vadd.f32 %v42811, %v42838 (stack82)
        %v42846 = vmul.f32 %v42842, %v42834 (stack83)
        %v42850 = vadd.f32 %v42807, %v42846 (stack82)
        %v42854 = vmul.f32 %v42850, %v42834 (stack83)
        %v42858 = vadd.f32 %v42803, %v42854 (stack82)
        %v42862 = vmul.f32 %v42858, %v42834 (stack83)
        %v42866 = vadd.f32 %v42799, %v42862 (stack82)
        %v42870 = vmul.f32 %v42866, %v42834 (stack83)
        %v42874 = vadd.f32 %v42795, %v42870 (stack82)
        %v42878 = vmul.f32 %v42874, %v42834 (stack83)
        %v42882 = vadd.f32 %v42791, %v42878 (stack82)
        %v42886 = vmul.f32 %v42882, %v42834 (stack83)
        %v42890 = vadd.f32 %v42787, %v42886 (stack82)
        %v42894 = vmul.f32 %v42890, %v42834 (stack83)
        %v42898 = vadd.f32 %v42783, %v42894 (stack82)
        %v42902 = vmul.f32 %v42898, %v42749 (stack83)
        %v42906 = vsel /*vm=*/%vm42754, /*on_true_vy=*/%v42759, /*on_false_vx=*/%v42902 (stack72)
        %v42910 = vmul.f32 %v42906, 1.4140625 (stack83)
        %s42912 = scalar_lea.vmem %s280, 300 [#allocation0] (stack107)
        %v42913 = vpack.c.bf16 0.0, %v42910 (stack104)
        %42914 = vst [vmem:[%s42912] sm:$0xf] /*vst_source=*/%v42913 (stack105)
        %v42917 = vadd.s32 %v1868, %v41531 (stack65)
        %s42919 = smul.u32 128, %s27 (stack66)
        %v42920 = vlaneseq (stack67)
        %v42921 = vand.u32 %v42920, 127 (stack68)
        %v42922 = vstv %s42919 (stack69)
        %v42923 = vadd.s32 %v42921, %v42922 (stack70)
        %v42927 = vadd.s32 %v42917, %v42923 (stack65)
        %vm42931 = vcmp.lt.u32.totalorder %v42927, %v42917 (stack71)
        %vm42936 = vcmp.lt.u32.totalorder %v42917, %v1868 (stack71)
        %v42941 = vadd.s32 %v1855, %v41514 (stack65)
        %v42945 = vadd.s32 %v42941, 1 (stack65)
        %v42949 = vsel /*vm=*/%vm42936, /*on_true_vy=*/%v42945, /*on_false_vx=*/%v42941 (stack72)
        %v42953 = vadd.s32 %v42949, 1 (stack65)
        %v42957 = vsel /*vm=*/%vm42931, /*on_true_vy=*/%v42953, /*on_false_vx=*/%v42949 (stack72)
        %v42962 = vadd.s32 %v42957, %v10 (stack65)
        %v42966 = vadd.s32 %v42927, %v9 (stack65)
        %v42970 = vadd.s32 %v42962, %v42966 (stack65)
        %v42972 = vshll.u32 %v42966, 13 (stack73)
        %v42973 = vshrl.u32 %v42966, 19 (stack74)
        %v42974 = vor.u32 %v42972, %v42973 (stack75)
        %v42975 = vxor.u32 %v42970, %v42974 (stack76)
        %v42978 = vadd.s32 %v42970, %v42975 (stack65)
        %v42980 = vshll.u32 %v42975, 15 (stack73)
        %v42981 = vshrl.u32 %v42975, 17 (stack74)
        %v42982 = vor.u32 %v42980, %v42981 (stack75)
        %v42983 = vxor.u32 %v42978, %v42982 (stack76)
        %v42986 = vadd.s32 %v42978, %v42983 (stack65)
        %v42988 = vshll.u32 %v42983, 26 (stack73)
        %v42989 = vshrl.u32 %v42983, 6 (stack74)
        %v42990 = vor.u32 %v42988, %v42989 (stack75)
        %v42991 = vxor.u32 %v42986, %v42990 (stack76)
        %v42994 = vadd.s32 %v42986, %v42991 (stack65)
        %v42998 = vadd.s32 %v42994, %v9 (stack65)
        %v43000 = vshll.u32 %v42991, 6 (stack73)
        %v43001 = vshrl.u32 %v42991, 26 (stack74)
        %v43002 = vor.u32 %v43000, %v43001 (stack75)
        %v43003 = vxor.u32 %v42994, %v43002 (stack76)
        %v43006 = vadd.s32 %v43003, %v8 (stack65)
        %v43010 = vadd.s32 %v43006, 1 (stack65)
        %v43014 = vadd.s32 %v42998, %v43010 (stack65)
        %v43016 = vshll.u32 %v43010, 17 (stack73)
        %v43017 = vshrl.u32 %v43010, 15 (stack74)
        %v43018 = vor.u32 %v43016, %v43017 (stack75)
        %v43019 = vxor.u32 %v43014, %v43018 (stack76)
        %v43022 = vadd.s32 %v43014, %v43019 (stack65)
        %v43024 = vshll.u32 %v43019, 29 (stack73)
        %v43025 = vshrl.u32 %v43019, 3 (stack74)
        %v43026 = vor.u32 %v43024, %v43025 (stack75)
        %v43027 = vxor.u32 %v43022, %v43026 (stack76)
        %v43030 = vadd.s32 %v43022, %v43027 (stack65)
        %v43032 = vshll.u32 %v43027, 16 (stack73)
        %v43033 = vshrl.u32 %v43027, 16 (stack74)
        %v43034 = vor.u32 %v43032, %v43033 (stack75)
        %v43035 = vxor.u32 %v43030, %v43034 (stack76)
        %v43038 = vadd.s32 %v43030, %v43035 (stack65)
        %v43042 = vadd.s32 %v43038, %v8 (stack65)
        %v43044 = vshll.u32 %v43035, 24 (stack73)
        %v43045 = vshrl.u32 %v43035, 8 (stack74)
        %v43046 = vor.u32 %v43044, %v43045 (stack75)
        %v43047 = vxor.u32 %v43038, %v43046 (stack76)
        %v43050 = vadd.s32 %v43047, %v10 (stack65)
        %v43054 = vadd.s32 %v43050, 2 (stack65)
        %v43058 = vadd.s32 %v43042, %v43054 (stack65)
        %v43060 = vshll.u32 %v43054, 13 (stack73)
        %v43061 = vshrl.u32 %v43054, 19 (stack74)
        %v43062 = vor.u32 %v43060, %v43061 (stack75)
        %v43063 = vxor.u32 %v43058, %v43062 (stack76)
        %v43066 = vadd.s32 %v43058, %v43063 (stack65)
        %v43068 = vshll.u32 %v43063, 15 (stack73)
        %v43069 = vshrl.u32 %v43063, 17 (stack74)
        %v43070 = vor.u32 %v43068, %v43069 (stack75)
        %v43071 = vxor.u32 %v43066, %v43070 (stack76)
        %v43074 = vadd.s32 %v43066, %v43071 (stack65)
        %v43076 = vshll.u32 %v43071, 26 (stack73)
        %v43077 = vshrl.u32 %v43071, 6 (stack74)
        %v43078 = vor.u32 %v43076, %v43077 (stack75)
        %v43079 = vxor.u32 %v43074, %v43078 (stack76)
        %v43082 = vadd.s32 %v43074, %v43079 (stack65)
        %v43086 = vadd.s32 %v43082, %v10 (stack65)
        %v43088 = vshll.u32 %v43079, 6 (stack73)
        %v43089 = vshrl.u32 %v43079, 26 (stack74)
        %v43090 = vor.u32 %v43088, %v43089 (stack75)
        %v43091 = vxor.u32 %v43082, %v43090 (stack76)
        %v43094 = vadd.s32 %v43091, %v9 (stack65)
        %v43098 = vadd.s32 %v43094, 3 (stack65)
        %v43102 = vadd.s32 %v43086, %v43098 (stack65)
        %v43104 = vshll.u32 %v43098, 17 (stack73)
        %v43105 = vshrl.u32 %v43098, 15 (stack74)
        %v43106 = vor.u32 %v43104, %v43105 (stack75)
        %v43107 = vxor.u32 %v43102, %v43106 (stack76)
        %v43110 = vadd.s32 %v43102, %v43107 (stack65)
        %v43112 = vshll.u32 %v43107, 29 (stack73)
        %v43113 = vshrl.u32 %v43107, 3 (stack74)
        %v43114 = vor.u32 %v43112, %v43113 (stack75)
        %v43115 = vxor.u32 %v43110, %v43114 (stack76)
        %v43118 = vadd.s32 %v43110, %v43115 (stack65)
        %v43120 = vshll.u32 %v43115, 16 (stack73)
        %v43121 = vshrl.u32 %v43115, 16 (stack74)
        %v43122 = vor.u32 %v43120, %v43121 (stack75)
        %v43123 = vxor.u32 %v43118, %v43122 (stack76)
        %v43126 = vadd.s32 %v43118, %v43123 (stack65)
        %v43130 = vadd.s32 %v43126, %v9 (stack65)
        %v43132 = vshll.u32 %v43123, 24 (stack73)
        %v43133 = vshrl.u32 %v43123, 8 (stack74)
        %v43134 = vor.u32 %v43132, %v43133 (stack75)
        %v43135 = vxor.u32 %v43126, %v43134 (stack76)
        %v43138 = vadd.s32 %v43135, %v8 (stack65)
        %v43142 = vadd.s32 %v43138, 4 (stack65)
        %v43146 = vadd.s32 %v43130, %v43142 (stack65)
        %v43148 = vshll.u32 %v43142, 13 (stack73)
        %v43149 = vshrl.u32 %v43142, 19 (stack74)
        %v43150 = vor.u32 %v43148, %v43149 (stack75)
        %v43151 = vxor.u32 %v43146, %v43150 (stack76)
        %v43154 = vadd.s32 %v43146, %v43151 (stack65)
        %v43156 = vshll.u32 %v43151, 15 (stack73)
        %v43157 = vshrl.u32 %v43151, 17 (stack74)
        %v43158 = vor.u32 %v43156, %v43157 (stack75)
        %v43159 = vxor.u32 %v43154, %v43158 (stack76)
        %v43162 = vadd.s32 %v43154, %v43159 (stack65)
        %v43164 = vshll.u32 %v43159, 26 (stack73)
        %v43165 = vshrl.u32 %v43159, 6 (stack74)
        %v43166 = vor.u32 %v43164, %v43165 (stack75)
        %v43167 = vxor.u32 %v43162, %v43166 (stack76)
        %v43170 = vadd.s32 %v43162, %v43167 (stack65)
        %v43174 = vadd.s32 %v43170, %v8 (stack65)
        %v43176 = vshll.u32 %v43167, 6 (stack73)
        %v43177 = vshrl.u32 %v43167, 26 (stack74)
        %v43178 = vor.u32 %v43176, %v43177 (stack75)
        %v43179 = vxor.u32 %v43170, %v43178 (stack76)
        %v43182 = vadd.s32 %v43179, %v10 (stack65)
        %v43186 = vadd.s32 %v43182, 5 (stack65)
        %v43188 = vxor.u32 %v43174, %v43186 (stack76)
        %v43189 = vand.u32.u8 %v43188, 255 (stack77)
        %v43190 = vand.u32 %v43189, 65535 (stack78)
        %v43191 = vshrl.u32 %v43190, 1 (stack79)
        %v43192 = vor.u32 %v43191, 16256 (stack75)
        %v43193 = vand.u32.u16 %v43192, 65535 (stack80)
        %v43194 = vunpack.i.l.bf16 %v43193 (stack81)
        %v43198 = vadd.f32 %v43194, -1.0 (stack82)
        %v43202 = vmul.f32 %v43198, 2.0 (stack83)
        %v43206 = vadd.f32 %v43202, -0.99609375 (stack82)
        %v43210 = vmax.f32 -0.99609375, %v43206 (stack84)
        %v43212 = vand.u32 2147483647, %v43210 (stack85)
        %vm43215 = vcmp.eq.f32.partialorder %v43212, 1.0 (stack86)
        %v43220 = vmul.f32 %v43210, inf (stack83)
        %v43222 = vxor.u32 %v43210, 2147483648 (stack87)
        %v43225 = vmul.f32 %v43210, %v43222 (stack83)
        %v43227 = vadd.f32 %v43225, 1.0 (stack88)
        %v43228 = vlog2.pop %v43227 (stack89)
        %v43229 = vmul.f32 %v43228, 0.6931472 (stack90)
        %v43230 = vmul.f32 -0.5, %v43225 (stack91)
        %v43231 = vadd.f32 %v43230, 1.0 (stack92)
        %v43232 = vmul.f32 %v43231, %v43225 (stack93)
        %v43233 = vand.u32 2147483647, %v43225 (stack94)
        %vm43234 = vcmp.lt.f32.partialorder %v43233, 0.0004427343 (stack95)
        %v43235 = vsel /*vm=*/%vm43234, /*on_true_vy=*/%v43232, /*on_false_vx=*/%v43229 (stack96)
        %v43236 = vxor.u32 %v43235, 2147483648 (stack87)
        %vm43239 = vcmp.lt.f32.partialorder %v43236, 5.0 (stack86)
        %v43244 = vsel /*vm=*/%vm43239, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v43248 = vsel /*vm=*/%vm43239, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v43252 = vsel /*vm=*/%vm43239, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v43256 = vsel /*vm=*/%vm43239, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v43260 = vsel /*vm=*/%vm43239, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v43264 = vsel /*vm=*/%vm43239, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v43268 = vsel /*vm=*/%vm43239, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v43272 = vsel /*vm=*/%vm43239, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v43276 = vsel /*vm=*/%vm43239, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v43280 = vadd.f32 %v43236, -2.5 (stack82)
        %v43282 = vrsqrt.pop %v43236 (stack97)
        %v43283 = vmul.f32 %v43236, %v43282 (stack98)
        %vm43284 = vcmp.eq.f32.partialorder %v43236, inf (stack99)
        %v43285 = vsel /*vm=*/%vm43284, /*on_true_vy=*/%v43236, /*on_false_vx=*/%v43283 (stack100)
        %vm43286 = vcmp.eq.f32.partialorder %v43236, 0.0 (stack101)
        %v43287 = vand.u32 %v43236, 2147483648 (stack102)
        %v43288 = vsel /*vm=*/%vm43286, /*on_true_vy=*/%v43287, /*on_false_vx=*/%v43285 (stack103)
        %v43291 = vadd.f32 %v43288, -3.0 (stack82)
        %v43295 = vsel /*vm=*/%vm43239, /*on_true_vy=*/%v43280, /*on_false_vx=*/%v43291 (stack72)
        %v43299 = vmul.f32 %v43276, %v43295 (stack83)
        %v43303 = vadd.f32 %v43272, %v43299 (stack82)
        %v43307 = vmul.f32 %v43303, %v43295 (stack83)
        %v43311 = vadd.f32 %v43268, %v43307 (stack82)
        %v43315 = vmul.f32 %v43311, %v43295 (stack83)
        %v43319 = vadd.f32 %v43264, %v43315 (stack82)
        %v43323 = vmul.f32 %v43319, %v43295 (stack83)
        %v43327 = vadd.f32 %v43260, %v43323 (stack82)
        %v43331 = vmul.f32 %v43327, %v43295 (stack83)
        %v43335 = vadd.f32 %v43256, %v43331 (stack82)
        %v43339 = vmul.f32 %v43335, %v43295 (stack83)
        %v43343 = vadd.f32 %v43252, %v43339 (stack82)
        %v43347 = vmul.f32 %v43343, %v43295 (stack83)
        %v43351 = vadd.f32 %v43248, %v43347 (stack82)
        %v43355 = vmul.f32 %v43351, %v43295 (stack83)
        %v43359 = vadd.f32 %v43244, %v43355 (stack82)
        %v43363 = vmul.f32 %v43359, %v43210 (stack83)
        %v43367 = vsel /*vm=*/%vm43215, /*on_true_vy=*/%v43220, /*on_false_vx=*/%v43363 (stack72)
        %v43371 = vmul.f32 %v43367, 1.4140625 (stack83)
        %s43373 = scalar_lea.vmem %s280, 428 [#allocation0] (stack107)
        %v43374 = vpack.c.bf16 0.0, %v43371 (stack104)
        %43375 = vst [vmem:[%s43373] sm:$0xf] /*vst_source=*/%v43374 (stack105)
        %v43378 = vadd.s32 %v2355, %v41531 (stack65)
        %s43380 = smul.u32 128, %s27 (stack66)
        %v43381 = vlaneseq (stack67)
        %v43382 = vand.u32 %v43381, 127 (stack68)
        %v43383 = vstv %s43380 (stack69)
        %v43384 = vadd.s32 %v43382, %v43383 (stack70)
        %v43388 = vadd.s32 %v43378, %v43384 (stack65)
        %vm43392 = vcmp.lt.u32.totalorder %v43388, %v43378 (stack71)
        %vm43397 = vcmp.lt.u32.totalorder %v43378, %v2355 (stack71)
        %v43402 = vadd.s32 %v2342, %v41514 (stack65)
        %v43406 = vadd.s32 %v43402, 1 (stack65)
        %v43410 = vsel /*vm=*/%vm43397, /*on_true_vy=*/%v43406, /*on_false_vx=*/%v43402 (stack72)
        %v43414 = vadd.s32 %v43410, 1 (stack65)
        %v43418 = vsel /*vm=*/%vm43392, /*on_true_vy=*/%v43414, /*on_false_vx=*/%v43410 (stack72)
        %v43423 = vadd.s32 %v43418, %v10 (stack65)
        %v43427 = vadd.s32 %v43388, %v9 (stack65)
        %v43431 = vadd.s32 %v43423, %v43427 (stack65)
        %v43433 = vshll.u32 %v43427, 13 (stack73)
        %v43434 = vshrl.u32 %v43427, 19 (stack74)
        %v43435 = vor.u32 %v43433, %v43434 (stack75)
        %v43436 = vxor.u32 %v43431, %v43435 (stack76)
        %v43439 = vadd.s32 %v43431, %v43436 (stack65)
        %v43441 = vshll.u32 %v43436, 15 (stack73)
        %v43442 = vshrl.u32 %v43436, 17 (stack74)
        %v43443 = vor.u32 %v43441, %v43442 (stack75)
        %v43444 = vxor.u32 %v43439, %v43443 (stack76)
        %v43447 = vadd.s32 %v43439, %v43444 (stack65)
        %v43449 = vshll.u32 %v43444, 26 (stack73)
        %v43450 = vshrl.u32 %v43444, 6 (stack74)
        %v43451 = vor.u32 %v43449, %v43450 (stack75)
        %v43452 = vxor.u32 %v43447, %v43451 (stack76)
        %v43455 = vadd.s32 %v43447, %v43452 (stack65)
        %v43459 = vadd.s32 %v43455, %v9 (stack65)
        %v43461 = vshll.u32 %v43452, 6 (stack73)
        %v43462 = vshrl.u32 %v43452, 26 (stack74)
        %v43463 = vor.u32 %v43461, %v43462 (stack75)
        %v43464 = vxor.u32 %v43455, %v43463 (stack76)
        %v43467 = vadd.s32 %v43464, %v8 (stack65)
        %v43471 = vadd.s32 %v43467, 1 (stack65)
        %v43475 = vadd.s32 %v43459, %v43471 (stack65)
        %v43477 = vshll.u32 %v43471, 17 (stack73)
        %v43478 = vshrl.u32 %v43471, 15 (stack74)
        %v43479 = vor.u32 %v43477, %v43478 (stack75)
        %v43480 = vxor.u32 %v43475, %v43479 (stack76)
        %v43483 = vadd.s32 %v43475, %v43480 (stack65)
        %v43485 = vshll.u32 %v43480, 29 (stack73)
        %v43486 = vshrl.u32 %v43480, 3 (stack74)
        %v43487 = vor.u32 %v43485, %v43486 (stack75)
        %v43488 = vxor.u32 %v43483, %v43487 (stack76)
        %v43491 = vadd.s32 %v43483, %v43488 (stack65)
        %v43493 = vshll.u32 %v43488, 16 (stack73)
        %v43494 = vshrl.u32 %v43488, 16 (stack74)
        %v43495 = vor.u32 %v43493, %v43494 (stack75)
        %v43496 = vxor.u32 %v43491, %v43495 (stack76)
        %v43499 = vadd.s32 %v43491, %v43496 (stack65)
        %v43503 = vadd.s32 %v43499, %v8 (stack65)
        %v43505 = vshll.u32 %v43496, 24 (stack73)
        %v43506 = vshrl.u32 %v43496, 8 (stack74)
        %v43507 = vor.u32 %v43505, %v43506 (stack75)
        %v43508 = vxor.u32 %v43499, %v43507 (stack76)
        %v43511 = vadd.s32 %v43508, %v10 (stack65)
        %v43515 = vadd.s32 %v43511, 2 (stack65)
        %v43519 = vadd.s32 %v43503, %v43515 (stack65)
        %v43521 = vshll.u32 %v43515, 13 (stack73)
        %v43522 = vshrl.u32 %v43515, 19 (stack74)
        %v43523 = vor.u32 %v43521, %v43522 (stack75)
        %v43524 = vxor.u32 %v43519, %v43523 (stack76)
        %v43527 = vadd.s32 %v43519, %v43524 (stack65)
        %v43529 = vshll.u32 %v43524, 15 (stack73)
        %v43530 = vshrl.u32 %v43524, 17 (stack74)
        %v43531 = vor.u32 %v43529, %v43530 (stack75)
        %v43532 = vxor.u32 %v43527, %v43531 (stack76)
        %v43535 = vadd.s32 %v43527, %v43532 (stack65)
        %v43537 = vshll.u32 %v43532, 26 (stack73)
        %v43538 = vshrl.u32 %v43532, 6 (stack74)
        %v43539 = vor.u32 %v43537, %v43538 (stack75)
        %v43540 = vxor.u32 %v43535, %v43539 (stack76)
        %v43543 = vadd.s32 %v43535, %v43540 (stack65)
        %v43547 = vadd.s32 %v43543, %v10 (stack65)
        %v43549 = vshll.u32 %v43540, 6 (stack73)
        %v43550 = vshrl.u32 %v43540, 26 (stack74)
        %v43551 = vor.u32 %v43549, %v43550 (stack75)
        %v43552 = vxor.u32 %v43543, %v43551 (stack76)
        %v43555 = vadd.s32 %v43552, %v9 (stack65)
        %v43559 = vadd.s32 %v43555, 3 (stack65)
        %v43563 = vadd.s32 %v43547, %v43559 (stack65)
        %v43565 = vshll.u32 %v43559, 17 (stack73)
        %v43566 = vshrl.u32 %v43559, 15 (stack74)
        %v43567 = vor.u32 %v43565, %v43566 (stack75)
        %v43568 = vxor.u32 %v43563, %v43567 (stack76)
        %v43571 = vadd.s32 %v43563, %v43568 (stack65)
        %v43573 = vshll.u32 %v43568, 29 (stack73)
        %v43574 = vshrl.u32 %v43568, 3 (stack74)
        %v43575 = vor.u32 %v43573, %v43574 (stack75)
        %v43576 = vxor.u32 %v43571, %v43575 (stack76)
        %v43579 = vadd.s32 %v43571, %v43576 (stack65)
        %v43581 = vshll.u32 %v43576, 16 (stack73)
        %v43582 = vshrl.u32 %v43576, 16 (stack74)
        %v43583 = vor.u32 %v43581, %v43582 (stack75)
        %v43584 = vxor.u32 %v43579, %v43583 (stack76)
        %v43587 = vadd.s32 %v43579, %v43584 (stack65)
        %v43591 = vadd.s32 %v43587, %v9 (stack65)
        %v43593 = vshll.u32 %v43584, 24 (stack73)
        %v43594 = vshrl.u32 %v43584, 8 (stack74)
        %v43595 = vor.u32 %v43593, %v43594 (stack75)
        %v43596 = vxor.u32 %v43587, %v43595 (stack76)
        %v43599 = vadd.s32 %v43596, %v8 (stack65)
        %v43603 = vadd.s32 %v43599, 4 (stack65)
        %v43607 = vadd.s32 %v43591, %v43603 (stack65)
        %v43609 = vshll.u32 %v43603, 13 (stack73)
        %v43610 = vshrl.u32 %v43603, 19 (stack74)
        %v43611 = vor.u32 %v43609, %v43610 (stack75)
        %v43612 = vxor.u32 %v43607, %v43611 (stack76)
        %v43615 = vadd.s32 %v43607, %v43612 (stack65)
        %v43617 = vshll.u32 %v43612, 15 (stack73)
        %v43618 = vshrl.u32 %v43612, 17 (stack74)
        %v43619 = vor.u32 %v43617, %v43618 (stack75)
        %v43620 = vxor.u32 %v43615, %v43619 (stack76)
        %v43623 = vadd.s32 %v43615, %v43620 (stack65)
        %v43625 = vshll.u32 %v43620, 26 (stack73)
        %v43626 = vshrl.u32 %v43620, 6 (stack74)
        %v43627 = vor.u32 %v43625, %v43626 (stack75)
        %v43628 = vxor.u32 %v43623, %v43627 (stack76)
        %v43631 = vadd.s32 %v43623, %v43628 (stack65)
        %v43635 = vadd.s32 %v43631, %v8 (stack65)
        %v43637 = vshll.u32 %v43628, 6 (stack73)
        %v43638 = vshrl.u32 %v43628, 26 (stack74)
        %v43639 = vor.u32 %v43637, %v43638 (stack75)
        %v43640 = vxor.u32 %v43631, %v43639 (stack76)
        %v43643 = vadd.s32 %v43640, %v10 (stack65)
        %v43647 = vadd.s32 %v43643, 5 (stack65)
        %v43649 = vxor.u32 %v43635, %v43647 (stack76)
        %v43650 = vand.u32.u8 %v43649, 255 (stack77)
        %v43651 = vand.u32 %v43650, 65535 (stack78)
        %v43652 = vshrl.u32 %v43651, 1 (stack79)
        %v43653 = vor.u32 %v43652, 16256 (stack75)
        %v43654 = vand.u32.u16 %v43653, 65535 (stack80)
        %v43655 = vunpack.i.l.bf16 %v43654 (stack81)
        %v43659 = vadd.f32 %v43655, -1.0 (stack82)
        %v43663 = vmul.f32 %v43659, 2.0 (stack83)
        %v43667 = vadd.f32 %v43663, -0.99609375 (stack82)
        %v43671 = vmax.f32 -0.99609375, %v43667 (stack84)
        %v43673 = vand.u32 2147483647, %v43671 (stack85)
        %vm43676 = vcmp.eq.f32.partialorder %v43673, 1.0 (stack86)
        %v43681 = vmul.f32 %v43671, inf (stack83)
        %v43683 = vxor.u32 %v43671, 2147483648 (stack87)
        %v43686 = vmul.f32 %v43671, %v43683 (stack83)
        %v43688 = vadd.f32 %v43686, 1.0 (stack88)
        %v43689 = vlog2.pop %v43688 (stack89)
        %v43690 = vmul.f32 %v43689, 0.6931472 (stack90)
        %v43691 = vmul.f32 -0.5, %v43686 (stack91)
        %v43692 = vadd.f32 %v43691, 1.0 (stack92)
        %v43693 = vmul.f32 %v43692, %v43686 (stack93)
        %v43694 = vand.u32 2147483647, %v43686 (stack94)
        %vm43695 = vcmp.lt.f32.partialorder %v43694, 0.0004427343 (stack95)
        %v43696 = vsel /*vm=*/%vm43695, /*on_true_vy=*/%v43693, /*on_false_vx=*/%v43690 (stack96)
        %v43697 = vxor.u32 %v43696, 2147483648 (stack87)
        %vm43700 = vcmp.lt.f32.partialorder %v43697, 5.0 (stack86)
        %v43705 = vsel /*vm=*/%vm43700, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v43709 = vsel /*vm=*/%vm43700, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v43713 = vsel /*vm=*/%vm43700, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v43717 = vsel /*vm=*/%vm43700, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v43721 = vsel /*vm=*/%vm43700, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v43725 = vsel /*vm=*/%vm43700, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v43729 = vsel /*vm=*/%vm43700, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v43733 = vsel /*vm=*/%vm43700, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v43737 = vsel /*vm=*/%vm43700, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v43741 = vadd.f32 %v43697, -2.5 (stack82)
        %v43743 = vrsqrt.pop %v43697 (stack97)
        %v43744 = vmul.f32 %v43697, %v43743 (stack98)
        %vm43745 = vcmp.eq.f32.partialorder %v43697, inf (stack99)
        %v43746 = vsel /*vm=*/%vm43745, /*on_true_vy=*/%v43697, /*on_false_vx=*/%v43744 (stack100)
        %vm43747 = vcmp.eq.f32.partialorder %v43697, 0.0 (stack101)
        %v43748 = vand.u32 %v43697, 2147483648 (stack102)
        %v43749 = vsel /*vm=*/%vm43747, /*on_true_vy=*/%v43748, /*on_false_vx=*/%v43746 (stack103)
        %v43752 = vadd.f32 %v43749, -3.0 (stack82)
        %v43756 = vsel /*vm=*/%vm43700, /*on_true_vy=*/%v43741, /*on_false_vx=*/%v43752 (stack72)
        %v43760 = vmul.f32 %v43737, %v43756 (stack83)
        %v43764 = vadd.f32 %v43733, %v43760 (stack82)
        %v43768 = vmul.f32 %v43764, %v43756 (stack83)
        %v43772 = vadd.f32 %v43729, %v43768 (stack82)
        %v43776 = vmul.f32 %v43772, %v43756 (stack83)
        %v43780 = vadd.f32 %v43725, %v43776 (stack82)
        %v43784 = vmul.f32 %v43780, %v43756 (stack83)
        %v43788 = vadd.f32 %v43721, %v43784 (stack82)
        %v43792 = vmul.f32 %v43788, %v43756 (stack83)
        %v43796 = vadd.f32 %v43717, %v43792 (stack82)
        %v43800 = vmul.f32 %v43796, %v43756 (stack83)
        %v43804 = vadd.f32 %v43713, %v43800 (stack82)
        %v43808 = vmul.f32 %v43804, %v43756 (stack83)
        %v43812 = vadd.f32 %v43709, %v43808 (stack82)
        %v43816 = vmul.f32 %v43812, %v43756 (stack83)
        %v43820 = vadd.f32 %v43705, %v43816 (stack82)
        %v43824 = vmul.f32 %v43820, %v43671 (stack83)
        %v43828 = vsel /*vm=*/%vm43676, /*on_true_vy=*/%v43681, /*on_false_vx=*/%v43824 (stack72)
        %v43832 = vmul.f32 %v43828, 1.4140625 (stack83)
        %s43834 = scalar_lea.vmem %s280, 556 [#allocation0] (stack107)
        %v43835 = vpack.c.bf16 0.0, %v43832 (stack104)
        %43836 = vst [vmem:[%s43834] sm:$0xf] /*vst_source=*/%v43835 (stack105)
        %v43839 = vadd.s32 %v2842, %v41531 (stack65)
        %s43841 = smul.u32 128, %s27 (stack66)
        %v43842 = vlaneseq (stack67)
        %v43843 = vand.u32 %v43842, 127 (stack68)
        %v43844 = vstv %s43841 (stack69)
        %v43845 = vadd.s32 %v43843, %v43844 (stack70)
        %v43849 = vadd.s32 %v43839, %v43845 (stack65)
        %vm43853 = vcmp.lt.u32.totalorder %v43849, %v43839 (stack71)
        %vm43858 = vcmp.lt.u32.totalorder %v43839, %v2842 (stack71)
        %v43863 = vadd.s32 %v2829, %v41514 (stack65)
        %v43867 = vadd.s32 %v43863, 1 (stack65)
        %v43871 = vsel /*vm=*/%vm43858, /*on_true_vy=*/%v43867, /*on_false_vx=*/%v43863 (stack72)
        %v43875 = vadd.s32 %v43871, 1 (stack65)
        %v43879 = vsel /*vm=*/%vm43853, /*on_true_vy=*/%v43875, /*on_false_vx=*/%v43871 (stack72)
        %v43884 = vadd.s32 %v43879, %v10 (stack65)
        %v43888 = vadd.s32 %v43849, %v9 (stack65)
        %v43892 = vadd.s32 %v43884, %v43888 (stack65)
        %v43894 = vshll.u32 %v43888, 13 (stack73)
        %v43895 = vshrl.u32 %v43888, 19 (stack74)
        %v43896 = vor.u32 %v43894, %v43895 (stack75)
        %v43897 = vxor.u32 %v43892, %v43896 (stack76)
        %v43900 = vadd.s32 %v43892, %v43897 (stack65)
        %v43902 = vshll.u32 %v43897, 15 (stack73)
        %v43903 = vshrl.u32 %v43897, 17 (stack74)
        %v43904 = vor.u32 %v43902, %v43903 (stack75)
        %v43905 = vxor.u32 %v43900, %v43904 (stack76)
        %v43908 = vadd.s32 %v43900, %v43905 (stack65)
        %v43910 = vshll.u32 %v43905, 26 (stack73)
        %v43911 = vshrl.u32 %v43905, 6 (stack74)
        %v43912 = vor.u32 %v43910, %v43911 (stack75)
        %v43913 = vxor.u32 %v43908, %v43912 (stack76)
        %v43916 = vadd.s32 %v43908, %v43913 (stack65)
        %v43920 = vadd.s32 %v43916, %v9 (stack65)
        %v43922 = vshll.u32 %v43913, 6 (stack73)
        %v43923 = vshrl.u32 %v43913, 26 (stack74)
        %v43924 = vor.u32 %v43922, %v43923 (stack75)
        %v43925 = vxor.u32 %v43916, %v43924 (stack76)
        %v43928 = vadd.s32 %v43925, %v8 (stack65)
        %v43932 = vadd.s32 %v43928, 1 (stack65)
        %v43936 = vadd.s32 %v43920, %v43932 (stack65)
        %v43938 = vshll.u32 %v43932, 17 (stack73)
        %v43939 = vshrl.u32 %v43932, 15 (stack74)
        %v43940 = vor.u32 %v43938, %v43939 (stack75)
        %v43941 = vxor.u32 %v43936, %v43940 (stack76)
        %v43944 = vadd.s32 %v43936, %v43941 (stack65)
        %v43946 = vshll.u32 %v43941, 29 (stack73)
        %v43947 = vshrl.u32 %v43941, 3 (stack74)
        %v43948 = vor.u32 %v43946, %v43947 (stack75)
        %v43949 = vxor.u32 %v43944, %v43948 (stack76)
        %v43952 = vadd.s32 %v43944, %v43949 (stack65)
        %v43954 = vshll.u32 %v43949, 16 (stack73)
        %v43955 = vshrl.u32 %v43949, 16 (stack74)
        %v43956 = vor.u32 %v43954, %v43955 (stack75)
        %v43957 = vxor.u32 %v43952, %v43956 (stack76)
        %v43960 = vadd.s32 %v43952, %v43957 (stack65)
        %v43964 = vadd.s32 %v43960, %v8 (stack65)
        %v43966 = vshll.u32 %v43957, 24 (stack73)
        %v43967 = vshrl.u32 %v43957, 8 (stack74)
        %v43968 = vor.u32 %v43966, %v43967 (stack75)
        %v43969 = vxor.u32 %v43960, %v43968 (stack76)
        %v43972 = vadd.s32 %v43969, %v10 (stack65)
        %v43976 = vadd.s32 %v43972, 2 (stack65)
        %v43980 = vadd.s32 %v43964, %v43976 (stack65)
        %v43982 = vshll.u32 %v43976, 13 (stack73)
        %v43983 = vshrl.u32 %v43976, 19 (stack74)
        %v43984 = vor.u32 %v43982, %v43983 (stack75)
        %v43985 = vxor.u32 %v43980, %v43984 (stack76)
        %v43988 = vadd.s32 %v43980, %v43985 (stack65)
        %v43990 = vshll.u32 %v43985, 15 (stack73)
        %v43991 = vshrl.u32 %v43985, 17 (stack74)
        %v43992 = vor.u32 %v43990, %v43991 (stack75)
        %v43993 = vxor.u32 %v43988, %v43992 (stack76)
        %v43996 = vadd.s32 %v43988, %v43993 (stack65)
        %v43998 = vshll.u32 %v43993, 26 (stack73)
        %v43999 = vshrl.u32 %v43993, 6 (stack74)
        %v44000 = vor.u32 %v43998, %v43999 (stack75)
        %v44001 = vxor.u32 %v43996, %v44000 (stack76)
        %v44004 = vadd.s32 %v43996, %v44001 (stack65)
        %v44008 = vadd.s32 %v44004, %v10 (stack65)
        %v44010 = vshll.u32 %v44001, 6 (stack73)
        %v44011 = vshrl.u32 %v44001, 26 (stack74)
        %v44012 = vor.u32 %v44010, %v44011 (stack75)
        %v44013 = vxor.u32 %v44004, %v44012 (stack76)
        %v44016 = vadd.s32 %v44013, %v9 (stack65)
        %v44020 = vadd.s32 %v44016, 3 (stack65)
        %v44024 = vadd.s32 %v44008, %v44020 (stack65)
        %v44026 = vshll.u32 %v44020, 17 (stack73)
        %v44027 = vshrl.u32 %v44020, 15 (stack74)
        %v44028 = vor.u32 %v44026, %v44027 (stack75)
        %v44029 = vxor.u32 %v44024, %v44028 (stack76)
        %v44032 = vadd.s32 %v44024, %v44029 (stack65)
        %v44034 = vshll.u32 %v44029, 29 (stack73)
        %v44035 = vshrl.u32 %v44029, 3 (stack74)
        %v44036 = vor.u32 %v44034, %v44035 (stack75)
        %v44037 = vxor.u32 %v44032, %v44036 (stack76)
        %v44040 = vadd.s32 %v44032, %v44037 (stack65)
        %v44042 = vshll.u32 %v44037, 16 (stack73)
        %v44043 = vshrl.u32 %v44037, 16 (stack74)
        %v44044 = vor.u32 %v44042, %v44043 (stack75)
        %v44045 = vxor.u32 %v44040, %v44044 (stack76)
        %v44048 = vadd.s32 %v44040, %v44045 (stack65)
        %v44052 = vadd.s32 %v44048, %v9 (stack65)
        %v44054 = vshll.u32 %v44045, 24 (stack73)
        %v44055 = vshrl.u32 %v44045, 8 (stack74)
        %v44056 = vor.u32 %v44054, %v44055 (stack75)
        %v44057 = vxor.u32 %v44048, %v44056 (stack76)
        %v44060 = vadd.s32 %v44057, %v8 (stack65)
        %v44064 = vadd.s32 %v44060, 4 (stack65)
        %v44068 = vadd.s32 %v44052, %v44064 (stack65)
        %v44070 = vshll.u32 %v44064, 13 (stack73)
        %v44071 = vshrl.u32 %v44064, 19 (stack74)
        %v44072 = vor.u32 %v44070, %v44071 (stack75)
        %v44073 = vxor.u32 %v44068, %v44072 (stack76)
        %v44076 = vadd.s32 %v44068, %v44073 (stack65)
        %v44078 = vshll.u32 %v44073, 15 (stack73)
        %v44079 = vshrl.u32 %v44073, 17 (stack74)
        %v44080 = vor.u32 %v44078, %v44079 (stack75)
        %v44081 = vxor.u32 %v44076, %v44080 (stack76)
        %v44084 = vadd.s32 %v44076, %v44081 (stack65)
        %v44086 = vshll.u32 %v44081, 26 (stack73)
        %v44087 = vshrl.u32 %v44081, 6 (stack74)
        %v44088 = vor.u32 %v44086, %v44087 (stack75)
        %v44089 = vxor.u32 %v44084, %v44088 (stack76)
        %v44092 = vadd.s32 %v44084, %v44089 (stack65)
        %v44096 = vadd.s32 %v44092, %v8 (stack65)
        %v44098 = vshll.u32 %v44089, 6 (stack73)
        %v44099 = vshrl.u32 %v44089, 26 (stack74)
        %v44100 = vor.u32 %v44098, %v44099 (stack75)
        %v44101 = vxor.u32 %v44092, %v44100 (stack76)
        %v44104 = vadd.s32 %v44101, %v10 (stack65)
        %v44108 = vadd.s32 %v44104, 5 (stack65)
        %v44110 = vxor.u32 %v44096, %v44108 (stack76)
        %v44111 = vand.u32.u8 %v44110, 255 (stack77)
        %v44112 = vand.u32 %v44111, 65535 (stack78)
        %v44113 = vshrl.u32 %v44112, 1 (stack79)
        %v44114 = vor.u32 %v44113, 16256 (stack75)
        %v44115 = vand.u32.u16 %v44114, 65535 (stack80)
        %v44116 = vunpack.i.l.bf16 %v44115 (stack81)
        %v44120 = vadd.f32 %v44116, -1.0 (stack82)
        %v44124 = vmul.f32 %v44120, 2.0 (stack83)
        %v44128 = vadd.f32 %v44124, -0.99609375 (stack82)
        %v44132 = vmax.f32 -0.99609375, %v44128 (stack84)
        %v44134 = vand.u32 2147483647, %v44132 (stack85)
        %vm44137 = vcmp.eq.f32.partialorder %v44134, 1.0 (stack86)
        %v44142 = vmul.f32 %v44132, inf (stack83)
        %v44144 = vxor.u32 %v44132, 2147483648 (stack87)
        %v44147 = vmul.f32 %v44132, %v44144 (stack83)
        %v44149 = vadd.f32 %v44147, 1.0 (stack88)
        %v44150 = vlog2.pop %v44149 (stack89)
        %v44151 = vmul.f32 %v44150, 0.6931472 (stack90)
        %v44152 = vmul.f32 -0.5, %v44147 (stack91)
        %v44153 = vadd.f32 %v44152, 1.0 (stack92)
        %v44154 = vmul.f32 %v44153, %v44147 (stack93)
        %v44155 = vand.u32 2147483647, %v44147 (stack94)
        %vm44156 = vcmp.lt.f32.partialorder %v44155, 0.0004427343 (stack95)
        %v44157 = vsel /*vm=*/%vm44156, /*on_true_vy=*/%v44154, /*on_false_vx=*/%v44151 (stack96)
        %v44158 = vxor.u32 %v44157, 2147483648 (stack87)
        %vm44161 = vcmp.lt.f32.partialorder %v44158, 5.0 (stack86)
        %v44166 = vsel /*vm=*/%vm44161, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v44170 = vsel /*vm=*/%vm44161, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v44174 = vsel /*vm=*/%vm44161, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v44178 = vsel /*vm=*/%vm44161, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v44182 = vsel /*vm=*/%vm44161, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v44186 = vsel /*vm=*/%vm44161, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v44190 = vsel /*vm=*/%vm44161, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v44194 = vsel /*vm=*/%vm44161, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v44198 = vsel /*vm=*/%vm44161, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v44202 = vadd.f32 %v44158, -2.5 (stack82)
        %v44204 = vrsqrt.pop %v44158 (stack97)
        %v44205 = vmul.f32 %v44158, %v44204 (stack98)
        %vm44206 = vcmp.eq.f32.partialorder %v44158, inf (stack99)
        %v44207 = vsel /*vm=*/%vm44206, /*on_true_vy=*/%v44158, /*on_false_vx=*/%v44205 (stack100)
        %vm44208 = vcmp.eq.f32.partialorder %v44158, 0.0 (stack101)
        %v44209 = vand.u32 %v44158, 2147483648 (stack102)
        %v44210 = vsel /*vm=*/%vm44208, /*on_true_vy=*/%v44209, /*on_false_vx=*/%v44207 (stack103)
        %v44213 = vadd.f32 %v44210, -3.0 (stack82)
        %v44217 = vsel /*vm=*/%vm44161, /*on_true_vy=*/%v44202, /*on_false_vx=*/%v44213 (stack72)
        %v44221 = vmul.f32 %v44198, %v44217 (stack83)
        %v44225 = vadd.f32 %v44194, %v44221 (stack82)
        %v44229 = vmul.f32 %v44225, %v44217 (stack83)
        %v44233 = vadd.f32 %v44190, %v44229 (stack82)
        %v44237 = vmul.f32 %v44233, %v44217 (stack83)
        %v44241 = vadd.f32 %v44186, %v44237 (stack82)
        %v44245 = vmul.f32 %v44241, %v44217 (stack83)
        %v44249 = vadd.f32 %v44182, %v44245 (stack82)
        %v44253 = vmul.f32 %v44249, %v44217 (stack83)
        %v44257 = vadd.f32 %v44178, %v44253 (stack82)
        %v44261 = vmul.f32 %v44257, %v44217 (stack83)
        %v44265 = vadd.f32 %v44174, %v44261 (stack82)
        %v44269 = vmul.f32 %v44265, %v44217 (stack83)
        %v44273 = vadd.f32 %v44170, %v44269 (stack82)
        %v44277 = vmul.f32 %v44273, %v44217 (stack83)
        %v44281 = vadd.f32 %v44166, %v44277 (stack82)
        %v44285 = vmul.f32 %v44281, %v44132 (stack83)
        %v44289 = vsel /*vm=*/%vm44137, /*on_true_vy=*/%v44142, /*on_false_vx=*/%v44285 (stack72)
        %v44293 = vmul.f32 %v44289, 1.4140625 (stack83)
        %s44295 = scalar_lea.vmem %s280, 684 [#allocation0] (stack107)
        %v44296 = vpack.c.bf16 0.0, %v44293 (stack104)
        %44297 = vst [vmem:[%s44295] sm:$0xf] /*vst_source=*/%v44296 (stack105)
        %v44300 = vadd.s32 %v3329, %v41531 (stack65)
        %s44302 = smul.u32 128, %s27 (stack66)
        %v44303 = vlaneseq (stack67)
        %v44304 = vand.u32 %v44303, 127 (stack68)
        %v44305 = vstv %s44302 (stack69)
        %v44306 = vadd.s32 %v44304, %v44305 (stack70)
        %v44310 = vadd.s32 %v44300, %v44306 (stack65)
        %vm44314 = vcmp.lt.u32.totalorder %v44310, %v44300 (stack71)
        %vm44319 = vcmp.lt.u32.totalorder %v44300, %v3329 (stack71)
        %v44324 = vadd.s32 %v3316, %v41514 (stack65)
        %v44328 = vadd.s32 %v44324, 1 (stack65)
        %v44332 = vsel /*vm=*/%vm44319, /*on_true_vy=*/%v44328, /*on_false_vx=*/%v44324 (stack72)
        %v44336 = vadd.s32 %v44332, 1 (stack65)
        %v44340 = vsel /*vm=*/%vm44314, /*on_true_vy=*/%v44336, /*on_false_vx=*/%v44332 (stack72)
        %v44345 = vadd.s32 %v44340, %v10 (stack65)
        %v44349 = vadd.s32 %v44310, %v9 (stack65)
        %v44353 = vadd.s32 %v44345, %v44349 (stack65)
        %v44355 = vshll.u32 %v44349, 13 (stack73)
        %v44356 = vshrl.u32 %v44349, 19 (stack74)
        %v44357 = vor.u32 %v44355, %v44356 (stack75)
        %v44358 = vxor.u32 %v44353, %v44357 (stack76)
        %v44361 = vadd.s32 %v44353, %v44358 (stack65)
        %v44363 = vshll.u32 %v44358, 15 (stack73)
        %v44364 = vshrl.u32 %v44358, 17 (stack74)
        %v44365 = vor.u32 %v44363, %v44364 (stack75)
        %v44366 = vxor.u32 %v44361, %v44365 (stack76)
        %v44369 = vadd.s32 %v44361, %v44366 (stack65)
        %v44371 = vshll.u32 %v44366, 26 (stack73)
        %v44372 = vshrl.u32 %v44366, 6 (stack74)
        %v44373 = vor.u32 %v44371, %v44372 (stack75)
        %v44374 = vxor.u32 %v44369, %v44373 (stack76)
        %v44377 = vadd.s32 %v44369, %v44374 (stack65)
        %v44381 = vadd.s32 %v44377, %v9 (stack65)
        %v44383 = vshll.u32 %v44374, 6 (stack73)
        %v44384 = vshrl.u32 %v44374, 26 (stack74)
        %v44385 = vor.u32 %v44383, %v44384 (stack75)
        %v44386 = vxor.u32 %v44377, %v44385 (stack76)
        %v44389 = vadd.s32 %v44386, %v8 (stack65)
        %v44393 = vadd.s32 %v44389, 1 (stack65)
        %v44397 = vadd.s32 %v44381, %v44393 (stack65)
        %v44399 = vshll.u32 %v44393, 17 (stack73)
        %v44400 = vshrl.u32 %v44393, 15 (stack74)
        %v44401 = vor.u32 %v44399, %v44400 (stack75)
        %v44402 = vxor.u32 %v44397, %v44401 (stack76)
        %v44405 = vadd.s32 %v44397, %v44402 (stack65)
        %v44407 = vshll.u32 %v44402, 29 (stack73)
        %v44408 = vshrl.u32 %v44402, 3 (stack74)
        %v44409 = vor.u32 %v44407, %v44408 (stack75)
        %v44410 = vxor.u32 %v44405, %v44409 (stack76)
        %v44413 = vadd.s32 %v44405, %v44410 (stack65)
        %v44415 = vshll.u32 %v44410, 16 (stack73)
        %v44416 = vshrl.u32 %v44410, 16 (stack74)
        %v44417 = vor.u32 %v44415, %v44416 (stack75)
        %v44418 = vxor.u32 %v44413, %v44417 (stack76)
        %v44421 = vadd.s32 %v44413, %v44418 (stack65)
        %v44425 = vadd.s32 %v44421, %v8 (stack65)
        %v44427 = vshll.u32 %v44418, 24 (stack73)
        %v44428 = vshrl.u32 %v44418, 8 (stack74)
        %v44429 = vor.u32 %v44427, %v44428 (stack75)
        %v44430 = vxor.u32 %v44421, %v44429 (stack76)
        %v44433 = vadd.s32 %v44430, %v10 (stack65)
        %v44437 = vadd.s32 %v44433, 2 (stack65)
        %v44441 = vadd.s32 %v44425, %v44437 (stack65)
        %v44443 = vshll.u32 %v44437, 13 (stack73)
        %v44444 = vshrl.u32 %v44437, 19 (stack74)
        %v44445 = vor.u32 %v44443, %v44444 (stack75)
        %v44446 = vxor.u32 %v44441, %v44445 (stack76)
        %v44449 = vadd.s32 %v44441, %v44446 (stack65)
        %v44451 = vshll.u32 %v44446, 15 (stack73)
        %v44452 = vshrl.u32 %v44446, 17 (stack74)
        %v44453 = vor.u32 %v44451, %v44452 (stack75)
        %v44454 = vxor.u32 %v44449, %v44453 (stack76)
        %v44457 = vadd.s32 %v44449, %v44454 (stack65)
        %v44459 = vshll.u32 %v44454, 26 (stack73)
        %v44460 = vshrl.u32 %v44454, 6 (stack74)
        %v44461 = vor.u32 %v44459, %v44460 (stack75)
        %v44462 = vxor.u32 %v44457, %v44461 (stack76)
        %v44465 = vadd.s32 %v44457, %v44462 (stack65)
        %v44469 = vadd.s32 %v44465, %v10 (stack65)
        %v44471 = vshll.u32 %v44462, 6 (stack73)
        %v44472 = vshrl.u32 %v44462, 26 (stack74)
        %v44473 = vor.u32 %v44471, %v44472 (stack75)
        %v44474 = vxor.u32 %v44465, %v44473 (stack76)
        %v44477 = vadd.s32 %v44474, %v9 (stack65)
        %v44481 = vadd.s32 %v44477, 3 (stack65)
        %v44485 = vadd.s32 %v44469, %v44481 (stack65)
        %v44487 = vshll.u32 %v44481, 17 (stack73)
        %v44488 = vshrl.u32 %v44481, 15 (stack74)
        %v44489 = vor.u32 %v44487, %v44488 (stack75)
        %v44490 = vxor.u32 %v44485, %v44489 (stack76)
        %v44493 = vadd.s32 %v44485, %v44490 (stack65)
        %v44495 = vshll.u32 %v44490, 29 (stack73)
        %v44496 = vshrl.u32 %v44490, 3 (stack74)
        %v44497 = vor.u32 %v44495, %v44496 (stack75)
        %v44498 = vxor.u32 %v44493, %v44497 (stack76)
        %v44501 = vadd.s32 %v44493, %v44498 (stack65)
        %v44503 = vshll.u32 %v44498, 16 (stack73)
        %v44504 = vshrl.u32 %v44498, 16 (stack74)
        %v44505 = vor.u32 %v44503, %v44504 (stack75)
        %v44506 = vxor.u32 %v44501, %v44505 (stack76)
        %v44509 = vadd.s32 %v44501, %v44506 (stack65)
        %v44513 = vadd.s32 %v44509, %v9 (stack65)
        %v44515 = vshll.u32 %v44506, 24 (stack73)
        %v44516 = vshrl.u32 %v44506, 8 (stack74)
        %v44517 = vor.u32 %v44515, %v44516 (stack75)
        %v44518 = vxor.u32 %v44509, %v44517 (stack76)
        %v44521 = vadd.s32 %v44518, %v8 (stack65)
        %v44525 = vadd.s32 %v44521, 4 (stack65)
        %v44529 = vadd.s32 %v44513, %v44525 (stack65)
        %v44531 = vshll.u32 %v44525, 13 (stack73)
        %v44532 = vshrl.u32 %v44525, 19 (stack74)
        %v44533 = vor.u32 %v44531, %v44532 (stack75)
        %v44534 = vxor.u32 %v44529, %v44533 (stack76)
        %v44537 = vadd.s32 %v44529, %v44534 (stack65)
        %v44539 = vshll.u32 %v44534, 15 (stack73)
        %v44540 = vshrl.u32 %v44534, 17 (stack74)
        %v44541 = vor.u32 %v44539, %v44540 (stack75)
        %v44542 = vxor.u32 %v44537, %v44541 (stack76)
        %v44545 = vadd.s32 %v44537, %v44542 (stack65)
        %v44547 = vshll.u32 %v44542, 26 (stack73)
        %v44548 = vshrl.u32 %v44542, 6 (stack74)
        %v44549 = vor.u32 %v44547, %v44548 (stack75)
        %v44550 = vxor.u32 %v44545, %v44549 (stack76)
        %v44553 = vadd.s32 %v44545, %v44550 (stack65)
        %v44557 = vadd.s32 %v44553, %v8 (stack65)
        %v44559 = vshll.u32 %v44550, 6 (stack73)
        %v44560 = vshrl.u32 %v44550, 26 (stack74)
        %v44561 = vor.u32 %v44559, %v44560 (stack75)
        %v44562 = vxor.u32 %v44553, %v44561 (stack76)
        %v44565 = vadd.s32 %v44562, %v10 (stack65)
        %v44569 = vadd.s32 %v44565, 5 (stack65)
        %v44571 = vxor.u32 %v44557, %v44569 (stack76)
        %v44572 = vand.u32.u8 %v44571, 255 (stack77)
        %v44573 = vand.u32 %v44572, 65535 (stack78)
        %v44574 = vshrl.u32 %v44573, 1 (stack79)
        %v44575 = vor.u32 %v44574, 16256 (stack75)
        %v44576 = vand.u32.u16 %v44575, 65535 (stack80)
        %v44577 = vunpack.i.l.bf16 %v44576 (stack81)
        %v44581 = vadd.f32 %v44577, -1.0 (stack82)
        %v44585 = vmul.f32 %v44581, 2.0 (stack83)
        %v44589 = vadd.f32 %v44585, -0.99609375 (stack82)
        %v44593 = vmax.f32 -0.99609375, %v44589 (stack84)
        %v44595 = vand.u32 2147483647, %v44593 (stack85)
        %vm44598 = vcmp.eq.f32.partialorder %v44595, 1.0 (stack86)
        %v44603 = vmul.f32 %v44593, inf (stack83)
        %v44605 = vxor.u32 %v44593, 2147483648 (stack87)
        %v44608 = vmul.f32 %v44593, %v44605 (stack83)
        %v44610 = vadd.f32 %v44608, 1.0 (stack88)
        %v44611 = vlog2.pop %v44610 (stack89)
        %v44612 = vmul.f32 %v44611, 0.6931472 (stack90)
        %v44613 = vmul.f32 -0.5, %v44608 (stack91)
        %v44614 = vadd.f32 %v44613, 1.0 (stack92)
        %v44615 = vmul.f32 %v44614, %v44608 (stack93)
        %v44616 = vand.u32 2147483647, %v44608 (stack94)
        %vm44617 = vcmp.lt.f32.partialorder %v44616, 0.0004427343 (stack95)
        %v44618 = vsel /*vm=*/%vm44617, /*on_true_vy=*/%v44615, /*on_false_vx=*/%v44612 (stack96)
        %v44619 = vxor.u32 %v44618, 2147483648 (stack87)
        %vm44622 = vcmp.lt.f32.partialorder %v44619, 5.0 (stack86)
        %v44627 = vsel /*vm=*/%vm44622, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v44631 = vsel /*vm=*/%vm44622, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v44635 = vsel /*vm=*/%vm44622, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v44639 = vsel /*vm=*/%vm44622, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v44643 = vsel /*vm=*/%vm44622, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v44647 = vsel /*vm=*/%vm44622, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v44651 = vsel /*vm=*/%vm44622, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v44655 = vsel /*vm=*/%vm44622, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v44659 = vsel /*vm=*/%vm44622, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v44663 = vadd.f32 %v44619, -2.5 (stack82)
        %v44665 = vrsqrt.pop %v44619 (stack97)
        %v44666 = vmul.f32 %v44619, %v44665 (stack98)
        %vm44667 = vcmp.eq.f32.partialorder %v44619, inf (stack99)
        %v44668 = vsel /*vm=*/%vm44667, /*on_true_vy=*/%v44619, /*on_false_vx=*/%v44666 (stack100)
        %vm44669 = vcmp.eq.f32.partialorder %v44619, 0.0 (stack101)
        %v44670 = vand.u32 %v44619, 2147483648 (stack102)
        %v44671 = vsel /*vm=*/%vm44669, /*on_true_vy=*/%v44670, /*on_false_vx=*/%v44668 (stack103)
        %v44674 = vadd.f32 %v44671, -3.0 (stack82)
        %v44678 = vsel /*vm=*/%vm44622, /*on_true_vy=*/%v44663, /*on_false_vx=*/%v44674 (stack72)
        %v44682 = vmul.f32 %v44659, %v44678 (stack83)
        %v44686 = vadd.f32 %v44655, %v44682 (stack82)
        %v44690 = vmul.f32 %v44686, %v44678 (stack83)
        %v44694 = vadd.f32 %v44651, %v44690 (stack82)
        %v44698 = vmul.f32 %v44694, %v44678 (stack83)
        %v44702 = vadd.f32 %v44647, %v44698 (stack82)
        %v44706 = vmul.f32 %v44702, %v44678 (stack83)
        %v44710 = vadd.f32 %v44643, %v44706 (stack82)
        %v44714 = vmul.f32 %v44710, %v44678 (stack83)
        %v44718 = vadd.f32 %v44639, %v44714 (stack82)
        %v44722 = vmul.f32 %v44718, %v44678 (stack83)
        %v44726 = vadd.f32 %v44635, %v44722 (stack82)
        %v44730 = vmul.f32 %v44726, %v44678 (stack83)
        %v44734 = vadd.f32 %v44631, %v44730 (stack82)
        %v44738 = vmul.f32 %v44734, %v44678 (stack83)
        %v44742 = vadd.f32 %v44627, %v44738 (stack82)
        %v44746 = vmul.f32 %v44742, %v44593 (stack83)
        %v44750 = vsel /*vm=*/%vm44598, /*on_true_vy=*/%v44603, /*on_false_vx=*/%v44746 (stack72)
        %v44754 = vmul.f32 %v44750, 1.4140625 (stack83)
        %s44756 = scalar_lea.vmem %s280, 812 [#allocation0] (stack107)
        %v44757 = vpack.c.bf16 0.0, %v44754 (stack104)
        %44758 = vst [vmem:[%s44756] sm:$0xf] /*vst_source=*/%v44757 (stack105)
        %v44761 = vadd.s32 %v3816, %v41531 (stack65)
        %s44763 = smul.u32 128, %s27 (stack66)
        %v44764 = vlaneseq (stack67)
        %v44765 = vand.u32 %v44764, 127 (stack68)
        %v44766 = vstv %s44763 (stack69)
        %v44767 = vadd.s32 %v44765, %v44766 (stack70)
        %v44771 = vadd.s32 %v44761, %v44767 (stack65)
        %vm44775 = vcmp.lt.u32.totalorder %v44771, %v44761 (stack71)
        %vm44780 = vcmp.lt.u32.totalorder %v44761, %v3816 (stack71)
        %v44785 = vadd.s32 %v3803, %v41514 (stack65)
        %v44789 = vadd.s32 %v44785, 1 (stack65)
        %v44793 = vsel /*vm=*/%vm44780, /*on_true_vy=*/%v44789, /*on_false_vx=*/%v44785 (stack72)
        %v44797 = vadd.s32 %v44793, 1 (stack65)
        %v44801 = vsel /*vm=*/%vm44775, /*on_true_vy=*/%v44797, /*on_false_vx=*/%v44793 (stack72)
        %v44806 = vadd.s32 %v44801, %v10 (stack65)
        %v44810 = vadd.s32 %v44771, %v9 (stack65)
        %v44814 = vadd.s32 %v44806, %v44810 (stack65)
        %v44816 = vshll.u32 %v44810, 13 (stack73)
        %v44817 = vshrl.u32 %v44810, 19 (stack74)
        %v44818 = vor.u32 %v44816, %v44817 (stack75)
        %v44819 = vxor.u32 %v44814, %v44818 (stack76)
        %v44822 = vadd.s32 %v44814, %v44819 (stack65)
        %v44824 = vshll.u32 %v44819, 15 (stack73)
        %v44825 = vshrl.u32 %v44819, 17 (stack74)
        %v44826 = vor.u32 %v44824, %v44825 (stack75)
        %v44827 = vxor.u32 %v44822, %v44826 (stack76)
        %v44830 = vadd.s32 %v44822, %v44827 (stack65)
        %v44832 = vshll.u32 %v44827, 26 (stack73)
        %v44833 = vshrl.u32 %v44827, 6 (stack74)
        %v44834 = vor.u32 %v44832, %v44833 (stack75)
        %v44835 = vxor.u32 %v44830, %v44834 (stack76)
        %v44838 = vadd.s32 %v44830, %v44835 (stack65)
        %v44842 = vadd.s32 %v44838, %v9 (stack65)
        %v44844 = vshll.u32 %v44835, 6 (stack73)
        %v44845 = vshrl.u32 %v44835, 26 (stack74)
        %v44846 = vor.u32 %v44844, %v44845 (stack75)
        %v44847 = vxor.u32 %v44838, %v44846 (stack76)
        %v44850 = vadd.s32 %v44847, %v8 (stack65)
        %v44854 = vadd.s32 %v44850, 1 (stack65)
        %v44858 = vadd.s32 %v44842, %v44854 (stack65)
        %v44860 = vshll.u32 %v44854, 17 (stack73)
        %v44861 = vshrl.u32 %v44854, 15 (stack74)
        %v44862 = vor.u32 %v44860, %v44861 (stack75)
        %v44863 = vxor.u32 %v44858, %v44862 (stack76)
        %v44866 = vadd.s32 %v44858, %v44863 (stack65)
        %v44868 = vshll.u32 %v44863, 29 (stack73)
        %v44869 = vshrl.u32 %v44863, 3 (stack74)
        %v44870 = vor.u32 %v44868, %v44869 (stack75)
        %v44871 = vxor.u32 %v44866, %v44870 (stack76)
        %v44874 = vadd.s32 %v44866, %v44871 (stack65)
        %v44876 = vshll.u32 %v44871, 16 (stack73)
        %v44877 = vshrl.u32 %v44871, 16 (stack74)
        %v44878 = vor.u32 %v44876, %v44877 (stack75)
        %v44879 = vxor.u32 %v44874, %v44878 (stack76)
        %v44882 = vadd.s32 %v44874, %v44879 (stack65)
        %v44886 = vadd.s32 %v44882, %v8 (stack65)
        %v44888 = vshll.u32 %v44879, 24 (stack73)
        %v44889 = vshrl.u32 %v44879, 8 (stack74)
        %v44890 = vor.u32 %v44888, %v44889 (stack75)
        %v44891 = vxor.u32 %v44882, %v44890 (stack76)
        %v44894 = vadd.s32 %v44891, %v10 (stack65)
        %v44898 = vadd.s32 %v44894, 2 (stack65)
        %v44902 = vadd.s32 %v44886, %v44898 (stack65)
        %v44904 = vshll.u32 %v44898, 13 (stack73)
        %v44905 = vshrl.u32 %v44898, 19 (stack74)
        %v44906 = vor.u32 %v44904, %v44905 (stack75)
        %v44907 = vxor.u32 %v44902, %v44906 (stack76)
        %v44910 = vadd.s32 %v44902, %v44907 (stack65)
        %v44912 = vshll.u32 %v44907, 15 (stack73)
        %v44913 = vshrl.u32 %v44907, 17 (stack74)
        %v44914 = vor.u32 %v44912, %v44913 (stack75)
        %v44915 = vxor.u32 %v44910, %v44914 (stack76)
        %v44918 = vadd.s32 %v44910, %v44915 (stack65)
        %v44920 = vshll.u32 %v44915, 26 (stack73)
        %v44921 = vshrl.u32 %v44915, 6 (stack74)
        %v44922 = vor.u32 %v44920, %v44921 (stack75)
        %v44923 = vxor.u32 %v44918, %v44922 (stack76)
        %v44926 = vadd.s32 %v44918, %v44923 (stack65)
        %v44930 = vadd.s32 %v44926, %v10 (stack65)
        %v44932 = vshll.u32 %v44923, 6 (stack73)
        %v44933 = vshrl.u32 %v44923, 26 (stack74)
        %v44934 = vor.u32 %v44932, %v44933 (stack75)
        %v44935 = vxor.u32 %v44926, %v44934 (stack76)
        %v44938 = vadd.s32 %v44935, %v9 (stack65)
        %v44942 = vadd.s32 %v44938, 3 (stack65)
        %v44946 = vadd.s32 %v44930, %v44942 (stack65)
        %v44948 = vshll.u32 %v44942, 17 (stack73)
        %v44949 = vshrl.u32 %v44942, 15 (stack74)
        %v44950 = vor.u32 %v44948, %v44949 (stack75)
        %v44951 = vxor.u32 %v44946, %v44950 (stack76)
        %v44954 = vadd.s32 %v44946, %v44951 (stack65)
        %v44956 = vshll.u32 %v44951, 29 (stack73)
        %v44957 = vshrl.u32 %v44951, 3 (stack74)
        %v44958 = vor.u32 %v44956, %v44957 (stack75)
        %v44959 = vxor.u32 %v44954, %v44958 (stack76)
        %v44962 = vadd.s32 %v44954, %v44959 (stack65)
        %v44964 = vshll.u32 %v44959, 16 (stack73)
        %v44965 = vshrl.u32 %v44959, 16 (stack74)
        %v44966 = vor.u32 %v44964, %v44965 (stack75)
        %v44967 = vxor.u32 %v44962, %v44966 (stack76)
        %v44970 = vadd.s32 %v44962, %v44967 (stack65)
        %v44974 = vadd.s32 %v44970, %v9 (stack65)
        %v44976 = vshll.u32 %v44967, 24 (stack73)
        %v44977 = vshrl.u32 %v44967, 8 (stack74)
        %v44978 = vor.u32 %v44976, %v44977 (stack75)
        %v44979 = vxor.u32 %v44970, %v44978 (stack76)
        %v44982 = vadd.s32 %v44979, %v8 (stack65)
        %v44986 = vadd.s32 %v44982, 4 (stack65)
        %v44990 = vadd.s32 %v44974, %v44986 (stack65)
        %v44992 = vshll.u32 %v44986, 13 (stack73)
        %v44993 = vshrl.u32 %v44986, 19 (stack74)
        %v44994 = vor.u32 %v44992, %v44993 (stack75)
        %v44995 = vxor.u32 %v44990, %v44994 (stack76)
        %v44998 = vadd.s32 %v44990, %v44995 (stack65)
        %v45000 = vshll.u32 %v44995, 15 (stack73)
        %v45001 = vshrl.u32 %v44995, 17 (stack74)
        %v45002 = vor.u32 %v45000, %v45001 (stack75)
        %v45003 = vxor.u32 %v44998, %v45002 (stack76)
        %v45006 = vadd.s32 %v44998, %v45003 (stack65)
        %v45008 = vshll.u32 %v45003, 26 (stack73)
        %v45009 = vshrl.u32 %v45003, 6 (stack74)
        %v45010 = vor.u32 %v45008, %v45009 (stack75)
        %v45011 = vxor.u32 %v45006, %v45010 (stack76)
        %v45014 = vadd.s32 %v45006, %v45011 (stack65)
        %v45018 = vadd.s32 %v45014, %v8 (stack65)
        %v45020 = vshll.u32 %v45011, 6 (stack73)
        %v45021 = vshrl.u32 %v45011, 26 (stack74)
        %v45022 = vor.u32 %v45020, %v45021 (stack75)
        %v45023 = vxor.u32 %v45014, %v45022 (stack76)
        %v45026 = vadd.s32 %v45023, %v10 (stack65)
        %v45030 = vadd.s32 %v45026, 5 (stack65)
        %v45032 = vxor.u32 %v45018, %v45030 (stack76)
        %v45033 = vand.u32.u8 %v45032, 255 (stack77)
        %v45034 = vand.u32 %v45033, 65535 (stack78)
        %v45035 = vshrl.u32 %v45034, 1 (stack79)
        %v45036 = vor.u32 %v45035, 16256 (stack75)
        %v45037 = vand.u32.u16 %v45036, 65535 (stack80)
        %v45038 = vunpack.i.l.bf16 %v45037 (stack81)
        %v45042 = vadd.f32 %v45038, -1.0 (stack82)
        %v45046 = vmul.f32 %v45042, 2.0 (stack83)
        %v45050 = vadd.f32 %v45046, -0.99609375 (stack82)
        %v45054 = vmax.f32 -0.99609375, %v45050 (stack84)
        %v45056 = vand.u32 2147483647, %v45054 (stack85)
        %vm45059 = vcmp.eq.f32.partialorder %v45056, 1.0 (stack86)
        %v45064 = vmul.f32 %v45054, inf (stack83)
        %v45066 = vxor.u32 %v45054, 2147483648 (stack87)
        %v45069 = vmul.f32 %v45054, %v45066 (stack83)
        %v45071 = vadd.f32 %v45069, 1.0 (stack88)
        %v45072 = vlog2.pop %v45071 (stack89)
        %v45073 = vmul.f32 %v45072, 0.6931472 (stack90)
        %v45074 = vmul.f32 -0.5, %v45069 (stack91)
        %v45075 = vadd.f32 %v45074, 1.0 (stack92)
        %v45076 = vmul.f32 %v45075, %v45069 (stack93)
        %v45077 = vand.u32 2147483647, %v45069 (stack94)
        %vm45078 = vcmp.lt.f32.partialorder %v45077, 0.0004427343 (stack95)
        %v45079 = vsel /*vm=*/%vm45078, /*on_true_vy=*/%v45076, /*on_false_vx=*/%v45073 (stack96)
        %v45080 = vxor.u32 %v45079, 2147483648 (stack87)
        %vm45083 = vcmp.lt.f32.partialorder %v45080, 5.0 (stack86)
        %v45088 = vsel /*vm=*/%vm45083, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v45092 = vsel /*vm=*/%vm45083, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v45096 = vsel /*vm=*/%vm45083, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v45100 = vsel /*vm=*/%vm45083, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v45104 = vsel /*vm=*/%vm45083, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v45108 = vsel /*vm=*/%vm45083, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v45112 = vsel /*vm=*/%vm45083, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v45116 = vsel /*vm=*/%vm45083, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v45120 = vsel /*vm=*/%vm45083, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v45124 = vadd.f32 %v45080, -2.5 (stack82)
        %v45126 = vrsqrt.pop %v45080 (stack97)
        %v45127 = vmul.f32 %v45080, %v45126 (stack98)
        %vm45128 = vcmp.eq.f32.partialorder %v45080, inf (stack99)
        %v45129 = vsel /*vm=*/%vm45128, /*on_true_vy=*/%v45080, /*on_false_vx=*/%v45127 (stack100)
        %vm45130 = vcmp.eq.f32.partialorder %v45080, 0.0 (stack101)
        %v45131 = vand.u32 %v45080, 2147483648 (stack102)
        %v45132 = vsel /*vm=*/%vm45130, /*on_true_vy=*/%v45131, /*on_false_vx=*/%v45129 (stack103)
        %v45135 = vadd.f32 %v45132, -3.0 (stack82)
        %v45139 = vsel /*vm=*/%vm45083, /*on_true_vy=*/%v45124, /*on_false_vx=*/%v45135 (stack72)
        %v45143 = vmul.f32 %v45120, %v45139 (stack83)
        %v45147 = vadd.f32 %v45116, %v45143 (stack82)
        %v45151 = vmul.f32 %v45147, %v45139 (stack83)
        %v45155 = vadd.f32 %v45112, %v45151 (stack82)
        %v45159 = vmul.f32 %v45155, %v45139 (stack83)
        %v45163 = vadd.f32 %v45108, %v45159 (stack82)
        %v45167 = vmul.f32 %v45163, %v45139 (stack83)
        %v45171 = vadd.f32 %v45104, %v45167 (stack82)
        %v45175 = vmul.f32 %v45171, %v45139 (stack83)
        %v45179 = vadd.f32 %v45100, %v45175 (stack82)
        %v45183 = vmul.f32 %v45179, %v45139 (stack83)
        %v45187 = vadd.f32 %v45096, %v45183 (stack82)
        %v45191 = vmul.f32 %v45187, %v45139 (stack83)
        %v45195 = vadd.f32 %v45092, %v45191 (stack82)
        %v45199 = vmul.f32 %v45195, %v45139 (stack83)
        %v45203 = vadd.f32 %v45088, %v45199 (stack82)
        %v45207 = vmul.f32 %v45203, %v45054 (stack83)
        %v45211 = vsel /*vm=*/%vm45059, /*on_true_vy=*/%v45064, /*on_false_vx=*/%v45207 (stack72)
        %v45215 = vmul.f32 %v45211, 1.4140625 (stack83)
        %s45217 = scalar_lea.vmem %s280, 940 [#allocation0] (stack107)
        %v45218 = vpack.c.bf16 0.0, %v45215 (stack104)
        %45219 = vst [vmem:[%s45217] sm:$0xf] /*vst_source=*/%v45218 (stack105)
        %s45220 = sadd.s32 %s339, 96 (stack106)
        %s45221 = sshrl.u32 %s45220, 10 (stack49)
        %p45222 = scmp.lt.s32.totalorder 1, %s45221 (stack50)
        %s45223 = scalar_select /*predicate=*/%p45222, /*on_true=*/1, /*on_false=*/%s45221 (stack51)
        %s45224 = sand.u32 %s45220, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s45225 = sshrl.u32 %s45224, 7 (stack53)
        %s45226 = sand.u32 %s45224, 127 /* smod.u32 w/div 128 */ (stack54)
        %s45227 = smul.addr %s45223, 8 (stack55)
        %s45228 = scalar_lea.vmem %s3, %s45227 (stack56)
        %s45230 = scalar_lea.vmem %s45228, %s45225 (stack57)
        %v45231 = vld [vmem:[%s45230] ss:$0 sm:$0xff] (stack58)
        %s45232 = sand.u32 %s45226, 255 (stack59)
        %s45234 = sor.u32 256, %s45232 (stack60)
        %45235 = vbcast.lane.b32.xlu0 %v45231, %s45234 (stack61)
        %v45236 = vpop.permute.xlu0 %45235 (stack62)
        %s45237 = sadd.s32 %s347, 96 (stack106)
        %s45238 = sshrl.u32 %s45237, 10 (stack49)
        %p45239 = scmp.lt.s32.totalorder 1, %s45238 (stack50)
        %s45240 = scalar_select /*predicate=*/%p45239, /*on_true=*/1, /*on_false=*/%s45238 (stack51)
        %s45241 = sand.u32 %s45237, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s45242 = sshrl.u32 %s45241, 7 (stack53)
        %s45243 = sand.u32 %s45241, 127 /* smod.u32 w/div 128 */ (stack54)
        %s45244 = smul.addr %s45240, 8 (stack55)
        %s45245 = scalar_lea.vmem %s5, %s45244 (stack56)
        %s45247 = scalar_lea.vmem %s45245, %s45242 (stack57)
        %v45248 = vld [vmem:[%s45247] ss:$0 sm:$0xff] (stack58)
        %s45249 = sand.u32 %s45243, 255 (stack59)
        %s45251 = sor.u32 256, %s45249 (stack60)
        %45252 = vbcast.lane.b32.xlu0 %v45248, %s45251 (stack61)
        %v45253 = vpop.permute.xlu0 %45252 (stack62)
        %v45256 = vadd.s32 %v408, %v45253 (stack65)
        %s45258 = smul.u32 128, %s27 (stack66)
        %v45259 = vlaneseq (stack67)
        %v45260 = vand.u32 %v45259, 127 (stack68)
        %v45261 = vstv %s45258 (stack69)
        %v45262 = vadd.s32 %v45260, %v45261 (stack70)
        %v45266 = vadd.s32 %v45256, %v45262 (stack65)
        %vm45270 = vcmp.lt.u32.totalorder %v45266, %v45256 (stack71)
        %vm45275 = vcmp.lt.u32.totalorder %v45256, %v408 (stack71)
        %v45280 = vadd.s32 %v380, %v45236 (stack65)
        %v45284 = vadd.s32 %v45280, 1 (stack65)
        %v45288 = vsel /*vm=*/%vm45275, /*on_true_vy=*/%v45284, /*on_false_vx=*/%v45280 (stack72)
        %v45292 = vadd.s32 %v45288, 1 (stack65)
        %v45296 = vsel /*vm=*/%vm45270, /*on_true_vy=*/%v45292, /*on_false_vx=*/%v45288 (stack72)
        %v45301 = vadd.s32 %v45296, %v10 (stack65)
        %v45305 = vadd.s32 %v45266, %v9 (stack65)
        %v45309 = vadd.s32 %v45301, %v45305 (stack65)
        %v45311 = vshll.u32 %v45305, 13 (stack73)
        %v45312 = vshrl.u32 %v45305, 19 (stack74)
        %v45313 = vor.u32 %v45311, %v45312 (stack75)
        %v45314 = vxor.u32 %v45309, %v45313 (stack76)
        %v45317 = vadd.s32 %v45309, %v45314 (stack65)
        %v45319 = vshll.u32 %v45314, 15 (stack73)
        %v45320 = vshrl.u32 %v45314, 17 (stack74)
        %v45321 = vor.u32 %v45319, %v45320 (stack75)
        %v45322 = vxor.u32 %v45317, %v45321 (stack76)
        %v45325 = vadd.s32 %v45317, %v45322 (stack65)
        %v45327 = vshll.u32 %v45322, 26 (stack73)
        %v45328 = vshrl.u32 %v45322, 6 (stack74)
        %v45329 = vor.u32 %v45327, %v45328 (stack75)
        %v45330 = vxor.u32 %v45325, %v45329 (stack76)
        %v45333 = vadd.s32 %v45325, %v45330 (stack65)
        %v45337 = vadd.s32 %v45333, %v9 (stack65)
        %v45339 = vshll.u32 %v45330, 6 (stack73)
        %v45340 = vshrl.u32 %v45330, 26 (stack74)
        %v45341 = vor.u32 %v45339, %v45340 (stack75)
        %v45342 = vxor.u32 %v45333, %v45341 (stack76)
        %v45345 = vadd.s32 %v45342, %v8 (stack65)
        %v45349 = vadd.s32 %v45345, 1 (stack65)
        %v45353 = vadd.s32 %v45337, %v45349 (stack65)
        %v45355 = vshll.u32 %v45349, 17 (stack73)
        %v45356 = vshrl.u32 %v45349, 15 (stack74)
        %v45357 = vor.u32 %v45355, %v45356 (stack75)
        %v45358 = vxor.u32 %v45353, %v45357 (stack76)
        %v45361 = vadd.s32 %v45353, %v45358 (stack65)
        %v45363 = vshll.u32 %v45358, 29 (stack73)
        %v45364 = vshrl.u32 %v45358, 3 (stack74)
        %v45365 = vor.u32 %v45363, %v45364 (stack75)
        %v45366 = vxor.u32 %v45361, %v45365 (stack76)
        %v45369 = vadd.s32 %v45361, %v45366 (stack65)
        %v45371 = vshll.u32 %v45366, 16 (stack73)
        %v45372 = vshrl.u32 %v45366, 16 (stack74)
        %v45373 = vor.u32 %v45371, %v45372 (stack75)
        %v45374 = vxor.u32 %v45369, %v45373 (stack76)
        %v45377 = vadd.s32 %v45369, %v45374 (stack65)
        %v45381 = vadd.s32 %v45377, %v8 (stack65)
        %v45383 = vshll.u32 %v45374, 24 (stack73)
        %v45384 = vshrl.u32 %v45374, 8 (stack74)
        %v45385 = vor.u32 %v45383, %v45384 (stack75)
        %v45386 = vxor.u32 %v45377, %v45385 (stack76)
        %v45389 = vadd.s32 %v45386, %v10 (stack65)
        %v45393 = vadd.s32 %v45389, 2 (stack65)
        %v45397 = vadd.s32 %v45381, %v45393 (stack65)
        %v45399 = vshll.u32 %v45393, 13 (stack73)
        %v45400 = vshrl.u32 %v45393, 19 (stack74)
        %v45401 = vor.u32 %v45399, %v45400 (stack75)
        %v45402 = vxor.u32 %v45397, %v45401 (stack76)
        %v45405 = vadd.s32 %v45397, %v45402 (stack65)
        %v45407 = vshll.u32 %v45402, 15 (stack73)
        %v45408 = vshrl.u32 %v45402, 17 (stack74)
        %v45409 = vor.u32 %v45407, %v45408 (stack75)
        %v45410 = vxor.u32 %v45405, %v45409 (stack76)
        %v45413 = vadd.s32 %v45405, %v45410 (stack65)
        %v45415 = vshll.u32 %v45410, 26 (stack73)
        %v45416 = vshrl.u32 %v45410, 6 (stack74)
        %v45417 = vor.u32 %v45415, %v45416 (stack75)
        %v45418 = vxor.u32 %v45413, %v45417 (stack76)
        %v45421 = vadd.s32 %v45413, %v45418 (stack65)
        %v45425 = vadd.s32 %v45421, %v10 (stack65)
        %v45427 = vshll.u32 %v45418, 6 (stack73)
        %v45428 = vshrl.u32 %v45418, 26 (stack74)
        %v45429 = vor.u32 %v45427, %v45428 (stack75)
        %v45430 = vxor.u32 %v45421, %v45429 (stack76)
        %v45433 = vadd.s32 %v45430, %v9 (stack65)
        %v45437 = vadd.s32 %v45433, 3 (stack65)
        %v45441 = vadd.s32 %v45425, %v45437 (stack65)
        %v45443 = vshll.u32 %v45437, 17 (stack73)
        %v45444 = vshrl.u32 %v45437, 15 (stack74)
        %v45445 = vor.u32 %v45443, %v45444 (stack75)
        %v45446 = vxor.u32 %v45441, %v45445 (stack76)
        %v45449 = vadd.s32 %v45441, %v45446 (stack65)
        %v45451 = vshll.u32 %v45446, 29 (stack73)
        %v45452 = vshrl.u32 %v45446, 3 (stack74)
        %v45453 = vor.u32 %v45451, %v45452 (stack75)
        %v45454 = vxor.u32 %v45449, %v45453 (stack76)
        %v45457 = vadd.s32 %v45449, %v45454 (stack65)
        %v45459 = vshll.u32 %v45454, 16 (stack73)
        %v45460 = vshrl.u32 %v45454, 16 (stack74)
        %v45461 = vor.u32 %v45459, %v45460 (stack75)
        %v45462 = vxor.u32 %v45457, %v45461 (stack76)
        %v45465 = vadd.s32 %v45457, %v45462 (stack65)
        %v45469 = vadd.s32 %v45465, %v9 (stack65)
        %v45471 = vshll.u32 %v45462, 24 (stack73)
        %v45472 = vshrl.u32 %v45462, 8 (stack74)
        %v45473 = vor.u32 %v45471, %v45472 (stack75)
        %v45474 = vxor.u32 %v45465, %v45473 (stack76)
        %v45477 = vadd.s32 %v45474, %v8 (stack65)
        %v45481 = vadd.s32 %v45477, 4 (stack65)
        %v45485 = vadd.s32 %v45469, %v45481 (stack65)
        %v45487 = vshll.u32 %v45481, 13 (stack73)
        %v45488 = vshrl.u32 %v45481, 19 (stack74)
        %v45489 = vor.u32 %v45487, %v45488 (stack75)
        %v45490 = vxor.u32 %v45485, %v45489 (stack76)
        %v45493 = vadd.s32 %v45485, %v45490 (stack65)
        %v45495 = vshll.u32 %v45490, 15 (stack73)
        %v45496 = vshrl.u32 %v45490, 17 (stack74)
        %v45497 = vor.u32 %v45495, %v45496 (stack75)
        %v45498 = vxor.u32 %v45493, %v45497 (stack76)
        %v45501 = vadd.s32 %v45493, %v45498 (stack65)
        %v45503 = vshll.u32 %v45498, 26 (stack73)
        %v45504 = vshrl.u32 %v45498, 6 (stack74)
        %v45505 = vor.u32 %v45503, %v45504 (stack75)
        %v45506 = vxor.u32 %v45501, %v45505 (stack76)
        %v45509 = vadd.s32 %v45501, %v45506 (stack65)
        %v45513 = vadd.s32 %v45509, %v8 (stack65)
        %v45515 = vshll.u32 %v45506, 6 (stack73)
        %v45516 = vshrl.u32 %v45506, 26 (stack74)
        %v45517 = vor.u32 %v45515, %v45516 (stack75)
        %v45518 = vxor.u32 %v45509, %v45517 (stack76)
        %v45521 = vadd.s32 %v45518, %v10 (stack65)
        %v45525 = vadd.s32 %v45521, 5 (stack65)
        %v45527 = vxor.u32 %v45513, %v45525 (stack76)
        %v45528 = vand.u32.u8 %v45527, 255 (stack77)
        %v45529 = vand.u32 %v45528, 65535 (stack78)
        %v45530 = vshrl.u32 %v45529, 1 (stack79)
        %v45531 = vor.u32 %v45530, 16256 (stack75)
        %v45532 = vand.u32.u16 %v45531, 65535 (stack80)
        %v45533 = vunpack.i.l.bf16 %v45532 (stack81)
        %v45537 = vadd.f32 %v45533, -1.0 (stack82)
        %v45541 = vmul.f32 %v45537, 2.0 (stack83)
        %v45545 = vadd.f32 %v45541, -0.99609375 (stack82)
        %v45549 = vmax.f32 -0.99609375, %v45545 (stack84)
        %v45551 = vand.u32 2147483647, %v45549 (stack85)
        %vm45554 = vcmp.eq.f32.partialorder %v45551, 1.0 (stack86)
        %v45559 = vmul.f32 %v45549, inf (stack83)
        %v45561 = vxor.u32 %v45549, 2147483648 (stack87)
        %v45564 = vmul.f32 %v45549, %v45561 (stack83)
        %v45566 = vadd.f32 %v45564, 1.0 (stack88)
        %v45567 = vlog2.pop %v45566 (stack89)
        %v45568 = vmul.f32 %v45567, 0.6931472 (stack90)
        %v45569 = vmul.f32 -0.5, %v45564 (stack91)
        %v45570 = vadd.f32 %v45569, 1.0 (stack92)
        %v45571 = vmul.f32 %v45570, %v45564 (stack93)
        %v45572 = vand.u32 2147483647, %v45564 (stack94)
        %vm45573 = vcmp.lt.f32.partialorder %v45572, 0.0004427343 (stack95)
        %v45574 = vsel /*vm=*/%vm45573, /*on_true_vy=*/%v45571, /*on_false_vx=*/%v45568 (stack96)
        %v45575 = vxor.u32 %v45574, 2147483648 (stack87)
        %vm45578 = vcmp.lt.f32.partialorder %v45575, 5.0 (stack86)
        %v45583 = vsel /*vm=*/%vm45578, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v45587 = vsel /*vm=*/%vm45578, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v45591 = vsel /*vm=*/%vm45578, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v45595 = vsel /*vm=*/%vm45578, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v45599 = vsel /*vm=*/%vm45578, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v45603 = vsel /*vm=*/%vm45578, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v45607 = vsel /*vm=*/%vm45578, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v45611 = vsel /*vm=*/%vm45578, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v45615 = vsel /*vm=*/%vm45578, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v45619 = vadd.f32 %v45575, -2.5 (stack82)
        %v45621 = vrsqrt.pop %v45575 (stack97)
        %v45622 = vmul.f32 %v45575, %v45621 (stack98)
        %vm45623 = vcmp.eq.f32.partialorder %v45575, inf (stack99)
        %v45624 = vsel /*vm=*/%vm45623, /*on_true_vy=*/%v45575, /*on_false_vx=*/%v45622 (stack100)
        %vm45625 = vcmp.eq.f32.partialorder %v45575, 0.0 (stack101)
        %v45626 = vand.u32 %v45575, 2147483648 (stack102)
        %v45627 = vsel /*vm=*/%vm45625, /*on_true_vy=*/%v45626, /*on_false_vx=*/%v45624 (stack103)
        %v45630 = vadd.f32 %v45627, -3.0 (stack82)
        %v45634 = vsel /*vm=*/%vm45578, /*on_true_vy=*/%v45619, /*on_false_vx=*/%v45630 (stack72)
        %v45638 = vmul.f32 %v45615, %v45634 (stack83)
        %v45642 = vadd.f32 %v45611, %v45638 (stack82)
        %v45646 = vmul.f32 %v45642, %v45634 (stack83)
        %v45650 = vadd.f32 %v45607, %v45646 (stack82)
        %v45654 = vmul.f32 %v45650, %v45634 (stack83)
        %v45658 = vadd.f32 %v45603, %v45654 (stack82)
        %v45662 = vmul.f32 %v45658, %v45634 (stack83)
        %v45666 = vadd.f32 %v45599, %v45662 (stack82)
        %v45670 = vmul.f32 %v45666, %v45634 (stack83)
        %v45674 = vadd.f32 %v45595, %v45670 (stack82)
        %v45678 = vmul.f32 %v45674, %v45634 (stack83)
        %v45682 = vadd.f32 %v45591, %v45678 (stack82)
        %v45686 = vmul.f32 %v45682, %v45634 (stack83)
        %v45690 = vadd.f32 %v45587, %v45686 (stack82)
        %v45694 = vmul.f32 %v45690, %v45634 (stack83)
        %v45698 = vadd.f32 %v45583, %v45694 (stack82)
        %v45702 = vmul.f32 %v45698, %v45549 (stack83)
        %v45706 = vsel /*vm=*/%vm45554, /*on_true_vy=*/%v45559, /*on_false_vx=*/%v45702 (stack72)
        %v45710 = vmul.f32 %v45706, 1.4140625 (stack83)
        %s45712 = scalar_lea.vmem %s280, 48 [#allocation0] (stack107)
        %v45713 = vpack.c.bf16 0.0, %v45710 (stack104)
        %45714 = vst [vmem:[%s45712] sm:$0xf] /*vst_source=*/%v45713 (stack105)
        %v45717 = vadd.s32 %v894, %v45253 (stack65)
        %s45719 = smul.u32 128, %s27 (stack66)
        %v45720 = vlaneseq (stack67)
        %v45721 = vand.u32 %v45720, 127 (stack68)
        %v45722 = vstv %s45719 (stack69)
        %v45723 = vadd.s32 %v45721, %v45722 (stack70)
        %v45727 = vadd.s32 %v45717, %v45723 (stack65)
        %vm45731 = vcmp.lt.u32.totalorder %v45727, %v45717 (stack71)
        %vm45736 = vcmp.lt.u32.totalorder %v45717, %v894 (stack71)
        %v45741 = vadd.s32 %v881, %v45236 (stack65)
        %v45745 = vadd.s32 %v45741, 1 (stack65)
        %v45749 = vsel /*vm=*/%vm45736, /*on_true_vy=*/%v45745, /*on_false_vx=*/%v45741 (stack72)
        %v45753 = vadd.s32 %v45749, 1 (stack65)
        %v45757 = vsel /*vm=*/%vm45731, /*on_true_vy=*/%v45753, /*on_false_vx=*/%v45749 (stack72)
        %v45762 = vadd.s32 %v45757, %v10 (stack65)
        %v45766 = vadd.s32 %v45727, %v9 (stack65)
        %v45770 = vadd.s32 %v45762, %v45766 (stack65)
        %v45772 = vshll.u32 %v45766, 13 (stack73)
        %v45773 = vshrl.u32 %v45766, 19 (stack74)
        %v45774 = vor.u32 %v45772, %v45773 (stack75)
        %v45775 = vxor.u32 %v45770, %v45774 (stack76)
        %v45778 = vadd.s32 %v45770, %v45775 (stack65)
        %v45780 = vshll.u32 %v45775, 15 (stack73)
        %v45781 = vshrl.u32 %v45775, 17 (stack74)
        %v45782 = vor.u32 %v45780, %v45781 (stack75)
        %v45783 = vxor.u32 %v45778, %v45782 (stack76)
        %v45786 = vadd.s32 %v45778, %v45783 (stack65)
        %v45788 = vshll.u32 %v45783, 26 (stack73)
        %v45789 = vshrl.u32 %v45783, 6 (stack74)
        %v45790 = vor.u32 %v45788, %v45789 (stack75)
        %v45791 = vxor.u32 %v45786, %v45790 (stack76)
        %v45794 = vadd.s32 %v45786, %v45791 (stack65)
        %v45798 = vadd.s32 %v45794, %v9 (stack65)
        %v45800 = vshll.u32 %v45791, 6 (stack73)
        %v45801 = vshrl.u32 %v45791, 26 (stack74)
        %v45802 = vor.u32 %v45800, %v45801 (stack75)
        %v45803 = vxor.u32 %v45794, %v45802 (stack76)
        %v45806 = vadd.s32 %v45803, %v8 (stack65)
        %v45810 = vadd.s32 %v45806, 1 (stack65)
        %v45814 = vadd.s32 %v45798, %v45810 (stack65)
        %v45816 = vshll.u32 %v45810, 17 (stack73)
        %v45817 = vshrl.u32 %v45810, 15 (stack74)
        %v45818 = vor.u32 %v45816, %v45817 (stack75)
        %v45819 = vxor.u32 %v45814, %v45818 (stack76)
        %v45822 = vadd.s32 %v45814, %v45819 (stack65)
        %v45824 = vshll.u32 %v45819, 29 (stack73)
        %v45825 = vshrl.u32 %v45819, 3 (stack74)
        %v45826 = vor.u32 %v45824, %v45825 (stack75)
        %v45827 = vxor.u32 %v45822, %v45826 (stack76)
        %v45830 = vadd.s32 %v45822, %v45827 (stack65)
        %v45832 = vshll.u32 %v45827, 16 (stack73)
        %v45833 = vshrl.u32 %v45827, 16 (stack74)
        %v45834 = vor.u32 %v45832, %v45833 (stack75)
        %v45835 = vxor.u32 %v45830, %v45834 (stack76)
        %v45838 = vadd.s32 %v45830, %v45835 (stack65)
        %v45842 = vadd.s32 %v45838, %v8 (stack65)
        %v45844 = vshll.u32 %v45835, 24 (stack73)
        %v45845 = vshrl.u32 %v45835, 8 (stack74)
        %v45846 = vor.u32 %v45844, %v45845 (stack75)
        %v45847 = vxor.u32 %v45838, %v45846 (stack76)
        %v45850 = vadd.s32 %v45847, %v10 (stack65)
        %v45854 = vadd.s32 %v45850, 2 (stack65)
        %v45858 = vadd.s32 %v45842, %v45854 (stack65)
        %v45860 = vshll.u32 %v45854, 13 (stack73)
        %v45861 = vshrl.u32 %v45854, 19 (stack74)
        %v45862 = vor.u32 %v45860, %v45861 (stack75)
        %v45863 = vxor.u32 %v45858, %v45862 (stack76)
        %v45866 = vadd.s32 %v45858, %v45863 (stack65)
        %v45868 = vshll.u32 %v45863, 15 (stack73)
        %v45869 = vshrl.u32 %v45863, 17 (stack74)
        %v45870 = vor.u32 %v45868, %v45869 (stack75)
        %v45871 = vxor.u32 %v45866, %v45870 (stack76)
        %v45874 = vadd.s32 %v45866, %v45871 (stack65)
        %v45876 = vshll.u32 %v45871, 26 (stack73)
        %v45877 = vshrl.u32 %v45871, 6 (stack74)
        %v45878 = vor.u32 %v45876, %v45877 (stack75)
        %v45879 = vxor.u32 %v45874, %v45878 (stack76)
        %v45882 = vadd.s32 %v45874, %v45879 (stack65)
        %v45886 = vadd.s32 %v45882, %v10 (stack65)
        %v45888 = vshll.u32 %v45879, 6 (stack73)
        %v45889 = vshrl.u32 %v45879, 26 (stack74)
        %v45890 = vor.u32 %v45888, %v45889 (stack75)
        %v45891 = vxor.u32 %v45882, %v45890 (stack76)
        %v45894 = vadd.s32 %v45891, %v9 (stack65)
        %v45898 = vadd.s32 %v45894, 3 (stack65)
        %v45902 = vadd.s32 %v45886, %v45898 (stack65)
        %v45904 = vshll.u32 %v45898, 17 (stack73)
        %v45905 = vshrl.u32 %v45898, 15 (stack74)
        %v45906 = vor.u32 %v45904, %v45905 (stack75)
        %v45907 = vxor.u32 %v45902, %v45906 (stack76)
        %v45910 = vadd.s32 %v45902, %v45907 (stack65)
        %v45912 = vshll.u32 %v45907, 29 (stack73)
        %v45913 = vshrl.u32 %v45907, 3 (stack74)
        %v45914 = vor.u32 %v45912, %v45913 (stack75)
        %v45915 = vxor.u32 %v45910, %v45914 (stack76)
        %v45918 = vadd.s32 %v45910, %v45915 (stack65)
        %v45920 = vshll.u32 %v45915, 16 (stack73)
        %v45921 = vshrl.u32 %v45915, 16 (stack74)
        %v45922 = vor.u32 %v45920, %v45921 (stack75)
        %v45923 = vxor.u32 %v45918, %v45922 (stack76)
        %v45926 = vadd.s32 %v45918, %v45923 (stack65)
        %v45930 = vadd.s32 %v45926, %v9 (stack65)
        %v45932 = vshll.u32 %v45923, 24 (stack73)
        %v45933 = vshrl.u32 %v45923, 8 (stack74)
        %v45934 = vor.u32 %v45932, %v45933 (stack75)
        %v45935 = vxor.u32 %v45926, %v45934 (stack76)
        %v45938 = vadd.s32 %v45935, %v8 (stack65)
        %v45942 = vadd.s32 %v45938, 4 (stack65)
        %v45946 = vadd.s32 %v45930, %v45942 (stack65)
        %v45948 = vshll.u32 %v45942, 13 (stack73)
        %v45949 = vshrl.u32 %v45942, 19 (stack74)
        %v45950 = vor.u32 %v45948, %v45949 (stack75)
        %v45951 = vxor.u32 %v45946, %v45950 (stack76)
        %v45954 = vadd.s32 %v45946, %v45951 (stack65)
        %v45956 = vshll.u32 %v45951, 15 (stack73)
        %v45957 = vshrl.u32 %v45951, 17 (stack74)
        %v45958 = vor.u32 %v45956, %v45957 (stack75)
        %v45959 = vxor.u32 %v45954, %v45958 (stack76)
        %v45962 = vadd.s32 %v45954, %v45959 (stack65)
        %v45964 = vshll.u32 %v45959, 26 (stack73)
        %v45965 = vshrl.u32 %v45959, 6 (stack74)
        %v45966 = vor.u32 %v45964, %v45965 (stack75)
        %v45967 = vxor.u32 %v45962, %v45966 (stack76)
        %v45970 = vadd.s32 %v45962, %v45967 (stack65)
        %v45974 = vadd.s32 %v45970, %v8 (stack65)
        %v45976 = vshll.u32 %v45967, 6 (stack73)
        %v45977 = vshrl.u32 %v45967, 26 (stack74)
        %v45978 = vor.u32 %v45976, %v45977 (stack75)
        %v45979 = vxor.u32 %v45970, %v45978 (stack76)
        %v45982 = vadd.s32 %v45979, %v10 (stack65)
        %v45986 = vadd.s32 %v45982, 5 (stack65)
        %v45988 = vxor.u32 %v45974, %v45986 (stack76)
        %v45989 = vand.u32.u8 %v45988, 255 (stack77)
        %v45990 = vand.u32 %v45989, 65535 (stack78)
        %v45991 = vshrl.u32 %v45990, 1 (stack79)
        %v45992 = vor.u32 %v45991, 16256 (stack75)
        %v45993 = vand.u32.u16 %v45992, 65535 (stack80)
        %v45994 = vunpack.i.l.bf16 %v45993 (stack81)
        %v45998 = vadd.f32 %v45994, -1.0 (stack82)
        %v46002 = vmul.f32 %v45998, 2.0 (stack83)
        %v46006 = vadd.f32 %v46002, -0.99609375 (stack82)
        %v46010 = vmax.f32 -0.99609375, %v46006 (stack84)
        %v46012 = vand.u32 2147483647, %v46010 (stack85)
        %vm46015 = vcmp.eq.f32.partialorder %v46012, 1.0 (stack86)
        %v46020 = vmul.f32 %v46010, inf (stack83)
        %v46022 = vxor.u32 %v46010, 2147483648 (stack87)
        %v46025 = vmul.f32 %v46010, %v46022 (stack83)
        %v46027 = vadd.f32 %v46025, 1.0 (stack88)
        %v46028 = vlog2.pop %v46027 (stack89)
        %v46029 = vmul.f32 %v46028, 0.6931472 (stack90)
        %v46030 = vmul.f32 -0.5, %v46025 (stack91)
        %v46031 = vadd.f32 %v46030, 1.0 (stack92)
        %v46032 = vmul.f32 %v46031, %v46025 (stack93)
        %v46033 = vand.u32 2147483647, %v46025 (stack94)
        %vm46034 = vcmp.lt.f32.partialorder %v46033, 0.0004427343 (stack95)
        %v46035 = vsel /*vm=*/%vm46034, /*on_true_vy=*/%v46032, /*on_false_vx=*/%v46029 (stack96)
        %v46036 = vxor.u32 %v46035, 2147483648 (stack87)
        %vm46039 = vcmp.lt.f32.partialorder %v46036, 5.0 (stack86)
        %v46044 = vsel /*vm=*/%vm46039, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v46048 = vsel /*vm=*/%vm46039, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v46052 = vsel /*vm=*/%vm46039, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v46056 = vsel /*vm=*/%vm46039, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v46060 = vsel /*vm=*/%vm46039, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v46064 = vsel /*vm=*/%vm46039, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v46068 = vsel /*vm=*/%vm46039, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v46072 = vsel /*vm=*/%vm46039, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v46076 = vsel /*vm=*/%vm46039, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v46080 = vadd.f32 %v46036, -2.5 (stack82)
        %v46082 = vrsqrt.pop %v46036 (stack97)
        %v46083 = vmul.f32 %v46036, %v46082 (stack98)
        %vm46084 = vcmp.eq.f32.partialorder %v46036, inf (stack99)
        %v46085 = vsel /*vm=*/%vm46084, /*on_true_vy=*/%v46036, /*on_false_vx=*/%v46083 (stack100)
        %vm46086 = vcmp.eq.f32.partialorder %v46036, 0.0 (stack101)
        %v46087 = vand.u32 %v46036, 2147483648 (stack102)
        %v46088 = vsel /*vm=*/%vm46086, /*on_true_vy=*/%v46087, /*on_false_vx=*/%v46085 (stack103)
        %v46091 = vadd.f32 %v46088, -3.0 (stack82)
        %v46095 = vsel /*vm=*/%vm46039, /*on_true_vy=*/%v46080, /*on_false_vx=*/%v46091 (stack72)
        %v46099 = vmul.f32 %v46076, %v46095 (stack83)
        %v46103 = vadd.f32 %v46072, %v46099 (stack82)
        %v46107 = vmul.f32 %v46103, %v46095 (stack83)
        %v46111 = vadd.f32 %v46068, %v46107 (stack82)
        %v46115 = vmul.f32 %v46111, %v46095 (stack83)
        %v46119 = vadd.f32 %v46064, %v46115 (stack82)
        %v46123 = vmul.f32 %v46119, %v46095 (stack83)
        %v46127 = vadd.f32 %v46060, %v46123 (stack82)
        %v46131 = vmul.f32 %v46127, %v46095 (stack83)
        %v46135 = vadd.f32 %v46056, %v46131 (stack82)
        %v46139 = vmul.f32 %v46135, %v46095 (stack83)
        %v46143 = vadd.f32 %v46052, %v46139 (stack82)
        %v46147 = vmul.f32 %v46143, %v46095 (stack83)
        %v46151 = vadd.f32 %v46048, %v46147 (stack82)
        %v46155 = vmul.f32 %v46151, %v46095 (stack83)
        %v46159 = vadd.f32 %v46044, %v46155 (stack82)
        %v46163 = vmul.f32 %v46159, %v46010 (stack83)
        %v46167 = vsel /*vm=*/%vm46015, /*on_true_vy=*/%v46020, /*on_false_vx=*/%v46163 (stack72)
        %v46171 = vmul.f32 %v46167, 1.4140625 (stack83)
        %s46173 = scalar_lea.vmem %s280, 176 [#allocation0] (stack107)
        %v46174 = vpack.c.bf16 0.0, %v46171 (stack104)
        %46175 = vst [vmem:[%s46173] sm:$0xf] /*vst_source=*/%v46174 (stack105)
        %v46178 = vadd.s32 %v1381, %v45253 (stack65)
        %s46180 = smul.u32 128, %s27 (stack66)
        %v46181 = vlaneseq (stack67)
        %v46182 = vand.u32 %v46181, 127 (stack68)
        %v46183 = vstv %s46180 (stack69)
        %v46184 = vadd.s32 %v46182, %v46183 (stack70)
        %v46188 = vadd.s32 %v46178, %v46184 (stack65)
        %vm46192 = vcmp.lt.u32.totalorder %v46188, %v46178 (stack71)
        %vm46197 = vcmp.lt.u32.totalorder %v46178, %v1381 (stack71)
        %v46202 = vadd.s32 %v1368, %v45236 (stack65)
        %v46206 = vadd.s32 %v46202, 1 (stack65)
        %v46210 = vsel /*vm=*/%vm46197, /*on_true_vy=*/%v46206, /*on_false_vx=*/%v46202 (stack72)
        %v46214 = vadd.s32 %v46210, 1 (stack65)
        %v46218 = vsel /*vm=*/%vm46192, /*on_true_vy=*/%v46214, /*on_false_vx=*/%v46210 (stack72)
        %v46223 = vadd.s32 %v46218, %v10 (stack65)
        %v46227 = vadd.s32 %v46188, %v9 (stack65)
        %v46231 = vadd.s32 %v46223, %v46227 (stack65)
        %v46233 = vshll.u32 %v46227, 13 (stack73)
        %v46234 = vshrl.u32 %v46227, 19 (stack74)
        %v46235 = vor.u32 %v46233, %v46234 (stack75)
        %v46236 = vxor.u32 %v46231, %v46235 (stack76)
        %v46239 = vadd.s32 %v46231, %v46236 (stack65)
        %v46241 = vshll.u32 %v46236, 15 (stack73)
        %v46242 = vshrl.u32 %v46236, 17 (stack74)
        %v46243 = vor.u32 %v46241, %v46242 (stack75)
        %v46244 = vxor.u32 %v46239, %v46243 (stack76)
        %v46247 = vadd.s32 %v46239, %v46244 (stack65)
        %v46249 = vshll.u32 %v46244, 26 (stack73)
        %v46250 = vshrl.u32 %v46244, 6 (stack74)
        %v46251 = vor.u32 %v46249, %v46250 (stack75)
        %v46252 = vxor.u32 %v46247, %v46251 (stack76)
        %v46255 = vadd.s32 %v46247, %v46252 (stack65)
        %v46259 = vadd.s32 %v46255, %v9 (stack65)
        %v46261 = vshll.u32 %v46252, 6 (stack73)
        %v46262 = vshrl.u32 %v46252, 26 (stack74)
        %v46263 = vor.u32 %v46261, %v46262 (stack75)
        %v46264 = vxor.u32 %v46255, %v46263 (stack76)
        %v46267 = vadd.s32 %v46264, %v8 (stack65)
        %v46271 = vadd.s32 %v46267, 1 (stack65)
        %v46275 = vadd.s32 %v46259, %v46271 (stack65)
        %v46277 = vshll.u32 %v46271, 17 (stack73)
        %v46278 = vshrl.u32 %v46271, 15 (stack74)
        %v46279 = vor.u32 %v46277, %v46278 (stack75)
        %v46280 = vxor.u32 %v46275, %v46279 (stack76)
        %v46283 = vadd.s32 %v46275, %v46280 (stack65)
        %v46285 = vshll.u32 %v46280, 29 (stack73)
        %v46286 = vshrl.u32 %v46280, 3 (stack74)
        %v46287 = vor.u32 %v46285, %v46286 (stack75)
        %v46288 = vxor.u32 %v46283, %v46287 (stack76)
        %v46291 = vadd.s32 %v46283, %v46288 (stack65)
        %v46293 = vshll.u32 %v46288, 16 (stack73)
        %v46294 = vshrl.u32 %v46288, 16 (stack74)
        %v46295 = vor.u32 %v46293, %v46294 (stack75)
        %v46296 = vxor.u32 %v46291, %v46295 (stack76)
        %v46299 = vadd.s32 %v46291, %v46296 (stack65)
        %v46303 = vadd.s32 %v46299, %v8 (stack65)
        %v46305 = vshll.u32 %v46296, 24 (stack73)
        %v46306 = vshrl.u32 %v46296, 8 (stack74)
        %v46307 = vor.u32 %v46305, %v46306 (stack75)
        %v46308 = vxor.u32 %v46299, %v46307 (stack76)
        %v46311 = vadd.s32 %v46308, %v10 (stack65)
        %v46315 = vadd.s32 %v46311, 2 (stack65)
        %v46319 = vadd.s32 %v46303, %v46315 (stack65)
        %v46321 = vshll.u32 %v46315, 13 (stack73)
        %v46322 = vshrl.u32 %v46315, 19 (stack74)
        %v46323 = vor.u32 %v46321, %v46322 (stack75)
        %v46324 = vxor.u32 %v46319, %v46323 (stack76)
        %v46327 = vadd.s32 %v46319, %v46324 (stack65)
        %v46329 = vshll.u32 %v46324, 15 (stack73)
        %v46330 = vshrl.u32 %v46324, 17 (stack74)
        %v46331 = vor.u32 %v46329, %v46330 (stack75)
        %v46332 = vxor.u32 %v46327, %v46331 (stack76)
        %v46335 = vadd.s32 %v46327, %v46332 (stack65)
        %v46337 = vshll.u32 %v46332, 26 (stack73)
        %v46338 = vshrl.u32 %v46332, 6 (stack74)
        %v46339 = vor.u32 %v46337, %v46338 (stack75)
        %v46340 = vxor.u32 %v46335, %v46339 (stack76)
        %v46343 = vadd.s32 %v46335, %v46340 (stack65)
        %v46347 = vadd.s32 %v46343, %v10 (stack65)
        %v46349 = vshll.u32 %v46340, 6 (stack73)
        %v46350 = vshrl.u32 %v46340, 26 (stack74)
        %v46351 = vor.u32 %v46349, %v46350 (stack75)
        %v46352 = vxor.u32 %v46343, %v46351 (stack76)
        %v46355 = vadd.s32 %v46352, %v9 (stack65)
        %v46359 = vadd.s32 %v46355, 3 (stack65)
        %v46363 = vadd.s32 %v46347, %v46359 (stack65)
        %v46365 = vshll.u32 %v46359, 17 (stack73)
        %v46366 = vshrl.u32 %v46359, 15 (stack74)
        %v46367 = vor.u32 %v46365, %v46366 (stack75)
        %v46368 = vxor.u32 %v46363, %v46367 (stack76)
        %v46371 = vadd.s32 %v46363, %v46368 (stack65)
        %v46373 = vshll.u32 %v46368, 29 (stack73)
        %v46374 = vshrl.u32 %v46368, 3 (stack74)
        %v46375 = vor.u32 %v46373, %v46374 (stack75)
        %v46376 = vxor.u32 %v46371, %v46375 (stack76)
        %v46379 = vadd.s32 %v46371, %v46376 (stack65)
        %v46381 = vshll.u32 %v46376, 16 (stack73)
        %v46382 = vshrl.u32 %v46376, 16 (stack74)
        %v46383 = vor.u32 %v46381, %v46382 (stack75)
        %v46384 = vxor.u32 %v46379, %v46383 (stack76)
        %v46387 = vadd.s32 %v46379, %v46384 (stack65)
        %v46391 = vadd.s32 %v46387, %v9 (stack65)
        %v46393 = vshll.u32 %v46384, 24 (stack73)
        %v46394 = vshrl.u32 %v46384, 8 (stack74)
        %v46395 = vor.u32 %v46393, %v46394 (stack75)
        %v46396 = vxor.u32 %v46387, %v46395 (stack76)
        %v46399 = vadd.s32 %v46396, %v8 (stack65)
        %v46403 = vadd.s32 %v46399, 4 (stack65)
        %v46407 = vadd.s32 %v46391, %v46403 (stack65)
        %v46409 = vshll.u32 %v46403, 13 (stack73)
        %v46410 = vshrl.u32 %v46403, 19 (stack74)
        %v46411 = vor.u32 %v46409, %v46410 (stack75)
        %v46412 = vxor.u32 %v46407, %v46411 (stack76)
        %v46415 = vadd.s32 %v46407, %v46412 (stack65)
        %v46417 = vshll.u32 %v46412, 15 (stack73)
        %v46418 = vshrl.u32 %v46412, 17 (stack74)
        %v46419 = vor.u32 %v46417, %v46418 (stack75)
        %v46420 = vxor.u32 %v46415, %v46419 (stack76)
        %v46423 = vadd.s32 %v46415, %v46420 (stack65)
        %v46425 = vshll.u32 %v46420, 26 (stack73)
        %v46426 = vshrl.u32 %v46420, 6 (stack74)
        %v46427 = vor.u32 %v46425, %v46426 (stack75)
        %v46428 = vxor.u32 %v46423, %v46427 (stack76)
        %v46431 = vadd.s32 %v46423, %v46428 (stack65)
        %v46435 = vadd.s32 %v46431, %v8 (stack65)
        %v46437 = vshll.u32 %v46428, 6 (stack73)
        %v46438 = vshrl.u32 %v46428, 26 (stack74)
        %v46439 = vor.u32 %v46437, %v46438 (stack75)
        %v46440 = vxor.u32 %v46431, %v46439 (stack76)
        %v46443 = vadd.s32 %v46440, %v10 (stack65)
        %v46447 = vadd.s32 %v46443, 5 (stack65)
        %v46449 = vxor.u32 %v46435, %v46447 (stack76)
        %v46450 = vand.u32.u8 %v46449, 255 (stack77)
        %v46451 = vand.u32 %v46450, 65535 (stack78)
        %v46452 = vshrl.u32 %v46451, 1 (stack79)
        %v46453 = vor.u32 %v46452, 16256 (stack75)
        %v46454 = vand.u32.u16 %v46453, 65535 (stack80)
        %v46455 = vunpack.i.l.bf16 %v46454 (stack81)
        %v46459 = vadd.f32 %v46455, -1.0 (stack82)
        %v46463 = vmul.f32 %v46459, 2.0 (stack83)
        %v46467 = vadd.f32 %v46463, -0.99609375 (stack82)
        %v46471 = vmax.f32 -0.99609375, %v46467 (stack84)
        %v46473 = vand.u32 2147483647, %v46471 (stack85)
        %vm46476 = vcmp.eq.f32.partialorder %v46473, 1.0 (stack86)
        %v46481 = vmul.f32 %v46471, inf (stack83)
        %v46483 = vxor.u32 %v46471, 2147483648 (stack87)
        %v46486 = vmul.f32 %v46471, %v46483 (stack83)
        %v46488 = vadd.f32 %v46486, 1.0 (stack88)
        %v46489 = vlog2.pop %v46488 (stack89)
        %v46490 = vmul.f32 %v46489, 0.6931472 (stack90)
        %v46491 = vmul.f32 -0.5, %v46486 (stack91)
        %v46492 = vadd.f32 %v46491, 1.0 (stack92)
        %v46493 = vmul.f32 %v46492, %v46486 (stack93)
        %v46494 = vand.u32 2147483647, %v46486 (stack94)
        %vm46495 = vcmp.lt.f32.partialorder %v46494, 0.0004427343 (stack95)
        %v46496 = vsel /*vm=*/%vm46495, /*on_true_vy=*/%v46493, /*on_false_vx=*/%v46490 (stack96)
        %v46497 = vxor.u32 %v46496, 2147483648 (stack87)
        %vm46500 = vcmp.lt.f32.partialorder %v46497, 5.0 (stack86)
        %v46505 = vsel /*vm=*/%vm46500, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v46509 = vsel /*vm=*/%vm46500, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v46513 = vsel /*vm=*/%vm46500, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v46517 = vsel /*vm=*/%vm46500, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v46521 = vsel /*vm=*/%vm46500, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v46525 = vsel /*vm=*/%vm46500, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v46529 = vsel /*vm=*/%vm46500, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v46533 = vsel /*vm=*/%vm46500, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v46537 = vsel /*vm=*/%vm46500, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v46541 = vadd.f32 %v46497, -2.5 (stack82)
        %v46543 = vrsqrt.pop %v46497 (stack97)
        %v46544 = vmul.f32 %v46497, %v46543 (stack98)
        %vm46545 = vcmp.eq.f32.partialorder %v46497, inf (stack99)
        %v46546 = vsel /*vm=*/%vm46545, /*on_true_vy=*/%v46497, /*on_false_vx=*/%v46544 (stack100)
        %vm46547 = vcmp.eq.f32.partialorder %v46497, 0.0 (stack101)
        %v46548 = vand.u32 %v46497, 2147483648 (stack102)
        %v46549 = vsel /*vm=*/%vm46547, /*on_true_vy=*/%v46548, /*on_false_vx=*/%v46546 (stack103)
        %v46552 = vadd.f32 %v46549, -3.0 (stack82)
        %v46556 = vsel /*vm=*/%vm46500, /*on_true_vy=*/%v46541, /*on_false_vx=*/%v46552 (stack72)
        %v46560 = vmul.f32 %v46537, %v46556 (stack83)
        %v46564 = vadd.f32 %v46533, %v46560 (stack82)
        %v46568 = vmul.f32 %v46564, %v46556 (stack83)
        %v46572 = vadd.f32 %v46529, %v46568 (stack82)
        %v46576 = vmul.f32 %v46572, %v46556 (stack83)
        %v46580 = vadd.f32 %v46525, %v46576 (stack82)
        %v46584 = vmul.f32 %v46580, %v46556 (stack83)
        %v46588 = vadd.f32 %v46521, %v46584 (stack82)
        %v46592 = vmul.f32 %v46588, %v46556 (stack83)
        %v46596 = vadd.f32 %v46517, %v46592 (stack82)
        %v46600 = vmul.f32 %v46596, %v46556 (stack83)
        %v46604 = vadd.f32 %v46513, %v46600 (stack82)
        %v46608 = vmul.f32 %v46604, %v46556 (stack83)
        %v46612 = vadd.f32 %v46509, %v46608 (stack82)
        %v46616 = vmul.f32 %v46612, %v46556 (stack83)
        %v46620 = vadd.f32 %v46505, %v46616 (stack82)
        %v46624 = vmul.f32 %v46620, %v46471 (stack83)
        %v46628 = vsel /*vm=*/%vm46476, /*on_true_vy=*/%v46481, /*on_false_vx=*/%v46624 (stack72)
        %v46632 = vmul.f32 %v46628, 1.4140625 (stack83)
        %s46634 = scalar_lea.vmem %s280, 304 [#allocation0] (stack107)
        %v46635 = vpack.c.bf16 0.0, %v46632 (stack104)
        %46636 = vst [vmem:[%s46634] sm:$0xf] /*vst_source=*/%v46635 (stack105)
        %v46639 = vadd.s32 %v1868, %v45253 (stack65)
        %s46641 = smul.u32 128, %s27 (stack66)
        %v46642 = vlaneseq (stack67)
        %v46643 = vand.u32 %v46642, 127 (stack68)
        %v46644 = vstv %s46641 (stack69)
        %v46645 = vadd.s32 %v46643, %v46644 (stack70)
        %v46649 = vadd.s32 %v46639, %v46645 (stack65)
        %vm46653 = vcmp.lt.u32.totalorder %v46649, %v46639 (stack71)
        %vm46658 = vcmp.lt.u32.totalorder %v46639, %v1868 (stack71)
        %v46663 = vadd.s32 %v1855, %v45236 (stack65)
        %v46667 = vadd.s32 %v46663, 1 (stack65)
        %v46671 = vsel /*vm=*/%vm46658, /*on_true_vy=*/%v46667, /*on_false_vx=*/%v46663 (stack72)
        %v46675 = vadd.s32 %v46671, 1 (stack65)
        %v46679 = vsel /*vm=*/%vm46653, /*on_true_vy=*/%v46675, /*on_false_vx=*/%v46671 (stack72)
        %v46684 = vadd.s32 %v46679, %v10 (stack65)
        %v46688 = vadd.s32 %v46649, %v9 (stack65)
        %v46692 = vadd.s32 %v46684, %v46688 (stack65)
        %v46694 = vshll.u32 %v46688, 13 (stack73)
        %v46695 = vshrl.u32 %v46688, 19 (stack74)
        %v46696 = vor.u32 %v46694, %v46695 (stack75)
        %v46697 = vxor.u32 %v46692, %v46696 (stack76)
        %v46700 = vadd.s32 %v46692, %v46697 (stack65)
        %v46702 = vshll.u32 %v46697, 15 (stack73)
        %v46703 = vshrl.u32 %v46697, 17 (stack74)
        %v46704 = vor.u32 %v46702, %v46703 (stack75)
        %v46705 = vxor.u32 %v46700, %v46704 (stack76)
        %v46708 = vadd.s32 %v46700, %v46705 (stack65)
        %v46710 = vshll.u32 %v46705, 26 (stack73)
        %v46711 = vshrl.u32 %v46705, 6 (stack74)
        %v46712 = vor.u32 %v46710, %v46711 (stack75)
        %v46713 = vxor.u32 %v46708, %v46712 (stack76)
        %v46716 = vadd.s32 %v46708, %v46713 (stack65)
        %v46720 = vadd.s32 %v46716, %v9 (stack65)
        %v46722 = vshll.u32 %v46713, 6 (stack73)
        %v46723 = vshrl.u32 %v46713, 26 (stack74)
        %v46724 = vor.u32 %v46722, %v46723 (stack75)
        %v46725 = vxor.u32 %v46716, %v46724 (stack76)
        %v46728 = vadd.s32 %v46725, %v8 (stack65)
        %v46732 = vadd.s32 %v46728, 1 (stack65)
        %v46736 = vadd.s32 %v46720, %v46732 (stack65)
        %v46738 = vshll.u32 %v46732, 17 (stack73)
        %v46739 = vshrl.u32 %v46732, 15 (stack74)
        %v46740 = vor.u32 %v46738, %v46739 (stack75)
        %v46741 = vxor.u32 %v46736, %v46740 (stack76)
        %v46744 = vadd.s32 %v46736, %v46741 (stack65)
        %v46746 = vshll.u32 %v46741, 29 (stack73)
        %v46747 = vshrl.u32 %v46741, 3 (stack74)
        %v46748 = vor.u32 %v46746, %v46747 (stack75)
        %v46749 = vxor.u32 %v46744, %v46748 (stack76)
        %v46752 = vadd.s32 %v46744, %v46749 (stack65)
        %v46754 = vshll.u32 %v46749, 16 (stack73)
        %v46755 = vshrl.u32 %v46749, 16 (stack74)
        %v46756 = vor.u32 %v46754, %v46755 (stack75)
        %v46757 = vxor.u32 %v46752, %v46756 (stack76)
        %v46760 = vadd.s32 %v46752, %v46757 (stack65)
        %v46764 = vadd.s32 %v46760, %v8 (stack65)
        %v46766 = vshll.u32 %v46757, 24 (stack73)
        %v46767 = vshrl.u32 %v46757, 8 (stack74)
        %v46768 = vor.u32 %v46766, %v46767 (stack75)
        %v46769 = vxor.u32 %v46760, %v46768 (stack76)
        %v46772 = vadd.s32 %v46769, %v10 (stack65)
        %v46776 = vadd.s32 %v46772, 2 (stack65)
        %v46780 = vadd.s32 %v46764, %v46776 (stack65)
        %v46782 = vshll.u32 %v46776, 13 (stack73)
        %v46783 = vshrl.u32 %v46776, 19 (stack74)
        %v46784 = vor.u32 %v46782, %v46783 (stack75)
        %v46785 = vxor.u32 %v46780, %v46784 (stack76)
        %v46788 = vadd.s32 %v46780, %v46785 (stack65)
        %v46790 = vshll.u32 %v46785, 15 (stack73)
        %v46791 = vshrl.u32 %v46785, 17 (stack74)
        %v46792 = vor.u32 %v46790, %v46791 (stack75)
        %v46793 = vxor.u32 %v46788, %v46792 (stack76)
        %v46796 = vadd.s32 %v46788, %v46793 (stack65)
        %v46798 = vshll.u32 %v46793, 26 (stack73)
        %v46799 = vshrl.u32 %v46793, 6 (stack74)
        %v46800 = vor.u32 %v46798, %v46799 (stack75)
        %v46801 = vxor.u32 %v46796, %v46800 (stack76)
        %v46804 = vadd.s32 %v46796, %v46801 (stack65)
        %v46808 = vadd.s32 %v46804, %v10 (stack65)
        %v46810 = vshll.u32 %v46801, 6 (stack73)
        %v46811 = vshrl.u32 %v46801, 26 (stack74)
        %v46812 = vor.u32 %v46810, %v46811 (stack75)
        %v46813 = vxor.u32 %v46804, %v46812 (stack76)
        %v46816 = vadd.s32 %v46813, %v9 (stack65)
        %v46820 = vadd.s32 %v46816, 3 (stack65)
        %v46824 = vadd.s32 %v46808, %v46820 (stack65)
        %v46826 = vshll.u32 %v46820, 17 (stack73)
        %v46827 = vshrl.u32 %v46820, 15 (stack74)
        %v46828 = vor.u32 %v46826, %v46827 (stack75)
        %v46829 = vxor.u32 %v46824, %v46828 (stack76)
        %v46832 = vadd.s32 %v46824, %v46829 (stack65)
        %v46834 = vshll.u32 %v46829, 29 (stack73)
        %v46835 = vshrl.u32 %v46829, 3 (stack74)
        %v46836 = vor.u32 %v46834, %v46835 (stack75)
        %v46837 = vxor.u32 %v46832, %v46836 (stack76)
        %v46840 = vadd.s32 %v46832, %v46837 (stack65)
        %v46842 = vshll.u32 %v46837, 16 (stack73)
        %v46843 = vshrl.u32 %v46837, 16 (stack74)
        %v46844 = vor.u32 %v46842, %v46843 (stack75)
        %v46845 = vxor.u32 %v46840, %v46844 (stack76)
        %v46848 = vadd.s32 %v46840, %v46845 (stack65)
        %v46852 = vadd.s32 %v46848, %v9 (stack65)
        %v46854 = vshll.u32 %v46845, 24 (stack73)
        %v46855 = vshrl.u32 %v46845, 8 (stack74)
        %v46856 = vor.u32 %v46854, %v46855 (stack75)
        %v46857 = vxor.u32 %v46848, %v46856 (stack76)
        %v46860 = vadd.s32 %v46857, %v8 (stack65)
        %v46864 = vadd.s32 %v46860, 4 (stack65)
        %v46868 = vadd.s32 %v46852, %v46864 (stack65)
        %v46870 = vshll.u32 %v46864, 13 (stack73)
        %v46871 = vshrl.u32 %v46864, 19 (stack74)
        %v46872 = vor.u32 %v46870, %v46871 (stack75)
        %v46873 = vxor.u32 %v46868, %v46872 (stack76)
        %v46876 = vadd.s32 %v46868, %v46873 (stack65)
        %v46878 = vshll.u32 %v46873, 15 (stack73)
        %v46879 = vshrl.u32 %v46873, 17 (stack74)
        %v46880 = vor.u32 %v46878, %v46879 (stack75)
        %v46881 = vxor.u32 %v46876, %v46880 (stack76)
        %v46884 = vadd.s32 %v46876, %v46881 (stack65)
        %v46886 = vshll.u32 %v46881, 26 (stack73)
        %v46887 = vshrl.u32 %v46881, 6 (stack74)
        %v46888 = vor.u32 %v46886, %v46887 (stack75)
        %v46889 = vxor.u32 %v46884, %v46888 (stack76)
        %v46892 = vadd.s32 %v46884, %v46889 (stack65)
        %v46896 = vadd.s32 %v46892, %v8 (stack65)
        %v46898 = vshll.u32 %v46889, 6 (stack73)
        %v46899 = vshrl.u32 %v46889, 26 (stack74)
        %v46900 = vor.u32 %v46898, %v46899 (stack75)
        %v46901 = vxor.u32 %v46892, %v46900 (stack76)
        %v46904 = vadd.s32 %v46901, %v10 (stack65)
        %v46908 = vadd.s32 %v46904, 5 (stack65)
        %v46910 = vxor.u32 %v46896, %v46908 (stack76)
        %v46911 = vand.u32.u8 %v46910, 255 (stack77)
        %v46912 = vand.u32 %v46911, 65535 (stack78)
        %v46913 = vshrl.u32 %v46912, 1 (stack79)
        %v46914 = vor.u32 %v46913, 16256 (stack75)
        %v46915 = vand.u32.u16 %v46914, 65535 (stack80)
        %v46916 = vunpack.i.l.bf16 %v46915 (stack81)
        %v46920 = vadd.f32 %v46916, -1.0 (stack82)
        %v46924 = vmul.f32 %v46920, 2.0 (stack83)
        %v46928 = vadd.f32 %v46924, -0.99609375 (stack82)
        %v46932 = vmax.f32 -0.99609375, %v46928 (stack84)
        %v46934 = vand.u32 2147483647, %v46932 (stack85)
        %vm46937 = vcmp.eq.f32.partialorder %v46934, 1.0 (stack86)
        %v46942 = vmul.f32 %v46932, inf (stack83)
        %v46944 = vxor.u32 %v46932, 2147483648 (stack87)
        %v46947 = vmul.f32 %v46932, %v46944 (stack83)
        %v46949 = vadd.f32 %v46947, 1.0 (stack88)
        %v46950 = vlog2.pop %v46949 (stack89)
        %v46951 = vmul.f32 %v46950, 0.6931472 (stack90)
        %v46952 = vmul.f32 -0.5, %v46947 (stack91)
        %v46953 = vadd.f32 %v46952, 1.0 (stack92)
        %v46954 = vmul.f32 %v46953, %v46947 (stack93)
        %v46955 = vand.u32 2147483647, %v46947 (stack94)
        %vm46956 = vcmp.lt.f32.partialorder %v46955, 0.0004427343 (stack95)
        %v46957 = vsel /*vm=*/%vm46956, /*on_true_vy=*/%v46954, /*on_false_vx=*/%v46951 (stack96)
        %v46958 = vxor.u32 %v46957, 2147483648 (stack87)
        %vm46961 = vcmp.lt.f32.partialorder %v46958, 5.0 (stack86)
        %v46966 = vsel /*vm=*/%vm46961, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v46970 = vsel /*vm=*/%vm46961, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v46974 = vsel /*vm=*/%vm46961, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v46978 = vsel /*vm=*/%vm46961, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v46982 = vsel /*vm=*/%vm46961, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v46986 = vsel /*vm=*/%vm46961, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v46990 = vsel /*vm=*/%vm46961, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v46994 = vsel /*vm=*/%vm46961, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v46998 = vsel /*vm=*/%vm46961, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v47002 = vadd.f32 %v46958, -2.5 (stack82)
        %v47004 = vrsqrt.pop %v46958 (stack97)
        %v47005 = vmul.f32 %v46958, %v47004 (stack98)
        %vm47006 = vcmp.eq.f32.partialorder %v46958, inf (stack99)
        %v47007 = vsel /*vm=*/%vm47006, /*on_true_vy=*/%v46958, /*on_false_vx=*/%v47005 (stack100)
        %vm47008 = vcmp.eq.f32.partialorder %v46958, 0.0 (stack101)
        %v47009 = vand.u32 %v46958, 2147483648 (stack102)
        %v47010 = vsel /*vm=*/%vm47008, /*on_true_vy=*/%v47009, /*on_false_vx=*/%v47007 (stack103)
        %v47013 = vadd.f32 %v47010, -3.0 (stack82)
        %v47017 = vsel /*vm=*/%vm46961, /*on_true_vy=*/%v47002, /*on_false_vx=*/%v47013 (stack72)
        %v47021 = vmul.f32 %v46998, %v47017 (stack83)
        %v47025 = vadd.f32 %v46994, %v47021 (stack82)
        %v47029 = vmul.f32 %v47025, %v47017 (stack83)
        %v47033 = vadd.f32 %v46990, %v47029 (stack82)
        %v47037 = vmul.f32 %v47033, %v47017 (stack83)
        %v47041 = vadd.f32 %v46986, %v47037 (stack82)
        %v47045 = vmul.f32 %v47041, %v47017 (stack83)
        %v47049 = vadd.f32 %v46982, %v47045 (stack82)
        %v47053 = vmul.f32 %v47049, %v47017 (stack83)
        %v47057 = vadd.f32 %v46978, %v47053 (stack82)
        %v47061 = vmul.f32 %v47057, %v47017 (stack83)
        %v47065 = vadd.f32 %v46974, %v47061 (stack82)
        %v47069 = vmul.f32 %v47065, %v47017 (stack83)
        %v47073 = vadd.f32 %v46970, %v47069 (stack82)
        %v47077 = vmul.f32 %v47073, %v47017 (stack83)
        %v47081 = vadd.f32 %v46966, %v47077 (stack82)
        %v47085 = vmul.f32 %v47081, %v46932 (stack83)
        %v47089 = vsel /*vm=*/%vm46937, /*on_true_vy=*/%v46942, /*on_false_vx=*/%v47085 (stack72)
        %v47093 = vmul.f32 %v47089, 1.4140625 (stack83)
        %s47095 = scalar_lea.vmem %s280, 432 [#allocation0] (stack107)
        %v47096 = vpack.c.bf16 0.0, %v47093 (stack104)
        %47097 = vst [vmem:[%s47095] sm:$0xf] /*vst_source=*/%v47096 (stack105)
        %v47100 = vadd.s32 %v2355, %v45253 (stack65)
        %s47102 = smul.u32 128, %s27 (stack66)
        %v47103 = vlaneseq (stack67)
        %v47104 = vand.u32 %v47103, 127 (stack68)
        %v47105 = vstv %s47102 (stack69)
        %v47106 = vadd.s32 %v47104, %v47105 (stack70)
        %v47110 = vadd.s32 %v47100, %v47106 (stack65)
        %vm47114 = vcmp.lt.u32.totalorder %v47110, %v47100 (stack71)
        %vm47119 = vcmp.lt.u32.totalorder %v47100, %v2355 (stack71)
        %v47124 = vadd.s32 %v2342, %v45236 (stack65)
        %v47128 = vadd.s32 %v47124, 1 (stack65)
        %v47132 = vsel /*vm=*/%vm47119, /*on_true_vy=*/%v47128, /*on_false_vx=*/%v47124 (stack72)
        %v47136 = vadd.s32 %v47132, 1 (stack65)
        %v47140 = vsel /*vm=*/%vm47114, /*on_true_vy=*/%v47136, /*on_false_vx=*/%v47132 (stack72)
        %v47145 = vadd.s32 %v47140, %v10 (stack65)
        %v47149 = vadd.s32 %v47110, %v9 (stack65)
        %v47153 = vadd.s32 %v47145, %v47149 (stack65)
        %v47155 = vshll.u32 %v47149, 13 (stack73)
        %v47156 = vshrl.u32 %v47149, 19 (stack74)
        %v47157 = vor.u32 %v47155, %v47156 (stack75)
        %v47158 = vxor.u32 %v47153, %v47157 (stack76)
        %v47161 = vadd.s32 %v47153, %v47158 (stack65)
        %v47163 = vshll.u32 %v47158, 15 (stack73)
        %v47164 = vshrl.u32 %v47158, 17 (stack74)
        %v47165 = vor.u32 %v47163, %v47164 (stack75)
        %v47166 = vxor.u32 %v47161, %v47165 (stack76)
        %v47169 = vadd.s32 %v47161, %v47166 (stack65)
        %v47171 = vshll.u32 %v47166, 26 (stack73)
        %v47172 = vshrl.u32 %v47166, 6 (stack74)
        %v47173 = vor.u32 %v47171, %v47172 (stack75)
        %v47174 = vxor.u32 %v47169, %v47173 (stack76)
        %v47177 = vadd.s32 %v47169, %v47174 (stack65)
        %v47181 = vadd.s32 %v47177, %v9 (stack65)
        %v47183 = vshll.u32 %v47174, 6 (stack73)
        %v47184 = vshrl.u32 %v47174, 26 (stack74)
        %v47185 = vor.u32 %v47183, %v47184 (stack75)
        %v47186 = vxor.u32 %v47177, %v47185 (stack76)
        %v47189 = vadd.s32 %v47186, %v8 (stack65)
        %v47193 = vadd.s32 %v47189, 1 (stack65)
        %v47197 = vadd.s32 %v47181, %v47193 (stack65)
        %v47199 = vshll.u32 %v47193, 17 (stack73)
        %v47200 = vshrl.u32 %v47193, 15 (stack74)
        %v47201 = vor.u32 %v47199, %v47200 (stack75)
        %v47202 = vxor.u32 %v47197, %v47201 (stack76)
        %v47205 = vadd.s32 %v47197, %v47202 (stack65)
        %v47207 = vshll.u32 %v47202, 29 (stack73)
        %v47208 = vshrl.u32 %v47202, 3 (stack74)
        %v47209 = vor.u32 %v47207, %v47208 (stack75)
        %v47210 = vxor.u32 %v47205, %v47209 (stack76)
        %v47213 = vadd.s32 %v47205, %v47210 (stack65)
        %v47215 = vshll.u32 %v47210, 16 (stack73)
        %v47216 = vshrl.u32 %v47210, 16 (stack74)
        %v47217 = vor.u32 %v47215, %v47216 (stack75)
        %v47218 = vxor.u32 %v47213, %v47217 (stack76)
        %v47221 = vadd.s32 %v47213, %v47218 (stack65)
        %v47225 = vadd.s32 %v47221, %v8 (stack65)
        %v47227 = vshll.u32 %v47218, 24 (stack73)
        %v47228 = vshrl.u32 %v47218, 8 (stack74)
        %v47229 = vor.u32 %v47227, %v47228 (stack75)
        %v47230 = vxor.u32 %v47221, %v47229 (stack76)
        %v47233 = vadd.s32 %v47230, %v10 (stack65)
        %v47237 = vadd.s32 %v47233, 2 (stack65)
        %v47241 = vadd.s32 %v47225, %v47237 (stack65)
        %v47243 = vshll.u32 %v47237, 13 (stack73)
        %v47244 = vshrl.u32 %v47237, 19 (stack74)
        %v47245 = vor.u32 %v47243, %v47244 (stack75)
        %v47246 = vxor.u32 %v47241, %v47245 (stack76)
        %v47249 = vadd.s32 %v47241, %v47246 (stack65)
        %v47251 = vshll.u32 %v47246, 15 (stack73)
        %v47252 = vshrl.u32 %v47246, 17 (stack74)
        %v47253 = vor.u32 %v47251, %v47252 (stack75)
        %v47254 = vxor.u32 %v47249, %v47253 (stack76)
        %v47257 = vadd.s32 %v47249, %v47254 (stack65)
        %v47259 = vshll.u32 %v47254, 26 (stack73)
        %v47260 = vshrl.u32 %v47254, 6 (stack74)
        %v47261 = vor.u32 %v47259, %v47260 (stack75)
        %v47262 = vxor.u32 %v47257, %v47261 (stack76)
        %v47265 = vadd.s32 %v47257, %v47262 (stack65)
        %v47269 = vadd.s32 %v47265, %v10 (stack65)
        %v47271 = vshll.u32 %v47262, 6 (stack73)
        %v47272 = vshrl.u32 %v47262, 26 (stack74)
        %v47273 = vor.u32 %v47271, %v47272 (stack75)
        %v47274 = vxor.u32 %v47265, %v47273 (stack76)
        %v47277 = vadd.s32 %v47274, %v9 (stack65)
        %v47281 = vadd.s32 %v47277, 3 (stack65)
        %v47285 = vadd.s32 %v47269, %v47281 (stack65)
        %v47287 = vshll.u32 %v47281, 17 (stack73)
        %v47288 = vshrl.u32 %v47281, 15 (stack74)
        %v47289 = vor.u32 %v47287, %v47288 (stack75)
        %v47290 = vxor.u32 %v47285, %v47289 (stack76)
        %v47293 = vadd.s32 %v47285, %v47290 (stack65)
        %v47295 = vshll.u32 %v47290, 29 (stack73)
        %v47296 = vshrl.u32 %v47290, 3 (stack74)
        %v47297 = vor.u32 %v47295, %v47296 (stack75)
        %v47298 = vxor.u32 %v47293, %v47297 (stack76)
        %v47301 = vadd.s32 %v47293, %v47298 (stack65)
        %v47303 = vshll.u32 %v47298, 16 (stack73)
        %v47304 = vshrl.u32 %v47298, 16 (stack74)
        %v47305 = vor.u32 %v47303, %v47304 (stack75)
        %v47306 = vxor.u32 %v47301, %v47305 (stack76)
        %v47309 = vadd.s32 %v47301, %v47306 (stack65)
        %v47313 = vadd.s32 %v47309, %v9 (stack65)
        %v47315 = vshll.u32 %v47306, 24 (stack73)
        %v47316 = vshrl.u32 %v47306, 8 (stack74)
        %v47317 = vor.u32 %v47315, %v47316 (stack75)
        %v47318 = vxor.u32 %v47309, %v47317 (stack76)
        %v47321 = vadd.s32 %v47318, %v8 (stack65)
        %v47325 = vadd.s32 %v47321, 4 (stack65)
        %v47329 = vadd.s32 %v47313, %v47325 (stack65)
        %v47331 = vshll.u32 %v47325, 13 (stack73)
        %v47332 = vshrl.u32 %v47325, 19 (stack74)
        %v47333 = vor.u32 %v47331, %v47332 (stack75)
        %v47334 = vxor.u32 %v47329, %v47333 (stack76)
        %v47337 = vadd.s32 %v47329, %v47334 (stack65)
        %v47339 = vshll.u32 %v47334, 15 (stack73)
        %v47340 = vshrl.u32 %v47334, 17 (stack74)
        %v47341 = vor.u32 %v47339, %v47340 (stack75)
        %v47342 = vxor.u32 %v47337, %v47341 (stack76)
        %v47345 = vadd.s32 %v47337, %v47342 (stack65)
        %v47347 = vshll.u32 %v47342, 26 (stack73)
        %v47348 = vshrl.u32 %v47342, 6 (stack74)
        %v47349 = vor.u32 %v47347, %v47348 (stack75)
        %v47350 = vxor.u32 %v47345, %v47349 (stack76)
        %v47353 = vadd.s32 %v47345, %v47350 (stack65)
        %v47357 = vadd.s32 %v47353, %v8 (stack65)
        %v47359 = vshll.u32 %v47350, 6 (stack73)
        %v47360 = vshrl.u32 %v47350, 26 (stack74)
        %v47361 = vor.u32 %v47359, %v47360 (stack75)
        %v47362 = vxor.u32 %v47353, %v47361 (stack76)
        %v47365 = vadd.s32 %v47362, %v10 (stack65)
        %v47369 = vadd.s32 %v47365, 5 (stack65)
        %v47371 = vxor.u32 %v47357, %v47369 (stack76)
        %v47372 = vand.u32.u8 %v47371, 255 (stack77)
        %v47373 = vand.u32 %v47372, 65535 (stack78)
        %v47374 = vshrl.u32 %v47373, 1 (stack79)
        %v47375 = vor.u32 %v47374, 16256 (stack75)
        %v47376 = vand.u32.u16 %v47375, 65535 (stack80)
        %v47377 = vunpack.i.l.bf16 %v47376 (stack81)
        %v47381 = vadd.f32 %v47377, -1.0 (stack82)
        %v47385 = vmul.f32 %v47381, 2.0 (stack83)
        %v47389 = vadd.f32 %v47385, -0.99609375 (stack82)
        %v47393 = vmax.f32 -0.99609375, %v47389 (stack84)
        %v47395 = vand.u32 2147483647, %v47393 (stack85)
        %vm47398 = vcmp.eq.f32.partialorder %v47395, 1.0 (stack86)
        %v47403 = vmul.f32 %v47393, inf (stack83)
        %v47405 = vxor.u32 %v47393, 2147483648 (stack87)
        %v47408 = vmul.f32 %v47393, %v47405 (stack83)
        %v47410 = vadd.f32 %v47408, 1.0 (stack88)
        %v47411 = vlog2.pop %v47410 (stack89)
        %v47412 = vmul.f32 %v47411, 0.6931472 (stack90)
        %v47413 = vmul.f32 -0.5, %v47408 (stack91)
        %v47414 = vadd.f32 %v47413, 1.0 (stack92)
        %v47415 = vmul.f32 %v47414, %v47408 (stack93)
        %v47416 = vand.u32 2147483647, %v47408 (stack94)
        %vm47417 = vcmp.lt.f32.partialorder %v47416, 0.0004427343 (stack95)
        %v47418 = vsel /*vm=*/%vm47417, /*on_true_vy=*/%v47415, /*on_false_vx=*/%v47412 (stack96)
        %v47419 = vxor.u32 %v47418, 2147483648 (stack87)
        %vm47422 = vcmp.lt.f32.partialorder %v47419, 5.0 (stack86)
        %v47427 = vsel /*vm=*/%vm47422, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v47431 = vsel /*vm=*/%vm47422, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v47435 = vsel /*vm=*/%vm47422, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v47439 = vsel /*vm=*/%vm47422, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v47443 = vsel /*vm=*/%vm47422, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v47447 = vsel /*vm=*/%vm47422, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v47451 = vsel /*vm=*/%vm47422, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v47455 = vsel /*vm=*/%vm47422, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v47459 = vsel /*vm=*/%vm47422, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v47463 = vadd.f32 %v47419, -2.5 (stack82)
        %v47465 = vrsqrt.pop %v47419 (stack97)
        %v47466 = vmul.f32 %v47419, %v47465 (stack98)
        %vm47467 = vcmp.eq.f32.partialorder %v47419, inf (stack99)
        %v47468 = vsel /*vm=*/%vm47467, /*on_true_vy=*/%v47419, /*on_false_vx=*/%v47466 (stack100)
        %vm47469 = vcmp.eq.f32.partialorder %v47419, 0.0 (stack101)
        %v47470 = vand.u32 %v47419, 2147483648 (stack102)
        %v47471 = vsel /*vm=*/%vm47469, /*on_true_vy=*/%v47470, /*on_false_vx=*/%v47468 (stack103)
        %v47474 = vadd.f32 %v47471, -3.0 (stack82)
        %v47478 = vsel /*vm=*/%vm47422, /*on_true_vy=*/%v47463, /*on_false_vx=*/%v47474 (stack72)
        %v47482 = vmul.f32 %v47459, %v47478 (stack83)
        %v47486 = vadd.f32 %v47455, %v47482 (stack82)
        %v47490 = vmul.f32 %v47486, %v47478 (stack83)
        %v47494 = vadd.f32 %v47451, %v47490 (stack82)
        %v47498 = vmul.f32 %v47494, %v47478 (stack83)
        %v47502 = vadd.f32 %v47447, %v47498 (stack82)
        %v47506 = vmul.f32 %v47502, %v47478 (stack83)
        %v47510 = vadd.f32 %v47443, %v47506 (stack82)
        %v47514 = vmul.f32 %v47510, %v47478 (stack83)
        %v47518 = vadd.f32 %v47439, %v47514 (stack82)
        %v47522 = vmul.f32 %v47518, %v47478 (stack83)
        %v47526 = vadd.f32 %v47435, %v47522 (stack82)
        %v47530 = vmul.f32 %v47526, %v47478 (stack83)
        %v47534 = vadd.f32 %v47431, %v47530 (stack82)
        %v47538 = vmul.f32 %v47534, %v47478 (stack83)
        %v47542 = vadd.f32 %v47427, %v47538 (stack82)
        %v47546 = vmul.f32 %v47542, %v47393 (stack83)
        %v47550 = vsel /*vm=*/%vm47398, /*on_true_vy=*/%v47403, /*on_false_vx=*/%v47546 (stack72)
        %v47554 = vmul.f32 %v47550, 1.4140625 (stack83)
        %s47556 = scalar_lea.vmem %s280, 560 [#allocation0] (stack107)
        %v47557 = vpack.c.bf16 0.0, %v47554 (stack104)
        %47558 = vst [vmem:[%s47556] sm:$0xf] /*vst_source=*/%v47557 (stack105)
        %v47561 = vadd.s32 %v2842, %v45253 (stack65)
        %s47563 = smul.u32 128, %s27 (stack66)
        %v47564 = vlaneseq (stack67)
        %v47565 = vand.u32 %v47564, 127 (stack68)
        %v47566 = vstv %s47563 (stack69)
        %v47567 = vadd.s32 %v47565, %v47566 (stack70)
        %v47571 = vadd.s32 %v47561, %v47567 (stack65)
        %vm47575 = vcmp.lt.u32.totalorder %v47571, %v47561 (stack71)
        %vm47580 = vcmp.lt.u32.totalorder %v47561, %v2842 (stack71)
        %v47585 = vadd.s32 %v2829, %v45236 (stack65)
        %v47589 = vadd.s32 %v47585, 1 (stack65)
        %v47593 = vsel /*vm=*/%vm47580, /*on_true_vy=*/%v47589, /*on_false_vx=*/%v47585 (stack72)
        %v47597 = vadd.s32 %v47593, 1 (stack65)
        %v47601 = vsel /*vm=*/%vm47575, /*on_true_vy=*/%v47597, /*on_false_vx=*/%v47593 (stack72)
        %v47606 = vadd.s32 %v47601, %v10 (stack65)
        %v47610 = vadd.s32 %v47571, %v9 (stack65)
        %v47614 = vadd.s32 %v47606, %v47610 (stack65)
        %v47616 = vshll.u32 %v47610, 13 (stack73)
        %v47617 = vshrl.u32 %v47610, 19 (stack74)
        %v47618 = vor.u32 %v47616, %v47617 (stack75)
        %v47619 = vxor.u32 %v47614, %v47618 (stack76)
        %v47622 = vadd.s32 %v47614, %v47619 (stack65)
        %v47624 = vshll.u32 %v47619, 15 (stack73)
        %v47625 = vshrl.u32 %v47619, 17 (stack74)
        %v47626 = vor.u32 %v47624, %v47625 (stack75)
        %v47627 = vxor.u32 %v47622, %v47626 (stack76)
        %v47630 = vadd.s32 %v47622, %v47627 (stack65)
        %v47632 = vshll.u32 %v47627, 26 (stack73)
        %v47633 = vshrl.u32 %v47627, 6 (stack74)
        %v47634 = vor.u32 %v47632, %v47633 (stack75)
        %v47635 = vxor.u32 %v47630, %v47634 (stack76)
        %v47638 = vadd.s32 %v47630, %v47635 (stack65)
        %v47642 = vadd.s32 %v47638, %v9 (stack65)
        %v47644 = vshll.u32 %v47635, 6 (stack73)
        %v47645 = vshrl.u32 %v47635, 26 (stack74)
        %v47646 = vor.u32 %v47644, %v47645 (stack75)
        %v47647 = vxor.u32 %v47638, %v47646 (stack76)
        %v47650 = vadd.s32 %v47647, %v8 (stack65)
        %v47654 = vadd.s32 %v47650, 1 (stack65)
        %v47658 = vadd.s32 %v47642, %v47654 (stack65)
        %v47660 = vshll.u32 %v47654, 17 (stack73)
        %v47661 = vshrl.u32 %v47654, 15 (stack74)
        %v47662 = vor.u32 %v47660, %v47661 (stack75)
        %v47663 = vxor.u32 %v47658, %v47662 (stack76)
        %v47666 = vadd.s32 %v47658, %v47663 (stack65)
        %v47668 = vshll.u32 %v47663, 29 (stack73)
        %v47669 = vshrl.u32 %v47663, 3 (stack74)
        %v47670 = vor.u32 %v47668, %v47669 (stack75)
        %v47671 = vxor.u32 %v47666, %v47670 (stack76)
        %v47674 = vadd.s32 %v47666, %v47671 (stack65)
        %v47676 = vshll.u32 %v47671, 16 (stack73)
        %v47677 = vshrl.u32 %v47671, 16 (stack74)
        %v47678 = vor.u32 %v47676, %v47677 (stack75)
        %v47679 = vxor.u32 %v47674, %v47678 (stack76)
        %v47682 = vadd.s32 %v47674, %v47679 (stack65)
        %v47686 = vadd.s32 %v47682, %v8 (stack65)
        %v47688 = vshll.u32 %v47679, 24 (stack73)
        %v47689 = vshrl.u32 %v47679, 8 (stack74)
        %v47690 = vor.u32 %v47688, %v47689 (stack75)
        %v47691 = vxor.u32 %v47682, %v47690 (stack76)
        %v47694 = vadd.s32 %v47691, %v10 (stack65)
        %v47698 = vadd.s32 %v47694, 2 (stack65)
        %v47702 = vadd.s32 %v47686, %v47698 (stack65)
        %v47704 = vshll.u32 %v47698, 13 (stack73)
        %v47705 = vshrl.u32 %v47698, 19 (stack74)
        %v47706 = vor.u32 %v47704, %v47705 (stack75)
        %v47707 = vxor.u32 %v47702, %v47706 (stack76)
        %v47710 = vadd.s32 %v47702, %v47707 (stack65)
        %v47712 = vshll.u32 %v47707, 15 (stack73)
        %v47713 = vshrl.u32 %v47707, 17 (stack74)
        %v47714 = vor.u32 %v47712, %v47713 (stack75)
        %v47715 = vxor.u32 %v47710, %v47714 (stack76)
        %v47718 = vadd.s32 %v47710, %v47715 (stack65)
        %v47720 = vshll.u32 %v47715, 26 (stack73)
        %v47721 = vshrl.u32 %v47715, 6 (stack74)
        %v47722 = vor.u32 %v47720, %v47721 (stack75)
        %v47723 = vxor.u32 %v47718, %v47722 (stack76)
        %v47726 = vadd.s32 %v47718, %v47723 (stack65)
        %v47730 = vadd.s32 %v47726, %v10 (stack65)
        %v47732 = vshll.u32 %v47723, 6 (stack73)
        %v47733 = vshrl.u32 %v47723, 26 (stack74)
        %v47734 = vor.u32 %v47732, %v47733 (stack75)
        %v47735 = vxor.u32 %v47726, %v47734 (stack76)
        %v47738 = vadd.s32 %v47735, %v9 (stack65)
        %v47742 = vadd.s32 %v47738, 3 (stack65)
        %v47746 = vadd.s32 %v47730, %v47742 (stack65)
        %v47748 = vshll.u32 %v47742, 17 (stack73)
        %v47749 = vshrl.u32 %v47742, 15 (stack74)
        %v47750 = vor.u32 %v47748, %v47749 (stack75)
        %v47751 = vxor.u32 %v47746, %v47750 (stack76)
        %v47754 = vadd.s32 %v47746, %v47751 (stack65)
        %v47756 = vshll.u32 %v47751, 29 (stack73)
        %v47757 = vshrl.u32 %v47751, 3 (stack74)
        %v47758 = vor.u32 %v47756, %v47757 (stack75)
        %v47759 = vxor.u32 %v47754, %v47758 (stack76)
        %v47762 = vadd.s32 %v47754, %v47759 (stack65)
        %v47764 = vshll.u32 %v47759, 16 (stack73)
        %v47765 = vshrl.u32 %v47759, 16 (stack74)
        %v47766 = vor.u32 %v47764, %v47765 (stack75)
        %v47767 = vxor.u32 %v47762, %v47766 (stack76)
        %v47770 = vadd.s32 %v47762, %v47767 (stack65)
        %v47774 = vadd.s32 %v47770, %v9 (stack65)
        %v47776 = vshll.u32 %v47767, 24 (stack73)
        %v47777 = vshrl.u32 %v47767, 8 (stack74)
        %v47778 = vor.u32 %v47776, %v47777 (stack75)
        %v47779 = vxor.u32 %v47770, %v47778 (stack76)
        %v47782 = vadd.s32 %v47779, %v8 (stack65)
        %v47786 = vadd.s32 %v47782, 4 (stack65)
        %v47790 = vadd.s32 %v47774, %v47786 (stack65)
        %v47792 = vshll.u32 %v47786, 13 (stack73)
        %v47793 = vshrl.u32 %v47786, 19 (stack74)
        %v47794 = vor.u32 %v47792, %v47793 (stack75)
        %v47795 = vxor.u32 %v47790, %v47794 (stack76)
        %v47798 = vadd.s32 %v47790, %v47795 (stack65)
        %v47800 = vshll.u32 %v47795, 15 (stack73)
        %v47801 = vshrl.u32 %v47795, 17 (stack74)
        %v47802 = vor.u32 %v47800, %v47801 (stack75)
        %v47803 = vxor.u32 %v47798, %v47802 (stack76)
        %v47806 = vadd.s32 %v47798, %v47803 (stack65)
        %v47808 = vshll.u32 %v47803, 26 (stack73)
        %v47809 = vshrl.u32 %v47803, 6 (stack74)
        %v47810 = vor.u32 %v47808, %v47809 (stack75)
        %v47811 = vxor.u32 %v47806, %v47810 (stack76)
        %v47814 = vadd.s32 %v47806, %v47811 (stack65)
        %v47818 = vadd.s32 %v47814, %v8 (stack65)
        %v47820 = vshll.u32 %v47811, 6 (stack73)
        %v47821 = vshrl.u32 %v47811, 26 (stack74)
        %v47822 = vor.u32 %v47820, %v47821 (stack75)
        %v47823 = vxor.u32 %v47814, %v47822 (stack76)
        %v47826 = vadd.s32 %v47823, %v10 (stack65)
        %v47830 = vadd.s32 %v47826, 5 (stack65)
        %v47832 = vxor.u32 %v47818, %v47830 (stack76)
        %v47833 = vand.u32.u8 %v47832, 255 (stack77)
        %v47834 = vand.u32 %v47833, 65535 (stack78)
        %v47835 = vshrl.u32 %v47834, 1 (stack79)
        %v47836 = vor.u32 %v47835, 16256 (stack75)
        %v47837 = vand.u32.u16 %v47836, 65535 (stack80)
        %v47838 = vunpack.i.l.bf16 %v47837 (stack81)
        %v47842 = vadd.f32 %v47838, -1.0 (stack82)
        %v47846 = vmul.f32 %v47842, 2.0 (stack83)
        %v47850 = vadd.f32 %v47846, -0.99609375 (stack82)
        %v47854 = vmax.f32 -0.99609375, %v47850 (stack84)
        %v47856 = vand.u32 2147483647, %v47854 (stack85)
        %vm47859 = vcmp.eq.f32.partialorder %v47856, 1.0 (stack86)
        %v47864 = vmul.f32 %v47854, inf (stack83)
        %v47866 = vxor.u32 %v47854, 2147483648 (stack87)
        %v47869 = vmul.f32 %v47854, %v47866 (stack83)
        %v47871 = vadd.f32 %v47869, 1.0 (stack88)
        %v47872 = vlog2.pop %v47871 (stack89)
        %v47873 = vmul.f32 %v47872, 0.6931472 (stack90)
        %v47874 = vmul.f32 -0.5, %v47869 (stack91)
        %v47875 = vadd.f32 %v47874, 1.0 (stack92)
        %v47876 = vmul.f32 %v47875, %v47869 (stack93)
        %v47877 = vand.u32 2147483647, %v47869 (stack94)
        %vm47878 = vcmp.lt.f32.partialorder %v47877, 0.0004427343 (stack95)
        %v47879 = vsel /*vm=*/%vm47878, /*on_true_vy=*/%v47876, /*on_false_vx=*/%v47873 (stack96)
        %v47880 = vxor.u32 %v47879, 2147483648 (stack87)
        %vm47883 = vcmp.lt.f32.partialorder %v47880, 5.0 (stack86)
        %v47888 = vsel /*vm=*/%vm47883, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v47892 = vsel /*vm=*/%vm47883, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v47896 = vsel /*vm=*/%vm47883, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v47900 = vsel /*vm=*/%vm47883, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v47904 = vsel /*vm=*/%vm47883, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v47908 = vsel /*vm=*/%vm47883, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v47912 = vsel /*vm=*/%vm47883, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v47916 = vsel /*vm=*/%vm47883, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v47920 = vsel /*vm=*/%vm47883, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v47924 = vadd.f32 %v47880, -2.5 (stack82)
        %v47926 = vrsqrt.pop %v47880 (stack97)
        %v47927 = vmul.f32 %v47880, %v47926 (stack98)
        %vm47928 = vcmp.eq.f32.partialorder %v47880, inf (stack99)
        %v47929 = vsel /*vm=*/%vm47928, /*on_true_vy=*/%v47880, /*on_false_vx=*/%v47927 (stack100)
        %vm47930 = vcmp.eq.f32.partialorder %v47880, 0.0 (stack101)
        %v47931 = vand.u32 %v47880, 2147483648 (stack102)
        %v47932 = vsel /*vm=*/%vm47930, /*on_true_vy=*/%v47931, /*on_false_vx=*/%v47929 (stack103)
        %v47935 = vadd.f32 %v47932, -3.0 (stack82)
        %v47939 = vsel /*vm=*/%vm47883, /*on_true_vy=*/%v47924, /*on_false_vx=*/%v47935 (stack72)
        %v47943 = vmul.f32 %v47920, %v47939 (stack83)
        %v47947 = vadd.f32 %v47916, %v47943 (stack82)
        %v47951 = vmul.f32 %v47947, %v47939 (stack83)
        %v47955 = vadd.f32 %v47912, %v47951 (stack82)
        %v47959 = vmul.f32 %v47955, %v47939 (stack83)
        %v47963 = vadd.f32 %v47908, %v47959 (stack82)
        %v47967 = vmul.f32 %v47963, %v47939 (stack83)
        %v47971 = vadd.f32 %v47904, %v47967 (stack82)
        %v47975 = vmul.f32 %v47971, %v47939 (stack83)
        %v47979 = vadd.f32 %v47900, %v47975 (stack82)
        %v47983 = vmul.f32 %v47979, %v47939 (stack83)
        %v47987 = vadd.f32 %v47896, %v47983 (stack82)
        %v47991 = vmul.f32 %v47987, %v47939 (stack83)
        %v47995 = vadd.f32 %v47892, %v47991 (stack82)
        %v47999 = vmul.f32 %v47995, %v47939 (stack83)
        %v48003 = vadd.f32 %v47888, %v47999 (stack82)
        %v48007 = vmul.f32 %v48003, %v47854 (stack83)
        %v48011 = vsel /*vm=*/%vm47859, /*on_true_vy=*/%v47864, /*on_false_vx=*/%v48007 (stack72)
        %v48015 = vmul.f32 %v48011, 1.4140625 (stack83)
        %s48017 = scalar_lea.vmem %s280, 688 [#allocation0] (stack107)
        %v48018 = vpack.c.bf16 0.0, %v48015 (stack104)
        %48019 = vst [vmem:[%s48017] sm:$0xf] /*vst_source=*/%v48018 (stack105)
        %v48022 = vadd.s32 %v3329, %v45253 (stack65)
        %s48024 = smul.u32 128, %s27 (stack66)
        %v48025 = vlaneseq (stack67)
        %v48026 = vand.u32 %v48025, 127 (stack68)
        %v48027 = vstv %s48024 (stack69)
        %v48028 = vadd.s32 %v48026, %v48027 (stack70)
        %v48032 = vadd.s32 %v48022, %v48028 (stack65)
        %vm48036 = vcmp.lt.u32.totalorder %v48032, %v48022 (stack71)
        %vm48041 = vcmp.lt.u32.totalorder %v48022, %v3329 (stack71)
        %v48046 = vadd.s32 %v3316, %v45236 (stack65)
        %v48050 = vadd.s32 %v48046, 1 (stack65)
        %v48054 = vsel /*vm=*/%vm48041, /*on_true_vy=*/%v48050, /*on_false_vx=*/%v48046 (stack72)
        %v48058 = vadd.s32 %v48054, 1 (stack65)
        %v48062 = vsel /*vm=*/%vm48036, /*on_true_vy=*/%v48058, /*on_false_vx=*/%v48054 (stack72)
        %v48067 = vadd.s32 %v48062, %v10 (stack65)
        %v48071 = vadd.s32 %v48032, %v9 (stack65)
        %v48075 = vadd.s32 %v48067, %v48071 (stack65)
        %v48077 = vshll.u32 %v48071, 13 (stack73)
        %v48078 = vshrl.u32 %v48071, 19 (stack74)
        %v48079 = vor.u32 %v48077, %v48078 (stack75)
        %v48080 = vxor.u32 %v48075, %v48079 (stack76)
        %v48083 = vadd.s32 %v48075, %v48080 (stack65)
        %v48085 = vshll.u32 %v48080, 15 (stack73)
        %v48086 = vshrl.u32 %v48080, 17 (stack74)
        %v48087 = vor.u32 %v48085, %v48086 (stack75)
        %v48088 = vxor.u32 %v48083, %v48087 (stack76)
        %v48091 = vadd.s32 %v48083, %v48088 (stack65)
        %v48093 = vshll.u32 %v48088, 26 (stack73)
        %v48094 = vshrl.u32 %v48088, 6 (stack74)
        %v48095 = vor.u32 %v48093, %v48094 (stack75)
        %v48096 = vxor.u32 %v48091, %v48095 (stack76)
        %v48099 = vadd.s32 %v48091, %v48096 (stack65)
        %v48103 = vadd.s32 %v48099, %v9 (stack65)
        %v48105 = vshll.u32 %v48096, 6 (stack73)
        %v48106 = vshrl.u32 %v48096, 26 (stack74)
        %v48107 = vor.u32 %v48105, %v48106 (stack75)
        %v48108 = vxor.u32 %v48099, %v48107 (stack76)
        %v48111 = vadd.s32 %v48108, %v8 (stack65)
        %v48115 = vadd.s32 %v48111, 1 (stack65)
        %v48119 = vadd.s32 %v48103, %v48115 (stack65)
        %v48121 = vshll.u32 %v48115, 17 (stack73)
        %v48122 = vshrl.u32 %v48115, 15 (stack74)
        %v48123 = vor.u32 %v48121, %v48122 (stack75)
        %v48124 = vxor.u32 %v48119, %v48123 (stack76)
        %v48127 = vadd.s32 %v48119, %v48124 (stack65)
        %v48129 = vshll.u32 %v48124, 29 (stack73)
        %v48130 = vshrl.u32 %v48124, 3 (stack74)
        %v48131 = vor.u32 %v48129, %v48130 (stack75)
        %v48132 = vxor.u32 %v48127, %v48131 (stack76)
        %v48135 = vadd.s32 %v48127, %v48132 (stack65)
        %v48137 = vshll.u32 %v48132, 16 (stack73)
        %v48138 = vshrl.u32 %v48132, 16 (stack74)
        %v48139 = vor.u32 %v48137, %v48138 (stack75)
        %v48140 = vxor.u32 %v48135, %v48139 (stack76)
        %v48143 = vadd.s32 %v48135, %v48140 (stack65)
        %v48147 = vadd.s32 %v48143, %v8 (stack65)
        %v48149 = vshll.u32 %v48140, 24 (stack73)
        %v48150 = vshrl.u32 %v48140, 8 (stack74)
        %v48151 = vor.u32 %v48149, %v48150 (stack75)
        %v48152 = vxor.u32 %v48143, %v48151 (stack76)
        %v48155 = vadd.s32 %v48152, %v10 (stack65)
        %v48159 = vadd.s32 %v48155, 2 (stack65)
        %v48163 = vadd.s32 %v48147, %v48159 (stack65)
        %v48165 = vshll.u32 %v48159, 13 (stack73)
        %v48166 = vshrl.u32 %v48159, 19 (stack74)
        %v48167 = vor.u32 %v48165, %v48166 (stack75)
        %v48168 = vxor.u32 %v48163, %v48167 (stack76)
        %v48171 = vadd.s32 %v48163, %v48168 (stack65)
        %v48173 = vshll.u32 %v48168, 15 (stack73)
        %v48174 = vshrl.u32 %v48168, 17 (stack74)
        %v48175 = vor.u32 %v48173, %v48174 (stack75)
        %v48176 = vxor.u32 %v48171, %v48175 (stack76)
        %v48179 = vadd.s32 %v48171, %v48176 (stack65)
        %v48181 = vshll.u32 %v48176, 26 (stack73)
        %v48182 = vshrl.u32 %v48176, 6 (stack74)
        %v48183 = vor.u32 %v48181, %v48182 (stack75)
        %v48184 = vxor.u32 %v48179, %v48183 (stack76)
        %v48187 = vadd.s32 %v48179, %v48184 (stack65)
        %v48191 = vadd.s32 %v48187, %v10 (stack65)
        %v48193 = vshll.u32 %v48184, 6 (stack73)
        %v48194 = vshrl.u32 %v48184, 26 (stack74)
        %v48195 = vor.u32 %v48193, %v48194 (stack75)
        %v48196 = vxor.u32 %v48187, %v48195 (stack76)
        %v48199 = vadd.s32 %v48196, %v9 (stack65)
        %v48203 = vadd.s32 %v48199, 3 (stack65)
        %v48207 = vadd.s32 %v48191, %v48203 (stack65)
        %v48209 = vshll.u32 %v48203, 17 (stack73)
        %v48210 = vshrl.u32 %v48203, 15 (stack74)
        %v48211 = vor.u32 %v48209, %v48210 (stack75)
        %v48212 = vxor.u32 %v48207, %v48211 (stack76)
        %v48215 = vadd.s32 %v48207, %v48212 (stack65)
        %v48217 = vshll.u32 %v48212, 29 (stack73)
        %v48218 = vshrl.u32 %v48212, 3 (stack74)
        %v48219 = vor.u32 %v48217, %v48218 (stack75)
        %v48220 = vxor.u32 %v48215, %v48219 (stack76)
        %v48223 = vadd.s32 %v48215, %v48220 (stack65)
        %v48225 = vshll.u32 %v48220, 16 (stack73)
        %v48226 = vshrl.u32 %v48220, 16 (stack74)
        %v48227 = vor.u32 %v48225, %v48226 (stack75)
        %v48228 = vxor.u32 %v48223, %v48227 (stack76)
        %v48231 = vadd.s32 %v48223, %v48228 (stack65)
        %v48235 = vadd.s32 %v48231, %v9 (stack65)
        %v48237 = vshll.u32 %v48228, 24 (stack73)
        %v48238 = vshrl.u32 %v48228, 8 (stack74)
        %v48239 = vor.u32 %v48237, %v48238 (stack75)
        %v48240 = vxor.u32 %v48231, %v48239 (stack76)
        %v48243 = vadd.s32 %v48240, %v8 (stack65)
        %v48247 = vadd.s32 %v48243, 4 (stack65)
        %v48251 = vadd.s32 %v48235, %v48247 (stack65)
        %v48253 = vshll.u32 %v48247, 13 (stack73)
        %v48254 = vshrl.u32 %v48247, 19 (stack74)
        %v48255 = vor.u32 %v48253, %v48254 (stack75)
        %v48256 = vxor.u32 %v48251, %v48255 (stack76)
        %v48259 = vadd.s32 %v48251, %v48256 (stack65)
        %v48261 = vshll.u32 %v48256, 15 (stack73)
        %v48262 = vshrl.u32 %v48256, 17 (stack74)
        %v48263 = vor.u32 %v48261, %v48262 (stack75)
        %v48264 = vxor.u32 %v48259, %v48263 (stack76)
        %v48267 = vadd.s32 %v48259, %v48264 (stack65)
        %v48269 = vshll.u32 %v48264, 26 (stack73)
        %v48270 = vshrl.u32 %v48264, 6 (stack74)
        %v48271 = vor.u32 %v48269, %v48270 (stack75)
        %v48272 = vxor.u32 %v48267, %v48271 (stack76)
        %v48275 = vadd.s32 %v48267, %v48272 (stack65)
        %v48279 = vadd.s32 %v48275, %v8 (stack65)
        %v48281 = vshll.u32 %v48272, 6 (stack73)
        %v48282 = vshrl.u32 %v48272, 26 (stack74)
        %v48283 = vor.u32 %v48281, %v48282 (stack75)
        %v48284 = vxor.u32 %v48275, %v48283 (stack76)
        %v48287 = vadd.s32 %v48284, %v10 (stack65)
        %v48291 = vadd.s32 %v48287, 5 (stack65)
        %v48293 = vxor.u32 %v48279, %v48291 (stack76)
        %v48294 = vand.u32.u8 %v48293, 255 (stack77)
        %v48295 = vand.u32 %v48294, 65535 (stack78)
        %v48296 = vshrl.u32 %v48295, 1 (stack79)
        %v48297 = vor.u32 %v48296, 16256 (stack75)
        %v48298 = vand.u32.u16 %v48297, 65535 (stack80)
        %v48299 = vunpack.i.l.bf16 %v48298 (stack81)
        %v48303 = vadd.f32 %v48299, -1.0 (stack82)
        %v48307 = vmul.f32 %v48303, 2.0 (stack83)
        %v48311 = vadd.f32 %v48307, -0.99609375 (stack82)
        %v48315 = vmax.f32 -0.99609375, %v48311 (stack84)
        %v48317 = vand.u32 2147483647, %v48315 (stack85)
        %vm48320 = vcmp.eq.f32.partialorder %v48317, 1.0 (stack86)
        %v48325 = vmul.f32 %v48315, inf (stack83)
        %v48327 = vxor.u32 %v48315, 2147483648 (stack87)
        %v48330 = vmul.f32 %v48315, %v48327 (stack83)
        %v48332 = vadd.f32 %v48330, 1.0 (stack88)
        %v48333 = vlog2.pop %v48332 (stack89)
        %v48334 = vmul.f32 %v48333, 0.6931472 (stack90)
        %v48335 = vmul.f32 -0.5, %v48330 (stack91)
        %v48336 = vadd.f32 %v48335, 1.0 (stack92)
        %v48337 = vmul.f32 %v48336, %v48330 (stack93)
        %v48338 = vand.u32 2147483647, %v48330 (stack94)
        %vm48339 = vcmp.lt.f32.partialorder %v48338, 0.0004427343 (stack95)
        %v48340 = vsel /*vm=*/%vm48339, /*on_true_vy=*/%v48337, /*on_false_vx=*/%v48334 (stack96)
        %v48341 = vxor.u32 %v48340, 2147483648 (stack87)
        %vm48344 = vcmp.lt.f32.partialorder %v48341, 5.0 (stack86)
        %v48349 = vsel /*vm=*/%vm48344, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v48353 = vsel /*vm=*/%vm48344, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v48357 = vsel /*vm=*/%vm48344, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v48361 = vsel /*vm=*/%vm48344, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v48365 = vsel /*vm=*/%vm48344, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v48369 = vsel /*vm=*/%vm48344, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v48373 = vsel /*vm=*/%vm48344, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v48377 = vsel /*vm=*/%vm48344, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v48381 = vsel /*vm=*/%vm48344, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v48385 = vadd.f32 %v48341, -2.5 (stack82)
        %v48387 = vrsqrt.pop %v48341 (stack97)
        %v48388 = vmul.f32 %v48341, %v48387 (stack98)
        %vm48389 = vcmp.eq.f32.partialorder %v48341, inf (stack99)
        %v48390 = vsel /*vm=*/%vm48389, /*on_true_vy=*/%v48341, /*on_false_vx=*/%v48388 (stack100)
        %vm48391 = vcmp.eq.f32.partialorder %v48341, 0.0 (stack101)
        %v48392 = vand.u32 %v48341, 2147483648 (stack102)
        %v48393 = vsel /*vm=*/%vm48391, /*on_true_vy=*/%v48392, /*on_false_vx=*/%v48390 (stack103)
        %v48396 = vadd.f32 %v48393, -3.0 (stack82)
        %v48400 = vsel /*vm=*/%vm48344, /*on_true_vy=*/%v48385, /*on_false_vx=*/%v48396 (stack72)
        %v48404 = vmul.f32 %v48381, %v48400 (stack83)
        %v48408 = vadd.f32 %v48377, %v48404 (stack82)
        %v48412 = vmul.f32 %v48408, %v48400 (stack83)
        %v48416 = vadd.f32 %v48373, %v48412 (stack82)
        %v48420 = vmul.f32 %v48416, %v48400 (stack83)
        %v48424 = vadd.f32 %v48369, %v48420 (stack82)
        %v48428 = vmul.f32 %v48424, %v48400 (stack83)
        %v48432 = vadd.f32 %v48365, %v48428 (stack82)
        %v48436 = vmul.f32 %v48432, %v48400 (stack83)
        %v48440 = vadd.f32 %v48361, %v48436 (stack82)
        %v48444 = vmul.f32 %v48440, %v48400 (stack83)
        %v48448 = vadd.f32 %v48357, %v48444 (stack82)
        %v48452 = vmul.f32 %v48448, %v48400 (stack83)
        %v48456 = vadd.f32 %v48353, %v48452 (stack82)
        %v48460 = vmul.f32 %v48456, %v48400 (stack83)
        %v48464 = vadd.f32 %v48349, %v48460 (stack82)
        %v48468 = vmul.f32 %v48464, %v48315 (stack83)
        %v48472 = vsel /*vm=*/%vm48320, /*on_true_vy=*/%v48325, /*on_false_vx=*/%v48468 (stack72)
        %v48476 = vmul.f32 %v48472, 1.4140625 (stack83)
        %s48478 = scalar_lea.vmem %s280, 816 [#allocation0] (stack107)
        %v48479 = vpack.c.bf16 0.0, %v48476 (stack104)
        %48480 = vst [vmem:[%s48478] sm:$0xf] /*vst_source=*/%v48479 (stack105)
        %v48483 = vadd.s32 %v3816, %v45253 (stack65)
        %s48485 = smul.u32 128, %s27 (stack66)
        %v48486 = vlaneseq (stack67)
        %v48487 = vand.u32 %v48486, 127 (stack68)
        %v48488 = vstv %s48485 (stack69)
        %v48489 = vadd.s32 %v48487, %v48488 (stack70)
        %v48493 = vadd.s32 %v48483, %v48489 (stack65)
        %vm48497 = vcmp.lt.u32.totalorder %v48493, %v48483 (stack71)
        %vm48502 = vcmp.lt.u32.totalorder %v48483, %v3816 (stack71)
        %v48507 = vadd.s32 %v3803, %v45236 (stack65)
        %v48511 = vadd.s32 %v48507, 1 (stack65)
        %v48515 = vsel /*vm=*/%vm48502, /*on_true_vy=*/%v48511, /*on_false_vx=*/%v48507 (stack72)
        %v48519 = vadd.s32 %v48515, 1 (stack65)
        %v48523 = vsel /*vm=*/%vm48497, /*on_true_vy=*/%v48519, /*on_false_vx=*/%v48515 (stack72)
        %v48528 = vadd.s32 %v48523, %v10 (stack65)
        %v48532 = vadd.s32 %v48493, %v9 (stack65)
        %v48536 = vadd.s32 %v48528, %v48532 (stack65)
        %v48538 = vshll.u32 %v48532, 13 (stack73)
        %v48539 = vshrl.u32 %v48532, 19 (stack74)
        %v48540 = vor.u32 %v48538, %v48539 (stack75)
        %v48541 = vxor.u32 %v48536, %v48540 (stack76)
        %v48544 = vadd.s32 %v48536, %v48541 (stack65)
        %v48546 = vshll.u32 %v48541, 15 (stack73)
        %v48547 = vshrl.u32 %v48541, 17 (stack74)
        %v48548 = vor.u32 %v48546, %v48547 (stack75)
        %v48549 = vxor.u32 %v48544, %v48548 (stack76)
        %v48552 = vadd.s32 %v48544, %v48549 (stack65)
        %v48554 = vshll.u32 %v48549, 26 (stack73)
        %v48555 = vshrl.u32 %v48549, 6 (stack74)
        %v48556 = vor.u32 %v48554, %v48555 (stack75)
        %v48557 = vxor.u32 %v48552, %v48556 (stack76)
        %v48560 = vadd.s32 %v48552, %v48557 (stack65)
        %v48564 = vadd.s32 %v48560, %v9 (stack65)
        %v48566 = vshll.u32 %v48557, 6 (stack73)
        %v48567 = vshrl.u32 %v48557, 26 (stack74)
        %v48568 = vor.u32 %v48566, %v48567 (stack75)
        %v48569 = vxor.u32 %v48560, %v48568 (stack76)
        %v48572 = vadd.s32 %v48569, %v8 (stack65)
        %v48576 = vadd.s32 %v48572, 1 (stack65)
        %v48580 = vadd.s32 %v48564, %v48576 (stack65)
        %v48582 = vshll.u32 %v48576, 17 (stack73)
        %v48583 = vshrl.u32 %v48576, 15 (stack74)
        %v48584 = vor.u32 %v48582, %v48583 (stack75)
        %v48585 = vxor.u32 %v48580, %v48584 (stack76)
        %v48588 = vadd.s32 %v48580, %v48585 (stack65)
        %v48590 = vshll.u32 %v48585, 29 (stack73)
        %v48591 = vshrl.u32 %v48585, 3 (stack74)
        %v48592 = vor.u32 %v48590, %v48591 (stack75)
        %v48593 = vxor.u32 %v48588, %v48592 (stack76)
        %v48596 = vadd.s32 %v48588, %v48593 (stack65)
        %v48598 = vshll.u32 %v48593, 16 (stack73)
        %v48599 = vshrl.u32 %v48593, 16 (stack74)
        %v48600 = vor.u32 %v48598, %v48599 (stack75)
        %v48601 = vxor.u32 %v48596, %v48600 (stack76)
        %v48604 = vadd.s32 %v48596, %v48601 (stack65)
        %v48608 = vadd.s32 %v48604, %v8 (stack65)
        %v48610 = vshll.u32 %v48601, 24 (stack73)
        %v48611 = vshrl.u32 %v48601, 8 (stack74)
        %v48612 = vor.u32 %v48610, %v48611 (stack75)
        %v48613 = vxor.u32 %v48604, %v48612 (stack76)
        %v48616 = vadd.s32 %v48613, %v10 (stack65)
        %v48620 = vadd.s32 %v48616, 2 (stack65)
        %v48624 = vadd.s32 %v48608, %v48620 (stack65)
        %v48626 = vshll.u32 %v48620, 13 (stack73)
        %v48627 = vshrl.u32 %v48620, 19 (stack74)
        %v48628 = vor.u32 %v48626, %v48627 (stack75)
        %v48629 = vxor.u32 %v48624, %v48628 (stack76)
        %v48632 = vadd.s32 %v48624, %v48629 (stack65)
        %v48634 = vshll.u32 %v48629, 15 (stack73)
        %v48635 = vshrl.u32 %v48629, 17 (stack74)
        %v48636 = vor.u32 %v48634, %v48635 (stack75)
        %v48637 = vxor.u32 %v48632, %v48636 (stack76)
        %v48640 = vadd.s32 %v48632, %v48637 (stack65)
        %v48642 = vshll.u32 %v48637, 26 (stack73)
        %v48643 = vshrl.u32 %v48637, 6 (stack74)
        %v48644 = vor.u32 %v48642, %v48643 (stack75)
        %v48645 = vxor.u32 %v48640, %v48644 (stack76)
        %v48648 = vadd.s32 %v48640, %v48645 (stack65)
        %v48652 = vadd.s32 %v48648, %v10 (stack65)
        %v48654 = vshll.u32 %v48645, 6 (stack73)
        %v48655 = vshrl.u32 %v48645, 26 (stack74)
        %v48656 = vor.u32 %v48654, %v48655 (stack75)
        %v48657 = vxor.u32 %v48648, %v48656 (stack76)
        %v48660 = vadd.s32 %v48657, %v9 (stack65)
        %v48664 = vadd.s32 %v48660, 3 (stack65)
        %v48668 = vadd.s32 %v48652, %v48664 (stack65)
        %v48670 = vshll.u32 %v48664, 17 (stack73)
        %v48671 = vshrl.u32 %v48664, 15 (stack74)
        %v48672 = vor.u32 %v48670, %v48671 (stack75)
        %v48673 = vxor.u32 %v48668, %v48672 (stack76)
        %v48676 = vadd.s32 %v48668, %v48673 (stack65)
        %v48678 = vshll.u32 %v48673, 29 (stack73)
        %v48679 = vshrl.u32 %v48673, 3 (stack74)
        %v48680 = vor.u32 %v48678, %v48679 (stack75)
        %v48681 = vxor.u32 %v48676, %v48680 (stack76)
        %v48684 = vadd.s32 %v48676, %v48681 (stack65)
        %v48686 = vshll.u32 %v48681, 16 (stack73)
        %v48687 = vshrl.u32 %v48681, 16 (stack74)
        %v48688 = vor.u32 %v48686, %v48687 (stack75)
        %v48689 = vxor.u32 %v48684, %v48688 (stack76)
        %v48692 = vadd.s32 %v48684, %v48689 (stack65)
        %v48696 = vadd.s32 %v48692, %v9 (stack65)
        %v48698 = vshll.u32 %v48689, 24 (stack73)
        %v48699 = vshrl.u32 %v48689, 8 (stack74)
        %v48700 = vor.u32 %v48698, %v48699 (stack75)
        %v48701 = vxor.u32 %v48692, %v48700 (stack76)
        %v48704 = vadd.s32 %v48701, %v8 (stack65)
        %v48708 = vadd.s32 %v48704, 4 (stack65)
        %v48712 = vadd.s32 %v48696, %v48708 (stack65)
        %v48714 = vshll.u32 %v48708, 13 (stack73)
        %v48715 = vshrl.u32 %v48708, 19 (stack74)
        %v48716 = vor.u32 %v48714, %v48715 (stack75)
        %v48717 = vxor.u32 %v48712, %v48716 (stack76)
        %v48720 = vadd.s32 %v48712, %v48717 (stack65)
        %v48722 = vshll.u32 %v48717, 15 (stack73)
        %v48723 = vshrl.u32 %v48717, 17 (stack74)
        %v48724 = vor.u32 %v48722, %v48723 (stack75)
        %v48725 = vxor.u32 %v48720, %v48724 (stack76)
        %v48728 = vadd.s32 %v48720, %v48725 (stack65)
        %v48730 = vshll.u32 %v48725, 26 (stack73)
        %v48731 = vshrl.u32 %v48725, 6 (stack74)
        %v48732 = vor.u32 %v48730, %v48731 (stack75)
        %v48733 = vxor.u32 %v48728, %v48732 (stack76)
        %v48736 = vadd.s32 %v48728, %v48733 (stack65)
        %v48740 = vadd.s32 %v48736, %v8 (stack65)
        %v48742 = vshll.u32 %v48733, 6 (stack73)
        %v48743 = vshrl.u32 %v48733, 26 (stack74)
        %v48744 = vor.u32 %v48742, %v48743 (stack75)
        %v48745 = vxor.u32 %v48736, %v48744 (stack76)
        %v48748 = vadd.s32 %v48745, %v10 (stack65)
        %v48752 = vadd.s32 %v48748, 5 (stack65)
        %v48754 = vxor.u32 %v48740, %v48752 (stack76)
        %v48755 = vand.u32.u8 %v48754, 255 (stack77)
        %v48756 = vand.u32 %v48755, 65535 (stack78)
        %v48757 = vshrl.u32 %v48756, 1 (stack79)
        %v48758 = vor.u32 %v48757, 16256 (stack75)
        %v48759 = vand.u32.u16 %v48758, 65535 (stack80)
        %v48760 = vunpack.i.l.bf16 %v48759 (stack81)
        %v48764 = vadd.f32 %v48760, -1.0 (stack82)
        %v48768 = vmul.f32 %v48764, 2.0 (stack83)
        %v48772 = vadd.f32 %v48768, -0.99609375 (stack82)
        %v48776 = vmax.f32 -0.99609375, %v48772 (stack84)
        %v48778 = vand.u32 2147483647, %v48776 (stack85)
        %vm48781 = vcmp.eq.f32.partialorder %v48778, 1.0 (stack86)
        %v48786 = vmul.f32 %v48776, inf (stack83)
        %v48788 = vxor.u32 %v48776, 2147483648 (stack87)
        %v48791 = vmul.f32 %v48776, %v48788 (stack83)
        %v48793 = vadd.f32 %v48791, 1.0 (stack88)
        %v48794 = vlog2.pop %v48793 (stack89)
        %v48795 = vmul.f32 %v48794, 0.6931472 (stack90)
        %v48796 = vmul.f32 -0.5, %v48791 (stack91)
        %v48797 = vadd.f32 %v48796, 1.0 (stack92)
        %v48798 = vmul.f32 %v48797, %v48791 (stack93)
        %v48799 = vand.u32 2147483647, %v48791 (stack94)
        %vm48800 = vcmp.lt.f32.partialorder %v48799, 0.0004427343 (stack95)
        %v48801 = vsel /*vm=*/%vm48800, /*on_true_vy=*/%v48798, /*on_false_vx=*/%v48795 (stack96)
        %v48802 = vxor.u32 %v48801, 2147483648 (stack87)
        %vm48805 = vcmp.lt.f32.partialorder %v48802, 5.0 (stack86)
        %v48810 = vsel /*vm=*/%vm48805, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v48814 = vsel /*vm=*/%vm48805, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v48818 = vsel /*vm=*/%vm48805, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v48822 = vsel /*vm=*/%vm48805, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v48826 = vsel /*vm=*/%vm48805, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v48830 = vsel /*vm=*/%vm48805, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v48834 = vsel /*vm=*/%vm48805, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v48838 = vsel /*vm=*/%vm48805, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v48842 = vsel /*vm=*/%vm48805, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v48846 = vadd.f32 %v48802, -2.5 (stack82)
        %v48848 = vrsqrt.pop %v48802 (stack97)
        %v48849 = vmul.f32 %v48802, %v48848 (stack98)
        %vm48850 = vcmp.eq.f32.partialorder %v48802, inf (stack99)
        %v48851 = vsel /*vm=*/%vm48850, /*on_true_vy=*/%v48802, /*on_false_vx=*/%v48849 (stack100)
        %vm48852 = vcmp.eq.f32.partialorder %v48802, 0.0 (stack101)
        %v48853 = vand.u32 %v48802, 2147483648 (stack102)
        %v48854 = vsel /*vm=*/%vm48852, /*on_true_vy=*/%v48853, /*on_false_vx=*/%v48851 (stack103)
        %v48857 = vadd.f32 %v48854, -3.0 (stack82)
        %v48861 = vsel /*vm=*/%vm48805, /*on_true_vy=*/%v48846, /*on_false_vx=*/%v48857 (stack72)
        %v48865 = vmul.f32 %v48842, %v48861 (stack83)
        %v48869 = vadd.f32 %v48838, %v48865 (stack82)
        %v48873 = vmul.f32 %v48869, %v48861 (stack83)
        %v48877 = vadd.f32 %v48834, %v48873 (stack82)
        %v48881 = vmul.f32 %v48877, %v48861 (stack83)
        %v48885 = vadd.f32 %v48830, %v48881 (stack82)
        %v48889 = vmul.f32 %v48885, %v48861 (stack83)
        %v48893 = vadd.f32 %v48826, %v48889 (stack82)
        %v48897 = vmul.f32 %v48893, %v48861 (stack83)
        %v48901 = vadd.f32 %v48822, %v48897 (stack82)
        %v48905 = vmul.f32 %v48901, %v48861 (stack83)
        %v48909 = vadd.f32 %v48818, %v48905 (stack82)
        %v48913 = vmul.f32 %v48909, %v48861 (stack83)
        %v48917 = vadd.f32 %v48814, %v48913 (stack82)
        %v48921 = vmul.f32 %v48917, %v48861 (stack83)
        %v48925 = vadd.f32 %v48810, %v48921 (stack82)
        %v48929 = vmul.f32 %v48925, %v48776 (stack83)
        %v48933 = vsel /*vm=*/%vm48781, /*on_true_vy=*/%v48786, /*on_false_vx=*/%v48929 (stack72)
        %v48937 = vmul.f32 %v48933, 1.4140625 (stack83)
        %s48939 = scalar_lea.vmem %s280, 944 [#allocation0] (stack107)
        %v48940 = vpack.c.bf16 0.0, %v48937 (stack104)
        %48941 = vst [vmem:[%s48939] sm:$0xf] /*vst_source=*/%v48940 (stack105)
        %s48942 = sadd.s32 %s339, 104 (stack106)
        %s48943 = sshrl.u32 %s48942, 10 (stack49)
        %p48944 = scmp.lt.s32.totalorder 1, %s48943 (stack50)
        %s48945 = scalar_select /*predicate=*/%p48944, /*on_true=*/1, /*on_false=*/%s48943 (stack51)
        %s48946 = sand.u32 %s48942, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s48947 = sshrl.u32 %s48946, 7 (stack53)
        %s48948 = sand.u32 %s48946, 127 /* smod.u32 w/div 128 */ (stack54)
        %s48949 = smul.addr %s48945, 8 (stack55)
        %s48950 = scalar_lea.vmem %s3, %s48949 (stack56)
        %s48952 = scalar_lea.vmem %s48950, %s48947 (stack57)
        %v48953 = vld [vmem:[%s48952] ss:$0 sm:$0xff] (stack58)
        %s48954 = sand.u32 %s48948, 255 (stack59)
        %s48956 = sor.u32 256, %s48954 (stack60)
        %48957 = vbcast.lane.b32.xlu0 %v48953, %s48956 (stack61)
        %v48958 = vpop.permute.xlu0 %48957 (stack62)
        %s48959 = sadd.s32 %s347, 104 (stack106)
        %s48960 = sshrl.u32 %s48959, 10 (stack49)
        %p48961 = scmp.lt.s32.totalorder 1, %s48960 (stack50)
        %s48962 = scalar_select /*predicate=*/%p48961, /*on_true=*/1, /*on_false=*/%s48960 (stack51)
        %s48963 = sand.u32 %s48959, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s48964 = sshrl.u32 %s48963, 7 (stack53)
        %s48965 = sand.u32 %s48963, 127 /* smod.u32 w/div 128 */ (stack54)
        %s48966 = smul.addr %s48962, 8 (stack55)
        %s48967 = scalar_lea.vmem %s5, %s48966 (stack56)
        %s48969 = scalar_lea.vmem %s48967, %s48964 (stack57)
        %v48970 = vld [vmem:[%s48969] ss:$0 sm:$0xff] (stack58)
        %s48971 = sand.u32 %s48965, 255 (stack59)
        %s48973 = sor.u32 256, %s48971 (stack60)
        %48974 = vbcast.lane.b32.xlu0 %v48970, %s48973 (stack61)
        %v48975 = vpop.permute.xlu0 %48974 (stack62)
        %v48978 = vadd.s32 %v408, %v48975 (stack65)
        %s48980 = smul.u32 128, %s27 (stack66)
        %v48981 = vlaneseq (stack67)
        %v48982 = vand.u32 %v48981, 127 (stack68)
        %v48983 = vstv %s48980 (stack69)
        %v48984 = vadd.s32 %v48982, %v48983 (stack70)
        %v48988 = vadd.s32 %v48978, %v48984 (stack65)
        %vm48992 = vcmp.lt.u32.totalorder %v48988, %v48978 (stack71)
        %vm48997 = vcmp.lt.u32.totalorder %v48978, %v408 (stack71)
        %v49002 = vadd.s32 %v380, %v48958 (stack65)
        %v49006 = vadd.s32 %v49002, 1 (stack65)
        %v49010 = vsel /*vm=*/%vm48997, /*on_true_vy=*/%v49006, /*on_false_vx=*/%v49002 (stack72)
        %v49014 = vadd.s32 %v49010, 1 (stack65)
        %v49018 = vsel /*vm=*/%vm48992, /*on_true_vy=*/%v49014, /*on_false_vx=*/%v49010 (stack72)
        %v49023 = vadd.s32 %v49018, %v10 (stack65)
        %v49027 = vadd.s32 %v48988, %v9 (stack65)
        %v49031 = vadd.s32 %v49023, %v49027 (stack65)
        %v49033 = vshll.u32 %v49027, 13 (stack73)
        %v49034 = vshrl.u32 %v49027, 19 (stack74)
        %v49035 = vor.u32 %v49033, %v49034 (stack75)
        %v49036 = vxor.u32 %v49031, %v49035 (stack76)
        %v49039 = vadd.s32 %v49031, %v49036 (stack65)
        %v49041 = vshll.u32 %v49036, 15 (stack73)
        %v49042 = vshrl.u32 %v49036, 17 (stack74)
        %v49043 = vor.u32 %v49041, %v49042 (stack75)
        %v49044 = vxor.u32 %v49039, %v49043 (stack76)
        %v49047 = vadd.s32 %v49039, %v49044 (stack65)
        %v49049 = vshll.u32 %v49044, 26 (stack73)
        %v49050 = vshrl.u32 %v49044, 6 (stack74)
        %v49051 = vor.u32 %v49049, %v49050 (stack75)
        %v49052 = vxor.u32 %v49047, %v49051 (stack76)
        %v49055 = vadd.s32 %v49047, %v49052 (stack65)
        %v49059 = vadd.s32 %v49055, %v9 (stack65)
        %v49061 = vshll.u32 %v49052, 6 (stack73)
        %v49062 = vshrl.u32 %v49052, 26 (stack74)
        %v49063 = vor.u32 %v49061, %v49062 (stack75)
        %v49064 = vxor.u32 %v49055, %v49063 (stack76)
        %v49067 = vadd.s32 %v49064, %v8 (stack65)
        %v49071 = vadd.s32 %v49067, 1 (stack65)
        %v49075 = vadd.s32 %v49059, %v49071 (stack65)
        %v49077 = vshll.u32 %v49071, 17 (stack73)
        %v49078 = vshrl.u32 %v49071, 15 (stack74)
        %v49079 = vor.u32 %v49077, %v49078 (stack75)
        %v49080 = vxor.u32 %v49075, %v49079 (stack76)
        %v49083 = vadd.s32 %v49075, %v49080 (stack65)
        %v49085 = vshll.u32 %v49080, 29 (stack73)
        %v49086 = vshrl.u32 %v49080, 3 (stack74)
        %v49087 = vor.u32 %v49085, %v49086 (stack75)
        %v49088 = vxor.u32 %v49083, %v49087 (stack76)
        %v49091 = vadd.s32 %v49083, %v49088 (stack65)
        %v49093 = vshll.u32 %v49088, 16 (stack73)
        %v49094 = vshrl.u32 %v49088, 16 (stack74)
        %v49095 = vor.u32 %v49093, %v49094 (stack75)
        %v49096 = vxor.u32 %v49091, %v49095 (stack76)
        %v49099 = vadd.s32 %v49091, %v49096 (stack65)
        %v49103 = vadd.s32 %v49099, %v8 (stack65)
        %v49105 = vshll.u32 %v49096, 24 (stack73)
        %v49106 = vshrl.u32 %v49096, 8 (stack74)
        %v49107 = vor.u32 %v49105, %v49106 (stack75)
        %v49108 = vxor.u32 %v49099, %v49107 (stack76)
        %v49111 = vadd.s32 %v49108, %v10 (stack65)
        %v49115 = vadd.s32 %v49111, 2 (stack65)
        %v49119 = vadd.s32 %v49103, %v49115 (stack65)
        %v49121 = vshll.u32 %v49115, 13 (stack73)
        %v49122 = vshrl.u32 %v49115, 19 (stack74)
        %v49123 = vor.u32 %v49121, %v49122 (stack75)
        %v49124 = vxor.u32 %v49119, %v49123 (stack76)
        %v49127 = vadd.s32 %v49119, %v49124 (stack65)
        %v49129 = vshll.u32 %v49124, 15 (stack73)
        %v49130 = vshrl.u32 %v49124, 17 (stack74)
        %v49131 = vor.u32 %v49129, %v49130 (stack75)
        %v49132 = vxor.u32 %v49127, %v49131 (stack76)
        %v49135 = vadd.s32 %v49127, %v49132 (stack65)
        %v49137 = vshll.u32 %v49132, 26 (stack73)
        %v49138 = vshrl.u32 %v49132, 6 (stack74)
        %v49139 = vor.u32 %v49137, %v49138 (stack75)
        %v49140 = vxor.u32 %v49135, %v49139 (stack76)
        %v49143 = vadd.s32 %v49135, %v49140 (stack65)
        %v49147 = vadd.s32 %v49143, %v10 (stack65)
        %v49149 = vshll.u32 %v49140, 6 (stack73)
        %v49150 = vshrl.u32 %v49140, 26 (stack74)
        %v49151 = vor.u32 %v49149, %v49150 (stack75)
        %v49152 = vxor.u32 %v49143, %v49151 (stack76)
        %v49155 = vadd.s32 %v49152, %v9 (stack65)
        %v49159 = vadd.s32 %v49155, 3 (stack65)
        %v49163 = vadd.s32 %v49147, %v49159 (stack65)
        %v49165 = vshll.u32 %v49159, 17 (stack73)
        %v49166 = vshrl.u32 %v49159, 15 (stack74)
        %v49167 = vor.u32 %v49165, %v49166 (stack75)
        %v49168 = vxor.u32 %v49163, %v49167 (stack76)
        %v49171 = vadd.s32 %v49163, %v49168 (stack65)
        %v49173 = vshll.u32 %v49168, 29 (stack73)
        %v49174 = vshrl.u32 %v49168, 3 (stack74)
        %v49175 = vor.u32 %v49173, %v49174 (stack75)
        %v49176 = vxor.u32 %v49171, %v49175 (stack76)
        %v49179 = vadd.s32 %v49171, %v49176 (stack65)
        %v49181 = vshll.u32 %v49176, 16 (stack73)
        %v49182 = vshrl.u32 %v49176, 16 (stack74)
        %v49183 = vor.u32 %v49181, %v49182 (stack75)
        %v49184 = vxor.u32 %v49179, %v49183 (stack76)
        %v49187 = vadd.s32 %v49179, %v49184 (stack65)
        %v49191 = vadd.s32 %v49187, %v9 (stack65)
        %v49193 = vshll.u32 %v49184, 24 (stack73)
        %v49194 = vshrl.u32 %v49184, 8 (stack74)
        %v49195 = vor.u32 %v49193, %v49194 (stack75)
        %v49196 = vxor.u32 %v49187, %v49195 (stack76)
        %v49199 = vadd.s32 %v49196, %v8 (stack65)
        %v49203 = vadd.s32 %v49199, 4 (stack65)
        %v49207 = vadd.s32 %v49191, %v49203 (stack65)
        %v49209 = vshll.u32 %v49203, 13 (stack73)
        %v49210 = vshrl.u32 %v49203, 19 (stack74)
        %v49211 = vor.u32 %v49209, %v49210 (stack75)
        %v49212 = vxor.u32 %v49207, %v49211 (stack76)
        %v49215 = vadd.s32 %v49207, %v49212 (stack65)
        %v49217 = vshll.u32 %v49212, 15 (stack73)
        %v49218 = vshrl.u32 %v49212, 17 (stack74)
        %v49219 = vor.u32 %v49217, %v49218 (stack75)
        %v49220 = vxor.u32 %v49215, %v49219 (stack76)
        %v49223 = vadd.s32 %v49215, %v49220 (stack65)
        %v49225 = vshll.u32 %v49220, 26 (stack73)
        %v49226 = vshrl.u32 %v49220, 6 (stack74)
        %v49227 = vor.u32 %v49225, %v49226 (stack75)
        %v49228 = vxor.u32 %v49223, %v49227 (stack76)
        %v49231 = vadd.s32 %v49223, %v49228 (stack65)
        %v49235 = vadd.s32 %v49231, %v8 (stack65)
        %v49237 = vshll.u32 %v49228, 6 (stack73)
        %v49238 = vshrl.u32 %v49228, 26 (stack74)
        %v49239 = vor.u32 %v49237, %v49238 (stack75)
        %v49240 = vxor.u32 %v49231, %v49239 (stack76)
        %v49243 = vadd.s32 %v49240, %v10 (stack65)
        %v49247 = vadd.s32 %v49243, 5 (stack65)
        %v49249 = vxor.u32 %v49235, %v49247 (stack76)
        %v49250 = vand.u32.u8 %v49249, 255 (stack77)
        %v49251 = vand.u32 %v49250, 65535 (stack78)
        %v49252 = vshrl.u32 %v49251, 1 (stack79)
        %v49253 = vor.u32 %v49252, 16256 (stack75)
        %v49254 = vand.u32.u16 %v49253, 65535 (stack80)
        %v49255 = vunpack.i.l.bf16 %v49254 (stack81)
        %v49259 = vadd.f32 %v49255, -1.0 (stack82)
        %v49263 = vmul.f32 %v49259, 2.0 (stack83)
        %v49267 = vadd.f32 %v49263, -0.99609375 (stack82)
        %v49271 = vmax.f32 -0.99609375, %v49267 (stack84)
        %v49273 = vand.u32 2147483647, %v49271 (stack85)
        %vm49276 = vcmp.eq.f32.partialorder %v49273, 1.0 (stack86)
        %v49281 = vmul.f32 %v49271, inf (stack83)
        %v49283 = vxor.u32 %v49271, 2147483648 (stack87)
        %v49286 = vmul.f32 %v49271, %v49283 (stack83)
        %v49288 = vadd.f32 %v49286, 1.0 (stack88)
        %v49289 = vlog2.pop %v49288 (stack89)
        %v49290 = vmul.f32 %v49289, 0.6931472 (stack90)
        %v49291 = vmul.f32 -0.5, %v49286 (stack91)
        %v49292 = vadd.f32 %v49291, 1.0 (stack92)
        %v49293 = vmul.f32 %v49292, %v49286 (stack93)
        %v49294 = vand.u32 2147483647, %v49286 (stack94)
        %vm49295 = vcmp.lt.f32.partialorder %v49294, 0.0004427343 (stack95)
        %v49296 = vsel /*vm=*/%vm49295, /*on_true_vy=*/%v49293, /*on_false_vx=*/%v49290 (stack96)
        %v49297 = vxor.u32 %v49296, 2147483648 (stack87)
        %vm49300 = vcmp.lt.f32.partialorder %v49297, 5.0 (stack86)
        %v49305 = vsel /*vm=*/%vm49300, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v49309 = vsel /*vm=*/%vm49300, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v49313 = vsel /*vm=*/%vm49300, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v49317 = vsel /*vm=*/%vm49300, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v49321 = vsel /*vm=*/%vm49300, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v49325 = vsel /*vm=*/%vm49300, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v49329 = vsel /*vm=*/%vm49300, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v49333 = vsel /*vm=*/%vm49300, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v49337 = vsel /*vm=*/%vm49300, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v49341 = vadd.f32 %v49297, -2.5 (stack82)
        %v49343 = vrsqrt.pop %v49297 (stack97)
        %v49344 = vmul.f32 %v49297, %v49343 (stack98)
        %vm49345 = vcmp.eq.f32.partialorder %v49297, inf (stack99)
        %v49346 = vsel /*vm=*/%vm49345, /*on_true_vy=*/%v49297, /*on_false_vx=*/%v49344 (stack100)
        %vm49347 = vcmp.eq.f32.partialorder %v49297, 0.0 (stack101)
        %v49348 = vand.u32 %v49297, 2147483648 (stack102)
        %v49349 = vsel /*vm=*/%vm49347, /*on_true_vy=*/%v49348, /*on_false_vx=*/%v49346 (stack103)
        %v49352 = vadd.f32 %v49349, -3.0 (stack82)
        %v49356 = vsel /*vm=*/%vm49300, /*on_true_vy=*/%v49341, /*on_false_vx=*/%v49352 (stack72)
        %v49360 = vmul.f32 %v49337, %v49356 (stack83)
        %v49364 = vadd.f32 %v49333, %v49360 (stack82)
        %v49368 = vmul.f32 %v49364, %v49356 (stack83)
        %v49372 = vadd.f32 %v49329, %v49368 (stack82)
        %v49376 = vmul.f32 %v49372, %v49356 (stack83)
        %v49380 = vadd.f32 %v49325, %v49376 (stack82)
        %v49384 = vmul.f32 %v49380, %v49356 (stack83)
        %v49388 = vadd.f32 %v49321, %v49384 (stack82)
        %v49392 = vmul.f32 %v49388, %v49356 (stack83)
        %v49396 = vadd.f32 %v49317, %v49392 (stack82)
        %v49400 = vmul.f32 %v49396, %v49356 (stack83)
        %v49404 = vadd.f32 %v49313, %v49400 (stack82)
        %v49408 = vmul.f32 %v49404, %v49356 (stack83)
        %v49412 = vadd.f32 %v49309, %v49408 (stack82)
        %v49416 = vmul.f32 %v49412, %v49356 (stack83)
        %v49420 = vadd.f32 %v49305, %v49416 (stack82)
        %v49424 = vmul.f32 %v49420, %v49271 (stack83)
        %v49428 = vsel /*vm=*/%vm49276, /*on_true_vy=*/%v49281, /*on_false_vx=*/%v49424 (stack72)
        %v49432 = vmul.f32 %v49428, 1.4140625 (stack83)
        %s49434 = scalar_lea.vmem %s280, 52 [#allocation0] (stack107)
        %v49435 = vpack.c.bf16 0.0, %v49432 (stack104)
        %49436 = vst [vmem:[%s49434] sm:$0xf] /*vst_source=*/%v49435 (stack105)
        %v49439 = vadd.s32 %v894, %v48975 (stack65)
        %s49441 = smul.u32 128, %s27 (stack66)
        %v49442 = vlaneseq (stack67)
        %v49443 = vand.u32 %v49442, 127 (stack68)
        %v49444 = vstv %s49441 (stack69)
        %v49445 = vadd.s32 %v49443, %v49444 (stack70)
        %v49449 = vadd.s32 %v49439, %v49445 (stack65)
        %vm49453 = vcmp.lt.u32.totalorder %v49449, %v49439 (stack71)
        %vm49458 = vcmp.lt.u32.totalorder %v49439, %v894 (stack71)
        %v49463 = vadd.s32 %v881, %v48958 (stack65)
        %v49467 = vadd.s32 %v49463, 1 (stack65)
        %v49471 = vsel /*vm=*/%vm49458, /*on_true_vy=*/%v49467, /*on_false_vx=*/%v49463 (stack72)
        %v49475 = vadd.s32 %v49471, 1 (stack65)
        %v49479 = vsel /*vm=*/%vm49453, /*on_true_vy=*/%v49475, /*on_false_vx=*/%v49471 (stack72)
        %v49484 = vadd.s32 %v49479, %v10 (stack65)
        %v49488 = vadd.s32 %v49449, %v9 (stack65)
        %v49492 = vadd.s32 %v49484, %v49488 (stack65)
        %v49494 = vshll.u32 %v49488, 13 (stack73)
        %v49495 = vshrl.u32 %v49488, 19 (stack74)
        %v49496 = vor.u32 %v49494, %v49495 (stack75)
        %v49497 = vxor.u32 %v49492, %v49496 (stack76)
        %v49500 = vadd.s32 %v49492, %v49497 (stack65)
        %v49502 = vshll.u32 %v49497, 15 (stack73)
        %v49503 = vshrl.u32 %v49497, 17 (stack74)
        %v49504 = vor.u32 %v49502, %v49503 (stack75)
        %v49505 = vxor.u32 %v49500, %v49504 (stack76)
        %v49508 = vadd.s32 %v49500, %v49505 (stack65)
        %v49510 = vshll.u32 %v49505, 26 (stack73)
        %v49511 = vshrl.u32 %v49505, 6 (stack74)
        %v49512 = vor.u32 %v49510, %v49511 (stack75)
        %v49513 = vxor.u32 %v49508, %v49512 (stack76)
        %v49516 = vadd.s32 %v49508, %v49513 (stack65)
        %v49520 = vadd.s32 %v49516, %v9 (stack65)
        %v49522 = vshll.u32 %v49513, 6 (stack73)
        %v49523 = vshrl.u32 %v49513, 26 (stack74)
        %v49524 = vor.u32 %v49522, %v49523 (stack75)
        %v49525 = vxor.u32 %v49516, %v49524 (stack76)
        %v49528 = vadd.s32 %v49525, %v8 (stack65)
        %v49532 = vadd.s32 %v49528, 1 (stack65)
        %v49536 = vadd.s32 %v49520, %v49532 (stack65)
        %v49538 = vshll.u32 %v49532, 17 (stack73)
        %v49539 = vshrl.u32 %v49532, 15 (stack74)
        %v49540 = vor.u32 %v49538, %v49539 (stack75)
        %v49541 = vxor.u32 %v49536, %v49540 (stack76)
        %v49544 = vadd.s32 %v49536, %v49541 (stack65)
        %v49546 = vshll.u32 %v49541, 29 (stack73)
        %v49547 = vshrl.u32 %v49541, 3 (stack74)
        %v49548 = vor.u32 %v49546, %v49547 (stack75)
        %v49549 = vxor.u32 %v49544, %v49548 (stack76)
        %v49552 = vadd.s32 %v49544, %v49549 (stack65)
        %v49554 = vshll.u32 %v49549, 16 (stack73)
        %v49555 = vshrl.u32 %v49549, 16 (stack74)
        %v49556 = vor.u32 %v49554, %v49555 (stack75)
        %v49557 = vxor.u32 %v49552, %v49556 (stack76)
        %v49560 = vadd.s32 %v49552, %v49557 (stack65)
        %v49564 = vadd.s32 %v49560, %v8 (stack65)
        %v49566 = vshll.u32 %v49557, 24 (stack73)
        %v49567 = vshrl.u32 %v49557, 8 (stack74)
        %v49568 = vor.u32 %v49566, %v49567 (stack75)
        %v49569 = vxor.u32 %v49560, %v49568 (stack76)
        %v49572 = vadd.s32 %v49569, %v10 (stack65)
        %v49576 = vadd.s32 %v49572, 2 (stack65)
        %v49580 = vadd.s32 %v49564, %v49576 (stack65)
        %v49582 = vshll.u32 %v49576, 13 (stack73)
        %v49583 = vshrl.u32 %v49576, 19 (stack74)
        %v49584 = vor.u32 %v49582, %v49583 (stack75)
        %v49585 = vxor.u32 %v49580, %v49584 (stack76)
        %v49588 = vadd.s32 %v49580, %v49585 (stack65)
        %v49590 = vshll.u32 %v49585, 15 (stack73)
        %v49591 = vshrl.u32 %v49585, 17 (stack74)
        %v49592 = vor.u32 %v49590, %v49591 (stack75)
        %v49593 = vxor.u32 %v49588, %v49592 (stack76)
        %v49596 = vadd.s32 %v49588, %v49593 (stack65)
        %v49598 = vshll.u32 %v49593, 26 (stack73)
        %v49599 = vshrl.u32 %v49593, 6 (stack74)
        %v49600 = vor.u32 %v49598, %v49599 (stack75)
        %v49601 = vxor.u32 %v49596, %v49600 (stack76)
        %v49604 = vadd.s32 %v49596, %v49601 (stack65)
        %v49608 = vadd.s32 %v49604, %v10 (stack65)
        %v49610 = vshll.u32 %v49601, 6 (stack73)
        %v49611 = vshrl.u32 %v49601, 26 (stack74)
        %v49612 = vor.u32 %v49610, %v49611 (stack75)
        %v49613 = vxor.u32 %v49604, %v49612 (stack76)
        %v49616 = vadd.s32 %v49613, %v9 (stack65)
        %v49620 = vadd.s32 %v49616, 3 (stack65)
        %v49624 = vadd.s32 %v49608, %v49620 (stack65)
        %v49626 = vshll.u32 %v49620, 17 (stack73)
        %v49627 = vshrl.u32 %v49620, 15 (stack74)
        %v49628 = vor.u32 %v49626, %v49627 (stack75)
        %v49629 = vxor.u32 %v49624, %v49628 (stack76)
        %v49632 = vadd.s32 %v49624, %v49629 (stack65)
        %v49634 = vshll.u32 %v49629, 29 (stack73)
        %v49635 = vshrl.u32 %v49629, 3 (stack74)
        %v49636 = vor.u32 %v49634, %v49635 (stack75)
        %v49637 = vxor.u32 %v49632, %v49636 (stack76)
        %v49640 = vadd.s32 %v49632, %v49637 (stack65)
        %v49642 = vshll.u32 %v49637, 16 (stack73)
        %v49643 = vshrl.u32 %v49637, 16 (stack74)
        %v49644 = vor.u32 %v49642, %v49643 (stack75)
        %v49645 = vxor.u32 %v49640, %v49644 (stack76)
        %v49648 = vadd.s32 %v49640, %v49645 (stack65)
        %v49652 = vadd.s32 %v49648, %v9 (stack65)
        %v49654 = vshll.u32 %v49645, 24 (stack73)
        %v49655 = vshrl.u32 %v49645, 8 (stack74)
        %v49656 = vor.u32 %v49654, %v49655 (stack75)
        %v49657 = vxor.u32 %v49648, %v49656 (stack76)
        %v49660 = vadd.s32 %v49657, %v8 (stack65)
        %v49664 = vadd.s32 %v49660, 4 (stack65)
        %v49668 = vadd.s32 %v49652, %v49664 (stack65)
        %v49670 = vshll.u32 %v49664, 13 (stack73)
        %v49671 = vshrl.u32 %v49664, 19 (stack74)
        %v49672 = vor.u32 %v49670, %v49671 (stack75)
        %v49673 = vxor.u32 %v49668, %v49672 (stack76)
        %v49676 = vadd.s32 %v49668, %v49673 (stack65)
        %v49678 = vshll.u32 %v49673, 15 (stack73)
        %v49679 = vshrl.u32 %v49673, 17 (stack74)
        %v49680 = vor.u32 %v49678, %v49679 (stack75)
        %v49681 = vxor.u32 %v49676, %v49680 (stack76)
        %v49684 = vadd.s32 %v49676, %v49681 (stack65)
        %v49686 = vshll.u32 %v49681, 26 (stack73)
        %v49687 = vshrl.u32 %v49681, 6 (stack74)
        %v49688 = vor.u32 %v49686, %v49687 (stack75)
        %v49689 = vxor.u32 %v49684, %v49688 (stack76)
        %v49692 = vadd.s32 %v49684, %v49689 (stack65)
        %v49696 = vadd.s32 %v49692, %v8 (stack65)
        %v49698 = vshll.u32 %v49689, 6 (stack73)
        %v49699 = vshrl.u32 %v49689, 26 (stack74)
        %v49700 = vor.u32 %v49698, %v49699 (stack75)
        %v49701 = vxor.u32 %v49692, %v49700 (stack76)
        %v49704 = vadd.s32 %v49701, %v10 (stack65)
        %v49708 = vadd.s32 %v49704, 5 (stack65)
        %v49710 = vxor.u32 %v49696, %v49708 (stack76)
        %v49711 = vand.u32.u8 %v49710, 255 (stack77)
        %v49712 = vand.u32 %v49711, 65535 (stack78)
        %v49713 = vshrl.u32 %v49712, 1 (stack79)
        %v49714 = vor.u32 %v49713, 16256 (stack75)
        %v49715 = vand.u32.u16 %v49714, 65535 (stack80)
        %v49716 = vunpack.i.l.bf16 %v49715 (stack81)
        %v49720 = vadd.f32 %v49716, -1.0 (stack82)
        %v49724 = vmul.f32 %v49720, 2.0 (stack83)
        %v49728 = vadd.f32 %v49724, -0.99609375 (stack82)
        %v49732 = vmax.f32 -0.99609375, %v49728 (stack84)
        %v49734 = vand.u32 2147483647, %v49732 (stack85)
        %vm49737 = vcmp.eq.f32.partialorder %v49734, 1.0 (stack86)
        %v49742 = vmul.f32 %v49732, inf (stack83)
        %v49744 = vxor.u32 %v49732, 2147483648 (stack87)
        %v49747 = vmul.f32 %v49732, %v49744 (stack83)
        %v49749 = vadd.f32 %v49747, 1.0 (stack88)
        %v49750 = vlog2.pop %v49749 (stack89)
        %v49751 = vmul.f32 %v49750, 0.6931472 (stack90)
        %v49752 = vmul.f32 -0.5, %v49747 (stack91)
        %v49753 = vadd.f32 %v49752, 1.0 (stack92)
        %v49754 = vmul.f32 %v49753, %v49747 (stack93)
        %v49755 = vand.u32 2147483647, %v49747 (stack94)
        %vm49756 = vcmp.lt.f32.partialorder %v49755, 0.0004427343 (stack95)
        %v49757 = vsel /*vm=*/%vm49756, /*on_true_vy=*/%v49754, /*on_false_vx=*/%v49751 (stack96)
        %v49758 = vxor.u32 %v49757, 2147483648 (stack87)
        %vm49761 = vcmp.lt.f32.partialorder %v49758, 5.0 (stack86)
        %v49766 = vsel /*vm=*/%vm49761, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v49770 = vsel /*vm=*/%vm49761, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v49774 = vsel /*vm=*/%vm49761, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v49778 = vsel /*vm=*/%vm49761, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v49782 = vsel /*vm=*/%vm49761, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v49786 = vsel /*vm=*/%vm49761, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v49790 = vsel /*vm=*/%vm49761, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v49794 = vsel /*vm=*/%vm49761, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v49798 = vsel /*vm=*/%vm49761, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v49802 = vadd.f32 %v49758, -2.5 (stack82)
        %v49804 = vrsqrt.pop %v49758 (stack97)
        %v49805 = vmul.f32 %v49758, %v49804 (stack98)
        %vm49806 = vcmp.eq.f32.partialorder %v49758, inf (stack99)
        %v49807 = vsel /*vm=*/%vm49806, /*on_true_vy=*/%v49758, /*on_false_vx=*/%v49805 (stack100)
        %vm49808 = vcmp.eq.f32.partialorder %v49758, 0.0 (stack101)
        %v49809 = vand.u32 %v49758, 2147483648 (stack102)
        %v49810 = vsel /*vm=*/%vm49808, /*on_true_vy=*/%v49809, /*on_false_vx=*/%v49807 (stack103)
        %v49813 = vadd.f32 %v49810, -3.0 (stack82)
        %v49817 = vsel /*vm=*/%vm49761, /*on_true_vy=*/%v49802, /*on_false_vx=*/%v49813 (stack72)
        %v49821 = vmul.f32 %v49798, %v49817 (stack83)
        %v49825 = vadd.f32 %v49794, %v49821 (stack82)
        %v49829 = vmul.f32 %v49825, %v49817 (stack83)
        %v49833 = vadd.f32 %v49790, %v49829 (stack82)
        %v49837 = vmul.f32 %v49833, %v49817 (stack83)
        %v49841 = vadd.f32 %v49786, %v49837 (stack82)
        %v49845 = vmul.f32 %v49841, %v49817 (stack83)
        %v49849 = vadd.f32 %v49782, %v49845 (stack82)
        %v49853 = vmul.f32 %v49849, %v49817 (stack83)
        %v49857 = vadd.f32 %v49778, %v49853 (stack82)
        %v49861 = vmul.f32 %v49857, %v49817 (stack83)
        %v49865 = vadd.f32 %v49774, %v49861 (stack82)
        %v49869 = vmul.f32 %v49865, %v49817 (stack83)
        %v49873 = vadd.f32 %v49770, %v49869 (stack82)
        %v49877 = vmul.f32 %v49873, %v49817 (stack83)
        %v49881 = vadd.f32 %v49766, %v49877 (stack82)
        %v49885 = vmul.f32 %v49881, %v49732 (stack83)
        %v49889 = vsel /*vm=*/%vm49737, /*on_true_vy=*/%v49742, /*on_false_vx=*/%v49885 (stack72)
        %v49893 = vmul.f32 %v49889, 1.4140625 (stack83)
        %s49895 = scalar_lea.vmem %s280, 180 [#allocation0] (stack107)
        %v49896 = vpack.c.bf16 0.0, %v49893 (stack104)
        %49897 = vst [vmem:[%s49895] sm:$0xf] /*vst_source=*/%v49896 (stack105)
        %v49900 = vadd.s32 %v1381, %v48975 (stack65)
        %s49902 = smul.u32 128, %s27 (stack66)
        %v49903 = vlaneseq (stack67)
        %v49904 = vand.u32 %v49903, 127 (stack68)
        %v49905 = vstv %s49902 (stack69)
        %v49906 = vadd.s32 %v49904, %v49905 (stack70)
        %v49910 = vadd.s32 %v49900, %v49906 (stack65)
        %vm49914 = vcmp.lt.u32.totalorder %v49910, %v49900 (stack71)
        %vm49919 = vcmp.lt.u32.totalorder %v49900, %v1381 (stack71)
        %v49924 = vadd.s32 %v1368, %v48958 (stack65)
        %v49928 = vadd.s32 %v49924, 1 (stack65)
        %v49932 = vsel /*vm=*/%vm49919, /*on_true_vy=*/%v49928, /*on_false_vx=*/%v49924 (stack72)
        %v49936 = vadd.s32 %v49932, 1 (stack65)
        %v49940 = vsel /*vm=*/%vm49914, /*on_true_vy=*/%v49936, /*on_false_vx=*/%v49932 (stack72)
        %v49945 = vadd.s32 %v49940, %v10 (stack65)
        %v49949 = vadd.s32 %v49910, %v9 (stack65)
        %v49953 = vadd.s32 %v49945, %v49949 (stack65)
        %v49955 = vshll.u32 %v49949, 13 (stack73)
        %v49956 = vshrl.u32 %v49949, 19 (stack74)
        %v49957 = vor.u32 %v49955, %v49956 (stack75)
        %v49958 = vxor.u32 %v49953, %v49957 (stack76)
        %v49961 = vadd.s32 %v49953, %v49958 (stack65)
        %v49963 = vshll.u32 %v49958, 15 (stack73)
        %v49964 = vshrl.u32 %v49958, 17 (stack74)
        %v49965 = vor.u32 %v49963, %v49964 (stack75)
        %v49966 = vxor.u32 %v49961, %v49965 (stack76)
        %v49969 = vadd.s32 %v49961, %v49966 (stack65)
        %v49971 = vshll.u32 %v49966, 26 (stack73)
        %v49972 = vshrl.u32 %v49966, 6 (stack74)
        %v49973 = vor.u32 %v49971, %v49972 (stack75)
        %v49974 = vxor.u32 %v49969, %v49973 (stack76)
        %v49977 = vadd.s32 %v49969, %v49974 (stack65)
        %v49981 = vadd.s32 %v49977, %v9 (stack65)
        %v49983 = vshll.u32 %v49974, 6 (stack73)
        %v49984 = vshrl.u32 %v49974, 26 (stack74)
        %v49985 = vor.u32 %v49983, %v49984 (stack75)
        %v49986 = vxor.u32 %v49977, %v49985 (stack76)
        %v49989 = vadd.s32 %v49986, %v8 (stack65)
        %v49993 = vadd.s32 %v49989, 1 (stack65)
        %v49997 = vadd.s32 %v49981, %v49993 (stack65)
        %v49999 = vshll.u32 %v49993, 17 (stack73)
        %v50000 = vshrl.u32 %v49993, 15 (stack74)
        %v50001 = vor.u32 %v49999, %v50000 (stack75)
        %v50002 = vxor.u32 %v49997, %v50001 (stack76)
        %v50005 = vadd.s32 %v49997, %v50002 (stack65)
        %v50007 = vshll.u32 %v50002, 29 (stack73)
        %v50008 = vshrl.u32 %v50002, 3 (stack74)
        %v50009 = vor.u32 %v50007, %v50008 (stack75)
        %v50010 = vxor.u32 %v50005, %v50009 (stack76)
        %v50013 = vadd.s32 %v50005, %v50010 (stack65)
        %v50015 = vshll.u32 %v50010, 16 (stack73)
        %v50016 = vshrl.u32 %v50010, 16 (stack74)
        %v50017 = vor.u32 %v50015, %v50016 (stack75)
        %v50018 = vxor.u32 %v50013, %v50017 (stack76)
        %v50021 = vadd.s32 %v50013, %v50018 (stack65)
        %v50025 = vadd.s32 %v50021, %v8 (stack65)
        %v50027 = vshll.u32 %v50018, 24 (stack73)
        %v50028 = vshrl.u32 %v50018, 8 (stack74)
        %v50029 = vor.u32 %v50027, %v50028 (stack75)
        %v50030 = vxor.u32 %v50021, %v50029 (stack76)
        %v50033 = vadd.s32 %v50030, %v10 (stack65)
        %v50037 = vadd.s32 %v50033, 2 (stack65)
        %v50041 = vadd.s32 %v50025, %v50037 (stack65)
        %v50043 = vshll.u32 %v50037, 13 (stack73)
        %v50044 = vshrl.u32 %v50037, 19 (stack74)
        %v50045 = vor.u32 %v50043, %v50044 (stack75)
        %v50046 = vxor.u32 %v50041, %v50045 (stack76)
        %v50049 = vadd.s32 %v50041, %v50046 (stack65)
        %v50051 = vshll.u32 %v50046, 15 (stack73)
        %v50052 = vshrl.u32 %v50046, 17 (stack74)
        %v50053 = vor.u32 %v50051, %v50052 (stack75)
        %v50054 = vxor.u32 %v50049, %v50053 (stack76)
        %v50057 = vadd.s32 %v50049, %v50054 (stack65)
        %v50059 = vshll.u32 %v50054, 26 (stack73)
        %v50060 = vshrl.u32 %v50054, 6 (stack74)
        %v50061 = vor.u32 %v50059, %v50060 (stack75)
        %v50062 = vxor.u32 %v50057, %v50061 (stack76)
        %v50065 = vadd.s32 %v50057, %v50062 (stack65)
        %v50069 = vadd.s32 %v50065, %v10 (stack65)
        %v50071 = vshll.u32 %v50062, 6 (stack73)
        %v50072 = vshrl.u32 %v50062, 26 (stack74)
        %v50073 = vor.u32 %v50071, %v50072 (stack75)
        %v50074 = vxor.u32 %v50065, %v50073 (stack76)
        %v50077 = vadd.s32 %v50074, %v9 (stack65)
        %v50081 = vadd.s32 %v50077, 3 (stack65)
        %v50085 = vadd.s32 %v50069, %v50081 (stack65)
        %v50087 = vshll.u32 %v50081, 17 (stack73)
        %v50088 = vshrl.u32 %v50081, 15 (stack74)
        %v50089 = vor.u32 %v50087, %v50088 (stack75)
        %v50090 = vxor.u32 %v50085, %v50089 (stack76)
        %v50093 = vadd.s32 %v50085, %v50090 (stack65)
        %v50095 = vshll.u32 %v50090, 29 (stack73)
        %v50096 = vshrl.u32 %v50090, 3 (stack74)
        %v50097 = vor.u32 %v50095, %v50096 (stack75)
        %v50098 = vxor.u32 %v50093, %v50097 (stack76)
        %v50101 = vadd.s32 %v50093, %v50098 (stack65)
        %v50103 = vshll.u32 %v50098, 16 (stack73)
        %v50104 = vshrl.u32 %v50098, 16 (stack74)
        %v50105 = vor.u32 %v50103, %v50104 (stack75)
        %v50106 = vxor.u32 %v50101, %v50105 (stack76)
        %v50109 = vadd.s32 %v50101, %v50106 (stack65)
        %v50113 = vadd.s32 %v50109, %v9 (stack65)
        %v50115 = vshll.u32 %v50106, 24 (stack73)
        %v50116 = vshrl.u32 %v50106, 8 (stack74)
        %v50117 = vor.u32 %v50115, %v50116 (stack75)
        %v50118 = vxor.u32 %v50109, %v50117 (stack76)
        %v50121 = vadd.s32 %v50118, %v8 (stack65)
        %v50125 = vadd.s32 %v50121, 4 (stack65)
        %v50129 = vadd.s32 %v50113, %v50125 (stack65)
        %v50131 = vshll.u32 %v50125, 13 (stack73)
        %v50132 = vshrl.u32 %v50125, 19 (stack74)
        %v50133 = vor.u32 %v50131, %v50132 (stack75)
        %v50134 = vxor.u32 %v50129, %v50133 (stack76)
        %v50137 = vadd.s32 %v50129, %v50134 (stack65)
        %v50139 = vshll.u32 %v50134, 15 (stack73)
        %v50140 = vshrl.u32 %v50134, 17 (stack74)
        %v50141 = vor.u32 %v50139, %v50140 (stack75)
        %v50142 = vxor.u32 %v50137, %v50141 (stack76)
        %v50145 = vadd.s32 %v50137, %v50142 (stack65)
        %v50147 = vshll.u32 %v50142, 26 (stack73)
        %v50148 = vshrl.u32 %v50142, 6 (stack74)
        %v50149 = vor.u32 %v50147, %v50148 (stack75)
        %v50150 = vxor.u32 %v50145, %v50149 (stack76)
        %v50153 = vadd.s32 %v50145, %v50150 (stack65)
        %v50157 = vadd.s32 %v50153, %v8 (stack65)
        %v50159 = vshll.u32 %v50150, 6 (stack73)
        %v50160 = vshrl.u32 %v50150, 26 (stack74)
        %v50161 = vor.u32 %v50159, %v50160 (stack75)
        %v50162 = vxor.u32 %v50153, %v50161 (stack76)
        %v50165 = vadd.s32 %v50162, %v10 (stack65)
        %v50169 = vadd.s32 %v50165, 5 (stack65)
        %v50171 = vxor.u32 %v50157, %v50169 (stack76)
        %v50172 = vand.u32.u8 %v50171, 255 (stack77)
        %v50173 = vand.u32 %v50172, 65535 (stack78)
        %v50174 = vshrl.u32 %v50173, 1 (stack79)
        %v50175 = vor.u32 %v50174, 16256 (stack75)
        %v50176 = vand.u32.u16 %v50175, 65535 (stack80)
        %v50177 = vunpack.i.l.bf16 %v50176 (stack81)
        %v50181 = vadd.f32 %v50177, -1.0 (stack82)
        %v50185 = vmul.f32 %v50181, 2.0 (stack83)
        %v50189 = vadd.f32 %v50185, -0.99609375 (stack82)
        %v50193 = vmax.f32 -0.99609375, %v50189 (stack84)
        %v50195 = vand.u32 2147483647, %v50193 (stack85)
        %vm50198 = vcmp.eq.f32.partialorder %v50195, 1.0 (stack86)
        %v50203 = vmul.f32 %v50193, inf (stack83)
        %v50205 = vxor.u32 %v50193, 2147483648 (stack87)
        %v50208 = vmul.f32 %v50193, %v50205 (stack83)
        %v50210 = vadd.f32 %v50208, 1.0 (stack88)
        %v50211 = vlog2.pop %v50210 (stack89)
        %v50212 = vmul.f32 %v50211, 0.6931472 (stack90)
        %v50213 = vmul.f32 -0.5, %v50208 (stack91)
        %v50214 = vadd.f32 %v50213, 1.0 (stack92)
        %v50215 = vmul.f32 %v50214, %v50208 (stack93)
        %v50216 = vand.u32 2147483647, %v50208 (stack94)
        %vm50217 = vcmp.lt.f32.partialorder %v50216, 0.0004427343 (stack95)
        %v50218 = vsel /*vm=*/%vm50217, /*on_true_vy=*/%v50215, /*on_false_vx=*/%v50212 (stack96)
        %v50219 = vxor.u32 %v50218, 2147483648 (stack87)
        %vm50222 = vcmp.lt.f32.partialorder %v50219, 5.0 (stack86)
        %v50227 = vsel /*vm=*/%vm50222, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v50231 = vsel /*vm=*/%vm50222, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v50235 = vsel /*vm=*/%vm50222, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v50239 = vsel /*vm=*/%vm50222, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v50243 = vsel /*vm=*/%vm50222, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v50247 = vsel /*vm=*/%vm50222, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v50251 = vsel /*vm=*/%vm50222, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v50255 = vsel /*vm=*/%vm50222, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v50259 = vsel /*vm=*/%vm50222, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v50263 = vadd.f32 %v50219, -2.5 (stack82)
        %v50265 = vrsqrt.pop %v50219 (stack97)
        %v50266 = vmul.f32 %v50219, %v50265 (stack98)
        %vm50267 = vcmp.eq.f32.partialorder %v50219, inf (stack99)
        %v50268 = vsel /*vm=*/%vm50267, /*on_true_vy=*/%v50219, /*on_false_vx=*/%v50266 (stack100)
        %vm50269 = vcmp.eq.f32.partialorder %v50219, 0.0 (stack101)
        %v50270 = vand.u32 %v50219, 2147483648 (stack102)
        %v50271 = vsel /*vm=*/%vm50269, /*on_true_vy=*/%v50270, /*on_false_vx=*/%v50268 (stack103)
        %v50274 = vadd.f32 %v50271, -3.0 (stack82)
        %v50278 = vsel /*vm=*/%vm50222, /*on_true_vy=*/%v50263, /*on_false_vx=*/%v50274 (stack72)
        %v50282 = vmul.f32 %v50259, %v50278 (stack83)
        %v50286 = vadd.f32 %v50255, %v50282 (stack82)
        %v50290 = vmul.f32 %v50286, %v50278 (stack83)
        %v50294 = vadd.f32 %v50251, %v50290 (stack82)
        %v50298 = vmul.f32 %v50294, %v50278 (stack83)
        %v50302 = vadd.f32 %v50247, %v50298 (stack82)
        %v50306 = vmul.f32 %v50302, %v50278 (stack83)
        %v50310 = vadd.f32 %v50243, %v50306 (stack82)
        %v50314 = vmul.f32 %v50310, %v50278 (stack83)
        %v50318 = vadd.f32 %v50239, %v50314 (stack82)
        %v50322 = vmul.f32 %v50318, %v50278 (stack83)
        %v50326 = vadd.f32 %v50235, %v50322 (stack82)
        %v50330 = vmul.f32 %v50326, %v50278 (stack83)
        %v50334 = vadd.f32 %v50231, %v50330 (stack82)
        %v50338 = vmul.f32 %v50334, %v50278 (stack83)
        %v50342 = vadd.f32 %v50227, %v50338 (stack82)
        %v50346 = vmul.f32 %v50342, %v50193 (stack83)
        %v50350 = vsel /*vm=*/%vm50198, /*on_true_vy=*/%v50203, /*on_false_vx=*/%v50346 (stack72)
        %v50354 = vmul.f32 %v50350, 1.4140625 (stack83)
        %s50356 = scalar_lea.vmem %s280, 308 [#allocation0] (stack107)
        %v50357 = vpack.c.bf16 0.0, %v50354 (stack104)
        %50358 = vst [vmem:[%s50356] sm:$0xf] /*vst_source=*/%v50357 (stack105)
        %v50361 = vadd.s32 %v1868, %v48975 (stack65)
        %s50363 = smul.u32 128, %s27 (stack66)
        %v50364 = vlaneseq (stack67)
        %v50365 = vand.u32 %v50364, 127 (stack68)
        %v50366 = vstv %s50363 (stack69)
        %v50367 = vadd.s32 %v50365, %v50366 (stack70)
        %v50371 = vadd.s32 %v50361, %v50367 (stack65)
        %vm50375 = vcmp.lt.u32.totalorder %v50371, %v50361 (stack71)
        %vm50380 = vcmp.lt.u32.totalorder %v50361, %v1868 (stack71)
        %v50385 = vadd.s32 %v1855, %v48958 (stack65)
        %v50389 = vadd.s32 %v50385, 1 (stack65)
        %v50393 = vsel /*vm=*/%vm50380, /*on_true_vy=*/%v50389, /*on_false_vx=*/%v50385 (stack72)
        %v50397 = vadd.s32 %v50393, 1 (stack65)
        %v50401 = vsel /*vm=*/%vm50375, /*on_true_vy=*/%v50397, /*on_false_vx=*/%v50393 (stack72)
        %v50406 = vadd.s32 %v50401, %v10 (stack65)
        %v50410 = vadd.s32 %v50371, %v9 (stack65)
        %v50414 = vadd.s32 %v50406, %v50410 (stack65)
        %v50416 = vshll.u32 %v50410, 13 (stack73)
        %v50417 = vshrl.u32 %v50410, 19 (stack74)
        %v50418 = vor.u32 %v50416, %v50417 (stack75)
        %v50419 = vxor.u32 %v50414, %v50418 (stack76)
        %v50422 = vadd.s32 %v50414, %v50419 (stack65)
        %v50424 = vshll.u32 %v50419, 15 (stack73)
        %v50425 = vshrl.u32 %v50419, 17 (stack74)
        %v50426 = vor.u32 %v50424, %v50425 (stack75)
        %v50427 = vxor.u32 %v50422, %v50426 (stack76)
        %v50430 = vadd.s32 %v50422, %v50427 (stack65)
        %v50432 = vshll.u32 %v50427, 26 (stack73)
        %v50433 = vshrl.u32 %v50427, 6 (stack74)
        %v50434 = vor.u32 %v50432, %v50433 (stack75)
        %v50435 = vxor.u32 %v50430, %v50434 (stack76)
        %v50438 = vadd.s32 %v50430, %v50435 (stack65)
        %v50442 = vadd.s32 %v50438, %v9 (stack65)
        %v50444 = vshll.u32 %v50435, 6 (stack73)
        %v50445 = vshrl.u32 %v50435, 26 (stack74)
        %v50446 = vor.u32 %v50444, %v50445 (stack75)
        %v50447 = vxor.u32 %v50438, %v50446 (stack76)
        %v50450 = vadd.s32 %v50447, %v8 (stack65)
        %v50454 = vadd.s32 %v50450, 1 (stack65)
        %v50458 = vadd.s32 %v50442, %v50454 (stack65)
        %v50460 = vshll.u32 %v50454, 17 (stack73)
        %v50461 = vshrl.u32 %v50454, 15 (stack74)
        %v50462 = vor.u32 %v50460, %v50461 (stack75)
        %v50463 = vxor.u32 %v50458, %v50462 (stack76)
        %v50466 = vadd.s32 %v50458, %v50463 (stack65)
        %v50468 = vshll.u32 %v50463, 29 (stack73)
        %v50469 = vshrl.u32 %v50463, 3 (stack74)
        %v50470 = vor.u32 %v50468, %v50469 (stack75)
        %v50471 = vxor.u32 %v50466, %v50470 (stack76)
        %v50474 = vadd.s32 %v50466, %v50471 (stack65)
        %v50476 = vshll.u32 %v50471, 16 (stack73)
        %v50477 = vshrl.u32 %v50471, 16 (stack74)
        %v50478 = vor.u32 %v50476, %v50477 (stack75)
        %v50479 = vxor.u32 %v50474, %v50478 (stack76)
        %v50482 = vadd.s32 %v50474, %v50479 (stack65)
        %v50486 = vadd.s32 %v50482, %v8 (stack65)
        %v50488 = vshll.u32 %v50479, 24 (stack73)
        %v50489 = vshrl.u32 %v50479, 8 (stack74)
        %v50490 = vor.u32 %v50488, %v50489 (stack75)
        %v50491 = vxor.u32 %v50482, %v50490 (stack76)
        %v50494 = vadd.s32 %v50491, %v10 (stack65)
        %v50498 = vadd.s32 %v50494, 2 (stack65)
        %v50502 = vadd.s32 %v50486, %v50498 (stack65)
        %v50504 = vshll.u32 %v50498, 13 (stack73)
        %v50505 = vshrl.u32 %v50498, 19 (stack74)
        %v50506 = vor.u32 %v50504, %v50505 (stack75)
        %v50507 = vxor.u32 %v50502, %v50506 (stack76)
        %v50510 = vadd.s32 %v50502, %v50507 (stack65)
        %v50512 = vshll.u32 %v50507, 15 (stack73)
        %v50513 = vshrl.u32 %v50507, 17 (stack74)
        %v50514 = vor.u32 %v50512, %v50513 (stack75)
        %v50515 = vxor.u32 %v50510, %v50514 (stack76)
        %v50518 = vadd.s32 %v50510, %v50515 (stack65)
        %v50520 = vshll.u32 %v50515, 26 (stack73)
        %v50521 = vshrl.u32 %v50515, 6 (stack74)
        %v50522 = vor.u32 %v50520, %v50521 (stack75)
        %v50523 = vxor.u32 %v50518, %v50522 (stack76)
        %v50526 = vadd.s32 %v50518, %v50523 (stack65)
        %v50530 = vadd.s32 %v50526, %v10 (stack65)
        %v50532 = vshll.u32 %v50523, 6 (stack73)
        %v50533 = vshrl.u32 %v50523, 26 (stack74)
        %v50534 = vor.u32 %v50532, %v50533 (stack75)
        %v50535 = vxor.u32 %v50526, %v50534 (stack76)
        %v50538 = vadd.s32 %v50535, %v9 (stack65)
        %v50542 = vadd.s32 %v50538, 3 (stack65)
        %v50546 = vadd.s32 %v50530, %v50542 (stack65)
        %v50548 = vshll.u32 %v50542, 17 (stack73)
        %v50549 = vshrl.u32 %v50542, 15 (stack74)
        %v50550 = vor.u32 %v50548, %v50549 (stack75)
        %v50551 = vxor.u32 %v50546, %v50550 (stack76)
        %v50554 = vadd.s32 %v50546, %v50551 (stack65)
        %v50556 = vshll.u32 %v50551, 29 (stack73)
        %v50557 = vshrl.u32 %v50551, 3 (stack74)
        %v50558 = vor.u32 %v50556, %v50557 (stack75)
        %v50559 = vxor.u32 %v50554, %v50558 (stack76)
        %v50562 = vadd.s32 %v50554, %v50559 (stack65)
        %v50564 = vshll.u32 %v50559, 16 (stack73)
        %v50565 = vshrl.u32 %v50559, 16 (stack74)
        %v50566 = vor.u32 %v50564, %v50565 (stack75)
        %v50567 = vxor.u32 %v50562, %v50566 (stack76)
        %v50570 = vadd.s32 %v50562, %v50567 (stack65)
        %v50574 = vadd.s32 %v50570, %v9 (stack65)
        %v50576 = vshll.u32 %v50567, 24 (stack73)
        %v50577 = vshrl.u32 %v50567, 8 (stack74)
        %v50578 = vor.u32 %v50576, %v50577 (stack75)
        %v50579 = vxor.u32 %v50570, %v50578 (stack76)
        %v50582 = vadd.s32 %v50579, %v8 (stack65)
        %v50586 = vadd.s32 %v50582, 4 (stack65)
        %v50590 = vadd.s32 %v50574, %v50586 (stack65)
        %v50592 = vshll.u32 %v50586, 13 (stack73)
        %v50593 = vshrl.u32 %v50586, 19 (stack74)
        %v50594 = vor.u32 %v50592, %v50593 (stack75)
        %v50595 = vxor.u32 %v50590, %v50594 (stack76)
        %v50598 = vadd.s32 %v50590, %v50595 (stack65)
        %v50600 = vshll.u32 %v50595, 15 (stack73)
        %v50601 = vshrl.u32 %v50595, 17 (stack74)
        %v50602 = vor.u32 %v50600, %v50601 (stack75)
        %v50603 = vxor.u32 %v50598, %v50602 (stack76)
        %v50606 = vadd.s32 %v50598, %v50603 (stack65)
        %v50608 = vshll.u32 %v50603, 26 (stack73)
        %v50609 = vshrl.u32 %v50603, 6 (stack74)
        %v50610 = vor.u32 %v50608, %v50609 (stack75)
        %v50611 = vxor.u32 %v50606, %v50610 (stack76)
        %v50614 = vadd.s32 %v50606, %v50611 (stack65)
        %v50618 = vadd.s32 %v50614, %v8 (stack65)
        %v50620 = vshll.u32 %v50611, 6 (stack73)
        %v50621 = vshrl.u32 %v50611, 26 (stack74)
        %v50622 = vor.u32 %v50620, %v50621 (stack75)
        %v50623 = vxor.u32 %v50614, %v50622 (stack76)
        %v50626 = vadd.s32 %v50623, %v10 (stack65)
        %v50630 = vadd.s32 %v50626, 5 (stack65)
        %v50632 = vxor.u32 %v50618, %v50630 (stack76)
        %v50633 = vand.u32.u8 %v50632, 255 (stack77)
        %v50634 = vand.u32 %v50633, 65535 (stack78)
        %v50635 = vshrl.u32 %v50634, 1 (stack79)
        %v50636 = vor.u32 %v50635, 16256 (stack75)
        %v50637 = vand.u32.u16 %v50636, 65535 (stack80)
        %v50638 = vunpack.i.l.bf16 %v50637 (stack81)
        %v50642 = vadd.f32 %v50638, -1.0 (stack82)
        %v50646 = vmul.f32 %v50642, 2.0 (stack83)
        %v50650 = vadd.f32 %v50646, -0.99609375 (stack82)
        %v50654 = vmax.f32 -0.99609375, %v50650 (stack84)
        %v50656 = vand.u32 2147483647, %v50654 (stack85)
        %vm50659 = vcmp.eq.f32.partialorder %v50656, 1.0 (stack86)
        %v50664 = vmul.f32 %v50654, inf (stack83)
        %v50666 = vxor.u32 %v50654, 2147483648 (stack87)
        %v50669 = vmul.f32 %v50654, %v50666 (stack83)
        %v50671 = vadd.f32 %v50669, 1.0 (stack88)
        %v50672 = vlog2.pop %v50671 (stack89)
        %v50673 = vmul.f32 %v50672, 0.6931472 (stack90)
        %v50674 = vmul.f32 -0.5, %v50669 (stack91)
        %v50675 = vadd.f32 %v50674, 1.0 (stack92)
        %v50676 = vmul.f32 %v50675, %v50669 (stack93)
        %v50677 = vand.u32 2147483647, %v50669 (stack94)
        %vm50678 = vcmp.lt.f32.partialorder %v50677, 0.0004427343 (stack95)
        %v50679 = vsel /*vm=*/%vm50678, /*on_true_vy=*/%v50676, /*on_false_vx=*/%v50673 (stack96)
        %v50680 = vxor.u32 %v50679, 2147483648 (stack87)
        %vm50683 = vcmp.lt.f32.partialorder %v50680, 5.0 (stack86)
        %v50688 = vsel /*vm=*/%vm50683, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v50692 = vsel /*vm=*/%vm50683, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v50696 = vsel /*vm=*/%vm50683, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v50700 = vsel /*vm=*/%vm50683, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v50704 = vsel /*vm=*/%vm50683, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v50708 = vsel /*vm=*/%vm50683, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v50712 = vsel /*vm=*/%vm50683, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v50716 = vsel /*vm=*/%vm50683, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v50720 = vsel /*vm=*/%vm50683, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v50724 = vadd.f32 %v50680, -2.5 (stack82)
        %v50726 = vrsqrt.pop %v50680 (stack97)
        %v50727 = vmul.f32 %v50680, %v50726 (stack98)
        %vm50728 = vcmp.eq.f32.partialorder %v50680, inf (stack99)
        %v50729 = vsel /*vm=*/%vm50728, /*on_true_vy=*/%v50680, /*on_false_vx=*/%v50727 (stack100)
        %vm50730 = vcmp.eq.f32.partialorder %v50680, 0.0 (stack101)
        %v50731 = vand.u32 %v50680, 2147483648 (stack102)
        %v50732 = vsel /*vm=*/%vm50730, /*on_true_vy=*/%v50731, /*on_false_vx=*/%v50729 (stack103)
        %v50735 = vadd.f32 %v50732, -3.0 (stack82)
        %v50739 = vsel /*vm=*/%vm50683, /*on_true_vy=*/%v50724, /*on_false_vx=*/%v50735 (stack72)
        %v50743 = vmul.f32 %v50720, %v50739 (stack83)
        %v50747 = vadd.f32 %v50716, %v50743 (stack82)
        %v50751 = vmul.f32 %v50747, %v50739 (stack83)
        %v50755 = vadd.f32 %v50712, %v50751 (stack82)
        %v50759 = vmul.f32 %v50755, %v50739 (stack83)
        %v50763 = vadd.f32 %v50708, %v50759 (stack82)
        %v50767 = vmul.f32 %v50763, %v50739 (stack83)
        %v50771 = vadd.f32 %v50704, %v50767 (stack82)
        %v50775 = vmul.f32 %v50771, %v50739 (stack83)
        %v50779 = vadd.f32 %v50700, %v50775 (stack82)
        %v50783 = vmul.f32 %v50779, %v50739 (stack83)
        %v50787 = vadd.f32 %v50696, %v50783 (stack82)
        %v50791 = vmul.f32 %v50787, %v50739 (stack83)
        %v50795 = vadd.f32 %v50692, %v50791 (stack82)
        %v50799 = vmul.f32 %v50795, %v50739 (stack83)
        %v50803 = vadd.f32 %v50688, %v50799 (stack82)
        %v50807 = vmul.f32 %v50803, %v50654 (stack83)
        %v50811 = vsel /*vm=*/%vm50659, /*on_true_vy=*/%v50664, /*on_false_vx=*/%v50807 (stack72)
        %v50815 = vmul.f32 %v50811, 1.4140625 (stack83)
        %s50817 = scalar_lea.vmem %s280, 436 [#allocation0] (stack107)
        %v50818 = vpack.c.bf16 0.0, %v50815 (stack104)
        %50819 = vst [vmem:[%s50817] sm:$0xf] /*vst_source=*/%v50818 (stack105)
        %v50822 = vadd.s32 %v2355, %v48975 (stack65)
        %s50824 = smul.u32 128, %s27 (stack66)
        %v50825 = vlaneseq (stack67)
        %v50826 = vand.u32 %v50825, 127 (stack68)
        %v50827 = vstv %s50824 (stack69)
        %v50828 = vadd.s32 %v50826, %v50827 (stack70)
        %v50832 = vadd.s32 %v50822, %v50828 (stack65)
        %vm50836 = vcmp.lt.u32.totalorder %v50832, %v50822 (stack71)
        %vm50841 = vcmp.lt.u32.totalorder %v50822, %v2355 (stack71)
        %v50846 = vadd.s32 %v2342, %v48958 (stack65)
        %v50850 = vadd.s32 %v50846, 1 (stack65)
        %v50854 = vsel /*vm=*/%vm50841, /*on_true_vy=*/%v50850, /*on_false_vx=*/%v50846 (stack72)
        %v50858 = vadd.s32 %v50854, 1 (stack65)
        %v50862 = vsel /*vm=*/%vm50836, /*on_true_vy=*/%v50858, /*on_false_vx=*/%v50854 (stack72)
        %v50867 = vadd.s32 %v50862, %v10 (stack65)
        %v50871 = vadd.s32 %v50832, %v9 (stack65)
        %v50875 = vadd.s32 %v50867, %v50871 (stack65)
        %v50877 = vshll.u32 %v50871, 13 (stack73)
        %v50878 = vshrl.u32 %v50871, 19 (stack74)
        %v50879 = vor.u32 %v50877, %v50878 (stack75)
        %v50880 = vxor.u32 %v50875, %v50879 (stack76)
        %v50883 = vadd.s32 %v50875, %v50880 (stack65)
        %v50885 = vshll.u32 %v50880, 15 (stack73)
        %v50886 = vshrl.u32 %v50880, 17 (stack74)
        %v50887 = vor.u32 %v50885, %v50886 (stack75)
        %v50888 = vxor.u32 %v50883, %v50887 (stack76)
        %v50891 = vadd.s32 %v50883, %v50888 (stack65)
        %v50893 = vshll.u32 %v50888, 26 (stack73)
        %v50894 = vshrl.u32 %v50888, 6 (stack74)
        %v50895 = vor.u32 %v50893, %v50894 (stack75)
        %v50896 = vxor.u32 %v50891, %v50895 (stack76)
        %v50899 = vadd.s32 %v50891, %v50896 (stack65)
        %v50903 = vadd.s32 %v50899, %v9 (stack65)
        %v50905 = vshll.u32 %v50896, 6 (stack73)
        %v50906 = vshrl.u32 %v50896, 26 (stack74)
        %v50907 = vor.u32 %v50905, %v50906 (stack75)
        %v50908 = vxor.u32 %v50899, %v50907 (stack76)
        %v50911 = vadd.s32 %v50908, %v8 (stack65)
        %v50915 = vadd.s32 %v50911, 1 (stack65)
        %v50919 = vadd.s32 %v50903, %v50915 (stack65)
        %v50921 = vshll.u32 %v50915, 17 (stack73)
        %v50922 = vshrl.u32 %v50915, 15 (stack74)
        %v50923 = vor.u32 %v50921, %v50922 (stack75)
        %v50924 = vxor.u32 %v50919, %v50923 (stack76)
        %v50927 = vadd.s32 %v50919, %v50924 (stack65)
        %v50929 = vshll.u32 %v50924, 29 (stack73)
        %v50930 = vshrl.u32 %v50924, 3 (stack74)
        %v50931 = vor.u32 %v50929, %v50930 (stack75)
        %v50932 = vxor.u32 %v50927, %v50931 (stack76)
        %v50935 = vadd.s32 %v50927, %v50932 (stack65)
        %v50937 = vshll.u32 %v50932, 16 (stack73)
        %v50938 = vshrl.u32 %v50932, 16 (stack74)
        %v50939 = vor.u32 %v50937, %v50938 (stack75)
        %v50940 = vxor.u32 %v50935, %v50939 (stack76)
        %v50943 = vadd.s32 %v50935, %v50940 (stack65)
        %v50947 = vadd.s32 %v50943, %v8 (stack65)
        %v50949 = vshll.u32 %v50940, 24 (stack73)
        %v50950 = vshrl.u32 %v50940, 8 (stack74)
        %v50951 = vor.u32 %v50949, %v50950 (stack75)
        %v50952 = vxor.u32 %v50943, %v50951 (stack76)
        %v50955 = vadd.s32 %v50952, %v10 (stack65)
        %v50959 = vadd.s32 %v50955, 2 (stack65)
        %v50963 = vadd.s32 %v50947, %v50959 (stack65)
        %v50965 = vshll.u32 %v50959, 13 (stack73)
        %v50966 = vshrl.u32 %v50959, 19 (stack74)
        %v50967 = vor.u32 %v50965, %v50966 (stack75)
        %v50968 = vxor.u32 %v50963, %v50967 (stack76)
        %v50971 = vadd.s32 %v50963, %v50968 (stack65)
        %v50973 = vshll.u32 %v50968, 15 (stack73)
        %v50974 = vshrl.u32 %v50968, 17 (stack74)
        %v50975 = vor.u32 %v50973, %v50974 (stack75)
        %v50976 = vxor.u32 %v50971, %v50975 (stack76)
        %v50979 = vadd.s32 %v50971, %v50976 (stack65)
        %v50981 = vshll.u32 %v50976, 26 (stack73)
        %v50982 = vshrl.u32 %v50976, 6 (stack74)
        %v50983 = vor.u32 %v50981, %v50982 (stack75)
        %v50984 = vxor.u32 %v50979, %v50983 (stack76)
        %v50987 = vadd.s32 %v50979, %v50984 (stack65)
        %v50991 = vadd.s32 %v50987, %v10 (stack65)
        %v50993 = vshll.u32 %v50984, 6 (stack73)
        %v50994 = vshrl.u32 %v50984, 26 (stack74)
        %v50995 = vor.u32 %v50993, %v50994 (stack75)
        %v50996 = vxor.u32 %v50987, %v50995 (stack76)
        %v50999 = vadd.s32 %v50996, %v9 (stack65)
        %v51003 = vadd.s32 %v50999, 3 (stack65)
        %v51007 = vadd.s32 %v50991, %v51003 (stack65)
        %v51009 = vshll.u32 %v51003, 17 (stack73)
        %v51010 = vshrl.u32 %v51003, 15 (stack74)
        %v51011 = vor.u32 %v51009, %v51010 (stack75)
        %v51012 = vxor.u32 %v51007, %v51011 (stack76)
        %v51015 = vadd.s32 %v51007, %v51012 (stack65)
        %v51017 = vshll.u32 %v51012, 29 (stack73)
        %v51018 = vshrl.u32 %v51012, 3 (stack74)
        %v51019 = vor.u32 %v51017, %v51018 (stack75)
        %v51020 = vxor.u32 %v51015, %v51019 (stack76)
        %v51023 = vadd.s32 %v51015, %v51020 (stack65)
        %v51025 = vshll.u32 %v51020, 16 (stack73)
        %v51026 = vshrl.u32 %v51020, 16 (stack74)
        %v51027 = vor.u32 %v51025, %v51026 (stack75)
        %v51028 = vxor.u32 %v51023, %v51027 (stack76)
        %v51031 = vadd.s32 %v51023, %v51028 (stack65)
        %v51035 = vadd.s32 %v51031, %v9 (stack65)
        %v51037 = vshll.u32 %v51028, 24 (stack73)
        %v51038 = vshrl.u32 %v51028, 8 (stack74)
        %v51039 = vor.u32 %v51037, %v51038 (stack75)
        %v51040 = vxor.u32 %v51031, %v51039 (stack76)
        %v51043 = vadd.s32 %v51040, %v8 (stack65)
        %v51047 = vadd.s32 %v51043, 4 (stack65)
        %v51051 = vadd.s32 %v51035, %v51047 (stack65)
        %v51053 = vshll.u32 %v51047, 13 (stack73)
        %v51054 = vshrl.u32 %v51047, 19 (stack74)
        %v51055 = vor.u32 %v51053, %v51054 (stack75)
        %v51056 = vxor.u32 %v51051, %v51055 (stack76)
        %v51059 = vadd.s32 %v51051, %v51056 (stack65)
        %v51061 = vshll.u32 %v51056, 15 (stack73)
        %v51062 = vshrl.u32 %v51056, 17 (stack74)
        %v51063 = vor.u32 %v51061, %v51062 (stack75)
        %v51064 = vxor.u32 %v51059, %v51063 (stack76)
        %v51067 = vadd.s32 %v51059, %v51064 (stack65)
        %v51069 = vshll.u32 %v51064, 26 (stack73)
        %v51070 = vshrl.u32 %v51064, 6 (stack74)
        %v51071 = vor.u32 %v51069, %v51070 (stack75)
        %v51072 = vxor.u32 %v51067, %v51071 (stack76)
        %v51075 = vadd.s32 %v51067, %v51072 (stack65)
        %v51079 = vadd.s32 %v51075, %v8 (stack65)
        %v51081 = vshll.u32 %v51072, 6 (stack73)
        %v51082 = vshrl.u32 %v51072, 26 (stack74)
        %v51083 = vor.u32 %v51081, %v51082 (stack75)
        %v51084 = vxor.u32 %v51075, %v51083 (stack76)
        %v51087 = vadd.s32 %v51084, %v10 (stack65)
        %v51091 = vadd.s32 %v51087, 5 (stack65)
        %v51093 = vxor.u32 %v51079, %v51091 (stack76)
        %v51094 = vand.u32.u8 %v51093, 255 (stack77)
        %v51095 = vand.u32 %v51094, 65535 (stack78)
        %v51096 = vshrl.u32 %v51095, 1 (stack79)
        %v51097 = vor.u32 %v51096, 16256 (stack75)
        %v51098 = vand.u32.u16 %v51097, 65535 (stack80)
        %v51099 = vunpack.i.l.bf16 %v51098 (stack81)
        %v51103 = vadd.f32 %v51099, -1.0 (stack82)
        %v51107 = vmul.f32 %v51103, 2.0 (stack83)
        %v51111 = vadd.f32 %v51107, -0.99609375 (stack82)
        %v51115 = vmax.f32 -0.99609375, %v51111 (stack84)
        %v51117 = vand.u32 2147483647, %v51115 (stack85)
        %vm51120 = vcmp.eq.f32.partialorder %v51117, 1.0 (stack86)
        %v51125 = vmul.f32 %v51115, inf (stack83)
        %v51127 = vxor.u32 %v51115, 2147483648 (stack87)
        %v51130 = vmul.f32 %v51115, %v51127 (stack83)
        %v51132 = vadd.f32 %v51130, 1.0 (stack88)
        %v51133 = vlog2.pop %v51132 (stack89)
        %v51134 = vmul.f32 %v51133, 0.6931472 (stack90)
        %v51135 = vmul.f32 -0.5, %v51130 (stack91)
        %v51136 = vadd.f32 %v51135, 1.0 (stack92)
        %v51137 = vmul.f32 %v51136, %v51130 (stack93)
        %v51138 = vand.u32 2147483647, %v51130 (stack94)
        %vm51139 = vcmp.lt.f32.partialorder %v51138, 0.0004427343 (stack95)
        %v51140 = vsel /*vm=*/%vm51139, /*on_true_vy=*/%v51137, /*on_false_vx=*/%v51134 (stack96)
        %v51141 = vxor.u32 %v51140, 2147483648 (stack87)
        %vm51144 = vcmp.lt.f32.partialorder %v51141, 5.0 (stack86)
        %v51149 = vsel /*vm=*/%vm51144, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v51153 = vsel /*vm=*/%vm51144, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v51157 = vsel /*vm=*/%vm51144, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v51161 = vsel /*vm=*/%vm51144, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v51165 = vsel /*vm=*/%vm51144, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v51169 = vsel /*vm=*/%vm51144, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v51173 = vsel /*vm=*/%vm51144, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v51177 = vsel /*vm=*/%vm51144, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v51181 = vsel /*vm=*/%vm51144, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v51185 = vadd.f32 %v51141, -2.5 (stack82)
        %v51187 = vrsqrt.pop %v51141 (stack97)
        %v51188 = vmul.f32 %v51141, %v51187 (stack98)
        %vm51189 = vcmp.eq.f32.partialorder %v51141, inf (stack99)
        %v51190 = vsel /*vm=*/%vm51189, /*on_true_vy=*/%v51141, /*on_false_vx=*/%v51188 (stack100)
        %vm51191 = vcmp.eq.f32.partialorder %v51141, 0.0 (stack101)
        %v51192 = vand.u32 %v51141, 2147483648 (stack102)
        %v51193 = vsel /*vm=*/%vm51191, /*on_true_vy=*/%v51192, /*on_false_vx=*/%v51190 (stack103)
        %v51196 = vadd.f32 %v51193, -3.0 (stack82)
        %v51200 = vsel /*vm=*/%vm51144, /*on_true_vy=*/%v51185, /*on_false_vx=*/%v51196 (stack72)
        %v51204 = vmul.f32 %v51181, %v51200 (stack83)
        %v51208 = vadd.f32 %v51177, %v51204 (stack82)
        %v51212 = vmul.f32 %v51208, %v51200 (stack83)
        %v51216 = vadd.f32 %v51173, %v51212 (stack82)
        %v51220 = vmul.f32 %v51216, %v51200 (stack83)
        %v51224 = vadd.f32 %v51169, %v51220 (stack82)
        %v51228 = vmul.f32 %v51224, %v51200 (stack83)
        %v51232 = vadd.f32 %v51165, %v51228 (stack82)
        %v51236 = vmul.f32 %v51232, %v51200 (stack83)
        %v51240 = vadd.f32 %v51161, %v51236 (stack82)
        %v51244 = vmul.f32 %v51240, %v51200 (stack83)
        %v51248 = vadd.f32 %v51157, %v51244 (stack82)
        %v51252 = vmul.f32 %v51248, %v51200 (stack83)
        %v51256 = vadd.f32 %v51153, %v51252 (stack82)
        %v51260 = vmul.f32 %v51256, %v51200 (stack83)
        %v51264 = vadd.f32 %v51149, %v51260 (stack82)
        %v51268 = vmul.f32 %v51264, %v51115 (stack83)
        %v51272 = vsel /*vm=*/%vm51120, /*on_true_vy=*/%v51125, /*on_false_vx=*/%v51268 (stack72)
        %v51276 = vmul.f32 %v51272, 1.4140625 (stack83)
        %s51278 = scalar_lea.vmem %s280, 564 [#allocation0] (stack107)
        %v51279 = vpack.c.bf16 0.0, %v51276 (stack104)
        %51280 = vst [vmem:[%s51278] sm:$0xf] /*vst_source=*/%v51279 (stack105)
        %v51283 = vadd.s32 %v2842, %v48975 (stack65)
        %s51285 = smul.u32 128, %s27 (stack66)
        %v51286 = vlaneseq (stack67)
        %v51287 = vand.u32 %v51286, 127 (stack68)
        %v51288 = vstv %s51285 (stack69)
        %v51289 = vadd.s32 %v51287, %v51288 (stack70)
        %v51293 = vadd.s32 %v51283, %v51289 (stack65)
        %vm51297 = vcmp.lt.u32.totalorder %v51293, %v51283 (stack71)
        %vm51302 = vcmp.lt.u32.totalorder %v51283, %v2842 (stack71)
        %v51307 = vadd.s32 %v2829, %v48958 (stack65)
        %v51311 = vadd.s32 %v51307, 1 (stack65)
        %v51315 = vsel /*vm=*/%vm51302, /*on_true_vy=*/%v51311, /*on_false_vx=*/%v51307 (stack72)
        %v51319 = vadd.s32 %v51315, 1 (stack65)
        %v51323 = vsel /*vm=*/%vm51297, /*on_true_vy=*/%v51319, /*on_false_vx=*/%v51315 (stack72)
        %v51328 = vadd.s32 %v51323, %v10 (stack65)
        %v51332 = vadd.s32 %v51293, %v9 (stack65)
        %v51336 = vadd.s32 %v51328, %v51332 (stack65)
        %v51338 = vshll.u32 %v51332, 13 (stack73)
        %v51339 = vshrl.u32 %v51332, 19 (stack74)
        %v51340 = vor.u32 %v51338, %v51339 (stack75)
        %v51341 = vxor.u32 %v51336, %v51340 (stack76)
        %v51344 = vadd.s32 %v51336, %v51341 (stack65)
        %v51346 = vshll.u32 %v51341, 15 (stack73)
        %v51347 = vshrl.u32 %v51341, 17 (stack74)
        %v51348 = vor.u32 %v51346, %v51347 (stack75)
        %v51349 = vxor.u32 %v51344, %v51348 (stack76)
        %v51352 = vadd.s32 %v51344, %v51349 (stack65)
        %v51354 = vshll.u32 %v51349, 26 (stack73)
        %v51355 = vshrl.u32 %v51349, 6 (stack74)
        %v51356 = vor.u32 %v51354, %v51355 (stack75)
        %v51357 = vxor.u32 %v51352, %v51356 (stack76)
        %v51360 = vadd.s32 %v51352, %v51357 (stack65)
        %v51364 = vadd.s32 %v51360, %v9 (stack65)
        %v51366 = vshll.u32 %v51357, 6 (stack73)
        %v51367 = vshrl.u32 %v51357, 26 (stack74)
        %v51368 = vor.u32 %v51366, %v51367 (stack75)
        %v51369 = vxor.u32 %v51360, %v51368 (stack76)
        %v51372 = vadd.s32 %v51369, %v8 (stack65)
        %v51376 = vadd.s32 %v51372, 1 (stack65)
        %v51380 = vadd.s32 %v51364, %v51376 (stack65)
        %v51382 = vshll.u32 %v51376, 17 (stack73)
        %v51383 = vshrl.u32 %v51376, 15 (stack74)
        %v51384 = vor.u32 %v51382, %v51383 (stack75)
        %v51385 = vxor.u32 %v51380, %v51384 (stack76)
        %v51388 = vadd.s32 %v51380, %v51385 (stack65)
        %v51390 = vshll.u32 %v51385, 29 (stack73)
        %v51391 = vshrl.u32 %v51385, 3 (stack74)
        %v51392 = vor.u32 %v51390, %v51391 (stack75)
        %v51393 = vxor.u32 %v51388, %v51392 (stack76)
        %v51396 = vadd.s32 %v51388, %v51393 (stack65)
        %v51398 = vshll.u32 %v51393, 16 (stack73)
        %v51399 = vshrl.u32 %v51393, 16 (stack74)
        %v51400 = vor.u32 %v51398, %v51399 (stack75)
        %v51401 = vxor.u32 %v51396, %v51400 (stack76)
        %v51404 = vadd.s32 %v51396, %v51401 (stack65)
        %v51408 = vadd.s32 %v51404, %v8 (stack65)
        %v51410 = vshll.u32 %v51401, 24 (stack73)
        %v51411 = vshrl.u32 %v51401, 8 (stack74)
        %v51412 = vor.u32 %v51410, %v51411 (stack75)
        %v51413 = vxor.u32 %v51404, %v51412 (stack76)
        %v51416 = vadd.s32 %v51413, %v10 (stack65)
        %v51420 = vadd.s32 %v51416, 2 (stack65)
        %v51424 = vadd.s32 %v51408, %v51420 (stack65)
        %v51426 = vshll.u32 %v51420, 13 (stack73)
        %v51427 = vshrl.u32 %v51420, 19 (stack74)
        %v51428 = vor.u32 %v51426, %v51427 (stack75)
        %v51429 = vxor.u32 %v51424, %v51428 (stack76)
        %v51432 = vadd.s32 %v51424, %v51429 (stack65)
        %v51434 = vshll.u32 %v51429, 15 (stack73)
        %v51435 = vshrl.u32 %v51429, 17 (stack74)
        %v51436 = vor.u32 %v51434, %v51435 (stack75)
        %v51437 = vxor.u32 %v51432, %v51436 (stack76)
        %v51440 = vadd.s32 %v51432, %v51437 (stack65)
        %v51442 = vshll.u32 %v51437, 26 (stack73)
        %v51443 = vshrl.u32 %v51437, 6 (stack74)
        %v51444 = vor.u32 %v51442, %v51443 (stack75)
        %v51445 = vxor.u32 %v51440, %v51444 (stack76)
        %v51448 = vadd.s32 %v51440, %v51445 (stack65)
        %v51452 = vadd.s32 %v51448, %v10 (stack65)
        %v51454 = vshll.u32 %v51445, 6 (stack73)
        %v51455 = vshrl.u32 %v51445, 26 (stack74)
        %v51456 = vor.u32 %v51454, %v51455 (stack75)
        %v51457 = vxor.u32 %v51448, %v51456 (stack76)
        %v51460 = vadd.s32 %v51457, %v9 (stack65)
        %v51464 = vadd.s32 %v51460, 3 (stack65)
        %v51468 = vadd.s32 %v51452, %v51464 (stack65)
        %v51470 = vshll.u32 %v51464, 17 (stack73)
        %v51471 = vshrl.u32 %v51464, 15 (stack74)
        %v51472 = vor.u32 %v51470, %v51471 (stack75)
        %v51473 = vxor.u32 %v51468, %v51472 (stack76)
        %v51476 = vadd.s32 %v51468, %v51473 (stack65)
        %v51478 = vshll.u32 %v51473, 29 (stack73)
        %v51479 = vshrl.u32 %v51473, 3 (stack74)
        %v51480 = vor.u32 %v51478, %v51479 (stack75)
        %v51481 = vxor.u32 %v51476, %v51480 (stack76)
        %v51484 = vadd.s32 %v51476, %v51481 (stack65)
        %v51486 = vshll.u32 %v51481, 16 (stack73)
        %v51487 = vshrl.u32 %v51481, 16 (stack74)
        %v51488 = vor.u32 %v51486, %v51487 (stack75)
        %v51489 = vxor.u32 %v51484, %v51488 (stack76)
        %v51492 = vadd.s32 %v51484, %v51489 (stack65)
        %v51496 = vadd.s32 %v51492, %v9 (stack65)
        %v51498 = vshll.u32 %v51489, 24 (stack73)
        %v51499 = vshrl.u32 %v51489, 8 (stack74)
        %v51500 = vor.u32 %v51498, %v51499 (stack75)
        %v51501 = vxor.u32 %v51492, %v51500 (stack76)
        %v51504 = vadd.s32 %v51501, %v8 (stack65)
        %v51508 = vadd.s32 %v51504, 4 (stack65)
        %v51512 = vadd.s32 %v51496, %v51508 (stack65)
        %v51514 = vshll.u32 %v51508, 13 (stack73)
        %v51515 = vshrl.u32 %v51508, 19 (stack74)
        %v51516 = vor.u32 %v51514, %v51515 (stack75)
        %v51517 = vxor.u32 %v51512, %v51516 (stack76)
        %v51520 = vadd.s32 %v51512, %v51517 (stack65)
        %v51522 = vshll.u32 %v51517, 15 (stack73)
        %v51523 = vshrl.u32 %v51517, 17 (stack74)
        %v51524 = vor.u32 %v51522, %v51523 (stack75)
        %v51525 = vxor.u32 %v51520, %v51524 (stack76)
        %v51528 = vadd.s32 %v51520, %v51525 (stack65)
        %v51530 = vshll.u32 %v51525, 26 (stack73)
        %v51531 = vshrl.u32 %v51525, 6 (stack74)
        %v51532 = vor.u32 %v51530, %v51531 (stack75)
        %v51533 = vxor.u32 %v51528, %v51532 (stack76)
        %v51536 = vadd.s32 %v51528, %v51533 (stack65)
        %v51540 = vadd.s32 %v51536, %v8 (stack65)
        %v51542 = vshll.u32 %v51533, 6 (stack73)
        %v51543 = vshrl.u32 %v51533, 26 (stack74)
        %v51544 = vor.u32 %v51542, %v51543 (stack75)
        %v51545 = vxor.u32 %v51536, %v51544 (stack76)
        %v51548 = vadd.s32 %v51545, %v10 (stack65)
        %v51552 = vadd.s32 %v51548, 5 (stack65)
        %v51554 = vxor.u32 %v51540, %v51552 (stack76)
        %v51555 = vand.u32.u8 %v51554, 255 (stack77)
        %v51556 = vand.u32 %v51555, 65535 (stack78)
        %v51557 = vshrl.u32 %v51556, 1 (stack79)
        %v51558 = vor.u32 %v51557, 16256 (stack75)
        %v51559 = vand.u32.u16 %v51558, 65535 (stack80)
        %v51560 = vunpack.i.l.bf16 %v51559 (stack81)
        %v51564 = vadd.f32 %v51560, -1.0 (stack82)
        %v51568 = vmul.f32 %v51564, 2.0 (stack83)
        %v51572 = vadd.f32 %v51568, -0.99609375 (stack82)
        %v51576 = vmax.f32 -0.99609375, %v51572 (stack84)
        %v51578 = vand.u32 2147483647, %v51576 (stack85)
        %vm51581 = vcmp.eq.f32.partialorder %v51578, 1.0 (stack86)
        %v51586 = vmul.f32 %v51576, inf (stack83)
        %v51588 = vxor.u32 %v51576, 2147483648 (stack87)
        %v51591 = vmul.f32 %v51576, %v51588 (stack83)
        %v51593 = vadd.f32 %v51591, 1.0 (stack88)
        %v51594 = vlog2.pop %v51593 (stack89)
        %v51595 = vmul.f32 %v51594, 0.6931472 (stack90)
        %v51596 = vmul.f32 -0.5, %v51591 (stack91)
        %v51597 = vadd.f32 %v51596, 1.0 (stack92)
        %v51598 = vmul.f32 %v51597, %v51591 (stack93)
        %v51599 = vand.u32 2147483647, %v51591 (stack94)
        %vm51600 = vcmp.lt.f32.partialorder %v51599, 0.0004427343 (stack95)
        %v51601 = vsel /*vm=*/%vm51600, /*on_true_vy=*/%v51598, /*on_false_vx=*/%v51595 (stack96)
        %v51602 = vxor.u32 %v51601, 2147483648 (stack87)
        %vm51605 = vcmp.lt.f32.partialorder %v51602, 5.0 (stack86)
        %v51610 = vsel /*vm=*/%vm51605, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v51614 = vsel /*vm=*/%vm51605, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v51618 = vsel /*vm=*/%vm51605, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v51622 = vsel /*vm=*/%vm51605, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v51626 = vsel /*vm=*/%vm51605, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v51630 = vsel /*vm=*/%vm51605, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v51634 = vsel /*vm=*/%vm51605, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v51638 = vsel /*vm=*/%vm51605, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v51642 = vsel /*vm=*/%vm51605, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v51646 = vadd.f32 %v51602, -2.5 (stack82)
        %v51648 = vrsqrt.pop %v51602 (stack97)
        %v51649 = vmul.f32 %v51602, %v51648 (stack98)
        %vm51650 = vcmp.eq.f32.partialorder %v51602, inf (stack99)
        %v51651 = vsel /*vm=*/%vm51650, /*on_true_vy=*/%v51602, /*on_false_vx=*/%v51649 (stack100)
        %vm51652 = vcmp.eq.f32.partialorder %v51602, 0.0 (stack101)
        %v51653 = vand.u32 %v51602, 2147483648 (stack102)
        %v51654 = vsel /*vm=*/%vm51652, /*on_true_vy=*/%v51653, /*on_false_vx=*/%v51651 (stack103)
        %v51657 = vadd.f32 %v51654, -3.0 (stack82)
        %v51661 = vsel /*vm=*/%vm51605, /*on_true_vy=*/%v51646, /*on_false_vx=*/%v51657 (stack72)
        %v51665 = vmul.f32 %v51642, %v51661 (stack83)
        %v51669 = vadd.f32 %v51638, %v51665 (stack82)
        %v51673 = vmul.f32 %v51669, %v51661 (stack83)
        %v51677 = vadd.f32 %v51634, %v51673 (stack82)
        %v51681 = vmul.f32 %v51677, %v51661 (stack83)
        %v51685 = vadd.f32 %v51630, %v51681 (stack82)
        %v51689 = vmul.f32 %v51685, %v51661 (stack83)
        %v51693 = vadd.f32 %v51626, %v51689 (stack82)
        %v51697 = vmul.f32 %v51693, %v51661 (stack83)
        %v51701 = vadd.f32 %v51622, %v51697 (stack82)
        %v51705 = vmul.f32 %v51701, %v51661 (stack83)
        %v51709 = vadd.f32 %v51618, %v51705 (stack82)
        %v51713 = vmul.f32 %v51709, %v51661 (stack83)
        %v51717 = vadd.f32 %v51614, %v51713 (stack82)
        %v51721 = vmul.f32 %v51717, %v51661 (stack83)
        %v51725 = vadd.f32 %v51610, %v51721 (stack82)
        %v51729 = vmul.f32 %v51725, %v51576 (stack83)
        %v51733 = vsel /*vm=*/%vm51581, /*on_true_vy=*/%v51586, /*on_false_vx=*/%v51729 (stack72)
        %v51737 = vmul.f32 %v51733, 1.4140625 (stack83)
        %s51739 = scalar_lea.vmem %s280, 692 [#allocation0] (stack107)
        %v51740 = vpack.c.bf16 0.0, %v51737 (stack104)
        %51741 = vst [vmem:[%s51739] sm:$0xf] /*vst_source=*/%v51740 (stack105)
        %v51744 = vadd.s32 %v3329, %v48975 (stack65)
        %s51746 = smul.u32 128, %s27 (stack66)
        %v51747 = vlaneseq (stack67)
        %v51748 = vand.u32 %v51747, 127 (stack68)
        %v51749 = vstv %s51746 (stack69)
        %v51750 = vadd.s32 %v51748, %v51749 (stack70)
        %v51754 = vadd.s32 %v51744, %v51750 (stack65)
        %vm51758 = vcmp.lt.u32.totalorder %v51754, %v51744 (stack71)
        %vm51763 = vcmp.lt.u32.totalorder %v51744, %v3329 (stack71)
        %v51768 = vadd.s32 %v3316, %v48958 (stack65)
        %v51772 = vadd.s32 %v51768, 1 (stack65)
        %v51776 = vsel /*vm=*/%vm51763, /*on_true_vy=*/%v51772, /*on_false_vx=*/%v51768 (stack72)
        %v51780 = vadd.s32 %v51776, 1 (stack65)
        %v51784 = vsel /*vm=*/%vm51758, /*on_true_vy=*/%v51780, /*on_false_vx=*/%v51776 (stack72)
        %v51789 = vadd.s32 %v51784, %v10 (stack65)
        %v51793 = vadd.s32 %v51754, %v9 (stack65)
        %v51797 = vadd.s32 %v51789, %v51793 (stack65)
        %v51799 = vshll.u32 %v51793, 13 (stack73)
        %v51800 = vshrl.u32 %v51793, 19 (stack74)
        %v51801 = vor.u32 %v51799, %v51800 (stack75)
        %v51802 = vxor.u32 %v51797, %v51801 (stack76)
        %v51805 = vadd.s32 %v51797, %v51802 (stack65)
        %v51807 = vshll.u32 %v51802, 15 (stack73)
        %v51808 = vshrl.u32 %v51802, 17 (stack74)
        %v51809 = vor.u32 %v51807, %v51808 (stack75)
        %v51810 = vxor.u32 %v51805, %v51809 (stack76)
        %v51813 = vadd.s32 %v51805, %v51810 (stack65)
        %v51815 = vshll.u32 %v51810, 26 (stack73)
        %v51816 = vshrl.u32 %v51810, 6 (stack74)
        %v51817 = vor.u32 %v51815, %v51816 (stack75)
        %v51818 = vxor.u32 %v51813, %v51817 (stack76)
        %v51821 = vadd.s32 %v51813, %v51818 (stack65)
        %v51825 = vadd.s32 %v51821, %v9 (stack65)
        %v51827 = vshll.u32 %v51818, 6 (stack73)
        %v51828 = vshrl.u32 %v51818, 26 (stack74)
        %v51829 = vor.u32 %v51827, %v51828 (stack75)
        %v51830 = vxor.u32 %v51821, %v51829 (stack76)
        %v51833 = vadd.s32 %v51830, %v8 (stack65)
        %v51837 = vadd.s32 %v51833, 1 (stack65)
        %v51841 = vadd.s32 %v51825, %v51837 (stack65)
        %v51843 = vshll.u32 %v51837, 17 (stack73)
        %v51844 = vshrl.u32 %v51837, 15 (stack74)
        %v51845 = vor.u32 %v51843, %v51844 (stack75)
        %v51846 = vxor.u32 %v51841, %v51845 (stack76)
        %v51849 = vadd.s32 %v51841, %v51846 (stack65)
        %v51851 = vshll.u32 %v51846, 29 (stack73)
        %v51852 = vshrl.u32 %v51846, 3 (stack74)
        %v51853 = vor.u32 %v51851, %v51852 (stack75)
        %v51854 = vxor.u32 %v51849, %v51853 (stack76)
        %v51857 = vadd.s32 %v51849, %v51854 (stack65)
        %v51859 = vshll.u32 %v51854, 16 (stack73)
        %v51860 = vshrl.u32 %v51854, 16 (stack74)
        %v51861 = vor.u32 %v51859, %v51860 (stack75)
        %v51862 = vxor.u32 %v51857, %v51861 (stack76)
        %v51865 = vadd.s32 %v51857, %v51862 (stack65)
        %v51869 = vadd.s32 %v51865, %v8 (stack65)
        %v51871 = vshll.u32 %v51862, 24 (stack73)
        %v51872 = vshrl.u32 %v51862, 8 (stack74)
        %v51873 = vor.u32 %v51871, %v51872 (stack75)
        %v51874 = vxor.u32 %v51865, %v51873 (stack76)
        %v51877 = vadd.s32 %v51874, %v10 (stack65)
        %v51881 = vadd.s32 %v51877, 2 (stack65)
        %v51885 = vadd.s32 %v51869, %v51881 (stack65)
        %v51887 = vshll.u32 %v51881, 13 (stack73)
        %v51888 = vshrl.u32 %v51881, 19 (stack74)
        %v51889 = vor.u32 %v51887, %v51888 (stack75)
        %v51890 = vxor.u32 %v51885, %v51889 (stack76)
        %v51893 = vadd.s32 %v51885, %v51890 (stack65)
        %v51895 = vshll.u32 %v51890, 15 (stack73)
        %v51896 = vshrl.u32 %v51890, 17 (stack74)
        %v51897 = vor.u32 %v51895, %v51896 (stack75)
        %v51898 = vxor.u32 %v51893, %v51897 (stack76)
        %v51901 = vadd.s32 %v51893, %v51898 (stack65)
        %v51903 = vshll.u32 %v51898, 26 (stack73)
        %v51904 = vshrl.u32 %v51898, 6 (stack74)
        %v51905 = vor.u32 %v51903, %v51904 (stack75)
        %v51906 = vxor.u32 %v51901, %v51905 (stack76)
        %v51909 = vadd.s32 %v51901, %v51906 (stack65)
        %v51913 = vadd.s32 %v51909, %v10 (stack65)
        %v51915 = vshll.u32 %v51906, 6 (stack73)
        %v51916 = vshrl.u32 %v51906, 26 (stack74)
        %v51917 = vor.u32 %v51915, %v51916 (stack75)
        %v51918 = vxor.u32 %v51909, %v51917 (stack76)
        %v51921 = vadd.s32 %v51918, %v9 (stack65)
        %v51925 = vadd.s32 %v51921, 3 (stack65)
        %v51929 = vadd.s32 %v51913, %v51925 (stack65)
        %v51931 = vshll.u32 %v51925, 17 (stack73)
        %v51932 = vshrl.u32 %v51925, 15 (stack74)
        %v51933 = vor.u32 %v51931, %v51932 (stack75)
        %v51934 = vxor.u32 %v51929, %v51933 (stack76)
        %v51937 = vadd.s32 %v51929, %v51934 (stack65)
        %v51939 = vshll.u32 %v51934, 29 (stack73)
        %v51940 = vshrl.u32 %v51934, 3 (stack74)
        %v51941 = vor.u32 %v51939, %v51940 (stack75)
        %v51942 = vxor.u32 %v51937, %v51941 (stack76)
        %v51945 = vadd.s32 %v51937, %v51942 (stack65)
        %v51947 = vshll.u32 %v51942, 16 (stack73)
        %v51948 = vshrl.u32 %v51942, 16 (stack74)
        %v51949 = vor.u32 %v51947, %v51948 (stack75)
        %v51950 = vxor.u32 %v51945, %v51949 (stack76)
        %v51953 = vadd.s32 %v51945, %v51950 (stack65)
        %v51957 = vadd.s32 %v51953, %v9 (stack65)
        %v51959 = vshll.u32 %v51950, 24 (stack73)
        %v51960 = vshrl.u32 %v51950, 8 (stack74)
        %v51961 = vor.u32 %v51959, %v51960 (stack75)
        %v51962 = vxor.u32 %v51953, %v51961 (stack76)
        %v51965 = vadd.s32 %v51962, %v8 (stack65)
        %v51969 = vadd.s32 %v51965, 4 (stack65)
        %v51973 = vadd.s32 %v51957, %v51969 (stack65)
        %v51975 = vshll.u32 %v51969, 13 (stack73)
        %v51976 = vshrl.u32 %v51969, 19 (stack74)
        %v51977 = vor.u32 %v51975, %v51976 (stack75)
        %v51978 = vxor.u32 %v51973, %v51977 (stack76)
        %v51981 = vadd.s32 %v51973, %v51978 (stack65)
        %v51983 = vshll.u32 %v51978, 15 (stack73)
        %v51984 = vshrl.u32 %v51978, 17 (stack74)
        %v51985 = vor.u32 %v51983, %v51984 (stack75)
        %v51986 = vxor.u32 %v51981, %v51985 (stack76)
        %v51989 = vadd.s32 %v51981, %v51986 (stack65)
        %v51991 = vshll.u32 %v51986, 26 (stack73)
        %v51992 = vshrl.u32 %v51986, 6 (stack74)
        %v51993 = vor.u32 %v51991, %v51992 (stack75)
        %v51994 = vxor.u32 %v51989, %v51993 (stack76)
        %v51997 = vadd.s32 %v51989, %v51994 (stack65)
        %v52001 = vadd.s32 %v51997, %v8 (stack65)
        %v52003 = vshll.u32 %v51994, 6 (stack73)
        %v52004 = vshrl.u32 %v51994, 26 (stack74)
        %v52005 = vor.u32 %v52003, %v52004 (stack75)
        %v52006 = vxor.u32 %v51997, %v52005 (stack76)
        %v52009 = vadd.s32 %v52006, %v10 (stack65)
        %v52013 = vadd.s32 %v52009, 5 (stack65)
        %v52015 = vxor.u32 %v52001, %v52013 (stack76)
        %v52016 = vand.u32.u8 %v52015, 255 (stack77)
        %v52017 = vand.u32 %v52016, 65535 (stack78)
        %v52018 = vshrl.u32 %v52017, 1 (stack79)
        %v52019 = vor.u32 %v52018, 16256 (stack75)
        %v52020 = vand.u32.u16 %v52019, 65535 (stack80)
        %v52021 = vunpack.i.l.bf16 %v52020 (stack81)
        %v52025 = vadd.f32 %v52021, -1.0 (stack82)
        %v52029 = vmul.f32 %v52025, 2.0 (stack83)
        %v52033 = vadd.f32 %v52029, -0.99609375 (stack82)
        %v52037 = vmax.f32 -0.99609375, %v52033 (stack84)
        %v52039 = vand.u32 2147483647, %v52037 (stack85)
        %vm52042 = vcmp.eq.f32.partialorder %v52039, 1.0 (stack86)
        %v52047 = vmul.f32 %v52037, inf (stack83)
        %v52049 = vxor.u32 %v52037, 2147483648 (stack87)
        %v52052 = vmul.f32 %v52037, %v52049 (stack83)
        %v52054 = vadd.f32 %v52052, 1.0 (stack88)
        %v52055 = vlog2.pop %v52054 (stack89)
        %v52056 = vmul.f32 %v52055, 0.6931472 (stack90)
        %v52057 = vmul.f32 -0.5, %v52052 (stack91)
        %v52058 = vadd.f32 %v52057, 1.0 (stack92)
        %v52059 = vmul.f32 %v52058, %v52052 (stack93)
        %v52060 = vand.u32 2147483647, %v52052 (stack94)
        %vm52061 = vcmp.lt.f32.partialorder %v52060, 0.0004427343 (stack95)
        %v52062 = vsel /*vm=*/%vm52061, /*on_true_vy=*/%v52059, /*on_false_vx=*/%v52056 (stack96)
        %v52063 = vxor.u32 %v52062, 2147483648 (stack87)
        %vm52066 = vcmp.lt.f32.partialorder %v52063, 5.0 (stack86)
        %v52071 = vsel /*vm=*/%vm52066, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v52075 = vsel /*vm=*/%vm52066, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v52079 = vsel /*vm=*/%vm52066, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v52083 = vsel /*vm=*/%vm52066, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v52087 = vsel /*vm=*/%vm52066, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v52091 = vsel /*vm=*/%vm52066, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v52095 = vsel /*vm=*/%vm52066, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v52099 = vsel /*vm=*/%vm52066, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v52103 = vsel /*vm=*/%vm52066, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v52107 = vadd.f32 %v52063, -2.5 (stack82)
        %v52109 = vrsqrt.pop %v52063 (stack97)
        %v52110 = vmul.f32 %v52063, %v52109 (stack98)
        %vm52111 = vcmp.eq.f32.partialorder %v52063, inf (stack99)
        %v52112 = vsel /*vm=*/%vm52111, /*on_true_vy=*/%v52063, /*on_false_vx=*/%v52110 (stack100)
        %vm52113 = vcmp.eq.f32.partialorder %v52063, 0.0 (stack101)
        %v52114 = vand.u32 %v52063, 2147483648 (stack102)
        %v52115 = vsel /*vm=*/%vm52113, /*on_true_vy=*/%v52114, /*on_false_vx=*/%v52112 (stack103)
        %v52118 = vadd.f32 %v52115, -3.0 (stack82)
        %v52122 = vsel /*vm=*/%vm52066, /*on_true_vy=*/%v52107, /*on_false_vx=*/%v52118 (stack72)
        %v52126 = vmul.f32 %v52103, %v52122 (stack83)
        %v52130 = vadd.f32 %v52099, %v52126 (stack82)
        %v52134 = vmul.f32 %v52130, %v52122 (stack83)
        %v52138 = vadd.f32 %v52095, %v52134 (stack82)
        %v52142 = vmul.f32 %v52138, %v52122 (stack83)
        %v52146 = vadd.f32 %v52091, %v52142 (stack82)
        %v52150 = vmul.f32 %v52146, %v52122 (stack83)
        %v52154 = vadd.f32 %v52087, %v52150 (stack82)
        %v52158 = vmul.f32 %v52154, %v52122 (stack83)
        %v52162 = vadd.f32 %v52083, %v52158 (stack82)
        %v52166 = vmul.f32 %v52162, %v52122 (stack83)
        %v52170 = vadd.f32 %v52079, %v52166 (stack82)
        %v52174 = vmul.f32 %v52170, %v52122 (stack83)
        %v52178 = vadd.f32 %v52075, %v52174 (stack82)
        %v52182 = vmul.f32 %v52178, %v52122 (stack83)
        %v52186 = vadd.f32 %v52071, %v52182 (stack82)
        %v52190 = vmul.f32 %v52186, %v52037 (stack83)
        %v52194 = vsel /*vm=*/%vm52042, /*on_true_vy=*/%v52047, /*on_false_vx=*/%v52190 (stack72)
        %v52198 = vmul.f32 %v52194, 1.4140625 (stack83)
        %s52200 = scalar_lea.vmem %s280, 820 [#allocation0] (stack107)
        %v52201 = vpack.c.bf16 0.0, %v52198 (stack104)
        %52202 = vst [vmem:[%s52200] sm:$0xf] /*vst_source=*/%v52201 (stack105)
        %v52205 = vadd.s32 %v3816, %v48975 (stack65)
        %s52207 = smul.u32 128, %s27 (stack66)
        %v52208 = vlaneseq (stack67)
        %v52209 = vand.u32 %v52208, 127 (stack68)
        %v52210 = vstv %s52207 (stack69)
        %v52211 = vadd.s32 %v52209, %v52210 (stack70)
        %v52215 = vadd.s32 %v52205, %v52211 (stack65)
        %vm52219 = vcmp.lt.u32.totalorder %v52215, %v52205 (stack71)
        %vm52224 = vcmp.lt.u32.totalorder %v52205, %v3816 (stack71)
        %v52229 = vadd.s32 %v3803, %v48958 (stack65)
        %v52233 = vadd.s32 %v52229, 1 (stack65)
        %v52237 = vsel /*vm=*/%vm52224, /*on_true_vy=*/%v52233, /*on_false_vx=*/%v52229 (stack72)
        %v52241 = vadd.s32 %v52237, 1 (stack65)
        %v52245 = vsel /*vm=*/%vm52219, /*on_true_vy=*/%v52241, /*on_false_vx=*/%v52237 (stack72)
        %v52250 = vadd.s32 %v52245, %v10 (stack65)
        %v52254 = vadd.s32 %v52215, %v9 (stack65)
        %v52258 = vadd.s32 %v52250, %v52254 (stack65)
        %v52260 = vshll.u32 %v52254, 13 (stack73)
        %v52261 = vshrl.u32 %v52254, 19 (stack74)
        %v52262 = vor.u32 %v52260, %v52261 (stack75)
        %v52263 = vxor.u32 %v52258, %v52262 (stack76)
        %v52266 = vadd.s32 %v52258, %v52263 (stack65)
        %v52268 = vshll.u32 %v52263, 15 (stack73)
        %v52269 = vshrl.u32 %v52263, 17 (stack74)
        %v52270 = vor.u32 %v52268, %v52269 (stack75)
        %v52271 = vxor.u32 %v52266, %v52270 (stack76)
        %v52274 = vadd.s32 %v52266, %v52271 (stack65)
        %v52276 = vshll.u32 %v52271, 26 (stack73)
        %v52277 = vshrl.u32 %v52271, 6 (stack74)
        %v52278 = vor.u32 %v52276, %v52277 (stack75)
        %v52279 = vxor.u32 %v52274, %v52278 (stack76)
        %v52282 = vadd.s32 %v52274, %v52279 (stack65)
        %v52286 = vadd.s32 %v52282, %v9 (stack65)
        %v52288 = vshll.u32 %v52279, 6 (stack73)
        %v52289 = vshrl.u32 %v52279, 26 (stack74)
        %v52290 = vor.u32 %v52288, %v52289 (stack75)
        %v52291 = vxor.u32 %v52282, %v52290 (stack76)
        %v52294 = vadd.s32 %v52291, %v8 (stack65)
        %v52298 = vadd.s32 %v52294, 1 (stack65)
        %v52302 = vadd.s32 %v52286, %v52298 (stack65)
        %v52304 = vshll.u32 %v52298, 17 (stack73)
        %v52305 = vshrl.u32 %v52298, 15 (stack74)
        %v52306 = vor.u32 %v52304, %v52305 (stack75)
        %v52307 = vxor.u32 %v52302, %v52306 (stack76)
        %v52310 = vadd.s32 %v52302, %v52307 (stack65)
        %v52312 = vshll.u32 %v52307, 29 (stack73)
        %v52313 = vshrl.u32 %v52307, 3 (stack74)
        %v52314 = vor.u32 %v52312, %v52313 (stack75)
        %v52315 = vxor.u32 %v52310, %v52314 (stack76)
        %v52318 = vadd.s32 %v52310, %v52315 (stack65)
        %v52320 = vshll.u32 %v52315, 16 (stack73)
        %v52321 = vshrl.u32 %v52315, 16 (stack74)
        %v52322 = vor.u32 %v52320, %v52321 (stack75)
        %v52323 = vxor.u32 %v52318, %v52322 (stack76)
        %v52326 = vadd.s32 %v52318, %v52323 (stack65)
        %v52330 = vadd.s32 %v52326, %v8 (stack65)
        %v52332 = vshll.u32 %v52323, 24 (stack73)
        %v52333 = vshrl.u32 %v52323, 8 (stack74)
        %v52334 = vor.u32 %v52332, %v52333 (stack75)
        %v52335 = vxor.u32 %v52326, %v52334 (stack76)
        %v52338 = vadd.s32 %v52335, %v10 (stack65)
        %v52342 = vadd.s32 %v52338, 2 (stack65)
        %v52346 = vadd.s32 %v52330, %v52342 (stack65)
        %v52348 = vshll.u32 %v52342, 13 (stack73)
        %v52349 = vshrl.u32 %v52342, 19 (stack74)
        %v52350 = vor.u32 %v52348, %v52349 (stack75)
        %v52351 = vxor.u32 %v52346, %v52350 (stack76)
        %v52354 = vadd.s32 %v52346, %v52351 (stack65)
        %v52356 = vshll.u32 %v52351, 15 (stack73)
        %v52357 = vshrl.u32 %v52351, 17 (stack74)
        %v52358 = vor.u32 %v52356, %v52357 (stack75)
        %v52359 = vxor.u32 %v52354, %v52358 (stack76)
        %v52362 = vadd.s32 %v52354, %v52359 (stack65)
        %v52364 = vshll.u32 %v52359, 26 (stack73)
        %v52365 = vshrl.u32 %v52359, 6 (stack74)
        %v52366 = vor.u32 %v52364, %v52365 (stack75)
        %v52367 = vxor.u32 %v52362, %v52366 (stack76)
        %v52370 = vadd.s32 %v52362, %v52367 (stack65)
        %v52374 = vadd.s32 %v52370, %v10 (stack65)
        %v52376 = vshll.u32 %v52367, 6 (stack73)
        %v52377 = vshrl.u32 %v52367, 26 (stack74)
        %v52378 = vor.u32 %v52376, %v52377 (stack75)
        %v52379 = vxor.u32 %v52370, %v52378 (stack76)
        %v52382 = vadd.s32 %v52379, %v9 (stack65)
        %v52386 = vadd.s32 %v52382, 3 (stack65)
        %v52390 = vadd.s32 %v52374, %v52386 (stack65)
        %v52392 = vshll.u32 %v52386, 17 (stack73)
        %v52393 = vshrl.u32 %v52386, 15 (stack74)
        %v52394 = vor.u32 %v52392, %v52393 (stack75)
        %v52395 = vxor.u32 %v52390, %v52394 (stack76)
        %v52398 = vadd.s32 %v52390, %v52395 (stack65)
        %v52400 = vshll.u32 %v52395, 29 (stack73)
        %v52401 = vshrl.u32 %v52395, 3 (stack74)
        %v52402 = vor.u32 %v52400, %v52401 (stack75)
        %v52403 = vxor.u32 %v52398, %v52402 (stack76)
        %v52406 = vadd.s32 %v52398, %v52403 (stack65)
        %v52408 = vshll.u32 %v52403, 16 (stack73)
        %v52409 = vshrl.u32 %v52403, 16 (stack74)
        %v52410 = vor.u32 %v52408, %v52409 (stack75)
        %v52411 = vxor.u32 %v52406, %v52410 (stack76)
        %v52414 = vadd.s32 %v52406, %v52411 (stack65)
        %v52418 = vadd.s32 %v52414, %v9 (stack65)
        %v52420 = vshll.u32 %v52411, 24 (stack73)
        %v52421 = vshrl.u32 %v52411, 8 (stack74)
        %v52422 = vor.u32 %v52420, %v52421 (stack75)
        %v52423 = vxor.u32 %v52414, %v52422 (stack76)
        %v52426 = vadd.s32 %v52423, %v8 (stack65)
        %v52430 = vadd.s32 %v52426, 4 (stack65)
        %v52434 = vadd.s32 %v52418, %v52430 (stack65)
        %v52436 = vshll.u32 %v52430, 13 (stack73)
        %v52437 = vshrl.u32 %v52430, 19 (stack74)
        %v52438 = vor.u32 %v52436, %v52437 (stack75)
        %v52439 = vxor.u32 %v52434, %v52438 (stack76)
        %v52442 = vadd.s32 %v52434, %v52439 (stack65)
        %v52444 = vshll.u32 %v52439, 15 (stack73)
        %v52445 = vshrl.u32 %v52439, 17 (stack74)
        %v52446 = vor.u32 %v52444, %v52445 (stack75)
        %v52447 = vxor.u32 %v52442, %v52446 (stack76)
        %v52450 = vadd.s32 %v52442, %v52447 (stack65)
        %v52452 = vshll.u32 %v52447, 26 (stack73)
        %v52453 = vshrl.u32 %v52447, 6 (stack74)
        %v52454 = vor.u32 %v52452, %v52453 (stack75)
        %v52455 = vxor.u32 %v52450, %v52454 (stack76)
        %v52458 = vadd.s32 %v52450, %v52455 (stack65)
        %v52462 = vadd.s32 %v52458, %v8 (stack65)
        %v52464 = vshll.u32 %v52455, 6 (stack73)
        %v52465 = vshrl.u32 %v52455, 26 (stack74)
        %v52466 = vor.u32 %v52464, %v52465 (stack75)
        %v52467 = vxor.u32 %v52458, %v52466 (stack76)
        %v52470 = vadd.s32 %v52467, %v10 (stack65)
        %v52474 = vadd.s32 %v52470, 5 (stack65)
        %v52476 = vxor.u32 %v52462, %v52474 (stack76)
        %v52477 = vand.u32.u8 %v52476, 255 (stack77)
        %v52478 = vand.u32 %v52477, 65535 (stack78)
        %v52479 = vshrl.u32 %v52478, 1 (stack79)
        %v52480 = vor.u32 %v52479, 16256 (stack75)
        %v52481 = vand.u32.u16 %v52480, 65535 (stack80)
        %v52482 = vunpack.i.l.bf16 %v52481 (stack81)
        %v52486 = vadd.f32 %v52482, -1.0 (stack82)
        %v52490 = vmul.f32 %v52486, 2.0 (stack83)
        %v52494 = vadd.f32 %v52490, -0.99609375 (stack82)
        %v52498 = vmax.f32 -0.99609375, %v52494 (stack84)
        %v52500 = vand.u32 2147483647, %v52498 (stack85)
        %vm52503 = vcmp.eq.f32.partialorder %v52500, 1.0 (stack86)
        %v52508 = vmul.f32 %v52498, inf (stack83)
        %v52510 = vxor.u32 %v52498, 2147483648 (stack87)
        %v52513 = vmul.f32 %v52498, %v52510 (stack83)
        %v52515 = vadd.f32 %v52513, 1.0 (stack88)
        %v52516 = vlog2.pop %v52515 (stack89)
        %v52517 = vmul.f32 %v52516, 0.6931472 (stack90)
        %v52518 = vmul.f32 -0.5, %v52513 (stack91)
        %v52519 = vadd.f32 %v52518, 1.0 (stack92)
        %v52520 = vmul.f32 %v52519, %v52513 (stack93)
        %v52521 = vand.u32 2147483647, %v52513 (stack94)
        %vm52522 = vcmp.lt.f32.partialorder %v52521, 0.0004427343 (stack95)
        %v52523 = vsel /*vm=*/%vm52522, /*on_true_vy=*/%v52520, /*on_false_vx=*/%v52517 (stack96)
        %v52524 = vxor.u32 %v52523, 2147483648 (stack87)
        %vm52527 = vcmp.lt.f32.partialorder %v52524, 5.0 (stack86)
        %v52532 = vsel /*vm=*/%vm52527, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v52536 = vsel /*vm=*/%vm52527, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v52540 = vsel /*vm=*/%vm52527, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v52544 = vsel /*vm=*/%vm52527, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v52548 = vsel /*vm=*/%vm52527, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v52552 = vsel /*vm=*/%vm52527, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v52556 = vsel /*vm=*/%vm52527, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v52560 = vsel /*vm=*/%vm52527, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v52564 = vsel /*vm=*/%vm52527, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v52568 = vadd.f32 %v52524, -2.5 (stack82)
        %v52570 = vrsqrt.pop %v52524 (stack97)
        %v52571 = vmul.f32 %v52524, %v52570 (stack98)
        %vm52572 = vcmp.eq.f32.partialorder %v52524, inf (stack99)
        %v52573 = vsel /*vm=*/%vm52572, /*on_true_vy=*/%v52524, /*on_false_vx=*/%v52571 (stack100)
        %vm52574 = vcmp.eq.f32.partialorder %v52524, 0.0 (stack101)
        %v52575 = vand.u32 %v52524, 2147483648 (stack102)
        %v52576 = vsel /*vm=*/%vm52574, /*on_true_vy=*/%v52575, /*on_false_vx=*/%v52573 (stack103)
        %v52579 = vadd.f32 %v52576, -3.0 (stack82)
        %v52583 = vsel /*vm=*/%vm52527, /*on_true_vy=*/%v52568, /*on_false_vx=*/%v52579 (stack72)
        %v52587 = vmul.f32 %v52564, %v52583 (stack83)
        %v52591 = vadd.f32 %v52560, %v52587 (stack82)
        %v52595 = vmul.f32 %v52591, %v52583 (stack83)
        %v52599 = vadd.f32 %v52556, %v52595 (stack82)
        %v52603 = vmul.f32 %v52599, %v52583 (stack83)
        %v52607 = vadd.f32 %v52552, %v52603 (stack82)
        %v52611 = vmul.f32 %v52607, %v52583 (stack83)
        %v52615 = vadd.f32 %v52548, %v52611 (stack82)
        %v52619 = vmul.f32 %v52615, %v52583 (stack83)
        %v52623 = vadd.f32 %v52544, %v52619 (stack82)
        %v52627 = vmul.f32 %v52623, %v52583 (stack83)
        %v52631 = vadd.f32 %v52540, %v52627 (stack82)
        %v52635 = vmul.f32 %v52631, %v52583 (stack83)
        %v52639 = vadd.f32 %v52536, %v52635 (stack82)
        %v52643 = vmul.f32 %v52639, %v52583 (stack83)
        %v52647 = vadd.f32 %v52532, %v52643 (stack82)
        %v52651 = vmul.f32 %v52647, %v52498 (stack83)
        %v52655 = vsel /*vm=*/%vm52503, /*on_true_vy=*/%v52508, /*on_false_vx=*/%v52651 (stack72)
        %v52659 = vmul.f32 %v52655, 1.4140625 (stack83)
        %s52661 = scalar_lea.vmem %s280, 948 [#allocation0] (stack107)
        %v52662 = vpack.c.bf16 0.0, %v52659 (stack104)
        %52663 = vst [vmem:[%s52661] sm:$0xf] /*vst_source=*/%v52662 (stack105)
        %s52664 = sadd.s32 %s339, 112 (stack106)
        %s52665 = sshrl.u32 %s52664, 10 (stack49)
        %p52666 = scmp.lt.s32.totalorder 1, %s52665 (stack50)
        %s52667 = scalar_select /*predicate=*/%p52666, /*on_true=*/1, /*on_false=*/%s52665 (stack51)
        %s52668 = sand.u32 %s52664, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s52669 = sshrl.u32 %s52668, 7 (stack53)
        %s52670 = sand.u32 %s52668, 127 /* smod.u32 w/div 128 */ (stack54)
        %s52671 = smul.addr %s52667, 8 (stack55)
        %s52672 = scalar_lea.vmem %s3, %s52671 (stack56)
        %s52674 = scalar_lea.vmem %s52672, %s52669 (stack57)
        %v52675 = vld [vmem:[%s52674] ss:$0 sm:$0xff] (stack58)
        %s52676 = sand.u32 %s52670, 255 (stack59)
        %s52678 = sor.u32 256, %s52676 (stack60)
        %52679 = vbcast.lane.b32.xlu0 %v52675, %s52678 (stack61)
        %v52680 = vpop.permute.xlu0 %52679 (stack62)
        %s52681 = sadd.s32 %s347, 112 (stack106)
        %s52682 = sshrl.u32 %s52681, 10 (stack49)
        %p52683 = scmp.lt.s32.totalorder 1, %s52682 (stack50)
        %s52684 = scalar_select /*predicate=*/%p52683, /*on_true=*/1, /*on_false=*/%s52682 (stack51)
        %s52685 = sand.u32 %s52681, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s52686 = sshrl.u32 %s52685, 7 (stack53)
        %s52687 = sand.u32 %s52685, 127 /* smod.u32 w/div 128 */ (stack54)
        %s52688 = smul.addr %s52684, 8 (stack55)
        %s52689 = scalar_lea.vmem %s5, %s52688 (stack56)
        %s52691 = scalar_lea.vmem %s52689, %s52686 (stack57)
        %v52692 = vld [vmem:[%s52691] ss:$0 sm:$0xff] (stack58)
        %s52693 = sand.u32 %s52687, 255 (stack59)
        %s52695 = sor.u32 256, %s52693 (stack60)
        %52696 = vbcast.lane.b32.xlu0 %v52692, %s52695 (stack61)
        %v52697 = vpop.permute.xlu0 %52696 (stack62)
        %v52700 = vadd.s32 %v408, %v52697 (stack65)
        %s52702 = smul.u32 128, %s27 (stack66)
        %v52703 = vlaneseq (stack67)
        %v52704 = vand.u32 %v52703, 127 (stack68)
        %v52705 = vstv %s52702 (stack69)
        %v52706 = vadd.s32 %v52704, %v52705 (stack70)
        %v52710 = vadd.s32 %v52700, %v52706 (stack65)
        %vm52714 = vcmp.lt.u32.totalorder %v52710, %v52700 (stack71)
        %vm52719 = vcmp.lt.u32.totalorder %v52700, %v408 (stack71)
        %v52724 = vadd.s32 %v380, %v52680 (stack65)
        %v52728 = vadd.s32 %v52724, 1 (stack65)
        %v52732 = vsel /*vm=*/%vm52719, /*on_true_vy=*/%v52728, /*on_false_vx=*/%v52724 (stack72)
        %v52736 = vadd.s32 %v52732, 1 (stack65)
        %v52740 = vsel /*vm=*/%vm52714, /*on_true_vy=*/%v52736, /*on_false_vx=*/%v52732 (stack72)
        %v52745 = vadd.s32 %v52740, %v10 (stack65)
        %v52749 = vadd.s32 %v52710, %v9 (stack65)
        %v52753 = vadd.s32 %v52745, %v52749 (stack65)
        %v52755 = vshll.u32 %v52749, 13 (stack73)
        %v52756 = vshrl.u32 %v52749, 19 (stack74)
        %v52757 = vor.u32 %v52755, %v52756 (stack75)
        %v52758 = vxor.u32 %v52753, %v52757 (stack76)
        %v52761 = vadd.s32 %v52753, %v52758 (stack65)
        %v52763 = vshll.u32 %v52758, 15 (stack73)
        %v52764 = vshrl.u32 %v52758, 17 (stack74)
        %v52765 = vor.u32 %v52763, %v52764 (stack75)
        %v52766 = vxor.u32 %v52761, %v52765 (stack76)
        %v52769 = vadd.s32 %v52761, %v52766 (stack65)
        %v52771 = vshll.u32 %v52766, 26 (stack73)
        %v52772 = vshrl.u32 %v52766, 6 (stack74)
        %v52773 = vor.u32 %v52771, %v52772 (stack75)
        %v52774 = vxor.u32 %v52769, %v52773 (stack76)
        %v52777 = vadd.s32 %v52769, %v52774 (stack65)
        %v52781 = vadd.s32 %v52777, %v9 (stack65)
        %v52783 = vshll.u32 %v52774, 6 (stack73)
        %v52784 = vshrl.u32 %v52774, 26 (stack74)
        %v52785 = vor.u32 %v52783, %v52784 (stack75)
        %v52786 = vxor.u32 %v52777, %v52785 (stack76)
        %v52789 = vadd.s32 %v52786, %v8 (stack65)
        %v52793 = vadd.s32 %v52789, 1 (stack65)
        %v52797 = vadd.s32 %v52781, %v52793 (stack65)
        %v52799 = vshll.u32 %v52793, 17 (stack73)
        %v52800 = vshrl.u32 %v52793, 15 (stack74)
        %v52801 = vor.u32 %v52799, %v52800 (stack75)
        %v52802 = vxor.u32 %v52797, %v52801 (stack76)
        %v52805 = vadd.s32 %v52797, %v52802 (stack65)
        %v52807 = vshll.u32 %v52802, 29 (stack73)
        %v52808 = vshrl.u32 %v52802, 3 (stack74)
        %v52809 = vor.u32 %v52807, %v52808 (stack75)
        %v52810 = vxor.u32 %v52805, %v52809 (stack76)
        %v52813 = vadd.s32 %v52805, %v52810 (stack65)
        %v52815 = vshll.u32 %v52810, 16 (stack73)
        %v52816 = vshrl.u32 %v52810, 16 (stack74)
        %v52817 = vor.u32 %v52815, %v52816 (stack75)
        %v52818 = vxor.u32 %v52813, %v52817 (stack76)
        %v52821 = vadd.s32 %v52813, %v52818 (stack65)
        %v52825 = vadd.s32 %v52821, %v8 (stack65)
        %v52827 = vshll.u32 %v52818, 24 (stack73)
        %v52828 = vshrl.u32 %v52818, 8 (stack74)
        %v52829 = vor.u32 %v52827, %v52828 (stack75)
        %v52830 = vxor.u32 %v52821, %v52829 (stack76)
        %v52833 = vadd.s32 %v52830, %v10 (stack65)
        %v52837 = vadd.s32 %v52833, 2 (stack65)
        %v52841 = vadd.s32 %v52825, %v52837 (stack65)
        %v52843 = vshll.u32 %v52837, 13 (stack73)
        %v52844 = vshrl.u32 %v52837, 19 (stack74)
        %v52845 = vor.u32 %v52843, %v52844 (stack75)
        %v52846 = vxor.u32 %v52841, %v52845 (stack76)
        %v52849 = vadd.s32 %v52841, %v52846 (stack65)
        %v52851 = vshll.u32 %v52846, 15 (stack73)
        %v52852 = vshrl.u32 %v52846, 17 (stack74)
        %v52853 = vor.u32 %v52851, %v52852 (stack75)
        %v52854 = vxor.u32 %v52849, %v52853 (stack76)
        %v52857 = vadd.s32 %v52849, %v52854 (stack65)
        %v52859 = vshll.u32 %v52854, 26 (stack73)
        %v52860 = vshrl.u32 %v52854, 6 (stack74)
        %v52861 = vor.u32 %v52859, %v52860 (stack75)
        %v52862 = vxor.u32 %v52857, %v52861 (stack76)
        %v52865 = vadd.s32 %v52857, %v52862 (stack65)
        %v52869 = vadd.s32 %v52865, %v10 (stack65)
        %v52871 = vshll.u32 %v52862, 6 (stack73)
        %v52872 = vshrl.u32 %v52862, 26 (stack74)
        %v52873 = vor.u32 %v52871, %v52872 (stack75)
        %v52874 = vxor.u32 %v52865, %v52873 (stack76)
        %v52877 = vadd.s32 %v52874, %v9 (stack65)
        %v52881 = vadd.s32 %v52877, 3 (stack65)
        %v52885 = vadd.s32 %v52869, %v52881 (stack65)
        %v52887 = vshll.u32 %v52881, 17 (stack73)
        %v52888 = vshrl.u32 %v52881, 15 (stack74)
        %v52889 = vor.u32 %v52887, %v52888 (stack75)
        %v52890 = vxor.u32 %v52885, %v52889 (stack76)
        %v52893 = vadd.s32 %v52885, %v52890 (stack65)
        %v52895 = vshll.u32 %v52890, 29 (stack73)
        %v52896 = vshrl.u32 %v52890, 3 (stack74)
        %v52897 = vor.u32 %v52895, %v52896 (stack75)
        %v52898 = vxor.u32 %v52893, %v52897 (stack76)
        %v52901 = vadd.s32 %v52893, %v52898 (stack65)
        %v52903 = vshll.u32 %v52898, 16 (stack73)
        %v52904 = vshrl.u32 %v52898, 16 (stack74)
        %v52905 = vor.u32 %v52903, %v52904 (stack75)
        %v52906 = vxor.u32 %v52901, %v52905 (stack76)
        %v52909 = vadd.s32 %v52901, %v52906 (stack65)
        %v52913 = vadd.s32 %v52909, %v9 (stack65)
        %v52915 = vshll.u32 %v52906, 24 (stack73)
        %v52916 = vshrl.u32 %v52906, 8 (stack74)
        %v52917 = vor.u32 %v52915, %v52916 (stack75)
        %v52918 = vxor.u32 %v52909, %v52917 (stack76)
        %v52921 = vadd.s32 %v52918, %v8 (stack65)
        %v52925 = vadd.s32 %v52921, 4 (stack65)
        %v52929 = vadd.s32 %v52913, %v52925 (stack65)
        %v52931 = vshll.u32 %v52925, 13 (stack73)
        %v52932 = vshrl.u32 %v52925, 19 (stack74)
        %v52933 = vor.u32 %v52931, %v52932 (stack75)
        %v52934 = vxor.u32 %v52929, %v52933 (stack76)
        %v52937 = vadd.s32 %v52929, %v52934 (stack65)
        %v52939 = vshll.u32 %v52934, 15 (stack73)
        %v52940 = vshrl.u32 %v52934, 17 (stack74)
        %v52941 = vor.u32 %v52939, %v52940 (stack75)
        %v52942 = vxor.u32 %v52937, %v52941 (stack76)
        %v52945 = vadd.s32 %v52937, %v52942 (stack65)
        %v52947 = vshll.u32 %v52942, 26 (stack73)
        %v52948 = vshrl.u32 %v52942, 6 (stack74)
        %v52949 = vor.u32 %v52947, %v52948 (stack75)
        %v52950 = vxor.u32 %v52945, %v52949 (stack76)
        %v52953 = vadd.s32 %v52945, %v52950 (stack65)
        %v52957 = vadd.s32 %v52953, %v8 (stack65)
        %v52959 = vshll.u32 %v52950, 6 (stack73)
        %v52960 = vshrl.u32 %v52950, 26 (stack74)
        %v52961 = vor.u32 %v52959, %v52960 (stack75)
        %v52962 = vxor.u32 %v52953, %v52961 (stack76)
        %v52965 = vadd.s32 %v52962, %v10 (stack65)
        %v52969 = vadd.s32 %v52965, 5 (stack65)
        %v52971 = vxor.u32 %v52957, %v52969 (stack76)
        %v52972 = vand.u32.u8 %v52971, 255 (stack77)
        %v52973 = vand.u32 %v52972, 65535 (stack78)
        %v52974 = vshrl.u32 %v52973, 1 (stack79)
        %v52975 = vor.u32 %v52974, 16256 (stack75)
        %v52976 = vand.u32.u16 %v52975, 65535 (stack80)
        %v52977 = vunpack.i.l.bf16 %v52976 (stack81)
        %v52981 = vadd.f32 %v52977, -1.0 (stack82)
        %v52985 = vmul.f32 %v52981, 2.0 (stack83)
        %v52989 = vadd.f32 %v52985, -0.99609375 (stack82)
        %v52993 = vmax.f32 -0.99609375, %v52989 (stack84)
        %v52995 = vand.u32 2147483647, %v52993 (stack85)
        %vm52998 = vcmp.eq.f32.partialorder %v52995, 1.0 (stack86)
        %v53003 = vmul.f32 %v52993, inf (stack83)
        %v53005 = vxor.u32 %v52993, 2147483648 (stack87)
        %v53008 = vmul.f32 %v52993, %v53005 (stack83)
        %v53010 = vadd.f32 %v53008, 1.0 (stack88)
        %v53011 = vlog2.pop %v53010 (stack89)
        %v53012 = vmul.f32 %v53011, 0.6931472 (stack90)
        %v53013 = vmul.f32 -0.5, %v53008 (stack91)
        %v53014 = vadd.f32 %v53013, 1.0 (stack92)
        %v53015 = vmul.f32 %v53014, %v53008 (stack93)
        %v53016 = vand.u32 2147483647, %v53008 (stack94)
        %vm53017 = vcmp.lt.f32.partialorder %v53016, 0.0004427343 (stack95)
        %v53018 = vsel /*vm=*/%vm53017, /*on_true_vy=*/%v53015, /*on_false_vx=*/%v53012 (stack96)
        %v53019 = vxor.u32 %v53018, 2147483648 (stack87)
        %vm53022 = vcmp.lt.f32.partialorder %v53019, 5.0 (stack86)
        %v53027 = vsel /*vm=*/%vm53022, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v53031 = vsel /*vm=*/%vm53022, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v53035 = vsel /*vm=*/%vm53022, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v53039 = vsel /*vm=*/%vm53022, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v53043 = vsel /*vm=*/%vm53022, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v53047 = vsel /*vm=*/%vm53022, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v53051 = vsel /*vm=*/%vm53022, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v53055 = vsel /*vm=*/%vm53022, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v53059 = vsel /*vm=*/%vm53022, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v53063 = vadd.f32 %v53019, -2.5 (stack82)
        %v53065 = vrsqrt.pop %v53019 (stack97)
        %v53066 = vmul.f32 %v53019, %v53065 (stack98)
        %vm53067 = vcmp.eq.f32.partialorder %v53019, inf (stack99)
        %v53068 = vsel /*vm=*/%vm53067, /*on_true_vy=*/%v53019, /*on_false_vx=*/%v53066 (stack100)
        %vm53069 = vcmp.eq.f32.partialorder %v53019, 0.0 (stack101)
        %v53070 = vand.u32 %v53019, 2147483648 (stack102)
        %v53071 = vsel /*vm=*/%vm53069, /*on_true_vy=*/%v53070, /*on_false_vx=*/%v53068 (stack103)
        %v53074 = vadd.f32 %v53071, -3.0 (stack82)
        %v53078 = vsel /*vm=*/%vm53022, /*on_true_vy=*/%v53063, /*on_false_vx=*/%v53074 (stack72)
        %v53082 = vmul.f32 %v53059, %v53078 (stack83)
        %v53086 = vadd.f32 %v53055, %v53082 (stack82)
        %v53090 = vmul.f32 %v53086, %v53078 (stack83)
        %v53094 = vadd.f32 %v53051, %v53090 (stack82)
        %v53098 = vmul.f32 %v53094, %v53078 (stack83)
        %v53102 = vadd.f32 %v53047, %v53098 (stack82)
        %v53106 = vmul.f32 %v53102, %v53078 (stack83)
        %v53110 = vadd.f32 %v53043, %v53106 (stack82)
        %v53114 = vmul.f32 %v53110, %v53078 (stack83)
        %v53118 = vadd.f32 %v53039, %v53114 (stack82)
        %v53122 = vmul.f32 %v53118, %v53078 (stack83)
        %v53126 = vadd.f32 %v53035, %v53122 (stack82)
        %v53130 = vmul.f32 %v53126, %v53078 (stack83)
        %v53134 = vadd.f32 %v53031, %v53130 (stack82)
        %v53138 = vmul.f32 %v53134, %v53078 (stack83)
        %v53142 = vadd.f32 %v53027, %v53138 (stack82)
        %v53146 = vmul.f32 %v53142, %v52993 (stack83)
        %v53150 = vsel /*vm=*/%vm52998, /*on_true_vy=*/%v53003, /*on_false_vx=*/%v53146 (stack72)
        %v53154 = vmul.f32 %v53150, 1.4140625 (stack83)
        %s53156 = scalar_lea.vmem %s280, 56 [#allocation0] (stack107)
        %v53157 = vpack.c.bf16 0.0, %v53154 (stack104)
        %53158 = vst [vmem:[%s53156] sm:$0xf] /*vst_source=*/%v53157 (stack105)
        %v53161 = vadd.s32 %v894, %v52697 (stack65)
        %s53163 = smul.u32 128, %s27 (stack66)
        %v53164 = vlaneseq (stack67)
        %v53165 = vand.u32 %v53164, 127 (stack68)
        %v53166 = vstv %s53163 (stack69)
        %v53167 = vadd.s32 %v53165, %v53166 (stack70)
        %v53171 = vadd.s32 %v53161, %v53167 (stack65)
        %vm53175 = vcmp.lt.u32.totalorder %v53171, %v53161 (stack71)
        %vm53180 = vcmp.lt.u32.totalorder %v53161, %v894 (stack71)
        %v53185 = vadd.s32 %v881, %v52680 (stack65)
        %v53189 = vadd.s32 %v53185, 1 (stack65)
        %v53193 = vsel /*vm=*/%vm53180, /*on_true_vy=*/%v53189, /*on_false_vx=*/%v53185 (stack72)
        %v53197 = vadd.s32 %v53193, 1 (stack65)
        %v53201 = vsel /*vm=*/%vm53175, /*on_true_vy=*/%v53197, /*on_false_vx=*/%v53193 (stack72)
        %v53206 = vadd.s32 %v53201, %v10 (stack65)
        %v53210 = vadd.s32 %v53171, %v9 (stack65)
        %v53214 = vadd.s32 %v53206, %v53210 (stack65)
        %v53216 = vshll.u32 %v53210, 13 (stack73)
        %v53217 = vshrl.u32 %v53210, 19 (stack74)
        %v53218 = vor.u32 %v53216, %v53217 (stack75)
        %v53219 = vxor.u32 %v53214, %v53218 (stack76)
        %v53222 = vadd.s32 %v53214, %v53219 (stack65)
        %v53224 = vshll.u32 %v53219, 15 (stack73)
        %v53225 = vshrl.u32 %v53219, 17 (stack74)
        %v53226 = vor.u32 %v53224, %v53225 (stack75)
        %v53227 = vxor.u32 %v53222, %v53226 (stack76)
        %v53230 = vadd.s32 %v53222, %v53227 (stack65)
        %v53232 = vshll.u32 %v53227, 26 (stack73)
        %v53233 = vshrl.u32 %v53227, 6 (stack74)
        %v53234 = vor.u32 %v53232, %v53233 (stack75)
        %v53235 = vxor.u32 %v53230, %v53234 (stack76)
        %v53238 = vadd.s32 %v53230, %v53235 (stack65)
        %v53242 = vadd.s32 %v53238, %v9 (stack65)
        %v53244 = vshll.u32 %v53235, 6 (stack73)
        %v53245 = vshrl.u32 %v53235, 26 (stack74)
        %v53246 = vor.u32 %v53244, %v53245 (stack75)
        %v53247 = vxor.u32 %v53238, %v53246 (stack76)
        %v53250 = vadd.s32 %v53247, %v8 (stack65)
        %v53254 = vadd.s32 %v53250, 1 (stack65)
        %v53258 = vadd.s32 %v53242, %v53254 (stack65)
        %v53260 = vshll.u32 %v53254, 17 (stack73)
        %v53261 = vshrl.u32 %v53254, 15 (stack74)
        %v53262 = vor.u32 %v53260, %v53261 (stack75)
        %v53263 = vxor.u32 %v53258, %v53262 (stack76)
        %v53266 = vadd.s32 %v53258, %v53263 (stack65)
        %v53268 = vshll.u32 %v53263, 29 (stack73)
        %v53269 = vshrl.u32 %v53263, 3 (stack74)
        %v53270 = vor.u32 %v53268, %v53269 (stack75)
        %v53271 = vxor.u32 %v53266, %v53270 (stack76)
        %v53274 = vadd.s32 %v53266, %v53271 (stack65)
        %v53276 = vshll.u32 %v53271, 16 (stack73)
        %v53277 = vshrl.u32 %v53271, 16 (stack74)
        %v53278 = vor.u32 %v53276, %v53277 (stack75)
        %v53279 = vxor.u32 %v53274, %v53278 (stack76)
        %v53282 = vadd.s32 %v53274, %v53279 (stack65)
        %v53286 = vadd.s32 %v53282, %v8 (stack65)
        %v53288 = vshll.u32 %v53279, 24 (stack73)
        %v53289 = vshrl.u32 %v53279, 8 (stack74)
        %v53290 = vor.u32 %v53288, %v53289 (stack75)
        %v53291 = vxor.u32 %v53282, %v53290 (stack76)
        %v53294 = vadd.s32 %v53291, %v10 (stack65)
        %v53298 = vadd.s32 %v53294, 2 (stack65)
        %v53302 = vadd.s32 %v53286, %v53298 (stack65)
        %v53304 = vshll.u32 %v53298, 13 (stack73)
        %v53305 = vshrl.u32 %v53298, 19 (stack74)
        %v53306 = vor.u32 %v53304, %v53305 (stack75)
        %v53307 = vxor.u32 %v53302, %v53306 (stack76)
        %v53310 = vadd.s32 %v53302, %v53307 (stack65)
        %v53312 = vshll.u32 %v53307, 15 (stack73)
        %v53313 = vshrl.u32 %v53307, 17 (stack74)
        %v53314 = vor.u32 %v53312, %v53313 (stack75)
        %v53315 = vxor.u32 %v53310, %v53314 (stack76)
        %v53318 = vadd.s32 %v53310, %v53315 (stack65)
        %v53320 = vshll.u32 %v53315, 26 (stack73)
        %v53321 = vshrl.u32 %v53315, 6 (stack74)
        %v53322 = vor.u32 %v53320, %v53321 (stack75)
        %v53323 = vxor.u32 %v53318, %v53322 (stack76)
        %v53326 = vadd.s32 %v53318, %v53323 (stack65)
        %v53330 = vadd.s32 %v53326, %v10 (stack65)
        %v53332 = vshll.u32 %v53323, 6 (stack73)
        %v53333 = vshrl.u32 %v53323, 26 (stack74)
        %v53334 = vor.u32 %v53332, %v53333 (stack75)
        %v53335 = vxor.u32 %v53326, %v53334 (stack76)
        %v53338 = vadd.s32 %v53335, %v9 (stack65)
        %v53342 = vadd.s32 %v53338, 3 (stack65)
        %v53346 = vadd.s32 %v53330, %v53342 (stack65)
        %v53348 = vshll.u32 %v53342, 17 (stack73)
        %v53349 = vshrl.u32 %v53342, 15 (stack74)
        %v53350 = vor.u32 %v53348, %v53349 (stack75)
        %v53351 = vxor.u32 %v53346, %v53350 (stack76)
        %v53354 = vadd.s32 %v53346, %v53351 (stack65)
        %v53356 = vshll.u32 %v53351, 29 (stack73)
        %v53357 = vshrl.u32 %v53351, 3 (stack74)
        %v53358 = vor.u32 %v53356, %v53357 (stack75)
        %v53359 = vxor.u32 %v53354, %v53358 (stack76)
        %v53362 = vadd.s32 %v53354, %v53359 (stack65)
        %v53364 = vshll.u32 %v53359, 16 (stack73)
        %v53365 = vshrl.u32 %v53359, 16 (stack74)
        %v53366 = vor.u32 %v53364, %v53365 (stack75)
        %v53367 = vxor.u32 %v53362, %v53366 (stack76)
        %v53370 = vadd.s32 %v53362, %v53367 (stack65)
        %v53374 = vadd.s32 %v53370, %v9 (stack65)
        %v53376 = vshll.u32 %v53367, 24 (stack73)
        %v53377 = vshrl.u32 %v53367, 8 (stack74)
        %v53378 = vor.u32 %v53376, %v53377 (stack75)
        %v53379 = vxor.u32 %v53370, %v53378 (stack76)
        %v53382 = vadd.s32 %v53379, %v8 (stack65)
        %v53386 = vadd.s32 %v53382, 4 (stack65)
        %v53390 = vadd.s32 %v53374, %v53386 (stack65)
        %v53392 = vshll.u32 %v53386, 13 (stack73)
        %v53393 = vshrl.u32 %v53386, 19 (stack74)
        %v53394 = vor.u32 %v53392, %v53393 (stack75)
        %v53395 = vxor.u32 %v53390, %v53394 (stack76)
        %v53398 = vadd.s32 %v53390, %v53395 (stack65)
        %v53400 = vshll.u32 %v53395, 15 (stack73)
        %v53401 = vshrl.u32 %v53395, 17 (stack74)
        %v53402 = vor.u32 %v53400, %v53401 (stack75)
        %v53403 = vxor.u32 %v53398, %v53402 (stack76)
        %v53406 = vadd.s32 %v53398, %v53403 (stack65)
        %v53408 = vshll.u32 %v53403, 26 (stack73)
        %v53409 = vshrl.u32 %v53403, 6 (stack74)
        %v53410 = vor.u32 %v53408, %v53409 (stack75)
        %v53411 = vxor.u32 %v53406, %v53410 (stack76)
        %v53414 = vadd.s32 %v53406, %v53411 (stack65)
        %v53418 = vadd.s32 %v53414, %v8 (stack65)
        %v53420 = vshll.u32 %v53411, 6 (stack73)
        %v53421 = vshrl.u32 %v53411, 26 (stack74)
        %v53422 = vor.u32 %v53420, %v53421 (stack75)
        %v53423 = vxor.u32 %v53414, %v53422 (stack76)
        %v53426 = vadd.s32 %v53423, %v10 (stack65)
        %v53430 = vadd.s32 %v53426, 5 (stack65)
        %v53432 = vxor.u32 %v53418, %v53430 (stack76)
        %v53433 = vand.u32.u8 %v53432, 255 (stack77)
        %v53434 = vand.u32 %v53433, 65535 (stack78)
        %v53435 = vshrl.u32 %v53434, 1 (stack79)
        %v53436 = vor.u32 %v53435, 16256 (stack75)
        %v53437 = vand.u32.u16 %v53436, 65535 (stack80)
        %v53438 = vunpack.i.l.bf16 %v53437 (stack81)
        %v53442 = vadd.f32 %v53438, -1.0 (stack82)
        %v53446 = vmul.f32 %v53442, 2.0 (stack83)
        %v53450 = vadd.f32 %v53446, -0.99609375 (stack82)
        %v53454 = vmax.f32 -0.99609375, %v53450 (stack84)
        %v53456 = vand.u32 2147483647, %v53454 (stack85)
        %vm53459 = vcmp.eq.f32.partialorder %v53456, 1.0 (stack86)
        %v53464 = vmul.f32 %v53454, inf (stack83)
        %v53466 = vxor.u32 %v53454, 2147483648 (stack87)
        %v53469 = vmul.f32 %v53454, %v53466 (stack83)
        %v53471 = vadd.f32 %v53469, 1.0 (stack88)
        %v53472 = vlog2.pop %v53471 (stack89)
        %v53473 = vmul.f32 %v53472, 0.6931472 (stack90)
        %v53474 = vmul.f32 -0.5, %v53469 (stack91)
        %v53475 = vadd.f32 %v53474, 1.0 (stack92)
        %v53476 = vmul.f32 %v53475, %v53469 (stack93)
        %v53477 = vand.u32 2147483647, %v53469 (stack94)
        %vm53478 = vcmp.lt.f32.partialorder %v53477, 0.0004427343 (stack95)
        %v53479 = vsel /*vm=*/%vm53478, /*on_true_vy=*/%v53476, /*on_false_vx=*/%v53473 (stack96)
        %v53480 = vxor.u32 %v53479, 2147483648 (stack87)
        %vm53483 = vcmp.lt.f32.partialorder %v53480, 5.0 (stack86)
        %v53488 = vsel /*vm=*/%vm53483, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v53492 = vsel /*vm=*/%vm53483, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v53496 = vsel /*vm=*/%vm53483, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v53500 = vsel /*vm=*/%vm53483, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v53504 = vsel /*vm=*/%vm53483, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v53508 = vsel /*vm=*/%vm53483, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v53512 = vsel /*vm=*/%vm53483, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v53516 = vsel /*vm=*/%vm53483, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v53520 = vsel /*vm=*/%vm53483, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v53524 = vadd.f32 %v53480, -2.5 (stack82)
        %v53526 = vrsqrt.pop %v53480 (stack97)
        %v53527 = vmul.f32 %v53480, %v53526 (stack98)
        %vm53528 = vcmp.eq.f32.partialorder %v53480, inf (stack99)
        %v53529 = vsel /*vm=*/%vm53528, /*on_true_vy=*/%v53480, /*on_false_vx=*/%v53527 (stack100)
        %vm53530 = vcmp.eq.f32.partialorder %v53480, 0.0 (stack101)
        %v53531 = vand.u32 %v53480, 2147483648 (stack102)
        %v53532 = vsel /*vm=*/%vm53530, /*on_true_vy=*/%v53531, /*on_false_vx=*/%v53529 (stack103)
        %v53535 = vadd.f32 %v53532, -3.0 (stack82)
        %v53539 = vsel /*vm=*/%vm53483, /*on_true_vy=*/%v53524, /*on_false_vx=*/%v53535 (stack72)
        %v53543 = vmul.f32 %v53520, %v53539 (stack83)
        %v53547 = vadd.f32 %v53516, %v53543 (stack82)
        %v53551 = vmul.f32 %v53547, %v53539 (stack83)
        %v53555 = vadd.f32 %v53512, %v53551 (stack82)
        %v53559 = vmul.f32 %v53555, %v53539 (stack83)
        %v53563 = vadd.f32 %v53508, %v53559 (stack82)
        %v53567 = vmul.f32 %v53563, %v53539 (stack83)
        %v53571 = vadd.f32 %v53504, %v53567 (stack82)
        %v53575 = vmul.f32 %v53571, %v53539 (stack83)
        %v53579 = vadd.f32 %v53500, %v53575 (stack82)
        %v53583 = vmul.f32 %v53579, %v53539 (stack83)
        %v53587 = vadd.f32 %v53496, %v53583 (stack82)
        %v53591 = vmul.f32 %v53587, %v53539 (stack83)
        %v53595 = vadd.f32 %v53492, %v53591 (stack82)
        %v53599 = vmul.f32 %v53595, %v53539 (stack83)
        %v53603 = vadd.f32 %v53488, %v53599 (stack82)
        %v53607 = vmul.f32 %v53603, %v53454 (stack83)
        %v53611 = vsel /*vm=*/%vm53459, /*on_true_vy=*/%v53464, /*on_false_vx=*/%v53607 (stack72)
        %v53615 = vmul.f32 %v53611, 1.4140625 (stack83)
        %s53617 = scalar_lea.vmem %s280, 184 [#allocation0] (stack107)
        %v53618 = vpack.c.bf16 0.0, %v53615 (stack104)
        %53619 = vst [vmem:[%s53617] sm:$0xf] /*vst_source=*/%v53618 (stack105)
        %v53622 = vadd.s32 %v1381, %v52697 (stack65)
        %s53624 = smul.u32 128, %s27 (stack66)
        %v53625 = vlaneseq (stack67)
        %v53626 = vand.u32 %v53625, 127 (stack68)
        %v53627 = vstv %s53624 (stack69)
        %v53628 = vadd.s32 %v53626, %v53627 (stack70)
        %v53632 = vadd.s32 %v53622, %v53628 (stack65)
        %vm53636 = vcmp.lt.u32.totalorder %v53632, %v53622 (stack71)
        %vm53641 = vcmp.lt.u32.totalorder %v53622, %v1381 (stack71)
        %v53646 = vadd.s32 %v1368, %v52680 (stack65)
        %v53650 = vadd.s32 %v53646, 1 (stack65)
        %v53654 = vsel /*vm=*/%vm53641, /*on_true_vy=*/%v53650, /*on_false_vx=*/%v53646 (stack72)
        %v53658 = vadd.s32 %v53654, 1 (stack65)
        %v53662 = vsel /*vm=*/%vm53636, /*on_true_vy=*/%v53658, /*on_false_vx=*/%v53654 (stack72)
        %v53667 = vadd.s32 %v53662, %v10 (stack65)
        %v53671 = vadd.s32 %v53632, %v9 (stack65)
        %v53675 = vadd.s32 %v53667, %v53671 (stack65)
        %v53677 = vshll.u32 %v53671, 13 (stack73)
        %v53678 = vshrl.u32 %v53671, 19 (stack74)
        %v53679 = vor.u32 %v53677, %v53678 (stack75)
        %v53680 = vxor.u32 %v53675, %v53679 (stack76)
        %v53683 = vadd.s32 %v53675, %v53680 (stack65)
        %v53685 = vshll.u32 %v53680, 15 (stack73)
        %v53686 = vshrl.u32 %v53680, 17 (stack74)
        %v53687 = vor.u32 %v53685, %v53686 (stack75)
        %v53688 = vxor.u32 %v53683, %v53687 (stack76)
        %v53691 = vadd.s32 %v53683, %v53688 (stack65)
        %v53693 = vshll.u32 %v53688, 26 (stack73)
        %v53694 = vshrl.u32 %v53688, 6 (stack74)
        %v53695 = vor.u32 %v53693, %v53694 (stack75)
        %v53696 = vxor.u32 %v53691, %v53695 (stack76)
        %v53699 = vadd.s32 %v53691, %v53696 (stack65)
        %v53703 = vadd.s32 %v53699, %v9 (stack65)
        %v53705 = vshll.u32 %v53696, 6 (stack73)
        %v53706 = vshrl.u32 %v53696, 26 (stack74)
        %v53707 = vor.u32 %v53705, %v53706 (stack75)
        %v53708 = vxor.u32 %v53699, %v53707 (stack76)
        %v53711 = vadd.s32 %v53708, %v8 (stack65)
        %v53715 = vadd.s32 %v53711, 1 (stack65)
        %v53719 = vadd.s32 %v53703, %v53715 (stack65)
        %v53721 = vshll.u32 %v53715, 17 (stack73)
        %v53722 = vshrl.u32 %v53715, 15 (stack74)
        %v53723 = vor.u32 %v53721, %v53722 (stack75)
        %v53724 = vxor.u32 %v53719, %v53723 (stack76)
        %v53727 = vadd.s32 %v53719, %v53724 (stack65)
        %v53729 = vshll.u32 %v53724, 29 (stack73)
        %v53730 = vshrl.u32 %v53724, 3 (stack74)
        %v53731 = vor.u32 %v53729, %v53730 (stack75)
        %v53732 = vxor.u32 %v53727, %v53731 (stack76)
        %v53735 = vadd.s32 %v53727, %v53732 (stack65)
        %v53737 = vshll.u32 %v53732, 16 (stack73)
        %v53738 = vshrl.u32 %v53732, 16 (stack74)
        %v53739 = vor.u32 %v53737, %v53738 (stack75)
        %v53740 = vxor.u32 %v53735, %v53739 (stack76)
        %v53743 = vadd.s32 %v53735, %v53740 (stack65)
        %v53747 = vadd.s32 %v53743, %v8 (stack65)
        %v53749 = vshll.u32 %v53740, 24 (stack73)
        %v53750 = vshrl.u32 %v53740, 8 (stack74)
        %v53751 = vor.u32 %v53749, %v53750 (stack75)
        %v53752 = vxor.u32 %v53743, %v53751 (stack76)
        %v53755 = vadd.s32 %v53752, %v10 (stack65)
        %v53759 = vadd.s32 %v53755, 2 (stack65)
        %v53763 = vadd.s32 %v53747, %v53759 (stack65)
        %v53765 = vshll.u32 %v53759, 13 (stack73)
        %v53766 = vshrl.u32 %v53759, 19 (stack74)
        %v53767 = vor.u32 %v53765, %v53766 (stack75)
        %v53768 = vxor.u32 %v53763, %v53767 (stack76)
        %v53771 = vadd.s32 %v53763, %v53768 (stack65)
        %v53773 = vshll.u32 %v53768, 15 (stack73)
        %v53774 = vshrl.u32 %v53768, 17 (stack74)
        %v53775 = vor.u32 %v53773, %v53774 (stack75)
        %v53776 = vxor.u32 %v53771, %v53775 (stack76)
        %v53779 = vadd.s32 %v53771, %v53776 (stack65)
        %v53781 = vshll.u32 %v53776, 26 (stack73)
        %v53782 = vshrl.u32 %v53776, 6 (stack74)
        %v53783 = vor.u32 %v53781, %v53782 (stack75)
        %v53784 = vxor.u32 %v53779, %v53783 (stack76)
        %v53787 = vadd.s32 %v53779, %v53784 (stack65)
        %v53791 = vadd.s32 %v53787, %v10 (stack65)
        %v53793 = vshll.u32 %v53784, 6 (stack73)
        %v53794 = vshrl.u32 %v53784, 26 (stack74)
        %v53795 = vor.u32 %v53793, %v53794 (stack75)
        %v53796 = vxor.u32 %v53787, %v53795 (stack76)
        %v53799 = vadd.s32 %v53796, %v9 (stack65)
        %v53803 = vadd.s32 %v53799, 3 (stack65)
        %v53807 = vadd.s32 %v53791, %v53803 (stack65)
        %v53809 = vshll.u32 %v53803, 17 (stack73)
        %v53810 = vshrl.u32 %v53803, 15 (stack74)
        %v53811 = vor.u32 %v53809, %v53810 (stack75)
        %v53812 = vxor.u32 %v53807, %v53811 (stack76)
        %v53815 = vadd.s32 %v53807, %v53812 (stack65)
        %v53817 = vshll.u32 %v53812, 29 (stack73)
        %v53818 = vshrl.u32 %v53812, 3 (stack74)
        %v53819 = vor.u32 %v53817, %v53818 (stack75)
        %v53820 = vxor.u32 %v53815, %v53819 (stack76)
        %v53823 = vadd.s32 %v53815, %v53820 (stack65)
        %v53825 = vshll.u32 %v53820, 16 (stack73)
        %v53826 = vshrl.u32 %v53820, 16 (stack74)
        %v53827 = vor.u32 %v53825, %v53826 (stack75)
        %v53828 = vxor.u32 %v53823, %v53827 (stack76)
        %v53831 = vadd.s32 %v53823, %v53828 (stack65)
        %v53835 = vadd.s32 %v53831, %v9 (stack65)
        %v53837 = vshll.u32 %v53828, 24 (stack73)
        %v53838 = vshrl.u32 %v53828, 8 (stack74)
        %v53839 = vor.u32 %v53837, %v53838 (stack75)
        %v53840 = vxor.u32 %v53831, %v53839 (stack76)
        %v53843 = vadd.s32 %v53840, %v8 (stack65)
        %v53847 = vadd.s32 %v53843, 4 (stack65)
        %v53851 = vadd.s32 %v53835, %v53847 (stack65)
        %v53853 = vshll.u32 %v53847, 13 (stack73)
        %v53854 = vshrl.u32 %v53847, 19 (stack74)
        %v53855 = vor.u32 %v53853, %v53854 (stack75)
        %v53856 = vxor.u32 %v53851, %v53855 (stack76)
        %v53859 = vadd.s32 %v53851, %v53856 (stack65)
        %v53861 = vshll.u32 %v53856, 15 (stack73)
        %v53862 = vshrl.u32 %v53856, 17 (stack74)
        %v53863 = vor.u32 %v53861, %v53862 (stack75)
        %v53864 = vxor.u32 %v53859, %v53863 (stack76)
        %v53867 = vadd.s32 %v53859, %v53864 (stack65)
        %v53869 = vshll.u32 %v53864, 26 (stack73)
        %v53870 = vshrl.u32 %v53864, 6 (stack74)
        %v53871 = vor.u32 %v53869, %v53870 (stack75)
        %v53872 = vxor.u32 %v53867, %v53871 (stack76)
        %v53875 = vadd.s32 %v53867, %v53872 (stack65)
        %v53879 = vadd.s32 %v53875, %v8 (stack65)
        %v53881 = vshll.u32 %v53872, 6 (stack73)
        %v53882 = vshrl.u32 %v53872, 26 (stack74)
        %v53883 = vor.u32 %v53881, %v53882 (stack75)
        %v53884 = vxor.u32 %v53875, %v53883 (stack76)
        %v53887 = vadd.s32 %v53884, %v10 (stack65)
        %v53891 = vadd.s32 %v53887, 5 (stack65)
        %v53893 = vxor.u32 %v53879, %v53891 (stack76)
        %v53894 = vand.u32.u8 %v53893, 255 (stack77)
        %v53895 = vand.u32 %v53894, 65535 (stack78)
        %v53896 = vshrl.u32 %v53895, 1 (stack79)
        %v53897 = vor.u32 %v53896, 16256 (stack75)
        %v53898 = vand.u32.u16 %v53897, 65535 (stack80)
        %v53899 = vunpack.i.l.bf16 %v53898 (stack81)
        %v53903 = vadd.f32 %v53899, -1.0 (stack82)
        %v53907 = vmul.f32 %v53903, 2.0 (stack83)
        %v53911 = vadd.f32 %v53907, -0.99609375 (stack82)
        %v53915 = vmax.f32 -0.99609375, %v53911 (stack84)
        %v53917 = vand.u32 2147483647, %v53915 (stack85)
        %vm53920 = vcmp.eq.f32.partialorder %v53917, 1.0 (stack86)
        %v53925 = vmul.f32 %v53915, inf (stack83)
        %v53927 = vxor.u32 %v53915, 2147483648 (stack87)
        %v53930 = vmul.f32 %v53915, %v53927 (stack83)
        %v53932 = vadd.f32 %v53930, 1.0 (stack88)
        %v53933 = vlog2.pop %v53932 (stack89)
        %v53934 = vmul.f32 %v53933, 0.6931472 (stack90)
        %v53935 = vmul.f32 -0.5, %v53930 (stack91)
        %v53936 = vadd.f32 %v53935, 1.0 (stack92)
        %v53937 = vmul.f32 %v53936, %v53930 (stack93)
        %v53938 = vand.u32 2147483647, %v53930 (stack94)
        %vm53939 = vcmp.lt.f32.partialorder %v53938, 0.0004427343 (stack95)
        %v53940 = vsel /*vm=*/%vm53939, /*on_true_vy=*/%v53937, /*on_false_vx=*/%v53934 (stack96)
        %v53941 = vxor.u32 %v53940, 2147483648 (stack87)
        %vm53944 = vcmp.lt.f32.partialorder %v53941, 5.0 (stack86)
        %v53949 = vsel /*vm=*/%vm53944, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v53953 = vsel /*vm=*/%vm53944, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v53957 = vsel /*vm=*/%vm53944, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v53961 = vsel /*vm=*/%vm53944, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v53965 = vsel /*vm=*/%vm53944, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v53969 = vsel /*vm=*/%vm53944, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v53973 = vsel /*vm=*/%vm53944, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v53977 = vsel /*vm=*/%vm53944, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v53981 = vsel /*vm=*/%vm53944, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v53985 = vadd.f32 %v53941, -2.5 (stack82)
        %v53987 = vrsqrt.pop %v53941 (stack97)
        %v53988 = vmul.f32 %v53941, %v53987 (stack98)
        %vm53989 = vcmp.eq.f32.partialorder %v53941, inf (stack99)
        %v53990 = vsel /*vm=*/%vm53989, /*on_true_vy=*/%v53941, /*on_false_vx=*/%v53988 (stack100)
        %vm53991 = vcmp.eq.f32.partialorder %v53941, 0.0 (stack101)
        %v53992 = vand.u32 %v53941, 2147483648 (stack102)
        %v53993 = vsel /*vm=*/%vm53991, /*on_true_vy=*/%v53992, /*on_false_vx=*/%v53990 (stack103)
        %v53996 = vadd.f32 %v53993, -3.0 (stack82)
        %v54000 = vsel /*vm=*/%vm53944, /*on_true_vy=*/%v53985, /*on_false_vx=*/%v53996 (stack72)
        %v54004 = vmul.f32 %v53981, %v54000 (stack83)
        %v54008 = vadd.f32 %v53977, %v54004 (stack82)
        %v54012 = vmul.f32 %v54008, %v54000 (stack83)
        %v54016 = vadd.f32 %v53973, %v54012 (stack82)
        %v54020 = vmul.f32 %v54016, %v54000 (stack83)
        %v54024 = vadd.f32 %v53969, %v54020 (stack82)
        %v54028 = vmul.f32 %v54024, %v54000 (stack83)
        %v54032 = vadd.f32 %v53965, %v54028 (stack82)
        %v54036 = vmul.f32 %v54032, %v54000 (stack83)
        %v54040 = vadd.f32 %v53961, %v54036 (stack82)
        %v54044 = vmul.f32 %v54040, %v54000 (stack83)
        %v54048 = vadd.f32 %v53957, %v54044 (stack82)
        %v54052 = vmul.f32 %v54048, %v54000 (stack83)
        %v54056 = vadd.f32 %v53953, %v54052 (stack82)
        %v54060 = vmul.f32 %v54056, %v54000 (stack83)
        %v54064 = vadd.f32 %v53949, %v54060 (stack82)
        %v54068 = vmul.f32 %v54064, %v53915 (stack83)
        %v54072 = vsel /*vm=*/%vm53920, /*on_true_vy=*/%v53925, /*on_false_vx=*/%v54068 (stack72)
        %v54076 = vmul.f32 %v54072, 1.4140625 (stack83)
        %s54078 = scalar_lea.vmem %s280, 312 [#allocation0] (stack107)
        %v54079 = vpack.c.bf16 0.0, %v54076 (stack104)
        %54080 = vst [vmem:[%s54078] sm:$0xf] /*vst_source=*/%v54079 (stack105)
        %v54083 = vadd.s32 %v1868, %v52697 (stack65)
        %s54085 = smul.u32 128, %s27 (stack66)
        %v54086 = vlaneseq (stack67)
        %v54087 = vand.u32 %v54086, 127 (stack68)
        %v54088 = vstv %s54085 (stack69)
        %v54089 = vadd.s32 %v54087, %v54088 (stack70)
        %v54093 = vadd.s32 %v54083, %v54089 (stack65)
        %vm54097 = vcmp.lt.u32.totalorder %v54093, %v54083 (stack71)
        %vm54102 = vcmp.lt.u32.totalorder %v54083, %v1868 (stack71)
        %v54107 = vadd.s32 %v1855, %v52680 (stack65)
        %v54111 = vadd.s32 %v54107, 1 (stack65)
        %v54115 = vsel /*vm=*/%vm54102, /*on_true_vy=*/%v54111, /*on_false_vx=*/%v54107 (stack72)
        %v54119 = vadd.s32 %v54115, 1 (stack65)
        %v54123 = vsel /*vm=*/%vm54097, /*on_true_vy=*/%v54119, /*on_false_vx=*/%v54115 (stack72)
        %v54128 = vadd.s32 %v54123, %v10 (stack65)
        %v54132 = vadd.s32 %v54093, %v9 (stack65)
        %v54136 = vadd.s32 %v54128, %v54132 (stack65)
        %v54138 = vshll.u32 %v54132, 13 (stack73)
        %v54139 = vshrl.u32 %v54132, 19 (stack74)
        %v54140 = vor.u32 %v54138, %v54139 (stack75)
        %v54141 = vxor.u32 %v54136, %v54140 (stack76)
        %v54144 = vadd.s32 %v54136, %v54141 (stack65)
        %v54146 = vshll.u32 %v54141, 15 (stack73)
        %v54147 = vshrl.u32 %v54141, 17 (stack74)
        %v54148 = vor.u32 %v54146, %v54147 (stack75)
        %v54149 = vxor.u32 %v54144, %v54148 (stack76)
        %v54152 = vadd.s32 %v54144, %v54149 (stack65)
        %v54154 = vshll.u32 %v54149, 26 (stack73)
        %v54155 = vshrl.u32 %v54149, 6 (stack74)
        %v54156 = vor.u32 %v54154, %v54155 (stack75)
        %v54157 = vxor.u32 %v54152, %v54156 (stack76)
        %v54160 = vadd.s32 %v54152, %v54157 (stack65)
        %v54164 = vadd.s32 %v54160, %v9 (stack65)
        %v54166 = vshll.u32 %v54157, 6 (stack73)
        %v54167 = vshrl.u32 %v54157, 26 (stack74)
        %v54168 = vor.u32 %v54166, %v54167 (stack75)
        %v54169 = vxor.u32 %v54160, %v54168 (stack76)
        %v54172 = vadd.s32 %v54169, %v8 (stack65)
        %v54176 = vadd.s32 %v54172, 1 (stack65)
        %v54180 = vadd.s32 %v54164, %v54176 (stack65)
        %v54182 = vshll.u32 %v54176, 17 (stack73)
        %v54183 = vshrl.u32 %v54176, 15 (stack74)
        %v54184 = vor.u32 %v54182, %v54183 (stack75)
        %v54185 = vxor.u32 %v54180, %v54184 (stack76)
        %v54188 = vadd.s32 %v54180, %v54185 (stack65)
        %v54190 = vshll.u32 %v54185, 29 (stack73)
        %v54191 = vshrl.u32 %v54185, 3 (stack74)
        %v54192 = vor.u32 %v54190, %v54191 (stack75)
        %v54193 = vxor.u32 %v54188, %v54192 (stack76)
        %v54196 = vadd.s32 %v54188, %v54193 (stack65)
        %v54198 = vshll.u32 %v54193, 16 (stack73)
        %v54199 = vshrl.u32 %v54193, 16 (stack74)
        %v54200 = vor.u32 %v54198, %v54199 (stack75)
        %v54201 = vxor.u32 %v54196, %v54200 (stack76)
        %v54204 = vadd.s32 %v54196, %v54201 (stack65)
        %v54208 = vadd.s32 %v54204, %v8 (stack65)
        %v54210 = vshll.u32 %v54201, 24 (stack73)
        %v54211 = vshrl.u32 %v54201, 8 (stack74)
        %v54212 = vor.u32 %v54210, %v54211 (stack75)
        %v54213 = vxor.u32 %v54204, %v54212 (stack76)
        %v54216 = vadd.s32 %v54213, %v10 (stack65)
        %v54220 = vadd.s32 %v54216, 2 (stack65)
        %v54224 = vadd.s32 %v54208, %v54220 (stack65)
        %v54226 = vshll.u32 %v54220, 13 (stack73)
        %v54227 = vshrl.u32 %v54220, 19 (stack74)
        %v54228 = vor.u32 %v54226, %v54227 (stack75)
        %v54229 = vxor.u32 %v54224, %v54228 (stack76)
        %v54232 = vadd.s32 %v54224, %v54229 (stack65)
        %v54234 = vshll.u32 %v54229, 15 (stack73)
        %v54235 = vshrl.u32 %v54229, 17 (stack74)
        %v54236 = vor.u32 %v54234, %v54235 (stack75)
        %v54237 = vxor.u32 %v54232, %v54236 (stack76)
        %v54240 = vadd.s32 %v54232, %v54237 (stack65)
        %v54242 = vshll.u32 %v54237, 26 (stack73)
        %v54243 = vshrl.u32 %v54237, 6 (stack74)
        %v54244 = vor.u32 %v54242, %v54243 (stack75)
        %v54245 = vxor.u32 %v54240, %v54244 (stack76)
        %v54248 = vadd.s32 %v54240, %v54245 (stack65)
        %v54252 = vadd.s32 %v54248, %v10 (stack65)
        %v54254 = vshll.u32 %v54245, 6 (stack73)
        %v54255 = vshrl.u32 %v54245, 26 (stack74)
        %v54256 = vor.u32 %v54254, %v54255 (stack75)
        %v54257 = vxor.u32 %v54248, %v54256 (stack76)
        %v54260 = vadd.s32 %v54257, %v9 (stack65)
        %v54264 = vadd.s32 %v54260, 3 (stack65)
        %v54268 = vadd.s32 %v54252, %v54264 (stack65)
        %v54270 = vshll.u32 %v54264, 17 (stack73)
        %v54271 = vshrl.u32 %v54264, 15 (stack74)
        %v54272 = vor.u32 %v54270, %v54271 (stack75)
        %v54273 = vxor.u32 %v54268, %v54272 (stack76)
        %v54276 = vadd.s32 %v54268, %v54273 (stack65)
        %v54278 = vshll.u32 %v54273, 29 (stack73)
        %v54279 = vshrl.u32 %v54273, 3 (stack74)
        %v54280 = vor.u32 %v54278, %v54279 (stack75)
        %v54281 = vxor.u32 %v54276, %v54280 (stack76)
        %v54284 = vadd.s32 %v54276, %v54281 (stack65)
        %v54286 = vshll.u32 %v54281, 16 (stack73)
        %v54287 = vshrl.u32 %v54281, 16 (stack74)
        %v54288 = vor.u32 %v54286, %v54287 (stack75)
        %v54289 = vxor.u32 %v54284, %v54288 (stack76)
        %v54292 = vadd.s32 %v54284, %v54289 (stack65)
        %v54296 = vadd.s32 %v54292, %v9 (stack65)
        %v54298 = vshll.u32 %v54289, 24 (stack73)
        %v54299 = vshrl.u32 %v54289, 8 (stack74)
        %v54300 = vor.u32 %v54298, %v54299 (stack75)
        %v54301 = vxor.u32 %v54292, %v54300 (stack76)
        %v54304 = vadd.s32 %v54301, %v8 (stack65)
        %v54308 = vadd.s32 %v54304, 4 (stack65)
        %v54312 = vadd.s32 %v54296, %v54308 (stack65)
        %v54314 = vshll.u32 %v54308, 13 (stack73)
        %v54315 = vshrl.u32 %v54308, 19 (stack74)
        %v54316 = vor.u32 %v54314, %v54315 (stack75)
        %v54317 = vxor.u32 %v54312, %v54316 (stack76)
        %v54320 = vadd.s32 %v54312, %v54317 (stack65)
        %v54322 = vshll.u32 %v54317, 15 (stack73)
        %v54323 = vshrl.u32 %v54317, 17 (stack74)
        %v54324 = vor.u32 %v54322, %v54323 (stack75)
        %v54325 = vxor.u32 %v54320, %v54324 (stack76)
        %v54328 = vadd.s32 %v54320, %v54325 (stack65)
        %v54330 = vshll.u32 %v54325, 26 (stack73)
        %v54331 = vshrl.u32 %v54325, 6 (stack74)
        %v54332 = vor.u32 %v54330, %v54331 (stack75)
        %v54333 = vxor.u32 %v54328, %v54332 (stack76)
        %v54336 = vadd.s32 %v54328, %v54333 (stack65)
        %v54340 = vadd.s32 %v54336, %v8 (stack65)
        %v54342 = vshll.u32 %v54333, 6 (stack73)
        %v54343 = vshrl.u32 %v54333, 26 (stack74)
        %v54344 = vor.u32 %v54342, %v54343 (stack75)
        %v54345 = vxor.u32 %v54336, %v54344 (stack76)
        %v54348 = vadd.s32 %v54345, %v10 (stack65)
        %v54352 = vadd.s32 %v54348, 5 (stack65)
        %v54354 = vxor.u32 %v54340, %v54352 (stack76)
        %v54355 = vand.u32.u8 %v54354, 255 (stack77)
        %v54356 = vand.u32 %v54355, 65535 (stack78)
        %v54357 = vshrl.u32 %v54356, 1 (stack79)
        %v54358 = vor.u32 %v54357, 16256 (stack75)
        %v54359 = vand.u32.u16 %v54358, 65535 (stack80)
        %v54360 = vunpack.i.l.bf16 %v54359 (stack81)
        %v54364 = vadd.f32 %v54360, -1.0 (stack82)
        %v54368 = vmul.f32 %v54364, 2.0 (stack83)
        %v54372 = vadd.f32 %v54368, -0.99609375 (stack82)
        %v54376 = vmax.f32 -0.99609375, %v54372 (stack84)
        %v54378 = vand.u32 2147483647, %v54376 (stack85)
        %vm54381 = vcmp.eq.f32.partialorder %v54378, 1.0 (stack86)
        %v54386 = vmul.f32 %v54376, inf (stack83)
        %v54388 = vxor.u32 %v54376, 2147483648 (stack87)
        %v54391 = vmul.f32 %v54376, %v54388 (stack83)
        %v54393 = vadd.f32 %v54391, 1.0 (stack88)
        %v54394 = vlog2.pop %v54393 (stack89)
        %v54395 = vmul.f32 %v54394, 0.6931472 (stack90)
        %v54396 = vmul.f32 -0.5, %v54391 (stack91)
        %v54397 = vadd.f32 %v54396, 1.0 (stack92)
        %v54398 = vmul.f32 %v54397, %v54391 (stack93)
        %v54399 = vand.u32 2147483647, %v54391 (stack94)
        %vm54400 = vcmp.lt.f32.partialorder %v54399, 0.0004427343 (stack95)
        %v54401 = vsel /*vm=*/%vm54400, /*on_true_vy=*/%v54398, /*on_false_vx=*/%v54395 (stack96)
        %v54402 = vxor.u32 %v54401, 2147483648 (stack87)
        %vm54405 = vcmp.lt.f32.partialorder %v54402, 5.0 (stack86)
        %v54410 = vsel /*vm=*/%vm54405, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v54414 = vsel /*vm=*/%vm54405, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v54418 = vsel /*vm=*/%vm54405, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v54422 = vsel /*vm=*/%vm54405, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v54426 = vsel /*vm=*/%vm54405, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v54430 = vsel /*vm=*/%vm54405, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v54434 = vsel /*vm=*/%vm54405, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v54438 = vsel /*vm=*/%vm54405, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v54442 = vsel /*vm=*/%vm54405, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v54446 = vadd.f32 %v54402, -2.5 (stack82)
        %v54448 = vrsqrt.pop %v54402 (stack97)
        %v54449 = vmul.f32 %v54402, %v54448 (stack98)
        %vm54450 = vcmp.eq.f32.partialorder %v54402, inf (stack99)
        %v54451 = vsel /*vm=*/%vm54450, /*on_true_vy=*/%v54402, /*on_false_vx=*/%v54449 (stack100)
        %vm54452 = vcmp.eq.f32.partialorder %v54402, 0.0 (stack101)
        %v54453 = vand.u32 %v54402, 2147483648 (stack102)
        %v54454 = vsel /*vm=*/%vm54452, /*on_true_vy=*/%v54453, /*on_false_vx=*/%v54451 (stack103)
        %v54457 = vadd.f32 %v54454, -3.0 (stack82)
        %v54461 = vsel /*vm=*/%vm54405, /*on_true_vy=*/%v54446, /*on_false_vx=*/%v54457 (stack72)
        %v54465 = vmul.f32 %v54442, %v54461 (stack83)
        %v54469 = vadd.f32 %v54438, %v54465 (stack82)
        %v54473 = vmul.f32 %v54469, %v54461 (stack83)
        %v54477 = vadd.f32 %v54434, %v54473 (stack82)
        %v54481 = vmul.f32 %v54477, %v54461 (stack83)
        %v54485 = vadd.f32 %v54430, %v54481 (stack82)
        %v54489 = vmul.f32 %v54485, %v54461 (stack83)
        %v54493 = vadd.f32 %v54426, %v54489 (stack82)
        %v54497 = vmul.f32 %v54493, %v54461 (stack83)
        %v54501 = vadd.f32 %v54422, %v54497 (stack82)
        %v54505 = vmul.f32 %v54501, %v54461 (stack83)
        %v54509 = vadd.f32 %v54418, %v54505 (stack82)
        %v54513 = vmul.f32 %v54509, %v54461 (stack83)
        %v54517 = vadd.f32 %v54414, %v54513 (stack82)
        %v54521 = vmul.f32 %v54517, %v54461 (stack83)
        %v54525 = vadd.f32 %v54410, %v54521 (stack82)
        %v54529 = vmul.f32 %v54525, %v54376 (stack83)
        %v54533 = vsel /*vm=*/%vm54381, /*on_true_vy=*/%v54386, /*on_false_vx=*/%v54529 (stack72)
        %v54537 = vmul.f32 %v54533, 1.4140625 (stack83)
        %s54539 = scalar_lea.vmem %s280, 440 [#allocation0] (stack107)
        %v54540 = vpack.c.bf16 0.0, %v54537 (stack104)
        %54541 = vst [vmem:[%s54539] sm:$0xf] /*vst_source=*/%v54540 (stack105)
        %v54544 = vadd.s32 %v2355, %v52697 (stack65)
        %s54546 = smul.u32 128, %s27 (stack66)
        %v54547 = vlaneseq (stack67)
        %v54548 = vand.u32 %v54547, 127 (stack68)
        %v54549 = vstv %s54546 (stack69)
        %v54550 = vadd.s32 %v54548, %v54549 (stack70)
        %v54554 = vadd.s32 %v54544, %v54550 (stack65)
        %vm54558 = vcmp.lt.u32.totalorder %v54554, %v54544 (stack71)
        %vm54563 = vcmp.lt.u32.totalorder %v54544, %v2355 (stack71)
        %v54568 = vadd.s32 %v2342, %v52680 (stack65)
        %v54572 = vadd.s32 %v54568, 1 (stack65)
        %v54576 = vsel /*vm=*/%vm54563, /*on_true_vy=*/%v54572, /*on_false_vx=*/%v54568 (stack72)
        %v54580 = vadd.s32 %v54576, 1 (stack65)
        %v54584 = vsel /*vm=*/%vm54558, /*on_true_vy=*/%v54580, /*on_false_vx=*/%v54576 (stack72)
        %v54589 = vadd.s32 %v54584, %v10 (stack65)
        %v54593 = vadd.s32 %v54554, %v9 (stack65)
        %v54597 = vadd.s32 %v54589, %v54593 (stack65)
        %v54599 = vshll.u32 %v54593, 13 (stack73)
        %v54600 = vshrl.u32 %v54593, 19 (stack74)
        %v54601 = vor.u32 %v54599, %v54600 (stack75)
        %v54602 = vxor.u32 %v54597, %v54601 (stack76)
        %v54605 = vadd.s32 %v54597, %v54602 (stack65)
        %v54607 = vshll.u32 %v54602, 15 (stack73)
        %v54608 = vshrl.u32 %v54602, 17 (stack74)
        %v54609 = vor.u32 %v54607, %v54608 (stack75)
        %v54610 = vxor.u32 %v54605, %v54609 (stack76)
        %v54613 = vadd.s32 %v54605, %v54610 (stack65)
        %v54615 = vshll.u32 %v54610, 26 (stack73)
        %v54616 = vshrl.u32 %v54610, 6 (stack74)
        %v54617 = vor.u32 %v54615, %v54616 (stack75)
        %v54618 = vxor.u32 %v54613, %v54617 (stack76)
        %v54621 = vadd.s32 %v54613, %v54618 (stack65)
        %v54625 = vadd.s32 %v54621, %v9 (stack65)
        %v54627 = vshll.u32 %v54618, 6 (stack73)
        %v54628 = vshrl.u32 %v54618, 26 (stack74)
        %v54629 = vor.u32 %v54627, %v54628 (stack75)
        %v54630 = vxor.u32 %v54621, %v54629 (stack76)
        %v54633 = vadd.s32 %v54630, %v8 (stack65)
        %v54637 = vadd.s32 %v54633, 1 (stack65)
        %v54641 = vadd.s32 %v54625, %v54637 (stack65)
        %v54643 = vshll.u32 %v54637, 17 (stack73)
        %v54644 = vshrl.u32 %v54637, 15 (stack74)
        %v54645 = vor.u32 %v54643, %v54644 (stack75)
        %v54646 = vxor.u32 %v54641, %v54645 (stack76)
        %v54649 = vadd.s32 %v54641, %v54646 (stack65)
        %v54651 = vshll.u32 %v54646, 29 (stack73)
        %v54652 = vshrl.u32 %v54646, 3 (stack74)
        %v54653 = vor.u32 %v54651, %v54652 (stack75)
        %v54654 = vxor.u32 %v54649, %v54653 (stack76)
        %v54657 = vadd.s32 %v54649, %v54654 (stack65)
        %v54659 = vshll.u32 %v54654, 16 (stack73)
        %v54660 = vshrl.u32 %v54654, 16 (stack74)
        %v54661 = vor.u32 %v54659, %v54660 (stack75)
        %v54662 = vxor.u32 %v54657, %v54661 (stack76)
        %v54665 = vadd.s32 %v54657, %v54662 (stack65)
        %v54669 = vadd.s32 %v54665, %v8 (stack65)
        %v54671 = vshll.u32 %v54662, 24 (stack73)
        %v54672 = vshrl.u32 %v54662, 8 (stack74)
        %v54673 = vor.u32 %v54671, %v54672 (stack75)
        %v54674 = vxor.u32 %v54665, %v54673 (stack76)
        %v54677 = vadd.s32 %v54674, %v10 (stack65)
        %v54681 = vadd.s32 %v54677, 2 (stack65)
        %v54685 = vadd.s32 %v54669, %v54681 (stack65)
        %v54687 = vshll.u32 %v54681, 13 (stack73)
        %v54688 = vshrl.u32 %v54681, 19 (stack74)
        %v54689 = vor.u32 %v54687, %v54688 (stack75)
        %v54690 = vxor.u32 %v54685, %v54689 (stack76)
        %v54693 = vadd.s32 %v54685, %v54690 (stack65)
        %v54695 = vshll.u32 %v54690, 15 (stack73)
        %v54696 = vshrl.u32 %v54690, 17 (stack74)
        %v54697 = vor.u32 %v54695, %v54696 (stack75)
        %v54698 = vxor.u32 %v54693, %v54697 (stack76)
        %v54701 = vadd.s32 %v54693, %v54698 (stack65)
        %v54703 = vshll.u32 %v54698, 26 (stack73)
        %v54704 = vshrl.u32 %v54698, 6 (stack74)
        %v54705 = vor.u32 %v54703, %v54704 (stack75)
        %v54706 = vxor.u32 %v54701, %v54705 (stack76)
        %v54709 = vadd.s32 %v54701, %v54706 (stack65)
        %v54713 = vadd.s32 %v54709, %v10 (stack65)
        %v54715 = vshll.u32 %v54706, 6 (stack73)
        %v54716 = vshrl.u32 %v54706, 26 (stack74)
        %v54717 = vor.u32 %v54715, %v54716 (stack75)
        %v54718 = vxor.u32 %v54709, %v54717 (stack76)
        %v54721 = vadd.s32 %v54718, %v9 (stack65)
        %v54725 = vadd.s32 %v54721, 3 (stack65)
        %v54729 = vadd.s32 %v54713, %v54725 (stack65)
        %v54731 = vshll.u32 %v54725, 17 (stack73)
        %v54732 = vshrl.u32 %v54725, 15 (stack74)
        %v54733 = vor.u32 %v54731, %v54732 (stack75)
        %v54734 = vxor.u32 %v54729, %v54733 (stack76)
        %v54737 = vadd.s32 %v54729, %v54734 (stack65)
        %v54739 = vshll.u32 %v54734, 29 (stack73)
        %v54740 = vshrl.u32 %v54734, 3 (stack74)
        %v54741 = vor.u32 %v54739, %v54740 (stack75)
        %v54742 = vxor.u32 %v54737, %v54741 (stack76)
        %v54745 = vadd.s32 %v54737, %v54742 (stack65)
        %v54747 = vshll.u32 %v54742, 16 (stack73)
        %v54748 = vshrl.u32 %v54742, 16 (stack74)
        %v54749 = vor.u32 %v54747, %v54748 (stack75)
        %v54750 = vxor.u32 %v54745, %v54749 (stack76)
        %v54753 = vadd.s32 %v54745, %v54750 (stack65)
        %v54757 = vadd.s32 %v54753, %v9 (stack65)
        %v54759 = vshll.u32 %v54750, 24 (stack73)
        %v54760 = vshrl.u32 %v54750, 8 (stack74)
        %v54761 = vor.u32 %v54759, %v54760 (stack75)
        %v54762 = vxor.u32 %v54753, %v54761 (stack76)
        %v54765 = vadd.s32 %v54762, %v8 (stack65)
        %v54769 = vadd.s32 %v54765, 4 (stack65)
        %v54773 = vadd.s32 %v54757, %v54769 (stack65)
        %v54775 = vshll.u32 %v54769, 13 (stack73)
        %v54776 = vshrl.u32 %v54769, 19 (stack74)
        %v54777 = vor.u32 %v54775, %v54776 (stack75)
        %v54778 = vxor.u32 %v54773, %v54777 (stack76)
        %v54781 = vadd.s32 %v54773, %v54778 (stack65)
        %v54783 = vshll.u32 %v54778, 15 (stack73)
        %v54784 = vshrl.u32 %v54778, 17 (stack74)
        %v54785 = vor.u32 %v54783, %v54784 (stack75)
        %v54786 = vxor.u32 %v54781, %v54785 (stack76)
        %v54789 = vadd.s32 %v54781, %v54786 (stack65)
        %v54791 = vshll.u32 %v54786, 26 (stack73)
        %v54792 = vshrl.u32 %v54786, 6 (stack74)
        %v54793 = vor.u32 %v54791, %v54792 (stack75)
        %v54794 = vxor.u32 %v54789, %v54793 (stack76)
        %v54797 = vadd.s32 %v54789, %v54794 (stack65)
        %v54801 = vadd.s32 %v54797, %v8 (stack65)
        %v54803 = vshll.u32 %v54794, 6 (stack73)
        %v54804 = vshrl.u32 %v54794, 26 (stack74)
        %v54805 = vor.u32 %v54803, %v54804 (stack75)
        %v54806 = vxor.u32 %v54797, %v54805 (stack76)
        %v54809 = vadd.s32 %v54806, %v10 (stack65)
        %v54813 = vadd.s32 %v54809, 5 (stack65)
        %v54815 = vxor.u32 %v54801, %v54813 (stack76)
        %v54816 = vand.u32.u8 %v54815, 255 (stack77)
        %v54817 = vand.u32 %v54816, 65535 (stack78)
        %v54818 = vshrl.u32 %v54817, 1 (stack79)
        %v54819 = vor.u32 %v54818, 16256 (stack75)
        %v54820 = vand.u32.u16 %v54819, 65535 (stack80)
        %v54821 = vunpack.i.l.bf16 %v54820 (stack81)
        %v54825 = vadd.f32 %v54821, -1.0 (stack82)
        %v54829 = vmul.f32 %v54825, 2.0 (stack83)
        %v54833 = vadd.f32 %v54829, -0.99609375 (stack82)
        %v54837 = vmax.f32 -0.99609375, %v54833 (stack84)
        %v54839 = vand.u32 2147483647, %v54837 (stack85)
        %vm54842 = vcmp.eq.f32.partialorder %v54839, 1.0 (stack86)
        %v54847 = vmul.f32 %v54837, inf (stack83)
        %v54849 = vxor.u32 %v54837, 2147483648 (stack87)
        %v54852 = vmul.f32 %v54837, %v54849 (stack83)
        %v54854 = vadd.f32 %v54852, 1.0 (stack88)
        %v54855 = vlog2.pop %v54854 (stack89)
        %v54856 = vmul.f32 %v54855, 0.6931472 (stack90)
        %v54857 = vmul.f32 -0.5, %v54852 (stack91)
        %v54858 = vadd.f32 %v54857, 1.0 (stack92)
        %v54859 = vmul.f32 %v54858, %v54852 (stack93)
        %v54860 = vand.u32 2147483647, %v54852 (stack94)
        %vm54861 = vcmp.lt.f32.partialorder %v54860, 0.0004427343 (stack95)
        %v54862 = vsel /*vm=*/%vm54861, /*on_true_vy=*/%v54859, /*on_false_vx=*/%v54856 (stack96)
        %v54863 = vxor.u32 %v54862, 2147483648 (stack87)
        %vm54866 = vcmp.lt.f32.partialorder %v54863, 5.0 (stack86)
        %v54871 = vsel /*vm=*/%vm54866, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v54875 = vsel /*vm=*/%vm54866, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v54879 = vsel /*vm=*/%vm54866, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v54883 = vsel /*vm=*/%vm54866, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v54887 = vsel /*vm=*/%vm54866, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v54891 = vsel /*vm=*/%vm54866, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v54895 = vsel /*vm=*/%vm54866, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v54899 = vsel /*vm=*/%vm54866, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v54903 = vsel /*vm=*/%vm54866, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v54907 = vadd.f32 %v54863, -2.5 (stack82)
        %v54909 = vrsqrt.pop %v54863 (stack97)
        %v54910 = vmul.f32 %v54863, %v54909 (stack98)
        %vm54911 = vcmp.eq.f32.partialorder %v54863, inf (stack99)
        %v54912 = vsel /*vm=*/%vm54911, /*on_true_vy=*/%v54863, /*on_false_vx=*/%v54910 (stack100)
        %vm54913 = vcmp.eq.f32.partialorder %v54863, 0.0 (stack101)
        %v54914 = vand.u32 %v54863, 2147483648 (stack102)
        %v54915 = vsel /*vm=*/%vm54913, /*on_true_vy=*/%v54914, /*on_false_vx=*/%v54912 (stack103)
        %v54918 = vadd.f32 %v54915, -3.0 (stack82)
        %v54922 = vsel /*vm=*/%vm54866, /*on_true_vy=*/%v54907, /*on_false_vx=*/%v54918 (stack72)
        %v54926 = vmul.f32 %v54903, %v54922 (stack83)
        %v54930 = vadd.f32 %v54899, %v54926 (stack82)
        %v54934 = vmul.f32 %v54930, %v54922 (stack83)
        %v54938 = vadd.f32 %v54895, %v54934 (stack82)
        %v54942 = vmul.f32 %v54938, %v54922 (stack83)
        %v54946 = vadd.f32 %v54891, %v54942 (stack82)
        %v54950 = vmul.f32 %v54946, %v54922 (stack83)
        %v54954 = vadd.f32 %v54887, %v54950 (stack82)
        %v54958 = vmul.f32 %v54954, %v54922 (stack83)
        %v54962 = vadd.f32 %v54883, %v54958 (stack82)
        %v54966 = vmul.f32 %v54962, %v54922 (stack83)
        %v54970 = vadd.f32 %v54879, %v54966 (stack82)
        %v54974 = vmul.f32 %v54970, %v54922 (stack83)
        %v54978 = vadd.f32 %v54875, %v54974 (stack82)
        %v54982 = vmul.f32 %v54978, %v54922 (stack83)
        %v54986 = vadd.f32 %v54871, %v54982 (stack82)
        %v54990 = vmul.f32 %v54986, %v54837 (stack83)
        %v54994 = vsel /*vm=*/%vm54842, /*on_true_vy=*/%v54847, /*on_false_vx=*/%v54990 (stack72)
        %v54998 = vmul.f32 %v54994, 1.4140625 (stack83)
        %s55000 = scalar_lea.vmem %s280, 568 [#allocation0] (stack107)
        %v55001 = vpack.c.bf16 0.0, %v54998 (stack104)
        %55002 = vst [vmem:[%s55000] sm:$0xf] /*vst_source=*/%v55001 (stack105)
        %v55005 = vadd.s32 %v2842, %v52697 (stack65)
        %s55007 = smul.u32 128, %s27 (stack66)
        %v55008 = vlaneseq (stack67)
        %v55009 = vand.u32 %v55008, 127 (stack68)
        %v55010 = vstv %s55007 (stack69)
        %v55011 = vadd.s32 %v55009, %v55010 (stack70)
        %v55015 = vadd.s32 %v55005, %v55011 (stack65)
        %vm55019 = vcmp.lt.u32.totalorder %v55015, %v55005 (stack71)
        %vm55024 = vcmp.lt.u32.totalorder %v55005, %v2842 (stack71)
        %v55029 = vadd.s32 %v2829, %v52680 (stack65)
        %v55033 = vadd.s32 %v55029, 1 (stack65)
        %v55037 = vsel /*vm=*/%vm55024, /*on_true_vy=*/%v55033, /*on_false_vx=*/%v55029 (stack72)
        %v55041 = vadd.s32 %v55037, 1 (stack65)
        %v55045 = vsel /*vm=*/%vm55019, /*on_true_vy=*/%v55041, /*on_false_vx=*/%v55037 (stack72)
        %v55050 = vadd.s32 %v55045, %v10 (stack65)
        %v55054 = vadd.s32 %v55015, %v9 (stack65)
        %v55058 = vadd.s32 %v55050, %v55054 (stack65)
        %v55060 = vshll.u32 %v55054, 13 (stack73)
        %v55061 = vshrl.u32 %v55054, 19 (stack74)
        %v55062 = vor.u32 %v55060, %v55061 (stack75)
        %v55063 = vxor.u32 %v55058, %v55062 (stack76)
        %v55066 = vadd.s32 %v55058, %v55063 (stack65)
        %v55068 = vshll.u32 %v55063, 15 (stack73)
        %v55069 = vshrl.u32 %v55063, 17 (stack74)
        %v55070 = vor.u32 %v55068, %v55069 (stack75)
        %v55071 = vxor.u32 %v55066, %v55070 (stack76)
        %v55074 = vadd.s32 %v55066, %v55071 (stack65)
        %v55076 = vshll.u32 %v55071, 26 (stack73)
        %v55077 = vshrl.u32 %v55071, 6 (stack74)
        %v55078 = vor.u32 %v55076, %v55077 (stack75)
        %v55079 = vxor.u32 %v55074, %v55078 (stack76)
        %v55082 = vadd.s32 %v55074, %v55079 (stack65)
        %v55086 = vadd.s32 %v55082, %v9 (stack65)
        %v55088 = vshll.u32 %v55079, 6 (stack73)
        %v55089 = vshrl.u32 %v55079, 26 (stack74)
        %v55090 = vor.u32 %v55088, %v55089 (stack75)
        %v55091 = vxor.u32 %v55082, %v55090 (stack76)
        %v55094 = vadd.s32 %v55091, %v8 (stack65)
        %v55098 = vadd.s32 %v55094, 1 (stack65)
        %v55102 = vadd.s32 %v55086, %v55098 (stack65)
        %v55104 = vshll.u32 %v55098, 17 (stack73)
        %v55105 = vshrl.u32 %v55098, 15 (stack74)
        %v55106 = vor.u32 %v55104, %v55105 (stack75)
        %v55107 = vxor.u32 %v55102, %v55106 (stack76)
        %v55110 = vadd.s32 %v55102, %v55107 (stack65)
        %v55112 = vshll.u32 %v55107, 29 (stack73)
        %v55113 = vshrl.u32 %v55107, 3 (stack74)
        %v55114 = vor.u32 %v55112, %v55113 (stack75)
        %v55115 = vxor.u32 %v55110, %v55114 (stack76)
        %v55118 = vadd.s32 %v55110, %v55115 (stack65)
        %v55120 = vshll.u32 %v55115, 16 (stack73)
        %v55121 = vshrl.u32 %v55115, 16 (stack74)
        %v55122 = vor.u32 %v55120, %v55121 (stack75)
        %v55123 = vxor.u32 %v55118, %v55122 (stack76)
        %v55126 = vadd.s32 %v55118, %v55123 (stack65)
        %v55130 = vadd.s32 %v55126, %v8 (stack65)
        %v55132 = vshll.u32 %v55123, 24 (stack73)
        %v55133 = vshrl.u32 %v55123, 8 (stack74)
        %v55134 = vor.u32 %v55132, %v55133 (stack75)
        %v55135 = vxor.u32 %v55126, %v55134 (stack76)
        %v55138 = vadd.s32 %v55135, %v10 (stack65)
        %v55142 = vadd.s32 %v55138, 2 (stack65)
        %v55146 = vadd.s32 %v55130, %v55142 (stack65)
        %v55148 = vshll.u32 %v55142, 13 (stack73)
        %v55149 = vshrl.u32 %v55142, 19 (stack74)
        %v55150 = vor.u32 %v55148, %v55149 (stack75)
        %v55151 = vxor.u32 %v55146, %v55150 (stack76)
        %v55154 = vadd.s32 %v55146, %v55151 (stack65)
        %v55156 = vshll.u32 %v55151, 15 (stack73)
        %v55157 = vshrl.u32 %v55151, 17 (stack74)
        %v55158 = vor.u32 %v55156, %v55157 (stack75)
        %v55159 = vxor.u32 %v55154, %v55158 (stack76)
        %v55162 = vadd.s32 %v55154, %v55159 (stack65)
        %v55164 = vshll.u32 %v55159, 26 (stack73)
        %v55165 = vshrl.u32 %v55159, 6 (stack74)
        %v55166 = vor.u32 %v55164, %v55165 (stack75)
        %v55167 = vxor.u32 %v55162, %v55166 (stack76)
        %v55170 = vadd.s32 %v55162, %v55167 (stack65)
        %v55174 = vadd.s32 %v55170, %v10 (stack65)
        %v55176 = vshll.u32 %v55167, 6 (stack73)
        %v55177 = vshrl.u32 %v55167, 26 (stack74)
        %v55178 = vor.u32 %v55176, %v55177 (stack75)
        %v55179 = vxor.u32 %v55170, %v55178 (stack76)
        %v55182 = vadd.s32 %v55179, %v9 (stack65)
        %v55186 = vadd.s32 %v55182, 3 (stack65)
        %v55190 = vadd.s32 %v55174, %v55186 (stack65)
        %v55192 = vshll.u32 %v55186, 17 (stack73)
        %v55193 = vshrl.u32 %v55186, 15 (stack74)
        %v55194 = vor.u32 %v55192, %v55193 (stack75)
        %v55195 = vxor.u32 %v55190, %v55194 (stack76)
        %v55198 = vadd.s32 %v55190, %v55195 (stack65)
        %v55200 = vshll.u32 %v55195, 29 (stack73)
        %v55201 = vshrl.u32 %v55195, 3 (stack74)
        %v55202 = vor.u32 %v55200, %v55201 (stack75)
        %v55203 = vxor.u32 %v55198, %v55202 (stack76)
        %v55206 = vadd.s32 %v55198, %v55203 (stack65)
        %v55208 = vshll.u32 %v55203, 16 (stack73)
        %v55209 = vshrl.u32 %v55203, 16 (stack74)
        %v55210 = vor.u32 %v55208, %v55209 (stack75)
        %v55211 = vxor.u32 %v55206, %v55210 (stack76)
        %v55214 = vadd.s32 %v55206, %v55211 (stack65)
        %v55218 = vadd.s32 %v55214, %v9 (stack65)
        %v55220 = vshll.u32 %v55211, 24 (stack73)
        %v55221 = vshrl.u32 %v55211, 8 (stack74)
        %v55222 = vor.u32 %v55220, %v55221 (stack75)
        %v55223 = vxor.u32 %v55214, %v55222 (stack76)
        %v55226 = vadd.s32 %v55223, %v8 (stack65)
        %v55230 = vadd.s32 %v55226, 4 (stack65)
        %v55234 = vadd.s32 %v55218, %v55230 (stack65)
        %v55236 = vshll.u32 %v55230, 13 (stack73)
        %v55237 = vshrl.u32 %v55230, 19 (stack74)
        %v55238 = vor.u32 %v55236, %v55237 (stack75)
        %v55239 = vxor.u32 %v55234, %v55238 (stack76)
        %v55242 = vadd.s32 %v55234, %v55239 (stack65)
        %v55244 = vshll.u32 %v55239, 15 (stack73)
        %v55245 = vshrl.u32 %v55239, 17 (stack74)
        %v55246 = vor.u32 %v55244, %v55245 (stack75)
        %v55247 = vxor.u32 %v55242, %v55246 (stack76)
        %v55250 = vadd.s32 %v55242, %v55247 (stack65)
        %v55252 = vshll.u32 %v55247, 26 (stack73)
        %v55253 = vshrl.u32 %v55247, 6 (stack74)
        %v55254 = vor.u32 %v55252, %v55253 (stack75)
        %v55255 = vxor.u32 %v55250, %v55254 (stack76)
        %v55258 = vadd.s32 %v55250, %v55255 (stack65)
        %v55262 = vadd.s32 %v55258, %v8 (stack65)
        %v55264 = vshll.u32 %v55255, 6 (stack73)
        %v55265 = vshrl.u32 %v55255, 26 (stack74)
        %v55266 = vor.u32 %v55264, %v55265 (stack75)
        %v55267 = vxor.u32 %v55258, %v55266 (stack76)
        %v55270 = vadd.s32 %v55267, %v10 (stack65)
        %v55274 = vadd.s32 %v55270, 5 (stack65)
        %v55276 = vxor.u32 %v55262, %v55274 (stack76)
        %v55277 = vand.u32.u8 %v55276, 255 (stack77)
        %v55278 = vand.u32 %v55277, 65535 (stack78)
        %v55279 = vshrl.u32 %v55278, 1 (stack79)
        %v55280 = vor.u32 %v55279, 16256 (stack75)
        %v55281 = vand.u32.u16 %v55280, 65535 (stack80)
        %v55282 = vunpack.i.l.bf16 %v55281 (stack81)
        %v55286 = vadd.f32 %v55282, -1.0 (stack82)
        %v55290 = vmul.f32 %v55286, 2.0 (stack83)
        %v55294 = vadd.f32 %v55290, -0.99609375 (stack82)
        %v55298 = vmax.f32 -0.99609375, %v55294 (stack84)
        %v55300 = vand.u32 2147483647, %v55298 (stack85)
        %vm55303 = vcmp.eq.f32.partialorder %v55300, 1.0 (stack86)
        %v55308 = vmul.f32 %v55298, inf (stack83)
        %v55310 = vxor.u32 %v55298, 2147483648 (stack87)
        %v55313 = vmul.f32 %v55298, %v55310 (stack83)
        %v55315 = vadd.f32 %v55313, 1.0 (stack88)
        %v55316 = vlog2.pop %v55315 (stack89)
        %v55317 = vmul.f32 %v55316, 0.6931472 (stack90)
        %v55318 = vmul.f32 -0.5, %v55313 (stack91)
        %v55319 = vadd.f32 %v55318, 1.0 (stack92)
        %v55320 = vmul.f32 %v55319, %v55313 (stack93)
        %v55321 = vand.u32 2147483647, %v55313 (stack94)
        %vm55322 = vcmp.lt.f32.partialorder %v55321, 0.0004427343 (stack95)
        %v55323 = vsel /*vm=*/%vm55322, /*on_true_vy=*/%v55320, /*on_false_vx=*/%v55317 (stack96)
        %v55324 = vxor.u32 %v55323, 2147483648 (stack87)
        %vm55327 = vcmp.lt.f32.partialorder %v55324, 5.0 (stack86)
        %v55332 = vsel /*vm=*/%vm55327, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v55336 = vsel /*vm=*/%vm55327, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v55340 = vsel /*vm=*/%vm55327, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v55344 = vsel /*vm=*/%vm55327, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v55348 = vsel /*vm=*/%vm55327, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v55352 = vsel /*vm=*/%vm55327, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v55356 = vsel /*vm=*/%vm55327, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v55360 = vsel /*vm=*/%vm55327, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v55364 = vsel /*vm=*/%vm55327, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v55368 = vadd.f32 %v55324, -2.5 (stack82)
        %v55370 = vrsqrt.pop %v55324 (stack97)
        %v55371 = vmul.f32 %v55324, %v55370 (stack98)
        %vm55372 = vcmp.eq.f32.partialorder %v55324, inf (stack99)
        %v55373 = vsel /*vm=*/%vm55372, /*on_true_vy=*/%v55324, /*on_false_vx=*/%v55371 (stack100)
        %vm55374 = vcmp.eq.f32.partialorder %v55324, 0.0 (stack101)
        %v55375 = vand.u32 %v55324, 2147483648 (stack102)
        %v55376 = vsel /*vm=*/%vm55374, /*on_true_vy=*/%v55375, /*on_false_vx=*/%v55373 (stack103)
        %v55379 = vadd.f32 %v55376, -3.0 (stack82)
        %v55383 = vsel /*vm=*/%vm55327, /*on_true_vy=*/%v55368, /*on_false_vx=*/%v55379 (stack72)
        %v55387 = vmul.f32 %v55364, %v55383 (stack83)
        %v55391 = vadd.f32 %v55360, %v55387 (stack82)
        %v55395 = vmul.f32 %v55391, %v55383 (stack83)
        %v55399 = vadd.f32 %v55356, %v55395 (stack82)
        %v55403 = vmul.f32 %v55399, %v55383 (stack83)
        %v55407 = vadd.f32 %v55352, %v55403 (stack82)
        %v55411 = vmul.f32 %v55407, %v55383 (stack83)
        %v55415 = vadd.f32 %v55348, %v55411 (stack82)
        %v55419 = vmul.f32 %v55415, %v55383 (stack83)
        %v55423 = vadd.f32 %v55344, %v55419 (stack82)
        %v55427 = vmul.f32 %v55423, %v55383 (stack83)
        %v55431 = vadd.f32 %v55340, %v55427 (stack82)
        %v55435 = vmul.f32 %v55431, %v55383 (stack83)
        %v55439 = vadd.f32 %v55336, %v55435 (stack82)
        %v55443 = vmul.f32 %v55439, %v55383 (stack83)
        %v55447 = vadd.f32 %v55332, %v55443 (stack82)
        %v55451 = vmul.f32 %v55447, %v55298 (stack83)
        %v55455 = vsel /*vm=*/%vm55303, /*on_true_vy=*/%v55308, /*on_false_vx=*/%v55451 (stack72)
        %v55459 = vmul.f32 %v55455, 1.4140625 (stack83)
        %s55461 = scalar_lea.vmem %s280, 696 [#allocation0] (stack107)
        %v55462 = vpack.c.bf16 0.0, %v55459 (stack104)
        %55463 = vst [vmem:[%s55461] sm:$0xf] /*vst_source=*/%v55462 (stack105)
        %v55466 = vadd.s32 %v3329, %v52697 (stack65)
        %s55468 = smul.u32 128, %s27 (stack66)
        %v55469 = vlaneseq (stack67)
        %v55470 = vand.u32 %v55469, 127 (stack68)
        %v55471 = vstv %s55468 (stack69)
        %v55472 = vadd.s32 %v55470, %v55471 (stack70)
        %v55476 = vadd.s32 %v55466, %v55472 (stack65)
        %vm55480 = vcmp.lt.u32.totalorder %v55476, %v55466 (stack71)
        %vm55485 = vcmp.lt.u32.totalorder %v55466, %v3329 (stack71)
        %v55490 = vadd.s32 %v3316, %v52680 (stack65)
        %v55494 = vadd.s32 %v55490, 1 (stack65)
        %v55498 = vsel /*vm=*/%vm55485, /*on_true_vy=*/%v55494, /*on_false_vx=*/%v55490 (stack72)
        %v55502 = vadd.s32 %v55498, 1 (stack65)
        %v55506 = vsel /*vm=*/%vm55480, /*on_true_vy=*/%v55502, /*on_false_vx=*/%v55498 (stack72)
        %v55511 = vadd.s32 %v55506, %v10 (stack65)
        %v55515 = vadd.s32 %v55476, %v9 (stack65)
        %v55519 = vadd.s32 %v55511, %v55515 (stack65)
        %v55521 = vshll.u32 %v55515, 13 (stack73)
        %v55522 = vshrl.u32 %v55515, 19 (stack74)
        %v55523 = vor.u32 %v55521, %v55522 (stack75)
        %v55524 = vxor.u32 %v55519, %v55523 (stack76)
        %v55527 = vadd.s32 %v55519, %v55524 (stack65)
        %v55529 = vshll.u32 %v55524, 15 (stack73)
        %v55530 = vshrl.u32 %v55524, 17 (stack74)
        %v55531 = vor.u32 %v55529, %v55530 (stack75)
        %v55532 = vxor.u32 %v55527, %v55531 (stack76)
        %v55535 = vadd.s32 %v55527, %v55532 (stack65)
        %v55537 = vshll.u32 %v55532, 26 (stack73)
        %v55538 = vshrl.u32 %v55532, 6 (stack74)
        %v55539 = vor.u32 %v55537, %v55538 (stack75)
        %v55540 = vxor.u32 %v55535, %v55539 (stack76)
        %v55543 = vadd.s32 %v55535, %v55540 (stack65)
        %v55547 = vadd.s32 %v55543, %v9 (stack65)
        %v55549 = vshll.u32 %v55540, 6 (stack73)
        %v55550 = vshrl.u32 %v55540, 26 (stack74)
        %v55551 = vor.u32 %v55549, %v55550 (stack75)
        %v55552 = vxor.u32 %v55543, %v55551 (stack76)
        %v55555 = vadd.s32 %v55552, %v8 (stack65)
        %v55559 = vadd.s32 %v55555, 1 (stack65)
        %v55563 = vadd.s32 %v55547, %v55559 (stack65)
        %v55565 = vshll.u32 %v55559, 17 (stack73)
        %v55566 = vshrl.u32 %v55559, 15 (stack74)
        %v55567 = vor.u32 %v55565, %v55566 (stack75)
        %v55568 = vxor.u32 %v55563, %v55567 (stack76)
        %v55571 = vadd.s32 %v55563, %v55568 (stack65)
        %v55573 = vshll.u32 %v55568, 29 (stack73)
        %v55574 = vshrl.u32 %v55568, 3 (stack74)
        %v55575 = vor.u32 %v55573, %v55574 (stack75)
        %v55576 = vxor.u32 %v55571, %v55575 (stack76)
        %v55579 = vadd.s32 %v55571, %v55576 (stack65)
        %v55581 = vshll.u32 %v55576, 16 (stack73)
        %v55582 = vshrl.u32 %v55576, 16 (stack74)
        %v55583 = vor.u32 %v55581, %v55582 (stack75)
        %v55584 = vxor.u32 %v55579, %v55583 (stack76)
        %v55587 = vadd.s32 %v55579, %v55584 (stack65)
        %v55591 = vadd.s32 %v55587, %v8 (stack65)
        %v55593 = vshll.u32 %v55584, 24 (stack73)
        %v55594 = vshrl.u32 %v55584, 8 (stack74)
        %v55595 = vor.u32 %v55593, %v55594 (stack75)
        %v55596 = vxor.u32 %v55587, %v55595 (stack76)
        %v55599 = vadd.s32 %v55596, %v10 (stack65)
        %v55603 = vadd.s32 %v55599, 2 (stack65)
        %v55607 = vadd.s32 %v55591, %v55603 (stack65)
        %v55609 = vshll.u32 %v55603, 13 (stack73)
        %v55610 = vshrl.u32 %v55603, 19 (stack74)
        %v55611 = vor.u32 %v55609, %v55610 (stack75)
        %v55612 = vxor.u32 %v55607, %v55611 (stack76)
        %v55615 = vadd.s32 %v55607, %v55612 (stack65)
        %v55617 = vshll.u32 %v55612, 15 (stack73)
        %v55618 = vshrl.u32 %v55612, 17 (stack74)
        %v55619 = vor.u32 %v55617, %v55618 (stack75)
        %v55620 = vxor.u32 %v55615, %v55619 (stack76)
        %v55623 = vadd.s32 %v55615, %v55620 (stack65)
        %v55625 = vshll.u32 %v55620, 26 (stack73)
        %v55626 = vshrl.u32 %v55620, 6 (stack74)
        %v55627 = vor.u32 %v55625, %v55626 (stack75)
        %v55628 = vxor.u32 %v55623, %v55627 (stack76)
        %v55631 = vadd.s32 %v55623, %v55628 (stack65)
        %v55635 = vadd.s32 %v55631, %v10 (stack65)
        %v55637 = vshll.u32 %v55628, 6 (stack73)
        %v55638 = vshrl.u32 %v55628, 26 (stack74)
        %v55639 = vor.u32 %v55637, %v55638 (stack75)
        %v55640 = vxor.u32 %v55631, %v55639 (stack76)
        %v55643 = vadd.s32 %v55640, %v9 (stack65)
        %v55647 = vadd.s32 %v55643, 3 (stack65)
        %v55651 = vadd.s32 %v55635, %v55647 (stack65)
        %v55653 = vshll.u32 %v55647, 17 (stack73)
        %v55654 = vshrl.u32 %v55647, 15 (stack74)
        %v55655 = vor.u32 %v55653, %v55654 (stack75)
        %v55656 = vxor.u32 %v55651, %v55655 (stack76)
        %v55659 = vadd.s32 %v55651, %v55656 (stack65)
        %v55661 = vshll.u32 %v55656, 29 (stack73)
        %v55662 = vshrl.u32 %v55656, 3 (stack74)
        %v55663 = vor.u32 %v55661, %v55662 (stack75)
        %v55664 = vxor.u32 %v55659, %v55663 (stack76)
        %v55667 = vadd.s32 %v55659, %v55664 (stack65)
        %v55669 = vshll.u32 %v55664, 16 (stack73)
        %v55670 = vshrl.u32 %v55664, 16 (stack74)
        %v55671 = vor.u32 %v55669, %v55670 (stack75)
        %v55672 = vxor.u32 %v55667, %v55671 (stack76)
        %v55675 = vadd.s32 %v55667, %v55672 (stack65)
        %v55679 = vadd.s32 %v55675, %v9 (stack65)
        %v55681 = vshll.u32 %v55672, 24 (stack73)
        %v55682 = vshrl.u32 %v55672, 8 (stack74)
        %v55683 = vor.u32 %v55681, %v55682 (stack75)
        %v55684 = vxor.u32 %v55675, %v55683 (stack76)
        %v55687 = vadd.s32 %v55684, %v8 (stack65)
        %v55691 = vadd.s32 %v55687, 4 (stack65)
        %v55695 = vadd.s32 %v55679, %v55691 (stack65)
        %v55697 = vshll.u32 %v55691, 13 (stack73)
        %v55698 = vshrl.u32 %v55691, 19 (stack74)
        %v55699 = vor.u32 %v55697, %v55698 (stack75)
        %v55700 = vxor.u32 %v55695, %v55699 (stack76)
        %v55703 = vadd.s32 %v55695, %v55700 (stack65)
        %v55705 = vshll.u32 %v55700, 15 (stack73)
        %v55706 = vshrl.u32 %v55700, 17 (stack74)
        %v55707 = vor.u32 %v55705, %v55706 (stack75)
        %v55708 = vxor.u32 %v55703, %v55707 (stack76)
        %v55711 = vadd.s32 %v55703, %v55708 (stack65)
        %v55713 = vshll.u32 %v55708, 26 (stack73)
        %v55714 = vshrl.u32 %v55708, 6 (stack74)
        %v55715 = vor.u32 %v55713, %v55714 (stack75)
        %v55716 = vxor.u32 %v55711, %v55715 (stack76)
        %v55719 = vadd.s32 %v55711, %v55716 (stack65)
        %v55723 = vadd.s32 %v55719, %v8 (stack65)
        %v55725 = vshll.u32 %v55716, 6 (stack73)
        %v55726 = vshrl.u32 %v55716, 26 (stack74)
        %v55727 = vor.u32 %v55725, %v55726 (stack75)
        %v55728 = vxor.u32 %v55719, %v55727 (stack76)
        %v55731 = vadd.s32 %v55728, %v10 (stack65)
        %v55735 = vadd.s32 %v55731, 5 (stack65)
        %v55737 = vxor.u32 %v55723, %v55735 (stack76)
        %v55738 = vand.u32.u8 %v55737, 255 (stack77)
        %v55739 = vand.u32 %v55738, 65535 (stack78)
        %v55740 = vshrl.u32 %v55739, 1 (stack79)
        %v55741 = vor.u32 %v55740, 16256 (stack75)
        %v55742 = vand.u32.u16 %v55741, 65535 (stack80)
        %v55743 = vunpack.i.l.bf16 %v55742 (stack81)
        %v55747 = vadd.f32 %v55743, -1.0 (stack82)
        %v55751 = vmul.f32 %v55747, 2.0 (stack83)
        %v55755 = vadd.f32 %v55751, -0.99609375 (stack82)
        %v55759 = vmax.f32 -0.99609375, %v55755 (stack84)
        %v55761 = vand.u32 2147483647, %v55759 (stack85)
        %vm55764 = vcmp.eq.f32.partialorder %v55761, 1.0 (stack86)
        %v55769 = vmul.f32 %v55759, inf (stack83)
        %v55771 = vxor.u32 %v55759, 2147483648 (stack87)
        %v55774 = vmul.f32 %v55759, %v55771 (stack83)
        %v55776 = vadd.f32 %v55774, 1.0 (stack88)
        %v55777 = vlog2.pop %v55776 (stack89)
        %v55778 = vmul.f32 %v55777, 0.6931472 (stack90)
        %v55779 = vmul.f32 -0.5, %v55774 (stack91)
        %v55780 = vadd.f32 %v55779, 1.0 (stack92)
        %v55781 = vmul.f32 %v55780, %v55774 (stack93)
        %v55782 = vand.u32 2147483647, %v55774 (stack94)
        %vm55783 = vcmp.lt.f32.partialorder %v55782, 0.0004427343 (stack95)
        %v55784 = vsel /*vm=*/%vm55783, /*on_true_vy=*/%v55781, /*on_false_vx=*/%v55778 (stack96)
        %v55785 = vxor.u32 %v55784, 2147483648 (stack87)
        %vm55788 = vcmp.lt.f32.partialorder %v55785, 5.0 (stack86)
        %v55793 = vsel /*vm=*/%vm55788, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v55797 = vsel /*vm=*/%vm55788, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v55801 = vsel /*vm=*/%vm55788, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v55805 = vsel /*vm=*/%vm55788, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v55809 = vsel /*vm=*/%vm55788, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v55813 = vsel /*vm=*/%vm55788, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v55817 = vsel /*vm=*/%vm55788, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v55821 = vsel /*vm=*/%vm55788, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v55825 = vsel /*vm=*/%vm55788, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v55829 = vadd.f32 %v55785, -2.5 (stack82)
        %v55831 = vrsqrt.pop %v55785 (stack97)
        %v55832 = vmul.f32 %v55785, %v55831 (stack98)
        %vm55833 = vcmp.eq.f32.partialorder %v55785, inf (stack99)
        %v55834 = vsel /*vm=*/%vm55833, /*on_true_vy=*/%v55785, /*on_false_vx=*/%v55832 (stack100)
        %vm55835 = vcmp.eq.f32.partialorder %v55785, 0.0 (stack101)
        %v55836 = vand.u32 %v55785, 2147483648 (stack102)
        %v55837 = vsel /*vm=*/%vm55835, /*on_true_vy=*/%v55836, /*on_false_vx=*/%v55834 (stack103)
        %v55840 = vadd.f32 %v55837, -3.0 (stack82)
        %v55844 = vsel /*vm=*/%vm55788, /*on_true_vy=*/%v55829, /*on_false_vx=*/%v55840 (stack72)
        %v55848 = vmul.f32 %v55825, %v55844 (stack83)
        %v55852 = vadd.f32 %v55821, %v55848 (stack82)
        %v55856 = vmul.f32 %v55852, %v55844 (stack83)
        %v55860 = vadd.f32 %v55817, %v55856 (stack82)
        %v55864 = vmul.f32 %v55860, %v55844 (stack83)
        %v55868 = vadd.f32 %v55813, %v55864 (stack82)
        %v55872 = vmul.f32 %v55868, %v55844 (stack83)
        %v55876 = vadd.f32 %v55809, %v55872 (stack82)
        %v55880 = vmul.f32 %v55876, %v55844 (stack83)
        %v55884 = vadd.f32 %v55805, %v55880 (stack82)
        %v55888 = vmul.f32 %v55884, %v55844 (stack83)
        %v55892 = vadd.f32 %v55801, %v55888 (stack82)
        %v55896 = vmul.f32 %v55892, %v55844 (stack83)
        %v55900 = vadd.f32 %v55797, %v55896 (stack82)
        %v55904 = vmul.f32 %v55900, %v55844 (stack83)
        %v55908 = vadd.f32 %v55793, %v55904 (stack82)
        %v55912 = vmul.f32 %v55908, %v55759 (stack83)
        %v55916 = vsel /*vm=*/%vm55764, /*on_true_vy=*/%v55769, /*on_false_vx=*/%v55912 (stack72)
        %v55920 = vmul.f32 %v55916, 1.4140625 (stack83)
        %s55922 = scalar_lea.vmem %s280, 824 [#allocation0] (stack107)
        %v55923 = vpack.c.bf16 0.0, %v55920 (stack104)
        %55924 = vst [vmem:[%s55922] sm:$0xf] /*vst_source=*/%v55923 (stack105)
        %v55927 = vadd.s32 %v3816, %v52697 (stack65)
        %s55929 = smul.u32 128, %s27 (stack66)
        %v55930 = vlaneseq (stack67)
        %v55931 = vand.u32 %v55930, 127 (stack68)
        %v55932 = vstv %s55929 (stack69)
        %v55933 = vadd.s32 %v55931, %v55932 (stack70)
        %v55937 = vadd.s32 %v55927, %v55933 (stack65)
        %vm55941 = vcmp.lt.u32.totalorder %v55937, %v55927 (stack71)
        %vm55946 = vcmp.lt.u32.totalorder %v55927, %v3816 (stack71)
        %v55951 = vadd.s32 %v3803, %v52680 (stack65)
        %v55955 = vadd.s32 %v55951, 1 (stack65)
        %v55959 = vsel /*vm=*/%vm55946, /*on_true_vy=*/%v55955, /*on_false_vx=*/%v55951 (stack72)
        %v55963 = vadd.s32 %v55959, 1 (stack65)
        %v55967 = vsel /*vm=*/%vm55941, /*on_true_vy=*/%v55963, /*on_false_vx=*/%v55959 (stack72)
        %v55972 = vadd.s32 %v55967, %v10 (stack65)
        %v55976 = vadd.s32 %v55937, %v9 (stack65)
        %v55980 = vadd.s32 %v55972, %v55976 (stack65)
        %v55982 = vshll.u32 %v55976, 13 (stack73)
        %v55983 = vshrl.u32 %v55976, 19 (stack74)
        %v55984 = vor.u32 %v55982, %v55983 (stack75)
        %v55985 = vxor.u32 %v55980, %v55984 (stack76)
        %v55988 = vadd.s32 %v55980, %v55985 (stack65)
        %v55990 = vshll.u32 %v55985, 15 (stack73)
        %v55991 = vshrl.u32 %v55985, 17 (stack74)
        %v55992 = vor.u32 %v55990, %v55991 (stack75)
        %v55993 = vxor.u32 %v55988, %v55992 (stack76)
        %v55996 = vadd.s32 %v55988, %v55993 (stack65)
        %v55998 = vshll.u32 %v55993, 26 (stack73)
        %v55999 = vshrl.u32 %v55993, 6 (stack74)
        %v56000 = vor.u32 %v55998, %v55999 (stack75)
        %v56001 = vxor.u32 %v55996, %v56000 (stack76)
        %v56004 = vadd.s32 %v55996, %v56001 (stack65)
        %v56008 = vadd.s32 %v56004, %v9 (stack65)
        %v56010 = vshll.u32 %v56001, 6 (stack73)
        %v56011 = vshrl.u32 %v56001, 26 (stack74)
        %v56012 = vor.u32 %v56010, %v56011 (stack75)
        %v56013 = vxor.u32 %v56004, %v56012 (stack76)
        %v56016 = vadd.s32 %v56013, %v8 (stack65)
        %v56020 = vadd.s32 %v56016, 1 (stack65)
        %v56024 = vadd.s32 %v56008, %v56020 (stack65)
        %v56026 = vshll.u32 %v56020, 17 (stack73)
        %v56027 = vshrl.u32 %v56020, 15 (stack74)
        %v56028 = vor.u32 %v56026, %v56027 (stack75)
        %v56029 = vxor.u32 %v56024, %v56028 (stack76)
        %v56032 = vadd.s32 %v56024, %v56029 (stack65)
        %v56034 = vshll.u32 %v56029, 29 (stack73)
        %v56035 = vshrl.u32 %v56029, 3 (stack74)
        %v56036 = vor.u32 %v56034, %v56035 (stack75)
        %v56037 = vxor.u32 %v56032, %v56036 (stack76)
        %v56040 = vadd.s32 %v56032, %v56037 (stack65)
        %v56042 = vshll.u32 %v56037, 16 (stack73)
        %v56043 = vshrl.u32 %v56037, 16 (stack74)
        %v56044 = vor.u32 %v56042, %v56043 (stack75)
        %v56045 = vxor.u32 %v56040, %v56044 (stack76)
        %v56048 = vadd.s32 %v56040, %v56045 (stack65)
        %v56052 = vadd.s32 %v56048, %v8 (stack65)
        %v56054 = vshll.u32 %v56045, 24 (stack73)
        %v56055 = vshrl.u32 %v56045, 8 (stack74)
        %v56056 = vor.u32 %v56054, %v56055 (stack75)
        %v56057 = vxor.u32 %v56048, %v56056 (stack76)
        %v56060 = vadd.s32 %v56057, %v10 (stack65)
        %v56064 = vadd.s32 %v56060, 2 (stack65)
        %v56068 = vadd.s32 %v56052, %v56064 (stack65)
        %v56070 = vshll.u32 %v56064, 13 (stack73)
        %v56071 = vshrl.u32 %v56064, 19 (stack74)
        %v56072 = vor.u32 %v56070, %v56071 (stack75)
        %v56073 = vxor.u32 %v56068, %v56072 (stack76)
        %v56076 = vadd.s32 %v56068, %v56073 (stack65)
        %v56078 = vshll.u32 %v56073, 15 (stack73)
        %v56079 = vshrl.u32 %v56073, 17 (stack74)
        %v56080 = vor.u32 %v56078, %v56079 (stack75)
        %v56081 = vxor.u32 %v56076, %v56080 (stack76)
        %v56084 = vadd.s32 %v56076, %v56081 (stack65)
        %v56086 = vshll.u32 %v56081, 26 (stack73)
        %v56087 = vshrl.u32 %v56081, 6 (stack74)
        %v56088 = vor.u32 %v56086, %v56087 (stack75)
        %v56089 = vxor.u32 %v56084, %v56088 (stack76)
        %v56092 = vadd.s32 %v56084, %v56089 (stack65)
        %v56096 = vadd.s32 %v56092, %v10 (stack65)
        %v56098 = vshll.u32 %v56089, 6 (stack73)
        %v56099 = vshrl.u32 %v56089, 26 (stack74)
        %v56100 = vor.u32 %v56098, %v56099 (stack75)
        %v56101 = vxor.u32 %v56092, %v56100 (stack76)
        %v56104 = vadd.s32 %v56101, %v9 (stack65)
        %v56108 = vadd.s32 %v56104, 3 (stack65)
        %v56112 = vadd.s32 %v56096, %v56108 (stack65)
        %v56114 = vshll.u32 %v56108, 17 (stack73)
        %v56115 = vshrl.u32 %v56108, 15 (stack74)
        %v56116 = vor.u32 %v56114, %v56115 (stack75)
        %v56117 = vxor.u32 %v56112, %v56116 (stack76)
        %v56120 = vadd.s32 %v56112, %v56117 (stack65)
        %v56122 = vshll.u32 %v56117, 29 (stack73)
        %v56123 = vshrl.u32 %v56117, 3 (stack74)
        %v56124 = vor.u32 %v56122, %v56123 (stack75)
        %v56125 = vxor.u32 %v56120, %v56124 (stack76)
        %v56128 = vadd.s32 %v56120, %v56125 (stack65)
        %v56130 = vshll.u32 %v56125, 16 (stack73)
        %v56131 = vshrl.u32 %v56125, 16 (stack74)
        %v56132 = vor.u32 %v56130, %v56131 (stack75)
        %v56133 = vxor.u32 %v56128, %v56132 (stack76)
        %v56136 = vadd.s32 %v56128, %v56133 (stack65)
        %v56140 = vadd.s32 %v56136, %v9 (stack65)
        %v56142 = vshll.u32 %v56133, 24 (stack73)
        %v56143 = vshrl.u32 %v56133, 8 (stack74)
        %v56144 = vor.u32 %v56142, %v56143 (stack75)
        %v56145 = vxor.u32 %v56136, %v56144 (stack76)
        %v56148 = vadd.s32 %v56145, %v8 (stack65)
        %v56152 = vadd.s32 %v56148, 4 (stack65)
        %v56156 = vadd.s32 %v56140, %v56152 (stack65)
        %v56158 = vshll.u32 %v56152, 13 (stack73)
        %v56159 = vshrl.u32 %v56152, 19 (stack74)
        %v56160 = vor.u32 %v56158, %v56159 (stack75)
        %v56161 = vxor.u32 %v56156, %v56160 (stack76)
        %v56164 = vadd.s32 %v56156, %v56161 (stack65)
        %v56166 = vshll.u32 %v56161, 15 (stack73)
        %v56167 = vshrl.u32 %v56161, 17 (stack74)
        %v56168 = vor.u32 %v56166, %v56167 (stack75)
        %v56169 = vxor.u32 %v56164, %v56168 (stack76)
        %v56172 = vadd.s32 %v56164, %v56169 (stack65)
        %v56174 = vshll.u32 %v56169, 26 (stack73)
        %v56175 = vshrl.u32 %v56169, 6 (stack74)
        %v56176 = vor.u32 %v56174, %v56175 (stack75)
        %v56177 = vxor.u32 %v56172, %v56176 (stack76)
        %v56180 = vadd.s32 %v56172, %v56177 (stack65)
        %v56184 = vadd.s32 %v56180, %v8 (stack65)
        %v56186 = vshll.u32 %v56177, 6 (stack73)
        %v56187 = vshrl.u32 %v56177, 26 (stack74)
        %v56188 = vor.u32 %v56186, %v56187 (stack75)
        %v56189 = vxor.u32 %v56180, %v56188 (stack76)
        %v56192 = vadd.s32 %v56189, %v10 (stack65)
        %v56196 = vadd.s32 %v56192, 5 (stack65)
        %v56198 = vxor.u32 %v56184, %v56196 (stack76)
        %v56199 = vand.u32.u8 %v56198, 255 (stack77)
        %v56200 = vand.u32 %v56199, 65535 (stack78)
        %v56201 = vshrl.u32 %v56200, 1 (stack79)
        %v56202 = vor.u32 %v56201, 16256 (stack75)
        %v56203 = vand.u32.u16 %v56202, 65535 (stack80)
        %v56204 = vunpack.i.l.bf16 %v56203 (stack81)
        %v56208 = vadd.f32 %v56204, -1.0 (stack82)
        %v56212 = vmul.f32 %v56208, 2.0 (stack83)
        %v56216 = vadd.f32 %v56212, -0.99609375 (stack82)
        %v56220 = vmax.f32 -0.99609375, %v56216 (stack84)
        %v56222 = vand.u32 2147483647, %v56220 (stack85)
        %vm56225 = vcmp.eq.f32.partialorder %v56222, 1.0 (stack86)
        %v56230 = vmul.f32 %v56220, inf (stack83)
        %v56232 = vxor.u32 %v56220, 2147483648 (stack87)
        %v56235 = vmul.f32 %v56220, %v56232 (stack83)
        %v56237 = vadd.f32 %v56235, 1.0 (stack88)
        %v56238 = vlog2.pop %v56237 (stack89)
        %v56239 = vmul.f32 %v56238, 0.6931472 (stack90)
        %v56240 = vmul.f32 -0.5, %v56235 (stack91)
        %v56241 = vadd.f32 %v56240, 1.0 (stack92)
        %v56242 = vmul.f32 %v56241, %v56235 (stack93)
        %v56243 = vand.u32 2147483647, %v56235 (stack94)
        %vm56244 = vcmp.lt.f32.partialorder %v56243, 0.0004427343 (stack95)
        %v56245 = vsel /*vm=*/%vm56244, /*on_true_vy=*/%v56242, /*on_false_vx=*/%v56239 (stack96)
        %v56246 = vxor.u32 %v56245, 2147483648 (stack87)
        %vm56249 = vcmp.lt.f32.partialorder %v56246, 5.0 (stack86)
        %v56254 = vsel /*vm=*/%vm56249, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v56258 = vsel /*vm=*/%vm56249, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v56262 = vsel /*vm=*/%vm56249, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v56266 = vsel /*vm=*/%vm56249, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v56270 = vsel /*vm=*/%vm56249, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v56274 = vsel /*vm=*/%vm56249, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v56278 = vsel /*vm=*/%vm56249, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v56282 = vsel /*vm=*/%vm56249, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v56286 = vsel /*vm=*/%vm56249, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v56290 = vadd.f32 %v56246, -2.5 (stack82)
        %v56292 = vrsqrt.pop %v56246 (stack97)
        %v56293 = vmul.f32 %v56246, %v56292 (stack98)
        %vm56294 = vcmp.eq.f32.partialorder %v56246, inf (stack99)
        %v56295 = vsel /*vm=*/%vm56294, /*on_true_vy=*/%v56246, /*on_false_vx=*/%v56293 (stack100)
        %vm56296 = vcmp.eq.f32.partialorder %v56246, 0.0 (stack101)
        %v56297 = vand.u32 %v56246, 2147483648 (stack102)
        %v56298 = vsel /*vm=*/%vm56296, /*on_true_vy=*/%v56297, /*on_false_vx=*/%v56295 (stack103)
        %v56301 = vadd.f32 %v56298, -3.0 (stack82)
        %v56305 = vsel /*vm=*/%vm56249, /*on_true_vy=*/%v56290, /*on_false_vx=*/%v56301 (stack72)
        %v56309 = vmul.f32 %v56286, %v56305 (stack83)
        %v56313 = vadd.f32 %v56282, %v56309 (stack82)
        %v56317 = vmul.f32 %v56313, %v56305 (stack83)
        %v56321 = vadd.f32 %v56278, %v56317 (stack82)
        %v56325 = vmul.f32 %v56321, %v56305 (stack83)
        %v56329 = vadd.f32 %v56274, %v56325 (stack82)
        %v56333 = vmul.f32 %v56329, %v56305 (stack83)
        %v56337 = vadd.f32 %v56270, %v56333 (stack82)
        %v56341 = vmul.f32 %v56337, %v56305 (stack83)
        %v56345 = vadd.f32 %v56266, %v56341 (stack82)
        %v56349 = vmul.f32 %v56345, %v56305 (stack83)
        %v56353 = vadd.f32 %v56262, %v56349 (stack82)
        %v56357 = vmul.f32 %v56353, %v56305 (stack83)
        %v56361 = vadd.f32 %v56258, %v56357 (stack82)
        %v56365 = vmul.f32 %v56361, %v56305 (stack83)
        %v56369 = vadd.f32 %v56254, %v56365 (stack82)
        %v56373 = vmul.f32 %v56369, %v56220 (stack83)
        %v56377 = vsel /*vm=*/%vm56225, /*on_true_vy=*/%v56230, /*on_false_vx=*/%v56373 (stack72)
        %v56381 = vmul.f32 %v56377, 1.4140625 (stack83)
        %s56383 = scalar_lea.vmem %s280, 952 [#allocation0] (stack107)
        %v56384 = vpack.c.bf16 0.0, %v56381 (stack104)
        %56385 = vst [vmem:[%s56383] sm:$0xf] /*vst_source=*/%v56384 (stack105)
        %s56386 = sadd.s32 %s339, 120 (stack106)
        %s56387 = sshrl.u32 %s56386, 10 (stack49)
        %p56388 = scmp.lt.s32.totalorder 1, %s56387 (stack50)
        %s56389 = scalar_select /*predicate=*/%p56388, /*on_true=*/1, /*on_false=*/%s56387 (stack51)
        %s56390 = sand.u32 %s56386, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s56391 = sshrl.u32 %s56390, 7 (stack53)
        %s56392 = sand.u32 %s56390, 127 /* smod.u32 w/div 128 */ (stack54)
        %s56393 = smul.addr %s56389, 8 (stack55)
        %s56394 = scalar_lea.vmem %s3, %s56393 (stack56)
        %s56396 = scalar_lea.vmem %s56394, %s56391 (stack57)
        %v56397 = vld [vmem:[%s56396] ss:$0 sm:$0xff] (stack58)
        %s56398 = sand.u32 %s56392, 255 (stack59)
        %s56400 = sor.u32 256, %s56398 (stack60)
        %56401 = vbcast.lane.b32.xlu0 %v56397, %s56400 (stack61)
        %v56402 = vpop.permute.xlu0 %56401 (stack62)
        %s56403 = sadd.s32 %s347, 120 (stack106)
        %s56404 = sshrl.u32 %s56403, 10 (stack49)
        %p56405 = scmp.lt.s32.totalorder 1, %s56404 (stack50)
        %s56406 = scalar_select /*predicate=*/%p56405, /*on_true=*/1, /*on_false=*/%s56404 (stack51)
        %s56407 = sand.u32 %s56403, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s56408 = sshrl.u32 %s56407, 7 (stack53)
        %s56409 = sand.u32 %s56407, 127 /* smod.u32 w/div 128 */ (stack54)
        %s56410 = smul.addr %s56406, 8 (stack55)
        %s56411 = scalar_lea.vmem %s5, %s56410 (stack56)
        %s56413 = scalar_lea.vmem %s56411, %s56408 (stack57)
        %v56414 = vld [vmem:[%s56413] ss:$0 sm:$0xff] (stack58)
        %s56415 = sand.u32 %s56409, 255 (stack59)
        %s56417 = sor.u32 256, %s56415 (stack60)
        %56418 = vbcast.lane.b32.xlu0 %v56414, %s56417 (stack61)
        %v56419 = vpop.permute.xlu0 %56418 (stack62)
        %v56422 = vadd.s32 %v408, %v56419 (stack65)
        %s56424 = smul.u32 128, %s27 (stack66)
        %v56425 = vlaneseq (stack67)
        %v56426 = vand.u32 %v56425, 127 (stack68)
        %v56427 = vstv %s56424 (stack69)
        %v56428 = vadd.s32 %v56426, %v56427 (stack70)
        %v56432 = vadd.s32 %v56422, %v56428 (stack65)
        %vm56436 = vcmp.lt.u32.totalorder %v56432, %v56422 (stack71)
        %vm56441 = vcmp.lt.u32.totalorder %v56422, %v408 (stack71)
        %v56446 = vadd.s32 %v380, %v56402 (stack65)
        %v56450 = vadd.s32 %v56446, 1 (stack65)
        %v56454 = vsel /*vm=*/%vm56441, /*on_true_vy=*/%v56450, /*on_false_vx=*/%v56446 (stack72)
        %v56458 = vadd.s32 %v56454, 1 (stack65)
        %v56462 = vsel /*vm=*/%vm56436, /*on_true_vy=*/%v56458, /*on_false_vx=*/%v56454 (stack72)
        %v56467 = vadd.s32 %v56462, %v10 (stack65)
        %v56471 = vadd.s32 %v56432, %v9 (stack65)
        %v56475 = vadd.s32 %v56467, %v56471 (stack65)
        %v56477 = vshll.u32 %v56471, 13 (stack73)
        %v56478 = vshrl.u32 %v56471, 19 (stack74)
        %v56479 = vor.u32 %v56477, %v56478 (stack75)
        %v56480 = vxor.u32 %v56475, %v56479 (stack76)
        %v56483 = vadd.s32 %v56475, %v56480 (stack65)
        %v56485 = vshll.u32 %v56480, 15 (stack73)
        %v56486 = vshrl.u32 %v56480, 17 (stack74)
        %v56487 = vor.u32 %v56485, %v56486 (stack75)
        %v56488 = vxor.u32 %v56483, %v56487 (stack76)
        %v56491 = vadd.s32 %v56483, %v56488 (stack65)
        %v56493 = vshll.u32 %v56488, 26 (stack73)
        %v56494 = vshrl.u32 %v56488, 6 (stack74)
        %v56495 = vor.u32 %v56493, %v56494 (stack75)
        %v56496 = vxor.u32 %v56491, %v56495 (stack76)
        %v56499 = vadd.s32 %v56491, %v56496 (stack65)
        %v56503 = vadd.s32 %v56499, %v9 (stack65)
        %v56505 = vshll.u32 %v56496, 6 (stack73)
        %v56506 = vshrl.u32 %v56496, 26 (stack74)
        %v56507 = vor.u32 %v56505, %v56506 (stack75)
        %v56508 = vxor.u32 %v56499, %v56507 (stack76)
        %v56511 = vadd.s32 %v56508, %v8 (stack65)
        %v56515 = vadd.s32 %v56511, 1 (stack65)
        %v56519 = vadd.s32 %v56503, %v56515 (stack65)
        %v56521 = vshll.u32 %v56515, 17 (stack73)
        %v56522 = vshrl.u32 %v56515, 15 (stack74)
        %v56523 = vor.u32 %v56521, %v56522 (stack75)
        %v56524 = vxor.u32 %v56519, %v56523 (stack76)
        %v56527 = vadd.s32 %v56519, %v56524 (stack65)
        %v56529 = vshll.u32 %v56524, 29 (stack73)
        %v56530 = vshrl.u32 %v56524, 3 (stack74)
        %v56531 = vor.u32 %v56529, %v56530 (stack75)
        %v56532 = vxor.u32 %v56527, %v56531 (stack76)
        %v56535 = vadd.s32 %v56527, %v56532 (stack65)
        %v56537 = vshll.u32 %v56532, 16 (stack73)
        %v56538 = vshrl.u32 %v56532, 16 (stack74)
        %v56539 = vor.u32 %v56537, %v56538 (stack75)
        %v56540 = vxor.u32 %v56535, %v56539 (stack76)
        %v56543 = vadd.s32 %v56535, %v56540 (stack65)
        %v56547 = vadd.s32 %v56543, %v8 (stack65)
        %v56549 = vshll.u32 %v56540, 24 (stack73)
        %v56550 = vshrl.u32 %v56540, 8 (stack74)
        %v56551 = vor.u32 %v56549, %v56550 (stack75)
        %v56552 = vxor.u32 %v56543, %v56551 (stack76)
        %v56555 = vadd.s32 %v56552, %v10 (stack65)
        %v56559 = vadd.s32 %v56555, 2 (stack65)
        %v56563 = vadd.s32 %v56547, %v56559 (stack65)
        %v56565 = vshll.u32 %v56559, 13 (stack73)
        %v56566 = vshrl.u32 %v56559, 19 (stack74)
        %v56567 = vor.u32 %v56565, %v56566 (stack75)
        %v56568 = vxor.u32 %v56563, %v56567 (stack76)
        %v56571 = vadd.s32 %v56563, %v56568 (stack65)
        %v56573 = vshll.u32 %v56568, 15 (stack73)
        %v56574 = vshrl.u32 %v56568, 17 (stack74)
        %v56575 = vor.u32 %v56573, %v56574 (stack75)
        %v56576 = vxor.u32 %v56571, %v56575 (stack76)
        %v56579 = vadd.s32 %v56571, %v56576 (stack65)
        %v56581 = vshll.u32 %v56576, 26 (stack73)
        %v56582 = vshrl.u32 %v56576, 6 (stack74)
        %v56583 = vor.u32 %v56581, %v56582 (stack75)
        %v56584 = vxor.u32 %v56579, %v56583 (stack76)
        %v56587 = vadd.s32 %v56579, %v56584 (stack65)
        %v56591 = vadd.s32 %v56587, %v10 (stack65)
        %v56593 = vshll.u32 %v56584, 6 (stack73)
        %v56594 = vshrl.u32 %v56584, 26 (stack74)
        %v56595 = vor.u32 %v56593, %v56594 (stack75)
        %v56596 = vxor.u32 %v56587, %v56595 (stack76)
        %v56599 = vadd.s32 %v56596, %v9 (stack65)
        %v56603 = vadd.s32 %v56599, 3 (stack65)
        %v56607 = vadd.s32 %v56591, %v56603 (stack65)
        %v56609 = vshll.u32 %v56603, 17 (stack73)
        %v56610 = vshrl.u32 %v56603, 15 (stack74)
        %v56611 = vor.u32 %v56609, %v56610 (stack75)
        %v56612 = vxor.u32 %v56607, %v56611 (stack76)
        %v56615 = vadd.s32 %v56607, %v56612 (stack65)
        %v56617 = vshll.u32 %v56612, 29 (stack73)
        %v56618 = vshrl.u32 %v56612, 3 (stack74)
        %v56619 = vor.u32 %v56617, %v56618 (stack75)
        %v56620 = vxor.u32 %v56615, %v56619 (stack76)
        %v56623 = vadd.s32 %v56615, %v56620 (stack65)
        %v56625 = vshll.u32 %v56620, 16 (stack73)
        %v56626 = vshrl.u32 %v56620, 16 (stack74)
        %v56627 = vor.u32 %v56625, %v56626 (stack75)
        %v56628 = vxor.u32 %v56623, %v56627 (stack76)
        %v56631 = vadd.s32 %v56623, %v56628 (stack65)
        %v56635 = vadd.s32 %v56631, %v9 (stack65)
        %v56637 = vshll.u32 %v56628, 24 (stack73)
        %v56638 = vshrl.u32 %v56628, 8 (stack74)
        %v56639 = vor.u32 %v56637, %v56638 (stack75)
        %v56640 = vxor.u32 %v56631, %v56639 (stack76)
        %v56643 = vadd.s32 %v56640, %v8 (stack65)
        %v56647 = vadd.s32 %v56643, 4 (stack65)
        %v56651 = vadd.s32 %v56635, %v56647 (stack65)
        %v56653 = vshll.u32 %v56647, 13 (stack73)
        %v56654 = vshrl.u32 %v56647, 19 (stack74)
        %v56655 = vor.u32 %v56653, %v56654 (stack75)
        %v56656 = vxor.u32 %v56651, %v56655 (stack76)
        %v56659 = vadd.s32 %v56651, %v56656 (stack65)
        %v56661 = vshll.u32 %v56656, 15 (stack73)
        %v56662 = vshrl.u32 %v56656, 17 (stack74)
        %v56663 = vor.u32 %v56661, %v56662 (stack75)
        %v56664 = vxor.u32 %v56659, %v56663 (stack76)
        %v56667 = vadd.s32 %v56659, %v56664 (stack65)
        %v56669 = vshll.u32 %v56664, 26 (stack73)
        %v56670 = vshrl.u32 %v56664, 6 (stack74)
        %v56671 = vor.u32 %v56669, %v56670 (stack75)
        %v56672 = vxor.u32 %v56667, %v56671 (stack76)
        %v56675 = vadd.s32 %v56667, %v56672 (stack65)
        %v56679 = vadd.s32 %v56675, %v8 (stack65)
        %v56681 = vshll.u32 %v56672, 6 (stack73)
        %v56682 = vshrl.u32 %v56672, 26 (stack74)
        %v56683 = vor.u32 %v56681, %v56682 (stack75)
        %v56684 = vxor.u32 %v56675, %v56683 (stack76)
        %v56687 = vadd.s32 %v56684, %v10 (stack65)
        %v56691 = vadd.s32 %v56687, 5 (stack65)
        %v56693 = vxor.u32 %v56679, %v56691 (stack76)
        %v56694 = vand.u32.u8 %v56693, 255 (stack77)
        %v56695 = vand.u32 %v56694, 65535 (stack78)
        %v56696 = vshrl.u32 %v56695, 1 (stack79)
        %v56697 = vor.u32 %v56696, 16256 (stack75)
        %v56698 = vand.u32.u16 %v56697, 65535 (stack80)
        %v56699 = vunpack.i.l.bf16 %v56698 (stack81)
        %v56703 = vadd.f32 %v56699, -1.0 (stack82)
        %v56707 = vmul.f32 %v56703, 2.0 (stack83)
        %v56711 = vadd.f32 %v56707, -0.99609375 (stack82)
        %v56715 = vmax.f32 -0.99609375, %v56711 (stack84)
        %v56717 = vand.u32 2147483647, %v56715 (stack85)
        %vm56720 = vcmp.eq.f32.partialorder %v56717, 1.0 (stack86)
        %v56725 = vmul.f32 %v56715, inf (stack83)
        %v56727 = vxor.u32 %v56715, 2147483648 (stack87)
        %v56730 = vmul.f32 %v56715, %v56727 (stack83)
        %v56732 = vadd.f32 %v56730, 1.0 (stack88)
        %v56733 = vlog2.pop %v56732 (stack89)
        %v56734 = vmul.f32 %v56733, 0.6931472 (stack90)
        %v56735 = vmul.f32 -0.5, %v56730 (stack91)
        %v56736 = vadd.f32 %v56735, 1.0 (stack92)
        %v56737 = vmul.f32 %v56736, %v56730 (stack93)
        %v56738 = vand.u32 2147483647, %v56730 (stack94)
        %vm56739 = vcmp.lt.f32.partialorder %v56738, 0.0004427343 (stack95)
        %v56740 = vsel /*vm=*/%vm56739, /*on_true_vy=*/%v56737, /*on_false_vx=*/%v56734 (stack96)
        %v56741 = vxor.u32 %v56740, 2147483648 (stack87)
        %vm56744 = vcmp.lt.f32.partialorder %v56741, 5.0 (stack86)
        %v56749 = vsel /*vm=*/%vm56744, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v56753 = vsel /*vm=*/%vm56744, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v56757 = vsel /*vm=*/%vm56744, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v56761 = vsel /*vm=*/%vm56744, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v56765 = vsel /*vm=*/%vm56744, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v56769 = vsel /*vm=*/%vm56744, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v56773 = vsel /*vm=*/%vm56744, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v56777 = vsel /*vm=*/%vm56744, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v56781 = vsel /*vm=*/%vm56744, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v56785 = vadd.f32 %v56741, -2.5 (stack82)
        %v56787 = vrsqrt.pop %v56741 (stack97)
        %v56788 = vmul.f32 %v56741, %v56787 (stack98)
        %vm56789 = vcmp.eq.f32.partialorder %v56741, inf (stack99)
        %v56790 = vsel /*vm=*/%vm56789, /*on_true_vy=*/%v56741, /*on_false_vx=*/%v56788 (stack100)
        %vm56791 = vcmp.eq.f32.partialorder %v56741, 0.0 (stack101)
        %v56792 = vand.u32 %v56741, 2147483648 (stack102)
        %v56793 = vsel /*vm=*/%vm56791, /*on_true_vy=*/%v56792, /*on_false_vx=*/%v56790 (stack103)
        %v56796 = vadd.f32 %v56793, -3.0 (stack82)
        %v56800 = vsel /*vm=*/%vm56744, /*on_true_vy=*/%v56785, /*on_false_vx=*/%v56796 (stack72)
        %v56804 = vmul.f32 %v56781, %v56800 (stack83)
        %v56808 = vadd.f32 %v56777, %v56804 (stack82)
        %v56812 = vmul.f32 %v56808, %v56800 (stack83)
        %v56816 = vadd.f32 %v56773, %v56812 (stack82)
        %v56820 = vmul.f32 %v56816, %v56800 (stack83)
        %v56824 = vadd.f32 %v56769, %v56820 (stack82)
        %v56828 = vmul.f32 %v56824, %v56800 (stack83)
        %v56832 = vadd.f32 %v56765, %v56828 (stack82)
        %v56836 = vmul.f32 %v56832, %v56800 (stack83)
        %v56840 = vadd.f32 %v56761, %v56836 (stack82)
        %v56844 = vmul.f32 %v56840, %v56800 (stack83)
        %v56848 = vadd.f32 %v56757, %v56844 (stack82)
        %v56852 = vmul.f32 %v56848, %v56800 (stack83)
        %v56856 = vadd.f32 %v56753, %v56852 (stack82)
        %v56860 = vmul.f32 %v56856, %v56800 (stack83)
        %v56864 = vadd.f32 %v56749, %v56860 (stack82)
        %v56868 = vmul.f32 %v56864, %v56715 (stack83)
        %v56872 = vsel /*vm=*/%vm56720, /*on_true_vy=*/%v56725, /*on_false_vx=*/%v56868 (stack72)
        %v56876 = vmul.f32 %v56872, 1.4140625 (stack83)
        %s56878 = scalar_lea.vmem %s280, 60 [#allocation0] (stack107)
        %v56879 = vpack.c.bf16 0.0, %v56876 (stack104)
        %56880 = vst [vmem:[%s56878] sm:$0xf] /*vst_source=*/%v56879 (stack105)
        %v56883 = vadd.s32 %v894, %v56419 (stack65)
        %s56885 = smul.u32 128, %s27 (stack66)
        %v56886 = vlaneseq (stack67)
        %v56887 = vand.u32 %v56886, 127 (stack68)
        %v56888 = vstv %s56885 (stack69)
        %v56889 = vadd.s32 %v56887, %v56888 (stack70)
        %v56893 = vadd.s32 %v56883, %v56889 (stack65)
        %vm56897 = vcmp.lt.u32.totalorder %v56893, %v56883 (stack71)
        %vm56902 = vcmp.lt.u32.totalorder %v56883, %v894 (stack71)
        %v56907 = vadd.s32 %v881, %v56402 (stack65)
        %v56911 = vadd.s32 %v56907, 1 (stack65)
        %v56915 = vsel /*vm=*/%vm56902, /*on_true_vy=*/%v56911, /*on_false_vx=*/%v56907 (stack72)
        %v56919 = vadd.s32 %v56915, 1 (stack65)
        %v56923 = vsel /*vm=*/%vm56897, /*on_true_vy=*/%v56919, /*on_false_vx=*/%v56915 (stack72)
        %v56928 = vadd.s32 %v56923, %v10 (stack65)
        %v56932 = vadd.s32 %v56893, %v9 (stack65)
        %v56936 = vadd.s32 %v56928, %v56932 (stack65)
        %v56938 = vshll.u32 %v56932, 13 (stack73)
        %v56939 = vshrl.u32 %v56932, 19 (stack74)
        %v56940 = vor.u32 %v56938, %v56939 (stack75)
        %v56941 = vxor.u32 %v56936, %v56940 (stack76)
        %v56944 = vadd.s32 %v56936, %v56941 (stack65)
        %v56946 = vshll.u32 %v56941, 15 (stack73)
        %v56947 = vshrl.u32 %v56941, 17 (stack74)
        %v56948 = vor.u32 %v56946, %v56947 (stack75)
        %v56949 = vxor.u32 %v56944, %v56948 (stack76)
        %v56952 = vadd.s32 %v56944, %v56949 (stack65)
        %v56954 = vshll.u32 %v56949, 26 (stack73)
        %v56955 = vshrl.u32 %v56949, 6 (stack74)
        %v56956 = vor.u32 %v56954, %v56955 (stack75)
        %v56957 = vxor.u32 %v56952, %v56956 (stack76)
        %v56960 = vadd.s32 %v56952, %v56957 (stack65)
        %v56964 = vadd.s32 %v56960, %v9 (stack65)
        %v56966 = vshll.u32 %v56957, 6 (stack73)
        %v56967 = vshrl.u32 %v56957, 26 (stack74)
        %v56968 = vor.u32 %v56966, %v56967 (stack75)
        %v56969 = vxor.u32 %v56960, %v56968 (stack76)
        %v56972 = vadd.s32 %v56969, %v8 (stack65)
        %v56976 = vadd.s32 %v56972, 1 (stack65)
        %v56980 = vadd.s32 %v56964, %v56976 (stack65)
        %v56982 = vshll.u32 %v56976, 17 (stack73)
        %v56983 = vshrl.u32 %v56976, 15 (stack74)
        %v56984 = vor.u32 %v56982, %v56983 (stack75)
        %v56985 = vxor.u32 %v56980, %v56984 (stack76)
        %v56988 = vadd.s32 %v56980, %v56985 (stack65)
        %v56990 = vshll.u32 %v56985, 29 (stack73)
        %v56991 = vshrl.u32 %v56985, 3 (stack74)
        %v56992 = vor.u32 %v56990, %v56991 (stack75)
        %v56993 = vxor.u32 %v56988, %v56992 (stack76)
        %v56996 = vadd.s32 %v56988, %v56993 (stack65)
        %v56998 = vshll.u32 %v56993, 16 (stack73)
        %v56999 = vshrl.u32 %v56993, 16 (stack74)
        %v57000 = vor.u32 %v56998, %v56999 (stack75)
        %v57001 = vxor.u32 %v56996, %v57000 (stack76)
        %v57004 = vadd.s32 %v56996, %v57001 (stack65)
        %v57008 = vadd.s32 %v57004, %v8 (stack65)
        %v57010 = vshll.u32 %v57001, 24 (stack73)
        %v57011 = vshrl.u32 %v57001, 8 (stack74)
        %v57012 = vor.u32 %v57010, %v57011 (stack75)
        %v57013 = vxor.u32 %v57004, %v57012 (stack76)
        %v57016 = vadd.s32 %v57013, %v10 (stack65)
        %v57020 = vadd.s32 %v57016, 2 (stack65)
        %v57024 = vadd.s32 %v57008, %v57020 (stack65)
        %v57026 = vshll.u32 %v57020, 13 (stack73)
        %v57027 = vshrl.u32 %v57020, 19 (stack74)
        %v57028 = vor.u32 %v57026, %v57027 (stack75)
        %v57029 = vxor.u32 %v57024, %v57028 (stack76)
        %v57032 = vadd.s32 %v57024, %v57029 (stack65)
        %v57034 = vshll.u32 %v57029, 15 (stack73)
        %v57035 = vshrl.u32 %v57029, 17 (stack74)
        %v57036 = vor.u32 %v57034, %v57035 (stack75)
        %v57037 = vxor.u32 %v57032, %v57036 (stack76)
        %v57040 = vadd.s32 %v57032, %v57037 (stack65)
        %v57042 = vshll.u32 %v57037, 26 (stack73)
        %v57043 = vshrl.u32 %v57037, 6 (stack74)
        %v57044 = vor.u32 %v57042, %v57043 (stack75)
        %v57045 = vxor.u32 %v57040, %v57044 (stack76)
        %v57048 = vadd.s32 %v57040, %v57045 (stack65)
        %v57052 = vadd.s32 %v57048, %v10 (stack65)
        %v57054 = vshll.u32 %v57045, 6 (stack73)
        %v57055 = vshrl.u32 %v57045, 26 (stack74)
        %v57056 = vor.u32 %v57054, %v57055 (stack75)
        %v57057 = vxor.u32 %v57048, %v57056 (stack76)
        %v57060 = vadd.s32 %v57057, %v9 (stack65)
        %v57064 = vadd.s32 %v57060, 3 (stack65)
        %v57068 = vadd.s32 %v57052, %v57064 (stack65)
        %v57070 = vshll.u32 %v57064, 17 (stack73)
        %v57071 = vshrl.u32 %v57064, 15 (stack74)
        %v57072 = vor.u32 %v57070, %v57071 (stack75)
        %v57073 = vxor.u32 %v57068, %v57072 (stack76)
        %v57076 = vadd.s32 %v57068, %v57073 (stack65)
        %v57078 = vshll.u32 %v57073, 29 (stack73)
        %v57079 = vshrl.u32 %v57073, 3 (stack74)
        %v57080 = vor.u32 %v57078, %v57079 (stack75)
        %v57081 = vxor.u32 %v57076, %v57080 (stack76)
        %v57084 = vadd.s32 %v57076, %v57081 (stack65)
        %v57086 = vshll.u32 %v57081, 16 (stack73)
        %v57087 = vshrl.u32 %v57081, 16 (stack74)
        %v57088 = vor.u32 %v57086, %v57087 (stack75)
        %v57089 = vxor.u32 %v57084, %v57088 (stack76)
        %v57092 = vadd.s32 %v57084, %v57089 (stack65)
        %v57096 = vadd.s32 %v57092, %v9 (stack65)
        %v57098 = vshll.u32 %v57089, 24 (stack73)
        %v57099 = vshrl.u32 %v57089, 8 (stack74)
        %v57100 = vor.u32 %v57098, %v57099 (stack75)
        %v57101 = vxor.u32 %v57092, %v57100 (stack76)
        %v57104 = vadd.s32 %v57101, %v8 (stack65)
        %v57108 = vadd.s32 %v57104, 4 (stack65)
        %v57112 = vadd.s32 %v57096, %v57108 (stack65)
        %v57114 = vshll.u32 %v57108, 13 (stack73)
        %v57115 = vshrl.u32 %v57108, 19 (stack74)
        %v57116 = vor.u32 %v57114, %v57115 (stack75)
        %v57117 = vxor.u32 %v57112, %v57116 (stack76)
        %v57120 = vadd.s32 %v57112, %v57117 (stack65)
        %v57122 = vshll.u32 %v57117, 15 (stack73)
        %v57123 = vshrl.u32 %v57117, 17 (stack74)
        %v57124 = vor.u32 %v57122, %v57123 (stack75)
        %v57125 = vxor.u32 %v57120, %v57124 (stack76)
        %v57128 = vadd.s32 %v57120, %v57125 (stack65)
        %v57130 = vshll.u32 %v57125, 26 (stack73)
        %v57131 = vshrl.u32 %v57125, 6 (stack74)
        %v57132 = vor.u32 %v57130, %v57131 (stack75)
        %v57133 = vxor.u32 %v57128, %v57132 (stack76)
        %v57136 = vadd.s32 %v57128, %v57133 (stack65)
        %v57140 = vadd.s32 %v57136, %v8 (stack65)
        %v57142 = vshll.u32 %v57133, 6 (stack73)
        %v57143 = vshrl.u32 %v57133, 26 (stack74)
        %v57144 = vor.u32 %v57142, %v57143 (stack75)
        %v57145 = vxor.u32 %v57136, %v57144 (stack76)
        %v57148 = vadd.s32 %v57145, %v10 (stack65)
        %v57152 = vadd.s32 %v57148, 5 (stack65)
        %v57154 = vxor.u32 %v57140, %v57152 (stack76)
        %v57155 = vand.u32.u8 %v57154, 255 (stack77)
        %v57156 = vand.u32 %v57155, 65535 (stack78)
        %v57157 = vshrl.u32 %v57156, 1 (stack79)
        %v57158 = vor.u32 %v57157, 16256 (stack75)
        %v57159 = vand.u32.u16 %v57158, 65535 (stack80)
        %v57160 = vunpack.i.l.bf16 %v57159 (stack81)
        %v57164 = vadd.f32 %v57160, -1.0 (stack82)
        %v57168 = vmul.f32 %v57164, 2.0 (stack83)
        %v57172 = vadd.f32 %v57168, -0.99609375 (stack82)
        %v57176 = vmax.f32 -0.99609375, %v57172 (stack84)
        %v57178 = vand.u32 2147483647, %v57176 (stack85)
        %vm57181 = vcmp.eq.f32.partialorder %v57178, 1.0 (stack86)
        %v57186 = vmul.f32 %v57176, inf (stack83)
        %v57188 = vxor.u32 %v57176, 2147483648 (stack87)
        %v57191 = vmul.f32 %v57176, %v57188 (stack83)
        %v57193 = vadd.f32 %v57191, 1.0 (stack88)
        %v57194 = vlog2.pop %v57193 (stack89)
        %v57195 = vmul.f32 %v57194, 0.6931472 (stack90)
        %v57196 = vmul.f32 -0.5, %v57191 (stack91)
        %v57197 = vadd.f32 %v57196, 1.0 (stack92)
        %v57198 = vmul.f32 %v57197, %v57191 (stack93)
        %v57199 = vand.u32 2147483647, %v57191 (stack94)
        %vm57200 = vcmp.lt.f32.partialorder %v57199, 0.0004427343 (stack95)
        %v57201 = vsel /*vm=*/%vm57200, /*on_true_vy=*/%v57198, /*on_false_vx=*/%v57195 (stack96)
        %v57202 = vxor.u32 %v57201, 2147483648 (stack87)
        %vm57205 = vcmp.lt.f32.partialorder %v57202, 5.0 (stack86)
        %v57210 = vsel /*vm=*/%vm57205, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v57214 = vsel /*vm=*/%vm57205, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v57218 = vsel /*vm=*/%vm57205, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v57222 = vsel /*vm=*/%vm57205, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v57226 = vsel /*vm=*/%vm57205, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v57230 = vsel /*vm=*/%vm57205, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v57234 = vsel /*vm=*/%vm57205, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v57238 = vsel /*vm=*/%vm57205, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v57242 = vsel /*vm=*/%vm57205, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v57246 = vadd.f32 %v57202, -2.5 (stack82)
        %v57248 = vrsqrt.pop %v57202 (stack97)
        %v57249 = vmul.f32 %v57202, %v57248 (stack98)
        %vm57250 = vcmp.eq.f32.partialorder %v57202, inf (stack99)
        %v57251 = vsel /*vm=*/%vm57250, /*on_true_vy=*/%v57202, /*on_false_vx=*/%v57249 (stack100)
        %vm57252 = vcmp.eq.f32.partialorder %v57202, 0.0 (stack101)
        %v57253 = vand.u32 %v57202, 2147483648 (stack102)
        %v57254 = vsel /*vm=*/%vm57252, /*on_true_vy=*/%v57253, /*on_false_vx=*/%v57251 (stack103)
        %v57257 = vadd.f32 %v57254, -3.0 (stack82)
        %v57261 = vsel /*vm=*/%vm57205, /*on_true_vy=*/%v57246, /*on_false_vx=*/%v57257 (stack72)
        %v57265 = vmul.f32 %v57242, %v57261 (stack83)
        %v57269 = vadd.f32 %v57238, %v57265 (stack82)
        %v57273 = vmul.f32 %v57269, %v57261 (stack83)
        %v57277 = vadd.f32 %v57234, %v57273 (stack82)
        %v57281 = vmul.f32 %v57277, %v57261 (stack83)
        %v57285 = vadd.f32 %v57230, %v57281 (stack82)
        %v57289 = vmul.f32 %v57285, %v57261 (stack83)
        %v57293 = vadd.f32 %v57226, %v57289 (stack82)
        %v57297 = vmul.f32 %v57293, %v57261 (stack83)
        %v57301 = vadd.f32 %v57222, %v57297 (stack82)
        %v57305 = vmul.f32 %v57301, %v57261 (stack83)
        %v57309 = vadd.f32 %v57218, %v57305 (stack82)
        %v57313 = vmul.f32 %v57309, %v57261 (stack83)
        %v57317 = vadd.f32 %v57214, %v57313 (stack82)
        %v57321 = vmul.f32 %v57317, %v57261 (stack83)
        %v57325 = vadd.f32 %v57210, %v57321 (stack82)
        %v57329 = vmul.f32 %v57325, %v57176 (stack83)
        %v57333 = vsel /*vm=*/%vm57181, /*on_true_vy=*/%v57186, /*on_false_vx=*/%v57329 (stack72)
        %v57337 = vmul.f32 %v57333, 1.4140625 (stack83)
        %s57339 = scalar_lea.vmem %s280, 188 [#allocation0] (stack107)
        %v57340 = vpack.c.bf16 0.0, %v57337 (stack104)
        %57341 = vst [vmem:[%s57339] sm:$0xf] /*vst_source=*/%v57340 (stack105)
        %v57344 = vadd.s32 %v1381, %v56419 (stack65)
        %s57346 = smul.u32 128, %s27 (stack66)
        %v57347 = vlaneseq (stack67)
        %v57348 = vand.u32 %v57347, 127 (stack68)
        %v57349 = vstv %s57346 (stack69)
        %v57350 = vadd.s32 %v57348, %v57349 (stack70)
        %v57354 = vadd.s32 %v57344, %v57350 (stack65)
        %vm57358 = vcmp.lt.u32.totalorder %v57354, %v57344 (stack71)
        %vm57363 = vcmp.lt.u32.totalorder %v57344, %v1381 (stack71)
        %v57368 = vadd.s32 %v1368, %v56402 (stack65)
        %v57372 = vadd.s32 %v57368, 1 (stack65)
        %v57376 = vsel /*vm=*/%vm57363, /*on_true_vy=*/%v57372, /*on_false_vx=*/%v57368 (stack72)
        %v57380 = vadd.s32 %v57376, 1 (stack65)
        %v57384 = vsel /*vm=*/%vm57358, /*on_true_vy=*/%v57380, /*on_false_vx=*/%v57376 (stack72)
        %v57389 = vadd.s32 %v57384, %v10 (stack65)
        %v57393 = vadd.s32 %v57354, %v9 (stack65)
        %v57397 = vadd.s32 %v57389, %v57393 (stack65)
        %v57399 = vshll.u32 %v57393, 13 (stack73)
        %v57400 = vshrl.u32 %v57393, 19 (stack74)
        %v57401 = vor.u32 %v57399, %v57400 (stack75)
        %v57402 = vxor.u32 %v57397, %v57401 (stack76)
        %v57405 = vadd.s32 %v57397, %v57402 (stack65)
        %v57407 = vshll.u32 %v57402, 15 (stack73)
        %v57408 = vshrl.u32 %v57402, 17 (stack74)
        %v57409 = vor.u32 %v57407, %v57408 (stack75)
        %v57410 = vxor.u32 %v57405, %v57409 (stack76)
        %v57413 = vadd.s32 %v57405, %v57410 (stack65)
        %v57415 = vshll.u32 %v57410, 26 (stack73)
        %v57416 = vshrl.u32 %v57410, 6 (stack74)
        %v57417 = vor.u32 %v57415, %v57416 (stack75)
        %v57418 = vxor.u32 %v57413, %v57417 (stack76)
        %v57421 = vadd.s32 %v57413, %v57418 (stack65)
        %v57425 = vadd.s32 %v57421, %v9 (stack65)
        %v57427 = vshll.u32 %v57418, 6 (stack73)
        %v57428 = vshrl.u32 %v57418, 26 (stack74)
        %v57429 = vor.u32 %v57427, %v57428 (stack75)
        %v57430 = vxor.u32 %v57421, %v57429 (stack76)
        %v57433 = vadd.s32 %v57430, %v8 (stack65)
        %v57437 = vadd.s32 %v57433, 1 (stack65)
        %v57441 = vadd.s32 %v57425, %v57437 (stack65)
        %v57443 = vshll.u32 %v57437, 17 (stack73)
        %v57444 = vshrl.u32 %v57437, 15 (stack74)
        %v57445 = vor.u32 %v57443, %v57444 (stack75)
        %v57446 = vxor.u32 %v57441, %v57445 (stack76)
        %v57449 = vadd.s32 %v57441, %v57446 (stack65)
        %v57451 = vshll.u32 %v57446, 29 (stack73)
        %v57452 = vshrl.u32 %v57446, 3 (stack74)
        %v57453 = vor.u32 %v57451, %v57452 (stack75)
        %v57454 = vxor.u32 %v57449, %v57453 (stack76)
        %v57457 = vadd.s32 %v57449, %v57454 (stack65)
        %v57459 = vshll.u32 %v57454, 16 (stack73)
        %v57460 = vshrl.u32 %v57454, 16 (stack74)
        %v57461 = vor.u32 %v57459, %v57460 (stack75)
        %v57462 = vxor.u32 %v57457, %v57461 (stack76)
        %v57465 = vadd.s32 %v57457, %v57462 (stack65)
        %v57469 = vadd.s32 %v57465, %v8 (stack65)
        %v57471 = vshll.u32 %v57462, 24 (stack73)
        %v57472 = vshrl.u32 %v57462, 8 (stack74)
        %v57473 = vor.u32 %v57471, %v57472 (stack75)
        %v57474 = vxor.u32 %v57465, %v57473 (stack76)
        %v57477 = vadd.s32 %v57474, %v10 (stack65)
        %v57481 = vadd.s32 %v57477, 2 (stack65)
        %v57485 = vadd.s32 %v57469, %v57481 (stack65)
        %v57487 = vshll.u32 %v57481, 13 (stack73)
        %v57488 = vshrl.u32 %v57481, 19 (stack74)
        %v57489 = vor.u32 %v57487, %v57488 (stack75)
        %v57490 = vxor.u32 %v57485, %v57489 (stack76)
        %v57493 = vadd.s32 %v57485, %v57490 (stack65)
        %v57495 = vshll.u32 %v57490, 15 (stack73)
        %v57496 = vshrl.u32 %v57490, 17 (stack74)
        %v57497 = vor.u32 %v57495, %v57496 (stack75)
        %v57498 = vxor.u32 %v57493, %v57497 (stack76)
        %v57501 = vadd.s32 %v57493, %v57498 (stack65)
        %v57503 = vshll.u32 %v57498, 26 (stack73)
        %v57504 = vshrl.u32 %v57498, 6 (stack74)
        %v57505 = vor.u32 %v57503, %v57504 (stack75)
        %v57506 = vxor.u32 %v57501, %v57505 (stack76)
        %v57509 = vadd.s32 %v57501, %v57506 (stack65)
        %v57513 = vadd.s32 %v57509, %v10 (stack65)
        %v57515 = vshll.u32 %v57506, 6 (stack73)
        %v57516 = vshrl.u32 %v57506, 26 (stack74)
        %v57517 = vor.u32 %v57515, %v57516 (stack75)
        %v57518 = vxor.u32 %v57509, %v57517 (stack76)
        %v57521 = vadd.s32 %v57518, %v9 (stack65)
        %v57525 = vadd.s32 %v57521, 3 (stack65)
        %v57529 = vadd.s32 %v57513, %v57525 (stack65)
        %v57531 = vshll.u32 %v57525, 17 (stack73)
        %v57532 = vshrl.u32 %v57525, 15 (stack74)
        %v57533 = vor.u32 %v57531, %v57532 (stack75)
        %v57534 = vxor.u32 %v57529, %v57533 (stack76)
        %v57537 = vadd.s32 %v57529, %v57534 (stack65)
        %v57539 = vshll.u32 %v57534, 29 (stack73)
        %v57540 = vshrl.u32 %v57534, 3 (stack74)
        %v57541 = vor.u32 %v57539, %v57540 (stack75)
        %v57542 = vxor.u32 %v57537, %v57541 (stack76)
        %v57545 = vadd.s32 %v57537, %v57542 (stack65)
        %v57547 = vshll.u32 %v57542, 16 (stack73)
        %v57548 = vshrl.u32 %v57542, 16 (stack74)
        %v57549 = vor.u32 %v57547, %v57548 (stack75)
        %v57550 = vxor.u32 %v57545, %v57549 (stack76)
        %v57553 = vadd.s32 %v57545, %v57550 (stack65)
        %v57557 = vadd.s32 %v57553, %v9 (stack65)
        %v57559 = vshll.u32 %v57550, 24 (stack73)
        %v57560 = vshrl.u32 %v57550, 8 (stack74)
        %v57561 = vor.u32 %v57559, %v57560 (stack75)
        %v57562 = vxor.u32 %v57553, %v57561 (stack76)
        %v57565 = vadd.s32 %v57562, %v8 (stack65)
        %v57569 = vadd.s32 %v57565, 4 (stack65)
        %v57573 = vadd.s32 %v57557, %v57569 (stack65)
        %v57575 = vshll.u32 %v57569, 13 (stack73)
        %v57576 = vshrl.u32 %v57569, 19 (stack74)
        %v57577 = vor.u32 %v57575, %v57576 (stack75)
        %v57578 = vxor.u32 %v57573, %v57577 (stack76)
        %v57581 = vadd.s32 %v57573, %v57578 (stack65)
        %v57583 = vshll.u32 %v57578, 15 (stack73)
        %v57584 = vshrl.u32 %v57578, 17 (stack74)
        %v57585 = vor.u32 %v57583, %v57584 (stack75)
        %v57586 = vxor.u32 %v57581, %v57585 (stack76)
        %v57589 = vadd.s32 %v57581, %v57586 (stack65)
        %v57591 = vshll.u32 %v57586, 26 (stack73)
        %v57592 = vshrl.u32 %v57586, 6 (stack74)
        %v57593 = vor.u32 %v57591, %v57592 (stack75)
        %v57594 = vxor.u32 %v57589, %v57593 (stack76)
        %v57597 = vadd.s32 %v57589, %v57594 (stack65)
        %v57601 = vadd.s32 %v57597, %v8 (stack65)
        %v57603 = vshll.u32 %v57594, 6 (stack73)
        %v57604 = vshrl.u32 %v57594, 26 (stack74)
        %v57605 = vor.u32 %v57603, %v57604 (stack75)
        %v57606 = vxor.u32 %v57597, %v57605 (stack76)
        %v57609 = vadd.s32 %v57606, %v10 (stack65)
        %v57613 = vadd.s32 %v57609, 5 (stack65)
        %v57615 = vxor.u32 %v57601, %v57613 (stack76)
        %v57616 = vand.u32.u8 %v57615, 255 (stack77)
        %v57617 = vand.u32 %v57616, 65535 (stack78)
        %v57618 = vshrl.u32 %v57617, 1 (stack79)
        %v57619 = vor.u32 %v57618, 16256 (stack75)
        %v57620 = vand.u32.u16 %v57619, 65535 (stack80)
        %v57621 = vunpack.i.l.bf16 %v57620 (stack81)
        %v57625 = vadd.f32 %v57621, -1.0 (stack82)
        %v57629 = vmul.f32 %v57625, 2.0 (stack83)
        %v57633 = vadd.f32 %v57629, -0.99609375 (stack82)
        %v57637 = vmax.f32 -0.99609375, %v57633 (stack84)
        %v57639 = vand.u32 2147483647, %v57637 (stack85)
        %vm57642 = vcmp.eq.f32.partialorder %v57639, 1.0 (stack86)
        %v57647 = vmul.f32 %v57637, inf (stack83)
        %v57649 = vxor.u32 %v57637, 2147483648 (stack87)
        %v57652 = vmul.f32 %v57637, %v57649 (stack83)
        %v57654 = vadd.f32 %v57652, 1.0 (stack88)
        %v57655 = vlog2.pop %v57654 (stack89)
        %v57656 = vmul.f32 %v57655, 0.6931472 (stack90)
        %v57657 = vmul.f32 -0.5, %v57652 (stack91)
        %v57658 = vadd.f32 %v57657, 1.0 (stack92)
        %v57659 = vmul.f32 %v57658, %v57652 (stack93)
        %v57660 = vand.u32 2147483647, %v57652 (stack94)
        %vm57661 = vcmp.lt.f32.partialorder %v57660, 0.0004427343 (stack95)
        %v57662 = vsel /*vm=*/%vm57661, /*on_true_vy=*/%v57659, /*on_false_vx=*/%v57656 (stack96)
        %v57663 = vxor.u32 %v57662, 2147483648 (stack87)
        %vm57666 = vcmp.lt.f32.partialorder %v57663, 5.0 (stack86)
        %v57671 = vsel /*vm=*/%vm57666, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v57675 = vsel /*vm=*/%vm57666, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v57679 = vsel /*vm=*/%vm57666, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v57683 = vsel /*vm=*/%vm57666, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v57687 = vsel /*vm=*/%vm57666, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v57691 = vsel /*vm=*/%vm57666, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v57695 = vsel /*vm=*/%vm57666, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v57699 = vsel /*vm=*/%vm57666, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v57703 = vsel /*vm=*/%vm57666, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v57707 = vadd.f32 %v57663, -2.5 (stack82)
        %v57709 = vrsqrt.pop %v57663 (stack97)
        %v57710 = vmul.f32 %v57663, %v57709 (stack98)
        %vm57711 = vcmp.eq.f32.partialorder %v57663, inf (stack99)
        %v57712 = vsel /*vm=*/%vm57711, /*on_true_vy=*/%v57663, /*on_false_vx=*/%v57710 (stack100)
        %vm57713 = vcmp.eq.f32.partialorder %v57663, 0.0 (stack101)
        %v57714 = vand.u32 %v57663, 2147483648 (stack102)
        %v57715 = vsel /*vm=*/%vm57713, /*on_true_vy=*/%v57714, /*on_false_vx=*/%v57712 (stack103)
        %v57718 = vadd.f32 %v57715, -3.0 (stack82)
        %v57722 = vsel /*vm=*/%vm57666, /*on_true_vy=*/%v57707, /*on_false_vx=*/%v57718 (stack72)
        %v57726 = vmul.f32 %v57703, %v57722 (stack83)
        %v57730 = vadd.f32 %v57699, %v57726 (stack82)
        %v57734 = vmul.f32 %v57730, %v57722 (stack83)
        %v57738 = vadd.f32 %v57695, %v57734 (stack82)
        %v57742 = vmul.f32 %v57738, %v57722 (stack83)
        %v57746 = vadd.f32 %v57691, %v57742 (stack82)
        %v57750 = vmul.f32 %v57746, %v57722 (stack83)
        %v57754 = vadd.f32 %v57687, %v57750 (stack82)
        %v57758 = vmul.f32 %v57754, %v57722 (stack83)
        %v57762 = vadd.f32 %v57683, %v57758 (stack82)
        %v57766 = vmul.f32 %v57762, %v57722 (stack83)
        %v57770 = vadd.f32 %v57679, %v57766 (stack82)
        %v57774 = vmul.f32 %v57770, %v57722 (stack83)
        %v57778 = vadd.f32 %v57675, %v57774 (stack82)
        %v57782 = vmul.f32 %v57778, %v57722 (stack83)
        %v57786 = vadd.f32 %v57671, %v57782 (stack82)
        %v57790 = vmul.f32 %v57786, %v57637 (stack83)
        %v57794 = vsel /*vm=*/%vm57642, /*on_true_vy=*/%v57647, /*on_false_vx=*/%v57790 (stack72)
        %v57798 = vmul.f32 %v57794, 1.4140625 (stack83)
        %s57800 = scalar_lea.vmem %s280, 316 [#allocation0] (stack107)
        %v57801 = vpack.c.bf16 0.0, %v57798 (stack104)
        %57802 = vst [vmem:[%s57800] sm:$0xf] /*vst_source=*/%v57801 (stack105)
        %v57805 = vadd.s32 %v1868, %v56419 (stack65)
        %s57807 = smul.u32 128, %s27 (stack66)
        %v57808 = vlaneseq (stack67)
        %v57809 = vand.u32 %v57808, 127 (stack68)
        %v57810 = vstv %s57807 (stack69)
        %v57811 = vadd.s32 %v57809, %v57810 (stack70)
        %v57815 = vadd.s32 %v57805, %v57811 (stack65)
        %vm57819 = vcmp.lt.u32.totalorder %v57815, %v57805 (stack71)
        %vm57824 = vcmp.lt.u32.totalorder %v57805, %v1868 (stack71)
        %v57829 = vadd.s32 %v1855, %v56402 (stack65)
        %v57833 = vadd.s32 %v57829, 1 (stack65)
        %v57837 = vsel /*vm=*/%vm57824, /*on_true_vy=*/%v57833, /*on_false_vx=*/%v57829 (stack72)
        %v57841 = vadd.s32 %v57837, 1 (stack65)
        %v57845 = vsel /*vm=*/%vm57819, /*on_true_vy=*/%v57841, /*on_false_vx=*/%v57837 (stack72)
        %v57850 = vadd.s32 %v57845, %v10 (stack65)
        %v57854 = vadd.s32 %v57815, %v9 (stack65)
        %v57858 = vadd.s32 %v57850, %v57854 (stack65)
        %v57860 = vshll.u32 %v57854, 13 (stack73)
        %v57861 = vshrl.u32 %v57854, 19 (stack74)
        %v57862 = vor.u32 %v57860, %v57861 (stack75)
        %v57863 = vxor.u32 %v57858, %v57862 (stack76)
        %v57866 = vadd.s32 %v57858, %v57863 (stack65)
        %v57868 = vshll.u32 %v57863, 15 (stack73)
        %v57869 = vshrl.u32 %v57863, 17 (stack74)
        %v57870 = vor.u32 %v57868, %v57869 (stack75)
        %v57871 = vxor.u32 %v57866, %v57870 (stack76)
        %v57874 = vadd.s32 %v57866, %v57871 (stack65)
        %v57876 = vshll.u32 %v57871, 26 (stack73)
        %v57877 = vshrl.u32 %v57871, 6 (stack74)
        %v57878 = vor.u32 %v57876, %v57877 (stack75)
        %v57879 = vxor.u32 %v57874, %v57878 (stack76)
        %v57882 = vadd.s32 %v57874, %v57879 (stack65)
        %v57886 = vadd.s32 %v57882, %v9 (stack65)
        %v57888 = vshll.u32 %v57879, 6 (stack73)
        %v57889 = vshrl.u32 %v57879, 26 (stack74)
        %v57890 = vor.u32 %v57888, %v57889 (stack75)
        %v57891 = vxor.u32 %v57882, %v57890 (stack76)
        %v57894 = vadd.s32 %v57891, %v8 (stack65)
        %v57898 = vadd.s32 %v57894, 1 (stack65)
        %v57902 = vadd.s32 %v57886, %v57898 (stack65)
        %v57904 = vshll.u32 %v57898, 17 (stack73)
        %v57905 = vshrl.u32 %v57898, 15 (stack74)
        %v57906 = vor.u32 %v57904, %v57905 (stack75)
        %v57907 = vxor.u32 %v57902, %v57906 (stack76)
        %v57910 = vadd.s32 %v57902, %v57907 (stack65)
        %v57912 = vshll.u32 %v57907, 29 (stack73)
        %v57913 = vshrl.u32 %v57907, 3 (stack74)
        %v57914 = vor.u32 %v57912, %v57913 (stack75)
        %v57915 = vxor.u32 %v57910, %v57914 (stack76)
        %v57918 = vadd.s32 %v57910, %v57915 (stack65)
        %v57920 = vshll.u32 %v57915, 16 (stack73)
        %v57921 = vshrl.u32 %v57915, 16 (stack74)
        %v57922 = vor.u32 %v57920, %v57921 (stack75)
        %v57923 = vxor.u32 %v57918, %v57922 (stack76)
        %v57926 = vadd.s32 %v57918, %v57923 (stack65)
        %v57930 = vadd.s32 %v57926, %v8 (stack65)
        %v57932 = vshll.u32 %v57923, 24 (stack73)
        %v57933 = vshrl.u32 %v57923, 8 (stack74)
        %v57934 = vor.u32 %v57932, %v57933 (stack75)
        %v57935 = vxor.u32 %v57926, %v57934 (stack76)
        %v57938 = vadd.s32 %v57935, %v10 (stack65)
        %v57942 = vadd.s32 %v57938, 2 (stack65)
        %v57946 = vadd.s32 %v57930, %v57942 (stack65)
        %v57948 = vshll.u32 %v57942, 13 (stack73)
        %v57949 = vshrl.u32 %v57942, 19 (stack74)
        %v57950 = vor.u32 %v57948, %v57949 (stack75)
        %v57951 = vxor.u32 %v57946, %v57950 (stack76)
        %v57954 = vadd.s32 %v57946, %v57951 (stack65)
        %v57956 = vshll.u32 %v57951, 15 (stack73)
        %v57957 = vshrl.u32 %v57951, 17 (stack74)
        %v57958 = vor.u32 %v57956, %v57957 (stack75)
        %v57959 = vxor.u32 %v57954, %v57958 (stack76)
        %v57962 = vadd.s32 %v57954, %v57959 (stack65)
        %v57964 = vshll.u32 %v57959, 26 (stack73)
        %v57965 = vshrl.u32 %v57959, 6 (stack74)
        %v57966 = vor.u32 %v57964, %v57965 (stack75)
        %v57967 = vxor.u32 %v57962, %v57966 (stack76)
        %v57970 = vadd.s32 %v57962, %v57967 (stack65)
        %v57974 = vadd.s32 %v57970, %v10 (stack65)
        %v57976 = vshll.u32 %v57967, 6 (stack73)
        %v57977 = vshrl.u32 %v57967, 26 (stack74)
        %v57978 = vor.u32 %v57976, %v57977 (stack75)
        %v57979 = vxor.u32 %v57970, %v57978 (stack76)
        %v57982 = vadd.s32 %v57979, %v9 (stack65)
        %v57986 = vadd.s32 %v57982, 3 (stack65)
        %v57990 = vadd.s32 %v57974, %v57986 (stack65)
        %v57992 = vshll.u32 %v57986, 17 (stack73)
        %v57993 = vshrl.u32 %v57986, 15 (stack74)
        %v57994 = vor.u32 %v57992, %v57993 (stack75)
        %v57995 = vxor.u32 %v57990, %v57994 (stack76)
        %v57998 = vadd.s32 %v57990, %v57995 (stack65)
        %v58000 = vshll.u32 %v57995, 29 (stack73)
        %v58001 = vshrl.u32 %v57995, 3 (stack74)
        %v58002 = vor.u32 %v58000, %v58001 (stack75)
        %v58003 = vxor.u32 %v57998, %v58002 (stack76)
        %v58006 = vadd.s32 %v57998, %v58003 (stack65)
        %v58008 = vshll.u32 %v58003, 16 (stack73)
        %v58009 = vshrl.u32 %v58003, 16 (stack74)
        %v58010 = vor.u32 %v58008, %v58009 (stack75)
        %v58011 = vxor.u32 %v58006, %v58010 (stack76)
        %v58014 = vadd.s32 %v58006, %v58011 (stack65)
        %v58018 = vadd.s32 %v58014, %v9 (stack65)
        %v58020 = vshll.u32 %v58011, 24 (stack73)
        %v58021 = vshrl.u32 %v58011, 8 (stack74)
        %v58022 = vor.u32 %v58020, %v58021 (stack75)
        %v58023 = vxor.u32 %v58014, %v58022 (stack76)
        %v58026 = vadd.s32 %v58023, %v8 (stack65)
        %v58030 = vadd.s32 %v58026, 4 (stack65)
        %v58034 = vadd.s32 %v58018, %v58030 (stack65)
        %v58036 = vshll.u32 %v58030, 13 (stack73)
        %v58037 = vshrl.u32 %v58030, 19 (stack74)
        %v58038 = vor.u32 %v58036, %v58037 (stack75)
        %v58039 = vxor.u32 %v58034, %v58038 (stack76)
        %v58042 = vadd.s32 %v58034, %v58039 (stack65)
        %v58044 = vshll.u32 %v58039, 15 (stack73)
        %v58045 = vshrl.u32 %v58039, 17 (stack74)
        %v58046 = vor.u32 %v58044, %v58045 (stack75)
        %v58047 = vxor.u32 %v58042, %v58046 (stack76)
        %v58050 = vadd.s32 %v58042, %v58047 (stack65)
        %v58052 = vshll.u32 %v58047, 26 (stack73)
        %v58053 = vshrl.u32 %v58047, 6 (stack74)
        %v58054 = vor.u32 %v58052, %v58053 (stack75)
        %v58055 = vxor.u32 %v58050, %v58054 (stack76)
        %v58058 = vadd.s32 %v58050, %v58055 (stack65)
        %v58062 = vadd.s32 %v58058, %v8 (stack65)
        %v58064 = vshll.u32 %v58055, 6 (stack73)
        %v58065 = vshrl.u32 %v58055, 26 (stack74)
        %v58066 = vor.u32 %v58064, %v58065 (stack75)
        %v58067 = vxor.u32 %v58058, %v58066 (stack76)
        %v58070 = vadd.s32 %v58067, %v10 (stack65)
        %v58074 = vadd.s32 %v58070, 5 (stack65)
        %v58076 = vxor.u32 %v58062, %v58074 (stack76)
        %v58077 = vand.u32.u8 %v58076, 255 (stack77)
        %v58078 = vand.u32 %v58077, 65535 (stack78)
        %v58079 = vshrl.u32 %v58078, 1 (stack79)
        %v58080 = vor.u32 %v58079, 16256 (stack75)
        %v58081 = vand.u32.u16 %v58080, 65535 (stack80)
        %v58082 = vunpack.i.l.bf16 %v58081 (stack81)
        %v58086 = vadd.f32 %v58082, -1.0 (stack82)
        %v58090 = vmul.f32 %v58086, 2.0 (stack83)
        %v58094 = vadd.f32 %v58090, -0.99609375 (stack82)
        %v58098 = vmax.f32 -0.99609375, %v58094 (stack84)
        %v58100 = vand.u32 2147483647, %v58098 (stack85)
        %vm58103 = vcmp.eq.f32.partialorder %v58100, 1.0 (stack86)
        %v58108 = vmul.f32 %v58098, inf (stack83)
        %v58110 = vxor.u32 %v58098, 2147483648 (stack87)
        %v58113 = vmul.f32 %v58098, %v58110 (stack83)
        %v58115 = vadd.f32 %v58113, 1.0 (stack88)
        %v58116 = vlog2.pop %v58115 (stack89)
        %v58117 = vmul.f32 %v58116, 0.6931472 (stack90)
        %v58118 = vmul.f32 -0.5, %v58113 (stack91)
        %v58119 = vadd.f32 %v58118, 1.0 (stack92)
        %v58120 = vmul.f32 %v58119, %v58113 (stack93)
        %v58121 = vand.u32 2147483647, %v58113 (stack94)
        %vm58122 = vcmp.lt.f32.partialorder %v58121, 0.0004427343 (stack95)
        %v58123 = vsel /*vm=*/%vm58122, /*on_true_vy=*/%v58120, /*on_false_vx=*/%v58117 (stack96)
        %v58124 = vxor.u32 %v58123, 2147483648 (stack87)
        %vm58127 = vcmp.lt.f32.partialorder %v58124, 5.0 (stack86)
        %v58132 = vsel /*vm=*/%vm58127, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v58136 = vsel /*vm=*/%vm58127, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v58140 = vsel /*vm=*/%vm58127, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v58144 = vsel /*vm=*/%vm58127, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v58148 = vsel /*vm=*/%vm58127, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v58152 = vsel /*vm=*/%vm58127, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v58156 = vsel /*vm=*/%vm58127, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v58160 = vsel /*vm=*/%vm58127, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v58164 = vsel /*vm=*/%vm58127, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v58168 = vadd.f32 %v58124, -2.5 (stack82)
        %v58170 = vrsqrt.pop %v58124 (stack97)
        %v58171 = vmul.f32 %v58124, %v58170 (stack98)
        %vm58172 = vcmp.eq.f32.partialorder %v58124, inf (stack99)
        %v58173 = vsel /*vm=*/%vm58172, /*on_true_vy=*/%v58124, /*on_false_vx=*/%v58171 (stack100)
        %vm58174 = vcmp.eq.f32.partialorder %v58124, 0.0 (stack101)
        %v58175 = vand.u32 %v58124, 2147483648 (stack102)
        %v58176 = vsel /*vm=*/%vm58174, /*on_true_vy=*/%v58175, /*on_false_vx=*/%v58173 (stack103)
        %v58179 = vadd.f32 %v58176, -3.0 (stack82)
        %v58183 = vsel /*vm=*/%vm58127, /*on_true_vy=*/%v58168, /*on_false_vx=*/%v58179 (stack72)
        %v58187 = vmul.f32 %v58164, %v58183 (stack83)
        %v58191 = vadd.f32 %v58160, %v58187 (stack82)
        %v58195 = vmul.f32 %v58191, %v58183 (stack83)
        %v58199 = vadd.f32 %v58156, %v58195 (stack82)
        %v58203 = vmul.f32 %v58199, %v58183 (stack83)
        %v58207 = vadd.f32 %v58152, %v58203 (stack82)
        %v58211 = vmul.f32 %v58207, %v58183 (stack83)
        %v58215 = vadd.f32 %v58148, %v58211 (stack82)
        %v58219 = vmul.f32 %v58215, %v58183 (stack83)
        %v58223 = vadd.f32 %v58144, %v58219 (stack82)
        %v58227 = vmul.f32 %v58223, %v58183 (stack83)
        %v58231 = vadd.f32 %v58140, %v58227 (stack82)
        %v58235 = vmul.f32 %v58231, %v58183 (stack83)
        %v58239 = vadd.f32 %v58136, %v58235 (stack82)
        %v58243 = vmul.f32 %v58239, %v58183 (stack83)
        %v58247 = vadd.f32 %v58132, %v58243 (stack82)
        %v58251 = vmul.f32 %v58247, %v58098 (stack83)
        %v58255 = vsel /*vm=*/%vm58103, /*on_true_vy=*/%v58108, /*on_false_vx=*/%v58251 (stack72)
        %v58259 = vmul.f32 %v58255, 1.4140625 (stack83)
        %s58261 = scalar_lea.vmem %s280, 444 [#allocation0] (stack107)
        %v58262 = vpack.c.bf16 0.0, %v58259 (stack104)
        %58263 = vst [vmem:[%s58261] sm:$0xf] /*vst_source=*/%v58262 (stack105)
        %v58266 = vadd.s32 %v2355, %v56419 (stack65)
        %s58268 = smul.u32 128, %s27 (stack66)
        %v58269 = vlaneseq (stack67)
        %v58270 = vand.u32 %v58269, 127 (stack68)
        %v58271 = vstv %s58268 (stack69)
        %v58272 = vadd.s32 %v58270, %v58271 (stack70)
        %v58276 = vadd.s32 %v58266, %v58272 (stack65)
        %vm58280 = vcmp.lt.u32.totalorder %v58276, %v58266 (stack71)
        %vm58285 = vcmp.lt.u32.totalorder %v58266, %v2355 (stack71)
        %v58290 = vadd.s32 %v2342, %v56402 (stack65)
        %v58294 = vadd.s32 %v58290, 1 (stack65)
        %v58298 = vsel /*vm=*/%vm58285, /*on_true_vy=*/%v58294, /*on_false_vx=*/%v58290 (stack72)
        %v58302 = vadd.s32 %v58298, 1 (stack65)
        %v58306 = vsel /*vm=*/%vm58280, /*on_true_vy=*/%v58302, /*on_false_vx=*/%v58298 (stack72)
        %v58311 = vadd.s32 %v58306, %v10 (stack65)
        %v58315 = vadd.s32 %v58276, %v9 (stack65)
        %v58319 = vadd.s32 %v58311, %v58315 (stack65)
        %v58321 = vshll.u32 %v58315, 13 (stack73)
        %v58322 = vshrl.u32 %v58315, 19 (stack74)
        %v58323 = vor.u32 %v58321, %v58322 (stack75)
        %v58324 = vxor.u32 %v58319, %v58323 (stack76)
        %v58327 = vadd.s32 %v58319, %v58324 (stack65)
        %v58329 = vshll.u32 %v58324, 15 (stack73)
        %v58330 = vshrl.u32 %v58324, 17 (stack74)
        %v58331 = vor.u32 %v58329, %v58330 (stack75)
        %v58332 = vxor.u32 %v58327, %v58331 (stack76)
        %v58335 = vadd.s32 %v58327, %v58332 (stack65)
        %v58337 = vshll.u32 %v58332, 26 (stack73)
        %v58338 = vshrl.u32 %v58332, 6 (stack74)
        %v58339 = vor.u32 %v58337, %v58338 (stack75)
        %v58340 = vxor.u32 %v58335, %v58339 (stack76)
        %v58343 = vadd.s32 %v58335, %v58340 (stack65)
        %v58347 = vadd.s32 %v58343, %v9 (stack65)
        %v58349 = vshll.u32 %v58340, 6 (stack73)
        %v58350 = vshrl.u32 %v58340, 26 (stack74)
        %v58351 = vor.u32 %v58349, %v58350 (stack75)
        %v58352 = vxor.u32 %v58343, %v58351 (stack76)
        %v58355 = vadd.s32 %v58352, %v8 (stack65)
        %v58359 = vadd.s32 %v58355, 1 (stack65)
        %v58363 = vadd.s32 %v58347, %v58359 (stack65)
        %v58365 = vshll.u32 %v58359, 17 (stack73)
        %v58366 = vshrl.u32 %v58359, 15 (stack74)
        %v58367 = vor.u32 %v58365, %v58366 (stack75)
        %v58368 = vxor.u32 %v58363, %v58367 (stack76)
        %v58371 = vadd.s32 %v58363, %v58368 (stack65)
        %v58373 = vshll.u32 %v58368, 29 (stack73)
        %v58374 = vshrl.u32 %v58368, 3 (stack74)
        %v58375 = vor.u32 %v58373, %v58374 (stack75)
        %v58376 = vxor.u32 %v58371, %v58375 (stack76)
        %v58379 = vadd.s32 %v58371, %v58376 (stack65)
        %v58381 = vshll.u32 %v58376, 16 (stack73)
        %v58382 = vshrl.u32 %v58376, 16 (stack74)
        %v58383 = vor.u32 %v58381, %v58382 (stack75)
        %v58384 = vxor.u32 %v58379, %v58383 (stack76)
        %v58387 = vadd.s32 %v58379, %v58384 (stack65)
        %v58391 = vadd.s32 %v58387, %v8 (stack65)
        %v58393 = vshll.u32 %v58384, 24 (stack73)
        %v58394 = vshrl.u32 %v58384, 8 (stack74)
        %v58395 = vor.u32 %v58393, %v58394 (stack75)
        %v58396 = vxor.u32 %v58387, %v58395 (stack76)
        %v58399 = vadd.s32 %v58396, %v10 (stack65)
        %v58403 = vadd.s32 %v58399, 2 (stack65)
        %v58407 = vadd.s32 %v58391, %v58403 (stack65)
        %v58409 = vshll.u32 %v58403, 13 (stack73)
        %v58410 = vshrl.u32 %v58403, 19 (stack74)
        %v58411 = vor.u32 %v58409, %v58410 (stack75)
        %v58412 = vxor.u32 %v58407, %v58411 (stack76)
        %v58415 = vadd.s32 %v58407, %v58412 (stack65)
        %v58417 = vshll.u32 %v58412, 15 (stack73)
        %v58418 = vshrl.u32 %v58412, 17 (stack74)
        %v58419 = vor.u32 %v58417, %v58418 (stack75)
        %v58420 = vxor.u32 %v58415, %v58419 (stack76)
        %v58423 = vadd.s32 %v58415, %v58420 (stack65)
        %v58425 = vshll.u32 %v58420, 26 (stack73)
        %v58426 = vshrl.u32 %v58420, 6 (stack74)
        %v58427 = vor.u32 %v58425, %v58426 (stack75)
        %v58428 = vxor.u32 %v58423, %v58427 (stack76)
        %v58431 = vadd.s32 %v58423, %v58428 (stack65)
        %v58435 = vadd.s32 %v58431, %v10 (stack65)
        %v58437 = vshll.u32 %v58428, 6 (stack73)
        %v58438 = vshrl.u32 %v58428, 26 (stack74)
        %v58439 = vor.u32 %v58437, %v58438 (stack75)
        %v58440 = vxor.u32 %v58431, %v58439 (stack76)
        %v58443 = vadd.s32 %v58440, %v9 (stack65)
        %v58447 = vadd.s32 %v58443, 3 (stack65)
        %v58451 = vadd.s32 %v58435, %v58447 (stack65)
        %v58453 = vshll.u32 %v58447, 17 (stack73)
        %v58454 = vshrl.u32 %v58447, 15 (stack74)
        %v58455 = vor.u32 %v58453, %v58454 (stack75)
        %v58456 = vxor.u32 %v58451, %v58455 (stack76)
        %v58459 = vadd.s32 %v58451, %v58456 (stack65)
        %v58461 = vshll.u32 %v58456, 29 (stack73)
        %v58462 = vshrl.u32 %v58456, 3 (stack74)
        %v58463 = vor.u32 %v58461, %v58462 (stack75)
        %v58464 = vxor.u32 %v58459, %v58463 (stack76)
        %v58467 = vadd.s32 %v58459, %v58464 (stack65)
        %v58469 = vshll.u32 %v58464, 16 (stack73)
        %v58470 = vshrl.u32 %v58464, 16 (stack74)
        %v58471 = vor.u32 %v58469, %v58470 (stack75)
        %v58472 = vxor.u32 %v58467, %v58471 (stack76)
        %v58475 = vadd.s32 %v58467, %v58472 (stack65)
        %v58479 = vadd.s32 %v58475, %v9 (stack65)
        %v58481 = vshll.u32 %v58472, 24 (stack73)
        %v58482 = vshrl.u32 %v58472, 8 (stack74)
        %v58483 = vor.u32 %v58481, %v58482 (stack75)
        %v58484 = vxor.u32 %v58475, %v58483 (stack76)
        %v58487 = vadd.s32 %v58484, %v8 (stack65)
        %v58491 = vadd.s32 %v58487, 4 (stack65)
        %v58495 = vadd.s32 %v58479, %v58491 (stack65)
        %v58497 = vshll.u32 %v58491, 13 (stack73)
        %v58498 = vshrl.u32 %v58491, 19 (stack74)
        %v58499 = vor.u32 %v58497, %v58498 (stack75)
        %v58500 = vxor.u32 %v58495, %v58499 (stack76)
        %v58503 = vadd.s32 %v58495, %v58500 (stack65)
        %v58505 = vshll.u32 %v58500, 15 (stack73)
        %v58506 = vshrl.u32 %v58500, 17 (stack74)
        %v58507 = vor.u32 %v58505, %v58506 (stack75)
        %v58508 = vxor.u32 %v58503, %v58507 (stack76)
        %v58511 = vadd.s32 %v58503, %v58508 (stack65)
        %v58513 = vshll.u32 %v58508, 26 (stack73)
        %v58514 = vshrl.u32 %v58508, 6 (stack74)
        %v58515 = vor.u32 %v58513, %v58514 (stack75)
        %v58516 = vxor.u32 %v58511, %v58515 (stack76)
        %v58519 = vadd.s32 %v58511, %v58516 (stack65)
        %v58523 = vadd.s32 %v58519, %v8 (stack65)
        %v58525 = vshll.u32 %v58516, 6 (stack73)
        %v58526 = vshrl.u32 %v58516, 26 (stack74)
        %v58527 = vor.u32 %v58525, %v58526 (stack75)
        %v58528 = vxor.u32 %v58519, %v58527 (stack76)
        %v58531 = vadd.s32 %v58528, %v10 (stack65)
        %v58535 = vadd.s32 %v58531, 5 (stack65)
        %v58537 = vxor.u32 %v58523, %v58535 (stack76)
        %v58538 = vand.u32.u8 %v58537, 255 (stack77)
        %v58539 = vand.u32 %v58538, 65535 (stack78)
        %v58540 = vshrl.u32 %v58539, 1 (stack79)
        %v58541 = vor.u32 %v58540, 16256 (stack75)
        %v58542 = vand.u32.u16 %v58541, 65535 (stack80)
        %v58543 = vunpack.i.l.bf16 %v58542 (stack81)
        %v58547 = vadd.f32 %v58543, -1.0 (stack82)
        %v58551 = vmul.f32 %v58547, 2.0 (stack83)
        %v58555 = vadd.f32 %v58551, -0.99609375 (stack82)
        %v58559 = vmax.f32 -0.99609375, %v58555 (stack84)
        %v58561 = vand.u32 2147483647, %v58559 (stack85)
        %vm58564 = vcmp.eq.f32.partialorder %v58561, 1.0 (stack86)
        %v58569 = vmul.f32 %v58559, inf (stack83)
        %v58571 = vxor.u32 %v58559, 2147483648 (stack87)
        %v58574 = vmul.f32 %v58559, %v58571 (stack83)
        %v58576 = vadd.f32 %v58574, 1.0 (stack88)
        %v58577 = vlog2.pop %v58576 (stack89)
        %v58578 = vmul.f32 %v58577, 0.6931472 (stack90)
        %v58579 = vmul.f32 -0.5, %v58574 (stack91)
        %v58580 = vadd.f32 %v58579, 1.0 (stack92)
        %v58581 = vmul.f32 %v58580, %v58574 (stack93)
        %v58582 = vand.u32 2147483647, %v58574 (stack94)
        %vm58583 = vcmp.lt.f32.partialorder %v58582, 0.0004427343 (stack95)
        %v58584 = vsel /*vm=*/%vm58583, /*on_true_vy=*/%v58581, /*on_false_vx=*/%v58578 (stack96)
        %v58585 = vxor.u32 %v58584, 2147483648 (stack87)
        %vm58588 = vcmp.lt.f32.partialorder %v58585, 5.0 (stack86)
        %v58593 = vsel /*vm=*/%vm58588, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v58597 = vsel /*vm=*/%vm58588, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v58601 = vsel /*vm=*/%vm58588, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v58605 = vsel /*vm=*/%vm58588, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v58609 = vsel /*vm=*/%vm58588, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v58613 = vsel /*vm=*/%vm58588, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v58617 = vsel /*vm=*/%vm58588, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v58621 = vsel /*vm=*/%vm58588, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v58625 = vsel /*vm=*/%vm58588, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v58629 = vadd.f32 %v58585, -2.5 (stack82)
        %v58631 = vrsqrt.pop %v58585 (stack97)
        %v58632 = vmul.f32 %v58585, %v58631 (stack98)
        %vm58633 = vcmp.eq.f32.partialorder %v58585, inf (stack99)
        %v58634 = vsel /*vm=*/%vm58633, /*on_true_vy=*/%v58585, /*on_false_vx=*/%v58632 (stack100)
        %vm58635 = vcmp.eq.f32.partialorder %v58585, 0.0 (stack101)
        %v58636 = vand.u32 %v58585, 2147483648 (stack102)
        %v58637 = vsel /*vm=*/%vm58635, /*on_true_vy=*/%v58636, /*on_false_vx=*/%v58634 (stack103)
        %v58640 = vadd.f32 %v58637, -3.0 (stack82)
        %v58644 = vsel /*vm=*/%vm58588, /*on_true_vy=*/%v58629, /*on_false_vx=*/%v58640 (stack72)
        %v58648 = vmul.f32 %v58625, %v58644 (stack83)
        %v58652 = vadd.f32 %v58621, %v58648 (stack82)
        %v58656 = vmul.f32 %v58652, %v58644 (stack83)
        %v58660 = vadd.f32 %v58617, %v58656 (stack82)
        %v58664 = vmul.f32 %v58660, %v58644 (stack83)
        %v58668 = vadd.f32 %v58613, %v58664 (stack82)
        %v58672 = vmul.f32 %v58668, %v58644 (stack83)
        %v58676 = vadd.f32 %v58609, %v58672 (stack82)
        %v58680 = vmul.f32 %v58676, %v58644 (stack83)
        %v58684 = vadd.f32 %v58605, %v58680 (stack82)
        %v58688 = vmul.f32 %v58684, %v58644 (stack83)
        %v58692 = vadd.f32 %v58601, %v58688 (stack82)
        %v58696 = vmul.f32 %v58692, %v58644 (stack83)
        %v58700 = vadd.f32 %v58597, %v58696 (stack82)
        %v58704 = vmul.f32 %v58700, %v58644 (stack83)
        %v58708 = vadd.f32 %v58593, %v58704 (stack82)
        %v58712 = vmul.f32 %v58708, %v58559 (stack83)
        %v58716 = vsel /*vm=*/%vm58564, /*on_true_vy=*/%v58569, /*on_false_vx=*/%v58712 (stack72)
        %v58720 = vmul.f32 %v58716, 1.4140625 (stack83)
        %s58722 = scalar_lea.vmem %s280, 572 [#allocation0] (stack107)
        %v58723 = vpack.c.bf16 0.0, %v58720 (stack104)
        %58724 = vst [vmem:[%s58722] sm:$0xf] /*vst_source=*/%v58723 (stack105)
        %v58727 = vadd.s32 %v2842, %v56419 (stack65)
        %s58729 = smul.u32 128, %s27 (stack66)
        %v58730 = vlaneseq (stack67)
        %v58731 = vand.u32 %v58730, 127 (stack68)
        %v58732 = vstv %s58729 (stack69)
        %v58733 = vadd.s32 %v58731, %v58732 (stack70)
        %v58737 = vadd.s32 %v58727, %v58733 (stack65)
        %vm58741 = vcmp.lt.u32.totalorder %v58737, %v58727 (stack71)
        %vm58746 = vcmp.lt.u32.totalorder %v58727, %v2842 (stack71)
        %v58751 = vadd.s32 %v2829, %v56402 (stack65)
        %v58755 = vadd.s32 %v58751, 1 (stack65)
        %v58759 = vsel /*vm=*/%vm58746, /*on_true_vy=*/%v58755, /*on_false_vx=*/%v58751 (stack72)
        %v58763 = vadd.s32 %v58759, 1 (stack65)
        %v58767 = vsel /*vm=*/%vm58741, /*on_true_vy=*/%v58763, /*on_false_vx=*/%v58759 (stack72)
        %v58772 = vadd.s32 %v58767, %v10 (stack65)
        %v58776 = vadd.s32 %v58737, %v9 (stack65)
        %v58780 = vadd.s32 %v58772, %v58776 (stack65)
        %v58782 = vshll.u32 %v58776, 13 (stack73)
        %v58783 = vshrl.u32 %v58776, 19 (stack74)
        %v58784 = vor.u32 %v58782, %v58783 (stack75)
        %v58785 = vxor.u32 %v58780, %v58784 (stack76)
        %v58788 = vadd.s32 %v58780, %v58785 (stack65)
        %v58790 = vshll.u32 %v58785, 15 (stack73)
        %v58791 = vshrl.u32 %v58785, 17 (stack74)
        %v58792 = vor.u32 %v58790, %v58791 (stack75)
        %v58793 = vxor.u32 %v58788, %v58792 (stack76)
        %v58796 = vadd.s32 %v58788, %v58793 (stack65)
        %v58798 = vshll.u32 %v58793, 26 (stack73)
        %v58799 = vshrl.u32 %v58793, 6 (stack74)
        %v58800 = vor.u32 %v58798, %v58799 (stack75)
        %v58801 = vxor.u32 %v58796, %v58800 (stack76)
        %v58804 = vadd.s32 %v58796, %v58801 (stack65)
        %v58808 = vadd.s32 %v58804, %v9 (stack65)
        %v58810 = vshll.u32 %v58801, 6 (stack73)
        %v58811 = vshrl.u32 %v58801, 26 (stack74)
        %v58812 = vor.u32 %v58810, %v58811 (stack75)
        %v58813 = vxor.u32 %v58804, %v58812 (stack76)
        %v58816 = vadd.s32 %v58813, %v8 (stack65)
        %v58820 = vadd.s32 %v58816, 1 (stack65)
        %v58824 = vadd.s32 %v58808, %v58820 (stack65)
        %v58826 = vshll.u32 %v58820, 17 (stack73)
        %v58827 = vshrl.u32 %v58820, 15 (stack74)
        %v58828 = vor.u32 %v58826, %v58827 (stack75)
        %v58829 = vxor.u32 %v58824, %v58828 (stack76)
        %v58832 = vadd.s32 %v58824, %v58829 (stack65)
        %v58834 = vshll.u32 %v58829, 29 (stack73)
        %v58835 = vshrl.u32 %v58829, 3 (stack74)
        %v58836 = vor.u32 %v58834, %v58835 (stack75)
        %v58837 = vxor.u32 %v58832, %v58836 (stack76)
        %v58840 = vadd.s32 %v58832, %v58837 (stack65)
        %v58842 = vshll.u32 %v58837, 16 (stack73)
        %v58843 = vshrl.u32 %v58837, 16 (stack74)
        %v58844 = vor.u32 %v58842, %v58843 (stack75)
        %v58845 = vxor.u32 %v58840, %v58844 (stack76)
        %v58848 = vadd.s32 %v58840, %v58845 (stack65)
        %v58852 = vadd.s32 %v58848, %v8 (stack65)
        %v58854 = vshll.u32 %v58845, 24 (stack73)
        %v58855 = vshrl.u32 %v58845, 8 (stack74)
        %v58856 = vor.u32 %v58854, %v58855 (stack75)
        %v58857 = vxor.u32 %v58848, %v58856 (stack76)
        %v58860 = vadd.s32 %v58857, %v10 (stack65)
        %v58864 = vadd.s32 %v58860, 2 (stack65)
        %v58868 = vadd.s32 %v58852, %v58864 (stack65)
        %v58870 = vshll.u32 %v58864, 13 (stack73)
        %v58871 = vshrl.u32 %v58864, 19 (stack74)
        %v58872 = vor.u32 %v58870, %v58871 (stack75)
        %v58873 = vxor.u32 %v58868, %v58872 (stack76)
        %v58876 = vadd.s32 %v58868, %v58873 (stack65)
        %v58878 = vshll.u32 %v58873, 15 (stack73)
        %v58879 = vshrl.u32 %v58873, 17 (stack74)
        %v58880 = vor.u32 %v58878, %v58879 (stack75)
        %v58881 = vxor.u32 %v58876, %v58880 (stack76)
        %v58884 = vadd.s32 %v58876, %v58881 (stack65)
        %v58886 = vshll.u32 %v58881, 26 (stack73)
        %v58887 = vshrl.u32 %v58881, 6 (stack74)
        %v58888 = vor.u32 %v58886, %v58887 (stack75)
        %v58889 = vxor.u32 %v58884, %v58888 (stack76)
        %v58892 = vadd.s32 %v58884, %v58889 (stack65)
        %v58896 = vadd.s32 %v58892, %v10 (stack65)
        %v58898 = vshll.u32 %v58889, 6 (stack73)
        %v58899 = vshrl.u32 %v58889, 26 (stack74)
        %v58900 = vor.u32 %v58898, %v58899 (stack75)
        %v58901 = vxor.u32 %v58892, %v58900 (stack76)
        %v58904 = vadd.s32 %v58901, %v9 (stack65)
        %v58908 = vadd.s32 %v58904, 3 (stack65)
        %v58912 = vadd.s32 %v58896, %v58908 (stack65)
        %v58914 = vshll.u32 %v58908, 17 (stack73)
        %v58915 = vshrl.u32 %v58908, 15 (stack74)
        %v58916 = vor.u32 %v58914, %v58915 (stack75)
        %v58917 = vxor.u32 %v58912, %v58916 (stack76)
        %v58920 = vadd.s32 %v58912, %v58917 (stack65)
        %v58922 = vshll.u32 %v58917, 29 (stack73)
        %v58923 = vshrl.u32 %v58917, 3 (stack74)
        %v58924 = vor.u32 %v58922, %v58923 (stack75)
        %v58925 = vxor.u32 %v58920, %v58924 (stack76)
        %v58928 = vadd.s32 %v58920, %v58925 (stack65)
        %v58930 = vshll.u32 %v58925, 16 (stack73)
        %v58931 = vshrl.u32 %v58925, 16 (stack74)
        %v58932 = vor.u32 %v58930, %v58931 (stack75)
        %v58933 = vxor.u32 %v58928, %v58932 (stack76)
        %v58936 = vadd.s32 %v58928, %v58933 (stack65)
        %v58940 = vadd.s32 %v58936, %v9 (stack65)
        %v58942 = vshll.u32 %v58933, 24 (stack73)
        %v58943 = vshrl.u32 %v58933, 8 (stack74)
        %v58944 = vor.u32 %v58942, %v58943 (stack75)
        %v58945 = vxor.u32 %v58936, %v58944 (stack76)
        %v58948 = vadd.s32 %v58945, %v8 (stack65)
        %v58952 = vadd.s32 %v58948, 4 (stack65)
        %v58956 = vadd.s32 %v58940, %v58952 (stack65)
        %v58958 = vshll.u32 %v58952, 13 (stack73)
        %v58959 = vshrl.u32 %v58952, 19 (stack74)
        %v58960 = vor.u32 %v58958, %v58959 (stack75)
        %v58961 = vxor.u32 %v58956, %v58960 (stack76)
        %v58964 = vadd.s32 %v58956, %v58961 (stack65)
        %v58966 = vshll.u32 %v58961, 15 (stack73)
        %v58967 = vshrl.u32 %v58961, 17 (stack74)
        %v58968 = vor.u32 %v58966, %v58967 (stack75)
        %v58969 = vxor.u32 %v58964, %v58968 (stack76)
        %v58972 = vadd.s32 %v58964, %v58969 (stack65)
        %v58974 = vshll.u32 %v58969, 26 (stack73)
        %v58975 = vshrl.u32 %v58969, 6 (stack74)
        %v58976 = vor.u32 %v58974, %v58975 (stack75)
        %v58977 = vxor.u32 %v58972, %v58976 (stack76)
        %v58980 = vadd.s32 %v58972, %v58977 (stack65)
        %v58984 = vadd.s32 %v58980, %v8 (stack65)
        %v58986 = vshll.u32 %v58977, 6 (stack73)
        %v58987 = vshrl.u32 %v58977, 26 (stack74)
        %v58988 = vor.u32 %v58986, %v58987 (stack75)
        %v58989 = vxor.u32 %v58980, %v58988 (stack76)
        %v58992 = vadd.s32 %v58989, %v10 (stack65)
        %v58996 = vadd.s32 %v58992, 5 (stack65)
        %v58998 = vxor.u32 %v58984, %v58996 (stack76)
        %v58999 = vand.u32.u8 %v58998, 255 (stack77)
        %v59000 = vand.u32 %v58999, 65535 (stack78)
        %v59001 = vshrl.u32 %v59000, 1 (stack79)
        %v59002 = vor.u32 %v59001, 16256 (stack75)
        %v59003 = vand.u32.u16 %v59002, 65535 (stack80)
        %v59004 = vunpack.i.l.bf16 %v59003 (stack81)
        %v59008 = vadd.f32 %v59004, -1.0 (stack82)
        %v59012 = vmul.f32 %v59008, 2.0 (stack83)
        %v59016 = vadd.f32 %v59012, -0.99609375 (stack82)
        %v59020 = vmax.f32 -0.99609375, %v59016 (stack84)
        %v59022 = vand.u32 2147483647, %v59020 (stack85)
        %vm59025 = vcmp.eq.f32.partialorder %v59022, 1.0 (stack86)
        %v59030 = vmul.f32 %v59020, inf (stack83)
        %v59032 = vxor.u32 %v59020, 2147483648 (stack87)
        %v59035 = vmul.f32 %v59020, %v59032 (stack83)
        %v59037 = vadd.f32 %v59035, 1.0 (stack88)
        %v59038 = vlog2.pop %v59037 (stack89)
        %v59039 = vmul.f32 %v59038, 0.6931472 (stack90)
        %v59040 = vmul.f32 -0.5, %v59035 (stack91)
        %v59041 = vadd.f32 %v59040, 1.0 (stack92)
        %v59042 = vmul.f32 %v59041, %v59035 (stack93)
        %v59043 = vand.u32 2147483647, %v59035 (stack94)
        %vm59044 = vcmp.lt.f32.partialorder %v59043, 0.0004427343 (stack95)
        %v59045 = vsel /*vm=*/%vm59044, /*on_true_vy=*/%v59042, /*on_false_vx=*/%v59039 (stack96)
        %v59046 = vxor.u32 %v59045, 2147483648 (stack87)
        %vm59049 = vcmp.lt.f32.partialorder %v59046, 5.0 (stack86)
        %v59054 = vsel /*vm=*/%vm59049, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v59058 = vsel /*vm=*/%vm59049, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v59062 = vsel /*vm=*/%vm59049, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v59066 = vsel /*vm=*/%vm59049, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v59070 = vsel /*vm=*/%vm59049, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v59074 = vsel /*vm=*/%vm59049, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v59078 = vsel /*vm=*/%vm59049, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v59082 = vsel /*vm=*/%vm59049, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v59086 = vsel /*vm=*/%vm59049, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v59090 = vadd.f32 %v59046, -2.5 (stack82)
        %v59092 = vrsqrt.pop %v59046 (stack97)
        %v59093 = vmul.f32 %v59046, %v59092 (stack98)
        %vm59094 = vcmp.eq.f32.partialorder %v59046, inf (stack99)
        %v59095 = vsel /*vm=*/%vm59094, /*on_true_vy=*/%v59046, /*on_false_vx=*/%v59093 (stack100)
        %vm59096 = vcmp.eq.f32.partialorder %v59046, 0.0 (stack101)
        %v59097 = vand.u32 %v59046, 2147483648 (stack102)
        %v59098 = vsel /*vm=*/%vm59096, /*on_true_vy=*/%v59097, /*on_false_vx=*/%v59095 (stack103)
        %v59101 = vadd.f32 %v59098, -3.0 (stack82)
        %v59105 = vsel /*vm=*/%vm59049, /*on_true_vy=*/%v59090, /*on_false_vx=*/%v59101 (stack72)
        %v59109 = vmul.f32 %v59086, %v59105 (stack83)
        %v59113 = vadd.f32 %v59082, %v59109 (stack82)
        %v59117 = vmul.f32 %v59113, %v59105 (stack83)
        %v59121 = vadd.f32 %v59078, %v59117 (stack82)
        %v59125 = vmul.f32 %v59121, %v59105 (stack83)
        %v59129 = vadd.f32 %v59074, %v59125 (stack82)
        %v59133 = vmul.f32 %v59129, %v59105 (stack83)
        %v59137 = vadd.f32 %v59070, %v59133 (stack82)
        %v59141 = vmul.f32 %v59137, %v59105 (stack83)
        %v59145 = vadd.f32 %v59066, %v59141 (stack82)
        %v59149 = vmul.f32 %v59145, %v59105 (stack83)
        %v59153 = vadd.f32 %v59062, %v59149 (stack82)
        %v59157 = vmul.f32 %v59153, %v59105 (stack83)
        %v59161 = vadd.f32 %v59058, %v59157 (stack82)
        %v59165 = vmul.f32 %v59161, %v59105 (stack83)
        %v59169 = vadd.f32 %v59054, %v59165 (stack82)
        %v59173 = vmul.f32 %v59169, %v59020 (stack83)
        %v59177 = vsel /*vm=*/%vm59025, /*on_true_vy=*/%v59030, /*on_false_vx=*/%v59173 (stack72)
        %v59181 = vmul.f32 %v59177, 1.4140625 (stack83)
        %s59183 = scalar_lea.vmem %s280, 700 [#allocation0] (stack107)
        %v59184 = vpack.c.bf16 0.0, %v59181 (stack104)
        %59185 = vst [vmem:[%s59183] sm:$0xf] /*vst_source=*/%v59184 (stack105)
        %v59188 = vadd.s32 %v3329, %v56419 (stack65)
        %s59190 = smul.u32 128, %s27 (stack66)
        %v59191 = vlaneseq (stack67)
        %v59192 = vand.u32 %v59191, 127 (stack68)
        %v59193 = vstv %s59190 (stack69)
        %v59194 = vadd.s32 %v59192, %v59193 (stack70)
        %v59198 = vadd.s32 %v59188, %v59194 (stack65)
        %vm59202 = vcmp.lt.u32.totalorder %v59198, %v59188 (stack71)
        %vm59207 = vcmp.lt.u32.totalorder %v59188, %v3329 (stack71)
        %v59212 = vadd.s32 %v3316, %v56402 (stack65)
        %v59216 = vadd.s32 %v59212, 1 (stack65)
        %v59220 = vsel /*vm=*/%vm59207, /*on_true_vy=*/%v59216, /*on_false_vx=*/%v59212 (stack72)
        %v59224 = vadd.s32 %v59220, 1 (stack65)
        %v59228 = vsel /*vm=*/%vm59202, /*on_true_vy=*/%v59224, /*on_false_vx=*/%v59220 (stack72)
        %v59233 = vadd.s32 %v59228, %v10 (stack65)
        %v59237 = vadd.s32 %v59198, %v9 (stack65)
        %v59241 = vadd.s32 %v59233, %v59237 (stack65)
        %v59243 = vshll.u32 %v59237, 13 (stack73)
        %v59244 = vshrl.u32 %v59237, 19 (stack74)
        %v59245 = vor.u32 %v59243, %v59244 (stack75)
        %v59246 = vxor.u32 %v59241, %v59245 (stack76)
        %v59249 = vadd.s32 %v59241, %v59246 (stack65)
        %v59251 = vshll.u32 %v59246, 15 (stack73)
        %v59252 = vshrl.u32 %v59246, 17 (stack74)
        %v59253 = vor.u32 %v59251, %v59252 (stack75)
        %v59254 = vxor.u32 %v59249, %v59253 (stack76)
        %v59257 = vadd.s32 %v59249, %v59254 (stack65)
        %v59259 = vshll.u32 %v59254, 26 (stack73)
        %v59260 = vshrl.u32 %v59254, 6 (stack74)
        %v59261 = vor.u32 %v59259, %v59260 (stack75)
        %v59262 = vxor.u32 %v59257, %v59261 (stack76)
        %v59265 = vadd.s32 %v59257, %v59262 (stack65)
        %v59269 = vadd.s32 %v59265, %v9 (stack65)
        %v59271 = vshll.u32 %v59262, 6 (stack73)
        %v59272 = vshrl.u32 %v59262, 26 (stack74)
        %v59273 = vor.u32 %v59271, %v59272 (stack75)
        %v59274 = vxor.u32 %v59265, %v59273 (stack76)
        %v59277 = vadd.s32 %v59274, %v8 (stack65)
        %v59281 = vadd.s32 %v59277, 1 (stack65)
        %v59285 = vadd.s32 %v59269, %v59281 (stack65)
        %v59287 = vshll.u32 %v59281, 17 (stack73)
        %v59288 = vshrl.u32 %v59281, 15 (stack74)
        %v59289 = vor.u32 %v59287, %v59288 (stack75)
        %v59290 = vxor.u32 %v59285, %v59289 (stack76)
        %v59293 = vadd.s32 %v59285, %v59290 (stack65)
        %v59295 = vshll.u32 %v59290, 29 (stack73)
        %v59296 = vshrl.u32 %v59290, 3 (stack74)
        %v59297 = vor.u32 %v59295, %v59296 (stack75)
        %v59298 = vxor.u32 %v59293, %v59297 (stack76)
        %v59301 = vadd.s32 %v59293, %v59298 (stack65)
        %v59303 = vshll.u32 %v59298, 16 (stack73)
        %v59304 = vshrl.u32 %v59298, 16 (stack74)
        %v59305 = vor.u32 %v59303, %v59304 (stack75)
        %v59306 = vxor.u32 %v59301, %v59305 (stack76)
        %v59309 = vadd.s32 %v59301, %v59306 (stack65)
        %v59313 = vadd.s32 %v59309, %v8 (stack65)
        %v59315 = vshll.u32 %v59306, 24 (stack73)
        %v59316 = vshrl.u32 %v59306, 8 (stack74)
        %v59317 = vor.u32 %v59315, %v59316 (stack75)
        %v59318 = vxor.u32 %v59309, %v59317 (stack76)
        %v59321 = vadd.s32 %v59318, %v10 (stack65)
        %v59325 = vadd.s32 %v59321, 2 (stack65)
        %v59329 = vadd.s32 %v59313, %v59325 (stack65)
        %v59331 = vshll.u32 %v59325, 13 (stack73)
        %v59332 = vshrl.u32 %v59325, 19 (stack74)
        %v59333 = vor.u32 %v59331, %v59332 (stack75)
        %v59334 = vxor.u32 %v59329, %v59333 (stack76)
        %v59337 = vadd.s32 %v59329, %v59334 (stack65)
        %v59339 = vshll.u32 %v59334, 15 (stack73)
        %v59340 = vshrl.u32 %v59334, 17 (stack74)
        %v59341 = vor.u32 %v59339, %v59340 (stack75)
        %v59342 = vxor.u32 %v59337, %v59341 (stack76)
        %v59345 = vadd.s32 %v59337, %v59342 (stack65)
        %v59347 = vshll.u32 %v59342, 26 (stack73)
        %v59348 = vshrl.u32 %v59342, 6 (stack74)
        %v59349 = vor.u32 %v59347, %v59348 (stack75)
        %v59350 = vxor.u32 %v59345, %v59349 (stack76)
        %v59353 = vadd.s32 %v59345, %v59350 (stack65)
        %v59357 = vadd.s32 %v59353, %v10 (stack65)
        %v59359 = vshll.u32 %v59350, 6 (stack73)
        %v59360 = vshrl.u32 %v59350, 26 (stack74)
        %v59361 = vor.u32 %v59359, %v59360 (stack75)
        %v59362 = vxor.u32 %v59353, %v59361 (stack76)
        %v59365 = vadd.s32 %v59362, %v9 (stack65)
        %v59369 = vadd.s32 %v59365, 3 (stack65)
        %v59373 = vadd.s32 %v59357, %v59369 (stack65)
        %v59375 = vshll.u32 %v59369, 17 (stack73)
        %v59376 = vshrl.u32 %v59369, 15 (stack74)
        %v59377 = vor.u32 %v59375, %v59376 (stack75)
        %v59378 = vxor.u32 %v59373, %v59377 (stack76)
        %v59381 = vadd.s32 %v59373, %v59378 (stack65)
        %v59383 = vshll.u32 %v59378, 29 (stack73)
        %v59384 = vshrl.u32 %v59378, 3 (stack74)
        %v59385 = vor.u32 %v59383, %v59384 (stack75)
        %v59386 = vxor.u32 %v59381, %v59385 (stack76)
        %v59389 = vadd.s32 %v59381, %v59386 (stack65)
        %v59391 = vshll.u32 %v59386, 16 (stack73)
        %v59392 = vshrl.u32 %v59386, 16 (stack74)
        %v59393 = vor.u32 %v59391, %v59392 (stack75)
        %v59394 = vxor.u32 %v59389, %v59393 (stack76)
        %v59397 = vadd.s32 %v59389, %v59394 (stack65)
        %v59401 = vadd.s32 %v59397, %v9 (stack65)
        %v59403 = vshll.u32 %v59394, 24 (stack73)
        %v59404 = vshrl.u32 %v59394, 8 (stack74)
        %v59405 = vor.u32 %v59403, %v59404 (stack75)
        %v59406 = vxor.u32 %v59397, %v59405 (stack76)
        %v59409 = vadd.s32 %v59406, %v8 (stack65)
        %v59413 = vadd.s32 %v59409, 4 (stack65)
        %v59417 = vadd.s32 %v59401, %v59413 (stack65)
        %v59419 = vshll.u32 %v59413, 13 (stack73)
        %v59420 = vshrl.u32 %v59413, 19 (stack74)
        %v59421 = vor.u32 %v59419, %v59420 (stack75)
        %v59422 = vxor.u32 %v59417, %v59421 (stack76)
        %v59425 = vadd.s32 %v59417, %v59422 (stack65)
        %v59427 = vshll.u32 %v59422, 15 (stack73)
        %v59428 = vshrl.u32 %v59422, 17 (stack74)
        %v59429 = vor.u32 %v59427, %v59428 (stack75)
        %v59430 = vxor.u32 %v59425, %v59429 (stack76)
        %v59433 = vadd.s32 %v59425, %v59430 (stack65)
        %v59435 = vshll.u32 %v59430, 26 (stack73)
        %v59436 = vshrl.u32 %v59430, 6 (stack74)
        %v59437 = vor.u32 %v59435, %v59436 (stack75)
        %v59438 = vxor.u32 %v59433, %v59437 (stack76)
        %v59441 = vadd.s32 %v59433, %v59438 (stack65)
        %v59445 = vadd.s32 %v59441, %v8 (stack65)
        %v59447 = vshll.u32 %v59438, 6 (stack73)
        %v59448 = vshrl.u32 %v59438, 26 (stack74)
        %v59449 = vor.u32 %v59447, %v59448 (stack75)
        %v59450 = vxor.u32 %v59441, %v59449 (stack76)
        %v59453 = vadd.s32 %v59450, %v10 (stack65)
        %v59457 = vadd.s32 %v59453, 5 (stack65)
        %v59459 = vxor.u32 %v59445, %v59457 (stack76)
        %v59460 = vand.u32.u8 %v59459, 255 (stack77)
        %v59461 = vand.u32 %v59460, 65535 (stack78)
        %v59462 = vshrl.u32 %v59461, 1 (stack79)
        %v59463 = vor.u32 %v59462, 16256 (stack75)
        %v59464 = vand.u32.u16 %v59463, 65535 (stack80)
        %v59465 = vunpack.i.l.bf16 %v59464 (stack81)
        %v59469 = vadd.f32 %v59465, -1.0 (stack82)
        %v59473 = vmul.f32 %v59469, 2.0 (stack83)
        %v59477 = vadd.f32 %v59473, -0.99609375 (stack82)
        %v59481 = vmax.f32 -0.99609375, %v59477 (stack84)
        %v59483 = vand.u32 2147483647, %v59481 (stack85)
        %vm59486 = vcmp.eq.f32.partialorder %v59483, 1.0 (stack86)
        %v59491 = vmul.f32 %v59481, inf (stack83)
        %v59493 = vxor.u32 %v59481, 2147483648 (stack87)
        %v59496 = vmul.f32 %v59481, %v59493 (stack83)
        %v59498 = vadd.f32 %v59496, 1.0 (stack88)
        %v59499 = vlog2.pop %v59498 (stack89)
        %v59500 = vmul.f32 %v59499, 0.6931472 (stack90)
        %v59501 = vmul.f32 -0.5, %v59496 (stack91)
        %v59502 = vadd.f32 %v59501, 1.0 (stack92)
        %v59503 = vmul.f32 %v59502, %v59496 (stack93)
        %v59504 = vand.u32 2147483647, %v59496 (stack94)
        %vm59505 = vcmp.lt.f32.partialorder %v59504, 0.0004427343 (stack95)
        %v59506 = vsel /*vm=*/%vm59505, /*on_true_vy=*/%v59503, /*on_false_vx=*/%v59500 (stack96)
        %v59507 = vxor.u32 %v59506, 2147483648 (stack87)
        %vm59510 = vcmp.lt.f32.partialorder %v59507, 5.0 (stack86)
        %v59515 = vsel /*vm=*/%vm59510, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v59519 = vsel /*vm=*/%vm59510, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v59523 = vsel /*vm=*/%vm59510, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v59527 = vsel /*vm=*/%vm59510, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v59531 = vsel /*vm=*/%vm59510, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v59535 = vsel /*vm=*/%vm59510, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v59539 = vsel /*vm=*/%vm59510, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v59543 = vsel /*vm=*/%vm59510, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v59547 = vsel /*vm=*/%vm59510, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v59551 = vadd.f32 %v59507, -2.5 (stack82)
        %v59553 = vrsqrt.pop %v59507 (stack97)
        %v59554 = vmul.f32 %v59507, %v59553 (stack98)
        %vm59555 = vcmp.eq.f32.partialorder %v59507, inf (stack99)
        %v59556 = vsel /*vm=*/%vm59555, /*on_true_vy=*/%v59507, /*on_false_vx=*/%v59554 (stack100)
        %vm59557 = vcmp.eq.f32.partialorder %v59507, 0.0 (stack101)
        %v59558 = vand.u32 %v59507, 2147483648 (stack102)
        %v59559 = vsel /*vm=*/%vm59557, /*on_true_vy=*/%v59558, /*on_false_vx=*/%v59556 (stack103)
        %v59562 = vadd.f32 %v59559, -3.0 (stack82)
        %v59566 = vsel /*vm=*/%vm59510, /*on_true_vy=*/%v59551, /*on_false_vx=*/%v59562 (stack72)
        %v59570 = vmul.f32 %v59547, %v59566 (stack83)
        %v59574 = vadd.f32 %v59543, %v59570 (stack82)
        %v59578 = vmul.f32 %v59574, %v59566 (stack83)
        %v59582 = vadd.f32 %v59539, %v59578 (stack82)
        %v59586 = vmul.f32 %v59582, %v59566 (stack83)
        %v59590 = vadd.f32 %v59535, %v59586 (stack82)
        %v59594 = vmul.f32 %v59590, %v59566 (stack83)
        %v59598 = vadd.f32 %v59531, %v59594 (stack82)
        %v59602 = vmul.f32 %v59598, %v59566 (stack83)
        %v59606 = vadd.f32 %v59527, %v59602 (stack82)
        %v59610 = vmul.f32 %v59606, %v59566 (stack83)
        %v59614 = vadd.f32 %v59523, %v59610 (stack82)
        %v59618 = vmul.f32 %v59614, %v59566 (stack83)
        %v59622 = vadd.f32 %v59519, %v59618 (stack82)
        %v59626 = vmul.f32 %v59622, %v59566 (stack83)
        %v59630 = vadd.f32 %v59515, %v59626 (stack82)
        %v59634 = vmul.f32 %v59630, %v59481 (stack83)
        %v59638 = vsel /*vm=*/%vm59486, /*on_true_vy=*/%v59491, /*on_false_vx=*/%v59634 (stack72)
        %v59642 = vmul.f32 %v59638, 1.4140625 (stack83)
        %s59644 = scalar_lea.vmem %s280, 828 [#allocation0] (stack107)
        %v59645 = vpack.c.bf16 0.0, %v59642 (stack104)
        %59646 = vst [vmem:[%s59644] sm:$0xf] /*vst_source=*/%v59645 (stack105)
        %v59649 = vadd.s32 %v3816, %v56419 (stack65)
        %s59651 = smul.u32 128, %s27 (stack66)
        %v59652 = vlaneseq (stack67)
        %v59653 = vand.u32 %v59652, 127 (stack68)
        %v59654 = vstv %s59651 (stack69)
        %v59655 = vadd.s32 %v59653, %v59654 (stack70)
        %v59659 = vadd.s32 %v59649, %v59655 (stack65)
        %vm59663 = vcmp.lt.u32.totalorder %v59659, %v59649 (stack71)
        %vm59668 = vcmp.lt.u32.totalorder %v59649, %v3816 (stack71)
        %v59673 = vadd.s32 %v3803, %v56402 (stack65)
        %v59677 = vadd.s32 %v59673, 1 (stack65)
        %v59681 = vsel /*vm=*/%vm59668, /*on_true_vy=*/%v59677, /*on_false_vx=*/%v59673 (stack72)
        %v59685 = vadd.s32 %v59681, 1 (stack65)
        %v59689 = vsel /*vm=*/%vm59663, /*on_true_vy=*/%v59685, /*on_false_vx=*/%v59681 (stack72)
        %v59694 = vadd.s32 %v59689, %v10 (stack65)
        %v59698 = vadd.s32 %v59659, %v9 (stack65)
        %v59702 = vadd.s32 %v59694, %v59698 (stack65)
        %v59704 = vshll.u32 %v59698, 13 (stack73)
        %v59705 = vshrl.u32 %v59698, 19 (stack74)
        %v59706 = vor.u32 %v59704, %v59705 (stack75)
        %v59707 = vxor.u32 %v59702, %v59706 (stack76)
        %v59710 = vadd.s32 %v59702, %v59707 (stack65)
        %v59712 = vshll.u32 %v59707, 15 (stack73)
        %v59713 = vshrl.u32 %v59707, 17 (stack74)
        %v59714 = vor.u32 %v59712, %v59713 (stack75)
        %v59715 = vxor.u32 %v59710, %v59714 (stack76)
        %v59718 = vadd.s32 %v59710, %v59715 (stack65)
        %v59720 = vshll.u32 %v59715, 26 (stack73)
        %v59721 = vshrl.u32 %v59715, 6 (stack74)
        %v59722 = vor.u32 %v59720, %v59721 (stack75)
        %v59723 = vxor.u32 %v59718, %v59722 (stack76)
        %v59726 = vadd.s32 %v59718, %v59723 (stack65)
        %v59730 = vadd.s32 %v59726, %v9 (stack65)
        %v59732 = vshll.u32 %v59723, 6 (stack73)
        %v59733 = vshrl.u32 %v59723, 26 (stack74)
        %v59734 = vor.u32 %v59732, %v59733 (stack75)
        %v59735 = vxor.u32 %v59726, %v59734 (stack76)
        %v59738 = vadd.s32 %v59735, %v8 (stack65)
        %v59742 = vadd.s32 %v59738, 1 (stack65)
        %v59746 = vadd.s32 %v59730, %v59742 (stack65)
        %v59748 = vshll.u32 %v59742, 17 (stack73)
        %v59749 = vshrl.u32 %v59742, 15 (stack74)
        %v59750 = vor.u32 %v59748, %v59749 (stack75)
        %v59751 = vxor.u32 %v59746, %v59750 (stack76)
        %v59754 = vadd.s32 %v59746, %v59751 (stack65)
        %v59756 = vshll.u32 %v59751, 29 (stack73)
        %v59757 = vshrl.u32 %v59751, 3 (stack74)
        %v59758 = vor.u32 %v59756, %v59757 (stack75)
        %v59759 = vxor.u32 %v59754, %v59758 (stack76)
        %v59762 = vadd.s32 %v59754, %v59759 (stack65)
        %v59764 = vshll.u32 %v59759, 16 (stack73)
        %v59765 = vshrl.u32 %v59759, 16 (stack74)
        %v59766 = vor.u32 %v59764, %v59765 (stack75)
        %v59767 = vxor.u32 %v59762, %v59766 (stack76)
        %v59770 = vadd.s32 %v59762, %v59767 (stack65)
        %v59774 = vadd.s32 %v59770, %v8 (stack65)
        %v59776 = vshll.u32 %v59767, 24 (stack73)
        %v59777 = vshrl.u32 %v59767, 8 (stack74)
        %v59778 = vor.u32 %v59776, %v59777 (stack75)
        %v59779 = vxor.u32 %v59770, %v59778 (stack76)
        %v59782 = vadd.s32 %v59779, %v10 (stack65)
        %v59786 = vadd.s32 %v59782, 2 (stack65)
        %v59790 = vadd.s32 %v59774, %v59786 (stack65)
        %v59792 = vshll.u32 %v59786, 13 (stack73)
        %v59793 = vshrl.u32 %v59786, 19 (stack74)
        %v59794 = vor.u32 %v59792, %v59793 (stack75)
        %v59795 = vxor.u32 %v59790, %v59794 (stack76)
        %v59798 = vadd.s32 %v59790, %v59795 (stack65)
        %v59800 = vshll.u32 %v59795, 15 (stack73)
        %v59801 = vshrl.u32 %v59795, 17 (stack74)
        %v59802 = vor.u32 %v59800, %v59801 (stack75)
        %v59803 = vxor.u32 %v59798, %v59802 (stack76)
        %v59806 = vadd.s32 %v59798, %v59803 (stack65)
        %v59808 = vshll.u32 %v59803, 26 (stack73)
        %v59809 = vshrl.u32 %v59803, 6 (stack74)
        %v59810 = vor.u32 %v59808, %v59809 (stack75)
        %v59811 = vxor.u32 %v59806, %v59810 (stack76)
        %v59814 = vadd.s32 %v59806, %v59811 (stack65)
        %v59818 = vadd.s32 %v59814, %v10 (stack65)
        %v59820 = vshll.u32 %v59811, 6 (stack73)
        %v59821 = vshrl.u32 %v59811, 26 (stack74)
        %v59822 = vor.u32 %v59820, %v59821 (stack75)
        %v59823 = vxor.u32 %v59814, %v59822 (stack76)
        %v59826 = vadd.s32 %v59823, %v9 (stack65)
        %v59830 = vadd.s32 %v59826, 3 (stack65)
        %v59834 = vadd.s32 %v59818, %v59830 (stack65)
        %v59836 = vshll.u32 %v59830, 17 (stack73)
        %v59837 = vshrl.u32 %v59830, 15 (stack74)
        %v59838 = vor.u32 %v59836, %v59837 (stack75)
        %v59839 = vxor.u32 %v59834, %v59838 (stack76)
        %v59842 = vadd.s32 %v59834, %v59839 (stack65)
        %v59844 = vshll.u32 %v59839, 29 (stack73)
        %v59845 = vshrl.u32 %v59839, 3 (stack74)
        %v59846 = vor.u32 %v59844, %v59845 (stack75)
        %v59847 = vxor.u32 %v59842, %v59846 (stack76)
        %v59850 = vadd.s32 %v59842, %v59847 (stack65)
        %v59852 = vshll.u32 %v59847, 16 (stack73)
        %v59853 = vshrl.u32 %v59847, 16 (stack74)
        %v59854 = vor.u32 %v59852, %v59853 (stack75)
        %v59855 = vxor.u32 %v59850, %v59854 (stack76)
        %v59858 = vadd.s32 %v59850, %v59855 (stack65)
        %v59862 = vadd.s32 %v59858, %v9 (stack65)
        %v59864 = vshll.u32 %v59855, 24 (stack73)
        %v59865 = vshrl.u32 %v59855, 8 (stack74)
        %v59866 = vor.u32 %v59864, %v59865 (stack75)
        %v59867 = vxor.u32 %v59858, %v59866 (stack76)
        %v59870 = vadd.s32 %v59867, %v8 (stack65)
        %v59874 = vadd.s32 %v59870, 4 (stack65)
        %v59878 = vadd.s32 %v59862, %v59874 (stack65)
        %v59880 = vshll.u32 %v59874, 13 (stack73)
        %v59881 = vshrl.u32 %v59874, 19 (stack74)
        %v59882 = vor.u32 %v59880, %v59881 (stack75)
        %v59883 = vxor.u32 %v59878, %v59882 (stack76)
        %v59886 = vadd.s32 %v59878, %v59883 (stack65)
        %v59888 = vshll.u32 %v59883, 15 (stack73)
        %v59889 = vshrl.u32 %v59883, 17 (stack74)
        %v59890 = vor.u32 %v59888, %v59889 (stack75)
        %v59891 = vxor.u32 %v59886, %v59890 (stack76)
        %v59894 = vadd.s32 %v59886, %v59891 (stack65)
        %v59896 = vshll.u32 %v59891, 26 (stack73)
        %v59897 = vshrl.u32 %v59891, 6 (stack74)
        %v59898 = vor.u32 %v59896, %v59897 (stack75)
        %v59899 = vxor.u32 %v59894, %v59898 (stack76)
        %v59902 = vadd.s32 %v59894, %v59899 (stack65)
        %v59906 = vadd.s32 %v59902, %v8 (stack65)
        %v59908 = vshll.u32 %v59899, 6 (stack73)
        %v59909 = vshrl.u32 %v59899, 26 (stack74)
        %v59910 = vor.u32 %v59908, %v59909 (stack75)
        %v59911 = vxor.u32 %v59902, %v59910 (stack76)
        %v59914 = vadd.s32 %v59911, %v10 (stack65)
        %v59918 = vadd.s32 %v59914, 5 (stack65)
        %v59920 = vxor.u32 %v59906, %v59918 (stack76)
        %v59921 = vand.u32.u8 %v59920, 255 (stack77)
        %v59922 = vand.u32 %v59921, 65535 (stack78)
        %v59923 = vshrl.u32 %v59922, 1 (stack79)
        %v59924 = vor.u32 %v59923, 16256 (stack75)
        %v59925 = vand.u32.u16 %v59924, 65535 (stack80)
        %v59926 = vunpack.i.l.bf16 %v59925 (stack81)
        %v59930 = vadd.f32 %v59926, -1.0 (stack82)
        %v59934 = vmul.f32 %v59930, 2.0 (stack83)
        %v59938 = vadd.f32 %v59934, -0.99609375 (stack82)
        %v59942 = vmax.f32 -0.99609375, %v59938 (stack84)
        %v59944 = vand.u32 2147483647, %v59942 (stack85)
        %vm59947 = vcmp.eq.f32.partialorder %v59944, 1.0 (stack86)
        %v59952 = vmul.f32 %v59942, inf (stack83)
        %v59954 = vxor.u32 %v59942, 2147483648 (stack87)
        %v59957 = vmul.f32 %v59942, %v59954 (stack83)
        %v59959 = vadd.f32 %v59957, 1.0 (stack88)
        %v59960 = vlog2.pop %v59959 (stack89)
        %v59961 = vmul.f32 %v59960, 0.6931472 (stack90)
        %v59962 = vmul.f32 -0.5, %v59957 (stack91)
        %v59963 = vadd.f32 %v59962, 1.0 (stack92)
        %v59964 = vmul.f32 %v59963, %v59957 (stack93)
        %v59965 = vand.u32 2147483647, %v59957 (stack94)
        %vm59966 = vcmp.lt.f32.partialorder %v59965, 0.0004427343 (stack95)
        %v59967 = vsel /*vm=*/%vm59966, /*on_true_vy=*/%v59964, /*on_false_vx=*/%v59961 (stack96)
        %v59968 = vxor.u32 %v59967, 2147483648 (stack87)
        %vm59971 = vcmp.lt.f32.partialorder %v59968, 5.0 (stack86)
        %v59976 = vsel /*vm=*/%vm59971, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v59980 = vsel /*vm=*/%vm59971, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v59984 = vsel /*vm=*/%vm59971, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v59988 = vsel /*vm=*/%vm59971, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v59992 = vsel /*vm=*/%vm59971, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v59996 = vsel /*vm=*/%vm59971, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v60000 = vsel /*vm=*/%vm59971, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v60004 = vsel /*vm=*/%vm59971, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v60008 = vsel /*vm=*/%vm59971, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v60012 = vadd.f32 %v59968, -2.5 (stack82)
        %v60014 = vrsqrt.pop %v59968 (stack97)
        %v60015 = vmul.f32 %v59968, %v60014 (stack98)
        %vm60016 = vcmp.eq.f32.partialorder %v59968, inf (stack99)
        %v60017 = vsel /*vm=*/%vm60016, /*on_true_vy=*/%v59968, /*on_false_vx=*/%v60015 (stack100)
        %vm60018 = vcmp.eq.f32.partialorder %v59968, 0.0 (stack101)
        %v60019 = vand.u32 %v59968, 2147483648 (stack102)
        %v60020 = vsel /*vm=*/%vm60018, /*on_true_vy=*/%v60019, /*on_false_vx=*/%v60017 (stack103)
        %v60023 = vadd.f32 %v60020, -3.0 (stack82)
        %v60027 = vsel /*vm=*/%vm59971, /*on_true_vy=*/%v60012, /*on_false_vx=*/%v60023 (stack72)
        %v60031 = vmul.f32 %v60008, %v60027 (stack83)
        %v60035 = vadd.f32 %v60004, %v60031 (stack82)
        %v60039 = vmul.f32 %v60035, %v60027 (stack83)
        %v60043 = vadd.f32 %v60000, %v60039 (stack82)
        %v60047 = vmul.f32 %v60043, %v60027 (stack83)
        %v60051 = vadd.f32 %v59996, %v60047 (stack82)
        %v60055 = vmul.f32 %v60051, %v60027 (stack83)
        %v60059 = vadd.f32 %v59992, %v60055 (stack82)
        %v60063 = vmul.f32 %v60059, %v60027 (stack83)
        %v60067 = vadd.f32 %v59988, %v60063 (stack82)
        %v60071 = vmul.f32 %v60067, %v60027 (stack83)
        %v60075 = vadd.f32 %v59984, %v60071 (stack82)
        %v60079 = vmul.f32 %v60075, %v60027 (stack83)
        %v60083 = vadd.f32 %v59980, %v60079 (stack82)
        %v60087 = vmul.f32 %v60083, %v60027 (stack83)
        %v60091 = vadd.f32 %v59976, %v60087 (stack82)
        %v60095 = vmul.f32 %v60091, %v59942 (stack83)
        %v60099 = vsel /*vm=*/%vm59947, /*on_true_vy=*/%v59952, /*on_false_vx=*/%v60095 (stack72)
        %v60103 = vmul.f32 %v60099, 1.4140625 (stack83)
        %s60105 = scalar_lea.vmem %s280, 956 [#allocation0] (stack107)
        %v60106 = vpack.c.bf16 0.0, %v60103 (stack104)
        %60107 = vst [vmem:[%s60105] sm:$0xf] /*vst_source=*/%v60106 (stack105)
        %s60108 = sadd.s32 %s339, 128 (stack106)
        %s60109 = sshrl.u32 %s60108, 10 (stack49)
        %p60110 = scmp.lt.s32.totalorder 1, %s60109 (stack50)
        %s60111 = scalar_select /*predicate=*/%p60110, /*on_true=*/1, /*on_false=*/%s60109 (stack51)
        %s60112 = sand.u32 %s60108, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s60113 = sshrl.u32 %s60112, 7 (stack53)
        %s60114 = sand.u32 %s60112, 127 /* smod.u32 w/div 128 */ (stack54)
        %s60115 = smul.addr %s60111, 8 (stack55)
        %s60116 = scalar_lea.vmem %s3, %s60115 (stack56)
        %s60118 = scalar_lea.vmem %s60116, %s60113 (stack57)
        %v60119 = vld [vmem:[%s60118] ss:$0 sm:$0xff] (stack58)
        %s60120 = sand.u32 %s60114, 255 (stack59)
        %s60122 = sor.u32 256, %s60120 (stack60)
        %60123 = vbcast.lane.b32.xlu0 %v60119, %s60122 (stack61)
        %v60124 = vpop.permute.xlu0 %60123 (stack62)
        %s60125 = sadd.s32 %s347, 128 (stack106)
        %s60126 = sshrl.u32 %s60125, 10 (stack49)
        %p60127 = scmp.lt.s32.totalorder 1, %s60126 (stack50)
        %s60128 = scalar_select /*predicate=*/%p60127, /*on_true=*/1, /*on_false=*/%s60126 (stack51)
        %s60129 = sand.u32 %s60125, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s60130 = sshrl.u32 %s60129, 7 (stack53)
        %s60131 = sand.u32 %s60129, 127 /* smod.u32 w/div 128 */ (stack54)
        %s60132 = smul.addr %s60128, 8 (stack55)
        %s60133 = scalar_lea.vmem %s5, %s60132 (stack56)
        %s60135 = scalar_lea.vmem %s60133, %s60130 (stack57)
        %v60136 = vld [vmem:[%s60135] ss:$0 sm:$0xff] (stack58)
        %s60137 = sand.u32 %s60131, 255 (stack59)
        %s60139 = sor.u32 256, %s60137 (stack60)
        %60140 = vbcast.lane.b32.xlu0 %v60136, %s60139 (stack61)
        %v60141 = vpop.permute.xlu0 %60140 (stack62)
        %v60144 = vadd.s32 %v408, %v60141 (stack65)
        %s60146 = smul.u32 128, %s27 (stack66)
        %v60147 = vlaneseq (stack67)
        %v60148 = vand.u32 %v60147, 127 (stack68)
        %v60149 = vstv %s60146 (stack69)
        %v60150 = vadd.s32 %v60148, %v60149 (stack70)
        %v60154 = vadd.s32 %v60144, %v60150 (stack65)
        %vm60158 = vcmp.lt.u32.totalorder %v60154, %v60144 (stack71)
        %vm60163 = vcmp.lt.u32.totalorder %v60144, %v408 (stack71)
        %v60168 = vadd.s32 %v380, %v60124 (stack65)
        %v60172 = vadd.s32 %v60168, 1 (stack65)
        %v60176 = vsel /*vm=*/%vm60163, /*on_true_vy=*/%v60172, /*on_false_vx=*/%v60168 (stack72)
        %v60180 = vadd.s32 %v60176, 1 (stack65)
        %v60184 = vsel /*vm=*/%vm60158, /*on_true_vy=*/%v60180, /*on_false_vx=*/%v60176 (stack72)
        %v60189 = vadd.s32 %v60184, %v10 (stack65)
        %v60193 = vadd.s32 %v60154, %v9 (stack65)
        %v60197 = vadd.s32 %v60189, %v60193 (stack65)
        %v60199 = vshll.u32 %v60193, 13 (stack73)
        %v60200 = vshrl.u32 %v60193, 19 (stack74)
        %v60201 = vor.u32 %v60199, %v60200 (stack75)
        %v60202 = vxor.u32 %v60197, %v60201 (stack76)
        %v60205 = vadd.s32 %v60197, %v60202 (stack65)
        %v60207 = vshll.u32 %v60202, 15 (stack73)
        %v60208 = vshrl.u32 %v60202, 17 (stack74)
        %v60209 = vor.u32 %v60207, %v60208 (stack75)
        %v60210 = vxor.u32 %v60205, %v60209 (stack76)
        %v60213 = vadd.s32 %v60205, %v60210 (stack65)
        %v60215 = vshll.u32 %v60210, 26 (stack73)
        %v60216 = vshrl.u32 %v60210, 6 (stack74)
        %v60217 = vor.u32 %v60215, %v60216 (stack75)
        %v60218 = vxor.u32 %v60213, %v60217 (stack76)
        %v60221 = vadd.s32 %v60213, %v60218 (stack65)
        %v60225 = vadd.s32 %v60221, %v9 (stack65)
        %v60227 = vshll.u32 %v60218, 6 (stack73)
        %v60228 = vshrl.u32 %v60218, 26 (stack74)
        %v60229 = vor.u32 %v60227, %v60228 (stack75)
        %v60230 = vxor.u32 %v60221, %v60229 (stack76)
        %v60233 = vadd.s32 %v60230, %v8 (stack65)
        %v60237 = vadd.s32 %v60233, 1 (stack65)
        %v60241 = vadd.s32 %v60225, %v60237 (stack65)
        %v60243 = vshll.u32 %v60237, 17 (stack73)
        %v60244 = vshrl.u32 %v60237, 15 (stack74)
        %v60245 = vor.u32 %v60243, %v60244 (stack75)
        %v60246 = vxor.u32 %v60241, %v60245 (stack76)
        %v60249 = vadd.s32 %v60241, %v60246 (stack65)
        %v60251 = vshll.u32 %v60246, 29 (stack73)
        %v60252 = vshrl.u32 %v60246, 3 (stack74)
        %v60253 = vor.u32 %v60251, %v60252 (stack75)
        %v60254 = vxor.u32 %v60249, %v60253 (stack76)
        %v60257 = vadd.s32 %v60249, %v60254 (stack65)
        %v60259 = vshll.u32 %v60254, 16 (stack73)
        %v60260 = vshrl.u32 %v60254, 16 (stack74)
        %v60261 = vor.u32 %v60259, %v60260 (stack75)
        %v60262 = vxor.u32 %v60257, %v60261 (stack76)
        %v60265 = vadd.s32 %v60257, %v60262 (stack65)
        %v60269 = vadd.s32 %v60265, %v8 (stack65)
        %v60271 = vshll.u32 %v60262, 24 (stack73)
        %v60272 = vshrl.u32 %v60262, 8 (stack74)
        %v60273 = vor.u32 %v60271, %v60272 (stack75)
        %v60274 = vxor.u32 %v60265, %v60273 (stack76)
        %v60277 = vadd.s32 %v60274, %v10 (stack65)
        %v60281 = vadd.s32 %v60277, 2 (stack65)
        %v60285 = vadd.s32 %v60269, %v60281 (stack65)
        %v60287 = vshll.u32 %v60281, 13 (stack73)
        %v60288 = vshrl.u32 %v60281, 19 (stack74)
        %v60289 = vor.u32 %v60287, %v60288 (stack75)
        %v60290 = vxor.u32 %v60285, %v60289 (stack76)
        %v60293 = vadd.s32 %v60285, %v60290 (stack65)
        %v60295 = vshll.u32 %v60290, 15 (stack73)
        %v60296 = vshrl.u32 %v60290, 17 (stack74)
        %v60297 = vor.u32 %v60295, %v60296 (stack75)
        %v60298 = vxor.u32 %v60293, %v60297 (stack76)
        %v60301 = vadd.s32 %v60293, %v60298 (stack65)
        %v60303 = vshll.u32 %v60298, 26 (stack73)
        %v60304 = vshrl.u32 %v60298, 6 (stack74)
        %v60305 = vor.u32 %v60303, %v60304 (stack75)
        %v60306 = vxor.u32 %v60301, %v60305 (stack76)
        %v60309 = vadd.s32 %v60301, %v60306 (stack65)
        %v60313 = vadd.s32 %v60309, %v10 (stack65)
        %v60315 = vshll.u32 %v60306, 6 (stack73)
        %v60316 = vshrl.u32 %v60306, 26 (stack74)
        %v60317 = vor.u32 %v60315, %v60316 (stack75)
        %v60318 = vxor.u32 %v60309, %v60317 (stack76)
        %v60321 = vadd.s32 %v60318, %v9 (stack65)
        %v60325 = vadd.s32 %v60321, 3 (stack65)
        %v60329 = vadd.s32 %v60313, %v60325 (stack65)
        %v60331 = vshll.u32 %v60325, 17 (stack73)
        %v60332 = vshrl.u32 %v60325, 15 (stack74)
        %v60333 = vor.u32 %v60331, %v60332 (stack75)
        %v60334 = vxor.u32 %v60329, %v60333 (stack76)
        %v60337 = vadd.s32 %v60329, %v60334 (stack65)
        %v60339 = vshll.u32 %v60334, 29 (stack73)
        %v60340 = vshrl.u32 %v60334, 3 (stack74)
        %v60341 = vor.u32 %v60339, %v60340 (stack75)
        %v60342 = vxor.u32 %v60337, %v60341 (stack76)
        %v60345 = vadd.s32 %v60337, %v60342 (stack65)
        %v60347 = vshll.u32 %v60342, 16 (stack73)
        %v60348 = vshrl.u32 %v60342, 16 (stack74)
        %v60349 = vor.u32 %v60347, %v60348 (stack75)
        %v60350 = vxor.u32 %v60345, %v60349 (stack76)
        %v60353 = vadd.s32 %v60345, %v60350 (stack65)
        %v60357 = vadd.s32 %v60353, %v9 (stack65)
        %v60359 = vshll.u32 %v60350, 24 (stack73)
        %v60360 = vshrl.u32 %v60350, 8 (stack74)
        %v60361 = vor.u32 %v60359, %v60360 (stack75)
        %v60362 = vxor.u32 %v60353, %v60361 (stack76)
        %v60365 = vadd.s32 %v60362, %v8 (stack65)
        %v60369 = vadd.s32 %v60365, 4 (stack65)
        %v60373 = vadd.s32 %v60357, %v60369 (stack65)
        %v60375 = vshll.u32 %v60369, 13 (stack73)
        %v60376 = vshrl.u32 %v60369, 19 (stack74)
        %v60377 = vor.u32 %v60375, %v60376 (stack75)
        %v60378 = vxor.u32 %v60373, %v60377 (stack76)
        %v60381 = vadd.s32 %v60373, %v60378 (stack65)
        %v60383 = vshll.u32 %v60378, 15 (stack73)
        %v60384 = vshrl.u32 %v60378, 17 (stack74)
        %v60385 = vor.u32 %v60383, %v60384 (stack75)
        %v60386 = vxor.u32 %v60381, %v60385 (stack76)
        %v60389 = vadd.s32 %v60381, %v60386 (stack65)
        %v60391 = vshll.u32 %v60386, 26 (stack73)
        %v60392 = vshrl.u32 %v60386, 6 (stack74)
        %v60393 = vor.u32 %v60391, %v60392 (stack75)
        %v60394 = vxor.u32 %v60389, %v60393 (stack76)
        %v60397 = vadd.s32 %v60389, %v60394 (stack65)
        %v60401 = vadd.s32 %v60397, %v8 (stack65)
        %v60403 = vshll.u32 %v60394, 6 (stack73)
        %v60404 = vshrl.u32 %v60394, 26 (stack74)
        %v60405 = vor.u32 %v60403, %v60404 (stack75)
        %v60406 = vxor.u32 %v60397, %v60405 (stack76)
        %v60409 = vadd.s32 %v60406, %v10 (stack65)
        %v60413 = vadd.s32 %v60409, 5 (stack65)
        %v60415 = vxor.u32 %v60401, %v60413 (stack76)
        %v60416 = vand.u32.u8 %v60415, 255 (stack77)
        %v60417 = vand.u32 %v60416, 65535 (stack78)
        %v60418 = vshrl.u32 %v60417, 1 (stack79)
        %v60419 = vor.u32 %v60418, 16256 (stack75)
        %v60420 = vand.u32.u16 %v60419, 65535 (stack80)
        %v60421 = vunpack.i.l.bf16 %v60420 (stack81)
        %v60425 = vadd.f32 %v60421, -1.0 (stack82)
        %v60429 = vmul.f32 %v60425, 2.0 (stack83)
        %v60433 = vadd.f32 %v60429, -0.99609375 (stack82)
        %v60437 = vmax.f32 -0.99609375, %v60433 (stack84)
        %v60439 = vand.u32 2147483647, %v60437 (stack85)
        %vm60442 = vcmp.eq.f32.partialorder %v60439, 1.0 (stack86)
        %v60447 = vmul.f32 %v60437, inf (stack83)
        %v60449 = vxor.u32 %v60437, 2147483648 (stack87)
        %v60452 = vmul.f32 %v60437, %v60449 (stack83)
        %v60454 = vadd.f32 %v60452, 1.0 (stack88)
        %v60455 = vlog2.pop %v60454 (stack89)
        %v60456 = vmul.f32 %v60455, 0.6931472 (stack90)
        %v60457 = vmul.f32 -0.5, %v60452 (stack91)
        %v60458 = vadd.f32 %v60457, 1.0 (stack92)
        %v60459 = vmul.f32 %v60458, %v60452 (stack93)
        %v60460 = vand.u32 2147483647, %v60452 (stack94)
        %vm60461 = vcmp.lt.f32.partialorder %v60460, 0.0004427343 (stack95)
        %v60462 = vsel /*vm=*/%vm60461, /*on_true_vy=*/%v60459, /*on_false_vx=*/%v60456 (stack96)
        %v60463 = vxor.u32 %v60462, 2147483648 (stack87)
        %vm60466 = vcmp.lt.f32.partialorder %v60463, 5.0 (stack86)
        %v60471 = vsel /*vm=*/%vm60466, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v60475 = vsel /*vm=*/%vm60466, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v60479 = vsel /*vm=*/%vm60466, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v60483 = vsel /*vm=*/%vm60466, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v60487 = vsel /*vm=*/%vm60466, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v60491 = vsel /*vm=*/%vm60466, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v60495 = vsel /*vm=*/%vm60466, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v60499 = vsel /*vm=*/%vm60466, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v60503 = vsel /*vm=*/%vm60466, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v60507 = vadd.f32 %v60463, -2.5 (stack82)
        %v60509 = vrsqrt.pop %v60463 (stack97)
        %v60510 = vmul.f32 %v60463, %v60509 (stack98)
        %vm60511 = vcmp.eq.f32.partialorder %v60463, inf (stack99)
        %v60512 = vsel /*vm=*/%vm60511, /*on_true_vy=*/%v60463, /*on_false_vx=*/%v60510 (stack100)
        %vm60513 = vcmp.eq.f32.partialorder %v60463, 0.0 (stack101)
        %v60514 = vand.u32 %v60463, 2147483648 (stack102)
        %v60515 = vsel /*vm=*/%vm60513, /*on_true_vy=*/%v60514, /*on_false_vx=*/%v60512 (stack103)
        %v60518 = vadd.f32 %v60515, -3.0 (stack82)
        %v60522 = vsel /*vm=*/%vm60466, /*on_true_vy=*/%v60507, /*on_false_vx=*/%v60518 (stack72)
        %v60526 = vmul.f32 %v60503, %v60522 (stack83)
        %v60530 = vadd.f32 %v60499, %v60526 (stack82)
        %v60534 = vmul.f32 %v60530, %v60522 (stack83)
        %v60538 = vadd.f32 %v60495, %v60534 (stack82)
        %v60542 = vmul.f32 %v60538, %v60522 (stack83)
        %v60546 = vadd.f32 %v60491, %v60542 (stack82)
        %v60550 = vmul.f32 %v60546, %v60522 (stack83)
        %v60554 = vadd.f32 %v60487, %v60550 (stack82)
        %v60558 = vmul.f32 %v60554, %v60522 (stack83)
        %v60562 = vadd.f32 %v60483, %v60558 (stack82)
        %v60566 = vmul.f32 %v60562, %v60522 (stack83)
        %v60570 = vadd.f32 %v60479, %v60566 (stack82)
        %v60574 = vmul.f32 %v60570, %v60522 (stack83)
        %v60578 = vadd.f32 %v60475, %v60574 (stack82)
        %v60582 = vmul.f32 %v60578, %v60522 (stack83)
        %v60586 = vadd.f32 %v60471, %v60582 (stack82)
        %v60590 = vmul.f32 %v60586, %v60437 (stack83)
        %v60594 = vsel /*vm=*/%vm60442, /*on_true_vy=*/%v60447, /*on_false_vx=*/%v60590 (stack72)
        %v60598 = vmul.f32 %v60594, 1.4140625 (stack83)
        %s60600 = scalar_lea.vmem %s280, 64 [#allocation0] (stack107)
        %v60601 = vpack.c.bf16 0.0, %v60598 (stack104)
        %60602 = vst [vmem:[%s60600] sm:$0xf] /*vst_source=*/%v60601 (stack105)
        %v60605 = vadd.s32 %v894, %v60141 (stack65)
        %s60607 = smul.u32 128, %s27 (stack66)
        %v60608 = vlaneseq (stack67)
        %v60609 = vand.u32 %v60608, 127 (stack68)
        %v60610 = vstv %s60607 (stack69)
        %v60611 = vadd.s32 %v60609, %v60610 (stack70)
        %v60615 = vadd.s32 %v60605, %v60611 (stack65)
        %vm60619 = vcmp.lt.u32.totalorder %v60615, %v60605 (stack71)
        %vm60624 = vcmp.lt.u32.totalorder %v60605, %v894 (stack71)
        %v60629 = vadd.s32 %v881, %v60124 (stack65)
        %v60633 = vadd.s32 %v60629, 1 (stack65)
        %v60637 = vsel /*vm=*/%vm60624, /*on_true_vy=*/%v60633, /*on_false_vx=*/%v60629 (stack72)
        %v60641 = vadd.s32 %v60637, 1 (stack65)
        %v60645 = vsel /*vm=*/%vm60619, /*on_true_vy=*/%v60641, /*on_false_vx=*/%v60637 (stack72)
        %v60650 = vadd.s32 %v60645, %v10 (stack65)
        %v60654 = vadd.s32 %v60615, %v9 (stack65)
        %v60658 = vadd.s32 %v60650, %v60654 (stack65)
        %v60660 = vshll.u32 %v60654, 13 (stack73)
        %v60661 = vshrl.u32 %v60654, 19 (stack74)
        %v60662 = vor.u32 %v60660, %v60661 (stack75)
        %v60663 = vxor.u32 %v60658, %v60662 (stack76)
        %v60666 = vadd.s32 %v60658, %v60663 (stack65)
        %v60668 = vshll.u32 %v60663, 15 (stack73)
        %v60669 = vshrl.u32 %v60663, 17 (stack74)
        %v60670 = vor.u32 %v60668, %v60669 (stack75)
        %v60671 = vxor.u32 %v60666, %v60670 (stack76)
        %v60674 = vadd.s32 %v60666, %v60671 (stack65)
        %v60676 = vshll.u32 %v60671, 26 (stack73)
        %v60677 = vshrl.u32 %v60671, 6 (stack74)
        %v60678 = vor.u32 %v60676, %v60677 (stack75)
        %v60679 = vxor.u32 %v60674, %v60678 (stack76)
        %v60682 = vadd.s32 %v60674, %v60679 (stack65)
        %v60686 = vadd.s32 %v60682, %v9 (stack65)
        %v60688 = vshll.u32 %v60679, 6 (stack73)
        %v60689 = vshrl.u32 %v60679, 26 (stack74)
        %v60690 = vor.u32 %v60688, %v60689 (stack75)
        %v60691 = vxor.u32 %v60682, %v60690 (stack76)
        %v60694 = vadd.s32 %v60691, %v8 (stack65)
        %v60698 = vadd.s32 %v60694, 1 (stack65)
        %v60702 = vadd.s32 %v60686, %v60698 (stack65)
        %v60704 = vshll.u32 %v60698, 17 (stack73)
        %v60705 = vshrl.u32 %v60698, 15 (stack74)
        %v60706 = vor.u32 %v60704, %v60705 (stack75)
        %v60707 = vxor.u32 %v60702, %v60706 (stack76)
        %v60710 = vadd.s32 %v60702, %v60707 (stack65)
        %v60712 = vshll.u32 %v60707, 29 (stack73)
        %v60713 = vshrl.u32 %v60707, 3 (stack74)
        %v60714 = vor.u32 %v60712, %v60713 (stack75)
        %v60715 = vxor.u32 %v60710, %v60714 (stack76)
        %v60718 = vadd.s32 %v60710, %v60715 (stack65)
        %v60720 = vshll.u32 %v60715, 16 (stack73)
        %v60721 = vshrl.u32 %v60715, 16 (stack74)
        %v60722 = vor.u32 %v60720, %v60721 (stack75)
        %v60723 = vxor.u32 %v60718, %v60722 (stack76)
        %v60726 = vadd.s32 %v60718, %v60723 (stack65)
        %v60730 = vadd.s32 %v60726, %v8 (stack65)
        %v60732 = vshll.u32 %v60723, 24 (stack73)
        %v60733 = vshrl.u32 %v60723, 8 (stack74)
        %v60734 = vor.u32 %v60732, %v60733 (stack75)
        %v60735 = vxor.u32 %v60726, %v60734 (stack76)
        %v60738 = vadd.s32 %v60735, %v10 (stack65)
        %v60742 = vadd.s32 %v60738, 2 (stack65)
        %v60746 = vadd.s32 %v60730, %v60742 (stack65)
        %v60748 = vshll.u32 %v60742, 13 (stack73)
        %v60749 = vshrl.u32 %v60742, 19 (stack74)
        %v60750 = vor.u32 %v60748, %v60749 (stack75)
        %v60751 = vxor.u32 %v60746, %v60750 (stack76)
        %v60754 = vadd.s32 %v60746, %v60751 (stack65)
        %v60756 = vshll.u32 %v60751, 15 (stack73)
        %v60757 = vshrl.u32 %v60751, 17 (stack74)
        %v60758 = vor.u32 %v60756, %v60757 (stack75)
        %v60759 = vxor.u32 %v60754, %v60758 (stack76)
        %v60762 = vadd.s32 %v60754, %v60759 (stack65)
        %v60764 = vshll.u32 %v60759, 26 (stack73)
        %v60765 = vshrl.u32 %v60759, 6 (stack74)
        %v60766 = vor.u32 %v60764, %v60765 (stack75)
        %v60767 = vxor.u32 %v60762, %v60766 (stack76)
        %v60770 = vadd.s32 %v60762, %v60767 (stack65)
        %v60774 = vadd.s32 %v60770, %v10 (stack65)
        %v60776 = vshll.u32 %v60767, 6 (stack73)
        %v60777 = vshrl.u32 %v60767, 26 (stack74)
        %v60778 = vor.u32 %v60776, %v60777 (stack75)
        %v60779 = vxor.u32 %v60770, %v60778 (stack76)
        %v60782 = vadd.s32 %v60779, %v9 (stack65)
        %v60786 = vadd.s32 %v60782, 3 (stack65)
        %v60790 = vadd.s32 %v60774, %v60786 (stack65)
        %v60792 = vshll.u32 %v60786, 17 (stack73)
        %v60793 = vshrl.u32 %v60786, 15 (stack74)
        %v60794 = vor.u32 %v60792, %v60793 (stack75)
        %v60795 = vxor.u32 %v60790, %v60794 (stack76)
        %v60798 = vadd.s32 %v60790, %v60795 (stack65)
        %v60800 = vshll.u32 %v60795, 29 (stack73)
        %v60801 = vshrl.u32 %v60795, 3 (stack74)
        %v60802 = vor.u32 %v60800, %v60801 (stack75)
        %v60803 = vxor.u32 %v60798, %v60802 (stack76)
        %v60806 = vadd.s32 %v60798, %v60803 (stack65)
        %v60808 = vshll.u32 %v60803, 16 (stack73)
        %v60809 = vshrl.u32 %v60803, 16 (stack74)
        %v60810 = vor.u32 %v60808, %v60809 (stack75)
        %v60811 = vxor.u32 %v60806, %v60810 (stack76)
        %v60814 = vadd.s32 %v60806, %v60811 (stack65)
        %v60818 = vadd.s32 %v60814, %v9 (stack65)
        %v60820 = vshll.u32 %v60811, 24 (stack73)
        %v60821 = vshrl.u32 %v60811, 8 (stack74)
        %v60822 = vor.u32 %v60820, %v60821 (stack75)
        %v60823 = vxor.u32 %v60814, %v60822 (stack76)
        %v60826 = vadd.s32 %v60823, %v8 (stack65)
        %v60830 = vadd.s32 %v60826, 4 (stack65)
        %v60834 = vadd.s32 %v60818, %v60830 (stack65)
        %v60836 = vshll.u32 %v60830, 13 (stack73)
        %v60837 = vshrl.u32 %v60830, 19 (stack74)
        %v60838 = vor.u32 %v60836, %v60837 (stack75)
        %v60839 = vxor.u32 %v60834, %v60838 (stack76)
        %v60842 = vadd.s32 %v60834, %v60839 (stack65)
        %v60844 = vshll.u32 %v60839, 15 (stack73)
        %v60845 = vshrl.u32 %v60839, 17 (stack74)
        %v60846 = vor.u32 %v60844, %v60845 (stack75)
        %v60847 = vxor.u32 %v60842, %v60846 (stack76)
        %v60850 = vadd.s32 %v60842, %v60847 (stack65)
        %v60852 = vshll.u32 %v60847, 26 (stack73)
        %v60853 = vshrl.u32 %v60847, 6 (stack74)
        %v60854 = vor.u32 %v60852, %v60853 (stack75)
        %v60855 = vxor.u32 %v60850, %v60854 (stack76)
        %v60858 = vadd.s32 %v60850, %v60855 (stack65)
        %v60862 = vadd.s32 %v60858, %v8 (stack65)
        %v60864 = vshll.u32 %v60855, 6 (stack73)
        %v60865 = vshrl.u32 %v60855, 26 (stack74)
        %v60866 = vor.u32 %v60864, %v60865 (stack75)
        %v60867 = vxor.u32 %v60858, %v60866 (stack76)
        %v60870 = vadd.s32 %v60867, %v10 (stack65)
        %v60874 = vadd.s32 %v60870, 5 (stack65)
        %v60876 = vxor.u32 %v60862, %v60874 (stack76)
        %v60877 = vand.u32.u8 %v60876, 255 (stack77)
        %v60878 = vand.u32 %v60877, 65535 (stack78)
        %v60879 = vshrl.u32 %v60878, 1 (stack79)
        %v60880 = vor.u32 %v60879, 16256 (stack75)
        %v60881 = vand.u32.u16 %v60880, 65535 (stack80)
        %v60882 = vunpack.i.l.bf16 %v60881 (stack81)
        %v60886 = vadd.f32 %v60882, -1.0 (stack82)
        %v60890 = vmul.f32 %v60886, 2.0 (stack83)
        %v60894 = vadd.f32 %v60890, -0.99609375 (stack82)
        %v60898 = vmax.f32 -0.99609375, %v60894 (stack84)
        %v60900 = vand.u32 2147483647, %v60898 (stack85)
        %vm60903 = vcmp.eq.f32.partialorder %v60900, 1.0 (stack86)
        %v60908 = vmul.f32 %v60898, inf (stack83)
        %v60910 = vxor.u32 %v60898, 2147483648 (stack87)
        %v60913 = vmul.f32 %v60898, %v60910 (stack83)
        %v60915 = vadd.f32 %v60913, 1.0 (stack88)
        %v60916 = vlog2.pop %v60915 (stack89)
        %v60917 = vmul.f32 %v60916, 0.6931472 (stack90)
        %v60918 = vmul.f32 -0.5, %v60913 (stack91)
        %v60919 = vadd.f32 %v60918, 1.0 (stack92)
        %v60920 = vmul.f32 %v60919, %v60913 (stack93)
        %v60921 = vand.u32 2147483647, %v60913 (stack94)
        %vm60922 = vcmp.lt.f32.partialorder %v60921, 0.0004427343 (stack95)
        %v60923 = vsel /*vm=*/%vm60922, /*on_true_vy=*/%v60920, /*on_false_vx=*/%v60917 (stack96)
        %v60924 = vxor.u32 %v60923, 2147483648 (stack87)
        %vm60927 = vcmp.lt.f32.partialorder %v60924, 5.0 (stack86)
        %v60932 = vsel /*vm=*/%vm60927, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v60936 = vsel /*vm=*/%vm60927, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v60940 = vsel /*vm=*/%vm60927, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v60944 = vsel /*vm=*/%vm60927, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v60948 = vsel /*vm=*/%vm60927, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v60952 = vsel /*vm=*/%vm60927, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v60956 = vsel /*vm=*/%vm60927, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v60960 = vsel /*vm=*/%vm60927, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v60964 = vsel /*vm=*/%vm60927, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v60968 = vadd.f32 %v60924, -2.5 (stack82)
        %v60970 = vrsqrt.pop %v60924 (stack97)
        %v60971 = vmul.f32 %v60924, %v60970 (stack98)
        %vm60972 = vcmp.eq.f32.partialorder %v60924, inf (stack99)
        %v60973 = vsel /*vm=*/%vm60972, /*on_true_vy=*/%v60924, /*on_false_vx=*/%v60971 (stack100)
        %vm60974 = vcmp.eq.f32.partialorder %v60924, 0.0 (stack101)
        %v60975 = vand.u32 %v60924, 2147483648 (stack102)
        %v60976 = vsel /*vm=*/%vm60974, /*on_true_vy=*/%v60975, /*on_false_vx=*/%v60973 (stack103)
        %v60979 = vadd.f32 %v60976, -3.0 (stack82)
        %v60983 = vsel /*vm=*/%vm60927, /*on_true_vy=*/%v60968, /*on_false_vx=*/%v60979 (stack72)
        %v60987 = vmul.f32 %v60964, %v60983 (stack83)
        %v60991 = vadd.f32 %v60960, %v60987 (stack82)
        %v60995 = vmul.f32 %v60991, %v60983 (stack83)
        %v60999 = vadd.f32 %v60956, %v60995 (stack82)
        %v61003 = vmul.f32 %v60999, %v60983 (stack83)
        %v61007 = vadd.f32 %v60952, %v61003 (stack82)
        %v61011 = vmul.f32 %v61007, %v60983 (stack83)
        %v61015 = vadd.f32 %v60948, %v61011 (stack82)
        %v61019 = vmul.f32 %v61015, %v60983 (stack83)
        %v61023 = vadd.f32 %v60944, %v61019 (stack82)
        %v61027 = vmul.f32 %v61023, %v60983 (stack83)
        %v61031 = vadd.f32 %v60940, %v61027 (stack82)
        %v61035 = vmul.f32 %v61031, %v60983 (stack83)
        %v61039 = vadd.f32 %v60936, %v61035 (stack82)
        %v61043 = vmul.f32 %v61039, %v60983 (stack83)
        %v61047 = vadd.f32 %v60932, %v61043 (stack82)
        %v61051 = vmul.f32 %v61047, %v60898 (stack83)
        %v61055 = vsel /*vm=*/%vm60903, /*on_true_vy=*/%v60908, /*on_false_vx=*/%v61051 (stack72)
        %v61059 = vmul.f32 %v61055, 1.4140625 (stack83)
        %s61061 = scalar_lea.vmem %s280, 192 [#allocation0] (stack107)
        %v61062 = vpack.c.bf16 0.0, %v61059 (stack104)
        %61063 = vst [vmem:[%s61061] sm:$0xf] /*vst_source=*/%v61062 (stack105)
        %v61066 = vadd.s32 %v1381, %v60141 (stack65)
        %s61068 = smul.u32 128, %s27 (stack66)
        %v61069 = vlaneseq (stack67)
        %v61070 = vand.u32 %v61069, 127 (stack68)
        %v61071 = vstv %s61068 (stack69)
        %v61072 = vadd.s32 %v61070, %v61071 (stack70)
        %v61076 = vadd.s32 %v61066, %v61072 (stack65)
        %vm61080 = vcmp.lt.u32.totalorder %v61076, %v61066 (stack71)
        %vm61085 = vcmp.lt.u32.totalorder %v61066, %v1381 (stack71)
        %v61090 = vadd.s32 %v1368, %v60124 (stack65)
        %v61094 = vadd.s32 %v61090, 1 (stack65)
        %v61098 = vsel /*vm=*/%vm61085, /*on_true_vy=*/%v61094, /*on_false_vx=*/%v61090 (stack72)
        %v61102 = vadd.s32 %v61098, 1 (stack65)
        %v61106 = vsel /*vm=*/%vm61080, /*on_true_vy=*/%v61102, /*on_false_vx=*/%v61098 (stack72)
        %v61111 = vadd.s32 %v61106, %v10 (stack65)
        %v61115 = vadd.s32 %v61076, %v9 (stack65)
        %v61119 = vadd.s32 %v61111, %v61115 (stack65)
        %v61121 = vshll.u32 %v61115, 13 (stack73)
        %v61122 = vshrl.u32 %v61115, 19 (stack74)
        %v61123 = vor.u32 %v61121, %v61122 (stack75)
        %v61124 = vxor.u32 %v61119, %v61123 (stack76)
        %v61127 = vadd.s32 %v61119, %v61124 (stack65)
        %v61129 = vshll.u32 %v61124, 15 (stack73)
        %v61130 = vshrl.u32 %v61124, 17 (stack74)
        %v61131 = vor.u32 %v61129, %v61130 (stack75)
        %v61132 = vxor.u32 %v61127, %v61131 (stack76)
        %v61135 = vadd.s32 %v61127, %v61132 (stack65)
        %v61137 = vshll.u32 %v61132, 26 (stack73)
        %v61138 = vshrl.u32 %v61132, 6 (stack74)
        %v61139 = vor.u32 %v61137, %v61138 (stack75)
        %v61140 = vxor.u32 %v61135, %v61139 (stack76)
        %v61143 = vadd.s32 %v61135, %v61140 (stack65)
        %v61147 = vadd.s32 %v61143, %v9 (stack65)
        %v61149 = vshll.u32 %v61140, 6 (stack73)
        %v61150 = vshrl.u32 %v61140, 26 (stack74)
        %v61151 = vor.u32 %v61149, %v61150 (stack75)
        %v61152 = vxor.u32 %v61143, %v61151 (stack76)
        %v61155 = vadd.s32 %v61152, %v8 (stack65)
        %v61159 = vadd.s32 %v61155, 1 (stack65)
        %v61163 = vadd.s32 %v61147, %v61159 (stack65)
        %v61165 = vshll.u32 %v61159, 17 (stack73)
        %v61166 = vshrl.u32 %v61159, 15 (stack74)
        %v61167 = vor.u32 %v61165, %v61166 (stack75)
        %v61168 = vxor.u32 %v61163, %v61167 (stack76)
        %v61171 = vadd.s32 %v61163, %v61168 (stack65)
        %v61173 = vshll.u32 %v61168, 29 (stack73)
        %v61174 = vshrl.u32 %v61168, 3 (stack74)
        %v61175 = vor.u32 %v61173, %v61174 (stack75)
        %v61176 = vxor.u32 %v61171, %v61175 (stack76)
        %v61179 = vadd.s32 %v61171, %v61176 (stack65)
        %v61181 = vshll.u32 %v61176, 16 (stack73)
        %v61182 = vshrl.u32 %v61176, 16 (stack74)
        %v61183 = vor.u32 %v61181, %v61182 (stack75)
        %v61184 = vxor.u32 %v61179, %v61183 (stack76)
        %v61187 = vadd.s32 %v61179, %v61184 (stack65)
        %v61191 = vadd.s32 %v61187, %v8 (stack65)
        %v61193 = vshll.u32 %v61184, 24 (stack73)
        %v61194 = vshrl.u32 %v61184, 8 (stack74)
        %v61195 = vor.u32 %v61193, %v61194 (stack75)
        %v61196 = vxor.u32 %v61187, %v61195 (stack76)
        %v61199 = vadd.s32 %v61196, %v10 (stack65)
        %v61203 = vadd.s32 %v61199, 2 (stack65)
        %v61207 = vadd.s32 %v61191, %v61203 (stack65)
        %v61209 = vshll.u32 %v61203, 13 (stack73)
        %v61210 = vshrl.u32 %v61203, 19 (stack74)
        %v61211 = vor.u32 %v61209, %v61210 (stack75)
        %v61212 = vxor.u32 %v61207, %v61211 (stack76)
        %v61215 = vadd.s32 %v61207, %v61212 (stack65)
        %v61217 = vshll.u32 %v61212, 15 (stack73)
        %v61218 = vshrl.u32 %v61212, 17 (stack74)
        %v61219 = vor.u32 %v61217, %v61218 (stack75)
        %v61220 = vxor.u32 %v61215, %v61219 (stack76)
        %v61223 = vadd.s32 %v61215, %v61220 (stack65)
        %v61225 = vshll.u32 %v61220, 26 (stack73)
        %v61226 = vshrl.u32 %v61220, 6 (stack74)
        %v61227 = vor.u32 %v61225, %v61226 (stack75)
        %v61228 = vxor.u32 %v61223, %v61227 (stack76)
        %v61231 = vadd.s32 %v61223, %v61228 (stack65)
        %v61235 = vadd.s32 %v61231, %v10 (stack65)
        %v61237 = vshll.u32 %v61228, 6 (stack73)
        %v61238 = vshrl.u32 %v61228, 26 (stack74)
        %v61239 = vor.u32 %v61237, %v61238 (stack75)
        %v61240 = vxor.u32 %v61231, %v61239 (stack76)
        %v61243 = vadd.s32 %v61240, %v9 (stack65)
        %v61247 = vadd.s32 %v61243, 3 (stack65)
        %v61251 = vadd.s32 %v61235, %v61247 (stack65)
        %v61253 = vshll.u32 %v61247, 17 (stack73)
        %v61254 = vshrl.u32 %v61247, 15 (stack74)
        %v61255 = vor.u32 %v61253, %v61254 (stack75)
        %v61256 = vxor.u32 %v61251, %v61255 (stack76)
        %v61259 = vadd.s32 %v61251, %v61256 (stack65)
        %v61261 = vshll.u32 %v61256, 29 (stack73)
        %v61262 = vshrl.u32 %v61256, 3 (stack74)
        %v61263 = vor.u32 %v61261, %v61262 (stack75)
        %v61264 = vxor.u32 %v61259, %v61263 (stack76)
        %v61267 = vadd.s32 %v61259, %v61264 (stack65)
        %v61269 = vshll.u32 %v61264, 16 (stack73)
        %v61270 = vshrl.u32 %v61264, 16 (stack74)
        %v61271 = vor.u32 %v61269, %v61270 (stack75)
        %v61272 = vxor.u32 %v61267, %v61271 (stack76)
        %v61275 = vadd.s32 %v61267, %v61272 (stack65)
        %v61279 = vadd.s32 %v61275, %v9 (stack65)
        %v61281 = vshll.u32 %v61272, 24 (stack73)
        %v61282 = vshrl.u32 %v61272, 8 (stack74)
        %v61283 = vor.u32 %v61281, %v61282 (stack75)
        %v61284 = vxor.u32 %v61275, %v61283 (stack76)
        %v61287 = vadd.s32 %v61284, %v8 (stack65)
        %v61291 = vadd.s32 %v61287, 4 (stack65)
        %v61295 = vadd.s32 %v61279, %v61291 (stack65)
        %v61297 = vshll.u32 %v61291, 13 (stack73)
        %v61298 = vshrl.u32 %v61291, 19 (stack74)
        %v61299 = vor.u32 %v61297, %v61298 (stack75)
        %v61300 = vxor.u32 %v61295, %v61299 (stack76)
        %v61303 = vadd.s32 %v61295, %v61300 (stack65)
        %v61305 = vshll.u32 %v61300, 15 (stack73)
        %v61306 = vshrl.u32 %v61300, 17 (stack74)
        %v61307 = vor.u32 %v61305, %v61306 (stack75)
        %v61308 = vxor.u32 %v61303, %v61307 (stack76)
        %v61311 = vadd.s32 %v61303, %v61308 (stack65)
        %v61313 = vshll.u32 %v61308, 26 (stack73)
        %v61314 = vshrl.u32 %v61308, 6 (stack74)
        %v61315 = vor.u32 %v61313, %v61314 (stack75)
        %v61316 = vxor.u32 %v61311, %v61315 (stack76)
        %v61319 = vadd.s32 %v61311, %v61316 (stack65)
        %v61323 = vadd.s32 %v61319, %v8 (stack65)
        %v61325 = vshll.u32 %v61316, 6 (stack73)
        %v61326 = vshrl.u32 %v61316, 26 (stack74)
        %v61327 = vor.u32 %v61325, %v61326 (stack75)
        %v61328 = vxor.u32 %v61319, %v61327 (stack76)
        %v61331 = vadd.s32 %v61328, %v10 (stack65)
        %v61335 = vadd.s32 %v61331, 5 (stack65)
        %v61337 = vxor.u32 %v61323, %v61335 (stack76)
        %v61338 = vand.u32.u8 %v61337, 255 (stack77)
        %v61339 = vand.u32 %v61338, 65535 (stack78)
        %v61340 = vshrl.u32 %v61339, 1 (stack79)
        %v61341 = vor.u32 %v61340, 16256 (stack75)
        %v61342 = vand.u32.u16 %v61341, 65535 (stack80)
        %v61343 = vunpack.i.l.bf16 %v61342 (stack81)
        %v61347 = vadd.f32 %v61343, -1.0 (stack82)
        %v61351 = vmul.f32 %v61347, 2.0 (stack83)
        %v61355 = vadd.f32 %v61351, -0.99609375 (stack82)
        %v61359 = vmax.f32 -0.99609375, %v61355 (stack84)
        %v61361 = vand.u32 2147483647, %v61359 (stack85)
        %vm61364 = vcmp.eq.f32.partialorder %v61361, 1.0 (stack86)
        %v61369 = vmul.f32 %v61359, inf (stack83)
        %v61371 = vxor.u32 %v61359, 2147483648 (stack87)
        %v61374 = vmul.f32 %v61359, %v61371 (stack83)
        %v61376 = vadd.f32 %v61374, 1.0 (stack88)
        %v61377 = vlog2.pop %v61376 (stack89)
        %v61378 = vmul.f32 %v61377, 0.6931472 (stack90)
        %v61379 = vmul.f32 -0.5, %v61374 (stack91)
        %v61380 = vadd.f32 %v61379, 1.0 (stack92)
        %v61381 = vmul.f32 %v61380, %v61374 (stack93)
        %v61382 = vand.u32 2147483647, %v61374 (stack94)
        %vm61383 = vcmp.lt.f32.partialorder %v61382, 0.0004427343 (stack95)
        %v61384 = vsel /*vm=*/%vm61383, /*on_true_vy=*/%v61381, /*on_false_vx=*/%v61378 (stack96)
        %v61385 = vxor.u32 %v61384, 2147483648 (stack87)
        %vm61388 = vcmp.lt.f32.partialorder %v61385, 5.0 (stack86)
        %v61393 = vsel /*vm=*/%vm61388, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v61397 = vsel /*vm=*/%vm61388, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v61401 = vsel /*vm=*/%vm61388, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v61405 = vsel /*vm=*/%vm61388, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v61409 = vsel /*vm=*/%vm61388, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v61413 = vsel /*vm=*/%vm61388, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v61417 = vsel /*vm=*/%vm61388, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v61421 = vsel /*vm=*/%vm61388, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v61425 = vsel /*vm=*/%vm61388, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v61429 = vadd.f32 %v61385, -2.5 (stack82)
        %v61431 = vrsqrt.pop %v61385 (stack97)
        %v61432 = vmul.f32 %v61385, %v61431 (stack98)
        %vm61433 = vcmp.eq.f32.partialorder %v61385, inf (stack99)
        %v61434 = vsel /*vm=*/%vm61433, /*on_true_vy=*/%v61385, /*on_false_vx=*/%v61432 (stack100)
        %vm61435 = vcmp.eq.f32.partialorder %v61385, 0.0 (stack101)
        %v61436 = vand.u32 %v61385, 2147483648 (stack102)
        %v61437 = vsel /*vm=*/%vm61435, /*on_true_vy=*/%v61436, /*on_false_vx=*/%v61434 (stack103)
        %v61440 = vadd.f32 %v61437, -3.0 (stack82)
        %v61444 = vsel /*vm=*/%vm61388, /*on_true_vy=*/%v61429, /*on_false_vx=*/%v61440 (stack72)
        %v61448 = vmul.f32 %v61425, %v61444 (stack83)
        %v61452 = vadd.f32 %v61421, %v61448 (stack82)
        %v61456 = vmul.f32 %v61452, %v61444 (stack83)
        %v61460 = vadd.f32 %v61417, %v61456 (stack82)
        %v61464 = vmul.f32 %v61460, %v61444 (stack83)
        %v61468 = vadd.f32 %v61413, %v61464 (stack82)
        %v61472 = vmul.f32 %v61468, %v61444 (stack83)
        %v61476 = vadd.f32 %v61409, %v61472 (stack82)
        %v61480 = vmul.f32 %v61476, %v61444 (stack83)
        %v61484 = vadd.f32 %v61405, %v61480 (stack82)
        %v61488 = vmul.f32 %v61484, %v61444 (stack83)
        %v61492 = vadd.f32 %v61401, %v61488 (stack82)
        %v61496 = vmul.f32 %v61492, %v61444 (stack83)
        %v61500 = vadd.f32 %v61397, %v61496 (stack82)
        %v61504 = vmul.f32 %v61500, %v61444 (stack83)
        %v61508 = vadd.f32 %v61393, %v61504 (stack82)
        %v61512 = vmul.f32 %v61508, %v61359 (stack83)
        %v61516 = vsel /*vm=*/%vm61364, /*on_true_vy=*/%v61369, /*on_false_vx=*/%v61512 (stack72)
        %v61520 = vmul.f32 %v61516, 1.4140625 (stack83)
        %s61522 = scalar_lea.vmem %s280, 320 [#allocation0] (stack107)
        %v61523 = vpack.c.bf16 0.0, %v61520 (stack104)
        %61524 = vst [vmem:[%s61522] sm:$0xf] /*vst_source=*/%v61523 (stack105)
        %v61527 = vadd.s32 %v1868, %v60141 (stack65)
        %s61529 = smul.u32 128, %s27 (stack66)
        %v61530 = vlaneseq (stack67)
        %v61531 = vand.u32 %v61530, 127 (stack68)
        %v61532 = vstv %s61529 (stack69)
        %v61533 = vadd.s32 %v61531, %v61532 (stack70)
        %v61537 = vadd.s32 %v61527, %v61533 (stack65)
        %vm61541 = vcmp.lt.u32.totalorder %v61537, %v61527 (stack71)
        %vm61546 = vcmp.lt.u32.totalorder %v61527, %v1868 (stack71)
        %v61551 = vadd.s32 %v1855, %v60124 (stack65)
        %v61555 = vadd.s32 %v61551, 1 (stack65)
        %v61559 = vsel /*vm=*/%vm61546, /*on_true_vy=*/%v61555, /*on_false_vx=*/%v61551 (stack72)
        %v61563 = vadd.s32 %v61559, 1 (stack65)
        %v61567 = vsel /*vm=*/%vm61541, /*on_true_vy=*/%v61563, /*on_false_vx=*/%v61559 (stack72)
        %v61572 = vadd.s32 %v61567, %v10 (stack65)
        %v61576 = vadd.s32 %v61537, %v9 (stack65)
        %v61580 = vadd.s32 %v61572, %v61576 (stack65)
        %v61582 = vshll.u32 %v61576, 13 (stack73)
        %v61583 = vshrl.u32 %v61576, 19 (stack74)
        %v61584 = vor.u32 %v61582, %v61583 (stack75)
        %v61585 = vxor.u32 %v61580, %v61584 (stack76)
        %v61588 = vadd.s32 %v61580, %v61585 (stack65)
        %v61590 = vshll.u32 %v61585, 15 (stack73)
        %v61591 = vshrl.u32 %v61585, 17 (stack74)
        %v61592 = vor.u32 %v61590, %v61591 (stack75)
        %v61593 = vxor.u32 %v61588, %v61592 (stack76)
        %v61596 = vadd.s32 %v61588, %v61593 (stack65)
        %v61598 = vshll.u32 %v61593, 26 (stack73)
        %v61599 = vshrl.u32 %v61593, 6 (stack74)
        %v61600 = vor.u32 %v61598, %v61599 (stack75)
        %v61601 = vxor.u32 %v61596, %v61600 (stack76)
        %v61604 = vadd.s32 %v61596, %v61601 (stack65)
        %v61608 = vadd.s32 %v61604, %v9 (stack65)
        %v61610 = vshll.u32 %v61601, 6 (stack73)
        %v61611 = vshrl.u32 %v61601, 26 (stack74)
        %v61612 = vor.u32 %v61610, %v61611 (stack75)
        %v61613 = vxor.u32 %v61604, %v61612 (stack76)
        %v61616 = vadd.s32 %v61613, %v8 (stack65)
        %v61620 = vadd.s32 %v61616, 1 (stack65)
        %v61624 = vadd.s32 %v61608, %v61620 (stack65)
        %v61626 = vshll.u32 %v61620, 17 (stack73)
        %v61627 = vshrl.u32 %v61620, 15 (stack74)
        %v61628 = vor.u32 %v61626, %v61627 (stack75)
        %v61629 = vxor.u32 %v61624, %v61628 (stack76)
        %v61632 = vadd.s32 %v61624, %v61629 (stack65)
        %v61634 = vshll.u32 %v61629, 29 (stack73)
        %v61635 = vshrl.u32 %v61629, 3 (stack74)
        %v61636 = vor.u32 %v61634, %v61635 (stack75)
        %v61637 = vxor.u32 %v61632, %v61636 (stack76)
        %v61640 = vadd.s32 %v61632, %v61637 (stack65)
        %v61642 = vshll.u32 %v61637, 16 (stack73)
        %v61643 = vshrl.u32 %v61637, 16 (stack74)
        %v61644 = vor.u32 %v61642, %v61643 (stack75)
        %v61645 = vxor.u32 %v61640, %v61644 (stack76)
        %v61648 = vadd.s32 %v61640, %v61645 (stack65)
        %v61652 = vadd.s32 %v61648, %v8 (stack65)
        %v61654 = vshll.u32 %v61645, 24 (stack73)
        %v61655 = vshrl.u32 %v61645, 8 (stack74)
        %v61656 = vor.u32 %v61654, %v61655 (stack75)
        %v61657 = vxor.u32 %v61648, %v61656 (stack76)
        %v61660 = vadd.s32 %v61657, %v10 (stack65)
        %v61664 = vadd.s32 %v61660, 2 (stack65)
        %v61668 = vadd.s32 %v61652, %v61664 (stack65)
        %v61670 = vshll.u32 %v61664, 13 (stack73)
        %v61671 = vshrl.u32 %v61664, 19 (stack74)
        %v61672 = vor.u32 %v61670, %v61671 (stack75)
        %v61673 = vxor.u32 %v61668, %v61672 (stack76)
        %v61676 = vadd.s32 %v61668, %v61673 (stack65)
        %v61678 = vshll.u32 %v61673, 15 (stack73)
        %v61679 = vshrl.u32 %v61673, 17 (stack74)
        %v61680 = vor.u32 %v61678, %v61679 (stack75)
        %v61681 = vxor.u32 %v61676, %v61680 (stack76)
        %v61684 = vadd.s32 %v61676, %v61681 (stack65)
        %v61686 = vshll.u32 %v61681, 26 (stack73)
        %v61687 = vshrl.u32 %v61681, 6 (stack74)
        %v61688 = vor.u32 %v61686, %v61687 (stack75)
        %v61689 = vxor.u32 %v61684, %v61688 (stack76)
        %v61692 = vadd.s32 %v61684, %v61689 (stack65)
        %v61696 = vadd.s32 %v61692, %v10 (stack65)
        %v61698 = vshll.u32 %v61689, 6 (stack73)
        %v61699 = vshrl.u32 %v61689, 26 (stack74)
        %v61700 = vor.u32 %v61698, %v61699 (stack75)
        %v61701 = vxor.u32 %v61692, %v61700 (stack76)
        %v61704 = vadd.s32 %v61701, %v9 (stack65)
        %v61708 = vadd.s32 %v61704, 3 (stack65)
        %v61712 = vadd.s32 %v61696, %v61708 (stack65)
        %v61714 = vshll.u32 %v61708, 17 (stack73)
        %v61715 = vshrl.u32 %v61708, 15 (stack74)
        %v61716 = vor.u32 %v61714, %v61715 (stack75)
        %v61717 = vxor.u32 %v61712, %v61716 (stack76)
        %v61720 = vadd.s32 %v61712, %v61717 (stack65)
        %v61722 = vshll.u32 %v61717, 29 (stack73)
        %v61723 = vshrl.u32 %v61717, 3 (stack74)
        %v61724 = vor.u32 %v61722, %v61723 (stack75)
        %v61725 = vxor.u32 %v61720, %v61724 (stack76)
        %v61728 = vadd.s32 %v61720, %v61725 (stack65)
        %v61730 = vshll.u32 %v61725, 16 (stack73)
        %v61731 = vshrl.u32 %v61725, 16 (stack74)
        %v61732 = vor.u32 %v61730, %v61731 (stack75)
        %v61733 = vxor.u32 %v61728, %v61732 (stack76)
        %v61736 = vadd.s32 %v61728, %v61733 (stack65)
        %v61740 = vadd.s32 %v61736, %v9 (stack65)
        %v61742 = vshll.u32 %v61733, 24 (stack73)
        %v61743 = vshrl.u32 %v61733, 8 (stack74)
        %v61744 = vor.u32 %v61742, %v61743 (stack75)
        %v61745 = vxor.u32 %v61736, %v61744 (stack76)
        %v61748 = vadd.s32 %v61745, %v8 (stack65)
        %v61752 = vadd.s32 %v61748, 4 (stack65)
        %v61756 = vadd.s32 %v61740, %v61752 (stack65)
        %v61758 = vshll.u32 %v61752, 13 (stack73)
        %v61759 = vshrl.u32 %v61752, 19 (stack74)
        %v61760 = vor.u32 %v61758, %v61759 (stack75)
        %v61761 = vxor.u32 %v61756, %v61760 (stack76)
        %v61764 = vadd.s32 %v61756, %v61761 (stack65)
        %v61766 = vshll.u32 %v61761, 15 (stack73)
        %v61767 = vshrl.u32 %v61761, 17 (stack74)
        %v61768 = vor.u32 %v61766, %v61767 (stack75)
        %v61769 = vxor.u32 %v61764, %v61768 (stack76)
        %v61772 = vadd.s32 %v61764, %v61769 (stack65)
        %v61774 = vshll.u32 %v61769, 26 (stack73)
        %v61775 = vshrl.u32 %v61769, 6 (stack74)
        %v61776 = vor.u32 %v61774, %v61775 (stack75)
        %v61777 = vxor.u32 %v61772, %v61776 (stack76)
        %v61780 = vadd.s32 %v61772, %v61777 (stack65)
        %v61784 = vadd.s32 %v61780, %v8 (stack65)
        %v61786 = vshll.u32 %v61777, 6 (stack73)
        %v61787 = vshrl.u32 %v61777, 26 (stack74)
        %v61788 = vor.u32 %v61786, %v61787 (stack75)
        %v61789 = vxor.u32 %v61780, %v61788 (stack76)
        %v61792 = vadd.s32 %v61789, %v10 (stack65)
        %v61796 = vadd.s32 %v61792, 5 (stack65)
        %v61798 = vxor.u32 %v61784, %v61796 (stack76)
        %v61799 = vand.u32.u8 %v61798, 255 (stack77)
        %v61800 = vand.u32 %v61799, 65535 (stack78)
        %v61801 = vshrl.u32 %v61800, 1 (stack79)
        %v61802 = vor.u32 %v61801, 16256 (stack75)
        %v61803 = vand.u32.u16 %v61802, 65535 (stack80)
        %v61804 = vunpack.i.l.bf16 %v61803 (stack81)
        %v61808 = vadd.f32 %v61804, -1.0 (stack82)
        %v61812 = vmul.f32 %v61808, 2.0 (stack83)
        %v61816 = vadd.f32 %v61812, -0.99609375 (stack82)
        %v61820 = vmax.f32 -0.99609375, %v61816 (stack84)
        %v61822 = vand.u32 2147483647, %v61820 (stack85)
        %vm61825 = vcmp.eq.f32.partialorder %v61822, 1.0 (stack86)
        %v61830 = vmul.f32 %v61820, inf (stack83)
        %v61832 = vxor.u32 %v61820, 2147483648 (stack87)
        %v61835 = vmul.f32 %v61820, %v61832 (stack83)
        %v61837 = vadd.f32 %v61835, 1.0 (stack88)
        %v61838 = vlog2.pop %v61837 (stack89)
        %v61839 = vmul.f32 %v61838, 0.6931472 (stack90)
        %v61840 = vmul.f32 -0.5, %v61835 (stack91)
        %v61841 = vadd.f32 %v61840, 1.0 (stack92)
        %v61842 = vmul.f32 %v61841, %v61835 (stack93)
        %v61843 = vand.u32 2147483647, %v61835 (stack94)
        %vm61844 = vcmp.lt.f32.partialorder %v61843, 0.0004427343 (stack95)
        %v61845 = vsel /*vm=*/%vm61844, /*on_true_vy=*/%v61842, /*on_false_vx=*/%v61839 (stack96)
        %v61846 = vxor.u32 %v61845, 2147483648 (stack87)
        %vm61849 = vcmp.lt.f32.partialorder %v61846, 5.0 (stack86)
        %v61854 = vsel /*vm=*/%vm61849, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v61858 = vsel /*vm=*/%vm61849, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v61862 = vsel /*vm=*/%vm61849, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v61866 = vsel /*vm=*/%vm61849, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v61870 = vsel /*vm=*/%vm61849, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v61874 = vsel /*vm=*/%vm61849, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v61878 = vsel /*vm=*/%vm61849, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v61882 = vsel /*vm=*/%vm61849, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v61886 = vsel /*vm=*/%vm61849, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v61890 = vadd.f32 %v61846, -2.5 (stack82)
        %v61892 = vrsqrt.pop %v61846 (stack97)
        %v61893 = vmul.f32 %v61846, %v61892 (stack98)
        %vm61894 = vcmp.eq.f32.partialorder %v61846, inf (stack99)
        %v61895 = vsel /*vm=*/%vm61894, /*on_true_vy=*/%v61846, /*on_false_vx=*/%v61893 (stack100)
        %vm61896 = vcmp.eq.f32.partialorder %v61846, 0.0 (stack101)
        %v61897 = vand.u32 %v61846, 2147483648 (stack102)
        %v61898 = vsel /*vm=*/%vm61896, /*on_true_vy=*/%v61897, /*on_false_vx=*/%v61895 (stack103)
        %v61901 = vadd.f32 %v61898, -3.0 (stack82)
        %v61905 = vsel /*vm=*/%vm61849, /*on_true_vy=*/%v61890, /*on_false_vx=*/%v61901 (stack72)
        %v61909 = vmul.f32 %v61886, %v61905 (stack83)
        %v61913 = vadd.f32 %v61882, %v61909 (stack82)
        %v61917 = vmul.f32 %v61913, %v61905 (stack83)
        %v61921 = vadd.f32 %v61878, %v61917 (stack82)
        %v61925 = vmul.f32 %v61921, %v61905 (stack83)
        %v61929 = vadd.f32 %v61874, %v61925 (stack82)
        %v61933 = vmul.f32 %v61929, %v61905 (stack83)
        %v61937 = vadd.f32 %v61870, %v61933 (stack82)
        %v61941 = vmul.f32 %v61937, %v61905 (stack83)
        %v61945 = vadd.f32 %v61866, %v61941 (stack82)
        %v61949 = vmul.f32 %v61945, %v61905 (stack83)
        %v61953 = vadd.f32 %v61862, %v61949 (stack82)
        %v61957 = vmul.f32 %v61953, %v61905 (stack83)
        %v61961 = vadd.f32 %v61858, %v61957 (stack82)
        %v61965 = vmul.f32 %v61961, %v61905 (stack83)
        %v61969 = vadd.f32 %v61854, %v61965 (stack82)
        %v61973 = vmul.f32 %v61969, %v61820 (stack83)
        %v61977 = vsel /*vm=*/%vm61825, /*on_true_vy=*/%v61830, /*on_false_vx=*/%v61973 (stack72)
        %v61981 = vmul.f32 %v61977, 1.4140625 (stack83)
        %s61983 = scalar_lea.vmem %s280, 448 [#allocation0] (stack107)
        %v61984 = vpack.c.bf16 0.0, %v61981 (stack104)
        %61985 = vst [vmem:[%s61983] sm:$0xf] /*vst_source=*/%v61984 (stack105)
        %v61988 = vadd.s32 %v2355, %v60141 (stack65)
        %s61990 = smul.u32 128, %s27 (stack66)
        %v61991 = vlaneseq (stack67)
        %v61992 = vand.u32 %v61991, 127 (stack68)
        %v61993 = vstv %s61990 (stack69)
        %v61994 = vadd.s32 %v61992, %v61993 (stack70)
        %v61998 = vadd.s32 %v61988, %v61994 (stack65)
        %vm62002 = vcmp.lt.u32.totalorder %v61998, %v61988 (stack71)
        %vm62007 = vcmp.lt.u32.totalorder %v61988, %v2355 (stack71)
        %v62012 = vadd.s32 %v2342, %v60124 (stack65)
        %v62016 = vadd.s32 %v62012, 1 (stack65)
        %v62020 = vsel /*vm=*/%vm62007, /*on_true_vy=*/%v62016, /*on_false_vx=*/%v62012 (stack72)
        %v62024 = vadd.s32 %v62020, 1 (stack65)
        %v62028 = vsel /*vm=*/%vm62002, /*on_true_vy=*/%v62024, /*on_false_vx=*/%v62020 (stack72)
        %v62033 = vadd.s32 %v62028, %v10 (stack65)
        %v62037 = vadd.s32 %v61998, %v9 (stack65)
        %v62041 = vadd.s32 %v62033, %v62037 (stack65)
        %v62043 = vshll.u32 %v62037, 13 (stack73)
        %v62044 = vshrl.u32 %v62037, 19 (stack74)
        %v62045 = vor.u32 %v62043, %v62044 (stack75)
        %v62046 = vxor.u32 %v62041, %v62045 (stack76)
        %v62049 = vadd.s32 %v62041, %v62046 (stack65)
        %v62051 = vshll.u32 %v62046, 15 (stack73)
        %v62052 = vshrl.u32 %v62046, 17 (stack74)
        %v62053 = vor.u32 %v62051, %v62052 (stack75)
        %v62054 = vxor.u32 %v62049, %v62053 (stack76)
        %v62057 = vadd.s32 %v62049, %v62054 (stack65)
        %v62059 = vshll.u32 %v62054, 26 (stack73)
        %v62060 = vshrl.u32 %v62054, 6 (stack74)
        %v62061 = vor.u32 %v62059, %v62060 (stack75)
        %v62062 = vxor.u32 %v62057, %v62061 (stack76)
        %v62065 = vadd.s32 %v62057, %v62062 (stack65)
        %v62069 = vadd.s32 %v62065, %v9 (stack65)
        %v62071 = vshll.u32 %v62062, 6 (stack73)
        %v62072 = vshrl.u32 %v62062, 26 (stack74)
        %v62073 = vor.u32 %v62071, %v62072 (stack75)
        %v62074 = vxor.u32 %v62065, %v62073 (stack76)
        %v62077 = vadd.s32 %v62074, %v8 (stack65)
        %v62081 = vadd.s32 %v62077, 1 (stack65)
        %v62085 = vadd.s32 %v62069, %v62081 (stack65)
        %v62087 = vshll.u32 %v62081, 17 (stack73)
        %v62088 = vshrl.u32 %v62081, 15 (stack74)
        %v62089 = vor.u32 %v62087, %v62088 (stack75)
        %v62090 = vxor.u32 %v62085, %v62089 (stack76)
        %v62093 = vadd.s32 %v62085, %v62090 (stack65)
        %v62095 = vshll.u32 %v62090, 29 (stack73)
        %v62096 = vshrl.u32 %v62090, 3 (stack74)
        %v62097 = vor.u32 %v62095, %v62096 (stack75)
        %v62098 = vxor.u32 %v62093, %v62097 (stack76)
        %v62101 = vadd.s32 %v62093, %v62098 (stack65)
        %v62103 = vshll.u32 %v62098, 16 (stack73)
        %v62104 = vshrl.u32 %v62098, 16 (stack74)
        %v62105 = vor.u32 %v62103, %v62104 (stack75)
        %v62106 = vxor.u32 %v62101, %v62105 (stack76)
        %v62109 = vadd.s32 %v62101, %v62106 (stack65)
        %v62113 = vadd.s32 %v62109, %v8 (stack65)
        %v62115 = vshll.u32 %v62106, 24 (stack73)
        %v62116 = vshrl.u32 %v62106, 8 (stack74)
        %v62117 = vor.u32 %v62115, %v62116 (stack75)
        %v62118 = vxor.u32 %v62109, %v62117 (stack76)
        %v62121 = vadd.s32 %v62118, %v10 (stack65)
        %v62125 = vadd.s32 %v62121, 2 (stack65)
        %v62129 = vadd.s32 %v62113, %v62125 (stack65)
        %v62131 = vshll.u32 %v62125, 13 (stack73)
        %v62132 = vshrl.u32 %v62125, 19 (stack74)
        %v62133 = vor.u32 %v62131, %v62132 (stack75)
        %v62134 = vxor.u32 %v62129, %v62133 (stack76)
        %v62137 = vadd.s32 %v62129, %v62134 (stack65)
        %v62139 = vshll.u32 %v62134, 15 (stack73)
        %v62140 = vshrl.u32 %v62134, 17 (stack74)
        %v62141 = vor.u32 %v62139, %v62140 (stack75)
        %v62142 = vxor.u32 %v62137, %v62141 (stack76)
        %v62145 = vadd.s32 %v62137, %v62142 (stack65)
        %v62147 = vshll.u32 %v62142, 26 (stack73)
        %v62148 = vshrl.u32 %v62142, 6 (stack74)
        %v62149 = vor.u32 %v62147, %v62148 (stack75)
        %v62150 = vxor.u32 %v62145, %v62149 (stack76)
        %v62153 = vadd.s32 %v62145, %v62150 (stack65)
        %v62157 = vadd.s32 %v62153, %v10 (stack65)
        %v62159 = vshll.u32 %v62150, 6 (stack73)
        %v62160 = vshrl.u32 %v62150, 26 (stack74)
        %v62161 = vor.u32 %v62159, %v62160 (stack75)
        %v62162 = vxor.u32 %v62153, %v62161 (stack76)
        %v62165 = vadd.s32 %v62162, %v9 (stack65)
        %v62169 = vadd.s32 %v62165, 3 (stack65)
        %v62173 = vadd.s32 %v62157, %v62169 (stack65)
        %v62175 = vshll.u32 %v62169, 17 (stack73)
        %v62176 = vshrl.u32 %v62169, 15 (stack74)
        %v62177 = vor.u32 %v62175, %v62176 (stack75)
        %v62178 = vxor.u32 %v62173, %v62177 (stack76)
        %v62181 = vadd.s32 %v62173, %v62178 (stack65)
        %v62183 = vshll.u32 %v62178, 29 (stack73)
        %v62184 = vshrl.u32 %v62178, 3 (stack74)
        %v62185 = vor.u32 %v62183, %v62184 (stack75)
        %v62186 = vxor.u32 %v62181, %v62185 (stack76)
        %v62189 = vadd.s32 %v62181, %v62186 (stack65)
        %v62191 = vshll.u32 %v62186, 16 (stack73)
        %v62192 = vshrl.u32 %v62186, 16 (stack74)
        %v62193 = vor.u32 %v62191, %v62192 (stack75)
        %v62194 = vxor.u32 %v62189, %v62193 (stack76)
        %v62197 = vadd.s32 %v62189, %v62194 (stack65)
        %v62201 = vadd.s32 %v62197, %v9 (stack65)
        %v62203 = vshll.u32 %v62194, 24 (stack73)
        %v62204 = vshrl.u32 %v62194, 8 (stack74)
        %v62205 = vor.u32 %v62203, %v62204 (stack75)
        %v62206 = vxor.u32 %v62197, %v62205 (stack76)
        %v62209 = vadd.s32 %v62206, %v8 (stack65)
        %v62213 = vadd.s32 %v62209, 4 (stack65)
        %v62217 = vadd.s32 %v62201, %v62213 (stack65)
        %v62219 = vshll.u32 %v62213, 13 (stack73)
        %v62220 = vshrl.u32 %v62213, 19 (stack74)
        %v62221 = vor.u32 %v62219, %v62220 (stack75)
        %v62222 = vxor.u32 %v62217, %v62221 (stack76)
        %v62225 = vadd.s32 %v62217, %v62222 (stack65)
        %v62227 = vshll.u32 %v62222, 15 (stack73)
        %v62228 = vshrl.u32 %v62222, 17 (stack74)
        %v62229 = vor.u32 %v62227, %v62228 (stack75)
        %v62230 = vxor.u32 %v62225, %v62229 (stack76)
        %v62233 = vadd.s32 %v62225, %v62230 (stack65)
        %v62235 = vshll.u32 %v62230, 26 (stack73)
        %v62236 = vshrl.u32 %v62230, 6 (stack74)
        %v62237 = vor.u32 %v62235, %v62236 (stack75)
        %v62238 = vxor.u32 %v62233, %v62237 (stack76)
        %v62241 = vadd.s32 %v62233, %v62238 (stack65)
        %v62245 = vadd.s32 %v62241, %v8 (stack65)
        %v62247 = vshll.u32 %v62238, 6 (stack73)
        %v62248 = vshrl.u32 %v62238, 26 (stack74)
        %v62249 = vor.u32 %v62247, %v62248 (stack75)
        %v62250 = vxor.u32 %v62241, %v62249 (stack76)
        %v62253 = vadd.s32 %v62250, %v10 (stack65)
        %v62257 = vadd.s32 %v62253, 5 (stack65)
        %v62259 = vxor.u32 %v62245, %v62257 (stack76)
        %v62260 = vand.u32.u8 %v62259, 255 (stack77)
        %v62261 = vand.u32 %v62260, 65535 (stack78)
        %v62262 = vshrl.u32 %v62261, 1 (stack79)
        %v62263 = vor.u32 %v62262, 16256 (stack75)
        %v62264 = vand.u32.u16 %v62263, 65535 (stack80)
        %v62265 = vunpack.i.l.bf16 %v62264 (stack81)
        %v62269 = vadd.f32 %v62265, -1.0 (stack82)
        %v62273 = vmul.f32 %v62269, 2.0 (stack83)
        %v62277 = vadd.f32 %v62273, -0.99609375 (stack82)
        %v62281 = vmax.f32 -0.99609375, %v62277 (stack84)
        %v62283 = vand.u32 2147483647, %v62281 (stack85)
        %vm62286 = vcmp.eq.f32.partialorder %v62283, 1.0 (stack86)
        %v62291 = vmul.f32 %v62281, inf (stack83)
        %v62293 = vxor.u32 %v62281, 2147483648 (stack87)
        %v62296 = vmul.f32 %v62281, %v62293 (stack83)
        %v62298 = vadd.f32 %v62296, 1.0 (stack88)
        %v62299 = vlog2.pop %v62298 (stack89)
        %v62300 = vmul.f32 %v62299, 0.6931472 (stack90)
        %v62301 = vmul.f32 -0.5, %v62296 (stack91)
        %v62302 = vadd.f32 %v62301, 1.0 (stack92)
        %v62303 = vmul.f32 %v62302, %v62296 (stack93)
        %v62304 = vand.u32 2147483647, %v62296 (stack94)
        %vm62305 = vcmp.lt.f32.partialorder %v62304, 0.0004427343 (stack95)
        %v62306 = vsel /*vm=*/%vm62305, /*on_true_vy=*/%v62303, /*on_false_vx=*/%v62300 (stack96)
        %v62307 = vxor.u32 %v62306, 2147483648 (stack87)
        %vm62310 = vcmp.lt.f32.partialorder %v62307, 5.0 (stack86)
        %v62315 = vsel /*vm=*/%vm62310, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v62319 = vsel /*vm=*/%vm62310, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v62323 = vsel /*vm=*/%vm62310, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v62327 = vsel /*vm=*/%vm62310, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v62331 = vsel /*vm=*/%vm62310, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v62335 = vsel /*vm=*/%vm62310, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v62339 = vsel /*vm=*/%vm62310, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v62343 = vsel /*vm=*/%vm62310, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v62347 = vsel /*vm=*/%vm62310, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v62351 = vadd.f32 %v62307, -2.5 (stack82)
        %v62353 = vrsqrt.pop %v62307 (stack97)
        %v62354 = vmul.f32 %v62307, %v62353 (stack98)
        %vm62355 = vcmp.eq.f32.partialorder %v62307, inf (stack99)
        %v62356 = vsel /*vm=*/%vm62355, /*on_true_vy=*/%v62307, /*on_false_vx=*/%v62354 (stack100)
        %vm62357 = vcmp.eq.f32.partialorder %v62307, 0.0 (stack101)
        %v62358 = vand.u32 %v62307, 2147483648 (stack102)
        %v62359 = vsel /*vm=*/%vm62357, /*on_true_vy=*/%v62358, /*on_false_vx=*/%v62356 (stack103)
        %v62362 = vadd.f32 %v62359, -3.0 (stack82)
        %v62366 = vsel /*vm=*/%vm62310, /*on_true_vy=*/%v62351, /*on_false_vx=*/%v62362 (stack72)
        %v62370 = vmul.f32 %v62347, %v62366 (stack83)
        %v62374 = vadd.f32 %v62343, %v62370 (stack82)
        %v62378 = vmul.f32 %v62374, %v62366 (stack83)
        %v62382 = vadd.f32 %v62339, %v62378 (stack82)
        %v62386 = vmul.f32 %v62382, %v62366 (stack83)
        %v62390 = vadd.f32 %v62335, %v62386 (stack82)
        %v62394 = vmul.f32 %v62390, %v62366 (stack83)
        %v62398 = vadd.f32 %v62331, %v62394 (stack82)
        %v62402 = vmul.f32 %v62398, %v62366 (stack83)
        %v62406 = vadd.f32 %v62327, %v62402 (stack82)
        %v62410 = vmul.f32 %v62406, %v62366 (stack83)
        %v62414 = vadd.f32 %v62323, %v62410 (stack82)
        %v62418 = vmul.f32 %v62414, %v62366 (stack83)
        %v62422 = vadd.f32 %v62319, %v62418 (stack82)
        %v62426 = vmul.f32 %v62422, %v62366 (stack83)
        %v62430 = vadd.f32 %v62315, %v62426 (stack82)
        %v62434 = vmul.f32 %v62430, %v62281 (stack83)
        %v62438 = vsel /*vm=*/%vm62286, /*on_true_vy=*/%v62291, /*on_false_vx=*/%v62434 (stack72)
        %v62442 = vmul.f32 %v62438, 1.4140625 (stack83)
        %s62444 = scalar_lea.vmem %s280, 576 [#allocation0] (stack107)
        %v62445 = vpack.c.bf16 0.0, %v62442 (stack104)
        %62446 = vst [vmem:[%s62444] sm:$0xf] /*vst_source=*/%v62445 (stack105)
        %v62449 = vadd.s32 %v2842, %v60141 (stack65)
        %s62451 = smul.u32 128, %s27 (stack66)
        %v62452 = vlaneseq (stack67)
        %v62453 = vand.u32 %v62452, 127 (stack68)
        %v62454 = vstv %s62451 (stack69)
        %v62455 = vadd.s32 %v62453, %v62454 (stack70)
        %v62459 = vadd.s32 %v62449, %v62455 (stack65)
        %vm62463 = vcmp.lt.u32.totalorder %v62459, %v62449 (stack71)
        %vm62468 = vcmp.lt.u32.totalorder %v62449, %v2842 (stack71)
        %v62473 = vadd.s32 %v2829, %v60124 (stack65)
        %v62477 = vadd.s32 %v62473, 1 (stack65)
        %v62481 = vsel /*vm=*/%vm62468, /*on_true_vy=*/%v62477, /*on_false_vx=*/%v62473 (stack72)
        %v62485 = vadd.s32 %v62481, 1 (stack65)
        %v62489 = vsel /*vm=*/%vm62463, /*on_true_vy=*/%v62485, /*on_false_vx=*/%v62481 (stack72)
        %v62494 = vadd.s32 %v62489, %v10 (stack65)
        %v62498 = vadd.s32 %v62459, %v9 (stack65)
        %v62502 = vadd.s32 %v62494, %v62498 (stack65)
        %v62504 = vshll.u32 %v62498, 13 (stack73)
        %v62505 = vshrl.u32 %v62498, 19 (stack74)
        %v62506 = vor.u32 %v62504, %v62505 (stack75)
        %v62507 = vxor.u32 %v62502, %v62506 (stack76)
        %v62510 = vadd.s32 %v62502, %v62507 (stack65)
        %v62512 = vshll.u32 %v62507, 15 (stack73)
        %v62513 = vshrl.u32 %v62507, 17 (stack74)
        %v62514 = vor.u32 %v62512, %v62513 (stack75)
        %v62515 = vxor.u32 %v62510, %v62514 (stack76)
        %v62518 = vadd.s32 %v62510, %v62515 (stack65)
        %v62520 = vshll.u32 %v62515, 26 (stack73)
        %v62521 = vshrl.u32 %v62515, 6 (stack74)
        %v62522 = vor.u32 %v62520, %v62521 (stack75)
        %v62523 = vxor.u32 %v62518, %v62522 (stack76)
        %v62526 = vadd.s32 %v62518, %v62523 (stack65)
        %v62530 = vadd.s32 %v62526, %v9 (stack65)
        %v62532 = vshll.u32 %v62523, 6 (stack73)
        %v62533 = vshrl.u32 %v62523, 26 (stack74)
        %v62534 = vor.u32 %v62532, %v62533 (stack75)
        %v62535 = vxor.u32 %v62526, %v62534 (stack76)
        %v62538 = vadd.s32 %v62535, %v8 (stack65)
        %v62542 = vadd.s32 %v62538, 1 (stack65)
        %v62546 = vadd.s32 %v62530, %v62542 (stack65)
        %v62548 = vshll.u32 %v62542, 17 (stack73)
        %v62549 = vshrl.u32 %v62542, 15 (stack74)
        %v62550 = vor.u32 %v62548, %v62549 (stack75)
        %v62551 = vxor.u32 %v62546, %v62550 (stack76)
        %v62554 = vadd.s32 %v62546, %v62551 (stack65)
        %v62556 = vshll.u32 %v62551, 29 (stack73)
        %v62557 = vshrl.u32 %v62551, 3 (stack74)
        %v62558 = vor.u32 %v62556, %v62557 (stack75)
        %v62559 = vxor.u32 %v62554, %v62558 (stack76)
        %v62562 = vadd.s32 %v62554, %v62559 (stack65)
        %v62564 = vshll.u32 %v62559, 16 (stack73)
        %v62565 = vshrl.u32 %v62559, 16 (stack74)
        %v62566 = vor.u32 %v62564, %v62565 (stack75)
        %v62567 = vxor.u32 %v62562, %v62566 (stack76)
        %v62570 = vadd.s32 %v62562, %v62567 (stack65)
        %v62574 = vadd.s32 %v62570, %v8 (stack65)
        %v62576 = vshll.u32 %v62567, 24 (stack73)
        %v62577 = vshrl.u32 %v62567, 8 (stack74)
        %v62578 = vor.u32 %v62576, %v62577 (stack75)
        %v62579 = vxor.u32 %v62570, %v62578 (stack76)
        %v62582 = vadd.s32 %v62579, %v10 (stack65)
        %v62586 = vadd.s32 %v62582, 2 (stack65)
        %v62590 = vadd.s32 %v62574, %v62586 (stack65)
        %v62592 = vshll.u32 %v62586, 13 (stack73)
        %v62593 = vshrl.u32 %v62586, 19 (stack74)
        %v62594 = vor.u32 %v62592, %v62593 (stack75)
        %v62595 = vxor.u32 %v62590, %v62594 (stack76)
        %v62598 = vadd.s32 %v62590, %v62595 (stack65)
        %v62600 = vshll.u32 %v62595, 15 (stack73)
        %v62601 = vshrl.u32 %v62595, 17 (stack74)
        %v62602 = vor.u32 %v62600, %v62601 (stack75)
        %v62603 = vxor.u32 %v62598, %v62602 (stack76)
        %v62606 = vadd.s32 %v62598, %v62603 (stack65)
        %v62608 = vshll.u32 %v62603, 26 (stack73)
        %v62609 = vshrl.u32 %v62603, 6 (stack74)
        %v62610 = vor.u32 %v62608, %v62609 (stack75)
        %v62611 = vxor.u32 %v62606, %v62610 (stack76)
        %v62614 = vadd.s32 %v62606, %v62611 (stack65)
        %v62618 = vadd.s32 %v62614, %v10 (stack65)
        %v62620 = vshll.u32 %v62611, 6 (stack73)
        %v62621 = vshrl.u32 %v62611, 26 (stack74)
        %v62622 = vor.u32 %v62620, %v62621 (stack75)
        %v62623 = vxor.u32 %v62614, %v62622 (stack76)
        %v62626 = vadd.s32 %v62623, %v9 (stack65)
        %v62630 = vadd.s32 %v62626, 3 (stack65)
        %v62634 = vadd.s32 %v62618, %v62630 (stack65)
        %v62636 = vshll.u32 %v62630, 17 (stack73)
        %v62637 = vshrl.u32 %v62630, 15 (stack74)
        %v62638 = vor.u32 %v62636, %v62637 (stack75)
        %v62639 = vxor.u32 %v62634, %v62638 (stack76)
        %v62642 = vadd.s32 %v62634, %v62639 (stack65)
        %v62644 = vshll.u32 %v62639, 29 (stack73)
        %v62645 = vshrl.u32 %v62639, 3 (stack74)
        %v62646 = vor.u32 %v62644, %v62645 (stack75)
        %v62647 = vxor.u32 %v62642, %v62646 (stack76)
        %v62650 = vadd.s32 %v62642, %v62647 (stack65)
        %v62652 = vshll.u32 %v62647, 16 (stack73)
        %v62653 = vshrl.u32 %v62647, 16 (stack74)
        %v62654 = vor.u32 %v62652, %v62653 (stack75)
        %v62655 = vxor.u32 %v62650, %v62654 (stack76)
        %v62658 = vadd.s32 %v62650, %v62655 (stack65)
        %v62662 = vadd.s32 %v62658, %v9 (stack65)
        %v62664 = vshll.u32 %v62655, 24 (stack73)
        %v62665 = vshrl.u32 %v62655, 8 (stack74)
        %v62666 = vor.u32 %v62664, %v62665 (stack75)
        %v62667 = vxor.u32 %v62658, %v62666 (stack76)
        %v62670 = vadd.s32 %v62667, %v8 (stack65)
        %v62674 = vadd.s32 %v62670, 4 (stack65)
        %v62678 = vadd.s32 %v62662, %v62674 (stack65)
        %v62680 = vshll.u32 %v62674, 13 (stack73)
        %v62681 = vshrl.u32 %v62674, 19 (stack74)
        %v62682 = vor.u32 %v62680, %v62681 (stack75)
        %v62683 = vxor.u32 %v62678, %v62682 (stack76)
        %v62686 = vadd.s32 %v62678, %v62683 (stack65)
        %v62688 = vshll.u32 %v62683, 15 (stack73)
        %v62689 = vshrl.u32 %v62683, 17 (stack74)
        %v62690 = vor.u32 %v62688, %v62689 (stack75)
        %v62691 = vxor.u32 %v62686, %v62690 (stack76)
        %v62694 = vadd.s32 %v62686, %v62691 (stack65)
        %v62696 = vshll.u32 %v62691, 26 (stack73)
        %v62697 = vshrl.u32 %v62691, 6 (stack74)
        %v62698 = vor.u32 %v62696, %v62697 (stack75)
        %v62699 = vxor.u32 %v62694, %v62698 (stack76)
        %v62702 = vadd.s32 %v62694, %v62699 (stack65)
        %v62706 = vadd.s32 %v62702, %v8 (stack65)
        %v62708 = vshll.u32 %v62699, 6 (stack73)
        %v62709 = vshrl.u32 %v62699, 26 (stack74)
        %v62710 = vor.u32 %v62708, %v62709 (stack75)
        %v62711 = vxor.u32 %v62702, %v62710 (stack76)
        %v62714 = vadd.s32 %v62711, %v10 (stack65)
        %v62718 = vadd.s32 %v62714, 5 (stack65)
        %v62720 = vxor.u32 %v62706, %v62718 (stack76)
        %v62721 = vand.u32.u8 %v62720, 255 (stack77)
        %v62722 = vand.u32 %v62721, 65535 (stack78)
        %v62723 = vshrl.u32 %v62722, 1 (stack79)
        %v62724 = vor.u32 %v62723, 16256 (stack75)
        %v62725 = vand.u32.u16 %v62724, 65535 (stack80)
        %v62726 = vunpack.i.l.bf16 %v62725 (stack81)
        %v62730 = vadd.f32 %v62726, -1.0 (stack82)
        %v62734 = vmul.f32 %v62730, 2.0 (stack83)
        %v62738 = vadd.f32 %v62734, -0.99609375 (stack82)
        %v62742 = vmax.f32 -0.99609375, %v62738 (stack84)
        %v62744 = vand.u32 2147483647, %v62742 (stack85)
        %vm62747 = vcmp.eq.f32.partialorder %v62744, 1.0 (stack86)
        %v62752 = vmul.f32 %v62742, inf (stack83)
        %v62754 = vxor.u32 %v62742, 2147483648 (stack87)
        %v62757 = vmul.f32 %v62742, %v62754 (stack83)
        %v62759 = vadd.f32 %v62757, 1.0 (stack88)
        %v62760 = vlog2.pop %v62759 (stack89)
        %v62761 = vmul.f32 %v62760, 0.6931472 (stack90)
        %v62762 = vmul.f32 -0.5, %v62757 (stack91)
        %v62763 = vadd.f32 %v62762, 1.0 (stack92)
        %v62764 = vmul.f32 %v62763, %v62757 (stack93)
        %v62765 = vand.u32 2147483647, %v62757 (stack94)
        %vm62766 = vcmp.lt.f32.partialorder %v62765, 0.0004427343 (stack95)
        %v62767 = vsel /*vm=*/%vm62766, /*on_true_vy=*/%v62764, /*on_false_vx=*/%v62761 (stack96)
        %v62768 = vxor.u32 %v62767, 2147483648 (stack87)
        %vm62771 = vcmp.lt.f32.partialorder %v62768, 5.0 (stack86)
        %v62776 = vsel /*vm=*/%vm62771, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v62780 = vsel /*vm=*/%vm62771, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v62784 = vsel /*vm=*/%vm62771, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v62788 = vsel /*vm=*/%vm62771, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v62792 = vsel /*vm=*/%vm62771, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v62796 = vsel /*vm=*/%vm62771, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v62800 = vsel /*vm=*/%vm62771, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v62804 = vsel /*vm=*/%vm62771, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v62808 = vsel /*vm=*/%vm62771, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v62812 = vadd.f32 %v62768, -2.5 (stack82)
        %v62814 = vrsqrt.pop %v62768 (stack97)
        %v62815 = vmul.f32 %v62768, %v62814 (stack98)
        %vm62816 = vcmp.eq.f32.partialorder %v62768, inf (stack99)
        %v62817 = vsel /*vm=*/%vm62816, /*on_true_vy=*/%v62768, /*on_false_vx=*/%v62815 (stack100)
        %vm62818 = vcmp.eq.f32.partialorder %v62768, 0.0 (stack101)
        %v62819 = vand.u32 %v62768, 2147483648 (stack102)
        %v62820 = vsel /*vm=*/%vm62818, /*on_true_vy=*/%v62819, /*on_false_vx=*/%v62817 (stack103)
        %v62823 = vadd.f32 %v62820, -3.0 (stack82)
        %v62827 = vsel /*vm=*/%vm62771, /*on_true_vy=*/%v62812, /*on_false_vx=*/%v62823 (stack72)
        %v62831 = vmul.f32 %v62808, %v62827 (stack83)
        %v62835 = vadd.f32 %v62804, %v62831 (stack82)
        %v62839 = vmul.f32 %v62835, %v62827 (stack83)
        %v62843 = vadd.f32 %v62800, %v62839 (stack82)
        %v62847 = vmul.f32 %v62843, %v62827 (stack83)
        %v62851 = vadd.f32 %v62796, %v62847 (stack82)
        %v62855 = vmul.f32 %v62851, %v62827 (stack83)
        %v62859 = vadd.f32 %v62792, %v62855 (stack82)
        %v62863 = vmul.f32 %v62859, %v62827 (stack83)
        %v62867 = vadd.f32 %v62788, %v62863 (stack82)
        %v62871 = vmul.f32 %v62867, %v62827 (stack83)
        %v62875 = vadd.f32 %v62784, %v62871 (stack82)
        %v62879 = vmul.f32 %v62875, %v62827 (stack83)
        %v62883 = vadd.f32 %v62780, %v62879 (stack82)
        %v62887 = vmul.f32 %v62883, %v62827 (stack83)
        %v62891 = vadd.f32 %v62776, %v62887 (stack82)
        %v62895 = vmul.f32 %v62891, %v62742 (stack83)
        %v62899 = vsel /*vm=*/%vm62747, /*on_true_vy=*/%v62752, /*on_false_vx=*/%v62895 (stack72)
        %v62903 = vmul.f32 %v62899, 1.4140625 (stack83)
        %s62905 = scalar_lea.vmem %s280, 704 [#allocation0] (stack107)
        %v62906 = vpack.c.bf16 0.0, %v62903 (stack104)
        %62907 = vst [vmem:[%s62905] sm:$0xf] /*vst_source=*/%v62906 (stack105)
        %v62910 = vadd.s32 %v3329, %v60141 (stack65)
        %s62912 = smul.u32 128, %s27 (stack66)
        %v62913 = vlaneseq (stack67)
        %v62914 = vand.u32 %v62913, 127 (stack68)
        %v62915 = vstv %s62912 (stack69)
        %v62916 = vadd.s32 %v62914, %v62915 (stack70)
        %v62920 = vadd.s32 %v62910, %v62916 (stack65)
        %vm62924 = vcmp.lt.u32.totalorder %v62920, %v62910 (stack71)
        %vm62929 = vcmp.lt.u32.totalorder %v62910, %v3329 (stack71)
        %v62934 = vadd.s32 %v3316, %v60124 (stack65)
        %v62938 = vadd.s32 %v62934, 1 (stack65)
        %v62942 = vsel /*vm=*/%vm62929, /*on_true_vy=*/%v62938, /*on_false_vx=*/%v62934 (stack72)
        %v62946 = vadd.s32 %v62942, 1 (stack65)
        %v62950 = vsel /*vm=*/%vm62924, /*on_true_vy=*/%v62946, /*on_false_vx=*/%v62942 (stack72)
        %v62955 = vadd.s32 %v62950, %v10 (stack65)
        %v62959 = vadd.s32 %v62920, %v9 (stack65)
        %v62963 = vadd.s32 %v62955, %v62959 (stack65)
        %v62965 = vshll.u32 %v62959, 13 (stack73)
        %v62966 = vshrl.u32 %v62959, 19 (stack74)
        %v62967 = vor.u32 %v62965, %v62966 (stack75)
        %v62968 = vxor.u32 %v62963, %v62967 (stack76)
        %v62971 = vadd.s32 %v62963, %v62968 (stack65)
        %v62973 = vshll.u32 %v62968, 15 (stack73)
        %v62974 = vshrl.u32 %v62968, 17 (stack74)
        %v62975 = vor.u32 %v62973, %v62974 (stack75)
        %v62976 = vxor.u32 %v62971, %v62975 (stack76)
        %v62979 = vadd.s32 %v62971, %v62976 (stack65)
        %v62981 = vshll.u32 %v62976, 26 (stack73)
        %v62982 = vshrl.u32 %v62976, 6 (stack74)
        %v62983 = vor.u32 %v62981, %v62982 (stack75)
        %v62984 = vxor.u32 %v62979, %v62983 (stack76)
        %v62987 = vadd.s32 %v62979, %v62984 (stack65)
        %v62991 = vadd.s32 %v62987, %v9 (stack65)
        %v62993 = vshll.u32 %v62984, 6 (stack73)
        %v62994 = vshrl.u32 %v62984, 26 (stack74)
        %v62995 = vor.u32 %v62993, %v62994 (stack75)
        %v62996 = vxor.u32 %v62987, %v62995 (stack76)
        %v62999 = vadd.s32 %v62996, %v8 (stack65)
        %v63003 = vadd.s32 %v62999, 1 (stack65)
        %v63007 = vadd.s32 %v62991, %v63003 (stack65)
        %v63009 = vshll.u32 %v63003, 17 (stack73)
        %v63010 = vshrl.u32 %v63003, 15 (stack74)
        %v63011 = vor.u32 %v63009, %v63010 (stack75)
        %v63012 = vxor.u32 %v63007, %v63011 (stack76)
        %v63015 = vadd.s32 %v63007, %v63012 (stack65)
        %v63017 = vshll.u32 %v63012, 29 (stack73)
        %v63018 = vshrl.u32 %v63012, 3 (stack74)
        %v63019 = vor.u32 %v63017, %v63018 (stack75)
        %v63020 = vxor.u32 %v63015, %v63019 (stack76)
        %v63023 = vadd.s32 %v63015, %v63020 (stack65)
        %v63025 = vshll.u32 %v63020, 16 (stack73)
        %v63026 = vshrl.u32 %v63020, 16 (stack74)
        %v63027 = vor.u32 %v63025, %v63026 (stack75)
        %v63028 = vxor.u32 %v63023, %v63027 (stack76)
        %v63031 = vadd.s32 %v63023, %v63028 (stack65)
        %v63035 = vadd.s32 %v63031, %v8 (stack65)
        %v63037 = vshll.u32 %v63028, 24 (stack73)
        %v63038 = vshrl.u32 %v63028, 8 (stack74)
        %v63039 = vor.u32 %v63037, %v63038 (stack75)
        %v63040 = vxor.u32 %v63031, %v63039 (stack76)
        %v63043 = vadd.s32 %v63040, %v10 (stack65)
        %v63047 = vadd.s32 %v63043, 2 (stack65)
        %v63051 = vadd.s32 %v63035, %v63047 (stack65)
        %v63053 = vshll.u32 %v63047, 13 (stack73)
        %v63054 = vshrl.u32 %v63047, 19 (stack74)
        %v63055 = vor.u32 %v63053, %v63054 (stack75)
        %v63056 = vxor.u32 %v63051, %v63055 (stack76)
        %v63059 = vadd.s32 %v63051, %v63056 (stack65)
        %v63061 = vshll.u32 %v63056, 15 (stack73)
        %v63062 = vshrl.u32 %v63056, 17 (stack74)
        %v63063 = vor.u32 %v63061, %v63062 (stack75)
        %v63064 = vxor.u32 %v63059, %v63063 (stack76)
        %v63067 = vadd.s32 %v63059, %v63064 (stack65)
        %v63069 = vshll.u32 %v63064, 26 (stack73)
        %v63070 = vshrl.u32 %v63064, 6 (stack74)
        %v63071 = vor.u32 %v63069, %v63070 (stack75)
        %v63072 = vxor.u32 %v63067, %v63071 (stack76)
        %v63075 = vadd.s32 %v63067, %v63072 (stack65)
        %v63079 = vadd.s32 %v63075, %v10 (stack65)
        %v63081 = vshll.u32 %v63072, 6 (stack73)
        %v63082 = vshrl.u32 %v63072, 26 (stack74)
        %v63083 = vor.u32 %v63081, %v63082 (stack75)
        %v63084 = vxor.u32 %v63075, %v63083 (stack76)
        %v63087 = vadd.s32 %v63084, %v9 (stack65)
        %v63091 = vadd.s32 %v63087, 3 (stack65)
        %v63095 = vadd.s32 %v63079, %v63091 (stack65)
        %v63097 = vshll.u32 %v63091, 17 (stack73)
        %v63098 = vshrl.u32 %v63091, 15 (stack74)
        %v63099 = vor.u32 %v63097, %v63098 (stack75)
        %v63100 = vxor.u32 %v63095, %v63099 (stack76)
        %v63103 = vadd.s32 %v63095, %v63100 (stack65)
        %v63105 = vshll.u32 %v63100, 29 (stack73)
        %v63106 = vshrl.u32 %v63100, 3 (stack74)
        %v63107 = vor.u32 %v63105, %v63106 (stack75)
        %v63108 = vxor.u32 %v63103, %v63107 (stack76)
        %v63111 = vadd.s32 %v63103, %v63108 (stack65)
        %v63113 = vshll.u32 %v63108, 16 (stack73)
        %v63114 = vshrl.u32 %v63108, 16 (stack74)
        %v63115 = vor.u32 %v63113, %v63114 (stack75)
        %v63116 = vxor.u32 %v63111, %v63115 (stack76)
        %v63119 = vadd.s32 %v63111, %v63116 (stack65)
        %v63123 = vadd.s32 %v63119, %v9 (stack65)
        %v63125 = vshll.u32 %v63116, 24 (stack73)
        %v63126 = vshrl.u32 %v63116, 8 (stack74)
        %v63127 = vor.u32 %v63125, %v63126 (stack75)
        %v63128 = vxor.u32 %v63119, %v63127 (stack76)
        %v63131 = vadd.s32 %v63128, %v8 (stack65)
        %v63135 = vadd.s32 %v63131, 4 (stack65)
        %v63139 = vadd.s32 %v63123, %v63135 (stack65)
        %v63141 = vshll.u32 %v63135, 13 (stack73)
        %v63142 = vshrl.u32 %v63135, 19 (stack74)
        %v63143 = vor.u32 %v63141, %v63142 (stack75)
        %v63144 = vxor.u32 %v63139, %v63143 (stack76)
        %v63147 = vadd.s32 %v63139, %v63144 (stack65)
        %v63149 = vshll.u32 %v63144, 15 (stack73)
        %v63150 = vshrl.u32 %v63144, 17 (stack74)
        %v63151 = vor.u32 %v63149, %v63150 (stack75)
        %v63152 = vxor.u32 %v63147, %v63151 (stack76)
        %v63155 = vadd.s32 %v63147, %v63152 (stack65)
        %v63157 = vshll.u32 %v63152, 26 (stack73)
        %v63158 = vshrl.u32 %v63152, 6 (stack74)
        %v63159 = vor.u32 %v63157, %v63158 (stack75)
        %v63160 = vxor.u32 %v63155, %v63159 (stack76)
        %v63163 = vadd.s32 %v63155, %v63160 (stack65)
        %v63167 = vadd.s32 %v63163, %v8 (stack65)
        %v63169 = vshll.u32 %v63160, 6 (stack73)
        %v63170 = vshrl.u32 %v63160, 26 (stack74)
        %v63171 = vor.u32 %v63169, %v63170 (stack75)
        %v63172 = vxor.u32 %v63163, %v63171 (stack76)
        %v63175 = vadd.s32 %v63172, %v10 (stack65)
        %v63179 = vadd.s32 %v63175, 5 (stack65)
        %v63181 = vxor.u32 %v63167, %v63179 (stack76)
        %v63182 = vand.u32.u8 %v63181, 255 (stack77)
        %v63183 = vand.u32 %v63182, 65535 (stack78)
        %v63184 = vshrl.u32 %v63183, 1 (stack79)
        %v63185 = vor.u32 %v63184, 16256 (stack75)
        %v63186 = vand.u32.u16 %v63185, 65535 (stack80)
        %v63187 = vunpack.i.l.bf16 %v63186 (stack81)
        %v63191 = vadd.f32 %v63187, -1.0 (stack82)
        %v63195 = vmul.f32 %v63191, 2.0 (stack83)
        %v63199 = vadd.f32 %v63195, -0.99609375 (stack82)
        %v63203 = vmax.f32 -0.99609375, %v63199 (stack84)
        %v63205 = vand.u32 2147483647, %v63203 (stack85)
        %vm63208 = vcmp.eq.f32.partialorder %v63205, 1.0 (stack86)
        %v63213 = vmul.f32 %v63203, inf (stack83)
        %v63215 = vxor.u32 %v63203, 2147483648 (stack87)
        %v63218 = vmul.f32 %v63203, %v63215 (stack83)
        %v63220 = vadd.f32 %v63218, 1.0 (stack88)
        %v63221 = vlog2.pop %v63220 (stack89)
        %v63222 = vmul.f32 %v63221, 0.6931472 (stack90)
        %v63223 = vmul.f32 -0.5, %v63218 (stack91)
        %v63224 = vadd.f32 %v63223, 1.0 (stack92)
        %v63225 = vmul.f32 %v63224, %v63218 (stack93)
        %v63226 = vand.u32 2147483647, %v63218 (stack94)
        %vm63227 = vcmp.lt.f32.partialorder %v63226, 0.0004427343 (stack95)
        %v63228 = vsel /*vm=*/%vm63227, /*on_true_vy=*/%v63225, /*on_false_vx=*/%v63222 (stack96)
        %v63229 = vxor.u32 %v63228, 2147483648 (stack87)
        %vm63232 = vcmp.lt.f32.partialorder %v63229, 5.0 (stack86)
        %v63237 = vsel /*vm=*/%vm63232, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v63241 = vsel /*vm=*/%vm63232, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v63245 = vsel /*vm=*/%vm63232, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v63249 = vsel /*vm=*/%vm63232, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v63253 = vsel /*vm=*/%vm63232, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v63257 = vsel /*vm=*/%vm63232, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v63261 = vsel /*vm=*/%vm63232, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v63265 = vsel /*vm=*/%vm63232, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v63269 = vsel /*vm=*/%vm63232, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v63273 = vadd.f32 %v63229, -2.5 (stack82)
        %v63275 = vrsqrt.pop %v63229 (stack97)
        %v63276 = vmul.f32 %v63229, %v63275 (stack98)
        %vm63277 = vcmp.eq.f32.partialorder %v63229, inf (stack99)
        %v63278 = vsel /*vm=*/%vm63277, /*on_true_vy=*/%v63229, /*on_false_vx=*/%v63276 (stack100)
        %vm63279 = vcmp.eq.f32.partialorder %v63229, 0.0 (stack101)
        %v63280 = vand.u32 %v63229, 2147483648 (stack102)
        %v63281 = vsel /*vm=*/%vm63279, /*on_true_vy=*/%v63280, /*on_false_vx=*/%v63278 (stack103)
        %v63284 = vadd.f32 %v63281, -3.0 (stack82)
        %v63288 = vsel /*vm=*/%vm63232, /*on_true_vy=*/%v63273, /*on_false_vx=*/%v63284 (stack72)
        %v63292 = vmul.f32 %v63269, %v63288 (stack83)
        %v63296 = vadd.f32 %v63265, %v63292 (stack82)
        %v63300 = vmul.f32 %v63296, %v63288 (stack83)
        %v63304 = vadd.f32 %v63261, %v63300 (stack82)
        %v63308 = vmul.f32 %v63304, %v63288 (stack83)
        %v63312 = vadd.f32 %v63257, %v63308 (stack82)
        %v63316 = vmul.f32 %v63312, %v63288 (stack83)
        %v63320 = vadd.f32 %v63253, %v63316 (stack82)
        %v63324 = vmul.f32 %v63320, %v63288 (stack83)
        %v63328 = vadd.f32 %v63249, %v63324 (stack82)
        %v63332 = vmul.f32 %v63328, %v63288 (stack83)
        %v63336 = vadd.f32 %v63245, %v63332 (stack82)
        %v63340 = vmul.f32 %v63336, %v63288 (stack83)
        %v63344 = vadd.f32 %v63241, %v63340 (stack82)
        %v63348 = vmul.f32 %v63344, %v63288 (stack83)
        %v63352 = vadd.f32 %v63237, %v63348 (stack82)
        %v63356 = vmul.f32 %v63352, %v63203 (stack83)
        %v63360 = vsel /*vm=*/%vm63208, /*on_true_vy=*/%v63213, /*on_false_vx=*/%v63356 (stack72)
        %v63364 = vmul.f32 %v63360, 1.4140625 (stack83)
        %s63366 = scalar_lea.vmem %s280, 832 [#allocation0] (stack107)
        %v63367 = vpack.c.bf16 0.0, %v63364 (stack104)
        %63368 = vst [vmem:[%s63366] sm:$0xf] /*vst_source=*/%v63367 (stack105)
        %v63371 = vadd.s32 %v3816, %v60141 (stack65)
        %s63373 = smul.u32 128, %s27 (stack66)
        %v63374 = vlaneseq (stack67)
        %v63375 = vand.u32 %v63374, 127 (stack68)
        %v63376 = vstv %s63373 (stack69)
        %v63377 = vadd.s32 %v63375, %v63376 (stack70)
        %v63381 = vadd.s32 %v63371, %v63377 (stack65)
        %vm63385 = vcmp.lt.u32.totalorder %v63381, %v63371 (stack71)
        %vm63390 = vcmp.lt.u32.totalorder %v63371, %v3816 (stack71)
        %v63395 = vadd.s32 %v3803, %v60124 (stack65)
        %v63399 = vadd.s32 %v63395, 1 (stack65)
        %v63403 = vsel /*vm=*/%vm63390, /*on_true_vy=*/%v63399, /*on_false_vx=*/%v63395 (stack72)
        %v63407 = vadd.s32 %v63403, 1 (stack65)
        %v63411 = vsel /*vm=*/%vm63385, /*on_true_vy=*/%v63407, /*on_false_vx=*/%v63403 (stack72)
        %v63416 = vadd.s32 %v63411, %v10 (stack65)
        %v63420 = vadd.s32 %v63381, %v9 (stack65)
        %v63424 = vadd.s32 %v63416, %v63420 (stack65)
        %v63426 = vshll.u32 %v63420, 13 (stack73)
        %v63427 = vshrl.u32 %v63420, 19 (stack74)
        %v63428 = vor.u32 %v63426, %v63427 (stack75)
        %v63429 = vxor.u32 %v63424, %v63428 (stack76)
        %v63432 = vadd.s32 %v63424, %v63429 (stack65)
        %v63434 = vshll.u32 %v63429, 15 (stack73)
        %v63435 = vshrl.u32 %v63429, 17 (stack74)
        %v63436 = vor.u32 %v63434, %v63435 (stack75)
        %v63437 = vxor.u32 %v63432, %v63436 (stack76)
        %v63440 = vadd.s32 %v63432, %v63437 (stack65)
        %v63442 = vshll.u32 %v63437, 26 (stack73)
        %v63443 = vshrl.u32 %v63437, 6 (stack74)
        %v63444 = vor.u32 %v63442, %v63443 (stack75)
        %v63445 = vxor.u32 %v63440, %v63444 (stack76)
        %v63448 = vadd.s32 %v63440, %v63445 (stack65)
        %v63452 = vadd.s32 %v63448, %v9 (stack65)
        %v63454 = vshll.u32 %v63445, 6 (stack73)
        %v63455 = vshrl.u32 %v63445, 26 (stack74)
        %v63456 = vor.u32 %v63454, %v63455 (stack75)
        %v63457 = vxor.u32 %v63448, %v63456 (stack76)
        %v63460 = vadd.s32 %v63457, %v8 (stack65)
        %v63464 = vadd.s32 %v63460, 1 (stack65)
        %v63468 = vadd.s32 %v63452, %v63464 (stack65)
        %v63470 = vshll.u32 %v63464, 17 (stack73)
        %v63471 = vshrl.u32 %v63464, 15 (stack74)
        %v63472 = vor.u32 %v63470, %v63471 (stack75)
        %v63473 = vxor.u32 %v63468, %v63472 (stack76)
        %v63476 = vadd.s32 %v63468, %v63473 (stack65)
        %v63478 = vshll.u32 %v63473, 29 (stack73)
        %v63479 = vshrl.u32 %v63473, 3 (stack74)
        %v63480 = vor.u32 %v63478, %v63479 (stack75)
        %v63481 = vxor.u32 %v63476, %v63480 (stack76)
        %v63484 = vadd.s32 %v63476, %v63481 (stack65)
        %v63486 = vshll.u32 %v63481, 16 (stack73)
        %v63487 = vshrl.u32 %v63481, 16 (stack74)
        %v63488 = vor.u32 %v63486, %v63487 (stack75)
        %v63489 = vxor.u32 %v63484, %v63488 (stack76)
        %v63492 = vadd.s32 %v63484, %v63489 (stack65)
        %v63496 = vadd.s32 %v63492, %v8 (stack65)
        %v63498 = vshll.u32 %v63489, 24 (stack73)
        %v63499 = vshrl.u32 %v63489, 8 (stack74)
        %v63500 = vor.u32 %v63498, %v63499 (stack75)
        %v63501 = vxor.u32 %v63492, %v63500 (stack76)
        %v63504 = vadd.s32 %v63501, %v10 (stack65)
        %v63508 = vadd.s32 %v63504, 2 (stack65)
        %v63512 = vadd.s32 %v63496, %v63508 (stack65)
        %v63514 = vshll.u32 %v63508, 13 (stack73)
        %v63515 = vshrl.u32 %v63508, 19 (stack74)
        %v63516 = vor.u32 %v63514, %v63515 (stack75)
        %v63517 = vxor.u32 %v63512, %v63516 (stack76)
        %v63520 = vadd.s32 %v63512, %v63517 (stack65)
        %v63522 = vshll.u32 %v63517, 15 (stack73)
        %v63523 = vshrl.u32 %v63517, 17 (stack74)
        %v63524 = vor.u32 %v63522, %v63523 (stack75)
        %v63525 = vxor.u32 %v63520, %v63524 (stack76)
        %v63528 = vadd.s32 %v63520, %v63525 (stack65)
        %v63530 = vshll.u32 %v63525, 26 (stack73)
        %v63531 = vshrl.u32 %v63525, 6 (stack74)
        %v63532 = vor.u32 %v63530, %v63531 (stack75)
        %v63533 = vxor.u32 %v63528, %v63532 (stack76)
        %v63536 = vadd.s32 %v63528, %v63533 (stack65)
        %v63540 = vadd.s32 %v63536, %v10 (stack65)
        %v63542 = vshll.u32 %v63533, 6 (stack73)
        %v63543 = vshrl.u32 %v63533, 26 (stack74)
        %v63544 = vor.u32 %v63542, %v63543 (stack75)
        %v63545 = vxor.u32 %v63536, %v63544 (stack76)
        %v63548 = vadd.s32 %v63545, %v9 (stack65)
        %v63552 = vadd.s32 %v63548, 3 (stack65)
        %v63556 = vadd.s32 %v63540, %v63552 (stack65)
        %v63558 = vshll.u32 %v63552, 17 (stack73)
        %v63559 = vshrl.u32 %v63552, 15 (stack74)
        %v63560 = vor.u32 %v63558, %v63559 (stack75)
        %v63561 = vxor.u32 %v63556, %v63560 (stack76)
        %v63564 = vadd.s32 %v63556, %v63561 (stack65)
        %v63566 = vshll.u32 %v63561, 29 (stack73)
        %v63567 = vshrl.u32 %v63561, 3 (stack74)
        %v63568 = vor.u32 %v63566, %v63567 (stack75)
        %v63569 = vxor.u32 %v63564, %v63568 (stack76)
        %v63572 = vadd.s32 %v63564, %v63569 (stack65)
        %v63574 = vshll.u32 %v63569, 16 (stack73)
        %v63575 = vshrl.u32 %v63569, 16 (stack74)
        %v63576 = vor.u32 %v63574, %v63575 (stack75)
        %v63577 = vxor.u32 %v63572, %v63576 (stack76)
        %v63580 = vadd.s32 %v63572, %v63577 (stack65)
        %v63584 = vadd.s32 %v63580, %v9 (stack65)
        %v63586 = vshll.u32 %v63577, 24 (stack73)
        %v63587 = vshrl.u32 %v63577, 8 (stack74)
        %v63588 = vor.u32 %v63586, %v63587 (stack75)
        %v63589 = vxor.u32 %v63580, %v63588 (stack76)
        %v63592 = vadd.s32 %v63589, %v8 (stack65)
        %v63596 = vadd.s32 %v63592, 4 (stack65)
        %v63600 = vadd.s32 %v63584, %v63596 (stack65)
        %v63602 = vshll.u32 %v63596, 13 (stack73)
        %v63603 = vshrl.u32 %v63596, 19 (stack74)
        %v63604 = vor.u32 %v63602, %v63603 (stack75)
        %v63605 = vxor.u32 %v63600, %v63604 (stack76)
        %v63608 = vadd.s32 %v63600, %v63605 (stack65)
        %v63610 = vshll.u32 %v63605, 15 (stack73)
        %v63611 = vshrl.u32 %v63605, 17 (stack74)
        %v63612 = vor.u32 %v63610, %v63611 (stack75)
        %v63613 = vxor.u32 %v63608, %v63612 (stack76)
        %v63616 = vadd.s32 %v63608, %v63613 (stack65)
        %v63618 = vshll.u32 %v63613, 26 (stack73)
        %v63619 = vshrl.u32 %v63613, 6 (stack74)
        %v63620 = vor.u32 %v63618, %v63619 (stack75)
        %v63621 = vxor.u32 %v63616, %v63620 (stack76)
        %v63624 = vadd.s32 %v63616, %v63621 (stack65)
        %v63628 = vadd.s32 %v63624, %v8 (stack65)
        %v63630 = vshll.u32 %v63621, 6 (stack73)
        %v63631 = vshrl.u32 %v63621, 26 (stack74)
        %v63632 = vor.u32 %v63630, %v63631 (stack75)
        %v63633 = vxor.u32 %v63624, %v63632 (stack76)
        %v63636 = vadd.s32 %v63633, %v10 (stack65)
        %v63640 = vadd.s32 %v63636, 5 (stack65)
        %v63642 = vxor.u32 %v63628, %v63640 (stack76)
        %v63643 = vand.u32.u8 %v63642, 255 (stack77)
        %v63644 = vand.u32 %v63643, 65535 (stack78)
        %v63645 = vshrl.u32 %v63644, 1 (stack79)
        %v63646 = vor.u32 %v63645, 16256 (stack75)
        %v63647 = vand.u32.u16 %v63646, 65535 (stack80)
        %v63648 = vunpack.i.l.bf16 %v63647 (stack81)
        %v63652 = vadd.f32 %v63648, -1.0 (stack82)
        %v63656 = vmul.f32 %v63652, 2.0 (stack83)
        %v63660 = vadd.f32 %v63656, -0.99609375 (stack82)
        %v63664 = vmax.f32 -0.99609375, %v63660 (stack84)
        %v63666 = vand.u32 2147483647, %v63664 (stack85)
        %vm63669 = vcmp.eq.f32.partialorder %v63666, 1.0 (stack86)
        %v63674 = vmul.f32 %v63664, inf (stack83)
        %v63676 = vxor.u32 %v63664, 2147483648 (stack87)
        %v63679 = vmul.f32 %v63664, %v63676 (stack83)
        %v63681 = vadd.f32 %v63679, 1.0 (stack88)
        %v63682 = vlog2.pop %v63681 (stack89)
        %v63683 = vmul.f32 %v63682, 0.6931472 (stack90)
        %v63684 = vmul.f32 -0.5, %v63679 (stack91)
        %v63685 = vadd.f32 %v63684, 1.0 (stack92)
        %v63686 = vmul.f32 %v63685, %v63679 (stack93)
        %v63687 = vand.u32 2147483647, %v63679 (stack94)
        %vm63688 = vcmp.lt.f32.partialorder %v63687, 0.0004427343 (stack95)
        %v63689 = vsel /*vm=*/%vm63688, /*on_true_vy=*/%v63686, /*on_false_vx=*/%v63683 (stack96)
        %v63690 = vxor.u32 %v63689, 2147483648 (stack87)
        %vm63693 = vcmp.lt.f32.partialorder %v63690, 5.0 (stack86)
        %v63698 = vsel /*vm=*/%vm63693, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v63702 = vsel /*vm=*/%vm63693, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v63706 = vsel /*vm=*/%vm63693, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v63710 = vsel /*vm=*/%vm63693, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v63714 = vsel /*vm=*/%vm63693, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v63718 = vsel /*vm=*/%vm63693, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v63722 = vsel /*vm=*/%vm63693, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v63726 = vsel /*vm=*/%vm63693, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v63730 = vsel /*vm=*/%vm63693, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v63734 = vadd.f32 %v63690, -2.5 (stack82)
        %v63736 = vrsqrt.pop %v63690 (stack97)
        %v63737 = vmul.f32 %v63690, %v63736 (stack98)
        %vm63738 = vcmp.eq.f32.partialorder %v63690, inf (stack99)
        %v63739 = vsel /*vm=*/%vm63738, /*on_true_vy=*/%v63690, /*on_false_vx=*/%v63737 (stack100)
        %vm63740 = vcmp.eq.f32.partialorder %v63690, 0.0 (stack101)
        %v63741 = vand.u32 %v63690, 2147483648 (stack102)
        %v63742 = vsel /*vm=*/%vm63740, /*on_true_vy=*/%v63741, /*on_false_vx=*/%v63739 (stack103)
        %v63745 = vadd.f32 %v63742, -3.0 (stack82)
        %v63749 = vsel /*vm=*/%vm63693, /*on_true_vy=*/%v63734, /*on_false_vx=*/%v63745 (stack72)
        %v63753 = vmul.f32 %v63730, %v63749 (stack83)
        %v63757 = vadd.f32 %v63726, %v63753 (stack82)
        %v63761 = vmul.f32 %v63757, %v63749 (stack83)
        %v63765 = vadd.f32 %v63722, %v63761 (stack82)
        %v63769 = vmul.f32 %v63765, %v63749 (stack83)
        %v63773 = vadd.f32 %v63718, %v63769 (stack82)
        %v63777 = vmul.f32 %v63773, %v63749 (stack83)
        %v63781 = vadd.f32 %v63714, %v63777 (stack82)
        %v63785 = vmul.f32 %v63781, %v63749 (stack83)
        %v63789 = vadd.f32 %v63710, %v63785 (stack82)
        %v63793 = vmul.f32 %v63789, %v63749 (stack83)
        %v63797 = vadd.f32 %v63706, %v63793 (stack82)
        %v63801 = vmul.f32 %v63797, %v63749 (stack83)
        %v63805 = vadd.f32 %v63702, %v63801 (stack82)
        %v63809 = vmul.f32 %v63805, %v63749 (stack83)
        %v63813 = vadd.f32 %v63698, %v63809 (stack82)
        %v63817 = vmul.f32 %v63813, %v63664 (stack83)
        %v63821 = vsel /*vm=*/%vm63669, /*on_true_vy=*/%v63674, /*on_false_vx=*/%v63817 (stack72)
        %v63825 = vmul.f32 %v63821, 1.4140625 (stack83)
        %s63827 = scalar_lea.vmem %s280, 960 [#allocation0] (stack107)
        %v63828 = vpack.c.bf16 0.0, %v63825 (stack104)
        %63829 = vst [vmem:[%s63827] sm:$0xf] /*vst_source=*/%v63828 (stack105)
        %s63830 = sadd.s32 %s339, 136 (stack106)
        %s63831 = sshrl.u32 %s63830, 10 (stack49)
        %p63832 = scmp.lt.s32.totalorder 1, %s63831 (stack50)
        %s63833 = scalar_select /*predicate=*/%p63832, /*on_true=*/1, /*on_false=*/%s63831 (stack51)
        %s63834 = sand.u32 %s63830, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s63835 = sshrl.u32 %s63834, 7 (stack53)
        %s63836 = sand.u32 %s63834, 127 /* smod.u32 w/div 128 */ (stack54)
        %s63837 = smul.addr %s63833, 8 (stack55)
        %s63838 = scalar_lea.vmem %s3, %s63837 (stack56)
        %s63840 = scalar_lea.vmem %s63838, %s63835 (stack57)
        %v63841 = vld [vmem:[%s63840] ss:$0 sm:$0xff] (stack58)
        %s63842 = sand.u32 %s63836, 255 (stack59)
        %s63844 = sor.u32 256, %s63842 (stack60)
        %63845 = vbcast.lane.b32.xlu0 %v63841, %s63844 (stack61)
        %v63846 = vpop.permute.xlu0 %63845 (stack62)
        %s63847 = sadd.s32 %s347, 136 (stack106)
        %s63848 = sshrl.u32 %s63847, 10 (stack49)
        %p63849 = scmp.lt.s32.totalorder 1, %s63848 (stack50)
        %s63850 = scalar_select /*predicate=*/%p63849, /*on_true=*/1, /*on_false=*/%s63848 (stack51)
        %s63851 = sand.u32 %s63847, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s63852 = sshrl.u32 %s63851, 7 (stack53)
        %s63853 = sand.u32 %s63851, 127 /* smod.u32 w/div 128 */ (stack54)
        %s63854 = smul.addr %s63850, 8 (stack55)
        %s63855 = scalar_lea.vmem %s5, %s63854 (stack56)
        %s63857 = scalar_lea.vmem %s63855, %s63852 (stack57)
        %v63858 = vld [vmem:[%s63857] ss:$0 sm:$0xff] (stack58)
        %s63859 = sand.u32 %s63853, 255 (stack59)
        %s63861 = sor.u32 256, %s63859 (stack60)
        %63862 = vbcast.lane.b32.xlu0 %v63858, %s63861 (stack61)
        %v63863 = vpop.permute.xlu0 %63862 (stack62)
        %v63866 = vadd.s32 %v408, %v63863 (stack65)
        %s63868 = smul.u32 128, %s27 (stack66)
        %v63869 = vlaneseq (stack67)
        %v63870 = vand.u32 %v63869, 127 (stack68)
        %v63871 = vstv %s63868 (stack69)
        %v63872 = vadd.s32 %v63870, %v63871 (stack70)
        %v63876 = vadd.s32 %v63866, %v63872 (stack65)
        %vm63880 = vcmp.lt.u32.totalorder %v63876, %v63866 (stack71)
        %vm63885 = vcmp.lt.u32.totalorder %v63866, %v408 (stack71)
        %v63890 = vadd.s32 %v380, %v63846 (stack65)
        %v63894 = vadd.s32 %v63890, 1 (stack65)
        %v63898 = vsel /*vm=*/%vm63885, /*on_true_vy=*/%v63894, /*on_false_vx=*/%v63890 (stack72)
        %v63902 = vadd.s32 %v63898, 1 (stack65)
        %v63906 = vsel /*vm=*/%vm63880, /*on_true_vy=*/%v63902, /*on_false_vx=*/%v63898 (stack72)
        %v63911 = vadd.s32 %v63906, %v10 (stack65)
        %v63915 = vadd.s32 %v63876, %v9 (stack65)
        %v63919 = vadd.s32 %v63911, %v63915 (stack65)
        %v63921 = vshll.u32 %v63915, 13 (stack73)
        %v63922 = vshrl.u32 %v63915, 19 (stack74)
        %v63923 = vor.u32 %v63921, %v63922 (stack75)
        %v63924 = vxor.u32 %v63919, %v63923 (stack76)
        %v63927 = vadd.s32 %v63919, %v63924 (stack65)
        %v63929 = vshll.u32 %v63924, 15 (stack73)
        %v63930 = vshrl.u32 %v63924, 17 (stack74)
        %v63931 = vor.u32 %v63929, %v63930 (stack75)
        %v63932 = vxor.u32 %v63927, %v63931 (stack76)
        %v63935 = vadd.s32 %v63927, %v63932 (stack65)
        %v63937 = vshll.u32 %v63932, 26 (stack73)
        %v63938 = vshrl.u32 %v63932, 6 (stack74)
        %v63939 = vor.u32 %v63937, %v63938 (stack75)
        %v63940 = vxor.u32 %v63935, %v63939 (stack76)
        %v63943 = vadd.s32 %v63935, %v63940 (stack65)
        %v63947 = vadd.s32 %v63943, %v9 (stack65)
        %v63949 = vshll.u32 %v63940, 6 (stack73)
        %v63950 = vshrl.u32 %v63940, 26 (stack74)
        %v63951 = vor.u32 %v63949, %v63950 (stack75)
        %v63952 = vxor.u32 %v63943, %v63951 (stack76)
        %v63955 = vadd.s32 %v63952, %v8 (stack65)
        %v63959 = vadd.s32 %v63955, 1 (stack65)
        %v63963 = vadd.s32 %v63947, %v63959 (stack65)
        %v63965 = vshll.u32 %v63959, 17 (stack73)
        %v63966 = vshrl.u32 %v63959, 15 (stack74)
        %v63967 = vor.u32 %v63965, %v63966 (stack75)
        %v63968 = vxor.u32 %v63963, %v63967 (stack76)
        %v63971 = vadd.s32 %v63963, %v63968 (stack65)
        %v63973 = vshll.u32 %v63968, 29 (stack73)
        %v63974 = vshrl.u32 %v63968, 3 (stack74)
        %v63975 = vor.u32 %v63973, %v63974 (stack75)
        %v63976 = vxor.u32 %v63971, %v63975 (stack76)
        %v63979 = vadd.s32 %v63971, %v63976 (stack65)
        %v63981 = vshll.u32 %v63976, 16 (stack73)
        %v63982 = vshrl.u32 %v63976, 16 (stack74)
        %v63983 = vor.u32 %v63981, %v63982 (stack75)
        %v63984 = vxor.u32 %v63979, %v63983 (stack76)
        %v63987 = vadd.s32 %v63979, %v63984 (stack65)
        %v63991 = vadd.s32 %v63987, %v8 (stack65)
        %v63993 = vshll.u32 %v63984, 24 (stack73)
        %v63994 = vshrl.u32 %v63984, 8 (stack74)
        %v63995 = vor.u32 %v63993, %v63994 (stack75)
        %v63996 = vxor.u32 %v63987, %v63995 (stack76)
        %v63999 = vadd.s32 %v63996, %v10 (stack65)
        %v64003 = vadd.s32 %v63999, 2 (stack65)
        %v64007 = vadd.s32 %v63991, %v64003 (stack65)
        %v64009 = vshll.u32 %v64003, 13 (stack73)
        %v64010 = vshrl.u32 %v64003, 19 (stack74)
        %v64011 = vor.u32 %v64009, %v64010 (stack75)
        %v64012 = vxor.u32 %v64007, %v64011 (stack76)
        %v64015 = vadd.s32 %v64007, %v64012 (stack65)
        %v64017 = vshll.u32 %v64012, 15 (stack73)
        %v64018 = vshrl.u32 %v64012, 17 (stack74)
        %v64019 = vor.u32 %v64017, %v64018 (stack75)
        %v64020 = vxor.u32 %v64015, %v64019 (stack76)
        %v64023 = vadd.s32 %v64015, %v64020 (stack65)
        %v64025 = vshll.u32 %v64020, 26 (stack73)
        %v64026 = vshrl.u32 %v64020, 6 (stack74)
        %v64027 = vor.u32 %v64025, %v64026 (stack75)
        %v64028 = vxor.u32 %v64023, %v64027 (stack76)
        %v64031 = vadd.s32 %v64023, %v64028 (stack65)
        %v64035 = vadd.s32 %v64031, %v10 (stack65)
        %v64037 = vshll.u32 %v64028, 6 (stack73)
        %v64038 = vshrl.u32 %v64028, 26 (stack74)
        %v64039 = vor.u32 %v64037, %v64038 (stack75)
        %v64040 = vxor.u32 %v64031, %v64039 (stack76)
        %v64043 = vadd.s32 %v64040, %v9 (stack65)
        %v64047 = vadd.s32 %v64043, 3 (stack65)
        %v64051 = vadd.s32 %v64035, %v64047 (stack65)
        %v64053 = vshll.u32 %v64047, 17 (stack73)
        %v64054 = vshrl.u32 %v64047, 15 (stack74)
        %v64055 = vor.u32 %v64053, %v64054 (stack75)
        %v64056 = vxor.u32 %v64051, %v64055 (stack76)
        %v64059 = vadd.s32 %v64051, %v64056 (stack65)
        %v64061 = vshll.u32 %v64056, 29 (stack73)
        %v64062 = vshrl.u32 %v64056, 3 (stack74)
        %v64063 = vor.u32 %v64061, %v64062 (stack75)
        %v64064 = vxor.u32 %v64059, %v64063 (stack76)
        %v64067 = vadd.s32 %v64059, %v64064 (stack65)
        %v64069 = vshll.u32 %v64064, 16 (stack73)
        %v64070 = vshrl.u32 %v64064, 16 (stack74)
        %v64071 = vor.u32 %v64069, %v64070 (stack75)
        %v64072 = vxor.u32 %v64067, %v64071 (stack76)
        %v64075 = vadd.s32 %v64067, %v64072 (stack65)
        %v64079 = vadd.s32 %v64075, %v9 (stack65)
        %v64081 = vshll.u32 %v64072, 24 (stack73)
        %v64082 = vshrl.u32 %v64072, 8 (stack74)
        %v64083 = vor.u32 %v64081, %v64082 (stack75)
        %v64084 = vxor.u32 %v64075, %v64083 (stack76)
        %v64087 = vadd.s32 %v64084, %v8 (stack65)
        %v64091 = vadd.s32 %v64087, 4 (stack65)
        %v64095 = vadd.s32 %v64079, %v64091 (stack65)
        %v64097 = vshll.u32 %v64091, 13 (stack73)
        %v64098 = vshrl.u32 %v64091, 19 (stack74)
        %v64099 = vor.u32 %v64097, %v64098 (stack75)
        %v64100 = vxor.u32 %v64095, %v64099 (stack76)
        %v64103 = vadd.s32 %v64095, %v64100 (stack65)
        %v64105 = vshll.u32 %v64100, 15 (stack73)
        %v64106 = vshrl.u32 %v64100, 17 (stack74)
        %v64107 = vor.u32 %v64105, %v64106 (stack75)
        %v64108 = vxor.u32 %v64103, %v64107 (stack76)
        %v64111 = vadd.s32 %v64103, %v64108 (stack65)
        %v64113 = vshll.u32 %v64108, 26 (stack73)
        %v64114 = vshrl.u32 %v64108, 6 (stack74)
        %v64115 = vor.u32 %v64113, %v64114 (stack75)
        %v64116 = vxor.u32 %v64111, %v64115 (stack76)
        %v64119 = vadd.s32 %v64111, %v64116 (stack65)
        %v64123 = vadd.s32 %v64119, %v8 (stack65)
        %v64125 = vshll.u32 %v64116, 6 (stack73)
        %v64126 = vshrl.u32 %v64116, 26 (stack74)
        %v64127 = vor.u32 %v64125, %v64126 (stack75)
        %v64128 = vxor.u32 %v64119, %v64127 (stack76)
        %v64131 = vadd.s32 %v64128, %v10 (stack65)
        %v64135 = vadd.s32 %v64131, 5 (stack65)
        %v64137 = vxor.u32 %v64123, %v64135 (stack76)
        %v64138 = vand.u32.u8 %v64137, 255 (stack77)
        %v64139 = vand.u32 %v64138, 65535 (stack78)
        %v64140 = vshrl.u32 %v64139, 1 (stack79)
        %v64141 = vor.u32 %v64140, 16256 (stack75)
        %v64142 = vand.u32.u16 %v64141, 65535 (stack80)
        %v64143 = vunpack.i.l.bf16 %v64142 (stack81)
        %v64147 = vadd.f32 %v64143, -1.0 (stack82)
        %v64151 = vmul.f32 %v64147, 2.0 (stack83)
        %v64155 = vadd.f32 %v64151, -0.99609375 (stack82)
        %v64159 = vmax.f32 -0.99609375, %v64155 (stack84)
        %v64161 = vand.u32 2147483647, %v64159 (stack85)
        %vm64164 = vcmp.eq.f32.partialorder %v64161, 1.0 (stack86)
        %v64169 = vmul.f32 %v64159, inf (stack83)
        %v64171 = vxor.u32 %v64159, 2147483648 (stack87)
        %v64174 = vmul.f32 %v64159, %v64171 (stack83)
        %v64176 = vadd.f32 %v64174, 1.0 (stack88)
        %v64177 = vlog2.pop %v64176 (stack89)
        %v64178 = vmul.f32 %v64177, 0.6931472 (stack90)
        %v64179 = vmul.f32 -0.5, %v64174 (stack91)
        %v64180 = vadd.f32 %v64179, 1.0 (stack92)
        %v64181 = vmul.f32 %v64180, %v64174 (stack93)
        %v64182 = vand.u32 2147483647, %v64174 (stack94)
        %vm64183 = vcmp.lt.f32.partialorder %v64182, 0.0004427343 (stack95)
        %v64184 = vsel /*vm=*/%vm64183, /*on_true_vy=*/%v64181, /*on_false_vx=*/%v64178 (stack96)
        %v64185 = vxor.u32 %v64184, 2147483648 (stack87)
        %vm64188 = vcmp.lt.f32.partialorder %v64185, 5.0 (stack86)
        %v64193 = vsel /*vm=*/%vm64188, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v64197 = vsel /*vm=*/%vm64188, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v64201 = vsel /*vm=*/%vm64188, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v64205 = vsel /*vm=*/%vm64188, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v64209 = vsel /*vm=*/%vm64188, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v64213 = vsel /*vm=*/%vm64188, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v64217 = vsel /*vm=*/%vm64188, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v64221 = vsel /*vm=*/%vm64188, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v64225 = vsel /*vm=*/%vm64188, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v64229 = vadd.f32 %v64185, -2.5 (stack82)
        %v64231 = vrsqrt.pop %v64185 (stack97)
        %v64232 = vmul.f32 %v64185, %v64231 (stack98)
        %vm64233 = vcmp.eq.f32.partialorder %v64185, inf (stack99)
        %v64234 = vsel /*vm=*/%vm64233, /*on_true_vy=*/%v64185, /*on_false_vx=*/%v64232 (stack100)
        %vm64235 = vcmp.eq.f32.partialorder %v64185, 0.0 (stack101)
        %v64236 = vand.u32 %v64185, 2147483648 (stack102)
        %v64237 = vsel /*vm=*/%vm64235, /*on_true_vy=*/%v64236, /*on_false_vx=*/%v64234 (stack103)
        %v64240 = vadd.f32 %v64237, -3.0 (stack82)
        %v64244 = vsel /*vm=*/%vm64188, /*on_true_vy=*/%v64229, /*on_false_vx=*/%v64240 (stack72)
        %v64248 = vmul.f32 %v64225, %v64244 (stack83)
        %v64252 = vadd.f32 %v64221, %v64248 (stack82)
        %v64256 = vmul.f32 %v64252, %v64244 (stack83)
        %v64260 = vadd.f32 %v64217, %v64256 (stack82)
        %v64264 = vmul.f32 %v64260, %v64244 (stack83)
        %v64268 = vadd.f32 %v64213, %v64264 (stack82)
        %v64272 = vmul.f32 %v64268, %v64244 (stack83)
        %v64276 = vadd.f32 %v64209, %v64272 (stack82)
        %v64280 = vmul.f32 %v64276, %v64244 (stack83)
        %v64284 = vadd.f32 %v64205, %v64280 (stack82)
        %v64288 = vmul.f32 %v64284, %v64244 (stack83)
        %v64292 = vadd.f32 %v64201, %v64288 (stack82)
        %v64296 = vmul.f32 %v64292, %v64244 (stack83)
        %v64300 = vadd.f32 %v64197, %v64296 (stack82)
        %v64304 = vmul.f32 %v64300, %v64244 (stack83)
        %v64308 = vadd.f32 %v64193, %v64304 (stack82)
        %v64312 = vmul.f32 %v64308, %v64159 (stack83)
        %v64316 = vsel /*vm=*/%vm64164, /*on_true_vy=*/%v64169, /*on_false_vx=*/%v64312 (stack72)
        %v64320 = vmul.f32 %v64316, 1.4140625 (stack83)
        %s64322 = scalar_lea.vmem %s280, 68 [#allocation0] (stack107)
        %v64323 = vpack.c.bf16 0.0, %v64320 (stack104)
        %64324 = vst [vmem:[%s64322] sm:$0xf] /*vst_source=*/%v64323 (stack105)
        %v64327 = vadd.s32 %v894, %v63863 (stack65)
        %s64329 = smul.u32 128, %s27 (stack66)
        %v64330 = vlaneseq (stack67)
        %v64331 = vand.u32 %v64330, 127 (stack68)
        %v64332 = vstv %s64329 (stack69)
        %v64333 = vadd.s32 %v64331, %v64332 (stack70)
        %v64337 = vadd.s32 %v64327, %v64333 (stack65)
        %vm64341 = vcmp.lt.u32.totalorder %v64337, %v64327 (stack71)
        %vm64346 = vcmp.lt.u32.totalorder %v64327, %v894 (stack71)
        %v64351 = vadd.s32 %v881, %v63846 (stack65)
        %v64355 = vadd.s32 %v64351, 1 (stack65)
        %v64359 = vsel /*vm=*/%vm64346, /*on_true_vy=*/%v64355, /*on_false_vx=*/%v64351 (stack72)
        %v64363 = vadd.s32 %v64359, 1 (stack65)
        %v64367 = vsel /*vm=*/%vm64341, /*on_true_vy=*/%v64363, /*on_false_vx=*/%v64359 (stack72)
        %v64372 = vadd.s32 %v64367, %v10 (stack65)
        %v64376 = vadd.s32 %v64337, %v9 (stack65)
        %v64380 = vadd.s32 %v64372, %v64376 (stack65)
        %v64382 = vshll.u32 %v64376, 13 (stack73)
        %v64383 = vshrl.u32 %v64376, 19 (stack74)
        %v64384 = vor.u32 %v64382, %v64383 (stack75)
        %v64385 = vxor.u32 %v64380, %v64384 (stack76)
        %v64388 = vadd.s32 %v64380, %v64385 (stack65)
        %v64390 = vshll.u32 %v64385, 15 (stack73)
        %v64391 = vshrl.u32 %v64385, 17 (stack74)
        %v64392 = vor.u32 %v64390, %v64391 (stack75)
        %v64393 = vxor.u32 %v64388, %v64392 (stack76)
        %v64396 = vadd.s32 %v64388, %v64393 (stack65)
        %v64398 = vshll.u32 %v64393, 26 (stack73)
        %v64399 = vshrl.u32 %v64393, 6 (stack74)
        %v64400 = vor.u32 %v64398, %v64399 (stack75)
        %v64401 = vxor.u32 %v64396, %v64400 (stack76)
        %v64404 = vadd.s32 %v64396, %v64401 (stack65)
        %v64408 = vadd.s32 %v64404, %v9 (stack65)
        %v64410 = vshll.u32 %v64401, 6 (stack73)
        %v64411 = vshrl.u32 %v64401, 26 (stack74)
        %v64412 = vor.u32 %v64410, %v64411 (stack75)
        %v64413 = vxor.u32 %v64404, %v64412 (stack76)
        %v64416 = vadd.s32 %v64413, %v8 (stack65)
        %v64420 = vadd.s32 %v64416, 1 (stack65)
        %v64424 = vadd.s32 %v64408, %v64420 (stack65)
        %v64426 = vshll.u32 %v64420, 17 (stack73)
        %v64427 = vshrl.u32 %v64420, 15 (stack74)
        %v64428 = vor.u32 %v64426, %v64427 (stack75)
        %v64429 = vxor.u32 %v64424, %v64428 (stack76)
        %v64432 = vadd.s32 %v64424, %v64429 (stack65)
        %v64434 = vshll.u32 %v64429, 29 (stack73)
        %v64435 = vshrl.u32 %v64429, 3 (stack74)
        %v64436 = vor.u32 %v64434, %v64435 (stack75)
        %v64437 = vxor.u32 %v64432, %v64436 (stack76)
        %v64440 = vadd.s32 %v64432, %v64437 (stack65)
        %v64442 = vshll.u32 %v64437, 16 (stack73)
        %v64443 = vshrl.u32 %v64437, 16 (stack74)
        %v64444 = vor.u32 %v64442, %v64443 (stack75)
        %v64445 = vxor.u32 %v64440, %v64444 (stack76)
        %v64448 = vadd.s32 %v64440, %v64445 (stack65)
        %v64452 = vadd.s32 %v64448, %v8 (stack65)
        %v64454 = vshll.u32 %v64445, 24 (stack73)
        %v64455 = vshrl.u32 %v64445, 8 (stack74)
        %v64456 = vor.u32 %v64454, %v64455 (stack75)
        %v64457 = vxor.u32 %v64448, %v64456 (stack76)
        %v64460 = vadd.s32 %v64457, %v10 (stack65)
        %v64464 = vadd.s32 %v64460, 2 (stack65)
        %v64468 = vadd.s32 %v64452, %v64464 (stack65)
        %v64470 = vshll.u32 %v64464, 13 (stack73)
        %v64471 = vshrl.u32 %v64464, 19 (stack74)
        %v64472 = vor.u32 %v64470, %v64471 (stack75)
        %v64473 = vxor.u32 %v64468, %v64472 (stack76)
        %v64476 = vadd.s32 %v64468, %v64473 (stack65)
        %v64478 = vshll.u32 %v64473, 15 (stack73)
        %v64479 = vshrl.u32 %v64473, 17 (stack74)
        %v64480 = vor.u32 %v64478, %v64479 (stack75)
        %v64481 = vxor.u32 %v64476, %v64480 (stack76)
        %v64484 = vadd.s32 %v64476, %v64481 (stack65)
        %v64486 = vshll.u32 %v64481, 26 (stack73)
        %v64487 = vshrl.u32 %v64481, 6 (stack74)
        %v64488 = vor.u32 %v64486, %v64487 (stack75)
        %v64489 = vxor.u32 %v64484, %v64488 (stack76)
        %v64492 = vadd.s32 %v64484, %v64489 (stack65)
        %v64496 = vadd.s32 %v64492, %v10 (stack65)
        %v64498 = vshll.u32 %v64489, 6 (stack73)
        %v64499 = vshrl.u32 %v64489, 26 (stack74)
        %v64500 = vor.u32 %v64498, %v64499 (stack75)
        %v64501 = vxor.u32 %v64492, %v64500 (stack76)
        %v64504 = vadd.s32 %v64501, %v9 (stack65)
        %v64508 = vadd.s32 %v64504, 3 (stack65)
        %v64512 = vadd.s32 %v64496, %v64508 (stack65)
        %v64514 = vshll.u32 %v64508, 17 (stack73)
        %v64515 = vshrl.u32 %v64508, 15 (stack74)
        %v64516 = vor.u32 %v64514, %v64515 (stack75)
        %v64517 = vxor.u32 %v64512, %v64516 (stack76)
        %v64520 = vadd.s32 %v64512, %v64517 (stack65)
        %v64522 = vshll.u32 %v64517, 29 (stack73)
        %v64523 = vshrl.u32 %v64517, 3 (stack74)
        %v64524 = vor.u32 %v64522, %v64523 (stack75)
        %v64525 = vxor.u32 %v64520, %v64524 (stack76)
        %v64528 = vadd.s32 %v64520, %v64525 (stack65)
        %v64530 = vshll.u32 %v64525, 16 (stack73)
        %v64531 = vshrl.u32 %v64525, 16 (stack74)
        %v64532 = vor.u32 %v64530, %v64531 (stack75)
        %v64533 = vxor.u32 %v64528, %v64532 (stack76)
        %v64536 = vadd.s32 %v64528, %v64533 (stack65)
        %v64540 = vadd.s32 %v64536, %v9 (stack65)
        %v64542 = vshll.u32 %v64533, 24 (stack73)
        %v64543 = vshrl.u32 %v64533, 8 (stack74)
        %v64544 = vor.u32 %v64542, %v64543 (stack75)
        %v64545 = vxor.u32 %v64536, %v64544 (stack76)
        %v64548 = vadd.s32 %v64545, %v8 (stack65)
        %v64552 = vadd.s32 %v64548, 4 (stack65)
        %v64556 = vadd.s32 %v64540, %v64552 (stack65)
        %v64558 = vshll.u32 %v64552, 13 (stack73)
        %v64559 = vshrl.u32 %v64552, 19 (stack74)
        %v64560 = vor.u32 %v64558, %v64559 (stack75)
        %v64561 = vxor.u32 %v64556, %v64560 (stack76)
        %v64564 = vadd.s32 %v64556, %v64561 (stack65)
        %v64566 = vshll.u32 %v64561, 15 (stack73)
        %v64567 = vshrl.u32 %v64561, 17 (stack74)
        %v64568 = vor.u32 %v64566, %v64567 (stack75)
        %v64569 = vxor.u32 %v64564, %v64568 (stack76)
        %v64572 = vadd.s32 %v64564, %v64569 (stack65)
        %v64574 = vshll.u32 %v64569, 26 (stack73)
        %v64575 = vshrl.u32 %v64569, 6 (stack74)
        %v64576 = vor.u32 %v64574, %v64575 (stack75)
        %v64577 = vxor.u32 %v64572, %v64576 (stack76)
        %v64580 = vadd.s32 %v64572, %v64577 (stack65)
        %v64584 = vadd.s32 %v64580, %v8 (stack65)
        %v64586 = vshll.u32 %v64577, 6 (stack73)
        %v64587 = vshrl.u32 %v64577, 26 (stack74)
        %v64588 = vor.u32 %v64586, %v64587 (stack75)
        %v64589 = vxor.u32 %v64580, %v64588 (stack76)
        %v64592 = vadd.s32 %v64589, %v10 (stack65)
        %v64596 = vadd.s32 %v64592, 5 (stack65)
        %v64598 = vxor.u32 %v64584, %v64596 (stack76)
        %v64599 = vand.u32.u8 %v64598, 255 (stack77)
        %v64600 = vand.u32 %v64599, 65535 (stack78)
        %v64601 = vshrl.u32 %v64600, 1 (stack79)
        %v64602 = vor.u32 %v64601, 16256 (stack75)
        %v64603 = vand.u32.u16 %v64602, 65535 (stack80)
        %v64604 = vunpack.i.l.bf16 %v64603 (stack81)
        %v64608 = vadd.f32 %v64604, -1.0 (stack82)
        %v64612 = vmul.f32 %v64608, 2.0 (stack83)
        %v64616 = vadd.f32 %v64612, -0.99609375 (stack82)
        %v64620 = vmax.f32 -0.99609375, %v64616 (stack84)
        %v64622 = vand.u32 2147483647, %v64620 (stack85)
        %vm64625 = vcmp.eq.f32.partialorder %v64622, 1.0 (stack86)
        %v64630 = vmul.f32 %v64620, inf (stack83)
        %v64632 = vxor.u32 %v64620, 2147483648 (stack87)
        %v64635 = vmul.f32 %v64620, %v64632 (stack83)
        %v64637 = vadd.f32 %v64635, 1.0 (stack88)
        %v64638 = vlog2.pop %v64637 (stack89)
        %v64639 = vmul.f32 %v64638, 0.6931472 (stack90)
        %v64640 = vmul.f32 -0.5, %v64635 (stack91)
        %v64641 = vadd.f32 %v64640, 1.0 (stack92)
        %v64642 = vmul.f32 %v64641, %v64635 (stack93)
        %v64643 = vand.u32 2147483647, %v64635 (stack94)
        %vm64644 = vcmp.lt.f32.partialorder %v64643, 0.0004427343 (stack95)
        %v64645 = vsel /*vm=*/%vm64644, /*on_true_vy=*/%v64642, /*on_false_vx=*/%v64639 (stack96)
        %v64646 = vxor.u32 %v64645, 2147483648 (stack87)
        %vm64649 = vcmp.lt.f32.partialorder %v64646, 5.0 (stack86)
        %v64654 = vsel /*vm=*/%vm64649, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v64658 = vsel /*vm=*/%vm64649, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v64662 = vsel /*vm=*/%vm64649, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v64666 = vsel /*vm=*/%vm64649, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v64670 = vsel /*vm=*/%vm64649, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v64674 = vsel /*vm=*/%vm64649, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v64678 = vsel /*vm=*/%vm64649, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v64682 = vsel /*vm=*/%vm64649, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v64686 = vsel /*vm=*/%vm64649, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v64690 = vadd.f32 %v64646, -2.5 (stack82)
        %v64692 = vrsqrt.pop %v64646 (stack97)
        %v64693 = vmul.f32 %v64646, %v64692 (stack98)
        %vm64694 = vcmp.eq.f32.partialorder %v64646, inf (stack99)
        %v64695 = vsel /*vm=*/%vm64694, /*on_true_vy=*/%v64646, /*on_false_vx=*/%v64693 (stack100)
        %vm64696 = vcmp.eq.f32.partialorder %v64646, 0.0 (stack101)
        %v64697 = vand.u32 %v64646, 2147483648 (stack102)
        %v64698 = vsel /*vm=*/%vm64696, /*on_true_vy=*/%v64697, /*on_false_vx=*/%v64695 (stack103)
        %v64701 = vadd.f32 %v64698, -3.0 (stack82)
        %v64705 = vsel /*vm=*/%vm64649, /*on_true_vy=*/%v64690, /*on_false_vx=*/%v64701 (stack72)
        %v64709 = vmul.f32 %v64686, %v64705 (stack83)
        %v64713 = vadd.f32 %v64682, %v64709 (stack82)
        %v64717 = vmul.f32 %v64713, %v64705 (stack83)
        %v64721 = vadd.f32 %v64678, %v64717 (stack82)
        %v64725 = vmul.f32 %v64721, %v64705 (stack83)
        %v64729 = vadd.f32 %v64674, %v64725 (stack82)
        %v64733 = vmul.f32 %v64729, %v64705 (stack83)
        %v64737 = vadd.f32 %v64670, %v64733 (stack82)
        %v64741 = vmul.f32 %v64737, %v64705 (stack83)
        %v64745 = vadd.f32 %v64666, %v64741 (stack82)
        %v64749 = vmul.f32 %v64745, %v64705 (stack83)
        %v64753 = vadd.f32 %v64662, %v64749 (stack82)
        %v64757 = vmul.f32 %v64753, %v64705 (stack83)
        %v64761 = vadd.f32 %v64658, %v64757 (stack82)
        %v64765 = vmul.f32 %v64761, %v64705 (stack83)
        %v64769 = vadd.f32 %v64654, %v64765 (stack82)
        %v64773 = vmul.f32 %v64769, %v64620 (stack83)
        %v64777 = vsel /*vm=*/%vm64625, /*on_true_vy=*/%v64630, /*on_false_vx=*/%v64773 (stack72)
        %v64781 = vmul.f32 %v64777, 1.4140625 (stack83)
        %s64783 = scalar_lea.vmem %s280, 196 [#allocation0] (stack107)
        %v64784 = vpack.c.bf16 0.0, %v64781 (stack104)
        %64785 = vst [vmem:[%s64783] sm:$0xf] /*vst_source=*/%v64784 (stack105)
        %v64788 = vadd.s32 %v1381, %v63863 (stack65)
        %s64790 = smul.u32 128, %s27 (stack66)
        %v64791 = vlaneseq (stack67)
        %v64792 = vand.u32 %v64791, 127 (stack68)
        %v64793 = vstv %s64790 (stack69)
        %v64794 = vadd.s32 %v64792, %v64793 (stack70)
        %v64798 = vadd.s32 %v64788, %v64794 (stack65)
        %vm64802 = vcmp.lt.u32.totalorder %v64798, %v64788 (stack71)
        %vm64807 = vcmp.lt.u32.totalorder %v64788, %v1381 (stack71)
        %v64812 = vadd.s32 %v1368, %v63846 (stack65)
        %v64816 = vadd.s32 %v64812, 1 (stack65)
        %v64820 = vsel /*vm=*/%vm64807, /*on_true_vy=*/%v64816, /*on_false_vx=*/%v64812 (stack72)
        %v64824 = vadd.s32 %v64820, 1 (stack65)
        %v64828 = vsel /*vm=*/%vm64802, /*on_true_vy=*/%v64824, /*on_false_vx=*/%v64820 (stack72)
        %v64833 = vadd.s32 %v64828, %v10 (stack65)
        %v64837 = vadd.s32 %v64798, %v9 (stack65)
        %v64841 = vadd.s32 %v64833, %v64837 (stack65)
        %v64843 = vshll.u32 %v64837, 13 (stack73)
        %v64844 = vshrl.u32 %v64837, 19 (stack74)
        %v64845 = vor.u32 %v64843, %v64844 (stack75)
        %v64846 = vxor.u32 %v64841, %v64845 (stack76)
        %v64849 = vadd.s32 %v64841, %v64846 (stack65)
        %v64851 = vshll.u32 %v64846, 15 (stack73)
        %v64852 = vshrl.u32 %v64846, 17 (stack74)
        %v64853 = vor.u32 %v64851, %v64852 (stack75)
        %v64854 = vxor.u32 %v64849, %v64853 (stack76)
        %v64857 = vadd.s32 %v64849, %v64854 (stack65)
        %v64859 = vshll.u32 %v64854, 26 (stack73)
        %v64860 = vshrl.u32 %v64854, 6 (stack74)
        %v64861 = vor.u32 %v64859, %v64860 (stack75)
        %v64862 = vxor.u32 %v64857, %v64861 (stack76)
        %v64865 = vadd.s32 %v64857, %v64862 (stack65)
        %v64869 = vadd.s32 %v64865, %v9 (stack65)
        %v64871 = vshll.u32 %v64862, 6 (stack73)
        %v64872 = vshrl.u32 %v64862, 26 (stack74)
        %v64873 = vor.u32 %v64871, %v64872 (stack75)
        %v64874 = vxor.u32 %v64865, %v64873 (stack76)
        %v64877 = vadd.s32 %v64874, %v8 (stack65)
        %v64881 = vadd.s32 %v64877, 1 (stack65)
        %v64885 = vadd.s32 %v64869, %v64881 (stack65)
        %v64887 = vshll.u32 %v64881, 17 (stack73)
        %v64888 = vshrl.u32 %v64881, 15 (stack74)
        %v64889 = vor.u32 %v64887, %v64888 (stack75)
        %v64890 = vxor.u32 %v64885, %v64889 (stack76)
        %v64893 = vadd.s32 %v64885, %v64890 (stack65)
        %v64895 = vshll.u32 %v64890, 29 (stack73)
        %v64896 = vshrl.u32 %v64890, 3 (stack74)
        %v64897 = vor.u32 %v64895, %v64896 (stack75)
        %v64898 = vxor.u32 %v64893, %v64897 (stack76)
        %v64901 = vadd.s32 %v64893, %v64898 (stack65)
        %v64903 = vshll.u32 %v64898, 16 (stack73)
        %v64904 = vshrl.u32 %v64898, 16 (stack74)
        %v64905 = vor.u32 %v64903, %v64904 (stack75)
        %v64906 = vxor.u32 %v64901, %v64905 (stack76)
        %v64909 = vadd.s32 %v64901, %v64906 (stack65)
        %v64913 = vadd.s32 %v64909, %v8 (stack65)
        %v64915 = vshll.u32 %v64906, 24 (stack73)
        %v64916 = vshrl.u32 %v64906, 8 (stack74)
        %v64917 = vor.u32 %v64915, %v64916 (stack75)
        %v64918 = vxor.u32 %v64909, %v64917 (stack76)
        %v64921 = vadd.s32 %v64918, %v10 (stack65)
        %v64925 = vadd.s32 %v64921, 2 (stack65)
        %v64929 = vadd.s32 %v64913, %v64925 (stack65)
        %v64931 = vshll.u32 %v64925, 13 (stack73)
        %v64932 = vshrl.u32 %v64925, 19 (stack74)
        %v64933 = vor.u32 %v64931, %v64932 (stack75)
        %v64934 = vxor.u32 %v64929, %v64933 (stack76)
        %v64937 = vadd.s32 %v64929, %v64934 (stack65)
        %v64939 = vshll.u32 %v64934, 15 (stack73)
        %v64940 = vshrl.u32 %v64934, 17 (stack74)
        %v64941 = vor.u32 %v64939, %v64940 (stack75)
        %v64942 = vxor.u32 %v64937, %v64941 (stack76)
        %v64945 = vadd.s32 %v64937, %v64942 (stack65)
        %v64947 = vshll.u32 %v64942, 26 (stack73)
        %v64948 = vshrl.u32 %v64942, 6 (stack74)
        %v64949 = vor.u32 %v64947, %v64948 (stack75)
        %v64950 = vxor.u32 %v64945, %v64949 (stack76)
        %v64953 = vadd.s32 %v64945, %v64950 (stack65)
        %v64957 = vadd.s32 %v64953, %v10 (stack65)
        %v64959 = vshll.u32 %v64950, 6 (stack73)
        %v64960 = vshrl.u32 %v64950, 26 (stack74)
        %v64961 = vor.u32 %v64959, %v64960 (stack75)
        %v64962 = vxor.u32 %v64953, %v64961 (stack76)
        %v64965 = vadd.s32 %v64962, %v9 (stack65)
        %v64969 = vadd.s32 %v64965, 3 (stack65)
        %v64973 = vadd.s32 %v64957, %v64969 (stack65)
        %v64975 = vshll.u32 %v64969, 17 (stack73)
        %v64976 = vshrl.u32 %v64969, 15 (stack74)
        %v64977 = vor.u32 %v64975, %v64976 (stack75)
        %v64978 = vxor.u32 %v64973, %v64977 (stack76)
        %v64981 = vadd.s32 %v64973, %v64978 (stack65)
        %v64983 = vshll.u32 %v64978, 29 (stack73)
        %v64984 = vshrl.u32 %v64978, 3 (stack74)
        %v64985 = vor.u32 %v64983, %v64984 (stack75)
        %v64986 = vxor.u32 %v64981, %v64985 (stack76)
        %v64989 = vadd.s32 %v64981, %v64986 (stack65)
        %v64991 = vshll.u32 %v64986, 16 (stack73)
        %v64992 = vshrl.u32 %v64986, 16 (stack74)
        %v64993 = vor.u32 %v64991, %v64992 (stack75)
        %v64994 = vxor.u32 %v64989, %v64993 (stack76)
        %v64997 = vadd.s32 %v64989, %v64994 (stack65)
        %v65001 = vadd.s32 %v64997, %v9 (stack65)
        %v65003 = vshll.u32 %v64994, 24 (stack73)
        %v65004 = vshrl.u32 %v64994, 8 (stack74)
        %v65005 = vor.u32 %v65003, %v65004 (stack75)
        %v65006 = vxor.u32 %v64997, %v65005 (stack76)
        %v65009 = vadd.s32 %v65006, %v8 (stack65)
        %v65013 = vadd.s32 %v65009, 4 (stack65)
        %v65017 = vadd.s32 %v65001, %v65013 (stack65)
        %v65019 = vshll.u32 %v65013, 13 (stack73)
        %v65020 = vshrl.u32 %v65013, 19 (stack74)
        %v65021 = vor.u32 %v65019, %v65020 (stack75)
        %v65022 = vxor.u32 %v65017, %v65021 (stack76)
        %v65025 = vadd.s32 %v65017, %v65022 (stack65)
        %v65027 = vshll.u32 %v65022, 15 (stack73)
        %v65028 = vshrl.u32 %v65022, 17 (stack74)
        %v65029 = vor.u32 %v65027, %v65028 (stack75)
        %v65030 = vxor.u32 %v65025, %v65029 (stack76)
        %v65033 = vadd.s32 %v65025, %v65030 (stack65)
        %v65035 = vshll.u32 %v65030, 26 (stack73)
        %v65036 = vshrl.u32 %v65030, 6 (stack74)
        %v65037 = vor.u32 %v65035, %v65036 (stack75)
        %v65038 = vxor.u32 %v65033, %v65037 (stack76)
        %v65041 = vadd.s32 %v65033, %v65038 (stack65)
        %v65045 = vadd.s32 %v65041, %v8 (stack65)
        %v65047 = vshll.u32 %v65038, 6 (stack73)
        %v65048 = vshrl.u32 %v65038, 26 (stack74)
        %v65049 = vor.u32 %v65047, %v65048 (stack75)
        %v65050 = vxor.u32 %v65041, %v65049 (stack76)
        %v65053 = vadd.s32 %v65050, %v10 (stack65)
        %v65057 = vadd.s32 %v65053, 5 (stack65)
        %v65059 = vxor.u32 %v65045, %v65057 (stack76)
        %v65060 = vand.u32.u8 %v65059, 255 (stack77)
        %v65061 = vand.u32 %v65060, 65535 (stack78)
        %v65062 = vshrl.u32 %v65061, 1 (stack79)
        %v65063 = vor.u32 %v65062, 16256 (stack75)
        %v65064 = vand.u32.u16 %v65063, 65535 (stack80)
        %v65065 = vunpack.i.l.bf16 %v65064 (stack81)
        %v65069 = vadd.f32 %v65065, -1.0 (stack82)
        %v65073 = vmul.f32 %v65069, 2.0 (stack83)
        %v65077 = vadd.f32 %v65073, -0.99609375 (stack82)
        %v65081 = vmax.f32 -0.99609375, %v65077 (stack84)
        %v65083 = vand.u32 2147483647, %v65081 (stack85)
        %vm65086 = vcmp.eq.f32.partialorder %v65083, 1.0 (stack86)
        %v65091 = vmul.f32 %v65081, inf (stack83)
        %v65093 = vxor.u32 %v65081, 2147483648 (stack87)
        %v65096 = vmul.f32 %v65081, %v65093 (stack83)
        %v65098 = vadd.f32 %v65096, 1.0 (stack88)
        %v65099 = vlog2.pop %v65098 (stack89)
        %v65100 = vmul.f32 %v65099, 0.6931472 (stack90)
        %v65101 = vmul.f32 -0.5, %v65096 (stack91)
        %v65102 = vadd.f32 %v65101, 1.0 (stack92)
        %v65103 = vmul.f32 %v65102, %v65096 (stack93)
        %v65104 = vand.u32 2147483647, %v65096 (stack94)
        %vm65105 = vcmp.lt.f32.partialorder %v65104, 0.0004427343 (stack95)
        %v65106 = vsel /*vm=*/%vm65105, /*on_true_vy=*/%v65103, /*on_false_vx=*/%v65100 (stack96)
        %v65107 = vxor.u32 %v65106, 2147483648 (stack87)
        %vm65110 = vcmp.lt.f32.partialorder %v65107, 5.0 (stack86)
        %v65115 = vsel /*vm=*/%vm65110, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v65119 = vsel /*vm=*/%vm65110, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v65123 = vsel /*vm=*/%vm65110, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v65127 = vsel /*vm=*/%vm65110, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v65131 = vsel /*vm=*/%vm65110, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v65135 = vsel /*vm=*/%vm65110, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v65139 = vsel /*vm=*/%vm65110, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v65143 = vsel /*vm=*/%vm65110, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v65147 = vsel /*vm=*/%vm65110, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v65151 = vadd.f32 %v65107, -2.5 (stack82)
        %v65153 = vrsqrt.pop %v65107 (stack97)
        %v65154 = vmul.f32 %v65107, %v65153 (stack98)
        %vm65155 = vcmp.eq.f32.partialorder %v65107, inf (stack99)
        %v65156 = vsel /*vm=*/%vm65155, /*on_true_vy=*/%v65107, /*on_false_vx=*/%v65154 (stack100)
        %vm65157 = vcmp.eq.f32.partialorder %v65107, 0.0 (stack101)
        %v65158 = vand.u32 %v65107, 2147483648 (stack102)
        %v65159 = vsel /*vm=*/%vm65157, /*on_true_vy=*/%v65158, /*on_false_vx=*/%v65156 (stack103)
        %v65162 = vadd.f32 %v65159, -3.0 (stack82)
        %v65166 = vsel /*vm=*/%vm65110, /*on_true_vy=*/%v65151, /*on_false_vx=*/%v65162 (stack72)
        %v65170 = vmul.f32 %v65147, %v65166 (stack83)
        %v65174 = vadd.f32 %v65143, %v65170 (stack82)
        %v65178 = vmul.f32 %v65174, %v65166 (stack83)
        %v65182 = vadd.f32 %v65139, %v65178 (stack82)
        %v65186 = vmul.f32 %v65182, %v65166 (stack83)
        %v65190 = vadd.f32 %v65135, %v65186 (stack82)
        %v65194 = vmul.f32 %v65190, %v65166 (stack83)
        %v65198 = vadd.f32 %v65131, %v65194 (stack82)
        %v65202 = vmul.f32 %v65198, %v65166 (stack83)
        %v65206 = vadd.f32 %v65127, %v65202 (stack82)
        %v65210 = vmul.f32 %v65206, %v65166 (stack83)
        %v65214 = vadd.f32 %v65123, %v65210 (stack82)
        %v65218 = vmul.f32 %v65214, %v65166 (stack83)
        %v65222 = vadd.f32 %v65119, %v65218 (stack82)
        %v65226 = vmul.f32 %v65222, %v65166 (stack83)
        %v65230 = vadd.f32 %v65115, %v65226 (stack82)
        %v65234 = vmul.f32 %v65230, %v65081 (stack83)
        %v65238 = vsel /*vm=*/%vm65086, /*on_true_vy=*/%v65091, /*on_false_vx=*/%v65234 (stack72)
        %v65242 = vmul.f32 %v65238, 1.4140625 (stack83)
        %s65244 = scalar_lea.vmem %s280, 324 [#allocation0] (stack107)
        %v65245 = vpack.c.bf16 0.0, %v65242 (stack104)
        %65246 = vst [vmem:[%s65244] sm:$0xf] /*vst_source=*/%v65245 (stack105)
        %v65249 = vadd.s32 %v1868, %v63863 (stack65)
        %s65251 = smul.u32 128, %s27 (stack66)
        %v65252 = vlaneseq (stack67)
        %v65253 = vand.u32 %v65252, 127 (stack68)
        %v65254 = vstv %s65251 (stack69)
        %v65255 = vadd.s32 %v65253, %v65254 (stack70)
        %v65259 = vadd.s32 %v65249, %v65255 (stack65)
        %vm65263 = vcmp.lt.u32.totalorder %v65259, %v65249 (stack71)
        %vm65268 = vcmp.lt.u32.totalorder %v65249, %v1868 (stack71)
        %v65273 = vadd.s32 %v1855, %v63846 (stack65)
        %v65277 = vadd.s32 %v65273, 1 (stack65)
        %v65281 = vsel /*vm=*/%vm65268, /*on_true_vy=*/%v65277, /*on_false_vx=*/%v65273 (stack72)
        %v65285 = vadd.s32 %v65281, 1 (stack65)
        %v65289 = vsel /*vm=*/%vm65263, /*on_true_vy=*/%v65285, /*on_false_vx=*/%v65281 (stack72)
        %v65294 = vadd.s32 %v65289, %v10 (stack65)
        %v65298 = vadd.s32 %v65259, %v9 (stack65)
        %v65302 = vadd.s32 %v65294, %v65298 (stack65)
        %v65304 = vshll.u32 %v65298, 13 (stack73)
        %v65305 = vshrl.u32 %v65298, 19 (stack74)
        %v65306 = vor.u32 %v65304, %v65305 (stack75)
        %v65307 = vxor.u32 %v65302, %v65306 (stack76)
        %v65310 = vadd.s32 %v65302, %v65307 (stack65)
        %v65312 = vshll.u32 %v65307, 15 (stack73)
        %v65313 = vshrl.u32 %v65307, 17 (stack74)
        %v65314 = vor.u32 %v65312, %v65313 (stack75)
        %v65315 = vxor.u32 %v65310, %v65314 (stack76)
        %v65318 = vadd.s32 %v65310, %v65315 (stack65)
        %v65320 = vshll.u32 %v65315, 26 (stack73)
        %v65321 = vshrl.u32 %v65315, 6 (stack74)
        %v65322 = vor.u32 %v65320, %v65321 (stack75)
        %v65323 = vxor.u32 %v65318, %v65322 (stack76)
        %v65326 = vadd.s32 %v65318, %v65323 (stack65)
        %v65330 = vadd.s32 %v65326, %v9 (stack65)
        %v65332 = vshll.u32 %v65323, 6 (stack73)
        %v65333 = vshrl.u32 %v65323, 26 (stack74)
        %v65334 = vor.u32 %v65332, %v65333 (stack75)
        %v65335 = vxor.u32 %v65326, %v65334 (stack76)
        %v65338 = vadd.s32 %v65335, %v8 (stack65)
        %v65342 = vadd.s32 %v65338, 1 (stack65)
        %v65346 = vadd.s32 %v65330, %v65342 (stack65)
        %v65348 = vshll.u32 %v65342, 17 (stack73)
        %v65349 = vshrl.u32 %v65342, 15 (stack74)
        %v65350 = vor.u32 %v65348, %v65349 (stack75)
        %v65351 = vxor.u32 %v65346, %v65350 (stack76)
        %v65354 = vadd.s32 %v65346, %v65351 (stack65)
        %v65356 = vshll.u32 %v65351, 29 (stack73)
        %v65357 = vshrl.u32 %v65351, 3 (stack74)
        %v65358 = vor.u32 %v65356, %v65357 (stack75)
        %v65359 = vxor.u32 %v65354, %v65358 (stack76)
        %v65362 = vadd.s32 %v65354, %v65359 (stack65)
        %v65364 = vshll.u32 %v65359, 16 (stack73)
        %v65365 = vshrl.u32 %v65359, 16 (stack74)
        %v65366 = vor.u32 %v65364, %v65365 (stack75)
        %v65367 = vxor.u32 %v65362, %v65366 (stack76)
        %v65370 = vadd.s32 %v65362, %v65367 (stack65)
        %v65374 = vadd.s32 %v65370, %v8 (stack65)
        %v65376 = vshll.u32 %v65367, 24 (stack73)
        %v65377 = vshrl.u32 %v65367, 8 (stack74)
        %v65378 = vor.u32 %v65376, %v65377 (stack75)
        %v65379 = vxor.u32 %v65370, %v65378 (stack76)
        %v65382 = vadd.s32 %v65379, %v10 (stack65)
        %v65386 = vadd.s32 %v65382, 2 (stack65)
        %v65390 = vadd.s32 %v65374, %v65386 (stack65)
        %v65392 = vshll.u32 %v65386, 13 (stack73)
        %v65393 = vshrl.u32 %v65386, 19 (stack74)
        %v65394 = vor.u32 %v65392, %v65393 (stack75)
        %v65395 = vxor.u32 %v65390, %v65394 (stack76)
        %v65398 = vadd.s32 %v65390, %v65395 (stack65)
        %v65400 = vshll.u32 %v65395, 15 (stack73)
        %v65401 = vshrl.u32 %v65395, 17 (stack74)
        %v65402 = vor.u32 %v65400, %v65401 (stack75)
        %v65403 = vxor.u32 %v65398, %v65402 (stack76)
        %v65406 = vadd.s32 %v65398, %v65403 (stack65)
        %v65408 = vshll.u32 %v65403, 26 (stack73)
        %v65409 = vshrl.u32 %v65403, 6 (stack74)
        %v65410 = vor.u32 %v65408, %v65409 (stack75)
        %v65411 = vxor.u32 %v65406, %v65410 (stack76)
        %v65414 = vadd.s32 %v65406, %v65411 (stack65)
        %v65418 = vadd.s32 %v65414, %v10 (stack65)
        %v65420 = vshll.u32 %v65411, 6 (stack73)
        %v65421 = vshrl.u32 %v65411, 26 (stack74)
        %v65422 = vor.u32 %v65420, %v65421 (stack75)
        %v65423 = vxor.u32 %v65414, %v65422 (stack76)
        %v65426 = vadd.s32 %v65423, %v9 (stack65)
        %v65430 = vadd.s32 %v65426, 3 (stack65)
        %v65434 = vadd.s32 %v65418, %v65430 (stack65)
        %v65436 = vshll.u32 %v65430, 17 (stack73)
        %v65437 = vshrl.u32 %v65430, 15 (stack74)
        %v65438 = vor.u32 %v65436, %v65437 (stack75)
        %v65439 = vxor.u32 %v65434, %v65438 (stack76)
        %v65442 = vadd.s32 %v65434, %v65439 (stack65)
        %v65444 = vshll.u32 %v65439, 29 (stack73)
        %v65445 = vshrl.u32 %v65439, 3 (stack74)
        %v65446 = vor.u32 %v65444, %v65445 (stack75)
        %v65447 = vxor.u32 %v65442, %v65446 (stack76)
        %v65450 = vadd.s32 %v65442, %v65447 (stack65)
        %v65452 = vshll.u32 %v65447, 16 (stack73)
        %v65453 = vshrl.u32 %v65447, 16 (stack74)
        %v65454 = vor.u32 %v65452, %v65453 (stack75)
        %v65455 = vxor.u32 %v65450, %v65454 (stack76)
        %v65458 = vadd.s32 %v65450, %v65455 (stack65)
        %v65462 = vadd.s32 %v65458, %v9 (stack65)
        %v65464 = vshll.u32 %v65455, 24 (stack73)
        %v65465 = vshrl.u32 %v65455, 8 (stack74)
        %v65466 = vor.u32 %v65464, %v65465 (stack75)
        %v65467 = vxor.u32 %v65458, %v65466 (stack76)
        %v65470 = vadd.s32 %v65467, %v8 (stack65)
        %v65474 = vadd.s32 %v65470, 4 (stack65)
        %v65478 = vadd.s32 %v65462, %v65474 (stack65)
        %v65480 = vshll.u32 %v65474, 13 (stack73)
        %v65481 = vshrl.u32 %v65474, 19 (stack74)
        %v65482 = vor.u32 %v65480, %v65481 (stack75)
        %v65483 = vxor.u32 %v65478, %v65482 (stack76)
        %v65486 = vadd.s32 %v65478, %v65483 (stack65)
        %v65488 = vshll.u32 %v65483, 15 (stack73)
        %v65489 = vshrl.u32 %v65483, 17 (stack74)
        %v65490 = vor.u32 %v65488, %v65489 (stack75)
        %v65491 = vxor.u32 %v65486, %v65490 (stack76)
        %v65494 = vadd.s32 %v65486, %v65491 (stack65)
        %v65496 = vshll.u32 %v65491, 26 (stack73)
        %v65497 = vshrl.u32 %v65491, 6 (stack74)
        %v65498 = vor.u32 %v65496, %v65497 (stack75)
        %v65499 = vxor.u32 %v65494, %v65498 (stack76)
        %v65502 = vadd.s32 %v65494, %v65499 (stack65)
        %v65506 = vadd.s32 %v65502, %v8 (stack65)
        %v65508 = vshll.u32 %v65499, 6 (stack73)
        %v65509 = vshrl.u32 %v65499, 26 (stack74)
        %v65510 = vor.u32 %v65508, %v65509 (stack75)
        %v65511 = vxor.u32 %v65502, %v65510 (stack76)
        %v65514 = vadd.s32 %v65511, %v10 (stack65)
        %v65518 = vadd.s32 %v65514, 5 (stack65)
        %v65520 = vxor.u32 %v65506, %v65518 (stack76)
        %v65521 = vand.u32.u8 %v65520, 255 (stack77)
        %v65522 = vand.u32 %v65521, 65535 (stack78)
        %v65523 = vshrl.u32 %v65522, 1 (stack79)
        %v65524 = vor.u32 %v65523, 16256 (stack75)
        %v65525 = vand.u32.u16 %v65524, 65535 (stack80)
        %v65526 = vunpack.i.l.bf16 %v65525 (stack81)
        %v65530 = vadd.f32 %v65526, -1.0 (stack82)
        %v65534 = vmul.f32 %v65530, 2.0 (stack83)
        %v65538 = vadd.f32 %v65534, -0.99609375 (stack82)
        %v65542 = vmax.f32 -0.99609375, %v65538 (stack84)
        %v65544 = vand.u32 2147483647, %v65542 (stack85)
        %vm65547 = vcmp.eq.f32.partialorder %v65544, 1.0 (stack86)
        %v65552 = vmul.f32 %v65542, inf (stack83)
        %v65554 = vxor.u32 %v65542, 2147483648 (stack87)
        %v65557 = vmul.f32 %v65542, %v65554 (stack83)
        %v65559 = vadd.f32 %v65557, 1.0 (stack88)
        %v65560 = vlog2.pop %v65559 (stack89)
        %v65561 = vmul.f32 %v65560, 0.6931472 (stack90)
        %v65562 = vmul.f32 -0.5, %v65557 (stack91)
        %v65563 = vadd.f32 %v65562, 1.0 (stack92)
        %v65564 = vmul.f32 %v65563, %v65557 (stack93)
        %v65565 = vand.u32 2147483647, %v65557 (stack94)
        %vm65566 = vcmp.lt.f32.partialorder %v65565, 0.0004427343 (stack95)
        %v65567 = vsel /*vm=*/%vm65566, /*on_true_vy=*/%v65564, /*on_false_vx=*/%v65561 (stack96)
        %v65568 = vxor.u32 %v65567, 2147483648 (stack87)
        %vm65571 = vcmp.lt.f32.partialorder %v65568, 5.0 (stack86)
        %v65576 = vsel /*vm=*/%vm65571, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v65580 = vsel /*vm=*/%vm65571, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v65584 = vsel /*vm=*/%vm65571, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v65588 = vsel /*vm=*/%vm65571, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v65592 = vsel /*vm=*/%vm65571, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v65596 = vsel /*vm=*/%vm65571, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v65600 = vsel /*vm=*/%vm65571, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v65604 = vsel /*vm=*/%vm65571, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v65608 = vsel /*vm=*/%vm65571, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v65612 = vadd.f32 %v65568, -2.5 (stack82)
        %v65614 = vrsqrt.pop %v65568 (stack97)
        %v65615 = vmul.f32 %v65568, %v65614 (stack98)
        %vm65616 = vcmp.eq.f32.partialorder %v65568, inf (stack99)
        %v65617 = vsel /*vm=*/%vm65616, /*on_true_vy=*/%v65568, /*on_false_vx=*/%v65615 (stack100)
        %vm65618 = vcmp.eq.f32.partialorder %v65568, 0.0 (stack101)
        %v65619 = vand.u32 %v65568, 2147483648 (stack102)
        %v65620 = vsel /*vm=*/%vm65618, /*on_true_vy=*/%v65619, /*on_false_vx=*/%v65617 (stack103)
        %v65623 = vadd.f32 %v65620, -3.0 (stack82)
        %v65627 = vsel /*vm=*/%vm65571, /*on_true_vy=*/%v65612, /*on_false_vx=*/%v65623 (stack72)
        %v65631 = vmul.f32 %v65608, %v65627 (stack83)
        %v65635 = vadd.f32 %v65604, %v65631 (stack82)
        %v65639 = vmul.f32 %v65635, %v65627 (stack83)
        %v65643 = vadd.f32 %v65600, %v65639 (stack82)
        %v65647 = vmul.f32 %v65643, %v65627 (stack83)
        %v65651 = vadd.f32 %v65596, %v65647 (stack82)
        %v65655 = vmul.f32 %v65651, %v65627 (stack83)
        %v65659 = vadd.f32 %v65592, %v65655 (stack82)
        %v65663 = vmul.f32 %v65659, %v65627 (stack83)
        %v65667 = vadd.f32 %v65588, %v65663 (stack82)
        %v65671 = vmul.f32 %v65667, %v65627 (stack83)
        %v65675 = vadd.f32 %v65584, %v65671 (stack82)
        %v65679 = vmul.f32 %v65675, %v65627 (stack83)
        %v65683 = vadd.f32 %v65580, %v65679 (stack82)
        %v65687 = vmul.f32 %v65683, %v65627 (stack83)
        %v65691 = vadd.f32 %v65576, %v65687 (stack82)
        %v65695 = vmul.f32 %v65691, %v65542 (stack83)
        %v65699 = vsel /*vm=*/%vm65547, /*on_true_vy=*/%v65552, /*on_false_vx=*/%v65695 (stack72)
        %v65703 = vmul.f32 %v65699, 1.4140625 (stack83)
        %s65705 = scalar_lea.vmem %s280, 452 [#allocation0] (stack107)
        %v65706 = vpack.c.bf16 0.0, %v65703 (stack104)
        %65707 = vst [vmem:[%s65705] sm:$0xf] /*vst_source=*/%v65706 (stack105)
        %v65710 = vadd.s32 %v2355, %v63863 (stack65)
        %s65712 = smul.u32 128, %s27 (stack66)
        %v65713 = vlaneseq (stack67)
        %v65714 = vand.u32 %v65713, 127 (stack68)
        %v65715 = vstv %s65712 (stack69)
        %v65716 = vadd.s32 %v65714, %v65715 (stack70)
        %v65720 = vadd.s32 %v65710, %v65716 (stack65)
        %vm65724 = vcmp.lt.u32.totalorder %v65720, %v65710 (stack71)
        %vm65729 = vcmp.lt.u32.totalorder %v65710, %v2355 (stack71)
        %v65734 = vadd.s32 %v2342, %v63846 (stack65)
        %v65738 = vadd.s32 %v65734, 1 (stack65)
        %v65742 = vsel /*vm=*/%vm65729, /*on_true_vy=*/%v65738, /*on_false_vx=*/%v65734 (stack72)
        %v65746 = vadd.s32 %v65742, 1 (stack65)
        %v65750 = vsel /*vm=*/%vm65724, /*on_true_vy=*/%v65746, /*on_false_vx=*/%v65742 (stack72)
        %v65755 = vadd.s32 %v65750, %v10 (stack65)
        %v65759 = vadd.s32 %v65720, %v9 (stack65)
        %v65763 = vadd.s32 %v65755, %v65759 (stack65)
        %v65765 = vshll.u32 %v65759, 13 (stack73)
        %v65766 = vshrl.u32 %v65759, 19 (stack74)
        %v65767 = vor.u32 %v65765, %v65766 (stack75)
        %v65768 = vxor.u32 %v65763, %v65767 (stack76)
        %v65771 = vadd.s32 %v65763, %v65768 (stack65)
        %v65773 = vshll.u32 %v65768, 15 (stack73)
        %v65774 = vshrl.u32 %v65768, 17 (stack74)
        %v65775 = vor.u32 %v65773, %v65774 (stack75)
        %v65776 = vxor.u32 %v65771, %v65775 (stack76)
        %v65779 = vadd.s32 %v65771, %v65776 (stack65)
        %v65781 = vshll.u32 %v65776, 26 (stack73)
        %v65782 = vshrl.u32 %v65776, 6 (stack74)
        %v65783 = vor.u32 %v65781, %v65782 (stack75)
        %v65784 = vxor.u32 %v65779, %v65783 (stack76)
        %v65787 = vadd.s32 %v65779, %v65784 (stack65)
        %v65791 = vadd.s32 %v65787, %v9 (stack65)
        %v65793 = vshll.u32 %v65784, 6 (stack73)
        %v65794 = vshrl.u32 %v65784, 26 (stack74)
        %v65795 = vor.u32 %v65793, %v65794 (stack75)
        %v65796 = vxor.u32 %v65787, %v65795 (stack76)
        %v65799 = vadd.s32 %v65796, %v8 (stack65)
        %v65803 = vadd.s32 %v65799, 1 (stack65)
        %v65807 = vadd.s32 %v65791, %v65803 (stack65)
        %v65809 = vshll.u32 %v65803, 17 (stack73)
        %v65810 = vshrl.u32 %v65803, 15 (stack74)
        %v65811 = vor.u32 %v65809, %v65810 (stack75)
        %v65812 = vxor.u32 %v65807, %v65811 (stack76)
        %v65815 = vadd.s32 %v65807, %v65812 (stack65)
        %v65817 = vshll.u32 %v65812, 29 (stack73)
        %v65818 = vshrl.u32 %v65812, 3 (stack74)
        %v65819 = vor.u32 %v65817, %v65818 (stack75)
        %v65820 = vxor.u32 %v65815, %v65819 (stack76)
        %v65823 = vadd.s32 %v65815, %v65820 (stack65)
        %v65825 = vshll.u32 %v65820, 16 (stack73)
        %v65826 = vshrl.u32 %v65820, 16 (stack74)
        %v65827 = vor.u32 %v65825, %v65826 (stack75)
        %v65828 = vxor.u32 %v65823, %v65827 (stack76)
        %v65831 = vadd.s32 %v65823, %v65828 (stack65)
        %v65835 = vadd.s32 %v65831, %v8 (stack65)
        %v65837 = vshll.u32 %v65828, 24 (stack73)
        %v65838 = vshrl.u32 %v65828, 8 (stack74)
        %v65839 = vor.u32 %v65837, %v65838 (stack75)
        %v65840 = vxor.u32 %v65831, %v65839 (stack76)
        %v65843 = vadd.s32 %v65840, %v10 (stack65)
        %v65847 = vadd.s32 %v65843, 2 (stack65)
        %v65851 = vadd.s32 %v65835, %v65847 (stack65)
        %v65853 = vshll.u32 %v65847, 13 (stack73)
        %v65854 = vshrl.u32 %v65847, 19 (stack74)
        %v65855 = vor.u32 %v65853, %v65854 (stack75)
        %v65856 = vxor.u32 %v65851, %v65855 (stack76)
        %v65859 = vadd.s32 %v65851, %v65856 (stack65)
        %v65861 = vshll.u32 %v65856, 15 (stack73)
        %v65862 = vshrl.u32 %v65856, 17 (stack74)
        %v65863 = vor.u32 %v65861, %v65862 (stack75)
        %v65864 = vxor.u32 %v65859, %v65863 (stack76)
        %v65867 = vadd.s32 %v65859, %v65864 (stack65)
        %v65869 = vshll.u32 %v65864, 26 (stack73)
        %v65870 = vshrl.u32 %v65864, 6 (stack74)
        %v65871 = vor.u32 %v65869, %v65870 (stack75)
        %v65872 = vxor.u32 %v65867, %v65871 (stack76)
        %v65875 = vadd.s32 %v65867, %v65872 (stack65)
        %v65879 = vadd.s32 %v65875, %v10 (stack65)
        %v65881 = vshll.u32 %v65872, 6 (stack73)
        %v65882 = vshrl.u32 %v65872, 26 (stack74)
        %v65883 = vor.u32 %v65881, %v65882 (stack75)
        %v65884 = vxor.u32 %v65875, %v65883 (stack76)
        %v65887 = vadd.s32 %v65884, %v9 (stack65)
        %v65891 = vadd.s32 %v65887, 3 (stack65)
        %v65895 = vadd.s32 %v65879, %v65891 (stack65)
        %v65897 = vshll.u32 %v65891, 17 (stack73)
        %v65898 = vshrl.u32 %v65891, 15 (stack74)
        %v65899 = vor.u32 %v65897, %v65898 (stack75)
        %v65900 = vxor.u32 %v65895, %v65899 (stack76)
        %v65903 = vadd.s32 %v65895, %v65900 (stack65)
        %v65905 = vshll.u32 %v65900, 29 (stack73)
        %v65906 = vshrl.u32 %v65900, 3 (stack74)
        %v65907 = vor.u32 %v65905, %v65906 (stack75)
        %v65908 = vxor.u32 %v65903, %v65907 (stack76)
        %v65911 = vadd.s32 %v65903, %v65908 (stack65)
        %v65913 = vshll.u32 %v65908, 16 (stack73)
        %v65914 = vshrl.u32 %v65908, 16 (stack74)
        %v65915 = vor.u32 %v65913, %v65914 (stack75)
        %v65916 = vxor.u32 %v65911, %v65915 (stack76)
        %v65919 = vadd.s32 %v65911, %v65916 (stack65)
        %v65923 = vadd.s32 %v65919, %v9 (stack65)
        %v65925 = vshll.u32 %v65916, 24 (stack73)
        %v65926 = vshrl.u32 %v65916, 8 (stack74)
        %v65927 = vor.u32 %v65925, %v65926 (stack75)
        %v65928 = vxor.u32 %v65919, %v65927 (stack76)
        %v65931 = vadd.s32 %v65928, %v8 (stack65)
        %v65935 = vadd.s32 %v65931, 4 (stack65)
        %v65939 = vadd.s32 %v65923, %v65935 (stack65)
        %v65941 = vshll.u32 %v65935, 13 (stack73)
        %v65942 = vshrl.u32 %v65935, 19 (stack74)
        %v65943 = vor.u32 %v65941, %v65942 (stack75)
        %v65944 = vxor.u32 %v65939, %v65943 (stack76)
        %v65947 = vadd.s32 %v65939, %v65944 (stack65)
        %v65949 = vshll.u32 %v65944, 15 (stack73)
        %v65950 = vshrl.u32 %v65944, 17 (stack74)
        %v65951 = vor.u32 %v65949, %v65950 (stack75)
        %v65952 = vxor.u32 %v65947, %v65951 (stack76)
        %v65955 = vadd.s32 %v65947, %v65952 (stack65)
        %v65957 = vshll.u32 %v65952, 26 (stack73)
        %v65958 = vshrl.u32 %v65952, 6 (stack74)
        %v65959 = vor.u32 %v65957, %v65958 (stack75)
        %v65960 = vxor.u32 %v65955, %v65959 (stack76)
        %v65963 = vadd.s32 %v65955, %v65960 (stack65)
        %v65967 = vadd.s32 %v65963, %v8 (stack65)
        %v65969 = vshll.u32 %v65960, 6 (stack73)
        %v65970 = vshrl.u32 %v65960, 26 (stack74)
        %v65971 = vor.u32 %v65969, %v65970 (stack75)
        %v65972 = vxor.u32 %v65963, %v65971 (stack76)
        %v65975 = vadd.s32 %v65972, %v10 (stack65)
        %v65979 = vadd.s32 %v65975, 5 (stack65)
        %v65981 = vxor.u32 %v65967, %v65979 (stack76)
        %v65982 = vand.u32.u8 %v65981, 255 (stack77)
        %v65983 = vand.u32 %v65982, 65535 (stack78)
        %v65984 = vshrl.u32 %v65983, 1 (stack79)
        %v65985 = vor.u32 %v65984, 16256 (stack75)
        %v65986 = vand.u32.u16 %v65985, 65535 (stack80)
        %v65987 = vunpack.i.l.bf16 %v65986 (stack81)
        %v65991 = vadd.f32 %v65987, -1.0 (stack82)
        %v65995 = vmul.f32 %v65991, 2.0 (stack83)
        %v65999 = vadd.f32 %v65995, -0.99609375 (stack82)
        %v66003 = vmax.f32 -0.99609375, %v65999 (stack84)
        %v66005 = vand.u32 2147483647, %v66003 (stack85)
        %vm66008 = vcmp.eq.f32.partialorder %v66005, 1.0 (stack86)
        %v66013 = vmul.f32 %v66003, inf (stack83)
        %v66015 = vxor.u32 %v66003, 2147483648 (stack87)
        %v66018 = vmul.f32 %v66003, %v66015 (stack83)
        %v66020 = vadd.f32 %v66018, 1.0 (stack88)
        %v66021 = vlog2.pop %v66020 (stack89)
        %v66022 = vmul.f32 %v66021, 0.6931472 (stack90)
        %v66023 = vmul.f32 -0.5, %v66018 (stack91)
        %v66024 = vadd.f32 %v66023, 1.0 (stack92)
        %v66025 = vmul.f32 %v66024, %v66018 (stack93)
        %v66026 = vand.u32 2147483647, %v66018 (stack94)
        %vm66027 = vcmp.lt.f32.partialorder %v66026, 0.0004427343 (stack95)
        %v66028 = vsel /*vm=*/%vm66027, /*on_true_vy=*/%v66025, /*on_false_vx=*/%v66022 (stack96)
        %v66029 = vxor.u32 %v66028, 2147483648 (stack87)
        %vm66032 = vcmp.lt.f32.partialorder %v66029, 5.0 (stack86)
        %v66037 = vsel /*vm=*/%vm66032, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v66041 = vsel /*vm=*/%vm66032, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v66045 = vsel /*vm=*/%vm66032, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v66049 = vsel /*vm=*/%vm66032, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v66053 = vsel /*vm=*/%vm66032, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v66057 = vsel /*vm=*/%vm66032, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v66061 = vsel /*vm=*/%vm66032, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v66065 = vsel /*vm=*/%vm66032, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v66069 = vsel /*vm=*/%vm66032, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v66073 = vadd.f32 %v66029, -2.5 (stack82)
        %v66075 = vrsqrt.pop %v66029 (stack97)
        %v66076 = vmul.f32 %v66029, %v66075 (stack98)
        %vm66077 = vcmp.eq.f32.partialorder %v66029, inf (stack99)
        %v66078 = vsel /*vm=*/%vm66077, /*on_true_vy=*/%v66029, /*on_false_vx=*/%v66076 (stack100)
        %vm66079 = vcmp.eq.f32.partialorder %v66029, 0.0 (stack101)
        %v66080 = vand.u32 %v66029, 2147483648 (stack102)
        %v66081 = vsel /*vm=*/%vm66079, /*on_true_vy=*/%v66080, /*on_false_vx=*/%v66078 (stack103)
        %v66084 = vadd.f32 %v66081, -3.0 (stack82)
        %v66088 = vsel /*vm=*/%vm66032, /*on_true_vy=*/%v66073, /*on_false_vx=*/%v66084 (stack72)
        %v66092 = vmul.f32 %v66069, %v66088 (stack83)
        %v66096 = vadd.f32 %v66065, %v66092 (stack82)
        %v66100 = vmul.f32 %v66096, %v66088 (stack83)
        %v66104 = vadd.f32 %v66061, %v66100 (stack82)
        %v66108 = vmul.f32 %v66104, %v66088 (stack83)
        %v66112 = vadd.f32 %v66057, %v66108 (stack82)
        %v66116 = vmul.f32 %v66112, %v66088 (stack83)
        %v66120 = vadd.f32 %v66053, %v66116 (stack82)
        %v66124 = vmul.f32 %v66120, %v66088 (stack83)
        %v66128 = vadd.f32 %v66049, %v66124 (stack82)
        %v66132 = vmul.f32 %v66128, %v66088 (stack83)
        %v66136 = vadd.f32 %v66045, %v66132 (stack82)
        %v66140 = vmul.f32 %v66136, %v66088 (stack83)
        %v66144 = vadd.f32 %v66041, %v66140 (stack82)
        %v66148 = vmul.f32 %v66144, %v66088 (stack83)
        %v66152 = vadd.f32 %v66037, %v66148 (stack82)
        %v66156 = vmul.f32 %v66152, %v66003 (stack83)
        %v66160 = vsel /*vm=*/%vm66008, /*on_true_vy=*/%v66013, /*on_false_vx=*/%v66156 (stack72)
        %v66164 = vmul.f32 %v66160, 1.4140625 (stack83)
        %s66166 = scalar_lea.vmem %s280, 580 [#allocation0] (stack107)
        %v66167 = vpack.c.bf16 0.0, %v66164 (stack104)
        %66168 = vst [vmem:[%s66166] sm:$0xf] /*vst_source=*/%v66167 (stack105)
        %v66171 = vadd.s32 %v2842, %v63863 (stack65)
        %s66173 = smul.u32 128, %s27 (stack66)
        %v66174 = vlaneseq (stack67)
        %v66175 = vand.u32 %v66174, 127 (stack68)
        %v66176 = vstv %s66173 (stack69)
        %v66177 = vadd.s32 %v66175, %v66176 (stack70)
        %v66181 = vadd.s32 %v66171, %v66177 (stack65)
        %vm66185 = vcmp.lt.u32.totalorder %v66181, %v66171 (stack71)
        %vm66190 = vcmp.lt.u32.totalorder %v66171, %v2842 (stack71)
        %v66195 = vadd.s32 %v2829, %v63846 (stack65)
        %v66199 = vadd.s32 %v66195, 1 (stack65)
        %v66203 = vsel /*vm=*/%vm66190, /*on_true_vy=*/%v66199, /*on_false_vx=*/%v66195 (stack72)
        %v66207 = vadd.s32 %v66203, 1 (stack65)
        %v66211 = vsel /*vm=*/%vm66185, /*on_true_vy=*/%v66207, /*on_false_vx=*/%v66203 (stack72)
        %v66216 = vadd.s32 %v66211, %v10 (stack65)
        %v66220 = vadd.s32 %v66181, %v9 (stack65)
        %v66224 = vadd.s32 %v66216, %v66220 (stack65)
        %v66226 = vshll.u32 %v66220, 13 (stack73)
        %v66227 = vshrl.u32 %v66220, 19 (stack74)
        %v66228 = vor.u32 %v66226, %v66227 (stack75)
        %v66229 = vxor.u32 %v66224, %v66228 (stack76)
        %v66232 = vadd.s32 %v66224, %v66229 (stack65)
        %v66234 = vshll.u32 %v66229, 15 (stack73)
        %v66235 = vshrl.u32 %v66229, 17 (stack74)
        %v66236 = vor.u32 %v66234, %v66235 (stack75)
        %v66237 = vxor.u32 %v66232, %v66236 (stack76)
        %v66240 = vadd.s32 %v66232, %v66237 (stack65)
        %v66242 = vshll.u32 %v66237, 26 (stack73)
        %v66243 = vshrl.u32 %v66237, 6 (stack74)
        %v66244 = vor.u32 %v66242, %v66243 (stack75)
        %v66245 = vxor.u32 %v66240, %v66244 (stack76)
        %v66248 = vadd.s32 %v66240, %v66245 (stack65)
        %v66252 = vadd.s32 %v66248, %v9 (stack65)
        %v66254 = vshll.u32 %v66245, 6 (stack73)
        %v66255 = vshrl.u32 %v66245, 26 (stack74)
        %v66256 = vor.u32 %v66254, %v66255 (stack75)
        %v66257 = vxor.u32 %v66248, %v66256 (stack76)
        %v66260 = vadd.s32 %v66257, %v8 (stack65)
        %v66264 = vadd.s32 %v66260, 1 (stack65)
        %v66268 = vadd.s32 %v66252, %v66264 (stack65)
        %v66270 = vshll.u32 %v66264, 17 (stack73)
        %v66271 = vshrl.u32 %v66264, 15 (stack74)
        %v66272 = vor.u32 %v66270, %v66271 (stack75)
        %v66273 = vxor.u32 %v66268, %v66272 (stack76)
        %v66276 = vadd.s32 %v66268, %v66273 (stack65)
        %v66278 = vshll.u32 %v66273, 29 (stack73)
        %v66279 = vshrl.u32 %v66273, 3 (stack74)
        %v66280 = vor.u32 %v66278, %v66279 (stack75)
        %v66281 = vxor.u32 %v66276, %v66280 (stack76)
        %v66284 = vadd.s32 %v66276, %v66281 (stack65)
        %v66286 = vshll.u32 %v66281, 16 (stack73)
        %v66287 = vshrl.u32 %v66281, 16 (stack74)
        %v66288 = vor.u32 %v66286, %v66287 (stack75)
        %v66289 = vxor.u32 %v66284, %v66288 (stack76)
        %v66292 = vadd.s32 %v66284, %v66289 (stack65)
        %v66296 = vadd.s32 %v66292, %v8 (stack65)
        %v66298 = vshll.u32 %v66289, 24 (stack73)
        %v66299 = vshrl.u32 %v66289, 8 (stack74)
        %v66300 = vor.u32 %v66298, %v66299 (stack75)
        %v66301 = vxor.u32 %v66292, %v66300 (stack76)
        %v66304 = vadd.s32 %v66301, %v10 (stack65)
        %v66308 = vadd.s32 %v66304, 2 (stack65)
        %v66312 = vadd.s32 %v66296, %v66308 (stack65)
        %v66314 = vshll.u32 %v66308, 13 (stack73)
        %v66315 = vshrl.u32 %v66308, 19 (stack74)
        %v66316 = vor.u32 %v66314, %v66315 (stack75)
        %v66317 = vxor.u32 %v66312, %v66316 (stack76)
        %v66320 = vadd.s32 %v66312, %v66317 (stack65)
        %v66322 = vshll.u32 %v66317, 15 (stack73)
        %v66323 = vshrl.u32 %v66317, 17 (stack74)
        %v66324 = vor.u32 %v66322, %v66323 (stack75)
        %v66325 = vxor.u32 %v66320, %v66324 (stack76)
        %v66328 = vadd.s32 %v66320, %v66325 (stack65)
        %v66330 = vshll.u32 %v66325, 26 (stack73)
        %v66331 = vshrl.u32 %v66325, 6 (stack74)
        %v66332 = vor.u32 %v66330, %v66331 (stack75)
        %v66333 = vxor.u32 %v66328, %v66332 (stack76)
        %v66336 = vadd.s32 %v66328, %v66333 (stack65)
        %v66340 = vadd.s32 %v66336, %v10 (stack65)
        %v66342 = vshll.u32 %v66333, 6 (stack73)
        %v66343 = vshrl.u32 %v66333, 26 (stack74)
        %v66344 = vor.u32 %v66342, %v66343 (stack75)
        %v66345 = vxor.u32 %v66336, %v66344 (stack76)
        %v66348 = vadd.s32 %v66345, %v9 (stack65)
        %v66352 = vadd.s32 %v66348, 3 (stack65)
        %v66356 = vadd.s32 %v66340, %v66352 (stack65)
        %v66358 = vshll.u32 %v66352, 17 (stack73)
        %v66359 = vshrl.u32 %v66352, 15 (stack74)
        %v66360 = vor.u32 %v66358, %v66359 (stack75)
        %v66361 = vxor.u32 %v66356, %v66360 (stack76)
        %v66364 = vadd.s32 %v66356, %v66361 (stack65)
        %v66366 = vshll.u32 %v66361, 29 (stack73)
        %v66367 = vshrl.u32 %v66361, 3 (stack74)
        %v66368 = vor.u32 %v66366, %v66367 (stack75)
        %v66369 = vxor.u32 %v66364, %v66368 (stack76)
        %v66372 = vadd.s32 %v66364, %v66369 (stack65)
        %v66374 = vshll.u32 %v66369, 16 (stack73)
        %v66375 = vshrl.u32 %v66369, 16 (stack74)
        %v66376 = vor.u32 %v66374, %v66375 (stack75)
        %v66377 = vxor.u32 %v66372, %v66376 (stack76)
        %v66380 = vadd.s32 %v66372, %v66377 (stack65)
        %v66384 = vadd.s32 %v66380, %v9 (stack65)
        %v66386 = vshll.u32 %v66377, 24 (stack73)
        %v66387 = vshrl.u32 %v66377, 8 (stack74)
        %v66388 = vor.u32 %v66386, %v66387 (stack75)
        %v66389 = vxor.u32 %v66380, %v66388 (stack76)
        %v66392 = vadd.s32 %v66389, %v8 (stack65)
        %v66396 = vadd.s32 %v66392, 4 (stack65)
        %v66400 = vadd.s32 %v66384, %v66396 (stack65)
        %v66402 = vshll.u32 %v66396, 13 (stack73)
        %v66403 = vshrl.u32 %v66396, 19 (stack74)
        %v66404 = vor.u32 %v66402, %v66403 (stack75)
        %v66405 = vxor.u32 %v66400, %v66404 (stack76)
        %v66408 = vadd.s32 %v66400, %v66405 (stack65)
        %v66410 = vshll.u32 %v66405, 15 (stack73)
        %v66411 = vshrl.u32 %v66405, 17 (stack74)
        %v66412 = vor.u32 %v66410, %v66411 (stack75)
        %v66413 = vxor.u32 %v66408, %v66412 (stack76)
        %v66416 = vadd.s32 %v66408, %v66413 (stack65)
        %v66418 = vshll.u32 %v66413, 26 (stack73)
        %v66419 = vshrl.u32 %v66413, 6 (stack74)
        %v66420 = vor.u32 %v66418, %v66419 (stack75)
        %v66421 = vxor.u32 %v66416, %v66420 (stack76)
        %v66424 = vadd.s32 %v66416, %v66421 (stack65)
        %v66428 = vadd.s32 %v66424, %v8 (stack65)
        %v66430 = vshll.u32 %v66421, 6 (stack73)
        %v66431 = vshrl.u32 %v66421, 26 (stack74)
        %v66432 = vor.u32 %v66430, %v66431 (stack75)
        %v66433 = vxor.u32 %v66424, %v66432 (stack76)
        %v66436 = vadd.s32 %v66433, %v10 (stack65)
        %v66440 = vadd.s32 %v66436, 5 (stack65)
        %v66442 = vxor.u32 %v66428, %v66440 (stack76)
        %v66443 = vand.u32.u8 %v66442, 255 (stack77)
        %v66444 = vand.u32 %v66443, 65535 (stack78)
        %v66445 = vshrl.u32 %v66444, 1 (stack79)
        %v66446 = vor.u32 %v66445, 16256 (stack75)
        %v66447 = vand.u32.u16 %v66446, 65535 (stack80)
        %v66448 = vunpack.i.l.bf16 %v66447 (stack81)
        %v66452 = vadd.f32 %v66448, -1.0 (stack82)
        %v66456 = vmul.f32 %v66452, 2.0 (stack83)
        %v66460 = vadd.f32 %v66456, -0.99609375 (stack82)
        %v66464 = vmax.f32 -0.99609375, %v66460 (stack84)
        %v66466 = vand.u32 2147483647, %v66464 (stack85)
        %vm66469 = vcmp.eq.f32.partialorder %v66466, 1.0 (stack86)
        %v66474 = vmul.f32 %v66464, inf (stack83)
        %v66476 = vxor.u32 %v66464, 2147483648 (stack87)
        %v66479 = vmul.f32 %v66464, %v66476 (stack83)
        %v66481 = vadd.f32 %v66479, 1.0 (stack88)
        %v66482 = vlog2.pop %v66481 (stack89)
        %v66483 = vmul.f32 %v66482, 0.6931472 (stack90)
        %v66484 = vmul.f32 -0.5, %v66479 (stack91)
        %v66485 = vadd.f32 %v66484, 1.0 (stack92)
        %v66486 = vmul.f32 %v66485, %v66479 (stack93)
        %v66487 = vand.u32 2147483647, %v66479 (stack94)
        %vm66488 = vcmp.lt.f32.partialorder %v66487, 0.0004427343 (stack95)
        %v66489 = vsel /*vm=*/%vm66488, /*on_true_vy=*/%v66486, /*on_false_vx=*/%v66483 (stack96)
        %v66490 = vxor.u32 %v66489, 2147483648 (stack87)
        %vm66493 = vcmp.lt.f32.partialorder %v66490, 5.0 (stack86)
        %v66498 = vsel /*vm=*/%vm66493, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v66502 = vsel /*vm=*/%vm66493, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v66506 = vsel /*vm=*/%vm66493, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v66510 = vsel /*vm=*/%vm66493, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v66514 = vsel /*vm=*/%vm66493, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v66518 = vsel /*vm=*/%vm66493, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v66522 = vsel /*vm=*/%vm66493, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v66526 = vsel /*vm=*/%vm66493, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v66530 = vsel /*vm=*/%vm66493, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v66534 = vadd.f32 %v66490, -2.5 (stack82)
        %v66536 = vrsqrt.pop %v66490 (stack97)
        %v66537 = vmul.f32 %v66490, %v66536 (stack98)
        %vm66538 = vcmp.eq.f32.partialorder %v66490, inf (stack99)
        %v66539 = vsel /*vm=*/%vm66538, /*on_true_vy=*/%v66490, /*on_false_vx=*/%v66537 (stack100)
        %vm66540 = vcmp.eq.f32.partialorder %v66490, 0.0 (stack101)
        %v66541 = vand.u32 %v66490, 2147483648 (stack102)
        %v66542 = vsel /*vm=*/%vm66540, /*on_true_vy=*/%v66541, /*on_false_vx=*/%v66539 (stack103)
        %v66545 = vadd.f32 %v66542, -3.0 (stack82)
        %v66549 = vsel /*vm=*/%vm66493, /*on_true_vy=*/%v66534, /*on_false_vx=*/%v66545 (stack72)
        %v66553 = vmul.f32 %v66530, %v66549 (stack83)
        %v66557 = vadd.f32 %v66526, %v66553 (stack82)
        %v66561 = vmul.f32 %v66557, %v66549 (stack83)
        %v66565 = vadd.f32 %v66522, %v66561 (stack82)
        %v66569 = vmul.f32 %v66565, %v66549 (stack83)
        %v66573 = vadd.f32 %v66518, %v66569 (stack82)
        %v66577 = vmul.f32 %v66573, %v66549 (stack83)
        %v66581 = vadd.f32 %v66514, %v66577 (stack82)
        %v66585 = vmul.f32 %v66581, %v66549 (stack83)
        %v66589 = vadd.f32 %v66510, %v66585 (stack82)
        %v66593 = vmul.f32 %v66589, %v66549 (stack83)
        %v66597 = vadd.f32 %v66506, %v66593 (stack82)
        %v66601 = vmul.f32 %v66597, %v66549 (stack83)
        %v66605 = vadd.f32 %v66502, %v66601 (stack82)
        %v66609 = vmul.f32 %v66605, %v66549 (stack83)
        %v66613 = vadd.f32 %v66498, %v66609 (stack82)
        %v66617 = vmul.f32 %v66613, %v66464 (stack83)
        %v66621 = vsel /*vm=*/%vm66469, /*on_true_vy=*/%v66474, /*on_false_vx=*/%v66617 (stack72)
        %v66625 = vmul.f32 %v66621, 1.4140625 (stack83)
        %s66627 = scalar_lea.vmem %s280, 708 [#allocation0] (stack107)
        %v66628 = vpack.c.bf16 0.0, %v66625 (stack104)
        %66629 = vst [vmem:[%s66627] sm:$0xf] /*vst_source=*/%v66628 (stack105)
        %v66632 = vadd.s32 %v3329, %v63863 (stack65)
        %s66634 = smul.u32 128, %s27 (stack66)
        %v66635 = vlaneseq (stack67)
        %v66636 = vand.u32 %v66635, 127 (stack68)
        %v66637 = vstv %s66634 (stack69)
        %v66638 = vadd.s32 %v66636, %v66637 (stack70)
        %v66642 = vadd.s32 %v66632, %v66638 (stack65)
        %vm66646 = vcmp.lt.u32.totalorder %v66642, %v66632 (stack71)
        %vm66651 = vcmp.lt.u32.totalorder %v66632, %v3329 (stack71)
        %v66656 = vadd.s32 %v3316, %v63846 (stack65)
        %v66660 = vadd.s32 %v66656, 1 (stack65)
        %v66664 = vsel /*vm=*/%vm66651, /*on_true_vy=*/%v66660, /*on_false_vx=*/%v66656 (stack72)
        %v66668 = vadd.s32 %v66664, 1 (stack65)
        %v66672 = vsel /*vm=*/%vm66646, /*on_true_vy=*/%v66668, /*on_false_vx=*/%v66664 (stack72)
        %v66677 = vadd.s32 %v66672, %v10 (stack65)
        %v66681 = vadd.s32 %v66642, %v9 (stack65)
        %v66685 = vadd.s32 %v66677, %v66681 (stack65)
        %v66687 = vshll.u32 %v66681, 13 (stack73)
        %v66688 = vshrl.u32 %v66681, 19 (stack74)
        %v66689 = vor.u32 %v66687, %v66688 (stack75)
        %v66690 = vxor.u32 %v66685, %v66689 (stack76)
        %v66693 = vadd.s32 %v66685, %v66690 (stack65)
        %v66695 = vshll.u32 %v66690, 15 (stack73)
        %v66696 = vshrl.u32 %v66690, 17 (stack74)
        %v66697 = vor.u32 %v66695, %v66696 (stack75)
        %v66698 = vxor.u32 %v66693, %v66697 (stack76)
        %v66701 = vadd.s32 %v66693, %v66698 (stack65)
        %v66703 = vshll.u32 %v66698, 26 (stack73)
        %v66704 = vshrl.u32 %v66698, 6 (stack74)
        %v66705 = vor.u32 %v66703, %v66704 (stack75)
        %v66706 = vxor.u32 %v66701, %v66705 (stack76)
        %v66709 = vadd.s32 %v66701, %v66706 (stack65)
        %v66713 = vadd.s32 %v66709, %v9 (stack65)
        %v66715 = vshll.u32 %v66706, 6 (stack73)
        %v66716 = vshrl.u32 %v66706, 26 (stack74)
        %v66717 = vor.u32 %v66715, %v66716 (stack75)
        %v66718 = vxor.u32 %v66709, %v66717 (stack76)
        %v66721 = vadd.s32 %v66718, %v8 (stack65)
        %v66725 = vadd.s32 %v66721, 1 (stack65)
        %v66729 = vadd.s32 %v66713, %v66725 (stack65)
        %v66731 = vshll.u32 %v66725, 17 (stack73)
        %v66732 = vshrl.u32 %v66725, 15 (stack74)
        %v66733 = vor.u32 %v66731, %v66732 (stack75)
        %v66734 = vxor.u32 %v66729, %v66733 (stack76)
        %v66737 = vadd.s32 %v66729, %v66734 (stack65)
        %v66739 = vshll.u32 %v66734, 29 (stack73)
        %v66740 = vshrl.u32 %v66734, 3 (stack74)
        %v66741 = vor.u32 %v66739, %v66740 (stack75)
        %v66742 = vxor.u32 %v66737, %v66741 (stack76)
        %v66745 = vadd.s32 %v66737, %v66742 (stack65)
        %v66747 = vshll.u32 %v66742, 16 (stack73)
        %v66748 = vshrl.u32 %v66742, 16 (stack74)
        %v66749 = vor.u32 %v66747, %v66748 (stack75)
        %v66750 = vxor.u32 %v66745, %v66749 (stack76)
        %v66753 = vadd.s32 %v66745, %v66750 (stack65)
        %v66757 = vadd.s32 %v66753, %v8 (stack65)
        %v66759 = vshll.u32 %v66750, 24 (stack73)
        %v66760 = vshrl.u32 %v66750, 8 (stack74)
        %v66761 = vor.u32 %v66759, %v66760 (stack75)
        %v66762 = vxor.u32 %v66753, %v66761 (stack76)
        %v66765 = vadd.s32 %v66762, %v10 (stack65)
        %v66769 = vadd.s32 %v66765, 2 (stack65)
        %v66773 = vadd.s32 %v66757, %v66769 (stack65)
        %v66775 = vshll.u32 %v66769, 13 (stack73)
        %v66776 = vshrl.u32 %v66769, 19 (stack74)
        %v66777 = vor.u32 %v66775, %v66776 (stack75)
        %v66778 = vxor.u32 %v66773, %v66777 (stack76)
        %v66781 = vadd.s32 %v66773, %v66778 (stack65)
        %v66783 = vshll.u32 %v66778, 15 (stack73)
        %v66784 = vshrl.u32 %v66778, 17 (stack74)
        %v66785 = vor.u32 %v66783, %v66784 (stack75)
        %v66786 = vxor.u32 %v66781, %v66785 (stack76)
        %v66789 = vadd.s32 %v66781, %v66786 (stack65)
        %v66791 = vshll.u32 %v66786, 26 (stack73)
        %v66792 = vshrl.u32 %v66786, 6 (stack74)
        %v66793 = vor.u32 %v66791, %v66792 (stack75)
        %v66794 = vxor.u32 %v66789, %v66793 (stack76)
        %v66797 = vadd.s32 %v66789, %v66794 (stack65)
        %v66801 = vadd.s32 %v66797, %v10 (stack65)
        %v66803 = vshll.u32 %v66794, 6 (stack73)
        %v66804 = vshrl.u32 %v66794, 26 (stack74)
        %v66805 = vor.u32 %v66803, %v66804 (stack75)
        %v66806 = vxor.u32 %v66797, %v66805 (stack76)
        %v66809 = vadd.s32 %v66806, %v9 (stack65)
        %v66813 = vadd.s32 %v66809, 3 (stack65)
        %v66817 = vadd.s32 %v66801, %v66813 (stack65)
        %v66819 = vshll.u32 %v66813, 17 (stack73)
        %v66820 = vshrl.u32 %v66813, 15 (stack74)
        %v66821 = vor.u32 %v66819, %v66820 (stack75)
        %v66822 = vxor.u32 %v66817, %v66821 (stack76)
        %v66825 = vadd.s32 %v66817, %v66822 (stack65)
        %v66827 = vshll.u32 %v66822, 29 (stack73)
        %v66828 = vshrl.u32 %v66822, 3 (stack74)
        %v66829 = vor.u32 %v66827, %v66828 (stack75)
        %v66830 = vxor.u32 %v66825, %v66829 (stack76)
        %v66833 = vadd.s32 %v66825, %v66830 (stack65)
        %v66835 = vshll.u32 %v66830, 16 (stack73)
        %v66836 = vshrl.u32 %v66830, 16 (stack74)
        %v66837 = vor.u32 %v66835, %v66836 (stack75)
        %v66838 = vxor.u32 %v66833, %v66837 (stack76)
        %v66841 = vadd.s32 %v66833, %v66838 (stack65)
        %v66845 = vadd.s32 %v66841, %v9 (stack65)
        %v66847 = vshll.u32 %v66838, 24 (stack73)
        %v66848 = vshrl.u32 %v66838, 8 (stack74)
        %v66849 = vor.u32 %v66847, %v66848 (stack75)
        %v66850 = vxor.u32 %v66841, %v66849 (stack76)
        %v66853 = vadd.s32 %v66850, %v8 (stack65)
        %v66857 = vadd.s32 %v66853, 4 (stack65)
        %v66861 = vadd.s32 %v66845, %v66857 (stack65)
        %v66863 = vshll.u32 %v66857, 13 (stack73)
        %v66864 = vshrl.u32 %v66857, 19 (stack74)
        %v66865 = vor.u32 %v66863, %v66864 (stack75)
        %v66866 = vxor.u32 %v66861, %v66865 (stack76)
        %v66869 = vadd.s32 %v66861, %v66866 (stack65)
        %v66871 = vshll.u32 %v66866, 15 (stack73)
        %v66872 = vshrl.u32 %v66866, 17 (stack74)
        %v66873 = vor.u32 %v66871, %v66872 (stack75)
        %v66874 = vxor.u32 %v66869, %v66873 (stack76)
        %v66877 = vadd.s32 %v66869, %v66874 (stack65)
        %v66879 = vshll.u32 %v66874, 26 (stack73)
        %v66880 = vshrl.u32 %v66874, 6 (stack74)
        %v66881 = vor.u32 %v66879, %v66880 (stack75)
        %v66882 = vxor.u32 %v66877, %v66881 (stack76)
        %v66885 = vadd.s32 %v66877, %v66882 (stack65)
        %v66889 = vadd.s32 %v66885, %v8 (stack65)
        %v66891 = vshll.u32 %v66882, 6 (stack73)
        %v66892 = vshrl.u32 %v66882, 26 (stack74)
        %v66893 = vor.u32 %v66891, %v66892 (stack75)
        %v66894 = vxor.u32 %v66885, %v66893 (stack76)
        %v66897 = vadd.s32 %v66894, %v10 (stack65)
        %v66901 = vadd.s32 %v66897, 5 (stack65)
        %v66903 = vxor.u32 %v66889, %v66901 (stack76)
        %v66904 = vand.u32.u8 %v66903, 255 (stack77)
        %v66905 = vand.u32 %v66904, 65535 (stack78)
        %v66906 = vshrl.u32 %v66905, 1 (stack79)
        %v66907 = vor.u32 %v66906, 16256 (stack75)
        %v66908 = vand.u32.u16 %v66907, 65535 (stack80)
        %v66909 = vunpack.i.l.bf16 %v66908 (stack81)
        %v66913 = vadd.f32 %v66909, -1.0 (stack82)
        %v66917 = vmul.f32 %v66913, 2.0 (stack83)
        %v66921 = vadd.f32 %v66917, -0.99609375 (stack82)
        %v66925 = vmax.f32 -0.99609375, %v66921 (stack84)
        %v66927 = vand.u32 2147483647, %v66925 (stack85)
        %vm66930 = vcmp.eq.f32.partialorder %v66927, 1.0 (stack86)
        %v66935 = vmul.f32 %v66925, inf (stack83)
        %v66937 = vxor.u32 %v66925, 2147483648 (stack87)
        %v66940 = vmul.f32 %v66925, %v66937 (stack83)
        %v66942 = vadd.f32 %v66940, 1.0 (stack88)
        %v66943 = vlog2.pop %v66942 (stack89)
        %v66944 = vmul.f32 %v66943, 0.6931472 (stack90)
        %v66945 = vmul.f32 -0.5, %v66940 (stack91)
        %v66946 = vadd.f32 %v66945, 1.0 (stack92)
        %v66947 = vmul.f32 %v66946, %v66940 (stack93)
        %v66948 = vand.u32 2147483647, %v66940 (stack94)
        %vm66949 = vcmp.lt.f32.partialorder %v66948, 0.0004427343 (stack95)
        %v66950 = vsel /*vm=*/%vm66949, /*on_true_vy=*/%v66947, /*on_false_vx=*/%v66944 (stack96)
        %v66951 = vxor.u32 %v66950, 2147483648 (stack87)
        %vm66954 = vcmp.lt.f32.partialorder %v66951, 5.0 (stack86)
        %v66959 = vsel /*vm=*/%vm66954, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v66963 = vsel /*vm=*/%vm66954, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v66967 = vsel /*vm=*/%vm66954, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v66971 = vsel /*vm=*/%vm66954, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v66975 = vsel /*vm=*/%vm66954, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v66979 = vsel /*vm=*/%vm66954, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v66983 = vsel /*vm=*/%vm66954, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v66987 = vsel /*vm=*/%vm66954, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v66991 = vsel /*vm=*/%vm66954, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v66995 = vadd.f32 %v66951, -2.5 (stack82)
        %v66997 = vrsqrt.pop %v66951 (stack97)
        %v66998 = vmul.f32 %v66951, %v66997 (stack98)
        %vm66999 = vcmp.eq.f32.partialorder %v66951, inf (stack99)
        %v67000 = vsel /*vm=*/%vm66999, /*on_true_vy=*/%v66951, /*on_false_vx=*/%v66998 (stack100)
        %vm67001 = vcmp.eq.f32.partialorder %v66951, 0.0 (stack101)
        %v67002 = vand.u32 %v66951, 2147483648 (stack102)
        %v67003 = vsel /*vm=*/%vm67001, /*on_true_vy=*/%v67002, /*on_false_vx=*/%v67000 (stack103)
        %v67006 = vadd.f32 %v67003, -3.0 (stack82)
        %v67010 = vsel /*vm=*/%vm66954, /*on_true_vy=*/%v66995, /*on_false_vx=*/%v67006 (stack72)
        %v67014 = vmul.f32 %v66991, %v67010 (stack83)
        %v67018 = vadd.f32 %v66987, %v67014 (stack82)
        %v67022 = vmul.f32 %v67018, %v67010 (stack83)
        %v67026 = vadd.f32 %v66983, %v67022 (stack82)
        %v67030 = vmul.f32 %v67026, %v67010 (stack83)
        %v67034 = vadd.f32 %v66979, %v67030 (stack82)
        %v67038 = vmul.f32 %v67034, %v67010 (stack83)
        %v67042 = vadd.f32 %v66975, %v67038 (stack82)
        %v67046 = vmul.f32 %v67042, %v67010 (stack83)
        %v67050 = vadd.f32 %v66971, %v67046 (stack82)
        %v67054 = vmul.f32 %v67050, %v67010 (stack83)
        %v67058 = vadd.f32 %v66967, %v67054 (stack82)
        %v67062 = vmul.f32 %v67058, %v67010 (stack83)
        %v67066 = vadd.f32 %v66963, %v67062 (stack82)
        %v67070 = vmul.f32 %v67066, %v67010 (stack83)
        %v67074 = vadd.f32 %v66959, %v67070 (stack82)
        %v67078 = vmul.f32 %v67074, %v66925 (stack83)
        %v67082 = vsel /*vm=*/%vm66930, /*on_true_vy=*/%v66935, /*on_false_vx=*/%v67078 (stack72)
        %v67086 = vmul.f32 %v67082, 1.4140625 (stack83)
        %s67088 = scalar_lea.vmem %s280, 836 [#allocation0] (stack107)
        %v67089 = vpack.c.bf16 0.0, %v67086 (stack104)
        %67090 = vst [vmem:[%s67088] sm:$0xf] /*vst_source=*/%v67089 (stack105)
        %v67093 = vadd.s32 %v3816, %v63863 (stack65)
        %s67095 = smul.u32 128, %s27 (stack66)
        %v67096 = vlaneseq (stack67)
        %v67097 = vand.u32 %v67096, 127 (stack68)
        %v67098 = vstv %s67095 (stack69)
        %v67099 = vadd.s32 %v67097, %v67098 (stack70)
        %v67103 = vadd.s32 %v67093, %v67099 (stack65)
        %vm67107 = vcmp.lt.u32.totalorder %v67103, %v67093 (stack71)
        %vm67112 = vcmp.lt.u32.totalorder %v67093, %v3816 (stack71)
        %v67117 = vadd.s32 %v3803, %v63846 (stack65)
        %v67121 = vadd.s32 %v67117, 1 (stack65)
        %v67125 = vsel /*vm=*/%vm67112, /*on_true_vy=*/%v67121, /*on_false_vx=*/%v67117 (stack72)
        %v67129 = vadd.s32 %v67125, 1 (stack65)
        %v67133 = vsel /*vm=*/%vm67107, /*on_true_vy=*/%v67129, /*on_false_vx=*/%v67125 (stack72)
        %v67138 = vadd.s32 %v67133, %v10 (stack65)
        %v67142 = vadd.s32 %v67103, %v9 (stack65)
        %v67146 = vadd.s32 %v67138, %v67142 (stack65)
        %v67148 = vshll.u32 %v67142, 13 (stack73)
        %v67149 = vshrl.u32 %v67142, 19 (stack74)
        %v67150 = vor.u32 %v67148, %v67149 (stack75)
        %v67151 = vxor.u32 %v67146, %v67150 (stack76)
        %v67154 = vadd.s32 %v67146, %v67151 (stack65)
        %v67156 = vshll.u32 %v67151, 15 (stack73)
        %v67157 = vshrl.u32 %v67151, 17 (stack74)
        %v67158 = vor.u32 %v67156, %v67157 (stack75)
        %v67159 = vxor.u32 %v67154, %v67158 (stack76)
        %v67162 = vadd.s32 %v67154, %v67159 (stack65)
        %v67164 = vshll.u32 %v67159, 26 (stack73)
        %v67165 = vshrl.u32 %v67159, 6 (stack74)
        %v67166 = vor.u32 %v67164, %v67165 (stack75)
        %v67167 = vxor.u32 %v67162, %v67166 (stack76)
        %v67170 = vadd.s32 %v67162, %v67167 (stack65)
        %v67174 = vadd.s32 %v67170, %v9 (stack65)
        %v67176 = vshll.u32 %v67167, 6 (stack73)
        %v67177 = vshrl.u32 %v67167, 26 (stack74)
        %v67178 = vor.u32 %v67176, %v67177 (stack75)
        %v67179 = vxor.u32 %v67170, %v67178 (stack76)
        %v67182 = vadd.s32 %v67179, %v8 (stack65)
        %v67186 = vadd.s32 %v67182, 1 (stack65)
        %v67190 = vadd.s32 %v67174, %v67186 (stack65)
        %v67192 = vshll.u32 %v67186, 17 (stack73)
        %v67193 = vshrl.u32 %v67186, 15 (stack74)
        %v67194 = vor.u32 %v67192, %v67193 (stack75)
        %v67195 = vxor.u32 %v67190, %v67194 (stack76)
        %v67198 = vadd.s32 %v67190, %v67195 (stack65)
        %v67200 = vshll.u32 %v67195, 29 (stack73)
        %v67201 = vshrl.u32 %v67195, 3 (stack74)
        %v67202 = vor.u32 %v67200, %v67201 (stack75)
        %v67203 = vxor.u32 %v67198, %v67202 (stack76)
        %v67206 = vadd.s32 %v67198, %v67203 (stack65)
        %v67208 = vshll.u32 %v67203, 16 (stack73)
        %v67209 = vshrl.u32 %v67203, 16 (stack74)
        %v67210 = vor.u32 %v67208, %v67209 (stack75)
        %v67211 = vxor.u32 %v67206, %v67210 (stack76)
        %v67214 = vadd.s32 %v67206, %v67211 (stack65)
        %v67218 = vadd.s32 %v67214, %v8 (stack65)
        %v67220 = vshll.u32 %v67211, 24 (stack73)
        %v67221 = vshrl.u32 %v67211, 8 (stack74)
        %v67222 = vor.u32 %v67220, %v67221 (stack75)
        %v67223 = vxor.u32 %v67214, %v67222 (stack76)
        %v67226 = vadd.s32 %v67223, %v10 (stack65)
        %v67230 = vadd.s32 %v67226, 2 (stack65)
        %v67234 = vadd.s32 %v67218, %v67230 (stack65)
        %v67236 = vshll.u32 %v67230, 13 (stack73)
        %v67237 = vshrl.u32 %v67230, 19 (stack74)
        %v67238 = vor.u32 %v67236, %v67237 (stack75)
        %v67239 = vxor.u32 %v67234, %v67238 (stack76)
        %v67242 = vadd.s32 %v67234, %v67239 (stack65)
        %v67244 = vshll.u32 %v67239, 15 (stack73)
        %v67245 = vshrl.u32 %v67239, 17 (stack74)
        %v67246 = vor.u32 %v67244, %v67245 (stack75)
        %v67247 = vxor.u32 %v67242, %v67246 (stack76)
        %v67250 = vadd.s32 %v67242, %v67247 (stack65)
        %v67252 = vshll.u32 %v67247, 26 (stack73)
        %v67253 = vshrl.u32 %v67247, 6 (stack74)
        %v67254 = vor.u32 %v67252, %v67253 (stack75)
        %v67255 = vxor.u32 %v67250, %v67254 (stack76)
        %v67258 = vadd.s32 %v67250, %v67255 (stack65)
        %v67262 = vadd.s32 %v67258, %v10 (stack65)
        %v67264 = vshll.u32 %v67255, 6 (stack73)
        %v67265 = vshrl.u32 %v67255, 26 (stack74)
        %v67266 = vor.u32 %v67264, %v67265 (stack75)
        %v67267 = vxor.u32 %v67258, %v67266 (stack76)
        %v67270 = vadd.s32 %v67267, %v9 (stack65)
        %v67274 = vadd.s32 %v67270, 3 (stack65)
        %v67278 = vadd.s32 %v67262, %v67274 (stack65)
        %v67280 = vshll.u32 %v67274, 17 (stack73)
        %v67281 = vshrl.u32 %v67274, 15 (stack74)
        %v67282 = vor.u32 %v67280, %v67281 (stack75)
        %v67283 = vxor.u32 %v67278, %v67282 (stack76)
        %v67286 = vadd.s32 %v67278, %v67283 (stack65)
        %v67288 = vshll.u32 %v67283, 29 (stack73)
        %v67289 = vshrl.u32 %v67283, 3 (stack74)
        %v67290 = vor.u32 %v67288, %v67289 (stack75)
        %v67291 = vxor.u32 %v67286, %v67290 (stack76)
        %v67294 = vadd.s32 %v67286, %v67291 (stack65)
        %v67296 = vshll.u32 %v67291, 16 (stack73)
        %v67297 = vshrl.u32 %v67291, 16 (stack74)
        %v67298 = vor.u32 %v67296, %v67297 (stack75)
        %v67299 = vxor.u32 %v67294, %v67298 (stack76)
        %v67302 = vadd.s32 %v67294, %v67299 (stack65)
        %v67306 = vadd.s32 %v67302, %v9 (stack65)
        %v67308 = vshll.u32 %v67299, 24 (stack73)
        %v67309 = vshrl.u32 %v67299, 8 (stack74)
        %v67310 = vor.u32 %v67308, %v67309 (stack75)
        %v67311 = vxor.u32 %v67302, %v67310 (stack76)
        %v67314 = vadd.s32 %v67311, %v8 (stack65)
        %v67318 = vadd.s32 %v67314, 4 (stack65)
        %v67322 = vadd.s32 %v67306, %v67318 (stack65)
        %v67324 = vshll.u32 %v67318, 13 (stack73)
        %v67325 = vshrl.u32 %v67318, 19 (stack74)
        %v67326 = vor.u32 %v67324, %v67325 (stack75)
        %v67327 = vxor.u32 %v67322, %v67326 (stack76)
        %v67330 = vadd.s32 %v67322, %v67327 (stack65)
        %v67332 = vshll.u32 %v67327, 15 (stack73)
        %v67333 = vshrl.u32 %v67327, 17 (stack74)
        %v67334 = vor.u32 %v67332, %v67333 (stack75)
        %v67335 = vxor.u32 %v67330, %v67334 (stack76)
        %v67338 = vadd.s32 %v67330, %v67335 (stack65)
        %v67340 = vshll.u32 %v67335, 26 (stack73)
        %v67341 = vshrl.u32 %v67335, 6 (stack74)
        %v67342 = vor.u32 %v67340, %v67341 (stack75)
        %v67343 = vxor.u32 %v67338, %v67342 (stack76)
        %v67346 = vadd.s32 %v67338, %v67343 (stack65)
        %v67350 = vadd.s32 %v67346, %v8 (stack65)
        %v67352 = vshll.u32 %v67343, 6 (stack73)
        %v67353 = vshrl.u32 %v67343, 26 (stack74)
        %v67354 = vor.u32 %v67352, %v67353 (stack75)
        %v67355 = vxor.u32 %v67346, %v67354 (stack76)
        %v67358 = vadd.s32 %v67355, %v10 (stack65)
        %v67362 = vadd.s32 %v67358, 5 (stack65)
        %v67364 = vxor.u32 %v67350, %v67362 (stack76)
        %v67365 = vand.u32.u8 %v67364, 255 (stack77)
        %v67366 = vand.u32 %v67365, 65535 (stack78)
        %v67367 = vshrl.u32 %v67366, 1 (stack79)
        %v67368 = vor.u32 %v67367, 16256 (stack75)
        %v67369 = vand.u32.u16 %v67368, 65535 (stack80)
        %v67370 = vunpack.i.l.bf16 %v67369 (stack81)
        %v67374 = vadd.f32 %v67370, -1.0 (stack82)
        %v67378 = vmul.f32 %v67374, 2.0 (stack83)
        %v67382 = vadd.f32 %v67378, -0.99609375 (stack82)
        %v67386 = vmax.f32 -0.99609375, %v67382 (stack84)
        %v67388 = vand.u32 2147483647, %v67386 (stack85)
        %vm67391 = vcmp.eq.f32.partialorder %v67388, 1.0 (stack86)
        %v67396 = vmul.f32 %v67386, inf (stack83)
        %v67398 = vxor.u32 %v67386, 2147483648 (stack87)
        %v67401 = vmul.f32 %v67386, %v67398 (stack83)
        %v67403 = vadd.f32 %v67401, 1.0 (stack88)
        %v67404 = vlog2.pop %v67403 (stack89)
        %v67405 = vmul.f32 %v67404, 0.6931472 (stack90)
        %v67406 = vmul.f32 -0.5, %v67401 (stack91)
        %v67407 = vadd.f32 %v67406, 1.0 (stack92)
        %v67408 = vmul.f32 %v67407, %v67401 (stack93)
        %v67409 = vand.u32 2147483647, %v67401 (stack94)
        %vm67410 = vcmp.lt.f32.partialorder %v67409, 0.0004427343 (stack95)
        %v67411 = vsel /*vm=*/%vm67410, /*on_true_vy=*/%v67408, /*on_false_vx=*/%v67405 (stack96)
        %v67412 = vxor.u32 %v67411, 2147483648 (stack87)
        %vm67415 = vcmp.lt.f32.partialorder %v67412, 5.0 (stack86)
        %v67420 = vsel /*vm=*/%vm67415, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v67424 = vsel /*vm=*/%vm67415, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v67428 = vsel /*vm=*/%vm67415, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v67432 = vsel /*vm=*/%vm67415, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v67436 = vsel /*vm=*/%vm67415, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v67440 = vsel /*vm=*/%vm67415, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v67444 = vsel /*vm=*/%vm67415, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v67448 = vsel /*vm=*/%vm67415, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v67452 = vsel /*vm=*/%vm67415, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v67456 = vadd.f32 %v67412, -2.5 (stack82)
        %v67458 = vrsqrt.pop %v67412 (stack97)
        %v67459 = vmul.f32 %v67412, %v67458 (stack98)
        %vm67460 = vcmp.eq.f32.partialorder %v67412, inf (stack99)
        %v67461 = vsel /*vm=*/%vm67460, /*on_true_vy=*/%v67412, /*on_false_vx=*/%v67459 (stack100)
        %vm67462 = vcmp.eq.f32.partialorder %v67412, 0.0 (stack101)
        %v67463 = vand.u32 %v67412, 2147483648 (stack102)
        %v67464 = vsel /*vm=*/%vm67462, /*on_true_vy=*/%v67463, /*on_false_vx=*/%v67461 (stack103)
        %v67467 = vadd.f32 %v67464, -3.0 (stack82)
        %v67471 = vsel /*vm=*/%vm67415, /*on_true_vy=*/%v67456, /*on_false_vx=*/%v67467 (stack72)
        %v67475 = vmul.f32 %v67452, %v67471 (stack83)
        %v67479 = vadd.f32 %v67448, %v67475 (stack82)
        %v67483 = vmul.f32 %v67479, %v67471 (stack83)
        %v67487 = vadd.f32 %v67444, %v67483 (stack82)
        %v67491 = vmul.f32 %v67487, %v67471 (stack83)
        %v67495 = vadd.f32 %v67440, %v67491 (stack82)
        %v67499 = vmul.f32 %v67495, %v67471 (stack83)
        %v67503 = vadd.f32 %v67436, %v67499 (stack82)
        %v67507 = vmul.f32 %v67503, %v67471 (stack83)
        %v67511 = vadd.f32 %v67432, %v67507 (stack82)
        %v67515 = vmul.f32 %v67511, %v67471 (stack83)
        %v67519 = vadd.f32 %v67428, %v67515 (stack82)
        %v67523 = vmul.f32 %v67519, %v67471 (stack83)
        %v67527 = vadd.f32 %v67424, %v67523 (stack82)
        %v67531 = vmul.f32 %v67527, %v67471 (stack83)
        %v67535 = vadd.f32 %v67420, %v67531 (stack82)
        %v67539 = vmul.f32 %v67535, %v67386 (stack83)
        %v67543 = vsel /*vm=*/%vm67391, /*on_true_vy=*/%v67396, /*on_false_vx=*/%v67539 (stack72)
        %v67547 = vmul.f32 %v67543, 1.4140625 (stack83)
        %s67549 = scalar_lea.vmem %s280, 964 [#allocation0] (stack107)
        %v67550 = vpack.c.bf16 0.0, %v67547 (stack104)
        %67551 = vst [vmem:[%s67549] sm:$0xf] /*vst_source=*/%v67550 (stack105)
        %s67552 = sadd.s32 %s339, 144 (stack106)
        %s67553 = sshrl.u32 %s67552, 10 (stack49)
        %p67554 = scmp.lt.s32.totalorder 1, %s67553 (stack50)
        %s67555 = scalar_select /*predicate=*/%p67554, /*on_true=*/1, /*on_false=*/%s67553 (stack51)
        %s67556 = sand.u32 %s67552, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s67557 = sshrl.u32 %s67556, 7 (stack53)
        %s67558 = sand.u32 %s67556, 127 /* smod.u32 w/div 128 */ (stack54)
        %s67559 = smul.addr %s67555, 8 (stack55)
        %s67560 = scalar_lea.vmem %s3, %s67559 (stack56)
        %s67562 = scalar_lea.vmem %s67560, %s67557 (stack57)
        %v67563 = vld [vmem:[%s67562] ss:$0 sm:$0xff] (stack58)
        %s67564 = sand.u32 %s67558, 255 (stack59)
        %s67566 = sor.u32 256, %s67564 (stack60)
        %67567 = vbcast.lane.b32.xlu0 %v67563, %s67566 (stack61)
        %v67568 = vpop.permute.xlu0 %67567 (stack62)
        %s67569 = sadd.s32 %s347, 144 (stack106)
        %s67570 = sshrl.u32 %s67569, 10 (stack49)
        %p67571 = scmp.lt.s32.totalorder 1, %s67570 (stack50)
        %s67572 = scalar_select /*predicate=*/%p67571, /*on_true=*/1, /*on_false=*/%s67570 (stack51)
        %s67573 = sand.u32 %s67569, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s67574 = sshrl.u32 %s67573, 7 (stack53)
        %s67575 = sand.u32 %s67573, 127 /* smod.u32 w/div 128 */ (stack54)
        %s67576 = smul.addr %s67572, 8 (stack55)
        %s67577 = scalar_lea.vmem %s5, %s67576 (stack56)
        %s67579 = scalar_lea.vmem %s67577, %s67574 (stack57)
        %v67580 = vld [vmem:[%s67579] ss:$0 sm:$0xff] (stack58)
        %s67581 = sand.u32 %s67575, 255 (stack59)
        %s67583 = sor.u32 256, %s67581 (stack60)
        %67584 = vbcast.lane.b32.xlu0 %v67580, %s67583 (stack61)
        %v67585 = vpop.permute.xlu0 %67584 (stack62)
        %v67588 = vadd.s32 %v408, %v67585 (stack65)
        %s67590 = smul.u32 128, %s27 (stack66)
        %v67591 = vlaneseq (stack67)
        %v67592 = vand.u32 %v67591, 127 (stack68)
        %v67593 = vstv %s67590 (stack69)
        %v67594 = vadd.s32 %v67592, %v67593 (stack70)
        %v67598 = vadd.s32 %v67588, %v67594 (stack65)
        %vm67602 = vcmp.lt.u32.totalorder %v67598, %v67588 (stack71)
        %vm67607 = vcmp.lt.u32.totalorder %v67588, %v408 (stack71)
        %v67612 = vadd.s32 %v380, %v67568 (stack65)
        %v67616 = vadd.s32 %v67612, 1 (stack65)
        %v67620 = vsel /*vm=*/%vm67607, /*on_true_vy=*/%v67616, /*on_false_vx=*/%v67612 (stack72)
        %v67624 = vadd.s32 %v67620, 1 (stack65)
        %v67628 = vsel /*vm=*/%vm67602, /*on_true_vy=*/%v67624, /*on_false_vx=*/%v67620 (stack72)
        %v67633 = vadd.s32 %v67628, %v10 (stack65)
        %v67637 = vadd.s32 %v67598, %v9 (stack65)
        %v67641 = vadd.s32 %v67633, %v67637 (stack65)
        %v67643 = vshll.u32 %v67637, 13 (stack73)
        %v67644 = vshrl.u32 %v67637, 19 (stack74)
        %v67645 = vor.u32 %v67643, %v67644 (stack75)
        %v67646 = vxor.u32 %v67641, %v67645 (stack76)
        %v67649 = vadd.s32 %v67641, %v67646 (stack65)
        %v67651 = vshll.u32 %v67646, 15 (stack73)
        %v67652 = vshrl.u32 %v67646, 17 (stack74)
        %v67653 = vor.u32 %v67651, %v67652 (stack75)
        %v67654 = vxor.u32 %v67649, %v67653 (stack76)
        %v67657 = vadd.s32 %v67649, %v67654 (stack65)
        %v67659 = vshll.u32 %v67654, 26 (stack73)
        %v67660 = vshrl.u32 %v67654, 6 (stack74)
        %v67661 = vor.u32 %v67659, %v67660 (stack75)
        %v67662 = vxor.u32 %v67657, %v67661 (stack76)
        %v67665 = vadd.s32 %v67657, %v67662 (stack65)
        %v67669 = vadd.s32 %v67665, %v9 (stack65)
        %v67671 = vshll.u32 %v67662, 6 (stack73)
        %v67672 = vshrl.u32 %v67662, 26 (stack74)
        %v67673 = vor.u32 %v67671, %v67672 (stack75)
        %v67674 = vxor.u32 %v67665, %v67673 (stack76)
        %v67677 = vadd.s32 %v67674, %v8 (stack65)
        %v67681 = vadd.s32 %v67677, 1 (stack65)
        %v67685 = vadd.s32 %v67669, %v67681 (stack65)
        %v67687 = vshll.u32 %v67681, 17 (stack73)
        %v67688 = vshrl.u32 %v67681, 15 (stack74)
        %v67689 = vor.u32 %v67687, %v67688 (stack75)
        %v67690 = vxor.u32 %v67685, %v67689 (stack76)
        %v67693 = vadd.s32 %v67685, %v67690 (stack65)
        %v67695 = vshll.u32 %v67690, 29 (stack73)
        %v67696 = vshrl.u32 %v67690, 3 (stack74)
        %v67697 = vor.u32 %v67695, %v67696 (stack75)
        %v67698 = vxor.u32 %v67693, %v67697 (stack76)
        %v67701 = vadd.s32 %v67693, %v67698 (stack65)
        %v67703 = vshll.u32 %v67698, 16 (stack73)
        %v67704 = vshrl.u32 %v67698, 16 (stack74)
        %v67705 = vor.u32 %v67703, %v67704 (stack75)
        %v67706 = vxor.u32 %v67701, %v67705 (stack76)
        %v67709 = vadd.s32 %v67701, %v67706 (stack65)
        %v67713 = vadd.s32 %v67709, %v8 (stack65)
        %v67715 = vshll.u32 %v67706, 24 (stack73)
        %v67716 = vshrl.u32 %v67706, 8 (stack74)
        %v67717 = vor.u32 %v67715, %v67716 (stack75)
        %v67718 = vxor.u32 %v67709, %v67717 (stack76)
        %v67721 = vadd.s32 %v67718, %v10 (stack65)
        %v67725 = vadd.s32 %v67721, 2 (stack65)
        %v67729 = vadd.s32 %v67713, %v67725 (stack65)
        %v67731 = vshll.u32 %v67725, 13 (stack73)
        %v67732 = vshrl.u32 %v67725, 19 (stack74)
        %v67733 = vor.u32 %v67731, %v67732 (stack75)
        %v67734 = vxor.u32 %v67729, %v67733 (stack76)
        %v67737 = vadd.s32 %v67729, %v67734 (stack65)
        %v67739 = vshll.u32 %v67734, 15 (stack73)
        %v67740 = vshrl.u32 %v67734, 17 (stack74)
        %v67741 = vor.u32 %v67739, %v67740 (stack75)
        %v67742 = vxor.u32 %v67737, %v67741 (stack76)
        %v67745 = vadd.s32 %v67737, %v67742 (stack65)
        %v67747 = vshll.u32 %v67742, 26 (stack73)
        %v67748 = vshrl.u32 %v67742, 6 (stack74)
        %v67749 = vor.u32 %v67747, %v67748 (stack75)
        %v67750 = vxor.u32 %v67745, %v67749 (stack76)
        %v67753 = vadd.s32 %v67745, %v67750 (stack65)
        %v67757 = vadd.s32 %v67753, %v10 (stack65)
        %v67759 = vshll.u32 %v67750, 6 (stack73)
        %v67760 = vshrl.u32 %v67750, 26 (stack74)
        %v67761 = vor.u32 %v67759, %v67760 (stack75)
        %v67762 = vxor.u32 %v67753, %v67761 (stack76)
        %v67765 = vadd.s32 %v67762, %v9 (stack65)
        %v67769 = vadd.s32 %v67765, 3 (stack65)
        %v67773 = vadd.s32 %v67757, %v67769 (stack65)
        %v67775 = vshll.u32 %v67769, 17 (stack73)
        %v67776 = vshrl.u32 %v67769, 15 (stack74)
        %v67777 = vor.u32 %v67775, %v67776 (stack75)
        %v67778 = vxor.u32 %v67773, %v67777 (stack76)
        %v67781 = vadd.s32 %v67773, %v67778 (stack65)
        %v67783 = vshll.u32 %v67778, 29 (stack73)
        %v67784 = vshrl.u32 %v67778, 3 (stack74)
        %v67785 = vor.u32 %v67783, %v67784 (stack75)
        %v67786 = vxor.u32 %v67781, %v67785 (stack76)
        %v67789 = vadd.s32 %v67781, %v67786 (stack65)
        %v67791 = vshll.u32 %v67786, 16 (stack73)
        %v67792 = vshrl.u32 %v67786, 16 (stack74)
        %v67793 = vor.u32 %v67791, %v67792 (stack75)
        %v67794 = vxor.u32 %v67789, %v67793 (stack76)
        %v67797 = vadd.s32 %v67789, %v67794 (stack65)
        %v67801 = vadd.s32 %v67797, %v9 (stack65)
        %v67803 = vshll.u32 %v67794, 24 (stack73)
        %v67804 = vshrl.u32 %v67794, 8 (stack74)
        %v67805 = vor.u32 %v67803, %v67804 (stack75)
        %v67806 = vxor.u32 %v67797, %v67805 (stack76)
        %v67809 = vadd.s32 %v67806, %v8 (stack65)
        %v67813 = vadd.s32 %v67809, 4 (stack65)
        %v67817 = vadd.s32 %v67801, %v67813 (stack65)
        %v67819 = vshll.u32 %v67813, 13 (stack73)
        %v67820 = vshrl.u32 %v67813, 19 (stack74)
        %v67821 = vor.u32 %v67819, %v67820 (stack75)
        %v67822 = vxor.u32 %v67817, %v67821 (stack76)
        %v67825 = vadd.s32 %v67817, %v67822 (stack65)
        %v67827 = vshll.u32 %v67822, 15 (stack73)
        %v67828 = vshrl.u32 %v67822, 17 (stack74)
        %v67829 = vor.u32 %v67827, %v67828 (stack75)
        %v67830 = vxor.u32 %v67825, %v67829 (stack76)
        %v67833 = vadd.s32 %v67825, %v67830 (stack65)
        %v67835 = vshll.u32 %v67830, 26 (stack73)
        %v67836 = vshrl.u32 %v67830, 6 (stack74)
        %v67837 = vor.u32 %v67835, %v67836 (stack75)
        %v67838 = vxor.u32 %v67833, %v67837 (stack76)
        %v67841 = vadd.s32 %v67833, %v67838 (stack65)
        %v67845 = vadd.s32 %v67841, %v8 (stack65)
        %v67847 = vshll.u32 %v67838, 6 (stack73)
        %v67848 = vshrl.u32 %v67838, 26 (stack74)
        %v67849 = vor.u32 %v67847, %v67848 (stack75)
        %v67850 = vxor.u32 %v67841, %v67849 (stack76)
        %v67853 = vadd.s32 %v67850, %v10 (stack65)
        %v67857 = vadd.s32 %v67853, 5 (stack65)
        %v67859 = vxor.u32 %v67845, %v67857 (stack76)
        %v67860 = vand.u32.u8 %v67859, 255 (stack77)
        %v67861 = vand.u32 %v67860, 65535 (stack78)
        %v67862 = vshrl.u32 %v67861, 1 (stack79)
        %v67863 = vor.u32 %v67862, 16256 (stack75)
        %v67864 = vand.u32.u16 %v67863, 65535 (stack80)
        %v67865 = vunpack.i.l.bf16 %v67864 (stack81)
        %v67869 = vadd.f32 %v67865, -1.0 (stack82)
        %v67873 = vmul.f32 %v67869, 2.0 (stack83)
        %v67877 = vadd.f32 %v67873, -0.99609375 (stack82)
        %v67881 = vmax.f32 -0.99609375, %v67877 (stack84)
        %v67883 = vand.u32 2147483647, %v67881 (stack85)
        %vm67886 = vcmp.eq.f32.partialorder %v67883, 1.0 (stack86)
        %v67891 = vmul.f32 %v67881, inf (stack83)
        %v67893 = vxor.u32 %v67881, 2147483648 (stack87)
        %v67896 = vmul.f32 %v67881, %v67893 (stack83)
        %v67898 = vadd.f32 %v67896, 1.0 (stack88)
        %v67899 = vlog2.pop %v67898 (stack89)
        %v67900 = vmul.f32 %v67899, 0.6931472 (stack90)
        %v67901 = vmul.f32 -0.5, %v67896 (stack91)
        %v67902 = vadd.f32 %v67901, 1.0 (stack92)
        %v67903 = vmul.f32 %v67902, %v67896 (stack93)
        %v67904 = vand.u32 2147483647, %v67896 (stack94)
        %vm67905 = vcmp.lt.f32.partialorder %v67904, 0.0004427343 (stack95)
        %v67906 = vsel /*vm=*/%vm67905, /*on_true_vy=*/%v67903, /*on_false_vx=*/%v67900 (stack96)
        %v67907 = vxor.u32 %v67906, 2147483648 (stack87)
        %vm67910 = vcmp.lt.f32.partialorder %v67907, 5.0 (stack86)
        %v67915 = vsel /*vm=*/%vm67910, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v67919 = vsel /*vm=*/%vm67910, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v67923 = vsel /*vm=*/%vm67910, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v67927 = vsel /*vm=*/%vm67910, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v67931 = vsel /*vm=*/%vm67910, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v67935 = vsel /*vm=*/%vm67910, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v67939 = vsel /*vm=*/%vm67910, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v67943 = vsel /*vm=*/%vm67910, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v67947 = vsel /*vm=*/%vm67910, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v67951 = vadd.f32 %v67907, -2.5 (stack82)
        %v67953 = vrsqrt.pop %v67907 (stack97)
        %v67954 = vmul.f32 %v67907, %v67953 (stack98)
        %vm67955 = vcmp.eq.f32.partialorder %v67907, inf (stack99)
        %v67956 = vsel /*vm=*/%vm67955, /*on_true_vy=*/%v67907, /*on_false_vx=*/%v67954 (stack100)
        %vm67957 = vcmp.eq.f32.partialorder %v67907, 0.0 (stack101)
        %v67958 = vand.u32 %v67907, 2147483648 (stack102)
        %v67959 = vsel /*vm=*/%vm67957, /*on_true_vy=*/%v67958, /*on_false_vx=*/%v67956 (stack103)
        %v67962 = vadd.f32 %v67959, -3.0 (stack82)
        %v67966 = vsel /*vm=*/%vm67910, /*on_true_vy=*/%v67951, /*on_false_vx=*/%v67962 (stack72)
        %v67970 = vmul.f32 %v67947, %v67966 (stack83)
        %v67974 = vadd.f32 %v67943, %v67970 (stack82)
        %v67978 = vmul.f32 %v67974, %v67966 (stack83)
        %v67982 = vadd.f32 %v67939, %v67978 (stack82)
        %v67986 = vmul.f32 %v67982, %v67966 (stack83)
        %v67990 = vadd.f32 %v67935, %v67986 (stack82)
        %v67994 = vmul.f32 %v67990, %v67966 (stack83)
        %v67998 = vadd.f32 %v67931, %v67994 (stack82)
        %v68002 = vmul.f32 %v67998, %v67966 (stack83)
        %v68006 = vadd.f32 %v67927, %v68002 (stack82)
        %v68010 = vmul.f32 %v68006, %v67966 (stack83)
        %v68014 = vadd.f32 %v67923, %v68010 (stack82)
        %v68018 = vmul.f32 %v68014, %v67966 (stack83)
        %v68022 = vadd.f32 %v67919, %v68018 (stack82)
        %v68026 = vmul.f32 %v68022, %v67966 (stack83)
        %v68030 = vadd.f32 %v67915, %v68026 (stack82)
        %v68034 = vmul.f32 %v68030, %v67881 (stack83)
        %v68038 = vsel /*vm=*/%vm67886, /*on_true_vy=*/%v67891, /*on_false_vx=*/%v68034 (stack72)
        %v68042 = vmul.f32 %v68038, 1.4140625 (stack83)
        %s68044 = scalar_lea.vmem %s280, 72 [#allocation0] (stack107)
        %v68045 = vpack.c.bf16 0.0, %v68042 (stack104)
        %68046 = vst [vmem:[%s68044] sm:$0xf] /*vst_source=*/%v68045 (stack105)
        %v68049 = vadd.s32 %v894, %v67585 (stack65)
        %s68051 = smul.u32 128, %s27 (stack66)
        %v68052 = vlaneseq (stack67)
        %v68053 = vand.u32 %v68052, 127 (stack68)
        %v68054 = vstv %s68051 (stack69)
        %v68055 = vadd.s32 %v68053, %v68054 (stack70)
        %v68059 = vadd.s32 %v68049, %v68055 (stack65)
        %vm68063 = vcmp.lt.u32.totalorder %v68059, %v68049 (stack71)
        %vm68068 = vcmp.lt.u32.totalorder %v68049, %v894 (stack71)
        %v68073 = vadd.s32 %v881, %v67568 (stack65)
        %v68077 = vadd.s32 %v68073, 1 (stack65)
        %v68081 = vsel /*vm=*/%vm68068, /*on_true_vy=*/%v68077, /*on_false_vx=*/%v68073 (stack72)
        %v68085 = vadd.s32 %v68081, 1 (stack65)
        %v68089 = vsel /*vm=*/%vm68063, /*on_true_vy=*/%v68085, /*on_false_vx=*/%v68081 (stack72)
        %v68094 = vadd.s32 %v68089, %v10 (stack65)
        %v68098 = vadd.s32 %v68059, %v9 (stack65)
        %v68102 = vadd.s32 %v68094, %v68098 (stack65)
        %v68104 = vshll.u32 %v68098, 13 (stack73)
        %v68105 = vshrl.u32 %v68098, 19 (stack74)
        %v68106 = vor.u32 %v68104, %v68105 (stack75)
        %v68107 = vxor.u32 %v68102, %v68106 (stack76)
        %v68110 = vadd.s32 %v68102, %v68107 (stack65)
        %v68112 = vshll.u32 %v68107, 15 (stack73)
        %v68113 = vshrl.u32 %v68107, 17 (stack74)
        %v68114 = vor.u32 %v68112, %v68113 (stack75)
        %v68115 = vxor.u32 %v68110, %v68114 (stack76)
        %v68118 = vadd.s32 %v68110, %v68115 (stack65)
        %v68120 = vshll.u32 %v68115, 26 (stack73)
        %v68121 = vshrl.u32 %v68115, 6 (stack74)
        %v68122 = vor.u32 %v68120, %v68121 (stack75)
        %v68123 = vxor.u32 %v68118, %v68122 (stack76)
        %v68126 = vadd.s32 %v68118, %v68123 (stack65)
        %v68130 = vadd.s32 %v68126, %v9 (stack65)
        %v68132 = vshll.u32 %v68123, 6 (stack73)
        %v68133 = vshrl.u32 %v68123, 26 (stack74)
        %v68134 = vor.u32 %v68132, %v68133 (stack75)
        %v68135 = vxor.u32 %v68126, %v68134 (stack76)
        %v68138 = vadd.s32 %v68135, %v8 (stack65)
        %v68142 = vadd.s32 %v68138, 1 (stack65)
        %v68146 = vadd.s32 %v68130, %v68142 (stack65)
        %v68148 = vshll.u32 %v68142, 17 (stack73)
        %v68149 = vshrl.u32 %v68142, 15 (stack74)
        %v68150 = vor.u32 %v68148, %v68149 (stack75)
        %v68151 = vxor.u32 %v68146, %v68150 (stack76)
        %v68154 = vadd.s32 %v68146, %v68151 (stack65)
        %v68156 = vshll.u32 %v68151, 29 (stack73)
        %v68157 = vshrl.u32 %v68151, 3 (stack74)
        %v68158 = vor.u32 %v68156, %v68157 (stack75)
        %v68159 = vxor.u32 %v68154, %v68158 (stack76)
        %v68162 = vadd.s32 %v68154, %v68159 (stack65)
        %v68164 = vshll.u32 %v68159, 16 (stack73)
        %v68165 = vshrl.u32 %v68159, 16 (stack74)
        %v68166 = vor.u32 %v68164, %v68165 (stack75)
        %v68167 = vxor.u32 %v68162, %v68166 (stack76)
        %v68170 = vadd.s32 %v68162, %v68167 (stack65)
        %v68174 = vadd.s32 %v68170, %v8 (stack65)
        %v68176 = vshll.u32 %v68167, 24 (stack73)
        %v68177 = vshrl.u32 %v68167, 8 (stack74)
        %v68178 = vor.u32 %v68176, %v68177 (stack75)
        %v68179 = vxor.u32 %v68170, %v68178 (stack76)
        %v68182 = vadd.s32 %v68179, %v10 (stack65)
        %v68186 = vadd.s32 %v68182, 2 (stack65)
        %v68190 = vadd.s32 %v68174, %v68186 (stack65)
        %v68192 = vshll.u32 %v68186, 13 (stack73)
        %v68193 = vshrl.u32 %v68186, 19 (stack74)
        %v68194 = vor.u32 %v68192, %v68193 (stack75)
        %v68195 = vxor.u32 %v68190, %v68194 (stack76)
        %v68198 = vadd.s32 %v68190, %v68195 (stack65)
        %v68200 = vshll.u32 %v68195, 15 (stack73)
        %v68201 = vshrl.u32 %v68195, 17 (stack74)
        %v68202 = vor.u32 %v68200, %v68201 (stack75)
        %v68203 = vxor.u32 %v68198, %v68202 (stack76)
        %v68206 = vadd.s32 %v68198, %v68203 (stack65)
        %v68208 = vshll.u32 %v68203, 26 (stack73)
        %v68209 = vshrl.u32 %v68203, 6 (stack74)
        %v68210 = vor.u32 %v68208, %v68209 (stack75)
        %v68211 = vxor.u32 %v68206, %v68210 (stack76)
        %v68214 = vadd.s32 %v68206, %v68211 (stack65)
        %v68218 = vadd.s32 %v68214, %v10 (stack65)
        %v68220 = vshll.u32 %v68211, 6 (stack73)
        %v68221 = vshrl.u32 %v68211, 26 (stack74)
        %v68222 = vor.u32 %v68220, %v68221 (stack75)
        %v68223 = vxor.u32 %v68214, %v68222 (stack76)
        %v68226 = vadd.s32 %v68223, %v9 (stack65)
        %v68230 = vadd.s32 %v68226, 3 (stack65)
        %v68234 = vadd.s32 %v68218, %v68230 (stack65)
        %v68236 = vshll.u32 %v68230, 17 (stack73)
        %v68237 = vshrl.u32 %v68230, 15 (stack74)
        %v68238 = vor.u32 %v68236, %v68237 (stack75)
        %v68239 = vxor.u32 %v68234, %v68238 (stack76)
        %v68242 = vadd.s32 %v68234, %v68239 (stack65)
        %v68244 = vshll.u32 %v68239, 29 (stack73)
        %v68245 = vshrl.u32 %v68239, 3 (stack74)
        %v68246 = vor.u32 %v68244, %v68245 (stack75)
        %v68247 = vxor.u32 %v68242, %v68246 (stack76)
        %v68250 = vadd.s32 %v68242, %v68247 (stack65)
        %v68252 = vshll.u32 %v68247, 16 (stack73)
        %v68253 = vshrl.u32 %v68247, 16 (stack74)
        %v68254 = vor.u32 %v68252, %v68253 (stack75)
        %v68255 = vxor.u32 %v68250, %v68254 (stack76)
        %v68258 = vadd.s32 %v68250, %v68255 (stack65)
        %v68262 = vadd.s32 %v68258, %v9 (stack65)
        %v68264 = vshll.u32 %v68255, 24 (stack73)
        %v68265 = vshrl.u32 %v68255, 8 (stack74)
        %v68266 = vor.u32 %v68264, %v68265 (stack75)
        %v68267 = vxor.u32 %v68258, %v68266 (stack76)
        %v68270 = vadd.s32 %v68267, %v8 (stack65)
        %v68274 = vadd.s32 %v68270, 4 (stack65)
        %v68278 = vadd.s32 %v68262, %v68274 (stack65)
        %v68280 = vshll.u32 %v68274, 13 (stack73)
        %v68281 = vshrl.u32 %v68274, 19 (stack74)
        %v68282 = vor.u32 %v68280, %v68281 (stack75)
        %v68283 = vxor.u32 %v68278, %v68282 (stack76)
        %v68286 = vadd.s32 %v68278, %v68283 (stack65)
        %v68288 = vshll.u32 %v68283, 15 (stack73)
        %v68289 = vshrl.u32 %v68283, 17 (stack74)
        %v68290 = vor.u32 %v68288, %v68289 (stack75)
        %v68291 = vxor.u32 %v68286, %v68290 (stack76)
        %v68294 = vadd.s32 %v68286, %v68291 (stack65)
        %v68296 = vshll.u32 %v68291, 26 (stack73)
        %v68297 = vshrl.u32 %v68291, 6 (stack74)
        %v68298 = vor.u32 %v68296, %v68297 (stack75)
        %v68299 = vxor.u32 %v68294, %v68298 (stack76)
        %v68302 = vadd.s32 %v68294, %v68299 (stack65)
        %v68306 = vadd.s32 %v68302, %v8 (stack65)
        %v68308 = vshll.u32 %v68299, 6 (stack73)
        %v68309 = vshrl.u32 %v68299, 26 (stack74)
        %v68310 = vor.u32 %v68308, %v68309 (stack75)
        %v68311 = vxor.u32 %v68302, %v68310 (stack76)
        %v68314 = vadd.s32 %v68311, %v10 (stack65)
        %v68318 = vadd.s32 %v68314, 5 (stack65)
        %v68320 = vxor.u32 %v68306, %v68318 (stack76)
        %v68321 = vand.u32.u8 %v68320, 255 (stack77)
        %v68322 = vand.u32 %v68321, 65535 (stack78)
        %v68323 = vshrl.u32 %v68322, 1 (stack79)
        %v68324 = vor.u32 %v68323, 16256 (stack75)
        %v68325 = vand.u32.u16 %v68324, 65535 (stack80)
        %v68326 = vunpack.i.l.bf16 %v68325 (stack81)
        %v68330 = vadd.f32 %v68326, -1.0 (stack82)
        %v68334 = vmul.f32 %v68330, 2.0 (stack83)
        %v68338 = vadd.f32 %v68334, -0.99609375 (stack82)
        %v68342 = vmax.f32 -0.99609375, %v68338 (stack84)
        %v68344 = vand.u32 2147483647, %v68342 (stack85)
        %vm68347 = vcmp.eq.f32.partialorder %v68344, 1.0 (stack86)
        %v68352 = vmul.f32 %v68342, inf (stack83)
        %v68354 = vxor.u32 %v68342, 2147483648 (stack87)
        %v68357 = vmul.f32 %v68342, %v68354 (stack83)
        %v68359 = vadd.f32 %v68357, 1.0 (stack88)
        %v68360 = vlog2.pop %v68359 (stack89)
        %v68361 = vmul.f32 %v68360, 0.6931472 (stack90)
        %v68362 = vmul.f32 -0.5, %v68357 (stack91)
        %v68363 = vadd.f32 %v68362, 1.0 (stack92)
        %v68364 = vmul.f32 %v68363, %v68357 (stack93)
        %v68365 = vand.u32 2147483647, %v68357 (stack94)
        %vm68366 = vcmp.lt.f32.partialorder %v68365, 0.0004427343 (stack95)
        %v68367 = vsel /*vm=*/%vm68366, /*on_true_vy=*/%v68364, /*on_false_vx=*/%v68361 (stack96)
        %v68368 = vxor.u32 %v68367, 2147483648 (stack87)
        %vm68371 = vcmp.lt.f32.partialorder %v68368, 5.0 (stack86)
        %v68376 = vsel /*vm=*/%vm68371, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v68380 = vsel /*vm=*/%vm68371, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v68384 = vsel /*vm=*/%vm68371, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v68388 = vsel /*vm=*/%vm68371, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v68392 = vsel /*vm=*/%vm68371, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v68396 = vsel /*vm=*/%vm68371, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v68400 = vsel /*vm=*/%vm68371, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v68404 = vsel /*vm=*/%vm68371, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v68408 = vsel /*vm=*/%vm68371, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v68412 = vadd.f32 %v68368, -2.5 (stack82)
        %v68414 = vrsqrt.pop %v68368 (stack97)
        %v68415 = vmul.f32 %v68368, %v68414 (stack98)
        %vm68416 = vcmp.eq.f32.partialorder %v68368, inf (stack99)
        %v68417 = vsel /*vm=*/%vm68416, /*on_true_vy=*/%v68368, /*on_false_vx=*/%v68415 (stack100)
        %vm68418 = vcmp.eq.f32.partialorder %v68368, 0.0 (stack101)
        %v68419 = vand.u32 %v68368, 2147483648 (stack102)
        %v68420 = vsel /*vm=*/%vm68418, /*on_true_vy=*/%v68419, /*on_false_vx=*/%v68417 (stack103)
        %v68423 = vadd.f32 %v68420, -3.0 (stack82)
        %v68427 = vsel /*vm=*/%vm68371, /*on_true_vy=*/%v68412, /*on_false_vx=*/%v68423 (stack72)
        %v68431 = vmul.f32 %v68408, %v68427 (stack83)
        %v68435 = vadd.f32 %v68404, %v68431 (stack82)
        %v68439 = vmul.f32 %v68435, %v68427 (stack83)
        %v68443 = vadd.f32 %v68400, %v68439 (stack82)
        %v68447 = vmul.f32 %v68443, %v68427 (stack83)
        %v68451 = vadd.f32 %v68396, %v68447 (stack82)
        %v68455 = vmul.f32 %v68451, %v68427 (stack83)
        %v68459 = vadd.f32 %v68392, %v68455 (stack82)
        %v68463 = vmul.f32 %v68459, %v68427 (stack83)
        %v68467 = vadd.f32 %v68388, %v68463 (stack82)
        %v68471 = vmul.f32 %v68467, %v68427 (stack83)
        %v68475 = vadd.f32 %v68384, %v68471 (stack82)
        %v68479 = vmul.f32 %v68475, %v68427 (stack83)
        %v68483 = vadd.f32 %v68380, %v68479 (stack82)
        %v68487 = vmul.f32 %v68483, %v68427 (stack83)
        %v68491 = vadd.f32 %v68376, %v68487 (stack82)
        %v68495 = vmul.f32 %v68491, %v68342 (stack83)
        %v68499 = vsel /*vm=*/%vm68347, /*on_true_vy=*/%v68352, /*on_false_vx=*/%v68495 (stack72)
        %v68503 = vmul.f32 %v68499, 1.4140625 (stack83)
        %s68505 = scalar_lea.vmem %s280, 200 [#allocation0] (stack107)
        %v68506 = vpack.c.bf16 0.0, %v68503 (stack104)
        %68507 = vst [vmem:[%s68505] sm:$0xf] /*vst_source=*/%v68506 (stack105)
        %v68510 = vadd.s32 %v1381, %v67585 (stack65)
        %s68512 = smul.u32 128, %s27 (stack66)
        %v68513 = vlaneseq (stack67)
        %v68514 = vand.u32 %v68513, 127 (stack68)
        %v68515 = vstv %s68512 (stack69)
        %v68516 = vadd.s32 %v68514, %v68515 (stack70)
        %v68520 = vadd.s32 %v68510, %v68516 (stack65)
        %vm68524 = vcmp.lt.u32.totalorder %v68520, %v68510 (stack71)
        %vm68529 = vcmp.lt.u32.totalorder %v68510, %v1381 (stack71)
        %v68534 = vadd.s32 %v1368, %v67568 (stack65)
        %v68538 = vadd.s32 %v68534, 1 (stack65)
        %v68542 = vsel /*vm=*/%vm68529, /*on_true_vy=*/%v68538, /*on_false_vx=*/%v68534 (stack72)
        %v68546 = vadd.s32 %v68542, 1 (stack65)
        %v68550 = vsel /*vm=*/%vm68524, /*on_true_vy=*/%v68546, /*on_false_vx=*/%v68542 (stack72)
        %v68555 = vadd.s32 %v68550, %v10 (stack65)
        %v68559 = vadd.s32 %v68520, %v9 (stack65)
        %v68563 = vadd.s32 %v68555, %v68559 (stack65)
        %v68565 = vshll.u32 %v68559, 13 (stack73)
        %v68566 = vshrl.u32 %v68559, 19 (stack74)
        %v68567 = vor.u32 %v68565, %v68566 (stack75)
        %v68568 = vxor.u32 %v68563, %v68567 (stack76)
        %v68571 = vadd.s32 %v68563, %v68568 (stack65)
        %v68573 = vshll.u32 %v68568, 15 (stack73)
        %v68574 = vshrl.u32 %v68568, 17 (stack74)
        %v68575 = vor.u32 %v68573, %v68574 (stack75)
        %v68576 = vxor.u32 %v68571, %v68575 (stack76)
        %v68579 = vadd.s32 %v68571, %v68576 (stack65)
        %v68581 = vshll.u32 %v68576, 26 (stack73)
        %v68582 = vshrl.u32 %v68576, 6 (stack74)
        %v68583 = vor.u32 %v68581, %v68582 (stack75)
        %v68584 = vxor.u32 %v68579, %v68583 (stack76)
        %v68587 = vadd.s32 %v68579, %v68584 (stack65)
        %v68591 = vadd.s32 %v68587, %v9 (stack65)
        %v68593 = vshll.u32 %v68584, 6 (stack73)
        %v68594 = vshrl.u32 %v68584, 26 (stack74)
        %v68595 = vor.u32 %v68593, %v68594 (stack75)
        %v68596 = vxor.u32 %v68587, %v68595 (stack76)
        %v68599 = vadd.s32 %v68596, %v8 (stack65)
        %v68603 = vadd.s32 %v68599, 1 (stack65)
        %v68607 = vadd.s32 %v68591, %v68603 (stack65)
        %v68609 = vshll.u32 %v68603, 17 (stack73)
        %v68610 = vshrl.u32 %v68603, 15 (stack74)
        %v68611 = vor.u32 %v68609, %v68610 (stack75)
        %v68612 = vxor.u32 %v68607, %v68611 (stack76)
        %v68615 = vadd.s32 %v68607, %v68612 (stack65)
        %v68617 = vshll.u32 %v68612, 29 (stack73)
        %v68618 = vshrl.u32 %v68612, 3 (stack74)
        %v68619 = vor.u32 %v68617, %v68618 (stack75)
        %v68620 = vxor.u32 %v68615, %v68619 (stack76)
        %v68623 = vadd.s32 %v68615, %v68620 (stack65)
        %v68625 = vshll.u32 %v68620, 16 (stack73)
        %v68626 = vshrl.u32 %v68620, 16 (stack74)
        %v68627 = vor.u32 %v68625, %v68626 (stack75)
        %v68628 = vxor.u32 %v68623, %v68627 (stack76)
        %v68631 = vadd.s32 %v68623, %v68628 (stack65)
        %v68635 = vadd.s32 %v68631, %v8 (stack65)
        %v68637 = vshll.u32 %v68628, 24 (stack73)
        %v68638 = vshrl.u32 %v68628, 8 (stack74)
        %v68639 = vor.u32 %v68637, %v68638 (stack75)
        %v68640 = vxor.u32 %v68631, %v68639 (stack76)
        %v68643 = vadd.s32 %v68640, %v10 (stack65)
        %v68647 = vadd.s32 %v68643, 2 (stack65)
        %v68651 = vadd.s32 %v68635, %v68647 (stack65)
        %v68653 = vshll.u32 %v68647, 13 (stack73)
        %v68654 = vshrl.u32 %v68647, 19 (stack74)
        %v68655 = vor.u32 %v68653, %v68654 (stack75)
        %v68656 = vxor.u32 %v68651, %v68655 (stack76)
        %v68659 = vadd.s32 %v68651, %v68656 (stack65)
        %v68661 = vshll.u32 %v68656, 15 (stack73)
        %v68662 = vshrl.u32 %v68656, 17 (stack74)
        %v68663 = vor.u32 %v68661, %v68662 (stack75)
        %v68664 = vxor.u32 %v68659, %v68663 (stack76)
        %v68667 = vadd.s32 %v68659, %v68664 (stack65)
        %v68669 = vshll.u32 %v68664, 26 (stack73)
        %v68670 = vshrl.u32 %v68664, 6 (stack74)
        %v68671 = vor.u32 %v68669, %v68670 (stack75)
        %v68672 = vxor.u32 %v68667, %v68671 (stack76)
        %v68675 = vadd.s32 %v68667, %v68672 (stack65)
        %v68679 = vadd.s32 %v68675, %v10 (stack65)
        %v68681 = vshll.u32 %v68672, 6 (stack73)
        %v68682 = vshrl.u32 %v68672, 26 (stack74)
        %v68683 = vor.u32 %v68681, %v68682 (stack75)
        %v68684 = vxor.u32 %v68675, %v68683 (stack76)
        %v68687 = vadd.s32 %v68684, %v9 (stack65)
        %v68691 = vadd.s32 %v68687, 3 (stack65)
        %v68695 = vadd.s32 %v68679, %v68691 (stack65)
        %v68697 = vshll.u32 %v68691, 17 (stack73)
        %v68698 = vshrl.u32 %v68691, 15 (stack74)
        %v68699 = vor.u32 %v68697, %v68698 (stack75)
        %v68700 = vxor.u32 %v68695, %v68699 (stack76)
        %v68703 = vadd.s32 %v68695, %v68700 (stack65)
        %v68705 = vshll.u32 %v68700, 29 (stack73)
        %v68706 = vshrl.u32 %v68700, 3 (stack74)
        %v68707 = vor.u32 %v68705, %v68706 (stack75)
        %v68708 = vxor.u32 %v68703, %v68707 (stack76)
        %v68711 = vadd.s32 %v68703, %v68708 (stack65)
        %v68713 = vshll.u32 %v68708, 16 (stack73)
        %v68714 = vshrl.u32 %v68708, 16 (stack74)
        %v68715 = vor.u32 %v68713, %v68714 (stack75)
        %v68716 = vxor.u32 %v68711, %v68715 (stack76)
        %v68719 = vadd.s32 %v68711, %v68716 (stack65)
        %v68723 = vadd.s32 %v68719, %v9 (stack65)
        %v68725 = vshll.u32 %v68716, 24 (stack73)
        %v68726 = vshrl.u32 %v68716, 8 (stack74)
        %v68727 = vor.u32 %v68725, %v68726 (stack75)
        %v68728 = vxor.u32 %v68719, %v68727 (stack76)
        %v68731 = vadd.s32 %v68728, %v8 (stack65)
        %v68735 = vadd.s32 %v68731, 4 (stack65)
        %v68739 = vadd.s32 %v68723, %v68735 (stack65)
        %v68741 = vshll.u32 %v68735, 13 (stack73)
        %v68742 = vshrl.u32 %v68735, 19 (stack74)
        %v68743 = vor.u32 %v68741, %v68742 (stack75)
        %v68744 = vxor.u32 %v68739, %v68743 (stack76)
        %v68747 = vadd.s32 %v68739, %v68744 (stack65)
        %v68749 = vshll.u32 %v68744, 15 (stack73)
        %v68750 = vshrl.u32 %v68744, 17 (stack74)
        %v68751 = vor.u32 %v68749, %v68750 (stack75)
        %v68752 = vxor.u32 %v68747, %v68751 (stack76)
        %v68755 = vadd.s32 %v68747, %v68752 (stack65)
        %v68757 = vshll.u32 %v68752, 26 (stack73)
        %v68758 = vshrl.u32 %v68752, 6 (stack74)
        %v68759 = vor.u32 %v68757, %v68758 (stack75)
        %v68760 = vxor.u32 %v68755, %v68759 (stack76)
        %v68763 = vadd.s32 %v68755, %v68760 (stack65)
        %v68767 = vadd.s32 %v68763, %v8 (stack65)
        %v68769 = vshll.u32 %v68760, 6 (stack73)
        %v68770 = vshrl.u32 %v68760, 26 (stack74)
        %v68771 = vor.u32 %v68769, %v68770 (stack75)
        %v68772 = vxor.u32 %v68763, %v68771 (stack76)
        %v68775 = vadd.s32 %v68772, %v10 (stack65)
        %v68779 = vadd.s32 %v68775, 5 (stack65)
        %v68781 = vxor.u32 %v68767, %v68779 (stack76)
        %v68782 = vand.u32.u8 %v68781, 255 (stack77)
        %v68783 = vand.u32 %v68782, 65535 (stack78)
        %v68784 = vshrl.u32 %v68783, 1 (stack79)
        %v68785 = vor.u32 %v68784, 16256 (stack75)
        %v68786 = vand.u32.u16 %v68785, 65535 (stack80)
        %v68787 = vunpack.i.l.bf16 %v68786 (stack81)
        %v68791 = vadd.f32 %v68787, -1.0 (stack82)
        %v68795 = vmul.f32 %v68791, 2.0 (stack83)
        %v68799 = vadd.f32 %v68795, -0.99609375 (stack82)
        %v68803 = vmax.f32 -0.99609375, %v68799 (stack84)
        %v68805 = vand.u32 2147483647, %v68803 (stack85)
        %vm68808 = vcmp.eq.f32.partialorder %v68805, 1.0 (stack86)
        %v68813 = vmul.f32 %v68803, inf (stack83)
        %v68815 = vxor.u32 %v68803, 2147483648 (stack87)
        %v68818 = vmul.f32 %v68803, %v68815 (stack83)
        %v68820 = vadd.f32 %v68818, 1.0 (stack88)
        %v68821 = vlog2.pop %v68820 (stack89)
        %v68822 = vmul.f32 %v68821, 0.6931472 (stack90)
        %v68823 = vmul.f32 -0.5, %v68818 (stack91)
        %v68824 = vadd.f32 %v68823, 1.0 (stack92)
        %v68825 = vmul.f32 %v68824, %v68818 (stack93)
        %v68826 = vand.u32 2147483647, %v68818 (stack94)
        %vm68827 = vcmp.lt.f32.partialorder %v68826, 0.0004427343 (stack95)
        %v68828 = vsel /*vm=*/%vm68827, /*on_true_vy=*/%v68825, /*on_false_vx=*/%v68822 (stack96)
        %v68829 = vxor.u32 %v68828, 2147483648 (stack87)
        %vm68832 = vcmp.lt.f32.partialorder %v68829, 5.0 (stack86)
        %v68837 = vsel /*vm=*/%vm68832, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v68841 = vsel /*vm=*/%vm68832, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v68845 = vsel /*vm=*/%vm68832, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v68849 = vsel /*vm=*/%vm68832, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v68853 = vsel /*vm=*/%vm68832, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v68857 = vsel /*vm=*/%vm68832, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v68861 = vsel /*vm=*/%vm68832, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v68865 = vsel /*vm=*/%vm68832, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v68869 = vsel /*vm=*/%vm68832, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v68873 = vadd.f32 %v68829, -2.5 (stack82)
        %v68875 = vrsqrt.pop %v68829 (stack97)
        %v68876 = vmul.f32 %v68829, %v68875 (stack98)
        %vm68877 = vcmp.eq.f32.partialorder %v68829, inf (stack99)
        %v68878 = vsel /*vm=*/%vm68877, /*on_true_vy=*/%v68829, /*on_false_vx=*/%v68876 (stack100)
        %vm68879 = vcmp.eq.f32.partialorder %v68829, 0.0 (stack101)
        %v68880 = vand.u32 %v68829, 2147483648 (stack102)
        %v68881 = vsel /*vm=*/%vm68879, /*on_true_vy=*/%v68880, /*on_false_vx=*/%v68878 (stack103)
        %v68884 = vadd.f32 %v68881, -3.0 (stack82)
        %v68888 = vsel /*vm=*/%vm68832, /*on_true_vy=*/%v68873, /*on_false_vx=*/%v68884 (stack72)
        %v68892 = vmul.f32 %v68869, %v68888 (stack83)
        %v68896 = vadd.f32 %v68865, %v68892 (stack82)
        %v68900 = vmul.f32 %v68896, %v68888 (stack83)
        %v68904 = vadd.f32 %v68861, %v68900 (stack82)
        %v68908 = vmul.f32 %v68904, %v68888 (stack83)
        %v68912 = vadd.f32 %v68857, %v68908 (stack82)
        %v68916 = vmul.f32 %v68912, %v68888 (stack83)
        %v68920 = vadd.f32 %v68853, %v68916 (stack82)
        %v68924 = vmul.f32 %v68920, %v68888 (stack83)
        %v68928 = vadd.f32 %v68849, %v68924 (stack82)
        %v68932 = vmul.f32 %v68928, %v68888 (stack83)
        %v68936 = vadd.f32 %v68845, %v68932 (stack82)
        %v68940 = vmul.f32 %v68936, %v68888 (stack83)
        %v68944 = vadd.f32 %v68841, %v68940 (stack82)
        %v68948 = vmul.f32 %v68944, %v68888 (stack83)
        %v68952 = vadd.f32 %v68837, %v68948 (stack82)
        %v68956 = vmul.f32 %v68952, %v68803 (stack83)
        %v68960 = vsel /*vm=*/%vm68808, /*on_true_vy=*/%v68813, /*on_false_vx=*/%v68956 (stack72)
        %v68964 = vmul.f32 %v68960, 1.4140625 (stack83)
        %s68966 = scalar_lea.vmem %s280, 328 [#allocation0] (stack107)
        %v68967 = vpack.c.bf16 0.0, %v68964 (stack104)
        %68968 = vst [vmem:[%s68966] sm:$0xf] /*vst_source=*/%v68967 (stack105)
        %v68971 = vadd.s32 %v1868, %v67585 (stack65)
        %s68973 = smul.u32 128, %s27 (stack66)
        %v68974 = vlaneseq (stack67)
        %v68975 = vand.u32 %v68974, 127 (stack68)
        %v68976 = vstv %s68973 (stack69)
        %v68977 = vadd.s32 %v68975, %v68976 (stack70)
        %v68981 = vadd.s32 %v68971, %v68977 (stack65)
        %vm68985 = vcmp.lt.u32.totalorder %v68981, %v68971 (stack71)
        %vm68990 = vcmp.lt.u32.totalorder %v68971, %v1868 (stack71)
        %v68995 = vadd.s32 %v1855, %v67568 (stack65)
        %v68999 = vadd.s32 %v68995, 1 (stack65)
        %v69003 = vsel /*vm=*/%vm68990, /*on_true_vy=*/%v68999, /*on_false_vx=*/%v68995 (stack72)
        %v69007 = vadd.s32 %v69003, 1 (stack65)
        %v69011 = vsel /*vm=*/%vm68985, /*on_true_vy=*/%v69007, /*on_false_vx=*/%v69003 (stack72)
        %v69016 = vadd.s32 %v69011, %v10 (stack65)
        %v69020 = vadd.s32 %v68981, %v9 (stack65)
        %v69024 = vadd.s32 %v69016, %v69020 (stack65)
        %v69026 = vshll.u32 %v69020, 13 (stack73)
        %v69027 = vshrl.u32 %v69020, 19 (stack74)
        %v69028 = vor.u32 %v69026, %v69027 (stack75)
        %v69029 = vxor.u32 %v69024, %v69028 (stack76)
        %v69032 = vadd.s32 %v69024, %v69029 (stack65)
        %v69034 = vshll.u32 %v69029, 15 (stack73)
        %v69035 = vshrl.u32 %v69029, 17 (stack74)
        %v69036 = vor.u32 %v69034, %v69035 (stack75)
        %v69037 = vxor.u32 %v69032, %v69036 (stack76)
        %v69040 = vadd.s32 %v69032, %v69037 (stack65)
        %v69042 = vshll.u32 %v69037, 26 (stack73)
        %v69043 = vshrl.u32 %v69037, 6 (stack74)
        %v69044 = vor.u32 %v69042, %v69043 (stack75)
        %v69045 = vxor.u32 %v69040, %v69044 (stack76)
        %v69048 = vadd.s32 %v69040, %v69045 (stack65)
        %v69052 = vadd.s32 %v69048, %v9 (stack65)
        %v69054 = vshll.u32 %v69045, 6 (stack73)
        %v69055 = vshrl.u32 %v69045, 26 (stack74)
        %v69056 = vor.u32 %v69054, %v69055 (stack75)
        %v69057 = vxor.u32 %v69048, %v69056 (stack76)
        %v69060 = vadd.s32 %v69057, %v8 (stack65)
        %v69064 = vadd.s32 %v69060, 1 (stack65)
        %v69068 = vadd.s32 %v69052, %v69064 (stack65)
        %v69070 = vshll.u32 %v69064, 17 (stack73)
        %v69071 = vshrl.u32 %v69064, 15 (stack74)
        %v69072 = vor.u32 %v69070, %v69071 (stack75)
        %v69073 = vxor.u32 %v69068, %v69072 (stack76)
        %v69076 = vadd.s32 %v69068, %v69073 (stack65)
        %v69078 = vshll.u32 %v69073, 29 (stack73)
        %v69079 = vshrl.u32 %v69073, 3 (stack74)
        %v69080 = vor.u32 %v69078, %v69079 (stack75)
        %v69081 = vxor.u32 %v69076, %v69080 (stack76)
        %v69084 = vadd.s32 %v69076, %v69081 (stack65)
        %v69086 = vshll.u32 %v69081, 16 (stack73)
        %v69087 = vshrl.u32 %v69081, 16 (stack74)
        %v69088 = vor.u32 %v69086, %v69087 (stack75)
        %v69089 = vxor.u32 %v69084, %v69088 (stack76)
        %v69092 = vadd.s32 %v69084, %v69089 (stack65)
        %v69096 = vadd.s32 %v69092, %v8 (stack65)
        %v69098 = vshll.u32 %v69089, 24 (stack73)
        %v69099 = vshrl.u32 %v69089, 8 (stack74)
        %v69100 = vor.u32 %v69098, %v69099 (stack75)
        %v69101 = vxor.u32 %v69092, %v69100 (stack76)
        %v69104 = vadd.s32 %v69101, %v10 (stack65)
        %v69108 = vadd.s32 %v69104, 2 (stack65)
        %v69112 = vadd.s32 %v69096, %v69108 (stack65)
        %v69114 = vshll.u32 %v69108, 13 (stack73)
        %v69115 = vshrl.u32 %v69108, 19 (stack74)
        %v69116 = vor.u32 %v69114, %v69115 (stack75)
        %v69117 = vxor.u32 %v69112, %v69116 (stack76)
        %v69120 = vadd.s32 %v69112, %v69117 (stack65)
        %v69122 = vshll.u32 %v69117, 15 (stack73)
        %v69123 = vshrl.u32 %v69117, 17 (stack74)
        %v69124 = vor.u32 %v69122, %v69123 (stack75)
        %v69125 = vxor.u32 %v69120, %v69124 (stack76)
        %v69128 = vadd.s32 %v69120, %v69125 (stack65)
        %v69130 = vshll.u32 %v69125, 26 (stack73)
        %v69131 = vshrl.u32 %v69125, 6 (stack74)
        %v69132 = vor.u32 %v69130, %v69131 (stack75)
        %v69133 = vxor.u32 %v69128, %v69132 (stack76)
        %v69136 = vadd.s32 %v69128, %v69133 (stack65)
        %v69140 = vadd.s32 %v69136, %v10 (stack65)
        %v69142 = vshll.u32 %v69133, 6 (stack73)
        %v69143 = vshrl.u32 %v69133, 26 (stack74)
        %v69144 = vor.u32 %v69142, %v69143 (stack75)
        %v69145 = vxor.u32 %v69136, %v69144 (stack76)
        %v69148 = vadd.s32 %v69145, %v9 (stack65)
        %v69152 = vadd.s32 %v69148, 3 (stack65)
        %v69156 = vadd.s32 %v69140, %v69152 (stack65)
        %v69158 = vshll.u32 %v69152, 17 (stack73)
        %v69159 = vshrl.u32 %v69152, 15 (stack74)
        %v69160 = vor.u32 %v69158, %v69159 (stack75)
        %v69161 = vxor.u32 %v69156, %v69160 (stack76)
        %v69164 = vadd.s32 %v69156, %v69161 (stack65)
        %v69166 = vshll.u32 %v69161, 29 (stack73)
        %v69167 = vshrl.u32 %v69161, 3 (stack74)
        %v69168 = vor.u32 %v69166, %v69167 (stack75)
        %v69169 = vxor.u32 %v69164, %v69168 (stack76)
        %v69172 = vadd.s32 %v69164, %v69169 (stack65)
        %v69174 = vshll.u32 %v69169, 16 (stack73)
        %v69175 = vshrl.u32 %v69169, 16 (stack74)
        %v69176 = vor.u32 %v69174, %v69175 (stack75)
        %v69177 = vxor.u32 %v69172, %v69176 (stack76)
        %v69180 = vadd.s32 %v69172, %v69177 (stack65)
        %v69184 = vadd.s32 %v69180, %v9 (stack65)
        %v69186 = vshll.u32 %v69177, 24 (stack73)
        %v69187 = vshrl.u32 %v69177, 8 (stack74)
        %v69188 = vor.u32 %v69186, %v69187 (stack75)
        %v69189 = vxor.u32 %v69180, %v69188 (stack76)
        %v69192 = vadd.s32 %v69189, %v8 (stack65)
        %v69196 = vadd.s32 %v69192, 4 (stack65)
        %v69200 = vadd.s32 %v69184, %v69196 (stack65)
        %v69202 = vshll.u32 %v69196, 13 (stack73)
        %v69203 = vshrl.u32 %v69196, 19 (stack74)
        %v69204 = vor.u32 %v69202, %v69203 (stack75)
        %v69205 = vxor.u32 %v69200, %v69204 (stack76)
        %v69208 = vadd.s32 %v69200, %v69205 (stack65)
        %v69210 = vshll.u32 %v69205, 15 (stack73)
        %v69211 = vshrl.u32 %v69205, 17 (stack74)
        %v69212 = vor.u32 %v69210, %v69211 (stack75)
        %v69213 = vxor.u32 %v69208, %v69212 (stack76)
        %v69216 = vadd.s32 %v69208, %v69213 (stack65)
        %v69218 = vshll.u32 %v69213, 26 (stack73)
        %v69219 = vshrl.u32 %v69213, 6 (stack74)
        %v69220 = vor.u32 %v69218, %v69219 (stack75)
        %v69221 = vxor.u32 %v69216, %v69220 (stack76)
        %v69224 = vadd.s32 %v69216, %v69221 (stack65)
        %v69228 = vadd.s32 %v69224, %v8 (stack65)
        %v69230 = vshll.u32 %v69221, 6 (stack73)
        %v69231 = vshrl.u32 %v69221, 26 (stack74)
        %v69232 = vor.u32 %v69230, %v69231 (stack75)
        %v69233 = vxor.u32 %v69224, %v69232 (stack76)
        %v69236 = vadd.s32 %v69233, %v10 (stack65)
        %v69240 = vadd.s32 %v69236, 5 (stack65)
        %v69242 = vxor.u32 %v69228, %v69240 (stack76)
        %v69243 = vand.u32.u8 %v69242, 255 (stack77)
        %v69244 = vand.u32 %v69243, 65535 (stack78)
        %v69245 = vshrl.u32 %v69244, 1 (stack79)
        %v69246 = vor.u32 %v69245, 16256 (stack75)
        %v69247 = vand.u32.u16 %v69246, 65535 (stack80)
        %v69248 = vunpack.i.l.bf16 %v69247 (stack81)
        %v69252 = vadd.f32 %v69248, -1.0 (stack82)
        %v69256 = vmul.f32 %v69252, 2.0 (stack83)
        %v69260 = vadd.f32 %v69256, -0.99609375 (stack82)
        %v69264 = vmax.f32 -0.99609375, %v69260 (stack84)
        %v69266 = vand.u32 2147483647, %v69264 (stack85)
        %vm69269 = vcmp.eq.f32.partialorder %v69266, 1.0 (stack86)
        %v69274 = vmul.f32 %v69264, inf (stack83)
        %v69276 = vxor.u32 %v69264, 2147483648 (stack87)
        %v69279 = vmul.f32 %v69264, %v69276 (stack83)
        %v69281 = vadd.f32 %v69279, 1.0 (stack88)
        %v69282 = vlog2.pop %v69281 (stack89)
        %v69283 = vmul.f32 %v69282, 0.6931472 (stack90)
        %v69284 = vmul.f32 -0.5, %v69279 (stack91)
        %v69285 = vadd.f32 %v69284, 1.0 (stack92)
        %v69286 = vmul.f32 %v69285, %v69279 (stack93)
        %v69287 = vand.u32 2147483647, %v69279 (stack94)
        %vm69288 = vcmp.lt.f32.partialorder %v69287, 0.0004427343 (stack95)
        %v69289 = vsel /*vm=*/%vm69288, /*on_true_vy=*/%v69286, /*on_false_vx=*/%v69283 (stack96)
        %v69290 = vxor.u32 %v69289, 2147483648 (stack87)
        %vm69293 = vcmp.lt.f32.partialorder %v69290, 5.0 (stack86)
        %v69298 = vsel /*vm=*/%vm69293, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v69302 = vsel /*vm=*/%vm69293, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v69306 = vsel /*vm=*/%vm69293, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v69310 = vsel /*vm=*/%vm69293, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v69314 = vsel /*vm=*/%vm69293, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v69318 = vsel /*vm=*/%vm69293, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v69322 = vsel /*vm=*/%vm69293, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v69326 = vsel /*vm=*/%vm69293, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v69330 = vsel /*vm=*/%vm69293, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v69334 = vadd.f32 %v69290, -2.5 (stack82)
        %v69336 = vrsqrt.pop %v69290 (stack97)
        %v69337 = vmul.f32 %v69290, %v69336 (stack98)
        %vm69338 = vcmp.eq.f32.partialorder %v69290, inf (stack99)
        %v69339 = vsel /*vm=*/%vm69338, /*on_true_vy=*/%v69290, /*on_false_vx=*/%v69337 (stack100)
        %vm69340 = vcmp.eq.f32.partialorder %v69290, 0.0 (stack101)
        %v69341 = vand.u32 %v69290, 2147483648 (stack102)
        %v69342 = vsel /*vm=*/%vm69340, /*on_true_vy=*/%v69341, /*on_false_vx=*/%v69339 (stack103)
        %v69345 = vadd.f32 %v69342, -3.0 (stack82)
        %v69349 = vsel /*vm=*/%vm69293, /*on_true_vy=*/%v69334, /*on_false_vx=*/%v69345 (stack72)
        %v69353 = vmul.f32 %v69330, %v69349 (stack83)
        %v69357 = vadd.f32 %v69326, %v69353 (stack82)
        %v69361 = vmul.f32 %v69357, %v69349 (stack83)
        %v69365 = vadd.f32 %v69322, %v69361 (stack82)
        %v69369 = vmul.f32 %v69365, %v69349 (stack83)
        %v69373 = vadd.f32 %v69318, %v69369 (stack82)
        %v69377 = vmul.f32 %v69373, %v69349 (stack83)
        %v69381 = vadd.f32 %v69314, %v69377 (stack82)
        %v69385 = vmul.f32 %v69381, %v69349 (stack83)
        %v69389 = vadd.f32 %v69310, %v69385 (stack82)
        %v69393 = vmul.f32 %v69389, %v69349 (stack83)
        %v69397 = vadd.f32 %v69306, %v69393 (stack82)
        %v69401 = vmul.f32 %v69397, %v69349 (stack83)
        %v69405 = vadd.f32 %v69302, %v69401 (stack82)
        %v69409 = vmul.f32 %v69405, %v69349 (stack83)
        %v69413 = vadd.f32 %v69298, %v69409 (stack82)
        %v69417 = vmul.f32 %v69413, %v69264 (stack83)
        %v69421 = vsel /*vm=*/%vm69269, /*on_true_vy=*/%v69274, /*on_false_vx=*/%v69417 (stack72)
        %v69425 = vmul.f32 %v69421, 1.4140625 (stack83)
        %s69427 = scalar_lea.vmem %s280, 456 [#allocation0] (stack107)
        %v69428 = vpack.c.bf16 0.0, %v69425 (stack104)
        %69429 = vst [vmem:[%s69427] sm:$0xf] /*vst_source=*/%v69428 (stack105)
        %v69432 = vadd.s32 %v2355, %v67585 (stack65)
        %s69434 = smul.u32 128, %s27 (stack66)
        %v69435 = vlaneseq (stack67)
        %v69436 = vand.u32 %v69435, 127 (stack68)
        %v69437 = vstv %s69434 (stack69)
        %v69438 = vadd.s32 %v69436, %v69437 (stack70)
        %v69442 = vadd.s32 %v69432, %v69438 (stack65)
        %vm69446 = vcmp.lt.u32.totalorder %v69442, %v69432 (stack71)
        %vm69451 = vcmp.lt.u32.totalorder %v69432, %v2355 (stack71)
        %v69456 = vadd.s32 %v2342, %v67568 (stack65)
        %v69460 = vadd.s32 %v69456, 1 (stack65)
        %v69464 = vsel /*vm=*/%vm69451, /*on_true_vy=*/%v69460, /*on_false_vx=*/%v69456 (stack72)
        %v69468 = vadd.s32 %v69464, 1 (stack65)
        %v69472 = vsel /*vm=*/%vm69446, /*on_true_vy=*/%v69468, /*on_false_vx=*/%v69464 (stack72)
        %v69477 = vadd.s32 %v69472, %v10 (stack65)
        %v69481 = vadd.s32 %v69442, %v9 (stack65)
        %v69485 = vadd.s32 %v69477, %v69481 (stack65)
        %v69487 = vshll.u32 %v69481, 13 (stack73)
        %v69488 = vshrl.u32 %v69481, 19 (stack74)
        %v69489 = vor.u32 %v69487, %v69488 (stack75)
        %v69490 = vxor.u32 %v69485, %v69489 (stack76)
        %v69493 = vadd.s32 %v69485, %v69490 (stack65)
        %v69495 = vshll.u32 %v69490, 15 (stack73)
        %v69496 = vshrl.u32 %v69490, 17 (stack74)
        %v69497 = vor.u32 %v69495, %v69496 (stack75)
        %v69498 = vxor.u32 %v69493, %v69497 (stack76)
        %v69501 = vadd.s32 %v69493, %v69498 (stack65)
        %v69503 = vshll.u32 %v69498, 26 (stack73)
        %v69504 = vshrl.u32 %v69498, 6 (stack74)
        %v69505 = vor.u32 %v69503, %v69504 (stack75)
        %v69506 = vxor.u32 %v69501, %v69505 (stack76)
        %v69509 = vadd.s32 %v69501, %v69506 (stack65)
        %v69513 = vadd.s32 %v69509, %v9 (stack65)
        %v69515 = vshll.u32 %v69506, 6 (stack73)
        %v69516 = vshrl.u32 %v69506, 26 (stack74)
        %v69517 = vor.u32 %v69515, %v69516 (stack75)
        %v69518 = vxor.u32 %v69509, %v69517 (stack76)
        %v69521 = vadd.s32 %v69518, %v8 (stack65)
        %v69525 = vadd.s32 %v69521, 1 (stack65)
        %v69529 = vadd.s32 %v69513, %v69525 (stack65)
        %v69531 = vshll.u32 %v69525, 17 (stack73)
        %v69532 = vshrl.u32 %v69525, 15 (stack74)
        %v69533 = vor.u32 %v69531, %v69532 (stack75)
        %v69534 = vxor.u32 %v69529, %v69533 (stack76)
        %v69537 = vadd.s32 %v69529, %v69534 (stack65)
        %v69539 = vshll.u32 %v69534, 29 (stack73)
        %v69540 = vshrl.u32 %v69534, 3 (stack74)
        %v69541 = vor.u32 %v69539, %v69540 (stack75)
        %v69542 = vxor.u32 %v69537, %v69541 (stack76)
        %v69545 = vadd.s32 %v69537, %v69542 (stack65)
        %v69547 = vshll.u32 %v69542, 16 (stack73)
        %v69548 = vshrl.u32 %v69542, 16 (stack74)
        %v69549 = vor.u32 %v69547, %v69548 (stack75)
        %v69550 = vxor.u32 %v69545, %v69549 (stack76)
        %v69553 = vadd.s32 %v69545, %v69550 (stack65)
        %v69557 = vadd.s32 %v69553, %v8 (stack65)
        %v69559 = vshll.u32 %v69550, 24 (stack73)
        %v69560 = vshrl.u32 %v69550, 8 (stack74)
        %v69561 = vor.u32 %v69559, %v69560 (stack75)
        %v69562 = vxor.u32 %v69553, %v69561 (stack76)
        %v69565 = vadd.s32 %v69562, %v10 (stack65)
        %v69569 = vadd.s32 %v69565, 2 (stack65)
        %v69573 = vadd.s32 %v69557, %v69569 (stack65)
        %v69575 = vshll.u32 %v69569, 13 (stack73)
        %v69576 = vshrl.u32 %v69569, 19 (stack74)
        %v69577 = vor.u32 %v69575, %v69576 (stack75)
        %v69578 = vxor.u32 %v69573, %v69577 (stack76)
        %v69581 = vadd.s32 %v69573, %v69578 (stack65)
        %v69583 = vshll.u32 %v69578, 15 (stack73)
        %v69584 = vshrl.u32 %v69578, 17 (stack74)
        %v69585 = vor.u32 %v69583, %v69584 (stack75)
        %v69586 = vxor.u32 %v69581, %v69585 (stack76)
        %v69589 = vadd.s32 %v69581, %v69586 (stack65)
        %v69591 = vshll.u32 %v69586, 26 (stack73)
        %v69592 = vshrl.u32 %v69586, 6 (stack74)
        %v69593 = vor.u32 %v69591, %v69592 (stack75)
        %v69594 = vxor.u32 %v69589, %v69593 (stack76)
        %v69597 = vadd.s32 %v69589, %v69594 (stack65)
        %v69601 = vadd.s32 %v69597, %v10 (stack65)
        %v69603 = vshll.u32 %v69594, 6 (stack73)
        %v69604 = vshrl.u32 %v69594, 26 (stack74)
        %v69605 = vor.u32 %v69603, %v69604 (stack75)
        %v69606 = vxor.u32 %v69597, %v69605 (stack76)
        %v69609 = vadd.s32 %v69606, %v9 (stack65)
        %v69613 = vadd.s32 %v69609, 3 (stack65)
        %v69617 = vadd.s32 %v69601, %v69613 (stack65)
        %v69619 = vshll.u32 %v69613, 17 (stack73)
        %v69620 = vshrl.u32 %v69613, 15 (stack74)
        %v69621 = vor.u32 %v69619, %v69620 (stack75)
        %v69622 = vxor.u32 %v69617, %v69621 (stack76)
        %v69625 = vadd.s32 %v69617, %v69622 (stack65)
        %v69627 = vshll.u32 %v69622, 29 (stack73)
        %v69628 = vshrl.u32 %v69622, 3 (stack74)
        %v69629 = vor.u32 %v69627, %v69628 (stack75)
        %v69630 = vxor.u32 %v69625, %v69629 (stack76)
        %v69633 = vadd.s32 %v69625, %v69630 (stack65)
        %v69635 = vshll.u32 %v69630, 16 (stack73)
        %v69636 = vshrl.u32 %v69630, 16 (stack74)
        %v69637 = vor.u32 %v69635, %v69636 (stack75)
        %v69638 = vxor.u32 %v69633, %v69637 (stack76)
        %v69641 = vadd.s32 %v69633, %v69638 (stack65)
        %v69645 = vadd.s32 %v69641, %v9 (stack65)
        %v69647 = vshll.u32 %v69638, 24 (stack73)
        %v69648 = vshrl.u32 %v69638, 8 (stack74)
        %v69649 = vor.u32 %v69647, %v69648 (stack75)
        %v69650 = vxor.u32 %v69641, %v69649 (stack76)
        %v69653 = vadd.s32 %v69650, %v8 (stack65)
        %v69657 = vadd.s32 %v69653, 4 (stack65)
        %v69661 = vadd.s32 %v69645, %v69657 (stack65)
        %v69663 = vshll.u32 %v69657, 13 (stack73)
        %v69664 = vshrl.u32 %v69657, 19 (stack74)
        %v69665 = vor.u32 %v69663, %v69664 (stack75)
        %v69666 = vxor.u32 %v69661, %v69665 (stack76)
        %v69669 = vadd.s32 %v69661, %v69666 (stack65)
        %v69671 = vshll.u32 %v69666, 15 (stack73)
        %v69672 = vshrl.u32 %v69666, 17 (stack74)
        %v69673 = vor.u32 %v69671, %v69672 (stack75)
        %v69674 = vxor.u32 %v69669, %v69673 (stack76)
        %v69677 = vadd.s32 %v69669, %v69674 (stack65)
        %v69679 = vshll.u32 %v69674, 26 (stack73)
        %v69680 = vshrl.u32 %v69674, 6 (stack74)
        %v69681 = vor.u32 %v69679, %v69680 (stack75)
        %v69682 = vxor.u32 %v69677, %v69681 (stack76)
        %v69685 = vadd.s32 %v69677, %v69682 (stack65)
        %v69689 = vadd.s32 %v69685, %v8 (stack65)
        %v69691 = vshll.u32 %v69682, 6 (stack73)
        %v69692 = vshrl.u32 %v69682, 26 (stack74)
        %v69693 = vor.u32 %v69691, %v69692 (stack75)
        %v69694 = vxor.u32 %v69685, %v69693 (stack76)
        %v69697 = vadd.s32 %v69694, %v10 (stack65)
        %v69701 = vadd.s32 %v69697, 5 (stack65)
        %v69703 = vxor.u32 %v69689, %v69701 (stack76)
        %v69704 = vand.u32.u8 %v69703, 255 (stack77)
        %v69705 = vand.u32 %v69704, 65535 (stack78)
        %v69706 = vshrl.u32 %v69705, 1 (stack79)
        %v69707 = vor.u32 %v69706, 16256 (stack75)
        %v69708 = vand.u32.u16 %v69707, 65535 (stack80)
        %v69709 = vunpack.i.l.bf16 %v69708 (stack81)
        %v69713 = vadd.f32 %v69709, -1.0 (stack82)
        %v69717 = vmul.f32 %v69713, 2.0 (stack83)
        %v69721 = vadd.f32 %v69717, -0.99609375 (stack82)
        %v69725 = vmax.f32 -0.99609375, %v69721 (stack84)
        %v69727 = vand.u32 2147483647, %v69725 (stack85)
        %vm69730 = vcmp.eq.f32.partialorder %v69727, 1.0 (stack86)
        %v69735 = vmul.f32 %v69725, inf (stack83)
        %v69737 = vxor.u32 %v69725, 2147483648 (stack87)
        %v69740 = vmul.f32 %v69725, %v69737 (stack83)
        %v69742 = vadd.f32 %v69740, 1.0 (stack88)
        %v69743 = vlog2.pop %v69742 (stack89)
        %v69744 = vmul.f32 %v69743, 0.6931472 (stack90)
        %v69745 = vmul.f32 -0.5, %v69740 (stack91)
        %v69746 = vadd.f32 %v69745, 1.0 (stack92)
        %v69747 = vmul.f32 %v69746, %v69740 (stack93)
        %v69748 = vand.u32 2147483647, %v69740 (stack94)
        %vm69749 = vcmp.lt.f32.partialorder %v69748, 0.0004427343 (stack95)
        %v69750 = vsel /*vm=*/%vm69749, /*on_true_vy=*/%v69747, /*on_false_vx=*/%v69744 (stack96)
        %v69751 = vxor.u32 %v69750, 2147483648 (stack87)
        %vm69754 = vcmp.lt.f32.partialorder %v69751, 5.0 (stack86)
        %v69759 = vsel /*vm=*/%vm69754, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v69763 = vsel /*vm=*/%vm69754, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v69767 = vsel /*vm=*/%vm69754, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v69771 = vsel /*vm=*/%vm69754, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v69775 = vsel /*vm=*/%vm69754, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v69779 = vsel /*vm=*/%vm69754, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v69783 = vsel /*vm=*/%vm69754, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v69787 = vsel /*vm=*/%vm69754, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v69791 = vsel /*vm=*/%vm69754, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v69795 = vadd.f32 %v69751, -2.5 (stack82)
        %v69797 = vrsqrt.pop %v69751 (stack97)
        %v69798 = vmul.f32 %v69751, %v69797 (stack98)
        %vm69799 = vcmp.eq.f32.partialorder %v69751, inf (stack99)
        %v69800 = vsel /*vm=*/%vm69799, /*on_true_vy=*/%v69751, /*on_false_vx=*/%v69798 (stack100)
        %vm69801 = vcmp.eq.f32.partialorder %v69751, 0.0 (stack101)
        %v69802 = vand.u32 %v69751, 2147483648 (stack102)
        %v69803 = vsel /*vm=*/%vm69801, /*on_true_vy=*/%v69802, /*on_false_vx=*/%v69800 (stack103)
        %v69806 = vadd.f32 %v69803, -3.0 (stack82)
        %v69810 = vsel /*vm=*/%vm69754, /*on_true_vy=*/%v69795, /*on_false_vx=*/%v69806 (stack72)
        %v69814 = vmul.f32 %v69791, %v69810 (stack83)
        %v69818 = vadd.f32 %v69787, %v69814 (stack82)
        %v69822 = vmul.f32 %v69818, %v69810 (stack83)
        %v69826 = vadd.f32 %v69783, %v69822 (stack82)
        %v69830 = vmul.f32 %v69826, %v69810 (stack83)
        %v69834 = vadd.f32 %v69779, %v69830 (stack82)
        %v69838 = vmul.f32 %v69834, %v69810 (stack83)
        %v69842 = vadd.f32 %v69775, %v69838 (stack82)
        %v69846 = vmul.f32 %v69842, %v69810 (stack83)
        %v69850 = vadd.f32 %v69771, %v69846 (stack82)
        %v69854 = vmul.f32 %v69850, %v69810 (stack83)
        %v69858 = vadd.f32 %v69767, %v69854 (stack82)
        %v69862 = vmul.f32 %v69858, %v69810 (stack83)
        %v69866 = vadd.f32 %v69763, %v69862 (stack82)
        %v69870 = vmul.f32 %v69866, %v69810 (stack83)
        %v69874 = vadd.f32 %v69759, %v69870 (stack82)
        %v69878 = vmul.f32 %v69874, %v69725 (stack83)
        %v69882 = vsel /*vm=*/%vm69730, /*on_true_vy=*/%v69735, /*on_false_vx=*/%v69878 (stack72)
        %v69886 = vmul.f32 %v69882, 1.4140625 (stack83)
        %s69888 = scalar_lea.vmem %s280, 584 [#allocation0] (stack107)
        %v69889 = vpack.c.bf16 0.0, %v69886 (stack104)
        %69890 = vst [vmem:[%s69888] sm:$0xf] /*vst_source=*/%v69889 (stack105)
        %v69893 = vadd.s32 %v2842, %v67585 (stack65)
        %s69895 = smul.u32 128, %s27 (stack66)
        %v69896 = vlaneseq (stack67)
        %v69897 = vand.u32 %v69896, 127 (stack68)
        %v69898 = vstv %s69895 (stack69)
        %v69899 = vadd.s32 %v69897, %v69898 (stack70)
        %v69903 = vadd.s32 %v69893, %v69899 (stack65)
        %vm69907 = vcmp.lt.u32.totalorder %v69903, %v69893 (stack71)
        %vm69912 = vcmp.lt.u32.totalorder %v69893, %v2842 (stack71)
        %v69917 = vadd.s32 %v2829, %v67568 (stack65)
        %v69921 = vadd.s32 %v69917, 1 (stack65)
        %v69925 = vsel /*vm=*/%vm69912, /*on_true_vy=*/%v69921, /*on_false_vx=*/%v69917 (stack72)
        %v69929 = vadd.s32 %v69925, 1 (stack65)
        %v69933 = vsel /*vm=*/%vm69907, /*on_true_vy=*/%v69929, /*on_false_vx=*/%v69925 (stack72)
        %v69938 = vadd.s32 %v69933, %v10 (stack65)
        %v69942 = vadd.s32 %v69903, %v9 (stack65)
        %v69946 = vadd.s32 %v69938, %v69942 (stack65)
        %v69948 = vshll.u32 %v69942, 13 (stack73)
        %v69949 = vshrl.u32 %v69942, 19 (stack74)
        %v69950 = vor.u32 %v69948, %v69949 (stack75)
        %v69951 = vxor.u32 %v69946, %v69950 (stack76)
        %v69954 = vadd.s32 %v69946, %v69951 (stack65)
        %v69956 = vshll.u32 %v69951, 15 (stack73)
        %v69957 = vshrl.u32 %v69951, 17 (stack74)
        %v69958 = vor.u32 %v69956, %v69957 (stack75)
        %v69959 = vxor.u32 %v69954, %v69958 (stack76)
        %v69962 = vadd.s32 %v69954, %v69959 (stack65)
        %v69964 = vshll.u32 %v69959, 26 (stack73)
        %v69965 = vshrl.u32 %v69959, 6 (stack74)
        %v69966 = vor.u32 %v69964, %v69965 (stack75)
        %v69967 = vxor.u32 %v69962, %v69966 (stack76)
        %v69970 = vadd.s32 %v69962, %v69967 (stack65)
        %v69974 = vadd.s32 %v69970, %v9 (stack65)
        %v69976 = vshll.u32 %v69967, 6 (stack73)
        %v69977 = vshrl.u32 %v69967, 26 (stack74)
        %v69978 = vor.u32 %v69976, %v69977 (stack75)
        %v69979 = vxor.u32 %v69970, %v69978 (stack76)
        %v69982 = vadd.s32 %v69979, %v8 (stack65)
        %v69986 = vadd.s32 %v69982, 1 (stack65)
        %v69990 = vadd.s32 %v69974, %v69986 (stack65)
        %v69992 = vshll.u32 %v69986, 17 (stack73)
        %v69993 = vshrl.u32 %v69986, 15 (stack74)
        %v69994 = vor.u32 %v69992, %v69993 (stack75)
        %v69995 = vxor.u32 %v69990, %v69994 (stack76)
        %v69998 = vadd.s32 %v69990, %v69995 (stack65)
        %v70000 = vshll.u32 %v69995, 29 (stack73)
        %v70001 = vshrl.u32 %v69995, 3 (stack74)
        %v70002 = vor.u32 %v70000, %v70001 (stack75)
        %v70003 = vxor.u32 %v69998, %v70002 (stack76)
        %v70006 = vadd.s32 %v69998, %v70003 (stack65)
        %v70008 = vshll.u32 %v70003, 16 (stack73)
        %v70009 = vshrl.u32 %v70003, 16 (stack74)
        %v70010 = vor.u32 %v70008, %v70009 (stack75)
        %v70011 = vxor.u32 %v70006, %v70010 (stack76)
        %v70014 = vadd.s32 %v70006, %v70011 (stack65)
        %v70018 = vadd.s32 %v70014, %v8 (stack65)
        %v70020 = vshll.u32 %v70011, 24 (stack73)
        %v70021 = vshrl.u32 %v70011, 8 (stack74)
        %v70022 = vor.u32 %v70020, %v70021 (stack75)
        %v70023 = vxor.u32 %v70014, %v70022 (stack76)
        %v70026 = vadd.s32 %v70023, %v10 (stack65)
        %v70030 = vadd.s32 %v70026, 2 (stack65)
        %v70034 = vadd.s32 %v70018, %v70030 (stack65)
        %v70036 = vshll.u32 %v70030, 13 (stack73)
        %v70037 = vshrl.u32 %v70030, 19 (stack74)
        %v70038 = vor.u32 %v70036, %v70037 (stack75)
        %v70039 = vxor.u32 %v70034, %v70038 (stack76)
        %v70042 = vadd.s32 %v70034, %v70039 (stack65)
        %v70044 = vshll.u32 %v70039, 15 (stack73)
        %v70045 = vshrl.u32 %v70039, 17 (stack74)
        %v70046 = vor.u32 %v70044, %v70045 (stack75)
        %v70047 = vxor.u32 %v70042, %v70046 (stack76)
        %v70050 = vadd.s32 %v70042, %v70047 (stack65)
        %v70052 = vshll.u32 %v70047, 26 (stack73)
        %v70053 = vshrl.u32 %v70047, 6 (stack74)
        %v70054 = vor.u32 %v70052, %v70053 (stack75)
        %v70055 = vxor.u32 %v70050, %v70054 (stack76)
        %v70058 = vadd.s32 %v70050, %v70055 (stack65)
        %v70062 = vadd.s32 %v70058, %v10 (stack65)
        %v70064 = vshll.u32 %v70055, 6 (stack73)
        %v70065 = vshrl.u32 %v70055, 26 (stack74)
        %v70066 = vor.u32 %v70064, %v70065 (stack75)
        %v70067 = vxor.u32 %v70058, %v70066 (stack76)
        %v70070 = vadd.s32 %v70067, %v9 (stack65)
        %v70074 = vadd.s32 %v70070, 3 (stack65)
        %v70078 = vadd.s32 %v70062, %v70074 (stack65)
        %v70080 = vshll.u32 %v70074, 17 (stack73)
        %v70081 = vshrl.u32 %v70074, 15 (stack74)
        %v70082 = vor.u32 %v70080, %v70081 (stack75)
        %v70083 = vxor.u32 %v70078, %v70082 (stack76)
        %v70086 = vadd.s32 %v70078, %v70083 (stack65)
        %v70088 = vshll.u32 %v70083, 29 (stack73)
        %v70089 = vshrl.u32 %v70083, 3 (stack74)
        %v70090 = vor.u32 %v70088, %v70089 (stack75)
        %v70091 = vxor.u32 %v70086, %v70090 (stack76)
        %v70094 = vadd.s32 %v70086, %v70091 (stack65)
        %v70096 = vshll.u32 %v70091, 16 (stack73)
        %v70097 = vshrl.u32 %v70091, 16 (stack74)
        %v70098 = vor.u32 %v70096, %v70097 (stack75)
        %v70099 = vxor.u32 %v70094, %v70098 (stack76)
        %v70102 = vadd.s32 %v70094, %v70099 (stack65)
        %v70106 = vadd.s32 %v70102, %v9 (stack65)
        %v70108 = vshll.u32 %v70099, 24 (stack73)
        %v70109 = vshrl.u32 %v70099, 8 (stack74)
        %v70110 = vor.u32 %v70108, %v70109 (stack75)
        %v70111 = vxor.u32 %v70102, %v70110 (stack76)
        %v70114 = vadd.s32 %v70111, %v8 (stack65)
        %v70118 = vadd.s32 %v70114, 4 (stack65)
        %v70122 = vadd.s32 %v70106, %v70118 (stack65)
        %v70124 = vshll.u32 %v70118, 13 (stack73)
        %v70125 = vshrl.u32 %v70118, 19 (stack74)
        %v70126 = vor.u32 %v70124, %v70125 (stack75)
        %v70127 = vxor.u32 %v70122, %v70126 (stack76)
        %v70130 = vadd.s32 %v70122, %v70127 (stack65)
        %v70132 = vshll.u32 %v70127, 15 (stack73)
        %v70133 = vshrl.u32 %v70127, 17 (stack74)
        %v70134 = vor.u32 %v70132, %v70133 (stack75)
        %v70135 = vxor.u32 %v70130, %v70134 (stack76)
        %v70138 = vadd.s32 %v70130, %v70135 (stack65)
        %v70140 = vshll.u32 %v70135, 26 (stack73)
        %v70141 = vshrl.u32 %v70135, 6 (stack74)
        %v70142 = vor.u32 %v70140, %v70141 (stack75)
        %v70143 = vxor.u32 %v70138, %v70142 (stack76)
        %v70146 = vadd.s32 %v70138, %v70143 (stack65)
        %v70150 = vadd.s32 %v70146, %v8 (stack65)
        %v70152 = vshll.u32 %v70143, 6 (stack73)
        %v70153 = vshrl.u32 %v70143, 26 (stack74)
        %v70154 = vor.u32 %v70152, %v70153 (stack75)
        %v70155 = vxor.u32 %v70146, %v70154 (stack76)
        %v70158 = vadd.s32 %v70155, %v10 (stack65)
        %v70162 = vadd.s32 %v70158, 5 (stack65)
        %v70164 = vxor.u32 %v70150, %v70162 (stack76)
        %v70165 = vand.u32.u8 %v70164, 255 (stack77)
        %v70166 = vand.u32 %v70165, 65535 (stack78)
        %v70167 = vshrl.u32 %v70166, 1 (stack79)
        %v70168 = vor.u32 %v70167, 16256 (stack75)
        %v70169 = vand.u32.u16 %v70168, 65535 (stack80)
        %v70170 = vunpack.i.l.bf16 %v70169 (stack81)
        %v70174 = vadd.f32 %v70170, -1.0 (stack82)
        %v70178 = vmul.f32 %v70174, 2.0 (stack83)
        %v70182 = vadd.f32 %v70178, -0.99609375 (stack82)
        %v70186 = vmax.f32 -0.99609375, %v70182 (stack84)
        %v70188 = vand.u32 2147483647, %v70186 (stack85)
        %vm70191 = vcmp.eq.f32.partialorder %v70188, 1.0 (stack86)
        %v70196 = vmul.f32 %v70186, inf (stack83)
        %v70198 = vxor.u32 %v70186, 2147483648 (stack87)
        %v70201 = vmul.f32 %v70186, %v70198 (stack83)
        %v70203 = vadd.f32 %v70201, 1.0 (stack88)
        %v70204 = vlog2.pop %v70203 (stack89)
        %v70205 = vmul.f32 %v70204, 0.6931472 (stack90)
        %v70206 = vmul.f32 -0.5, %v70201 (stack91)
        %v70207 = vadd.f32 %v70206, 1.0 (stack92)
        %v70208 = vmul.f32 %v70207, %v70201 (stack93)
        %v70209 = vand.u32 2147483647, %v70201 (stack94)
        %vm70210 = vcmp.lt.f32.partialorder %v70209, 0.0004427343 (stack95)
        %v70211 = vsel /*vm=*/%vm70210, /*on_true_vy=*/%v70208, /*on_false_vx=*/%v70205 (stack96)
        %v70212 = vxor.u32 %v70211, 2147483648 (stack87)
        %vm70215 = vcmp.lt.f32.partialorder %v70212, 5.0 (stack86)
        %v70220 = vsel /*vm=*/%vm70215, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v70224 = vsel /*vm=*/%vm70215, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v70228 = vsel /*vm=*/%vm70215, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v70232 = vsel /*vm=*/%vm70215, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v70236 = vsel /*vm=*/%vm70215, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v70240 = vsel /*vm=*/%vm70215, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v70244 = vsel /*vm=*/%vm70215, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v70248 = vsel /*vm=*/%vm70215, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v70252 = vsel /*vm=*/%vm70215, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v70256 = vadd.f32 %v70212, -2.5 (stack82)
        %v70258 = vrsqrt.pop %v70212 (stack97)
        %v70259 = vmul.f32 %v70212, %v70258 (stack98)
        %vm70260 = vcmp.eq.f32.partialorder %v70212, inf (stack99)
        %v70261 = vsel /*vm=*/%vm70260, /*on_true_vy=*/%v70212, /*on_false_vx=*/%v70259 (stack100)
        %vm70262 = vcmp.eq.f32.partialorder %v70212, 0.0 (stack101)
        %v70263 = vand.u32 %v70212, 2147483648 (stack102)
        %v70264 = vsel /*vm=*/%vm70262, /*on_true_vy=*/%v70263, /*on_false_vx=*/%v70261 (stack103)
        %v70267 = vadd.f32 %v70264, -3.0 (stack82)
        %v70271 = vsel /*vm=*/%vm70215, /*on_true_vy=*/%v70256, /*on_false_vx=*/%v70267 (stack72)
        %v70275 = vmul.f32 %v70252, %v70271 (stack83)
        %v70279 = vadd.f32 %v70248, %v70275 (stack82)
        %v70283 = vmul.f32 %v70279, %v70271 (stack83)
        %v70287 = vadd.f32 %v70244, %v70283 (stack82)
        %v70291 = vmul.f32 %v70287, %v70271 (stack83)
        %v70295 = vadd.f32 %v70240, %v70291 (stack82)
        %v70299 = vmul.f32 %v70295, %v70271 (stack83)
        %v70303 = vadd.f32 %v70236, %v70299 (stack82)
        %v70307 = vmul.f32 %v70303, %v70271 (stack83)
        %v70311 = vadd.f32 %v70232, %v70307 (stack82)
        %v70315 = vmul.f32 %v70311, %v70271 (stack83)
        %v70319 = vadd.f32 %v70228, %v70315 (stack82)
        %v70323 = vmul.f32 %v70319, %v70271 (stack83)
        %v70327 = vadd.f32 %v70224, %v70323 (stack82)
        %v70331 = vmul.f32 %v70327, %v70271 (stack83)
        %v70335 = vadd.f32 %v70220, %v70331 (stack82)
        %v70339 = vmul.f32 %v70335, %v70186 (stack83)
        %v70343 = vsel /*vm=*/%vm70191, /*on_true_vy=*/%v70196, /*on_false_vx=*/%v70339 (stack72)
        %v70347 = vmul.f32 %v70343, 1.4140625 (stack83)
        %s70349 = scalar_lea.vmem %s280, 712 [#allocation0] (stack107)
        %v70350 = vpack.c.bf16 0.0, %v70347 (stack104)
        %70351 = vst [vmem:[%s70349] sm:$0xf] /*vst_source=*/%v70350 (stack105)
        %v70354 = vadd.s32 %v3329, %v67585 (stack65)
        %s70356 = smul.u32 128, %s27 (stack66)
        %v70357 = vlaneseq (stack67)
        %v70358 = vand.u32 %v70357, 127 (stack68)
        %v70359 = vstv %s70356 (stack69)
        %v70360 = vadd.s32 %v70358, %v70359 (stack70)
        %v70364 = vadd.s32 %v70354, %v70360 (stack65)
        %vm70368 = vcmp.lt.u32.totalorder %v70364, %v70354 (stack71)
        %vm70373 = vcmp.lt.u32.totalorder %v70354, %v3329 (stack71)
        %v70378 = vadd.s32 %v3316, %v67568 (stack65)
        %v70382 = vadd.s32 %v70378, 1 (stack65)
        %v70386 = vsel /*vm=*/%vm70373, /*on_true_vy=*/%v70382, /*on_false_vx=*/%v70378 (stack72)
        %v70390 = vadd.s32 %v70386, 1 (stack65)
        %v70394 = vsel /*vm=*/%vm70368, /*on_true_vy=*/%v70390, /*on_false_vx=*/%v70386 (stack72)
        %v70399 = vadd.s32 %v70394, %v10 (stack65)
        %v70403 = vadd.s32 %v70364, %v9 (stack65)
        %v70407 = vadd.s32 %v70399, %v70403 (stack65)
        %v70409 = vshll.u32 %v70403, 13 (stack73)
        %v70410 = vshrl.u32 %v70403, 19 (stack74)
        %v70411 = vor.u32 %v70409, %v70410 (stack75)
        %v70412 = vxor.u32 %v70407, %v70411 (stack76)
        %v70415 = vadd.s32 %v70407, %v70412 (stack65)
        %v70417 = vshll.u32 %v70412, 15 (stack73)
        %v70418 = vshrl.u32 %v70412, 17 (stack74)
        %v70419 = vor.u32 %v70417, %v70418 (stack75)
        %v70420 = vxor.u32 %v70415, %v70419 (stack76)
        %v70423 = vadd.s32 %v70415, %v70420 (stack65)
        %v70425 = vshll.u32 %v70420, 26 (stack73)
        %v70426 = vshrl.u32 %v70420, 6 (stack74)
        %v70427 = vor.u32 %v70425, %v70426 (stack75)
        %v70428 = vxor.u32 %v70423, %v70427 (stack76)
        %v70431 = vadd.s32 %v70423, %v70428 (stack65)
        %v70435 = vadd.s32 %v70431, %v9 (stack65)
        %v70437 = vshll.u32 %v70428, 6 (stack73)
        %v70438 = vshrl.u32 %v70428, 26 (stack74)
        %v70439 = vor.u32 %v70437, %v70438 (stack75)
        %v70440 = vxor.u32 %v70431, %v70439 (stack76)
        %v70443 = vadd.s32 %v70440, %v8 (stack65)
        %v70447 = vadd.s32 %v70443, 1 (stack65)
        %v70451 = vadd.s32 %v70435, %v70447 (stack65)
        %v70453 = vshll.u32 %v70447, 17 (stack73)
        %v70454 = vshrl.u32 %v70447, 15 (stack74)
        %v70455 = vor.u32 %v70453, %v70454 (stack75)
        %v70456 = vxor.u32 %v70451, %v70455 (stack76)
        %v70459 = vadd.s32 %v70451, %v70456 (stack65)
        %v70461 = vshll.u32 %v70456, 29 (stack73)
        %v70462 = vshrl.u32 %v70456, 3 (stack74)
        %v70463 = vor.u32 %v70461, %v70462 (stack75)
        %v70464 = vxor.u32 %v70459, %v70463 (stack76)
        %v70467 = vadd.s32 %v70459, %v70464 (stack65)
        %v70469 = vshll.u32 %v70464, 16 (stack73)
        %v70470 = vshrl.u32 %v70464, 16 (stack74)
        %v70471 = vor.u32 %v70469, %v70470 (stack75)
        %v70472 = vxor.u32 %v70467, %v70471 (stack76)
        %v70475 = vadd.s32 %v70467, %v70472 (stack65)
        %v70479 = vadd.s32 %v70475, %v8 (stack65)
        %v70481 = vshll.u32 %v70472, 24 (stack73)
        %v70482 = vshrl.u32 %v70472, 8 (stack74)
        %v70483 = vor.u32 %v70481, %v70482 (stack75)
        %v70484 = vxor.u32 %v70475, %v70483 (stack76)
        %v70487 = vadd.s32 %v70484, %v10 (stack65)
        %v70491 = vadd.s32 %v70487, 2 (stack65)
        %v70495 = vadd.s32 %v70479, %v70491 (stack65)
        %v70497 = vshll.u32 %v70491, 13 (stack73)
        %v70498 = vshrl.u32 %v70491, 19 (stack74)
        %v70499 = vor.u32 %v70497, %v70498 (stack75)
        %v70500 = vxor.u32 %v70495, %v70499 (stack76)
        %v70503 = vadd.s32 %v70495, %v70500 (stack65)
        %v70505 = vshll.u32 %v70500, 15 (stack73)
        %v70506 = vshrl.u32 %v70500, 17 (stack74)
        %v70507 = vor.u32 %v70505, %v70506 (stack75)
        %v70508 = vxor.u32 %v70503, %v70507 (stack76)
        %v70511 = vadd.s32 %v70503, %v70508 (stack65)
        %v70513 = vshll.u32 %v70508, 26 (stack73)
        %v70514 = vshrl.u32 %v70508, 6 (stack74)
        %v70515 = vor.u32 %v70513, %v70514 (stack75)
        %v70516 = vxor.u32 %v70511, %v70515 (stack76)
        %v70519 = vadd.s32 %v70511, %v70516 (stack65)
        %v70523 = vadd.s32 %v70519, %v10 (stack65)
        %v70525 = vshll.u32 %v70516, 6 (stack73)
        %v70526 = vshrl.u32 %v70516, 26 (stack74)
        %v70527 = vor.u32 %v70525, %v70526 (stack75)
        %v70528 = vxor.u32 %v70519, %v70527 (stack76)
        %v70531 = vadd.s32 %v70528, %v9 (stack65)
        %v70535 = vadd.s32 %v70531, 3 (stack65)
        %v70539 = vadd.s32 %v70523, %v70535 (stack65)
        %v70541 = vshll.u32 %v70535, 17 (stack73)
        %v70542 = vshrl.u32 %v70535, 15 (stack74)
        %v70543 = vor.u32 %v70541, %v70542 (stack75)
        %v70544 = vxor.u32 %v70539, %v70543 (stack76)
        %v70547 = vadd.s32 %v70539, %v70544 (stack65)
        %v70549 = vshll.u32 %v70544, 29 (stack73)
        %v70550 = vshrl.u32 %v70544, 3 (stack74)
        %v70551 = vor.u32 %v70549, %v70550 (stack75)
        %v70552 = vxor.u32 %v70547, %v70551 (stack76)
        %v70555 = vadd.s32 %v70547, %v70552 (stack65)
        %v70557 = vshll.u32 %v70552, 16 (stack73)
        %v70558 = vshrl.u32 %v70552, 16 (stack74)
        %v70559 = vor.u32 %v70557, %v70558 (stack75)
        %v70560 = vxor.u32 %v70555, %v70559 (stack76)
        %v70563 = vadd.s32 %v70555, %v70560 (stack65)
        %v70567 = vadd.s32 %v70563, %v9 (stack65)
        %v70569 = vshll.u32 %v70560, 24 (stack73)
        %v70570 = vshrl.u32 %v70560, 8 (stack74)
        %v70571 = vor.u32 %v70569, %v70570 (stack75)
        %v70572 = vxor.u32 %v70563, %v70571 (stack76)
        %v70575 = vadd.s32 %v70572, %v8 (stack65)
        %v70579 = vadd.s32 %v70575, 4 (stack65)
        %v70583 = vadd.s32 %v70567, %v70579 (stack65)
        %v70585 = vshll.u32 %v70579, 13 (stack73)
        %v70586 = vshrl.u32 %v70579, 19 (stack74)
        %v70587 = vor.u32 %v70585, %v70586 (stack75)
        %v70588 = vxor.u32 %v70583, %v70587 (stack76)
        %v70591 = vadd.s32 %v70583, %v70588 (stack65)
        %v70593 = vshll.u32 %v70588, 15 (stack73)
        %v70594 = vshrl.u32 %v70588, 17 (stack74)
        %v70595 = vor.u32 %v70593, %v70594 (stack75)
        %v70596 = vxor.u32 %v70591, %v70595 (stack76)
        %v70599 = vadd.s32 %v70591, %v70596 (stack65)
        %v70601 = vshll.u32 %v70596, 26 (stack73)
        %v70602 = vshrl.u32 %v70596, 6 (stack74)
        %v70603 = vor.u32 %v70601, %v70602 (stack75)
        %v70604 = vxor.u32 %v70599, %v70603 (stack76)
        %v70607 = vadd.s32 %v70599, %v70604 (stack65)
        %v70611 = vadd.s32 %v70607, %v8 (stack65)
        %v70613 = vshll.u32 %v70604, 6 (stack73)
        %v70614 = vshrl.u32 %v70604, 26 (stack74)
        %v70615 = vor.u32 %v70613, %v70614 (stack75)
        %v70616 = vxor.u32 %v70607, %v70615 (stack76)
        %v70619 = vadd.s32 %v70616, %v10 (stack65)
        %v70623 = vadd.s32 %v70619, 5 (stack65)
        %v70625 = vxor.u32 %v70611, %v70623 (stack76)
        %v70626 = vand.u32.u8 %v70625, 255 (stack77)
        %v70627 = vand.u32 %v70626, 65535 (stack78)
        %v70628 = vshrl.u32 %v70627, 1 (stack79)
        %v70629 = vor.u32 %v70628, 16256 (stack75)
        %v70630 = vand.u32.u16 %v70629, 65535 (stack80)
        %v70631 = vunpack.i.l.bf16 %v70630 (stack81)
        %v70635 = vadd.f32 %v70631, -1.0 (stack82)
        %v70639 = vmul.f32 %v70635, 2.0 (stack83)
        %v70643 = vadd.f32 %v70639, -0.99609375 (stack82)
        %v70647 = vmax.f32 -0.99609375, %v70643 (stack84)
        %v70649 = vand.u32 2147483647, %v70647 (stack85)
        %vm70652 = vcmp.eq.f32.partialorder %v70649, 1.0 (stack86)
        %v70657 = vmul.f32 %v70647, inf (stack83)
        %v70659 = vxor.u32 %v70647, 2147483648 (stack87)
        %v70662 = vmul.f32 %v70647, %v70659 (stack83)
        %v70664 = vadd.f32 %v70662, 1.0 (stack88)
        %v70665 = vlog2.pop %v70664 (stack89)
        %v70666 = vmul.f32 %v70665, 0.6931472 (stack90)
        %v70667 = vmul.f32 -0.5, %v70662 (stack91)
        %v70668 = vadd.f32 %v70667, 1.0 (stack92)
        %v70669 = vmul.f32 %v70668, %v70662 (stack93)
        %v70670 = vand.u32 2147483647, %v70662 (stack94)
        %vm70671 = vcmp.lt.f32.partialorder %v70670, 0.0004427343 (stack95)
        %v70672 = vsel /*vm=*/%vm70671, /*on_true_vy=*/%v70669, /*on_false_vx=*/%v70666 (stack96)
        %v70673 = vxor.u32 %v70672, 2147483648 (stack87)
        %vm70676 = vcmp.lt.f32.partialorder %v70673, 5.0 (stack86)
        %v70681 = vsel /*vm=*/%vm70676, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v70685 = vsel /*vm=*/%vm70676, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v70689 = vsel /*vm=*/%vm70676, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v70693 = vsel /*vm=*/%vm70676, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v70697 = vsel /*vm=*/%vm70676, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v70701 = vsel /*vm=*/%vm70676, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v70705 = vsel /*vm=*/%vm70676, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v70709 = vsel /*vm=*/%vm70676, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v70713 = vsel /*vm=*/%vm70676, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v70717 = vadd.f32 %v70673, -2.5 (stack82)
        %v70719 = vrsqrt.pop %v70673 (stack97)
        %v70720 = vmul.f32 %v70673, %v70719 (stack98)
        %vm70721 = vcmp.eq.f32.partialorder %v70673, inf (stack99)
        %v70722 = vsel /*vm=*/%vm70721, /*on_true_vy=*/%v70673, /*on_false_vx=*/%v70720 (stack100)
        %vm70723 = vcmp.eq.f32.partialorder %v70673, 0.0 (stack101)
        %v70724 = vand.u32 %v70673, 2147483648 (stack102)
        %v70725 = vsel /*vm=*/%vm70723, /*on_true_vy=*/%v70724, /*on_false_vx=*/%v70722 (stack103)
        %v70728 = vadd.f32 %v70725, -3.0 (stack82)
        %v70732 = vsel /*vm=*/%vm70676, /*on_true_vy=*/%v70717, /*on_false_vx=*/%v70728 (stack72)
        %v70736 = vmul.f32 %v70713, %v70732 (stack83)
        %v70740 = vadd.f32 %v70709, %v70736 (stack82)
        %v70744 = vmul.f32 %v70740, %v70732 (stack83)
        %v70748 = vadd.f32 %v70705, %v70744 (stack82)
        %v70752 = vmul.f32 %v70748, %v70732 (stack83)
        %v70756 = vadd.f32 %v70701, %v70752 (stack82)
        %v70760 = vmul.f32 %v70756, %v70732 (stack83)
        %v70764 = vadd.f32 %v70697, %v70760 (stack82)
        %v70768 = vmul.f32 %v70764, %v70732 (stack83)
        %v70772 = vadd.f32 %v70693, %v70768 (stack82)
        %v70776 = vmul.f32 %v70772, %v70732 (stack83)
        %v70780 = vadd.f32 %v70689, %v70776 (stack82)
        %v70784 = vmul.f32 %v70780, %v70732 (stack83)
        %v70788 = vadd.f32 %v70685, %v70784 (stack82)
        %v70792 = vmul.f32 %v70788, %v70732 (stack83)
        %v70796 = vadd.f32 %v70681, %v70792 (stack82)
        %v70800 = vmul.f32 %v70796, %v70647 (stack83)
        %v70804 = vsel /*vm=*/%vm70652, /*on_true_vy=*/%v70657, /*on_false_vx=*/%v70800 (stack72)
        %v70808 = vmul.f32 %v70804, 1.4140625 (stack83)
        %s70810 = scalar_lea.vmem %s280, 840 [#allocation0] (stack107)
        %v70811 = vpack.c.bf16 0.0, %v70808 (stack104)
        %70812 = vst [vmem:[%s70810] sm:$0xf] /*vst_source=*/%v70811 (stack105)
        %v70815 = vadd.s32 %v3816, %v67585 (stack65)
        %s70817 = smul.u32 128, %s27 (stack66)
        %v70818 = vlaneseq (stack67)
        %v70819 = vand.u32 %v70818, 127 (stack68)
        %v70820 = vstv %s70817 (stack69)
        %v70821 = vadd.s32 %v70819, %v70820 (stack70)
        %v70825 = vadd.s32 %v70815, %v70821 (stack65)
        %vm70829 = vcmp.lt.u32.totalorder %v70825, %v70815 (stack71)
        %vm70834 = vcmp.lt.u32.totalorder %v70815, %v3816 (stack71)
        %v70839 = vadd.s32 %v3803, %v67568 (stack65)
        %v70843 = vadd.s32 %v70839, 1 (stack65)
        %v70847 = vsel /*vm=*/%vm70834, /*on_true_vy=*/%v70843, /*on_false_vx=*/%v70839 (stack72)
        %v70851 = vadd.s32 %v70847, 1 (stack65)
        %v70855 = vsel /*vm=*/%vm70829, /*on_true_vy=*/%v70851, /*on_false_vx=*/%v70847 (stack72)
        %v70860 = vadd.s32 %v70855, %v10 (stack65)
        %v70864 = vadd.s32 %v70825, %v9 (stack65)
        %v70868 = vadd.s32 %v70860, %v70864 (stack65)
        %v70870 = vshll.u32 %v70864, 13 (stack73)
        %v70871 = vshrl.u32 %v70864, 19 (stack74)
        %v70872 = vor.u32 %v70870, %v70871 (stack75)
        %v70873 = vxor.u32 %v70868, %v70872 (stack76)
        %v70876 = vadd.s32 %v70868, %v70873 (stack65)
        %v70878 = vshll.u32 %v70873, 15 (stack73)
        %v70879 = vshrl.u32 %v70873, 17 (stack74)
        %v70880 = vor.u32 %v70878, %v70879 (stack75)
        %v70881 = vxor.u32 %v70876, %v70880 (stack76)
        %v70884 = vadd.s32 %v70876, %v70881 (stack65)
        %v70886 = vshll.u32 %v70881, 26 (stack73)
        %v70887 = vshrl.u32 %v70881, 6 (stack74)
        %v70888 = vor.u32 %v70886, %v70887 (stack75)
        %v70889 = vxor.u32 %v70884, %v70888 (stack76)
        %v70892 = vadd.s32 %v70884, %v70889 (stack65)
        %v70896 = vadd.s32 %v70892, %v9 (stack65)
        %v70898 = vshll.u32 %v70889, 6 (stack73)
        %v70899 = vshrl.u32 %v70889, 26 (stack74)
        %v70900 = vor.u32 %v70898, %v70899 (stack75)
        %v70901 = vxor.u32 %v70892, %v70900 (stack76)
        %v70904 = vadd.s32 %v70901, %v8 (stack65)
        %v70908 = vadd.s32 %v70904, 1 (stack65)
        %v70912 = vadd.s32 %v70896, %v70908 (stack65)
        %v70914 = vshll.u32 %v70908, 17 (stack73)
        %v70915 = vshrl.u32 %v70908, 15 (stack74)
        %v70916 = vor.u32 %v70914, %v70915 (stack75)
        %v70917 = vxor.u32 %v70912, %v70916 (stack76)
        %v70920 = vadd.s32 %v70912, %v70917 (stack65)
        %v70922 = vshll.u32 %v70917, 29 (stack73)
        %v70923 = vshrl.u32 %v70917, 3 (stack74)
        %v70924 = vor.u32 %v70922, %v70923 (stack75)
        %v70925 = vxor.u32 %v70920, %v70924 (stack76)
        %v70928 = vadd.s32 %v70920, %v70925 (stack65)
        %v70930 = vshll.u32 %v70925, 16 (stack73)
        %v70931 = vshrl.u32 %v70925, 16 (stack74)
        %v70932 = vor.u32 %v70930, %v70931 (stack75)
        %v70933 = vxor.u32 %v70928, %v70932 (stack76)
        %v70936 = vadd.s32 %v70928, %v70933 (stack65)
        %v70940 = vadd.s32 %v70936, %v8 (stack65)
        %v70942 = vshll.u32 %v70933, 24 (stack73)
        %v70943 = vshrl.u32 %v70933, 8 (stack74)
        %v70944 = vor.u32 %v70942, %v70943 (stack75)
        %v70945 = vxor.u32 %v70936, %v70944 (stack76)
        %v70948 = vadd.s32 %v70945, %v10 (stack65)
        %v70952 = vadd.s32 %v70948, 2 (stack65)
        %v70956 = vadd.s32 %v70940, %v70952 (stack65)
        %v70958 = vshll.u32 %v70952, 13 (stack73)
        %v70959 = vshrl.u32 %v70952, 19 (stack74)
        %v70960 = vor.u32 %v70958, %v70959 (stack75)
        %v70961 = vxor.u32 %v70956, %v70960 (stack76)
        %v70964 = vadd.s32 %v70956, %v70961 (stack65)
        %v70966 = vshll.u32 %v70961, 15 (stack73)
        %v70967 = vshrl.u32 %v70961, 17 (stack74)
        %v70968 = vor.u32 %v70966, %v70967 (stack75)
        %v70969 = vxor.u32 %v70964, %v70968 (stack76)
        %v70972 = vadd.s32 %v70964, %v70969 (stack65)
        %v70974 = vshll.u32 %v70969, 26 (stack73)
        %v70975 = vshrl.u32 %v70969, 6 (stack74)
        %v70976 = vor.u32 %v70974, %v70975 (stack75)
        %v70977 = vxor.u32 %v70972, %v70976 (stack76)
        %v70980 = vadd.s32 %v70972, %v70977 (stack65)
        %v70984 = vadd.s32 %v70980, %v10 (stack65)
        %v70986 = vshll.u32 %v70977, 6 (stack73)
        %v70987 = vshrl.u32 %v70977, 26 (stack74)
        %v70988 = vor.u32 %v70986, %v70987 (stack75)
        %v70989 = vxor.u32 %v70980, %v70988 (stack76)
        %v70992 = vadd.s32 %v70989, %v9 (stack65)
        %v70996 = vadd.s32 %v70992, 3 (stack65)
        %v71000 = vadd.s32 %v70984, %v70996 (stack65)
        %v71002 = vshll.u32 %v70996, 17 (stack73)
        %v71003 = vshrl.u32 %v70996, 15 (stack74)
        %v71004 = vor.u32 %v71002, %v71003 (stack75)
        %v71005 = vxor.u32 %v71000, %v71004 (stack76)
        %v71008 = vadd.s32 %v71000, %v71005 (stack65)
        %v71010 = vshll.u32 %v71005, 29 (stack73)
        %v71011 = vshrl.u32 %v71005, 3 (stack74)
        %v71012 = vor.u32 %v71010, %v71011 (stack75)
        %v71013 = vxor.u32 %v71008, %v71012 (stack76)
        %v71016 = vadd.s32 %v71008, %v71013 (stack65)
        %v71018 = vshll.u32 %v71013, 16 (stack73)
        %v71019 = vshrl.u32 %v71013, 16 (stack74)
        %v71020 = vor.u32 %v71018, %v71019 (stack75)
        %v71021 = vxor.u32 %v71016, %v71020 (stack76)
        %v71024 = vadd.s32 %v71016, %v71021 (stack65)
        %v71028 = vadd.s32 %v71024, %v9 (stack65)
        %v71030 = vshll.u32 %v71021, 24 (stack73)
        %v71031 = vshrl.u32 %v71021, 8 (stack74)
        %v71032 = vor.u32 %v71030, %v71031 (stack75)
        %v71033 = vxor.u32 %v71024, %v71032 (stack76)
        %v71036 = vadd.s32 %v71033, %v8 (stack65)
        %v71040 = vadd.s32 %v71036, 4 (stack65)
        %v71044 = vadd.s32 %v71028, %v71040 (stack65)
        %v71046 = vshll.u32 %v71040, 13 (stack73)
        %v71047 = vshrl.u32 %v71040, 19 (stack74)
        %v71048 = vor.u32 %v71046, %v71047 (stack75)
        %v71049 = vxor.u32 %v71044, %v71048 (stack76)
        %v71052 = vadd.s32 %v71044, %v71049 (stack65)
        %v71054 = vshll.u32 %v71049, 15 (stack73)
        %v71055 = vshrl.u32 %v71049, 17 (stack74)
        %v71056 = vor.u32 %v71054, %v71055 (stack75)
        %v71057 = vxor.u32 %v71052, %v71056 (stack76)
        %v71060 = vadd.s32 %v71052, %v71057 (stack65)
        %v71062 = vshll.u32 %v71057, 26 (stack73)
        %v71063 = vshrl.u32 %v71057, 6 (stack74)
        %v71064 = vor.u32 %v71062, %v71063 (stack75)
        %v71065 = vxor.u32 %v71060, %v71064 (stack76)
        %v71068 = vadd.s32 %v71060, %v71065 (stack65)
        %v71072 = vadd.s32 %v71068, %v8 (stack65)
        %v71074 = vshll.u32 %v71065, 6 (stack73)
        %v71075 = vshrl.u32 %v71065, 26 (stack74)
        %v71076 = vor.u32 %v71074, %v71075 (stack75)
        %v71077 = vxor.u32 %v71068, %v71076 (stack76)
        %v71080 = vadd.s32 %v71077, %v10 (stack65)
        %v71084 = vadd.s32 %v71080, 5 (stack65)
        %v71086 = vxor.u32 %v71072, %v71084 (stack76)
        %v71087 = vand.u32.u8 %v71086, 255 (stack77)
        %v71088 = vand.u32 %v71087, 65535 (stack78)
        %v71089 = vshrl.u32 %v71088, 1 (stack79)
        %v71090 = vor.u32 %v71089, 16256 (stack75)
        %v71091 = vand.u32.u16 %v71090, 65535 (stack80)
        %v71092 = vunpack.i.l.bf16 %v71091 (stack81)
        %v71096 = vadd.f32 %v71092, -1.0 (stack82)
        %v71100 = vmul.f32 %v71096, 2.0 (stack83)
        %v71104 = vadd.f32 %v71100, -0.99609375 (stack82)
        %v71108 = vmax.f32 -0.99609375, %v71104 (stack84)
        %v71110 = vand.u32 2147483647, %v71108 (stack85)
        %vm71113 = vcmp.eq.f32.partialorder %v71110, 1.0 (stack86)
        %v71118 = vmul.f32 %v71108, inf (stack83)
        %v71120 = vxor.u32 %v71108, 2147483648 (stack87)
        %v71123 = vmul.f32 %v71108, %v71120 (stack83)
        %v71125 = vadd.f32 %v71123, 1.0 (stack88)
        %v71126 = vlog2.pop %v71125 (stack89)
        %v71127 = vmul.f32 %v71126, 0.6931472 (stack90)
        %v71128 = vmul.f32 -0.5, %v71123 (stack91)
        %v71129 = vadd.f32 %v71128, 1.0 (stack92)
        %v71130 = vmul.f32 %v71129, %v71123 (stack93)
        %v71131 = vand.u32 2147483647, %v71123 (stack94)
        %vm71132 = vcmp.lt.f32.partialorder %v71131, 0.0004427343 (stack95)
        %v71133 = vsel /*vm=*/%vm71132, /*on_true_vy=*/%v71130, /*on_false_vx=*/%v71127 (stack96)
        %v71134 = vxor.u32 %v71133, 2147483648 (stack87)
        %vm71137 = vcmp.lt.f32.partialorder %v71134, 5.0 (stack86)
        %v71142 = vsel /*vm=*/%vm71137, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v71146 = vsel /*vm=*/%vm71137, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v71150 = vsel /*vm=*/%vm71137, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v71154 = vsel /*vm=*/%vm71137, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v71158 = vsel /*vm=*/%vm71137, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v71162 = vsel /*vm=*/%vm71137, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v71166 = vsel /*vm=*/%vm71137, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v71170 = vsel /*vm=*/%vm71137, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v71174 = vsel /*vm=*/%vm71137, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v71178 = vadd.f32 %v71134, -2.5 (stack82)
        %v71180 = vrsqrt.pop %v71134 (stack97)
        %v71181 = vmul.f32 %v71134, %v71180 (stack98)
        %vm71182 = vcmp.eq.f32.partialorder %v71134, inf (stack99)
        %v71183 = vsel /*vm=*/%vm71182, /*on_true_vy=*/%v71134, /*on_false_vx=*/%v71181 (stack100)
        %vm71184 = vcmp.eq.f32.partialorder %v71134, 0.0 (stack101)
        %v71185 = vand.u32 %v71134, 2147483648 (stack102)
        %v71186 = vsel /*vm=*/%vm71184, /*on_true_vy=*/%v71185, /*on_false_vx=*/%v71183 (stack103)
        %v71189 = vadd.f32 %v71186, -3.0 (stack82)
        %v71193 = vsel /*vm=*/%vm71137, /*on_true_vy=*/%v71178, /*on_false_vx=*/%v71189 (stack72)
        %v71197 = vmul.f32 %v71174, %v71193 (stack83)
        %v71201 = vadd.f32 %v71170, %v71197 (stack82)
        %v71205 = vmul.f32 %v71201, %v71193 (stack83)
        %v71209 = vadd.f32 %v71166, %v71205 (stack82)
        %v71213 = vmul.f32 %v71209, %v71193 (stack83)
        %v71217 = vadd.f32 %v71162, %v71213 (stack82)
        %v71221 = vmul.f32 %v71217, %v71193 (stack83)
        %v71225 = vadd.f32 %v71158, %v71221 (stack82)
        %v71229 = vmul.f32 %v71225, %v71193 (stack83)
        %v71233 = vadd.f32 %v71154, %v71229 (stack82)
        %v71237 = vmul.f32 %v71233, %v71193 (stack83)
        %v71241 = vadd.f32 %v71150, %v71237 (stack82)
        %v71245 = vmul.f32 %v71241, %v71193 (stack83)
        %v71249 = vadd.f32 %v71146, %v71245 (stack82)
        %v71253 = vmul.f32 %v71249, %v71193 (stack83)
        %v71257 = vadd.f32 %v71142, %v71253 (stack82)
        %v71261 = vmul.f32 %v71257, %v71108 (stack83)
        %v71265 = vsel /*vm=*/%vm71113, /*on_true_vy=*/%v71118, /*on_false_vx=*/%v71261 (stack72)
        %v71269 = vmul.f32 %v71265, 1.4140625 (stack83)
        %s71271 = scalar_lea.vmem %s280, 968 [#allocation0] (stack107)
        %v71272 = vpack.c.bf16 0.0, %v71269 (stack104)
        %71273 = vst [vmem:[%s71271] sm:$0xf] /*vst_source=*/%v71272 (stack105)
        %s71274 = sadd.s32 %s339, 152 (stack106)
        %s71275 = sshrl.u32 %s71274, 10 (stack49)
        %p71276 = scmp.lt.s32.totalorder 1, %s71275 (stack50)
        %s71277 = scalar_select /*predicate=*/%p71276, /*on_true=*/1, /*on_false=*/%s71275 (stack51)
        %s71278 = sand.u32 %s71274, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s71279 = sshrl.u32 %s71278, 7 (stack53)
        %s71280 = sand.u32 %s71278, 127 /* smod.u32 w/div 128 */ (stack54)
        %s71281 = smul.addr %s71277, 8 (stack55)
        %s71282 = scalar_lea.vmem %s3, %s71281 (stack56)
        %s71284 = scalar_lea.vmem %s71282, %s71279 (stack57)
        %v71285 = vld [vmem:[%s71284] ss:$0 sm:$0xff] (stack58)
        %s71286 = sand.u32 %s71280, 255 (stack59)
        %s71288 = sor.u32 256, %s71286 (stack60)
        %71289 = vbcast.lane.b32.xlu0 %v71285, %s71288 (stack61)
        %v71290 = vpop.permute.xlu0 %71289 (stack62)
        %s71291 = sadd.s32 %s347, 152 (stack106)
        %s71292 = sshrl.u32 %s71291, 10 (stack49)
        %p71293 = scmp.lt.s32.totalorder 1, %s71292 (stack50)
        %s71294 = scalar_select /*predicate=*/%p71293, /*on_true=*/1, /*on_false=*/%s71292 (stack51)
        %s71295 = sand.u32 %s71291, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s71296 = sshrl.u32 %s71295, 7 (stack53)
        %s71297 = sand.u32 %s71295, 127 /* smod.u32 w/div 128 */ (stack54)
        %s71298 = smul.addr %s71294, 8 (stack55)
        %s71299 = scalar_lea.vmem %s5, %s71298 (stack56)
        %s71301 = scalar_lea.vmem %s71299, %s71296 (stack57)
        %v71302 = vld [vmem:[%s71301] ss:$0 sm:$0xff] (stack58)
        %s71303 = sand.u32 %s71297, 255 (stack59)
        %s71305 = sor.u32 256, %s71303 (stack60)
        %71306 = vbcast.lane.b32.xlu0 %v71302, %s71305 (stack61)
        %v71307 = vpop.permute.xlu0 %71306 (stack62)
        %v71310 = vadd.s32 %v408, %v71307 (stack65)
        %s71312 = smul.u32 128, %s27 (stack66)
        %v71313 = vlaneseq (stack67)
        %v71314 = vand.u32 %v71313, 127 (stack68)
        %v71315 = vstv %s71312 (stack69)
        %v71316 = vadd.s32 %v71314, %v71315 (stack70)
        %v71320 = vadd.s32 %v71310, %v71316 (stack65)
        %vm71324 = vcmp.lt.u32.totalorder %v71320, %v71310 (stack71)
        %vm71329 = vcmp.lt.u32.totalorder %v71310, %v408 (stack71)
        %v71334 = vadd.s32 %v380, %v71290 (stack65)
        %v71338 = vadd.s32 %v71334, 1 (stack65)
        %v71342 = vsel /*vm=*/%vm71329, /*on_true_vy=*/%v71338, /*on_false_vx=*/%v71334 (stack72)
        %v71346 = vadd.s32 %v71342, 1 (stack65)
        %v71350 = vsel /*vm=*/%vm71324, /*on_true_vy=*/%v71346, /*on_false_vx=*/%v71342 (stack72)
        %v71355 = vadd.s32 %v71350, %v10 (stack65)
        %v71359 = vadd.s32 %v71320, %v9 (stack65)
        %v71363 = vadd.s32 %v71355, %v71359 (stack65)
        %v71365 = vshll.u32 %v71359, 13 (stack73)
        %v71366 = vshrl.u32 %v71359, 19 (stack74)
        %v71367 = vor.u32 %v71365, %v71366 (stack75)
        %v71368 = vxor.u32 %v71363, %v71367 (stack76)
        %v71371 = vadd.s32 %v71363, %v71368 (stack65)
        %v71373 = vshll.u32 %v71368, 15 (stack73)
        %v71374 = vshrl.u32 %v71368, 17 (stack74)
        %v71375 = vor.u32 %v71373, %v71374 (stack75)
        %v71376 = vxor.u32 %v71371, %v71375 (stack76)
        %v71379 = vadd.s32 %v71371, %v71376 (stack65)
        %v71381 = vshll.u32 %v71376, 26 (stack73)
        %v71382 = vshrl.u32 %v71376, 6 (stack74)
        %v71383 = vor.u32 %v71381, %v71382 (stack75)
        %v71384 = vxor.u32 %v71379, %v71383 (stack76)
        %v71387 = vadd.s32 %v71379, %v71384 (stack65)
        %v71391 = vadd.s32 %v71387, %v9 (stack65)
        %v71393 = vshll.u32 %v71384, 6 (stack73)
        %v71394 = vshrl.u32 %v71384, 26 (stack74)
        %v71395 = vor.u32 %v71393, %v71394 (stack75)
        %v71396 = vxor.u32 %v71387, %v71395 (stack76)
        %v71399 = vadd.s32 %v71396, %v8 (stack65)
        %v71403 = vadd.s32 %v71399, 1 (stack65)
        %v71407 = vadd.s32 %v71391, %v71403 (stack65)
        %v71409 = vshll.u32 %v71403, 17 (stack73)
        %v71410 = vshrl.u32 %v71403, 15 (stack74)
        %v71411 = vor.u32 %v71409, %v71410 (stack75)
        %v71412 = vxor.u32 %v71407, %v71411 (stack76)
        %v71415 = vadd.s32 %v71407, %v71412 (stack65)
        %v71417 = vshll.u32 %v71412, 29 (stack73)
        %v71418 = vshrl.u32 %v71412, 3 (stack74)
        %v71419 = vor.u32 %v71417, %v71418 (stack75)
        %v71420 = vxor.u32 %v71415, %v71419 (stack76)
        %v71423 = vadd.s32 %v71415, %v71420 (stack65)
        %v71425 = vshll.u32 %v71420, 16 (stack73)
        %v71426 = vshrl.u32 %v71420, 16 (stack74)
        %v71427 = vor.u32 %v71425, %v71426 (stack75)
        %v71428 = vxor.u32 %v71423, %v71427 (stack76)
        %v71431 = vadd.s32 %v71423, %v71428 (stack65)
        %v71435 = vadd.s32 %v71431, %v8 (stack65)
        %v71437 = vshll.u32 %v71428, 24 (stack73)
        %v71438 = vshrl.u32 %v71428, 8 (stack74)
        %v71439 = vor.u32 %v71437, %v71438 (stack75)
        %v71440 = vxor.u32 %v71431, %v71439 (stack76)
        %v71443 = vadd.s32 %v71440, %v10 (stack65)
        %v71447 = vadd.s32 %v71443, 2 (stack65)
        %v71451 = vadd.s32 %v71435, %v71447 (stack65)
        %v71453 = vshll.u32 %v71447, 13 (stack73)
        %v71454 = vshrl.u32 %v71447, 19 (stack74)
        %v71455 = vor.u32 %v71453, %v71454 (stack75)
        %v71456 = vxor.u32 %v71451, %v71455 (stack76)
        %v71459 = vadd.s32 %v71451, %v71456 (stack65)
        %v71461 = vshll.u32 %v71456, 15 (stack73)
        %v71462 = vshrl.u32 %v71456, 17 (stack74)
        %v71463 = vor.u32 %v71461, %v71462 (stack75)
        %v71464 = vxor.u32 %v71459, %v71463 (stack76)
        %v71467 = vadd.s32 %v71459, %v71464 (stack65)
        %v71469 = vshll.u32 %v71464, 26 (stack73)
        %v71470 = vshrl.u32 %v71464, 6 (stack74)
        %v71471 = vor.u32 %v71469, %v71470 (stack75)
        %v71472 = vxor.u32 %v71467, %v71471 (stack76)
        %v71475 = vadd.s32 %v71467, %v71472 (stack65)
        %v71479 = vadd.s32 %v71475, %v10 (stack65)
        %v71481 = vshll.u32 %v71472, 6 (stack73)
        %v71482 = vshrl.u32 %v71472, 26 (stack74)
        %v71483 = vor.u32 %v71481, %v71482 (stack75)
        %v71484 = vxor.u32 %v71475, %v71483 (stack76)
        %v71487 = vadd.s32 %v71484, %v9 (stack65)
        %v71491 = vadd.s32 %v71487, 3 (stack65)
        %v71495 = vadd.s32 %v71479, %v71491 (stack65)
        %v71497 = vshll.u32 %v71491, 17 (stack73)
        %v71498 = vshrl.u32 %v71491, 15 (stack74)
        %v71499 = vor.u32 %v71497, %v71498 (stack75)
        %v71500 = vxor.u32 %v71495, %v71499 (stack76)
        %v71503 = vadd.s32 %v71495, %v71500 (stack65)
        %v71505 = vshll.u32 %v71500, 29 (stack73)
        %v71506 = vshrl.u32 %v71500, 3 (stack74)
        %v71507 = vor.u32 %v71505, %v71506 (stack75)
        %v71508 = vxor.u32 %v71503, %v71507 (stack76)
        %v71511 = vadd.s32 %v71503, %v71508 (stack65)
        %v71513 = vshll.u32 %v71508, 16 (stack73)
        %v71514 = vshrl.u32 %v71508, 16 (stack74)
        %v71515 = vor.u32 %v71513, %v71514 (stack75)
        %v71516 = vxor.u32 %v71511, %v71515 (stack76)
        %v71519 = vadd.s32 %v71511, %v71516 (stack65)
        %v71523 = vadd.s32 %v71519, %v9 (stack65)
        %v71525 = vshll.u32 %v71516, 24 (stack73)
        %v71526 = vshrl.u32 %v71516, 8 (stack74)
        %v71527 = vor.u32 %v71525, %v71526 (stack75)
        %v71528 = vxor.u32 %v71519, %v71527 (stack76)
        %v71531 = vadd.s32 %v71528, %v8 (stack65)
        %v71535 = vadd.s32 %v71531, 4 (stack65)
        %v71539 = vadd.s32 %v71523, %v71535 (stack65)
        %v71541 = vshll.u32 %v71535, 13 (stack73)
        %v71542 = vshrl.u32 %v71535, 19 (stack74)
        %v71543 = vor.u32 %v71541, %v71542 (stack75)
        %v71544 = vxor.u32 %v71539, %v71543 (stack76)
        %v71547 = vadd.s32 %v71539, %v71544 (stack65)
        %v71549 = vshll.u32 %v71544, 15 (stack73)
        %v71550 = vshrl.u32 %v71544, 17 (stack74)
        %v71551 = vor.u32 %v71549, %v71550 (stack75)
        %v71552 = vxor.u32 %v71547, %v71551 (stack76)
        %v71555 = vadd.s32 %v71547, %v71552 (stack65)
        %v71557 = vshll.u32 %v71552, 26 (stack73)
        %v71558 = vshrl.u32 %v71552, 6 (stack74)
        %v71559 = vor.u32 %v71557, %v71558 (stack75)
        %v71560 = vxor.u32 %v71555, %v71559 (stack76)
        %v71563 = vadd.s32 %v71555, %v71560 (stack65)
        %v71567 = vadd.s32 %v71563, %v8 (stack65)
        %v71569 = vshll.u32 %v71560, 6 (stack73)
        %v71570 = vshrl.u32 %v71560, 26 (stack74)
        %v71571 = vor.u32 %v71569, %v71570 (stack75)
        %v71572 = vxor.u32 %v71563, %v71571 (stack76)
        %v71575 = vadd.s32 %v71572, %v10 (stack65)
        %v71579 = vadd.s32 %v71575, 5 (stack65)
        %v71581 = vxor.u32 %v71567, %v71579 (stack76)
        %v71582 = vand.u32.u8 %v71581, 255 (stack77)
        %v71583 = vand.u32 %v71582, 65535 (stack78)
        %v71584 = vshrl.u32 %v71583, 1 (stack79)
        %v71585 = vor.u32 %v71584, 16256 (stack75)
        %v71586 = vand.u32.u16 %v71585, 65535 (stack80)
        %v71587 = vunpack.i.l.bf16 %v71586 (stack81)
        %v71591 = vadd.f32 %v71587, -1.0 (stack82)
        %v71595 = vmul.f32 %v71591, 2.0 (stack83)
        %v71599 = vadd.f32 %v71595, -0.99609375 (stack82)
        %v71603 = vmax.f32 -0.99609375, %v71599 (stack84)
        %v71605 = vand.u32 2147483647, %v71603 (stack85)
        %vm71608 = vcmp.eq.f32.partialorder %v71605, 1.0 (stack86)
        %v71613 = vmul.f32 %v71603, inf (stack83)
        %v71615 = vxor.u32 %v71603, 2147483648 (stack87)
        %v71618 = vmul.f32 %v71603, %v71615 (stack83)
        %v71620 = vadd.f32 %v71618, 1.0 (stack88)
        %v71621 = vlog2.pop %v71620 (stack89)
        %v71622 = vmul.f32 %v71621, 0.6931472 (stack90)
        %v71623 = vmul.f32 -0.5, %v71618 (stack91)
        %v71624 = vadd.f32 %v71623, 1.0 (stack92)
        %v71625 = vmul.f32 %v71624, %v71618 (stack93)
        %v71626 = vand.u32 2147483647, %v71618 (stack94)
        %vm71627 = vcmp.lt.f32.partialorder %v71626, 0.0004427343 (stack95)
        %v71628 = vsel /*vm=*/%vm71627, /*on_true_vy=*/%v71625, /*on_false_vx=*/%v71622 (stack96)
        %v71629 = vxor.u32 %v71628, 2147483648 (stack87)
        %vm71632 = vcmp.lt.f32.partialorder %v71629, 5.0 (stack86)
        %v71637 = vsel /*vm=*/%vm71632, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v71641 = vsel /*vm=*/%vm71632, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v71645 = vsel /*vm=*/%vm71632, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v71649 = vsel /*vm=*/%vm71632, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v71653 = vsel /*vm=*/%vm71632, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v71657 = vsel /*vm=*/%vm71632, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v71661 = vsel /*vm=*/%vm71632, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v71665 = vsel /*vm=*/%vm71632, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v71669 = vsel /*vm=*/%vm71632, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v71673 = vadd.f32 %v71629, -2.5 (stack82)
        %v71675 = vrsqrt.pop %v71629 (stack97)
        %v71676 = vmul.f32 %v71629, %v71675 (stack98)
        %vm71677 = vcmp.eq.f32.partialorder %v71629, inf (stack99)
        %v71678 = vsel /*vm=*/%vm71677, /*on_true_vy=*/%v71629, /*on_false_vx=*/%v71676 (stack100)
        %vm71679 = vcmp.eq.f32.partialorder %v71629, 0.0 (stack101)
        %v71680 = vand.u32 %v71629, 2147483648 (stack102)
        %v71681 = vsel /*vm=*/%vm71679, /*on_true_vy=*/%v71680, /*on_false_vx=*/%v71678 (stack103)
        %v71684 = vadd.f32 %v71681, -3.0 (stack82)
        %v71688 = vsel /*vm=*/%vm71632, /*on_true_vy=*/%v71673, /*on_false_vx=*/%v71684 (stack72)
        %v71692 = vmul.f32 %v71669, %v71688 (stack83)
        %v71696 = vadd.f32 %v71665, %v71692 (stack82)
        %v71700 = vmul.f32 %v71696, %v71688 (stack83)
        %v71704 = vadd.f32 %v71661, %v71700 (stack82)
        %v71708 = vmul.f32 %v71704, %v71688 (stack83)
        %v71712 = vadd.f32 %v71657, %v71708 (stack82)
        %v71716 = vmul.f32 %v71712, %v71688 (stack83)
        %v71720 = vadd.f32 %v71653, %v71716 (stack82)
        %v71724 = vmul.f32 %v71720, %v71688 (stack83)
        %v71728 = vadd.f32 %v71649, %v71724 (stack82)
        %v71732 = vmul.f32 %v71728, %v71688 (stack83)
        %v71736 = vadd.f32 %v71645, %v71732 (stack82)
        %v71740 = vmul.f32 %v71736, %v71688 (stack83)
        %v71744 = vadd.f32 %v71641, %v71740 (stack82)
        %v71748 = vmul.f32 %v71744, %v71688 (stack83)
        %v71752 = vadd.f32 %v71637, %v71748 (stack82)
        %v71756 = vmul.f32 %v71752, %v71603 (stack83)
        %v71760 = vsel /*vm=*/%vm71608, /*on_true_vy=*/%v71613, /*on_false_vx=*/%v71756 (stack72)
        %v71764 = vmul.f32 %v71760, 1.4140625 (stack83)
        %s71766 = scalar_lea.vmem %s280, 76 [#allocation0] (stack107)
        %v71767 = vpack.c.bf16 0.0, %v71764 (stack104)
        %71768 = vst [vmem:[%s71766] sm:$0xf] /*vst_source=*/%v71767 (stack105)
        %v71771 = vadd.s32 %v894, %v71307 (stack65)
        %s71773 = smul.u32 128, %s27 (stack66)
        %v71774 = vlaneseq (stack67)
        %v71775 = vand.u32 %v71774, 127 (stack68)
        %v71776 = vstv %s71773 (stack69)
        %v71777 = vadd.s32 %v71775, %v71776 (stack70)
        %v71781 = vadd.s32 %v71771, %v71777 (stack65)
        %vm71785 = vcmp.lt.u32.totalorder %v71781, %v71771 (stack71)
        %vm71790 = vcmp.lt.u32.totalorder %v71771, %v894 (stack71)
        %v71795 = vadd.s32 %v881, %v71290 (stack65)
        %v71799 = vadd.s32 %v71795, 1 (stack65)
        %v71803 = vsel /*vm=*/%vm71790, /*on_true_vy=*/%v71799, /*on_false_vx=*/%v71795 (stack72)
        %v71807 = vadd.s32 %v71803, 1 (stack65)
        %v71811 = vsel /*vm=*/%vm71785, /*on_true_vy=*/%v71807, /*on_false_vx=*/%v71803 (stack72)
        %v71816 = vadd.s32 %v71811, %v10 (stack65)
        %v71820 = vadd.s32 %v71781, %v9 (stack65)
        %v71824 = vadd.s32 %v71816, %v71820 (stack65)
        %v71826 = vshll.u32 %v71820, 13 (stack73)
        %v71827 = vshrl.u32 %v71820, 19 (stack74)
        %v71828 = vor.u32 %v71826, %v71827 (stack75)
        %v71829 = vxor.u32 %v71824, %v71828 (stack76)
        %v71832 = vadd.s32 %v71824, %v71829 (stack65)
        %v71834 = vshll.u32 %v71829, 15 (stack73)
        %v71835 = vshrl.u32 %v71829, 17 (stack74)
        %v71836 = vor.u32 %v71834, %v71835 (stack75)
        %v71837 = vxor.u32 %v71832, %v71836 (stack76)
        %v71840 = vadd.s32 %v71832, %v71837 (stack65)
        %v71842 = vshll.u32 %v71837, 26 (stack73)
        %v71843 = vshrl.u32 %v71837, 6 (stack74)
        %v71844 = vor.u32 %v71842, %v71843 (stack75)
        %v71845 = vxor.u32 %v71840, %v71844 (stack76)
        %v71848 = vadd.s32 %v71840, %v71845 (stack65)
        %v71852 = vadd.s32 %v71848, %v9 (stack65)
        %v71854 = vshll.u32 %v71845, 6 (stack73)
        %v71855 = vshrl.u32 %v71845, 26 (stack74)
        %v71856 = vor.u32 %v71854, %v71855 (stack75)
        %v71857 = vxor.u32 %v71848, %v71856 (stack76)
        %v71860 = vadd.s32 %v71857, %v8 (stack65)
        %v71864 = vadd.s32 %v71860, 1 (stack65)
        %v71868 = vadd.s32 %v71852, %v71864 (stack65)
        %v71870 = vshll.u32 %v71864, 17 (stack73)
        %v71871 = vshrl.u32 %v71864, 15 (stack74)
        %v71872 = vor.u32 %v71870, %v71871 (stack75)
        %v71873 = vxor.u32 %v71868, %v71872 (stack76)
        %v71876 = vadd.s32 %v71868, %v71873 (stack65)
        %v71878 = vshll.u32 %v71873, 29 (stack73)
        %v71879 = vshrl.u32 %v71873, 3 (stack74)
        %v71880 = vor.u32 %v71878, %v71879 (stack75)
        %v71881 = vxor.u32 %v71876, %v71880 (stack76)
        %v71884 = vadd.s32 %v71876, %v71881 (stack65)
        %v71886 = vshll.u32 %v71881, 16 (stack73)
        %v71887 = vshrl.u32 %v71881, 16 (stack74)
        %v71888 = vor.u32 %v71886, %v71887 (stack75)
        %v71889 = vxor.u32 %v71884, %v71888 (stack76)
        %v71892 = vadd.s32 %v71884, %v71889 (stack65)
        %v71896 = vadd.s32 %v71892, %v8 (stack65)
        %v71898 = vshll.u32 %v71889, 24 (stack73)
        %v71899 = vshrl.u32 %v71889, 8 (stack74)
        %v71900 = vor.u32 %v71898, %v71899 (stack75)
        %v71901 = vxor.u32 %v71892, %v71900 (stack76)
        %v71904 = vadd.s32 %v71901, %v10 (stack65)
        %v71908 = vadd.s32 %v71904, 2 (stack65)
        %v71912 = vadd.s32 %v71896, %v71908 (stack65)
        %v71914 = vshll.u32 %v71908, 13 (stack73)
        %v71915 = vshrl.u32 %v71908, 19 (stack74)
        %v71916 = vor.u32 %v71914, %v71915 (stack75)
        %v71917 = vxor.u32 %v71912, %v71916 (stack76)
        %v71920 = vadd.s32 %v71912, %v71917 (stack65)
        %v71922 = vshll.u32 %v71917, 15 (stack73)
        %v71923 = vshrl.u32 %v71917, 17 (stack74)
        %v71924 = vor.u32 %v71922, %v71923 (stack75)
        %v71925 = vxor.u32 %v71920, %v71924 (stack76)
        %v71928 = vadd.s32 %v71920, %v71925 (stack65)
        %v71930 = vshll.u32 %v71925, 26 (stack73)
        %v71931 = vshrl.u32 %v71925, 6 (stack74)
        %v71932 = vor.u32 %v71930, %v71931 (stack75)
        %v71933 = vxor.u32 %v71928, %v71932 (stack76)
        %v71936 = vadd.s32 %v71928, %v71933 (stack65)
        %v71940 = vadd.s32 %v71936, %v10 (stack65)
        %v71942 = vshll.u32 %v71933, 6 (stack73)
        %v71943 = vshrl.u32 %v71933, 26 (stack74)
        %v71944 = vor.u32 %v71942, %v71943 (stack75)
        %v71945 = vxor.u32 %v71936, %v71944 (stack76)
        %v71948 = vadd.s32 %v71945, %v9 (stack65)
        %v71952 = vadd.s32 %v71948, 3 (stack65)
        %v71956 = vadd.s32 %v71940, %v71952 (stack65)
        %v71958 = vshll.u32 %v71952, 17 (stack73)
        %v71959 = vshrl.u32 %v71952, 15 (stack74)
        %v71960 = vor.u32 %v71958, %v71959 (stack75)
        %v71961 = vxor.u32 %v71956, %v71960 (stack76)
        %v71964 = vadd.s32 %v71956, %v71961 (stack65)
        %v71966 = vshll.u32 %v71961, 29 (stack73)
        %v71967 = vshrl.u32 %v71961, 3 (stack74)
        %v71968 = vor.u32 %v71966, %v71967 (stack75)
        %v71969 = vxor.u32 %v71964, %v71968 (stack76)
        %v71972 = vadd.s32 %v71964, %v71969 (stack65)
        %v71974 = vshll.u32 %v71969, 16 (stack73)
        %v71975 = vshrl.u32 %v71969, 16 (stack74)
        %v71976 = vor.u32 %v71974, %v71975 (stack75)
        %v71977 = vxor.u32 %v71972, %v71976 (stack76)
        %v71980 = vadd.s32 %v71972, %v71977 (stack65)
        %v71984 = vadd.s32 %v71980, %v9 (stack65)
        %v71986 = vshll.u32 %v71977, 24 (stack73)
        %v71987 = vshrl.u32 %v71977, 8 (stack74)
        %v71988 = vor.u32 %v71986, %v71987 (stack75)
        %v71989 = vxor.u32 %v71980, %v71988 (stack76)
        %v71992 = vadd.s32 %v71989, %v8 (stack65)
        %v71996 = vadd.s32 %v71992, 4 (stack65)
        %v72000 = vadd.s32 %v71984, %v71996 (stack65)
        %v72002 = vshll.u32 %v71996, 13 (stack73)
        %v72003 = vshrl.u32 %v71996, 19 (stack74)
        %v72004 = vor.u32 %v72002, %v72003 (stack75)
        %v72005 = vxor.u32 %v72000, %v72004 (stack76)
        %v72008 = vadd.s32 %v72000, %v72005 (stack65)
        %v72010 = vshll.u32 %v72005, 15 (stack73)
        %v72011 = vshrl.u32 %v72005, 17 (stack74)
        %v72012 = vor.u32 %v72010, %v72011 (stack75)
        %v72013 = vxor.u32 %v72008, %v72012 (stack76)
        %v72016 = vadd.s32 %v72008, %v72013 (stack65)
        %v72018 = vshll.u32 %v72013, 26 (stack73)
        %v72019 = vshrl.u32 %v72013, 6 (stack74)
        %v72020 = vor.u32 %v72018, %v72019 (stack75)
        %v72021 = vxor.u32 %v72016, %v72020 (stack76)
        %v72024 = vadd.s32 %v72016, %v72021 (stack65)
        %v72028 = vadd.s32 %v72024, %v8 (stack65)
        %v72030 = vshll.u32 %v72021, 6 (stack73)
        %v72031 = vshrl.u32 %v72021, 26 (stack74)
        %v72032 = vor.u32 %v72030, %v72031 (stack75)
        %v72033 = vxor.u32 %v72024, %v72032 (stack76)
        %v72036 = vadd.s32 %v72033, %v10 (stack65)
        %v72040 = vadd.s32 %v72036, 5 (stack65)
        %v72042 = vxor.u32 %v72028, %v72040 (stack76)
        %v72043 = vand.u32.u8 %v72042, 255 (stack77)
        %v72044 = vand.u32 %v72043, 65535 (stack78)
        %v72045 = vshrl.u32 %v72044, 1 (stack79)
        %v72046 = vor.u32 %v72045, 16256 (stack75)
        %v72047 = vand.u32.u16 %v72046, 65535 (stack80)
        %v72048 = vunpack.i.l.bf16 %v72047 (stack81)
        %v72052 = vadd.f32 %v72048, -1.0 (stack82)
        %v72056 = vmul.f32 %v72052, 2.0 (stack83)
        %v72060 = vadd.f32 %v72056, -0.99609375 (stack82)
        %v72064 = vmax.f32 -0.99609375, %v72060 (stack84)
        %v72066 = vand.u32 2147483647, %v72064 (stack85)
        %vm72069 = vcmp.eq.f32.partialorder %v72066, 1.0 (stack86)
        %v72074 = vmul.f32 %v72064, inf (stack83)
        %v72076 = vxor.u32 %v72064, 2147483648 (stack87)
        %v72079 = vmul.f32 %v72064, %v72076 (stack83)
        %v72081 = vadd.f32 %v72079, 1.0 (stack88)
        %v72082 = vlog2.pop %v72081 (stack89)
        %v72083 = vmul.f32 %v72082, 0.6931472 (stack90)
        %v72084 = vmul.f32 -0.5, %v72079 (stack91)
        %v72085 = vadd.f32 %v72084, 1.0 (stack92)
        %v72086 = vmul.f32 %v72085, %v72079 (stack93)
        %v72087 = vand.u32 2147483647, %v72079 (stack94)
        %vm72088 = vcmp.lt.f32.partialorder %v72087, 0.0004427343 (stack95)
        %v72089 = vsel /*vm=*/%vm72088, /*on_true_vy=*/%v72086, /*on_false_vx=*/%v72083 (stack96)
        %v72090 = vxor.u32 %v72089, 2147483648 (stack87)
        %vm72093 = vcmp.lt.f32.partialorder %v72090, 5.0 (stack86)
        %v72098 = vsel /*vm=*/%vm72093, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v72102 = vsel /*vm=*/%vm72093, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v72106 = vsel /*vm=*/%vm72093, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v72110 = vsel /*vm=*/%vm72093, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v72114 = vsel /*vm=*/%vm72093, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v72118 = vsel /*vm=*/%vm72093, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v72122 = vsel /*vm=*/%vm72093, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v72126 = vsel /*vm=*/%vm72093, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v72130 = vsel /*vm=*/%vm72093, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v72134 = vadd.f32 %v72090, -2.5 (stack82)
        %v72136 = vrsqrt.pop %v72090 (stack97)
        %v72137 = vmul.f32 %v72090, %v72136 (stack98)
        %vm72138 = vcmp.eq.f32.partialorder %v72090, inf (stack99)
        %v72139 = vsel /*vm=*/%vm72138, /*on_true_vy=*/%v72090, /*on_false_vx=*/%v72137 (stack100)
        %vm72140 = vcmp.eq.f32.partialorder %v72090, 0.0 (stack101)
        %v72141 = vand.u32 %v72090, 2147483648 (stack102)
        %v72142 = vsel /*vm=*/%vm72140, /*on_true_vy=*/%v72141, /*on_false_vx=*/%v72139 (stack103)
        %v72145 = vadd.f32 %v72142, -3.0 (stack82)
        %v72149 = vsel /*vm=*/%vm72093, /*on_true_vy=*/%v72134, /*on_false_vx=*/%v72145 (stack72)
        %v72153 = vmul.f32 %v72130, %v72149 (stack83)
        %v72157 = vadd.f32 %v72126, %v72153 (stack82)
        %v72161 = vmul.f32 %v72157, %v72149 (stack83)
        %v72165 = vadd.f32 %v72122, %v72161 (stack82)
        %v72169 = vmul.f32 %v72165, %v72149 (stack83)
        %v72173 = vadd.f32 %v72118, %v72169 (stack82)
        %v72177 = vmul.f32 %v72173, %v72149 (stack83)
        %v72181 = vadd.f32 %v72114, %v72177 (stack82)
        %v72185 = vmul.f32 %v72181, %v72149 (stack83)
        %v72189 = vadd.f32 %v72110, %v72185 (stack82)
        %v72193 = vmul.f32 %v72189, %v72149 (stack83)
        %v72197 = vadd.f32 %v72106, %v72193 (stack82)
        %v72201 = vmul.f32 %v72197, %v72149 (stack83)
        %v72205 = vadd.f32 %v72102, %v72201 (stack82)
        %v72209 = vmul.f32 %v72205, %v72149 (stack83)
        %v72213 = vadd.f32 %v72098, %v72209 (stack82)
        %v72217 = vmul.f32 %v72213, %v72064 (stack83)
        %v72221 = vsel /*vm=*/%vm72069, /*on_true_vy=*/%v72074, /*on_false_vx=*/%v72217 (stack72)
        %v72225 = vmul.f32 %v72221, 1.4140625 (stack83)
        %s72227 = scalar_lea.vmem %s280, 204 [#allocation0] (stack107)
        %v72228 = vpack.c.bf16 0.0, %v72225 (stack104)
        %72229 = vst [vmem:[%s72227] sm:$0xf] /*vst_source=*/%v72228 (stack105)
        %v72232 = vadd.s32 %v1381, %v71307 (stack65)
        %s72234 = smul.u32 128, %s27 (stack66)
        %v72235 = vlaneseq (stack67)
        %v72236 = vand.u32 %v72235, 127 (stack68)
        %v72237 = vstv %s72234 (stack69)
        %v72238 = vadd.s32 %v72236, %v72237 (stack70)
        %v72242 = vadd.s32 %v72232, %v72238 (stack65)
        %vm72246 = vcmp.lt.u32.totalorder %v72242, %v72232 (stack71)
        %vm72251 = vcmp.lt.u32.totalorder %v72232, %v1381 (stack71)
        %v72256 = vadd.s32 %v1368, %v71290 (stack65)
        %v72260 = vadd.s32 %v72256, 1 (stack65)
        %v72264 = vsel /*vm=*/%vm72251, /*on_true_vy=*/%v72260, /*on_false_vx=*/%v72256 (stack72)
        %v72268 = vadd.s32 %v72264, 1 (stack65)
        %v72272 = vsel /*vm=*/%vm72246, /*on_true_vy=*/%v72268, /*on_false_vx=*/%v72264 (stack72)
        %v72277 = vadd.s32 %v72272, %v10 (stack65)
        %v72281 = vadd.s32 %v72242, %v9 (stack65)
        %v72285 = vadd.s32 %v72277, %v72281 (stack65)
        %v72287 = vshll.u32 %v72281, 13 (stack73)
        %v72288 = vshrl.u32 %v72281, 19 (stack74)
        %v72289 = vor.u32 %v72287, %v72288 (stack75)
        %v72290 = vxor.u32 %v72285, %v72289 (stack76)
        %v72293 = vadd.s32 %v72285, %v72290 (stack65)
        %v72295 = vshll.u32 %v72290, 15 (stack73)
        %v72296 = vshrl.u32 %v72290, 17 (stack74)
        %v72297 = vor.u32 %v72295, %v72296 (stack75)
        %v72298 = vxor.u32 %v72293, %v72297 (stack76)
        %v72301 = vadd.s32 %v72293, %v72298 (stack65)
        %v72303 = vshll.u32 %v72298, 26 (stack73)
        %v72304 = vshrl.u32 %v72298, 6 (stack74)
        %v72305 = vor.u32 %v72303, %v72304 (stack75)
        %v72306 = vxor.u32 %v72301, %v72305 (stack76)
        %v72309 = vadd.s32 %v72301, %v72306 (stack65)
        %v72313 = vadd.s32 %v72309, %v9 (stack65)
        %v72315 = vshll.u32 %v72306, 6 (stack73)
        %v72316 = vshrl.u32 %v72306, 26 (stack74)
        %v72317 = vor.u32 %v72315, %v72316 (stack75)
        %v72318 = vxor.u32 %v72309, %v72317 (stack76)
        %v72321 = vadd.s32 %v72318, %v8 (stack65)
        %v72325 = vadd.s32 %v72321, 1 (stack65)
        %v72329 = vadd.s32 %v72313, %v72325 (stack65)
        %v72331 = vshll.u32 %v72325, 17 (stack73)
        %v72332 = vshrl.u32 %v72325, 15 (stack74)
        %v72333 = vor.u32 %v72331, %v72332 (stack75)
        %v72334 = vxor.u32 %v72329, %v72333 (stack76)
        %v72337 = vadd.s32 %v72329, %v72334 (stack65)
        %v72339 = vshll.u32 %v72334, 29 (stack73)
        %v72340 = vshrl.u32 %v72334, 3 (stack74)
        %v72341 = vor.u32 %v72339, %v72340 (stack75)
        %v72342 = vxor.u32 %v72337, %v72341 (stack76)
        %v72345 = vadd.s32 %v72337, %v72342 (stack65)
        %v72347 = vshll.u32 %v72342, 16 (stack73)
        %v72348 = vshrl.u32 %v72342, 16 (stack74)
        %v72349 = vor.u32 %v72347, %v72348 (stack75)
        %v72350 = vxor.u32 %v72345, %v72349 (stack76)
        %v72353 = vadd.s32 %v72345, %v72350 (stack65)
        %v72357 = vadd.s32 %v72353, %v8 (stack65)
        %v72359 = vshll.u32 %v72350, 24 (stack73)
        %v72360 = vshrl.u32 %v72350, 8 (stack74)
        %v72361 = vor.u32 %v72359, %v72360 (stack75)
        %v72362 = vxor.u32 %v72353, %v72361 (stack76)
        %v72365 = vadd.s32 %v72362, %v10 (stack65)
        %v72369 = vadd.s32 %v72365, 2 (stack65)
        %v72373 = vadd.s32 %v72357, %v72369 (stack65)
        %v72375 = vshll.u32 %v72369, 13 (stack73)
        %v72376 = vshrl.u32 %v72369, 19 (stack74)
        %v72377 = vor.u32 %v72375, %v72376 (stack75)
        %v72378 = vxor.u32 %v72373, %v72377 (stack76)
        %v72381 = vadd.s32 %v72373, %v72378 (stack65)
        %v72383 = vshll.u32 %v72378, 15 (stack73)
        %v72384 = vshrl.u32 %v72378, 17 (stack74)
        %v72385 = vor.u32 %v72383, %v72384 (stack75)
        %v72386 = vxor.u32 %v72381, %v72385 (stack76)
        %v72389 = vadd.s32 %v72381, %v72386 (stack65)
        %v72391 = vshll.u32 %v72386, 26 (stack73)
        %v72392 = vshrl.u32 %v72386, 6 (stack74)
        %v72393 = vor.u32 %v72391, %v72392 (stack75)
        %v72394 = vxor.u32 %v72389, %v72393 (stack76)
        %v72397 = vadd.s32 %v72389, %v72394 (stack65)
        %v72401 = vadd.s32 %v72397, %v10 (stack65)
        %v72403 = vshll.u32 %v72394, 6 (stack73)
        %v72404 = vshrl.u32 %v72394, 26 (stack74)
        %v72405 = vor.u32 %v72403, %v72404 (stack75)
        %v72406 = vxor.u32 %v72397, %v72405 (stack76)
        %v72409 = vadd.s32 %v72406, %v9 (stack65)
        %v72413 = vadd.s32 %v72409, 3 (stack65)
        %v72417 = vadd.s32 %v72401, %v72413 (stack65)
        %v72419 = vshll.u32 %v72413, 17 (stack73)
        %v72420 = vshrl.u32 %v72413, 15 (stack74)
        %v72421 = vor.u32 %v72419, %v72420 (stack75)
        %v72422 = vxor.u32 %v72417, %v72421 (stack76)
        %v72425 = vadd.s32 %v72417, %v72422 (stack65)
        %v72427 = vshll.u32 %v72422, 29 (stack73)
        %v72428 = vshrl.u32 %v72422, 3 (stack74)
        %v72429 = vor.u32 %v72427, %v72428 (stack75)
        %v72430 = vxor.u32 %v72425, %v72429 (stack76)
        %v72433 = vadd.s32 %v72425, %v72430 (stack65)
        %v72435 = vshll.u32 %v72430, 16 (stack73)
        %v72436 = vshrl.u32 %v72430, 16 (stack74)
        %v72437 = vor.u32 %v72435, %v72436 (stack75)
        %v72438 = vxor.u32 %v72433, %v72437 (stack76)
        %v72441 = vadd.s32 %v72433, %v72438 (stack65)
        %v72445 = vadd.s32 %v72441, %v9 (stack65)
        %v72447 = vshll.u32 %v72438, 24 (stack73)
        %v72448 = vshrl.u32 %v72438, 8 (stack74)
        %v72449 = vor.u32 %v72447, %v72448 (stack75)
        %v72450 = vxor.u32 %v72441, %v72449 (stack76)
        %v72453 = vadd.s32 %v72450, %v8 (stack65)
        %v72457 = vadd.s32 %v72453, 4 (stack65)
        %v72461 = vadd.s32 %v72445, %v72457 (stack65)
        %v72463 = vshll.u32 %v72457, 13 (stack73)
        %v72464 = vshrl.u32 %v72457, 19 (stack74)
        %v72465 = vor.u32 %v72463, %v72464 (stack75)
        %v72466 = vxor.u32 %v72461, %v72465 (stack76)
        %v72469 = vadd.s32 %v72461, %v72466 (stack65)
        %v72471 = vshll.u32 %v72466, 15 (stack73)
        %v72472 = vshrl.u32 %v72466, 17 (stack74)
        %v72473 = vor.u32 %v72471, %v72472 (stack75)
        %v72474 = vxor.u32 %v72469, %v72473 (stack76)
        %v72477 = vadd.s32 %v72469, %v72474 (stack65)
        %v72479 = vshll.u32 %v72474, 26 (stack73)
        %v72480 = vshrl.u32 %v72474, 6 (stack74)
        %v72481 = vor.u32 %v72479, %v72480 (stack75)
        %v72482 = vxor.u32 %v72477, %v72481 (stack76)
        %v72485 = vadd.s32 %v72477, %v72482 (stack65)
        %v72489 = vadd.s32 %v72485, %v8 (stack65)
        %v72491 = vshll.u32 %v72482, 6 (stack73)
        %v72492 = vshrl.u32 %v72482, 26 (stack74)
        %v72493 = vor.u32 %v72491, %v72492 (stack75)
        %v72494 = vxor.u32 %v72485, %v72493 (stack76)
        %v72497 = vadd.s32 %v72494, %v10 (stack65)
        %v72501 = vadd.s32 %v72497, 5 (stack65)
        %v72503 = vxor.u32 %v72489, %v72501 (stack76)
        %v72504 = vand.u32.u8 %v72503, 255 (stack77)
        %v72505 = vand.u32 %v72504, 65535 (stack78)
        %v72506 = vshrl.u32 %v72505, 1 (stack79)
        %v72507 = vor.u32 %v72506, 16256 (stack75)
        %v72508 = vand.u32.u16 %v72507, 65535 (stack80)
        %v72509 = vunpack.i.l.bf16 %v72508 (stack81)
        %v72513 = vadd.f32 %v72509, -1.0 (stack82)
        %v72517 = vmul.f32 %v72513, 2.0 (stack83)
        %v72521 = vadd.f32 %v72517, -0.99609375 (stack82)
        %v72525 = vmax.f32 -0.99609375, %v72521 (stack84)
        %v72527 = vand.u32 2147483647, %v72525 (stack85)
        %vm72530 = vcmp.eq.f32.partialorder %v72527, 1.0 (stack86)
        %v72535 = vmul.f32 %v72525, inf (stack83)
        %v72537 = vxor.u32 %v72525, 2147483648 (stack87)
        %v72540 = vmul.f32 %v72525, %v72537 (stack83)
        %v72542 = vadd.f32 %v72540, 1.0 (stack88)
        %v72543 = vlog2.pop %v72542 (stack89)
        %v72544 = vmul.f32 %v72543, 0.6931472 (stack90)
        %v72545 = vmul.f32 -0.5, %v72540 (stack91)
        %v72546 = vadd.f32 %v72545, 1.0 (stack92)
        %v72547 = vmul.f32 %v72546, %v72540 (stack93)
        %v72548 = vand.u32 2147483647, %v72540 (stack94)
        %vm72549 = vcmp.lt.f32.partialorder %v72548, 0.0004427343 (stack95)
        %v72550 = vsel /*vm=*/%vm72549, /*on_true_vy=*/%v72547, /*on_false_vx=*/%v72544 (stack96)
        %v72551 = vxor.u32 %v72550, 2147483648 (stack87)
        %vm72554 = vcmp.lt.f32.partialorder %v72551, 5.0 (stack86)
        %v72559 = vsel /*vm=*/%vm72554, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v72563 = vsel /*vm=*/%vm72554, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v72567 = vsel /*vm=*/%vm72554, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v72571 = vsel /*vm=*/%vm72554, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v72575 = vsel /*vm=*/%vm72554, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v72579 = vsel /*vm=*/%vm72554, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v72583 = vsel /*vm=*/%vm72554, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v72587 = vsel /*vm=*/%vm72554, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v72591 = vsel /*vm=*/%vm72554, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v72595 = vadd.f32 %v72551, -2.5 (stack82)
        %v72597 = vrsqrt.pop %v72551 (stack97)
        %v72598 = vmul.f32 %v72551, %v72597 (stack98)
        %vm72599 = vcmp.eq.f32.partialorder %v72551, inf (stack99)
        %v72600 = vsel /*vm=*/%vm72599, /*on_true_vy=*/%v72551, /*on_false_vx=*/%v72598 (stack100)
        %vm72601 = vcmp.eq.f32.partialorder %v72551, 0.0 (stack101)
        %v72602 = vand.u32 %v72551, 2147483648 (stack102)
        %v72603 = vsel /*vm=*/%vm72601, /*on_true_vy=*/%v72602, /*on_false_vx=*/%v72600 (stack103)
        %v72606 = vadd.f32 %v72603, -3.0 (stack82)
        %v72610 = vsel /*vm=*/%vm72554, /*on_true_vy=*/%v72595, /*on_false_vx=*/%v72606 (stack72)
        %v72614 = vmul.f32 %v72591, %v72610 (stack83)
        %v72618 = vadd.f32 %v72587, %v72614 (stack82)
        %v72622 = vmul.f32 %v72618, %v72610 (stack83)
        %v72626 = vadd.f32 %v72583, %v72622 (stack82)
        %v72630 = vmul.f32 %v72626, %v72610 (stack83)
        %v72634 = vadd.f32 %v72579, %v72630 (stack82)
        %v72638 = vmul.f32 %v72634, %v72610 (stack83)
        %v72642 = vadd.f32 %v72575, %v72638 (stack82)
        %v72646 = vmul.f32 %v72642, %v72610 (stack83)
        %v72650 = vadd.f32 %v72571, %v72646 (stack82)
        %v72654 = vmul.f32 %v72650, %v72610 (stack83)
        %v72658 = vadd.f32 %v72567, %v72654 (stack82)
        %v72662 = vmul.f32 %v72658, %v72610 (stack83)
        %v72666 = vadd.f32 %v72563, %v72662 (stack82)
        %v72670 = vmul.f32 %v72666, %v72610 (stack83)
        %v72674 = vadd.f32 %v72559, %v72670 (stack82)
        %v72678 = vmul.f32 %v72674, %v72525 (stack83)
        %v72682 = vsel /*vm=*/%vm72530, /*on_true_vy=*/%v72535, /*on_false_vx=*/%v72678 (stack72)
        %v72686 = vmul.f32 %v72682, 1.4140625 (stack83)
        %s72688 = scalar_lea.vmem %s280, 332 [#allocation0] (stack107)
        %v72689 = vpack.c.bf16 0.0, %v72686 (stack104)
        %72690 = vst [vmem:[%s72688] sm:$0xf] /*vst_source=*/%v72689 (stack105)
        %v72693 = vadd.s32 %v1868, %v71307 (stack65)
        %s72695 = smul.u32 128, %s27 (stack66)
        %v72696 = vlaneseq (stack67)
        %v72697 = vand.u32 %v72696, 127 (stack68)
        %v72698 = vstv %s72695 (stack69)
        %v72699 = vadd.s32 %v72697, %v72698 (stack70)
        %v72703 = vadd.s32 %v72693, %v72699 (stack65)
        %vm72707 = vcmp.lt.u32.totalorder %v72703, %v72693 (stack71)
        %vm72712 = vcmp.lt.u32.totalorder %v72693, %v1868 (stack71)
        %v72717 = vadd.s32 %v1855, %v71290 (stack65)
        %v72721 = vadd.s32 %v72717, 1 (stack65)
        %v72725 = vsel /*vm=*/%vm72712, /*on_true_vy=*/%v72721, /*on_false_vx=*/%v72717 (stack72)
        %v72729 = vadd.s32 %v72725, 1 (stack65)
        %v72733 = vsel /*vm=*/%vm72707, /*on_true_vy=*/%v72729, /*on_false_vx=*/%v72725 (stack72)
        %v72738 = vadd.s32 %v72733, %v10 (stack65)
        %v72742 = vadd.s32 %v72703, %v9 (stack65)
        %v72746 = vadd.s32 %v72738, %v72742 (stack65)
        %v72748 = vshll.u32 %v72742, 13 (stack73)
        %v72749 = vshrl.u32 %v72742, 19 (stack74)
        %v72750 = vor.u32 %v72748, %v72749 (stack75)
        %v72751 = vxor.u32 %v72746, %v72750 (stack76)
        %v72754 = vadd.s32 %v72746, %v72751 (stack65)
        %v72756 = vshll.u32 %v72751, 15 (stack73)
        %v72757 = vshrl.u32 %v72751, 17 (stack74)
        %v72758 = vor.u32 %v72756, %v72757 (stack75)
        %v72759 = vxor.u32 %v72754, %v72758 (stack76)
        %v72762 = vadd.s32 %v72754, %v72759 (stack65)
        %v72764 = vshll.u32 %v72759, 26 (stack73)
        %v72765 = vshrl.u32 %v72759, 6 (stack74)
        %v72766 = vor.u32 %v72764, %v72765 (stack75)
        %v72767 = vxor.u32 %v72762, %v72766 (stack76)
        %v72770 = vadd.s32 %v72762, %v72767 (stack65)
        %v72774 = vadd.s32 %v72770, %v9 (stack65)
        %v72776 = vshll.u32 %v72767, 6 (stack73)
        %v72777 = vshrl.u32 %v72767, 26 (stack74)
        %v72778 = vor.u32 %v72776, %v72777 (stack75)
        %v72779 = vxor.u32 %v72770, %v72778 (stack76)
        %v72782 = vadd.s32 %v72779, %v8 (stack65)
        %v72786 = vadd.s32 %v72782, 1 (stack65)
        %v72790 = vadd.s32 %v72774, %v72786 (stack65)
        %v72792 = vshll.u32 %v72786, 17 (stack73)
        %v72793 = vshrl.u32 %v72786, 15 (stack74)
        %v72794 = vor.u32 %v72792, %v72793 (stack75)
        %v72795 = vxor.u32 %v72790, %v72794 (stack76)
        %v72798 = vadd.s32 %v72790, %v72795 (stack65)
        %v72800 = vshll.u32 %v72795, 29 (stack73)
        %v72801 = vshrl.u32 %v72795, 3 (stack74)
        %v72802 = vor.u32 %v72800, %v72801 (stack75)
        %v72803 = vxor.u32 %v72798, %v72802 (stack76)
        %v72806 = vadd.s32 %v72798, %v72803 (stack65)
        %v72808 = vshll.u32 %v72803, 16 (stack73)
        %v72809 = vshrl.u32 %v72803, 16 (stack74)
        %v72810 = vor.u32 %v72808, %v72809 (stack75)
        %v72811 = vxor.u32 %v72806, %v72810 (stack76)
        %v72814 = vadd.s32 %v72806, %v72811 (stack65)
        %v72818 = vadd.s32 %v72814, %v8 (stack65)
        %v72820 = vshll.u32 %v72811, 24 (stack73)
        %v72821 = vshrl.u32 %v72811, 8 (stack74)
        %v72822 = vor.u32 %v72820, %v72821 (stack75)
        %v72823 = vxor.u32 %v72814, %v72822 (stack76)
        %v72826 = vadd.s32 %v72823, %v10 (stack65)
        %v72830 = vadd.s32 %v72826, 2 (stack65)
        %v72834 = vadd.s32 %v72818, %v72830 (stack65)
        %v72836 = vshll.u32 %v72830, 13 (stack73)
        %v72837 = vshrl.u32 %v72830, 19 (stack74)
        %v72838 = vor.u32 %v72836, %v72837 (stack75)
        %v72839 = vxor.u32 %v72834, %v72838 (stack76)
        %v72842 = vadd.s32 %v72834, %v72839 (stack65)
        %v72844 = vshll.u32 %v72839, 15 (stack73)
        %v72845 = vshrl.u32 %v72839, 17 (stack74)
        %v72846 = vor.u32 %v72844, %v72845 (stack75)
        %v72847 = vxor.u32 %v72842, %v72846 (stack76)
        %v72850 = vadd.s32 %v72842, %v72847 (stack65)
        %v72852 = vshll.u32 %v72847, 26 (stack73)
        %v72853 = vshrl.u32 %v72847, 6 (stack74)
        %v72854 = vor.u32 %v72852, %v72853 (stack75)
        %v72855 = vxor.u32 %v72850, %v72854 (stack76)
        %v72858 = vadd.s32 %v72850, %v72855 (stack65)
        %v72862 = vadd.s32 %v72858, %v10 (stack65)
        %v72864 = vshll.u32 %v72855, 6 (stack73)
        %v72865 = vshrl.u32 %v72855, 26 (stack74)
        %v72866 = vor.u32 %v72864, %v72865 (stack75)
        %v72867 = vxor.u32 %v72858, %v72866 (stack76)
        %v72870 = vadd.s32 %v72867, %v9 (stack65)
        %v72874 = vadd.s32 %v72870, 3 (stack65)
        %v72878 = vadd.s32 %v72862, %v72874 (stack65)
        %v72880 = vshll.u32 %v72874, 17 (stack73)
        %v72881 = vshrl.u32 %v72874, 15 (stack74)
        %v72882 = vor.u32 %v72880, %v72881 (stack75)
        %v72883 = vxor.u32 %v72878, %v72882 (stack76)
        %v72886 = vadd.s32 %v72878, %v72883 (stack65)
        %v72888 = vshll.u32 %v72883, 29 (stack73)
        %v72889 = vshrl.u32 %v72883, 3 (stack74)
        %v72890 = vor.u32 %v72888, %v72889 (stack75)
        %v72891 = vxor.u32 %v72886, %v72890 (stack76)
        %v72894 = vadd.s32 %v72886, %v72891 (stack65)
        %v72896 = vshll.u32 %v72891, 16 (stack73)
        %v72897 = vshrl.u32 %v72891, 16 (stack74)
        %v72898 = vor.u32 %v72896, %v72897 (stack75)
        %v72899 = vxor.u32 %v72894, %v72898 (stack76)
        %v72902 = vadd.s32 %v72894, %v72899 (stack65)
        %v72906 = vadd.s32 %v72902, %v9 (stack65)
        %v72908 = vshll.u32 %v72899, 24 (stack73)
        %v72909 = vshrl.u32 %v72899, 8 (stack74)
        %v72910 = vor.u32 %v72908, %v72909 (stack75)
        %v72911 = vxor.u32 %v72902, %v72910 (stack76)
        %v72914 = vadd.s32 %v72911, %v8 (stack65)
        %v72918 = vadd.s32 %v72914, 4 (stack65)
        %v72922 = vadd.s32 %v72906, %v72918 (stack65)
        %v72924 = vshll.u32 %v72918, 13 (stack73)
        %v72925 = vshrl.u32 %v72918, 19 (stack74)
        %v72926 = vor.u32 %v72924, %v72925 (stack75)
        %v72927 = vxor.u32 %v72922, %v72926 (stack76)
        %v72930 = vadd.s32 %v72922, %v72927 (stack65)
        %v72932 = vshll.u32 %v72927, 15 (stack73)
        %v72933 = vshrl.u32 %v72927, 17 (stack74)
        %v72934 = vor.u32 %v72932, %v72933 (stack75)
        %v72935 = vxor.u32 %v72930, %v72934 (stack76)
        %v72938 = vadd.s32 %v72930, %v72935 (stack65)
        %v72940 = vshll.u32 %v72935, 26 (stack73)
        %v72941 = vshrl.u32 %v72935, 6 (stack74)
        %v72942 = vor.u32 %v72940, %v72941 (stack75)
        %v72943 = vxor.u32 %v72938, %v72942 (stack76)
        %v72946 = vadd.s32 %v72938, %v72943 (stack65)
        %v72950 = vadd.s32 %v72946, %v8 (stack65)
        %v72952 = vshll.u32 %v72943, 6 (stack73)
        %v72953 = vshrl.u32 %v72943, 26 (stack74)
        %v72954 = vor.u32 %v72952, %v72953 (stack75)
        %v72955 = vxor.u32 %v72946, %v72954 (stack76)
        %v72958 = vadd.s32 %v72955, %v10 (stack65)
        %v72962 = vadd.s32 %v72958, 5 (stack65)
        %v72964 = vxor.u32 %v72950, %v72962 (stack76)
        %v72965 = vand.u32.u8 %v72964, 255 (stack77)
        %v72966 = vand.u32 %v72965, 65535 (stack78)
        %v72967 = vshrl.u32 %v72966, 1 (stack79)
        %v72968 = vor.u32 %v72967, 16256 (stack75)
        %v72969 = vand.u32.u16 %v72968, 65535 (stack80)
        %v72970 = vunpack.i.l.bf16 %v72969 (stack81)
        %v72974 = vadd.f32 %v72970, -1.0 (stack82)
        %v72978 = vmul.f32 %v72974, 2.0 (stack83)
        %v72982 = vadd.f32 %v72978, -0.99609375 (stack82)
        %v72986 = vmax.f32 -0.99609375, %v72982 (stack84)
        %v72988 = vand.u32 2147483647, %v72986 (stack85)
        %vm72991 = vcmp.eq.f32.partialorder %v72988, 1.0 (stack86)
        %v72996 = vmul.f32 %v72986, inf (stack83)
        %v72998 = vxor.u32 %v72986, 2147483648 (stack87)
        %v73001 = vmul.f32 %v72986, %v72998 (stack83)
        %v73003 = vadd.f32 %v73001, 1.0 (stack88)
        %v73004 = vlog2.pop %v73003 (stack89)
        %v73005 = vmul.f32 %v73004, 0.6931472 (stack90)
        %v73006 = vmul.f32 -0.5, %v73001 (stack91)
        %v73007 = vadd.f32 %v73006, 1.0 (stack92)
        %v73008 = vmul.f32 %v73007, %v73001 (stack93)
        %v73009 = vand.u32 2147483647, %v73001 (stack94)
        %vm73010 = vcmp.lt.f32.partialorder %v73009, 0.0004427343 (stack95)
        %v73011 = vsel /*vm=*/%vm73010, /*on_true_vy=*/%v73008, /*on_false_vx=*/%v73005 (stack96)
        %v73012 = vxor.u32 %v73011, 2147483648 (stack87)
        %vm73015 = vcmp.lt.f32.partialorder %v73012, 5.0 (stack86)
        %v73020 = vsel /*vm=*/%vm73015, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v73024 = vsel /*vm=*/%vm73015, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v73028 = vsel /*vm=*/%vm73015, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v73032 = vsel /*vm=*/%vm73015, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v73036 = vsel /*vm=*/%vm73015, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v73040 = vsel /*vm=*/%vm73015, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v73044 = vsel /*vm=*/%vm73015, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v73048 = vsel /*vm=*/%vm73015, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v73052 = vsel /*vm=*/%vm73015, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v73056 = vadd.f32 %v73012, -2.5 (stack82)
        %v73058 = vrsqrt.pop %v73012 (stack97)
        %v73059 = vmul.f32 %v73012, %v73058 (stack98)
        %vm73060 = vcmp.eq.f32.partialorder %v73012, inf (stack99)
        %v73061 = vsel /*vm=*/%vm73060, /*on_true_vy=*/%v73012, /*on_false_vx=*/%v73059 (stack100)
        %vm73062 = vcmp.eq.f32.partialorder %v73012, 0.0 (stack101)
        %v73063 = vand.u32 %v73012, 2147483648 (stack102)
        %v73064 = vsel /*vm=*/%vm73062, /*on_true_vy=*/%v73063, /*on_false_vx=*/%v73061 (stack103)
        %v73067 = vadd.f32 %v73064, -3.0 (stack82)
        %v73071 = vsel /*vm=*/%vm73015, /*on_true_vy=*/%v73056, /*on_false_vx=*/%v73067 (stack72)
        %v73075 = vmul.f32 %v73052, %v73071 (stack83)
        %v73079 = vadd.f32 %v73048, %v73075 (stack82)
        %v73083 = vmul.f32 %v73079, %v73071 (stack83)
        %v73087 = vadd.f32 %v73044, %v73083 (stack82)
        %v73091 = vmul.f32 %v73087, %v73071 (stack83)
        %v73095 = vadd.f32 %v73040, %v73091 (stack82)
        %v73099 = vmul.f32 %v73095, %v73071 (stack83)
        %v73103 = vadd.f32 %v73036, %v73099 (stack82)
        %v73107 = vmul.f32 %v73103, %v73071 (stack83)
        %v73111 = vadd.f32 %v73032, %v73107 (stack82)
        %v73115 = vmul.f32 %v73111, %v73071 (stack83)
        %v73119 = vadd.f32 %v73028, %v73115 (stack82)
        %v73123 = vmul.f32 %v73119, %v73071 (stack83)
        %v73127 = vadd.f32 %v73024, %v73123 (stack82)
        %v73131 = vmul.f32 %v73127, %v73071 (stack83)
        %v73135 = vadd.f32 %v73020, %v73131 (stack82)
        %v73139 = vmul.f32 %v73135, %v72986 (stack83)
        %v73143 = vsel /*vm=*/%vm72991, /*on_true_vy=*/%v72996, /*on_false_vx=*/%v73139 (stack72)
        %v73147 = vmul.f32 %v73143, 1.4140625 (stack83)
        %s73149 = scalar_lea.vmem %s280, 460 [#allocation0] (stack107)
        %v73150 = vpack.c.bf16 0.0, %v73147 (stack104)
        %73151 = vst [vmem:[%s73149] sm:$0xf] /*vst_source=*/%v73150 (stack105)
        %v73154 = vadd.s32 %v2355, %v71307 (stack65)
        %s73156 = smul.u32 128, %s27 (stack66)
        %v73157 = vlaneseq (stack67)
        %v73158 = vand.u32 %v73157, 127 (stack68)
        %v73159 = vstv %s73156 (stack69)
        %v73160 = vadd.s32 %v73158, %v73159 (stack70)
        %v73164 = vadd.s32 %v73154, %v73160 (stack65)
        %vm73168 = vcmp.lt.u32.totalorder %v73164, %v73154 (stack71)
        %vm73173 = vcmp.lt.u32.totalorder %v73154, %v2355 (stack71)
        %v73178 = vadd.s32 %v2342, %v71290 (stack65)
        %v73182 = vadd.s32 %v73178, 1 (stack65)
        %v73186 = vsel /*vm=*/%vm73173, /*on_true_vy=*/%v73182, /*on_false_vx=*/%v73178 (stack72)
        %v73190 = vadd.s32 %v73186, 1 (stack65)
        %v73194 = vsel /*vm=*/%vm73168, /*on_true_vy=*/%v73190, /*on_false_vx=*/%v73186 (stack72)
        %v73199 = vadd.s32 %v73194, %v10 (stack65)
        %v73203 = vadd.s32 %v73164, %v9 (stack65)
        %v73207 = vadd.s32 %v73199, %v73203 (stack65)
        %v73209 = vshll.u32 %v73203, 13 (stack73)
        %v73210 = vshrl.u32 %v73203, 19 (stack74)
        %v73211 = vor.u32 %v73209, %v73210 (stack75)
        %v73212 = vxor.u32 %v73207, %v73211 (stack76)
        %v73215 = vadd.s32 %v73207, %v73212 (stack65)
        %v73217 = vshll.u32 %v73212, 15 (stack73)
        %v73218 = vshrl.u32 %v73212, 17 (stack74)
        %v73219 = vor.u32 %v73217, %v73218 (stack75)
        %v73220 = vxor.u32 %v73215, %v73219 (stack76)
        %v73223 = vadd.s32 %v73215, %v73220 (stack65)
        %v73225 = vshll.u32 %v73220, 26 (stack73)
        %v73226 = vshrl.u32 %v73220, 6 (stack74)
        %v73227 = vor.u32 %v73225, %v73226 (stack75)
        %v73228 = vxor.u32 %v73223, %v73227 (stack76)
        %v73231 = vadd.s32 %v73223, %v73228 (stack65)
        %v73235 = vadd.s32 %v73231, %v9 (stack65)
        %v73237 = vshll.u32 %v73228, 6 (stack73)
        %v73238 = vshrl.u32 %v73228, 26 (stack74)
        %v73239 = vor.u32 %v73237, %v73238 (stack75)
        %v73240 = vxor.u32 %v73231, %v73239 (stack76)
        %v73243 = vadd.s32 %v73240, %v8 (stack65)
        %v73247 = vadd.s32 %v73243, 1 (stack65)
        %v73251 = vadd.s32 %v73235, %v73247 (stack65)
        %v73253 = vshll.u32 %v73247, 17 (stack73)
        %v73254 = vshrl.u32 %v73247, 15 (stack74)
        %v73255 = vor.u32 %v73253, %v73254 (stack75)
        %v73256 = vxor.u32 %v73251, %v73255 (stack76)
        %v73259 = vadd.s32 %v73251, %v73256 (stack65)
        %v73261 = vshll.u32 %v73256, 29 (stack73)
        %v73262 = vshrl.u32 %v73256, 3 (stack74)
        %v73263 = vor.u32 %v73261, %v73262 (stack75)
        %v73264 = vxor.u32 %v73259, %v73263 (stack76)
        %v73267 = vadd.s32 %v73259, %v73264 (stack65)
        %v73269 = vshll.u32 %v73264, 16 (stack73)
        %v73270 = vshrl.u32 %v73264, 16 (stack74)
        %v73271 = vor.u32 %v73269, %v73270 (stack75)
        %v73272 = vxor.u32 %v73267, %v73271 (stack76)
        %v73275 = vadd.s32 %v73267, %v73272 (stack65)
        %v73279 = vadd.s32 %v73275, %v8 (stack65)
        %v73281 = vshll.u32 %v73272, 24 (stack73)
        %v73282 = vshrl.u32 %v73272, 8 (stack74)
        %v73283 = vor.u32 %v73281, %v73282 (stack75)
        %v73284 = vxor.u32 %v73275, %v73283 (stack76)
        %v73287 = vadd.s32 %v73284, %v10 (stack65)
        %v73291 = vadd.s32 %v73287, 2 (stack65)
        %v73295 = vadd.s32 %v73279, %v73291 (stack65)
        %v73297 = vshll.u32 %v73291, 13 (stack73)
        %v73298 = vshrl.u32 %v73291, 19 (stack74)
        %v73299 = vor.u32 %v73297, %v73298 (stack75)
        %v73300 = vxor.u32 %v73295, %v73299 (stack76)
        %v73303 = vadd.s32 %v73295, %v73300 (stack65)
        %v73305 = vshll.u32 %v73300, 15 (stack73)
        %v73306 = vshrl.u32 %v73300, 17 (stack74)
        %v73307 = vor.u32 %v73305, %v73306 (stack75)
        %v73308 = vxor.u32 %v73303, %v73307 (stack76)
        %v73311 = vadd.s32 %v73303, %v73308 (stack65)
        %v73313 = vshll.u32 %v73308, 26 (stack73)
        %v73314 = vshrl.u32 %v73308, 6 (stack74)
        %v73315 = vor.u32 %v73313, %v73314 (stack75)
        %v73316 = vxor.u32 %v73311, %v73315 (stack76)
        %v73319 = vadd.s32 %v73311, %v73316 (stack65)
        %v73323 = vadd.s32 %v73319, %v10 (stack65)
        %v73325 = vshll.u32 %v73316, 6 (stack73)
        %v73326 = vshrl.u32 %v73316, 26 (stack74)
        %v73327 = vor.u32 %v73325, %v73326 (stack75)
        %v73328 = vxor.u32 %v73319, %v73327 (stack76)
        %v73331 = vadd.s32 %v73328, %v9 (stack65)
        %v73335 = vadd.s32 %v73331, 3 (stack65)
        %v73339 = vadd.s32 %v73323, %v73335 (stack65)
        %v73341 = vshll.u32 %v73335, 17 (stack73)
        %v73342 = vshrl.u32 %v73335, 15 (stack74)
        %v73343 = vor.u32 %v73341, %v73342 (stack75)
        %v73344 = vxor.u32 %v73339, %v73343 (stack76)
        %v73347 = vadd.s32 %v73339, %v73344 (stack65)
        %v73349 = vshll.u32 %v73344, 29 (stack73)
        %v73350 = vshrl.u32 %v73344, 3 (stack74)
        %v73351 = vor.u32 %v73349, %v73350 (stack75)
        %v73352 = vxor.u32 %v73347, %v73351 (stack76)
        %v73355 = vadd.s32 %v73347, %v73352 (stack65)
        %v73357 = vshll.u32 %v73352, 16 (stack73)
        %v73358 = vshrl.u32 %v73352, 16 (stack74)
        %v73359 = vor.u32 %v73357, %v73358 (stack75)
        %v73360 = vxor.u32 %v73355, %v73359 (stack76)
        %v73363 = vadd.s32 %v73355, %v73360 (stack65)
        %v73367 = vadd.s32 %v73363, %v9 (stack65)
        %v73369 = vshll.u32 %v73360, 24 (stack73)
        %v73370 = vshrl.u32 %v73360, 8 (stack74)
        %v73371 = vor.u32 %v73369, %v73370 (stack75)
        %v73372 = vxor.u32 %v73363, %v73371 (stack76)
        %v73375 = vadd.s32 %v73372, %v8 (stack65)
        %v73379 = vadd.s32 %v73375, 4 (stack65)
        %v73383 = vadd.s32 %v73367, %v73379 (stack65)
        %v73385 = vshll.u32 %v73379, 13 (stack73)
        %v73386 = vshrl.u32 %v73379, 19 (stack74)
        %v73387 = vor.u32 %v73385, %v73386 (stack75)
        %v73388 = vxor.u32 %v73383, %v73387 (stack76)
        %v73391 = vadd.s32 %v73383, %v73388 (stack65)
        %v73393 = vshll.u32 %v73388, 15 (stack73)
        %v73394 = vshrl.u32 %v73388, 17 (stack74)
        %v73395 = vor.u32 %v73393, %v73394 (stack75)
        %v73396 = vxor.u32 %v73391, %v73395 (stack76)
        %v73399 = vadd.s32 %v73391, %v73396 (stack65)
        %v73401 = vshll.u32 %v73396, 26 (stack73)
        %v73402 = vshrl.u32 %v73396, 6 (stack74)
        %v73403 = vor.u32 %v73401, %v73402 (stack75)
        %v73404 = vxor.u32 %v73399, %v73403 (stack76)
        %v73407 = vadd.s32 %v73399, %v73404 (stack65)
        %v73411 = vadd.s32 %v73407, %v8 (stack65)
        %v73413 = vshll.u32 %v73404, 6 (stack73)
        %v73414 = vshrl.u32 %v73404, 26 (stack74)
        %v73415 = vor.u32 %v73413, %v73414 (stack75)
        %v73416 = vxor.u32 %v73407, %v73415 (stack76)
        %v73419 = vadd.s32 %v73416, %v10 (stack65)
        %v73423 = vadd.s32 %v73419, 5 (stack65)
        %v73425 = vxor.u32 %v73411, %v73423 (stack76)
        %v73426 = vand.u32.u8 %v73425, 255 (stack77)
        %v73427 = vand.u32 %v73426, 65535 (stack78)
        %v73428 = vshrl.u32 %v73427, 1 (stack79)
        %v73429 = vor.u32 %v73428, 16256 (stack75)
        %v73430 = vand.u32.u16 %v73429, 65535 (stack80)
        %v73431 = vunpack.i.l.bf16 %v73430 (stack81)
        %v73435 = vadd.f32 %v73431, -1.0 (stack82)
        %v73439 = vmul.f32 %v73435, 2.0 (stack83)
        %v73443 = vadd.f32 %v73439, -0.99609375 (stack82)
        %v73447 = vmax.f32 -0.99609375, %v73443 (stack84)
        %v73449 = vand.u32 2147483647, %v73447 (stack85)
        %vm73452 = vcmp.eq.f32.partialorder %v73449, 1.0 (stack86)
        %v73457 = vmul.f32 %v73447, inf (stack83)
        %v73459 = vxor.u32 %v73447, 2147483648 (stack87)
        %v73462 = vmul.f32 %v73447, %v73459 (stack83)
        %v73464 = vadd.f32 %v73462, 1.0 (stack88)
        %v73465 = vlog2.pop %v73464 (stack89)
        %v73466 = vmul.f32 %v73465, 0.6931472 (stack90)
        %v73467 = vmul.f32 -0.5, %v73462 (stack91)
        %v73468 = vadd.f32 %v73467, 1.0 (stack92)
        %v73469 = vmul.f32 %v73468, %v73462 (stack93)
        %v73470 = vand.u32 2147483647, %v73462 (stack94)
        %vm73471 = vcmp.lt.f32.partialorder %v73470, 0.0004427343 (stack95)
        %v73472 = vsel /*vm=*/%vm73471, /*on_true_vy=*/%v73469, /*on_false_vx=*/%v73466 (stack96)
        %v73473 = vxor.u32 %v73472, 2147483648 (stack87)
        %vm73476 = vcmp.lt.f32.partialorder %v73473, 5.0 (stack86)
        %v73481 = vsel /*vm=*/%vm73476, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v73485 = vsel /*vm=*/%vm73476, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v73489 = vsel /*vm=*/%vm73476, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v73493 = vsel /*vm=*/%vm73476, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v73497 = vsel /*vm=*/%vm73476, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v73501 = vsel /*vm=*/%vm73476, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v73505 = vsel /*vm=*/%vm73476, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v73509 = vsel /*vm=*/%vm73476, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v73513 = vsel /*vm=*/%vm73476, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v73517 = vadd.f32 %v73473, -2.5 (stack82)
        %v73519 = vrsqrt.pop %v73473 (stack97)
        %v73520 = vmul.f32 %v73473, %v73519 (stack98)
        %vm73521 = vcmp.eq.f32.partialorder %v73473, inf (stack99)
        %v73522 = vsel /*vm=*/%vm73521, /*on_true_vy=*/%v73473, /*on_false_vx=*/%v73520 (stack100)
        %vm73523 = vcmp.eq.f32.partialorder %v73473, 0.0 (stack101)
        %v73524 = vand.u32 %v73473, 2147483648 (stack102)
        %v73525 = vsel /*vm=*/%vm73523, /*on_true_vy=*/%v73524, /*on_false_vx=*/%v73522 (stack103)
        %v73528 = vadd.f32 %v73525, -3.0 (stack82)
        %v73532 = vsel /*vm=*/%vm73476, /*on_true_vy=*/%v73517, /*on_false_vx=*/%v73528 (stack72)
        %v73536 = vmul.f32 %v73513, %v73532 (stack83)
        %v73540 = vadd.f32 %v73509, %v73536 (stack82)
        %v73544 = vmul.f32 %v73540, %v73532 (stack83)
        %v73548 = vadd.f32 %v73505, %v73544 (stack82)
        %v73552 = vmul.f32 %v73548, %v73532 (stack83)
        %v73556 = vadd.f32 %v73501, %v73552 (stack82)
        %v73560 = vmul.f32 %v73556, %v73532 (stack83)
        %v73564 = vadd.f32 %v73497, %v73560 (stack82)
        %v73568 = vmul.f32 %v73564, %v73532 (stack83)
        %v73572 = vadd.f32 %v73493, %v73568 (stack82)
        %v73576 = vmul.f32 %v73572, %v73532 (stack83)
        %v73580 = vadd.f32 %v73489, %v73576 (stack82)
        %v73584 = vmul.f32 %v73580, %v73532 (stack83)
        %v73588 = vadd.f32 %v73485, %v73584 (stack82)
        %v73592 = vmul.f32 %v73588, %v73532 (stack83)
        %v73596 = vadd.f32 %v73481, %v73592 (stack82)
        %v73600 = vmul.f32 %v73596, %v73447 (stack83)
        %v73604 = vsel /*vm=*/%vm73452, /*on_true_vy=*/%v73457, /*on_false_vx=*/%v73600 (stack72)
        %v73608 = vmul.f32 %v73604, 1.4140625 (stack83)
        %s73610 = scalar_lea.vmem %s280, 588 [#allocation0] (stack107)
        %v73611 = vpack.c.bf16 0.0, %v73608 (stack104)
        %73612 = vst [vmem:[%s73610] sm:$0xf] /*vst_source=*/%v73611 (stack105)
        %v73615 = vadd.s32 %v2842, %v71307 (stack65)
        %s73617 = smul.u32 128, %s27 (stack66)
        %v73618 = vlaneseq (stack67)
        %v73619 = vand.u32 %v73618, 127 (stack68)
        %v73620 = vstv %s73617 (stack69)
        %v73621 = vadd.s32 %v73619, %v73620 (stack70)
        %v73625 = vadd.s32 %v73615, %v73621 (stack65)
        %vm73629 = vcmp.lt.u32.totalorder %v73625, %v73615 (stack71)
        %vm73634 = vcmp.lt.u32.totalorder %v73615, %v2842 (stack71)
        %v73639 = vadd.s32 %v2829, %v71290 (stack65)
        %v73643 = vadd.s32 %v73639, 1 (stack65)
        %v73647 = vsel /*vm=*/%vm73634, /*on_true_vy=*/%v73643, /*on_false_vx=*/%v73639 (stack72)
        %v73651 = vadd.s32 %v73647, 1 (stack65)
        %v73655 = vsel /*vm=*/%vm73629, /*on_true_vy=*/%v73651, /*on_false_vx=*/%v73647 (stack72)
        %v73660 = vadd.s32 %v73655, %v10 (stack65)
        %v73664 = vadd.s32 %v73625, %v9 (stack65)
        %v73668 = vadd.s32 %v73660, %v73664 (stack65)
        %v73670 = vshll.u32 %v73664, 13 (stack73)
        %v73671 = vshrl.u32 %v73664, 19 (stack74)
        %v73672 = vor.u32 %v73670, %v73671 (stack75)
        %v73673 = vxor.u32 %v73668, %v73672 (stack76)
        %v73676 = vadd.s32 %v73668, %v73673 (stack65)
        %v73678 = vshll.u32 %v73673, 15 (stack73)
        %v73679 = vshrl.u32 %v73673, 17 (stack74)
        %v73680 = vor.u32 %v73678, %v73679 (stack75)
        %v73681 = vxor.u32 %v73676, %v73680 (stack76)
        %v73684 = vadd.s32 %v73676, %v73681 (stack65)
        %v73686 = vshll.u32 %v73681, 26 (stack73)
        %v73687 = vshrl.u32 %v73681, 6 (stack74)
        %v73688 = vor.u32 %v73686, %v73687 (stack75)
        %v73689 = vxor.u32 %v73684, %v73688 (stack76)
        %v73692 = vadd.s32 %v73684, %v73689 (stack65)
        %v73696 = vadd.s32 %v73692, %v9 (stack65)
        %v73698 = vshll.u32 %v73689, 6 (stack73)
        %v73699 = vshrl.u32 %v73689, 26 (stack74)
        %v73700 = vor.u32 %v73698, %v73699 (stack75)
        %v73701 = vxor.u32 %v73692, %v73700 (stack76)
        %v73704 = vadd.s32 %v73701, %v8 (stack65)
        %v73708 = vadd.s32 %v73704, 1 (stack65)
        %v73712 = vadd.s32 %v73696, %v73708 (stack65)
        %v73714 = vshll.u32 %v73708, 17 (stack73)
        %v73715 = vshrl.u32 %v73708, 15 (stack74)
        %v73716 = vor.u32 %v73714, %v73715 (stack75)
        %v73717 = vxor.u32 %v73712, %v73716 (stack76)
        %v73720 = vadd.s32 %v73712, %v73717 (stack65)
        %v73722 = vshll.u32 %v73717, 29 (stack73)
        %v73723 = vshrl.u32 %v73717, 3 (stack74)
        %v73724 = vor.u32 %v73722, %v73723 (stack75)
        %v73725 = vxor.u32 %v73720, %v73724 (stack76)
        %v73728 = vadd.s32 %v73720, %v73725 (stack65)
        %v73730 = vshll.u32 %v73725, 16 (stack73)
        %v73731 = vshrl.u32 %v73725, 16 (stack74)
        %v73732 = vor.u32 %v73730, %v73731 (stack75)
        %v73733 = vxor.u32 %v73728, %v73732 (stack76)
        %v73736 = vadd.s32 %v73728, %v73733 (stack65)
        %v73740 = vadd.s32 %v73736, %v8 (stack65)
        %v73742 = vshll.u32 %v73733, 24 (stack73)
        %v73743 = vshrl.u32 %v73733, 8 (stack74)
        %v73744 = vor.u32 %v73742, %v73743 (stack75)
        %v73745 = vxor.u32 %v73736, %v73744 (stack76)
        %v73748 = vadd.s32 %v73745, %v10 (stack65)
        %v73752 = vadd.s32 %v73748, 2 (stack65)
        %v73756 = vadd.s32 %v73740, %v73752 (stack65)
        %v73758 = vshll.u32 %v73752, 13 (stack73)
        %v73759 = vshrl.u32 %v73752, 19 (stack74)
        %v73760 = vor.u32 %v73758, %v73759 (stack75)
        %v73761 = vxor.u32 %v73756, %v73760 (stack76)
        %v73764 = vadd.s32 %v73756, %v73761 (stack65)
        %v73766 = vshll.u32 %v73761, 15 (stack73)
        %v73767 = vshrl.u32 %v73761, 17 (stack74)
        %v73768 = vor.u32 %v73766, %v73767 (stack75)
        %v73769 = vxor.u32 %v73764, %v73768 (stack76)
        %v73772 = vadd.s32 %v73764, %v73769 (stack65)
        %v73774 = vshll.u32 %v73769, 26 (stack73)
        %v73775 = vshrl.u32 %v73769, 6 (stack74)
        %v73776 = vor.u32 %v73774, %v73775 (stack75)
        %v73777 = vxor.u32 %v73772, %v73776 (stack76)
        %v73780 = vadd.s32 %v73772, %v73777 (stack65)
        %v73784 = vadd.s32 %v73780, %v10 (stack65)
        %v73786 = vshll.u32 %v73777, 6 (stack73)
        %v73787 = vshrl.u32 %v73777, 26 (stack74)
        %v73788 = vor.u32 %v73786, %v73787 (stack75)
        %v73789 = vxor.u32 %v73780, %v73788 (stack76)
        %v73792 = vadd.s32 %v73789, %v9 (stack65)
        %v73796 = vadd.s32 %v73792, 3 (stack65)
        %v73800 = vadd.s32 %v73784, %v73796 (stack65)
        %v73802 = vshll.u32 %v73796, 17 (stack73)
        %v73803 = vshrl.u32 %v73796, 15 (stack74)
        %v73804 = vor.u32 %v73802, %v73803 (stack75)
        %v73805 = vxor.u32 %v73800, %v73804 (stack76)
        %v73808 = vadd.s32 %v73800, %v73805 (stack65)
        %v73810 = vshll.u32 %v73805, 29 (stack73)
        %v73811 = vshrl.u32 %v73805, 3 (stack74)
        %v73812 = vor.u32 %v73810, %v73811 (stack75)
        %v73813 = vxor.u32 %v73808, %v73812 (stack76)
        %v73816 = vadd.s32 %v73808, %v73813 (stack65)
        %v73818 = vshll.u32 %v73813, 16 (stack73)
        %v73819 = vshrl.u32 %v73813, 16 (stack74)
        %v73820 = vor.u32 %v73818, %v73819 (stack75)
        %v73821 = vxor.u32 %v73816, %v73820 (stack76)
        %v73824 = vadd.s32 %v73816, %v73821 (stack65)
        %v73828 = vadd.s32 %v73824, %v9 (stack65)
        %v73830 = vshll.u32 %v73821, 24 (stack73)
        %v73831 = vshrl.u32 %v73821, 8 (stack74)
        %v73832 = vor.u32 %v73830, %v73831 (stack75)
        %v73833 = vxor.u32 %v73824, %v73832 (stack76)
        %v73836 = vadd.s32 %v73833, %v8 (stack65)
        %v73840 = vadd.s32 %v73836, 4 (stack65)
        %v73844 = vadd.s32 %v73828, %v73840 (stack65)
        %v73846 = vshll.u32 %v73840, 13 (stack73)
        %v73847 = vshrl.u32 %v73840, 19 (stack74)
        %v73848 = vor.u32 %v73846, %v73847 (stack75)
        %v73849 = vxor.u32 %v73844, %v73848 (stack76)
        %v73852 = vadd.s32 %v73844, %v73849 (stack65)
        %v73854 = vshll.u32 %v73849, 15 (stack73)
        %v73855 = vshrl.u32 %v73849, 17 (stack74)
        %v73856 = vor.u32 %v73854, %v73855 (stack75)
        %v73857 = vxor.u32 %v73852, %v73856 (stack76)
        %v73860 = vadd.s32 %v73852, %v73857 (stack65)
        %v73862 = vshll.u32 %v73857, 26 (stack73)
        %v73863 = vshrl.u32 %v73857, 6 (stack74)
        %v73864 = vor.u32 %v73862, %v73863 (stack75)
        %v73865 = vxor.u32 %v73860, %v73864 (stack76)
        %v73868 = vadd.s32 %v73860, %v73865 (stack65)
        %v73872 = vadd.s32 %v73868, %v8 (stack65)
        %v73874 = vshll.u32 %v73865, 6 (stack73)
        %v73875 = vshrl.u32 %v73865, 26 (stack74)
        %v73876 = vor.u32 %v73874, %v73875 (stack75)
        %v73877 = vxor.u32 %v73868, %v73876 (stack76)
        %v73880 = vadd.s32 %v73877, %v10 (stack65)
        %v73884 = vadd.s32 %v73880, 5 (stack65)
        %v73886 = vxor.u32 %v73872, %v73884 (stack76)
        %v73887 = vand.u32.u8 %v73886, 255 (stack77)
        %v73888 = vand.u32 %v73887, 65535 (stack78)
        %v73889 = vshrl.u32 %v73888, 1 (stack79)
        %v73890 = vor.u32 %v73889, 16256 (stack75)
        %v73891 = vand.u32.u16 %v73890, 65535 (stack80)
        %v73892 = vunpack.i.l.bf16 %v73891 (stack81)
        %v73896 = vadd.f32 %v73892, -1.0 (stack82)
        %v73900 = vmul.f32 %v73896, 2.0 (stack83)
        %v73904 = vadd.f32 %v73900, -0.99609375 (stack82)
        %v73908 = vmax.f32 -0.99609375, %v73904 (stack84)
        %v73910 = vand.u32 2147483647, %v73908 (stack85)
        %vm73913 = vcmp.eq.f32.partialorder %v73910, 1.0 (stack86)
        %v73918 = vmul.f32 %v73908, inf (stack83)
        %v73920 = vxor.u32 %v73908, 2147483648 (stack87)
        %v73923 = vmul.f32 %v73908, %v73920 (stack83)
        %v73925 = vadd.f32 %v73923, 1.0 (stack88)
        %v73926 = vlog2.pop %v73925 (stack89)
        %v73927 = vmul.f32 %v73926, 0.6931472 (stack90)
        %v73928 = vmul.f32 -0.5, %v73923 (stack91)
        %v73929 = vadd.f32 %v73928, 1.0 (stack92)
        %v73930 = vmul.f32 %v73929, %v73923 (stack93)
        %v73931 = vand.u32 2147483647, %v73923 (stack94)
        %vm73932 = vcmp.lt.f32.partialorder %v73931, 0.0004427343 (stack95)
        %v73933 = vsel /*vm=*/%vm73932, /*on_true_vy=*/%v73930, /*on_false_vx=*/%v73927 (stack96)
        %v73934 = vxor.u32 %v73933, 2147483648 (stack87)
        %vm73937 = vcmp.lt.f32.partialorder %v73934, 5.0 (stack86)
        %v73942 = vsel /*vm=*/%vm73937, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v73946 = vsel /*vm=*/%vm73937, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v73950 = vsel /*vm=*/%vm73937, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v73954 = vsel /*vm=*/%vm73937, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v73958 = vsel /*vm=*/%vm73937, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v73962 = vsel /*vm=*/%vm73937, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v73966 = vsel /*vm=*/%vm73937, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v73970 = vsel /*vm=*/%vm73937, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v73974 = vsel /*vm=*/%vm73937, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v73978 = vadd.f32 %v73934, -2.5 (stack82)
        %v73980 = vrsqrt.pop %v73934 (stack97)
        %v73981 = vmul.f32 %v73934, %v73980 (stack98)
        %vm73982 = vcmp.eq.f32.partialorder %v73934, inf (stack99)
        %v73983 = vsel /*vm=*/%vm73982, /*on_true_vy=*/%v73934, /*on_false_vx=*/%v73981 (stack100)
        %vm73984 = vcmp.eq.f32.partialorder %v73934, 0.0 (stack101)
        %v73985 = vand.u32 %v73934, 2147483648 (stack102)
        %v73986 = vsel /*vm=*/%vm73984, /*on_true_vy=*/%v73985, /*on_false_vx=*/%v73983 (stack103)
        %v73989 = vadd.f32 %v73986, -3.0 (stack82)
        %v73993 = vsel /*vm=*/%vm73937, /*on_true_vy=*/%v73978, /*on_false_vx=*/%v73989 (stack72)
        %v73997 = vmul.f32 %v73974, %v73993 (stack83)
        %v74001 = vadd.f32 %v73970, %v73997 (stack82)
        %v74005 = vmul.f32 %v74001, %v73993 (stack83)
        %v74009 = vadd.f32 %v73966, %v74005 (stack82)
        %v74013 = vmul.f32 %v74009, %v73993 (stack83)
        %v74017 = vadd.f32 %v73962, %v74013 (stack82)
        %v74021 = vmul.f32 %v74017, %v73993 (stack83)
        %v74025 = vadd.f32 %v73958, %v74021 (stack82)
        %v74029 = vmul.f32 %v74025, %v73993 (stack83)
        %v74033 = vadd.f32 %v73954, %v74029 (stack82)
        %v74037 = vmul.f32 %v74033, %v73993 (stack83)
        %v74041 = vadd.f32 %v73950, %v74037 (stack82)
        %v74045 = vmul.f32 %v74041, %v73993 (stack83)
        %v74049 = vadd.f32 %v73946, %v74045 (stack82)
        %v74053 = vmul.f32 %v74049, %v73993 (stack83)
        %v74057 = vadd.f32 %v73942, %v74053 (stack82)
        %v74061 = vmul.f32 %v74057, %v73908 (stack83)
        %v74065 = vsel /*vm=*/%vm73913, /*on_true_vy=*/%v73918, /*on_false_vx=*/%v74061 (stack72)
        %v74069 = vmul.f32 %v74065, 1.4140625 (stack83)
        %s74071 = scalar_lea.vmem %s280, 716 [#allocation0] (stack107)
        %v74072 = vpack.c.bf16 0.0, %v74069 (stack104)
        %74073 = vst [vmem:[%s74071] sm:$0xf] /*vst_source=*/%v74072 (stack105)
        %v74076 = vadd.s32 %v3329, %v71307 (stack65)
        %s74078 = smul.u32 128, %s27 (stack66)
        %v74079 = vlaneseq (stack67)
        %v74080 = vand.u32 %v74079, 127 (stack68)
        %v74081 = vstv %s74078 (stack69)
        %v74082 = vadd.s32 %v74080, %v74081 (stack70)
        %v74086 = vadd.s32 %v74076, %v74082 (stack65)
        %vm74090 = vcmp.lt.u32.totalorder %v74086, %v74076 (stack71)
        %vm74095 = vcmp.lt.u32.totalorder %v74076, %v3329 (stack71)
        %v74100 = vadd.s32 %v3316, %v71290 (stack65)
        %v74104 = vadd.s32 %v74100, 1 (stack65)
        %v74108 = vsel /*vm=*/%vm74095, /*on_true_vy=*/%v74104, /*on_false_vx=*/%v74100 (stack72)
        %v74112 = vadd.s32 %v74108, 1 (stack65)
        %v74116 = vsel /*vm=*/%vm74090, /*on_true_vy=*/%v74112, /*on_false_vx=*/%v74108 (stack72)
        %v74121 = vadd.s32 %v74116, %v10 (stack65)
        %v74125 = vadd.s32 %v74086, %v9 (stack65)
        %v74129 = vadd.s32 %v74121, %v74125 (stack65)
        %v74131 = vshll.u32 %v74125, 13 (stack73)
        %v74132 = vshrl.u32 %v74125, 19 (stack74)
        %v74133 = vor.u32 %v74131, %v74132 (stack75)
        %v74134 = vxor.u32 %v74129, %v74133 (stack76)
        %v74137 = vadd.s32 %v74129, %v74134 (stack65)
        %v74139 = vshll.u32 %v74134, 15 (stack73)
        %v74140 = vshrl.u32 %v74134, 17 (stack74)
        %v74141 = vor.u32 %v74139, %v74140 (stack75)
        %v74142 = vxor.u32 %v74137, %v74141 (stack76)
        %v74145 = vadd.s32 %v74137, %v74142 (stack65)
        %v74147 = vshll.u32 %v74142, 26 (stack73)
        %v74148 = vshrl.u32 %v74142, 6 (stack74)
        %v74149 = vor.u32 %v74147, %v74148 (stack75)
        %v74150 = vxor.u32 %v74145, %v74149 (stack76)
        %v74153 = vadd.s32 %v74145, %v74150 (stack65)
        %v74157 = vadd.s32 %v74153, %v9 (stack65)
        %v74159 = vshll.u32 %v74150, 6 (stack73)
        %v74160 = vshrl.u32 %v74150, 26 (stack74)
        %v74161 = vor.u32 %v74159, %v74160 (stack75)
        %v74162 = vxor.u32 %v74153, %v74161 (stack76)
        %v74165 = vadd.s32 %v74162, %v8 (stack65)
        %v74169 = vadd.s32 %v74165, 1 (stack65)
        %v74173 = vadd.s32 %v74157, %v74169 (stack65)
        %v74175 = vshll.u32 %v74169, 17 (stack73)
        %v74176 = vshrl.u32 %v74169, 15 (stack74)
        %v74177 = vor.u32 %v74175, %v74176 (stack75)
        %v74178 = vxor.u32 %v74173, %v74177 (stack76)
        %v74181 = vadd.s32 %v74173, %v74178 (stack65)
        %v74183 = vshll.u32 %v74178, 29 (stack73)
        %v74184 = vshrl.u32 %v74178, 3 (stack74)
        %v74185 = vor.u32 %v74183, %v74184 (stack75)
        %v74186 = vxor.u32 %v74181, %v74185 (stack76)
        %v74189 = vadd.s32 %v74181, %v74186 (stack65)
        %v74191 = vshll.u32 %v74186, 16 (stack73)
        %v74192 = vshrl.u32 %v74186, 16 (stack74)
        %v74193 = vor.u32 %v74191, %v74192 (stack75)
        %v74194 = vxor.u32 %v74189, %v74193 (stack76)
        %v74197 = vadd.s32 %v74189, %v74194 (stack65)
        %v74201 = vadd.s32 %v74197, %v8 (stack65)
        %v74203 = vshll.u32 %v74194, 24 (stack73)
        %v74204 = vshrl.u32 %v74194, 8 (stack74)
        %v74205 = vor.u32 %v74203, %v74204 (stack75)
        %v74206 = vxor.u32 %v74197, %v74205 (stack76)
        %v74209 = vadd.s32 %v74206, %v10 (stack65)
        %v74213 = vadd.s32 %v74209, 2 (stack65)
        %v74217 = vadd.s32 %v74201, %v74213 (stack65)
        %v74219 = vshll.u32 %v74213, 13 (stack73)
        %v74220 = vshrl.u32 %v74213, 19 (stack74)
        %v74221 = vor.u32 %v74219, %v74220 (stack75)
        %v74222 = vxor.u32 %v74217, %v74221 (stack76)
        %v74225 = vadd.s32 %v74217, %v74222 (stack65)
        %v74227 = vshll.u32 %v74222, 15 (stack73)
        %v74228 = vshrl.u32 %v74222, 17 (stack74)
        %v74229 = vor.u32 %v74227, %v74228 (stack75)
        %v74230 = vxor.u32 %v74225, %v74229 (stack76)
        %v74233 = vadd.s32 %v74225, %v74230 (stack65)
        %v74235 = vshll.u32 %v74230, 26 (stack73)
        %v74236 = vshrl.u32 %v74230, 6 (stack74)
        %v74237 = vor.u32 %v74235, %v74236 (stack75)
        %v74238 = vxor.u32 %v74233, %v74237 (stack76)
        %v74241 = vadd.s32 %v74233, %v74238 (stack65)
        %v74245 = vadd.s32 %v74241, %v10 (stack65)
        %v74247 = vshll.u32 %v74238, 6 (stack73)
        %v74248 = vshrl.u32 %v74238, 26 (stack74)
        %v74249 = vor.u32 %v74247, %v74248 (stack75)
        %v74250 = vxor.u32 %v74241, %v74249 (stack76)
        %v74253 = vadd.s32 %v74250, %v9 (stack65)
        %v74257 = vadd.s32 %v74253, 3 (stack65)
        %v74261 = vadd.s32 %v74245, %v74257 (stack65)
        %v74263 = vshll.u32 %v74257, 17 (stack73)
        %v74264 = vshrl.u32 %v74257, 15 (stack74)
        %v74265 = vor.u32 %v74263, %v74264 (stack75)
        %v74266 = vxor.u32 %v74261, %v74265 (stack76)
        %v74269 = vadd.s32 %v74261, %v74266 (stack65)
        %v74271 = vshll.u32 %v74266, 29 (stack73)
        %v74272 = vshrl.u32 %v74266, 3 (stack74)
        %v74273 = vor.u32 %v74271, %v74272 (stack75)
        %v74274 = vxor.u32 %v74269, %v74273 (stack76)
        %v74277 = vadd.s32 %v74269, %v74274 (stack65)
        %v74279 = vshll.u32 %v74274, 16 (stack73)
        %v74280 = vshrl.u32 %v74274, 16 (stack74)
        %v74281 = vor.u32 %v74279, %v74280 (stack75)
        %v74282 = vxor.u32 %v74277, %v74281 (stack76)
        %v74285 = vadd.s32 %v74277, %v74282 (stack65)
        %v74289 = vadd.s32 %v74285, %v9 (stack65)
        %v74291 = vshll.u32 %v74282, 24 (stack73)
        %v74292 = vshrl.u32 %v74282, 8 (stack74)
        %v74293 = vor.u32 %v74291, %v74292 (stack75)
        %v74294 = vxor.u32 %v74285, %v74293 (stack76)
        %v74297 = vadd.s32 %v74294, %v8 (stack65)
        %v74301 = vadd.s32 %v74297, 4 (stack65)
        %v74305 = vadd.s32 %v74289, %v74301 (stack65)
        %v74307 = vshll.u32 %v74301, 13 (stack73)
        %v74308 = vshrl.u32 %v74301, 19 (stack74)
        %v74309 = vor.u32 %v74307, %v74308 (stack75)
        %v74310 = vxor.u32 %v74305, %v74309 (stack76)
        %v74313 = vadd.s32 %v74305, %v74310 (stack65)
        %v74315 = vshll.u32 %v74310, 15 (stack73)
        %v74316 = vshrl.u32 %v74310, 17 (stack74)
        %v74317 = vor.u32 %v74315, %v74316 (stack75)
        %v74318 = vxor.u32 %v74313, %v74317 (stack76)
        %v74321 = vadd.s32 %v74313, %v74318 (stack65)
        %v74323 = vshll.u32 %v74318, 26 (stack73)
        %v74324 = vshrl.u32 %v74318, 6 (stack74)
        %v74325 = vor.u32 %v74323, %v74324 (stack75)
        %v74326 = vxor.u32 %v74321, %v74325 (stack76)
        %v74329 = vadd.s32 %v74321, %v74326 (stack65)
        %v74333 = vadd.s32 %v74329, %v8 (stack65)
        %v74335 = vshll.u32 %v74326, 6 (stack73)
        %v74336 = vshrl.u32 %v74326, 26 (stack74)
        %v74337 = vor.u32 %v74335, %v74336 (stack75)
        %v74338 = vxor.u32 %v74329, %v74337 (stack76)
        %v74341 = vadd.s32 %v74338, %v10 (stack65)
        %v74345 = vadd.s32 %v74341, 5 (stack65)
        %v74347 = vxor.u32 %v74333, %v74345 (stack76)
        %v74348 = vand.u32.u8 %v74347, 255 (stack77)
        %v74349 = vand.u32 %v74348, 65535 (stack78)
        %v74350 = vshrl.u32 %v74349, 1 (stack79)
        %v74351 = vor.u32 %v74350, 16256 (stack75)
        %v74352 = vand.u32.u16 %v74351, 65535 (stack80)
        %v74353 = vunpack.i.l.bf16 %v74352 (stack81)
        %v74357 = vadd.f32 %v74353, -1.0 (stack82)
        %v74361 = vmul.f32 %v74357, 2.0 (stack83)
        %v74365 = vadd.f32 %v74361, -0.99609375 (stack82)
        %v74369 = vmax.f32 -0.99609375, %v74365 (stack84)
        %v74371 = vand.u32 2147483647, %v74369 (stack85)
        %vm74374 = vcmp.eq.f32.partialorder %v74371, 1.0 (stack86)
        %v74379 = vmul.f32 %v74369, inf (stack83)
        %v74381 = vxor.u32 %v74369, 2147483648 (stack87)
        %v74384 = vmul.f32 %v74369, %v74381 (stack83)
        %v74386 = vadd.f32 %v74384, 1.0 (stack88)
        %v74387 = vlog2.pop %v74386 (stack89)
        %v74388 = vmul.f32 %v74387, 0.6931472 (stack90)
        %v74389 = vmul.f32 -0.5, %v74384 (stack91)
        %v74390 = vadd.f32 %v74389, 1.0 (stack92)
        %v74391 = vmul.f32 %v74390, %v74384 (stack93)
        %v74392 = vand.u32 2147483647, %v74384 (stack94)
        %vm74393 = vcmp.lt.f32.partialorder %v74392, 0.0004427343 (stack95)
        %v74394 = vsel /*vm=*/%vm74393, /*on_true_vy=*/%v74391, /*on_false_vx=*/%v74388 (stack96)
        %v74395 = vxor.u32 %v74394, 2147483648 (stack87)
        %vm74398 = vcmp.lt.f32.partialorder %v74395, 5.0 (stack86)
        %v74403 = vsel /*vm=*/%vm74398, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v74407 = vsel /*vm=*/%vm74398, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v74411 = vsel /*vm=*/%vm74398, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v74415 = vsel /*vm=*/%vm74398, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v74419 = vsel /*vm=*/%vm74398, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v74423 = vsel /*vm=*/%vm74398, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v74427 = vsel /*vm=*/%vm74398, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v74431 = vsel /*vm=*/%vm74398, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v74435 = vsel /*vm=*/%vm74398, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v74439 = vadd.f32 %v74395, -2.5 (stack82)
        %v74441 = vrsqrt.pop %v74395 (stack97)
        %v74442 = vmul.f32 %v74395, %v74441 (stack98)
        %vm74443 = vcmp.eq.f32.partialorder %v74395, inf (stack99)
        %v74444 = vsel /*vm=*/%vm74443, /*on_true_vy=*/%v74395, /*on_false_vx=*/%v74442 (stack100)
        %vm74445 = vcmp.eq.f32.partialorder %v74395, 0.0 (stack101)
        %v74446 = vand.u32 %v74395, 2147483648 (stack102)
        %v74447 = vsel /*vm=*/%vm74445, /*on_true_vy=*/%v74446, /*on_false_vx=*/%v74444 (stack103)
        %v74450 = vadd.f32 %v74447, -3.0 (stack82)
        %v74454 = vsel /*vm=*/%vm74398, /*on_true_vy=*/%v74439, /*on_false_vx=*/%v74450 (stack72)
        %v74458 = vmul.f32 %v74435, %v74454 (stack83)
        %v74462 = vadd.f32 %v74431, %v74458 (stack82)
        %v74466 = vmul.f32 %v74462, %v74454 (stack83)
        %v74470 = vadd.f32 %v74427, %v74466 (stack82)
        %v74474 = vmul.f32 %v74470, %v74454 (stack83)
        %v74478 = vadd.f32 %v74423, %v74474 (stack82)
        %v74482 = vmul.f32 %v74478, %v74454 (stack83)
        %v74486 = vadd.f32 %v74419, %v74482 (stack82)
        %v74490 = vmul.f32 %v74486, %v74454 (stack83)
        %v74494 = vadd.f32 %v74415, %v74490 (stack82)
        %v74498 = vmul.f32 %v74494, %v74454 (stack83)
        %v74502 = vadd.f32 %v74411, %v74498 (stack82)
        %v74506 = vmul.f32 %v74502, %v74454 (stack83)
        %v74510 = vadd.f32 %v74407, %v74506 (stack82)
        %v74514 = vmul.f32 %v74510, %v74454 (stack83)
        %v74518 = vadd.f32 %v74403, %v74514 (stack82)
        %v74522 = vmul.f32 %v74518, %v74369 (stack83)
        %v74526 = vsel /*vm=*/%vm74374, /*on_true_vy=*/%v74379, /*on_false_vx=*/%v74522 (stack72)
        %v74530 = vmul.f32 %v74526, 1.4140625 (stack83)
        %s74532 = scalar_lea.vmem %s280, 844 [#allocation0] (stack107)
        %v74533 = vpack.c.bf16 0.0, %v74530 (stack104)
        %74534 = vst [vmem:[%s74532] sm:$0xf] /*vst_source=*/%v74533 (stack105)
        %v74537 = vadd.s32 %v3816, %v71307 (stack65)
        %s74539 = smul.u32 128, %s27 (stack66)
        %v74540 = vlaneseq (stack67)
        %v74541 = vand.u32 %v74540, 127 (stack68)
        %v74542 = vstv %s74539 (stack69)
        %v74543 = vadd.s32 %v74541, %v74542 (stack70)
        %v74547 = vadd.s32 %v74537, %v74543 (stack65)
        %vm74551 = vcmp.lt.u32.totalorder %v74547, %v74537 (stack71)
        %vm74556 = vcmp.lt.u32.totalorder %v74537, %v3816 (stack71)
        %v74561 = vadd.s32 %v3803, %v71290 (stack65)
        %v74565 = vadd.s32 %v74561, 1 (stack65)
        %v74569 = vsel /*vm=*/%vm74556, /*on_true_vy=*/%v74565, /*on_false_vx=*/%v74561 (stack72)
        %v74573 = vadd.s32 %v74569, 1 (stack65)
        %v74577 = vsel /*vm=*/%vm74551, /*on_true_vy=*/%v74573, /*on_false_vx=*/%v74569 (stack72)
        %v74582 = vadd.s32 %v74577, %v10 (stack65)
        %v74586 = vadd.s32 %v74547, %v9 (stack65)
        %v74590 = vadd.s32 %v74582, %v74586 (stack65)
        %v74592 = vshll.u32 %v74586, 13 (stack73)
        %v74593 = vshrl.u32 %v74586, 19 (stack74)
        %v74594 = vor.u32 %v74592, %v74593 (stack75)
        %v74595 = vxor.u32 %v74590, %v74594 (stack76)
        %v74598 = vadd.s32 %v74590, %v74595 (stack65)
        %v74600 = vshll.u32 %v74595, 15 (stack73)
        %v74601 = vshrl.u32 %v74595, 17 (stack74)
        %v74602 = vor.u32 %v74600, %v74601 (stack75)
        %v74603 = vxor.u32 %v74598, %v74602 (stack76)
        %v74606 = vadd.s32 %v74598, %v74603 (stack65)
        %v74608 = vshll.u32 %v74603, 26 (stack73)
        %v74609 = vshrl.u32 %v74603, 6 (stack74)
        %v74610 = vor.u32 %v74608, %v74609 (stack75)
        %v74611 = vxor.u32 %v74606, %v74610 (stack76)
        %v74614 = vadd.s32 %v74606, %v74611 (stack65)
        %v74618 = vadd.s32 %v74614, %v9 (stack65)
        %v74620 = vshll.u32 %v74611, 6 (stack73)
        %v74621 = vshrl.u32 %v74611, 26 (stack74)
        %v74622 = vor.u32 %v74620, %v74621 (stack75)
        %v74623 = vxor.u32 %v74614, %v74622 (stack76)
        %v74626 = vadd.s32 %v74623, %v8 (stack65)
        %v74630 = vadd.s32 %v74626, 1 (stack65)
        %v74634 = vadd.s32 %v74618, %v74630 (stack65)
        %v74636 = vshll.u32 %v74630, 17 (stack73)
        %v74637 = vshrl.u32 %v74630, 15 (stack74)
        %v74638 = vor.u32 %v74636, %v74637 (stack75)
        %v74639 = vxor.u32 %v74634, %v74638 (stack76)
        %v74642 = vadd.s32 %v74634, %v74639 (stack65)
        %v74644 = vshll.u32 %v74639, 29 (stack73)
        %v74645 = vshrl.u32 %v74639, 3 (stack74)
        %v74646 = vor.u32 %v74644, %v74645 (stack75)
        %v74647 = vxor.u32 %v74642, %v74646 (stack76)
        %v74650 = vadd.s32 %v74642, %v74647 (stack65)
        %v74652 = vshll.u32 %v74647, 16 (stack73)
        %v74653 = vshrl.u32 %v74647, 16 (stack74)
        %v74654 = vor.u32 %v74652, %v74653 (stack75)
        %v74655 = vxor.u32 %v74650, %v74654 (stack76)
        %v74658 = vadd.s32 %v74650, %v74655 (stack65)
        %v74662 = vadd.s32 %v74658, %v8 (stack65)
        %v74664 = vshll.u32 %v74655, 24 (stack73)
        %v74665 = vshrl.u32 %v74655, 8 (stack74)
        %v74666 = vor.u32 %v74664, %v74665 (stack75)
        %v74667 = vxor.u32 %v74658, %v74666 (stack76)
        %v74670 = vadd.s32 %v74667, %v10 (stack65)
        %v74674 = vadd.s32 %v74670, 2 (stack65)
        %v74678 = vadd.s32 %v74662, %v74674 (stack65)
        %v74680 = vshll.u32 %v74674, 13 (stack73)
        %v74681 = vshrl.u32 %v74674, 19 (stack74)
        %v74682 = vor.u32 %v74680, %v74681 (stack75)
        %v74683 = vxor.u32 %v74678, %v74682 (stack76)
        %v74686 = vadd.s32 %v74678, %v74683 (stack65)
        %v74688 = vshll.u32 %v74683, 15 (stack73)
        %v74689 = vshrl.u32 %v74683, 17 (stack74)
        %v74690 = vor.u32 %v74688, %v74689 (stack75)
        %v74691 = vxor.u32 %v74686, %v74690 (stack76)
        %v74694 = vadd.s32 %v74686, %v74691 (stack65)
        %v74696 = vshll.u32 %v74691, 26 (stack73)
        %v74697 = vshrl.u32 %v74691, 6 (stack74)
        %v74698 = vor.u32 %v74696, %v74697 (stack75)
        %v74699 = vxor.u32 %v74694, %v74698 (stack76)
        %v74702 = vadd.s32 %v74694, %v74699 (stack65)
        %v74706 = vadd.s32 %v74702, %v10 (stack65)
        %v74708 = vshll.u32 %v74699, 6 (stack73)
        %v74709 = vshrl.u32 %v74699, 26 (stack74)
        %v74710 = vor.u32 %v74708, %v74709 (stack75)
        %v74711 = vxor.u32 %v74702, %v74710 (stack76)
        %v74714 = vadd.s32 %v74711, %v9 (stack65)
        %v74718 = vadd.s32 %v74714, 3 (stack65)
        %v74722 = vadd.s32 %v74706, %v74718 (stack65)
        %v74724 = vshll.u32 %v74718, 17 (stack73)
        %v74725 = vshrl.u32 %v74718, 15 (stack74)
        %v74726 = vor.u32 %v74724, %v74725 (stack75)
        %v74727 = vxor.u32 %v74722, %v74726 (stack76)
        %v74730 = vadd.s32 %v74722, %v74727 (stack65)
        %v74732 = vshll.u32 %v74727, 29 (stack73)
        %v74733 = vshrl.u32 %v74727, 3 (stack74)
        %v74734 = vor.u32 %v74732, %v74733 (stack75)
        %v74735 = vxor.u32 %v74730, %v74734 (stack76)
        %v74738 = vadd.s32 %v74730, %v74735 (stack65)
        %v74740 = vshll.u32 %v74735, 16 (stack73)
        %v74741 = vshrl.u32 %v74735, 16 (stack74)
        %v74742 = vor.u32 %v74740, %v74741 (stack75)
        %v74743 = vxor.u32 %v74738, %v74742 (stack76)
        %v74746 = vadd.s32 %v74738, %v74743 (stack65)
        %v74750 = vadd.s32 %v74746, %v9 (stack65)
        %v74752 = vshll.u32 %v74743, 24 (stack73)
        %v74753 = vshrl.u32 %v74743, 8 (stack74)
        %v74754 = vor.u32 %v74752, %v74753 (stack75)
        %v74755 = vxor.u32 %v74746, %v74754 (stack76)
        %v74758 = vadd.s32 %v74755, %v8 (stack65)
        %v74762 = vadd.s32 %v74758, 4 (stack65)
        %v74766 = vadd.s32 %v74750, %v74762 (stack65)
        %v74768 = vshll.u32 %v74762, 13 (stack73)
        %v74769 = vshrl.u32 %v74762, 19 (stack74)
        %v74770 = vor.u32 %v74768, %v74769 (stack75)
        %v74771 = vxor.u32 %v74766, %v74770 (stack76)
        %v74774 = vadd.s32 %v74766, %v74771 (stack65)
        %v74776 = vshll.u32 %v74771, 15 (stack73)
        %v74777 = vshrl.u32 %v74771, 17 (stack74)
        %v74778 = vor.u32 %v74776, %v74777 (stack75)
        %v74779 = vxor.u32 %v74774, %v74778 (stack76)
        %v74782 = vadd.s32 %v74774, %v74779 (stack65)
        %v74784 = vshll.u32 %v74779, 26 (stack73)
        %v74785 = vshrl.u32 %v74779, 6 (stack74)
        %v74786 = vor.u32 %v74784, %v74785 (stack75)
        %v74787 = vxor.u32 %v74782, %v74786 (stack76)
        %v74790 = vadd.s32 %v74782, %v74787 (stack65)
        %v74794 = vadd.s32 %v74790, %v8 (stack65)
        %v74796 = vshll.u32 %v74787, 6 (stack73)
        %v74797 = vshrl.u32 %v74787, 26 (stack74)
        %v74798 = vor.u32 %v74796, %v74797 (stack75)
        %v74799 = vxor.u32 %v74790, %v74798 (stack76)
        %v74802 = vadd.s32 %v74799, %v10 (stack65)
        %v74806 = vadd.s32 %v74802, 5 (stack65)
        %v74808 = vxor.u32 %v74794, %v74806 (stack76)
        %v74809 = vand.u32.u8 %v74808, 255 (stack77)
        %v74810 = vand.u32 %v74809, 65535 (stack78)
        %v74811 = vshrl.u32 %v74810, 1 (stack79)
        %v74812 = vor.u32 %v74811, 16256 (stack75)
        %v74813 = vand.u32.u16 %v74812, 65535 (stack80)
        %v74814 = vunpack.i.l.bf16 %v74813 (stack81)
        %v74818 = vadd.f32 %v74814, -1.0 (stack82)
        %v74822 = vmul.f32 %v74818, 2.0 (stack83)
        %v74826 = vadd.f32 %v74822, -0.99609375 (stack82)
        %v74830 = vmax.f32 -0.99609375, %v74826 (stack84)
        %v74832 = vand.u32 2147483647, %v74830 (stack85)
        %vm74835 = vcmp.eq.f32.partialorder %v74832, 1.0 (stack86)
        %v74840 = vmul.f32 %v74830, inf (stack83)
        %v74842 = vxor.u32 %v74830, 2147483648 (stack87)
        %v74845 = vmul.f32 %v74830, %v74842 (stack83)
        %v74847 = vadd.f32 %v74845, 1.0 (stack88)
        %v74848 = vlog2.pop %v74847 (stack89)
        %v74849 = vmul.f32 %v74848, 0.6931472 (stack90)
        %v74850 = vmul.f32 -0.5, %v74845 (stack91)
        %v74851 = vadd.f32 %v74850, 1.0 (stack92)
        %v74852 = vmul.f32 %v74851, %v74845 (stack93)
        %v74853 = vand.u32 2147483647, %v74845 (stack94)
        %vm74854 = vcmp.lt.f32.partialorder %v74853, 0.0004427343 (stack95)
        %v74855 = vsel /*vm=*/%vm74854, /*on_true_vy=*/%v74852, /*on_false_vx=*/%v74849 (stack96)
        %v74856 = vxor.u32 %v74855, 2147483648 (stack87)
        %vm74859 = vcmp.lt.f32.partialorder %v74856, 5.0 (stack86)
        %v74864 = vsel /*vm=*/%vm74859, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v74868 = vsel /*vm=*/%vm74859, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v74872 = vsel /*vm=*/%vm74859, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v74876 = vsel /*vm=*/%vm74859, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v74880 = vsel /*vm=*/%vm74859, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v74884 = vsel /*vm=*/%vm74859, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v74888 = vsel /*vm=*/%vm74859, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v74892 = vsel /*vm=*/%vm74859, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v74896 = vsel /*vm=*/%vm74859, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v74900 = vadd.f32 %v74856, -2.5 (stack82)
        %v74902 = vrsqrt.pop %v74856 (stack97)
        %v74903 = vmul.f32 %v74856, %v74902 (stack98)
        %vm74904 = vcmp.eq.f32.partialorder %v74856, inf (stack99)
        %v74905 = vsel /*vm=*/%vm74904, /*on_true_vy=*/%v74856, /*on_false_vx=*/%v74903 (stack100)
        %vm74906 = vcmp.eq.f32.partialorder %v74856, 0.0 (stack101)
        %v74907 = vand.u32 %v74856, 2147483648 (stack102)
        %v74908 = vsel /*vm=*/%vm74906, /*on_true_vy=*/%v74907, /*on_false_vx=*/%v74905 (stack103)
        %v74911 = vadd.f32 %v74908, -3.0 (stack82)
        %v74915 = vsel /*vm=*/%vm74859, /*on_true_vy=*/%v74900, /*on_false_vx=*/%v74911 (stack72)
        %v74919 = vmul.f32 %v74896, %v74915 (stack83)
        %v74923 = vadd.f32 %v74892, %v74919 (stack82)
        %v74927 = vmul.f32 %v74923, %v74915 (stack83)
        %v74931 = vadd.f32 %v74888, %v74927 (stack82)
        %v74935 = vmul.f32 %v74931, %v74915 (stack83)
        %v74939 = vadd.f32 %v74884, %v74935 (stack82)
        %v74943 = vmul.f32 %v74939, %v74915 (stack83)
        %v74947 = vadd.f32 %v74880, %v74943 (stack82)
        %v74951 = vmul.f32 %v74947, %v74915 (stack83)
        %v74955 = vadd.f32 %v74876, %v74951 (stack82)
        %v74959 = vmul.f32 %v74955, %v74915 (stack83)
        %v74963 = vadd.f32 %v74872, %v74959 (stack82)
        %v74967 = vmul.f32 %v74963, %v74915 (stack83)
        %v74971 = vadd.f32 %v74868, %v74967 (stack82)
        %v74975 = vmul.f32 %v74971, %v74915 (stack83)
        %v74979 = vadd.f32 %v74864, %v74975 (stack82)
        %v74983 = vmul.f32 %v74979, %v74830 (stack83)
        %v74987 = vsel /*vm=*/%vm74835, /*on_true_vy=*/%v74840, /*on_false_vx=*/%v74983 (stack72)
        %v74991 = vmul.f32 %v74987, 1.4140625 (stack83)
        %s74993 = scalar_lea.vmem %s280, 972 [#allocation0] (stack107)
        %v74994 = vpack.c.bf16 0.0, %v74991 (stack104)
        %74995 = vst [vmem:[%s74993] sm:$0xf] /*vst_source=*/%v74994 (stack105)
        %s74996 = sadd.s32 %s339, 160 (stack106)
        %s74997 = sshrl.u32 %s74996, 10 (stack49)
        %p74998 = scmp.lt.s32.totalorder 1, %s74997 (stack50)
        %s74999 = scalar_select /*predicate=*/%p74998, /*on_true=*/1, /*on_false=*/%s74997 (stack51)
        %s75000 = sand.u32 %s74996, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s75001 = sshrl.u32 %s75000, 7 (stack53)
        %s75002 = sand.u32 %s75000, 127 /* smod.u32 w/div 128 */ (stack54)
        %s75003 = smul.addr %s74999, 8 (stack55)
        %s75004 = scalar_lea.vmem %s3, %s75003 (stack56)
        %s75006 = scalar_lea.vmem %s75004, %s75001 (stack57)
        %v75007 = vld [vmem:[%s75006] ss:$0 sm:$0xff] (stack58)
        %s75008 = sand.u32 %s75002, 255 (stack59)
        %s75010 = sor.u32 256, %s75008 (stack60)
        %75011 = vbcast.lane.b32.xlu0 %v75007, %s75010 (stack61)
        %v75012 = vpop.permute.xlu0 %75011 (stack62)
        %s75013 = sadd.s32 %s347, 160 (stack106)
        %s75014 = sshrl.u32 %s75013, 10 (stack49)
        %p75015 = scmp.lt.s32.totalorder 1, %s75014 (stack50)
        %s75016 = scalar_select /*predicate=*/%p75015, /*on_true=*/1, /*on_false=*/%s75014 (stack51)
        %s75017 = sand.u32 %s75013, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s75018 = sshrl.u32 %s75017, 7 (stack53)
        %s75019 = sand.u32 %s75017, 127 /* smod.u32 w/div 128 */ (stack54)
        %s75020 = smul.addr %s75016, 8 (stack55)
        %s75021 = scalar_lea.vmem %s5, %s75020 (stack56)
        %s75023 = scalar_lea.vmem %s75021, %s75018 (stack57)
        %v75024 = vld [vmem:[%s75023] ss:$0 sm:$0xff] (stack58)
        %s75025 = sand.u32 %s75019, 255 (stack59)
        %s75027 = sor.u32 256, %s75025 (stack60)
        %75028 = vbcast.lane.b32.xlu0 %v75024, %s75027 (stack61)
        %v75029 = vpop.permute.xlu0 %75028 (stack62)
        %v75032 = vadd.s32 %v408, %v75029 (stack65)
        %s75034 = smul.u32 128, %s27 (stack66)
        %v75035 = vlaneseq (stack67)
        %v75036 = vand.u32 %v75035, 127 (stack68)
        %v75037 = vstv %s75034 (stack69)
        %v75038 = vadd.s32 %v75036, %v75037 (stack70)
        %v75042 = vadd.s32 %v75032, %v75038 (stack65)
        %vm75046 = vcmp.lt.u32.totalorder %v75042, %v75032 (stack71)
        %vm75051 = vcmp.lt.u32.totalorder %v75032, %v408 (stack71)
        %v75056 = vadd.s32 %v380, %v75012 (stack65)
        %v75060 = vadd.s32 %v75056, 1 (stack65)
        %v75064 = vsel /*vm=*/%vm75051, /*on_true_vy=*/%v75060, /*on_false_vx=*/%v75056 (stack72)
        %v75068 = vadd.s32 %v75064, 1 (stack65)
        %v75072 = vsel /*vm=*/%vm75046, /*on_true_vy=*/%v75068, /*on_false_vx=*/%v75064 (stack72)
        %v75077 = vadd.s32 %v75072, %v10 (stack65)
        %v75081 = vadd.s32 %v75042, %v9 (stack65)
        %v75085 = vadd.s32 %v75077, %v75081 (stack65)
        %v75087 = vshll.u32 %v75081, 13 (stack73)
        %v75088 = vshrl.u32 %v75081, 19 (stack74)
        %v75089 = vor.u32 %v75087, %v75088 (stack75)
        %v75090 = vxor.u32 %v75085, %v75089 (stack76)
        %v75093 = vadd.s32 %v75085, %v75090 (stack65)
        %v75095 = vshll.u32 %v75090, 15 (stack73)
        %v75096 = vshrl.u32 %v75090, 17 (stack74)
        %v75097 = vor.u32 %v75095, %v75096 (stack75)
        %v75098 = vxor.u32 %v75093, %v75097 (stack76)
        %v75101 = vadd.s32 %v75093, %v75098 (stack65)
        %v75103 = vshll.u32 %v75098, 26 (stack73)
        %v75104 = vshrl.u32 %v75098, 6 (stack74)
        %v75105 = vor.u32 %v75103, %v75104 (stack75)
        %v75106 = vxor.u32 %v75101, %v75105 (stack76)
        %v75109 = vadd.s32 %v75101, %v75106 (stack65)
        %v75113 = vadd.s32 %v75109, %v9 (stack65)
        %v75115 = vshll.u32 %v75106, 6 (stack73)
        %v75116 = vshrl.u32 %v75106, 26 (stack74)
        %v75117 = vor.u32 %v75115, %v75116 (stack75)
        %v75118 = vxor.u32 %v75109, %v75117 (stack76)
        %v75121 = vadd.s32 %v75118, %v8 (stack65)
        %v75125 = vadd.s32 %v75121, 1 (stack65)
        %v75129 = vadd.s32 %v75113, %v75125 (stack65)
        %v75131 = vshll.u32 %v75125, 17 (stack73)
        %v75132 = vshrl.u32 %v75125, 15 (stack74)
        %v75133 = vor.u32 %v75131, %v75132 (stack75)
        %v75134 = vxor.u32 %v75129, %v75133 (stack76)
        %v75137 = vadd.s32 %v75129, %v75134 (stack65)
        %v75139 = vshll.u32 %v75134, 29 (stack73)
        %v75140 = vshrl.u32 %v75134, 3 (stack74)
        %v75141 = vor.u32 %v75139, %v75140 (stack75)
        %v75142 = vxor.u32 %v75137, %v75141 (stack76)
        %v75145 = vadd.s32 %v75137, %v75142 (stack65)
        %v75147 = vshll.u32 %v75142, 16 (stack73)
        %v75148 = vshrl.u32 %v75142, 16 (stack74)
        %v75149 = vor.u32 %v75147, %v75148 (stack75)
        %v75150 = vxor.u32 %v75145, %v75149 (stack76)
        %v75153 = vadd.s32 %v75145, %v75150 (stack65)
        %v75157 = vadd.s32 %v75153, %v8 (stack65)
        %v75159 = vshll.u32 %v75150, 24 (stack73)
        %v75160 = vshrl.u32 %v75150, 8 (stack74)
        %v75161 = vor.u32 %v75159, %v75160 (stack75)
        %v75162 = vxor.u32 %v75153, %v75161 (stack76)
        %v75165 = vadd.s32 %v75162, %v10 (stack65)
        %v75169 = vadd.s32 %v75165, 2 (stack65)
        %v75173 = vadd.s32 %v75157, %v75169 (stack65)
        %v75175 = vshll.u32 %v75169, 13 (stack73)
        %v75176 = vshrl.u32 %v75169, 19 (stack74)
        %v75177 = vor.u32 %v75175, %v75176 (stack75)
        %v75178 = vxor.u32 %v75173, %v75177 (stack76)
        %v75181 = vadd.s32 %v75173, %v75178 (stack65)
        %v75183 = vshll.u32 %v75178, 15 (stack73)
        %v75184 = vshrl.u32 %v75178, 17 (stack74)
        %v75185 = vor.u32 %v75183, %v75184 (stack75)
        %v75186 = vxor.u32 %v75181, %v75185 (stack76)
        %v75189 = vadd.s32 %v75181, %v75186 (stack65)
        %v75191 = vshll.u32 %v75186, 26 (stack73)
        %v75192 = vshrl.u32 %v75186, 6 (stack74)
        %v75193 = vor.u32 %v75191, %v75192 (stack75)
        %v75194 = vxor.u32 %v75189, %v75193 (stack76)
        %v75197 = vadd.s32 %v75189, %v75194 (stack65)
        %v75201 = vadd.s32 %v75197, %v10 (stack65)
        %v75203 = vshll.u32 %v75194, 6 (stack73)
        %v75204 = vshrl.u32 %v75194, 26 (stack74)
        %v75205 = vor.u32 %v75203, %v75204 (stack75)
        %v75206 = vxor.u32 %v75197, %v75205 (stack76)
        %v75209 = vadd.s32 %v75206, %v9 (stack65)
        %v75213 = vadd.s32 %v75209, 3 (stack65)
        %v75217 = vadd.s32 %v75201, %v75213 (stack65)
        %v75219 = vshll.u32 %v75213, 17 (stack73)
        %v75220 = vshrl.u32 %v75213, 15 (stack74)
        %v75221 = vor.u32 %v75219, %v75220 (stack75)
        %v75222 = vxor.u32 %v75217, %v75221 (stack76)
        %v75225 = vadd.s32 %v75217, %v75222 (stack65)
        %v75227 = vshll.u32 %v75222, 29 (stack73)
        %v75228 = vshrl.u32 %v75222, 3 (stack74)
        %v75229 = vor.u32 %v75227, %v75228 (stack75)
        %v75230 = vxor.u32 %v75225, %v75229 (stack76)
        %v75233 = vadd.s32 %v75225, %v75230 (stack65)
        %v75235 = vshll.u32 %v75230, 16 (stack73)
        %v75236 = vshrl.u32 %v75230, 16 (stack74)
        %v75237 = vor.u32 %v75235, %v75236 (stack75)
        %v75238 = vxor.u32 %v75233, %v75237 (stack76)
        %v75241 = vadd.s32 %v75233, %v75238 (stack65)
        %v75245 = vadd.s32 %v75241, %v9 (stack65)
        %v75247 = vshll.u32 %v75238, 24 (stack73)
        %v75248 = vshrl.u32 %v75238, 8 (stack74)
        %v75249 = vor.u32 %v75247, %v75248 (stack75)
        %v75250 = vxor.u32 %v75241, %v75249 (stack76)
        %v75253 = vadd.s32 %v75250, %v8 (stack65)
        %v75257 = vadd.s32 %v75253, 4 (stack65)
        %v75261 = vadd.s32 %v75245, %v75257 (stack65)
        %v75263 = vshll.u32 %v75257, 13 (stack73)
        %v75264 = vshrl.u32 %v75257, 19 (stack74)
        %v75265 = vor.u32 %v75263, %v75264 (stack75)
        %v75266 = vxor.u32 %v75261, %v75265 (stack76)
        %v75269 = vadd.s32 %v75261, %v75266 (stack65)
        %v75271 = vshll.u32 %v75266, 15 (stack73)
        %v75272 = vshrl.u32 %v75266, 17 (stack74)
        %v75273 = vor.u32 %v75271, %v75272 (stack75)
        %v75274 = vxor.u32 %v75269, %v75273 (stack76)
        %v75277 = vadd.s32 %v75269, %v75274 (stack65)
        %v75279 = vshll.u32 %v75274, 26 (stack73)
        %v75280 = vshrl.u32 %v75274, 6 (stack74)
        %v75281 = vor.u32 %v75279, %v75280 (stack75)
        %v75282 = vxor.u32 %v75277, %v75281 (stack76)
        %v75285 = vadd.s32 %v75277, %v75282 (stack65)
        %v75289 = vadd.s32 %v75285, %v8 (stack65)
        %v75291 = vshll.u32 %v75282, 6 (stack73)
        %v75292 = vshrl.u32 %v75282, 26 (stack74)
        %v75293 = vor.u32 %v75291, %v75292 (stack75)
        %v75294 = vxor.u32 %v75285, %v75293 (stack76)
        %v75297 = vadd.s32 %v75294, %v10 (stack65)
        %v75301 = vadd.s32 %v75297, 5 (stack65)
        %v75303 = vxor.u32 %v75289, %v75301 (stack76)
        %v75304 = vand.u32.u8 %v75303, 255 (stack77)
        %v75305 = vand.u32 %v75304, 65535 (stack78)
        %v75306 = vshrl.u32 %v75305, 1 (stack79)
        %v75307 = vor.u32 %v75306, 16256 (stack75)
        %v75308 = vand.u32.u16 %v75307, 65535 (stack80)
        %v75309 = vunpack.i.l.bf16 %v75308 (stack81)
        %v75313 = vadd.f32 %v75309, -1.0 (stack82)
        %v75317 = vmul.f32 %v75313, 2.0 (stack83)
        %v75321 = vadd.f32 %v75317, -0.99609375 (stack82)
        %v75325 = vmax.f32 -0.99609375, %v75321 (stack84)
        %v75327 = vand.u32 2147483647, %v75325 (stack85)
        %vm75330 = vcmp.eq.f32.partialorder %v75327, 1.0 (stack86)
        %v75335 = vmul.f32 %v75325, inf (stack83)
        %v75337 = vxor.u32 %v75325, 2147483648 (stack87)
        %v75340 = vmul.f32 %v75325, %v75337 (stack83)
        %v75342 = vadd.f32 %v75340, 1.0 (stack88)
        %v75343 = vlog2.pop %v75342 (stack89)
        %v75344 = vmul.f32 %v75343, 0.6931472 (stack90)
        %v75345 = vmul.f32 -0.5, %v75340 (stack91)
        %v75346 = vadd.f32 %v75345, 1.0 (stack92)
        %v75347 = vmul.f32 %v75346, %v75340 (stack93)
        %v75348 = vand.u32 2147483647, %v75340 (stack94)
        %vm75349 = vcmp.lt.f32.partialorder %v75348, 0.0004427343 (stack95)
        %v75350 = vsel /*vm=*/%vm75349, /*on_true_vy=*/%v75347, /*on_false_vx=*/%v75344 (stack96)
        %v75351 = vxor.u32 %v75350, 2147483648 (stack87)
        %vm75354 = vcmp.lt.f32.partialorder %v75351, 5.0 (stack86)
        %v75359 = vsel /*vm=*/%vm75354, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v75363 = vsel /*vm=*/%vm75354, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v75367 = vsel /*vm=*/%vm75354, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v75371 = vsel /*vm=*/%vm75354, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v75375 = vsel /*vm=*/%vm75354, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v75379 = vsel /*vm=*/%vm75354, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v75383 = vsel /*vm=*/%vm75354, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v75387 = vsel /*vm=*/%vm75354, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v75391 = vsel /*vm=*/%vm75354, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v75395 = vadd.f32 %v75351, -2.5 (stack82)
        %v75397 = vrsqrt.pop %v75351 (stack97)
        %v75398 = vmul.f32 %v75351, %v75397 (stack98)
        %vm75399 = vcmp.eq.f32.partialorder %v75351, inf (stack99)
        %v75400 = vsel /*vm=*/%vm75399, /*on_true_vy=*/%v75351, /*on_false_vx=*/%v75398 (stack100)
        %vm75401 = vcmp.eq.f32.partialorder %v75351, 0.0 (stack101)
        %v75402 = vand.u32 %v75351, 2147483648 (stack102)
        %v75403 = vsel /*vm=*/%vm75401, /*on_true_vy=*/%v75402, /*on_false_vx=*/%v75400 (stack103)
        %v75406 = vadd.f32 %v75403, -3.0 (stack82)
        %v75410 = vsel /*vm=*/%vm75354, /*on_true_vy=*/%v75395, /*on_false_vx=*/%v75406 (stack72)
        %v75414 = vmul.f32 %v75391, %v75410 (stack83)
        %v75418 = vadd.f32 %v75387, %v75414 (stack82)
        %v75422 = vmul.f32 %v75418, %v75410 (stack83)
        %v75426 = vadd.f32 %v75383, %v75422 (stack82)
        %v75430 = vmul.f32 %v75426, %v75410 (stack83)
        %v75434 = vadd.f32 %v75379, %v75430 (stack82)
        %v75438 = vmul.f32 %v75434, %v75410 (stack83)
        %v75442 = vadd.f32 %v75375, %v75438 (stack82)
        %v75446 = vmul.f32 %v75442, %v75410 (stack83)
        %v75450 = vadd.f32 %v75371, %v75446 (stack82)
        %v75454 = vmul.f32 %v75450, %v75410 (stack83)
        %v75458 = vadd.f32 %v75367, %v75454 (stack82)
        %v75462 = vmul.f32 %v75458, %v75410 (stack83)
        %v75466 = vadd.f32 %v75363, %v75462 (stack82)
        %v75470 = vmul.f32 %v75466, %v75410 (stack83)
        %v75474 = vadd.f32 %v75359, %v75470 (stack82)
        %v75478 = vmul.f32 %v75474, %v75325 (stack83)
        %v75482 = vsel /*vm=*/%vm75330, /*on_true_vy=*/%v75335, /*on_false_vx=*/%v75478 (stack72)
        %v75486 = vmul.f32 %v75482, 1.4140625 (stack83)
        %s75488 = scalar_lea.vmem %s280, 80 [#allocation0] (stack107)
        %v75489 = vpack.c.bf16 0.0, %v75486 (stack104)
        %75490 = vst [vmem:[%s75488] sm:$0xf] /*vst_source=*/%v75489 (stack105)
        %v75493 = vadd.s32 %v894, %v75029 (stack65)
        %s75495 = smul.u32 128, %s27 (stack66)
        %v75496 = vlaneseq (stack67)
        %v75497 = vand.u32 %v75496, 127 (stack68)
        %v75498 = vstv %s75495 (stack69)
        %v75499 = vadd.s32 %v75497, %v75498 (stack70)
        %v75503 = vadd.s32 %v75493, %v75499 (stack65)
        %vm75507 = vcmp.lt.u32.totalorder %v75503, %v75493 (stack71)
        %vm75512 = vcmp.lt.u32.totalorder %v75493, %v894 (stack71)
        %v75517 = vadd.s32 %v881, %v75012 (stack65)
        %v75521 = vadd.s32 %v75517, 1 (stack65)
        %v75525 = vsel /*vm=*/%vm75512, /*on_true_vy=*/%v75521, /*on_false_vx=*/%v75517 (stack72)
        %v75529 = vadd.s32 %v75525, 1 (stack65)
        %v75533 = vsel /*vm=*/%vm75507, /*on_true_vy=*/%v75529, /*on_false_vx=*/%v75525 (stack72)
        %v75538 = vadd.s32 %v75533, %v10 (stack65)
        %v75542 = vadd.s32 %v75503, %v9 (stack65)
        %v75546 = vadd.s32 %v75538, %v75542 (stack65)
        %v75548 = vshll.u32 %v75542, 13 (stack73)
        %v75549 = vshrl.u32 %v75542, 19 (stack74)
        %v75550 = vor.u32 %v75548, %v75549 (stack75)
        %v75551 = vxor.u32 %v75546, %v75550 (stack76)
        %v75554 = vadd.s32 %v75546, %v75551 (stack65)
        %v75556 = vshll.u32 %v75551, 15 (stack73)
        %v75557 = vshrl.u32 %v75551, 17 (stack74)
        %v75558 = vor.u32 %v75556, %v75557 (stack75)
        %v75559 = vxor.u32 %v75554, %v75558 (stack76)
        %v75562 = vadd.s32 %v75554, %v75559 (stack65)
        %v75564 = vshll.u32 %v75559, 26 (stack73)
        %v75565 = vshrl.u32 %v75559, 6 (stack74)
        %v75566 = vor.u32 %v75564, %v75565 (stack75)
        %v75567 = vxor.u32 %v75562, %v75566 (stack76)
        %v75570 = vadd.s32 %v75562, %v75567 (stack65)
        %v75574 = vadd.s32 %v75570, %v9 (stack65)
        %v75576 = vshll.u32 %v75567, 6 (stack73)
        %v75577 = vshrl.u32 %v75567, 26 (stack74)
        %v75578 = vor.u32 %v75576, %v75577 (stack75)
        %v75579 = vxor.u32 %v75570, %v75578 (stack76)
        %v75582 = vadd.s32 %v75579, %v8 (stack65)
        %v75586 = vadd.s32 %v75582, 1 (stack65)
        %v75590 = vadd.s32 %v75574, %v75586 (stack65)
        %v75592 = vshll.u32 %v75586, 17 (stack73)
        %v75593 = vshrl.u32 %v75586, 15 (stack74)
        %v75594 = vor.u32 %v75592, %v75593 (stack75)
        %v75595 = vxor.u32 %v75590, %v75594 (stack76)
        %v75598 = vadd.s32 %v75590, %v75595 (stack65)
        %v75600 = vshll.u32 %v75595, 29 (stack73)
        %v75601 = vshrl.u32 %v75595, 3 (stack74)
        %v75602 = vor.u32 %v75600, %v75601 (stack75)
        %v75603 = vxor.u32 %v75598, %v75602 (stack76)
        %v75606 = vadd.s32 %v75598, %v75603 (stack65)
        %v75608 = vshll.u32 %v75603, 16 (stack73)
        %v75609 = vshrl.u32 %v75603, 16 (stack74)
        %v75610 = vor.u32 %v75608, %v75609 (stack75)
        %v75611 = vxor.u32 %v75606, %v75610 (stack76)
        %v75614 = vadd.s32 %v75606, %v75611 (stack65)
        %v75618 = vadd.s32 %v75614, %v8 (stack65)
        %v75620 = vshll.u32 %v75611, 24 (stack73)
        %v75621 = vshrl.u32 %v75611, 8 (stack74)
        %v75622 = vor.u32 %v75620, %v75621 (stack75)
        %v75623 = vxor.u32 %v75614, %v75622 (stack76)
        %v75626 = vadd.s32 %v75623, %v10 (stack65)
        %v75630 = vadd.s32 %v75626, 2 (stack65)
        %v75634 = vadd.s32 %v75618, %v75630 (stack65)
        %v75636 = vshll.u32 %v75630, 13 (stack73)
        %v75637 = vshrl.u32 %v75630, 19 (stack74)
        %v75638 = vor.u32 %v75636, %v75637 (stack75)
        %v75639 = vxor.u32 %v75634, %v75638 (stack76)
        %v75642 = vadd.s32 %v75634, %v75639 (stack65)
        %v75644 = vshll.u32 %v75639, 15 (stack73)
        %v75645 = vshrl.u32 %v75639, 17 (stack74)
        %v75646 = vor.u32 %v75644, %v75645 (stack75)
        %v75647 = vxor.u32 %v75642, %v75646 (stack76)
        %v75650 = vadd.s32 %v75642, %v75647 (stack65)
        %v75652 = vshll.u32 %v75647, 26 (stack73)
        %v75653 = vshrl.u32 %v75647, 6 (stack74)
        %v75654 = vor.u32 %v75652, %v75653 (stack75)
        %v75655 = vxor.u32 %v75650, %v75654 (stack76)
        %v75658 = vadd.s32 %v75650, %v75655 (stack65)
        %v75662 = vadd.s32 %v75658, %v10 (stack65)
        %v75664 = vshll.u32 %v75655, 6 (stack73)
        %v75665 = vshrl.u32 %v75655, 26 (stack74)
        %v75666 = vor.u32 %v75664, %v75665 (stack75)
        %v75667 = vxor.u32 %v75658, %v75666 (stack76)
        %v75670 = vadd.s32 %v75667, %v9 (stack65)
        %v75674 = vadd.s32 %v75670, 3 (stack65)
        %v75678 = vadd.s32 %v75662, %v75674 (stack65)
        %v75680 = vshll.u32 %v75674, 17 (stack73)
        %v75681 = vshrl.u32 %v75674, 15 (stack74)
        %v75682 = vor.u32 %v75680, %v75681 (stack75)
        %v75683 = vxor.u32 %v75678, %v75682 (stack76)
        %v75686 = vadd.s32 %v75678, %v75683 (stack65)
        %v75688 = vshll.u32 %v75683, 29 (stack73)
        %v75689 = vshrl.u32 %v75683, 3 (stack74)
        %v75690 = vor.u32 %v75688, %v75689 (stack75)
        %v75691 = vxor.u32 %v75686, %v75690 (stack76)
        %v75694 = vadd.s32 %v75686, %v75691 (stack65)
        %v75696 = vshll.u32 %v75691, 16 (stack73)
        %v75697 = vshrl.u32 %v75691, 16 (stack74)
        %v75698 = vor.u32 %v75696, %v75697 (stack75)
        %v75699 = vxor.u32 %v75694, %v75698 (stack76)
        %v75702 = vadd.s32 %v75694, %v75699 (stack65)
        %v75706 = vadd.s32 %v75702, %v9 (stack65)
        %v75708 = vshll.u32 %v75699, 24 (stack73)
        %v75709 = vshrl.u32 %v75699, 8 (stack74)
        %v75710 = vor.u32 %v75708, %v75709 (stack75)
        %v75711 = vxor.u32 %v75702, %v75710 (stack76)
        %v75714 = vadd.s32 %v75711, %v8 (stack65)
        %v75718 = vadd.s32 %v75714, 4 (stack65)
        %v75722 = vadd.s32 %v75706, %v75718 (stack65)
        %v75724 = vshll.u32 %v75718, 13 (stack73)
        %v75725 = vshrl.u32 %v75718, 19 (stack74)
        %v75726 = vor.u32 %v75724, %v75725 (stack75)
        %v75727 = vxor.u32 %v75722, %v75726 (stack76)
        %v75730 = vadd.s32 %v75722, %v75727 (stack65)
        %v75732 = vshll.u32 %v75727, 15 (stack73)
        %v75733 = vshrl.u32 %v75727, 17 (stack74)
        %v75734 = vor.u32 %v75732, %v75733 (stack75)
        %v75735 = vxor.u32 %v75730, %v75734 (stack76)
        %v75738 = vadd.s32 %v75730, %v75735 (stack65)
        %v75740 = vshll.u32 %v75735, 26 (stack73)
        %v75741 = vshrl.u32 %v75735, 6 (stack74)
        %v75742 = vor.u32 %v75740, %v75741 (stack75)
        %v75743 = vxor.u32 %v75738, %v75742 (stack76)
        %v75746 = vadd.s32 %v75738, %v75743 (stack65)
        %v75750 = vadd.s32 %v75746, %v8 (stack65)
        %v75752 = vshll.u32 %v75743, 6 (stack73)
        %v75753 = vshrl.u32 %v75743, 26 (stack74)
        %v75754 = vor.u32 %v75752, %v75753 (stack75)
        %v75755 = vxor.u32 %v75746, %v75754 (stack76)
        %v75758 = vadd.s32 %v75755, %v10 (stack65)
        %v75762 = vadd.s32 %v75758, 5 (stack65)
        %v75764 = vxor.u32 %v75750, %v75762 (stack76)
        %v75765 = vand.u32.u8 %v75764, 255 (stack77)
        %v75766 = vand.u32 %v75765, 65535 (stack78)
        %v75767 = vshrl.u32 %v75766, 1 (stack79)
        %v75768 = vor.u32 %v75767, 16256 (stack75)
        %v75769 = vand.u32.u16 %v75768, 65535 (stack80)
        %v75770 = vunpack.i.l.bf16 %v75769 (stack81)
        %v75774 = vadd.f32 %v75770, -1.0 (stack82)
        %v75778 = vmul.f32 %v75774, 2.0 (stack83)
        %v75782 = vadd.f32 %v75778, -0.99609375 (stack82)
        %v75786 = vmax.f32 -0.99609375, %v75782 (stack84)
        %v75788 = vand.u32 2147483647, %v75786 (stack85)
        %vm75791 = vcmp.eq.f32.partialorder %v75788, 1.0 (stack86)
        %v75796 = vmul.f32 %v75786, inf (stack83)
        %v75798 = vxor.u32 %v75786, 2147483648 (stack87)
        %v75801 = vmul.f32 %v75786, %v75798 (stack83)
        %v75803 = vadd.f32 %v75801, 1.0 (stack88)
        %v75804 = vlog2.pop %v75803 (stack89)
        %v75805 = vmul.f32 %v75804, 0.6931472 (stack90)
        %v75806 = vmul.f32 -0.5, %v75801 (stack91)
        %v75807 = vadd.f32 %v75806, 1.0 (stack92)
        %v75808 = vmul.f32 %v75807, %v75801 (stack93)
        %v75809 = vand.u32 2147483647, %v75801 (stack94)
        %vm75810 = vcmp.lt.f32.partialorder %v75809, 0.0004427343 (stack95)
        %v75811 = vsel /*vm=*/%vm75810, /*on_true_vy=*/%v75808, /*on_false_vx=*/%v75805 (stack96)
        %v75812 = vxor.u32 %v75811, 2147483648 (stack87)
        %vm75815 = vcmp.lt.f32.partialorder %v75812, 5.0 (stack86)
        %v75820 = vsel /*vm=*/%vm75815, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v75824 = vsel /*vm=*/%vm75815, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v75828 = vsel /*vm=*/%vm75815, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v75832 = vsel /*vm=*/%vm75815, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v75836 = vsel /*vm=*/%vm75815, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v75840 = vsel /*vm=*/%vm75815, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v75844 = vsel /*vm=*/%vm75815, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v75848 = vsel /*vm=*/%vm75815, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v75852 = vsel /*vm=*/%vm75815, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v75856 = vadd.f32 %v75812, -2.5 (stack82)
        %v75858 = vrsqrt.pop %v75812 (stack97)
        %v75859 = vmul.f32 %v75812, %v75858 (stack98)
        %vm75860 = vcmp.eq.f32.partialorder %v75812, inf (stack99)
        %v75861 = vsel /*vm=*/%vm75860, /*on_true_vy=*/%v75812, /*on_false_vx=*/%v75859 (stack100)
        %vm75862 = vcmp.eq.f32.partialorder %v75812, 0.0 (stack101)
        %v75863 = vand.u32 %v75812, 2147483648 (stack102)
        %v75864 = vsel /*vm=*/%vm75862, /*on_true_vy=*/%v75863, /*on_false_vx=*/%v75861 (stack103)
        %v75867 = vadd.f32 %v75864, -3.0 (stack82)
        %v75871 = vsel /*vm=*/%vm75815, /*on_true_vy=*/%v75856, /*on_false_vx=*/%v75867 (stack72)
        %v75875 = vmul.f32 %v75852, %v75871 (stack83)
        %v75879 = vadd.f32 %v75848, %v75875 (stack82)
        %v75883 = vmul.f32 %v75879, %v75871 (stack83)
        %v75887 = vadd.f32 %v75844, %v75883 (stack82)
        %v75891 = vmul.f32 %v75887, %v75871 (stack83)
        %v75895 = vadd.f32 %v75840, %v75891 (stack82)
        %v75899 = vmul.f32 %v75895, %v75871 (stack83)
        %v75903 = vadd.f32 %v75836, %v75899 (stack82)
        %v75907 = vmul.f32 %v75903, %v75871 (stack83)
        %v75911 = vadd.f32 %v75832, %v75907 (stack82)
        %v75915 = vmul.f32 %v75911, %v75871 (stack83)
        %v75919 = vadd.f32 %v75828, %v75915 (stack82)
        %v75923 = vmul.f32 %v75919, %v75871 (stack83)
        %v75927 = vadd.f32 %v75824, %v75923 (stack82)
        %v75931 = vmul.f32 %v75927, %v75871 (stack83)
        %v75935 = vadd.f32 %v75820, %v75931 (stack82)
        %v75939 = vmul.f32 %v75935, %v75786 (stack83)
        %v75943 = vsel /*vm=*/%vm75791, /*on_true_vy=*/%v75796, /*on_false_vx=*/%v75939 (stack72)
        %v75947 = vmul.f32 %v75943, 1.4140625 (stack83)
        %s75949 = scalar_lea.vmem %s280, 208 [#allocation0] (stack107)
        %v75950 = vpack.c.bf16 0.0, %v75947 (stack104)
        %75951 = vst [vmem:[%s75949] sm:$0xf] /*vst_source=*/%v75950 (stack105)
        %v75954 = vadd.s32 %v1381, %v75029 (stack65)
        %s75956 = smul.u32 128, %s27 (stack66)
        %v75957 = vlaneseq (stack67)
        %v75958 = vand.u32 %v75957, 127 (stack68)
        %v75959 = vstv %s75956 (stack69)
        %v75960 = vadd.s32 %v75958, %v75959 (stack70)
        %v75964 = vadd.s32 %v75954, %v75960 (stack65)
        %vm75968 = vcmp.lt.u32.totalorder %v75964, %v75954 (stack71)
        %vm75973 = vcmp.lt.u32.totalorder %v75954, %v1381 (stack71)
        %v75978 = vadd.s32 %v1368, %v75012 (stack65)
        %v75982 = vadd.s32 %v75978, 1 (stack65)
        %v75986 = vsel /*vm=*/%vm75973, /*on_true_vy=*/%v75982, /*on_false_vx=*/%v75978 (stack72)
        %v75990 = vadd.s32 %v75986, 1 (stack65)
        %v75994 = vsel /*vm=*/%vm75968, /*on_true_vy=*/%v75990, /*on_false_vx=*/%v75986 (stack72)
        %v75999 = vadd.s32 %v75994, %v10 (stack65)
        %v76003 = vadd.s32 %v75964, %v9 (stack65)
        %v76007 = vadd.s32 %v75999, %v76003 (stack65)
        %v76009 = vshll.u32 %v76003, 13 (stack73)
        %v76010 = vshrl.u32 %v76003, 19 (stack74)
        %v76011 = vor.u32 %v76009, %v76010 (stack75)
        %v76012 = vxor.u32 %v76007, %v76011 (stack76)
        %v76015 = vadd.s32 %v76007, %v76012 (stack65)
        %v76017 = vshll.u32 %v76012, 15 (stack73)
        %v76018 = vshrl.u32 %v76012, 17 (stack74)
        %v76019 = vor.u32 %v76017, %v76018 (stack75)
        %v76020 = vxor.u32 %v76015, %v76019 (stack76)
        %v76023 = vadd.s32 %v76015, %v76020 (stack65)
        %v76025 = vshll.u32 %v76020, 26 (stack73)
        %v76026 = vshrl.u32 %v76020, 6 (stack74)
        %v76027 = vor.u32 %v76025, %v76026 (stack75)
        %v76028 = vxor.u32 %v76023, %v76027 (stack76)
        %v76031 = vadd.s32 %v76023, %v76028 (stack65)
        %v76035 = vadd.s32 %v76031, %v9 (stack65)
        %v76037 = vshll.u32 %v76028, 6 (stack73)
        %v76038 = vshrl.u32 %v76028, 26 (stack74)
        %v76039 = vor.u32 %v76037, %v76038 (stack75)
        %v76040 = vxor.u32 %v76031, %v76039 (stack76)
        %v76043 = vadd.s32 %v76040, %v8 (stack65)
        %v76047 = vadd.s32 %v76043, 1 (stack65)
        %v76051 = vadd.s32 %v76035, %v76047 (stack65)
        %v76053 = vshll.u32 %v76047, 17 (stack73)
        %v76054 = vshrl.u32 %v76047, 15 (stack74)
        %v76055 = vor.u32 %v76053, %v76054 (stack75)
        %v76056 = vxor.u32 %v76051, %v76055 (stack76)
        %v76059 = vadd.s32 %v76051, %v76056 (stack65)
        %v76061 = vshll.u32 %v76056, 29 (stack73)
        %v76062 = vshrl.u32 %v76056, 3 (stack74)
        %v76063 = vor.u32 %v76061, %v76062 (stack75)
        %v76064 = vxor.u32 %v76059, %v76063 (stack76)
        %v76067 = vadd.s32 %v76059, %v76064 (stack65)
        %v76069 = vshll.u32 %v76064, 16 (stack73)
        %v76070 = vshrl.u32 %v76064, 16 (stack74)
        %v76071 = vor.u32 %v76069, %v76070 (stack75)
        %v76072 = vxor.u32 %v76067, %v76071 (stack76)
        %v76075 = vadd.s32 %v76067, %v76072 (stack65)
        %v76079 = vadd.s32 %v76075, %v8 (stack65)
        %v76081 = vshll.u32 %v76072, 24 (stack73)
        %v76082 = vshrl.u32 %v76072, 8 (stack74)
        %v76083 = vor.u32 %v76081, %v76082 (stack75)
        %v76084 = vxor.u32 %v76075, %v76083 (stack76)
        %v76087 = vadd.s32 %v76084, %v10 (stack65)
        %v76091 = vadd.s32 %v76087, 2 (stack65)
        %v76095 = vadd.s32 %v76079, %v76091 (stack65)
        %v76097 = vshll.u32 %v76091, 13 (stack73)
        %v76098 = vshrl.u32 %v76091, 19 (stack74)
        %v76099 = vor.u32 %v76097, %v76098 (stack75)
        %v76100 = vxor.u32 %v76095, %v76099 (stack76)
        %v76103 = vadd.s32 %v76095, %v76100 (stack65)
        %v76105 = vshll.u32 %v76100, 15 (stack73)
        %v76106 = vshrl.u32 %v76100, 17 (stack74)
        %v76107 = vor.u32 %v76105, %v76106 (stack75)
        %v76108 = vxor.u32 %v76103, %v76107 (stack76)
        %v76111 = vadd.s32 %v76103, %v76108 (stack65)
        %v76113 = vshll.u32 %v76108, 26 (stack73)
        %v76114 = vshrl.u32 %v76108, 6 (stack74)
        %v76115 = vor.u32 %v76113, %v76114 (stack75)
        %v76116 = vxor.u32 %v76111, %v76115 (stack76)
        %v76119 = vadd.s32 %v76111, %v76116 (stack65)
        %v76123 = vadd.s32 %v76119, %v10 (stack65)
        %v76125 = vshll.u32 %v76116, 6 (stack73)
        %v76126 = vshrl.u32 %v76116, 26 (stack74)
        %v76127 = vor.u32 %v76125, %v76126 (stack75)
        %v76128 = vxor.u32 %v76119, %v76127 (stack76)
        %v76131 = vadd.s32 %v76128, %v9 (stack65)
        %v76135 = vadd.s32 %v76131, 3 (stack65)
        %v76139 = vadd.s32 %v76123, %v76135 (stack65)
        %v76141 = vshll.u32 %v76135, 17 (stack73)
        %v76142 = vshrl.u32 %v76135, 15 (stack74)
        %v76143 = vor.u32 %v76141, %v76142 (stack75)
        %v76144 = vxor.u32 %v76139, %v76143 (stack76)
        %v76147 = vadd.s32 %v76139, %v76144 (stack65)
        %v76149 = vshll.u32 %v76144, 29 (stack73)
        %v76150 = vshrl.u32 %v76144, 3 (stack74)
        %v76151 = vor.u32 %v76149, %v76150 (stack75)
        %v76152 = vxor.u32 %v76147, %v76151 (stack76)
        %v76155 = vadd.s32 %v76147, %v76152 (stack65)
        %v76157 = vshll.u32 %v76152, 16 (stack73)
        %v76158 = vshrl.u32 %v76152, 16 (stack74)
        %v76159 = vor.u32 %v76157, %v76158 (stack75)
        %v76160 = vxor.u32 %v76155, %v76159 (stack76)
        %v76163 = vadd.s32 %v76155, %v76160 (stack65)
        %v76167 = vadd.s32 %v76163, %v9 (stack65)
        %v76169 = vshll.u32 %v76160, 24 (stack73)
        %v76170 = vshrl.u32 %v76160, 8 (stack74)
        %v76171 = vor.u32 %v76169, %v76170 (stack75)
        %v76172 = vxor.u32 %v76163, %v76171 (stack76)
        %v76175 = vadd.s32 %v76172, %v8 (stack65)
        %v76179 = vadd.s32 %v76175, 4 (stack65)
        %v76183 = vadd.s32 %v76167, %v76179 (stack65)
        %v76185 = vshll.u32 %v76179, 13 (stack73)
        %v76186 = vshrl.u32 %v76179, 19 (stack74)
        %v76187 = vor.u32 %v76185, %v76186 (stack75)
        %v76188 = vxor.u32 %v76183, %v76187 (stack76)
        %v76191 = vadd.s32 %v76183, %v76188 (stack65)
        %v76193 = vshll.u32 %v76188, 15 (stack73)
        %v76194 = vshrl.u32 %v76188, 17 (stack74)
        %v76195 = vor.u32 %v76193, %v76194 (stack75)
        %v76196 = vxor.u32 %v76191, %v76195 (stack76)
        %v76199 = vadd.s32 %v76191, %v76196 (stack65)
        %v76201 = vshll.u32 %v76196, 26 (stack73)
        %v76202 = vshrl.u32 %v76196, 6 (stack74)
        %v76203 = vor.u32 %v76201, %v76202 (stack75)
        %v76204 = vxor.u32 %v76199, %v76203 (stack76)
        %v76207 = vadd.s32 %v76199, %v76204 (stack65)
        %v76211 = vadd.s32 %v76207, %v8 (stack65)
        %v76213 = vshll.u32 %v76204, 6 (stack73)
        %v76214 = vshrl.u32 %v76204, 26 (stack74)
        %v76215 = vor.u32 %v76213, %v76214 (stack75)
        %v76216 = vxor.u32 %v76207, %v76215 (stack76)
        %v76219 = vadd.s32 %v76216, %v10 (stack65)
        %v76223 = vadd.s32 %v76219, 5 (stack65)
        %v76225 = vxor.u32 %v76211, %v76223 (stack76)
        %v76226 = vand.u32.u8 %v76225, 255 (stack77)
        %v76227 = vand.u32 %v76226, 65535 (stack78)
        %v76228 = vshrl.u32 %v76227, 1 (stack79)
        %v76229 = vor.u32 %v76228, 16256 (stack75)
        %v76230 = vand.u32.u16 %v76229, 65535 (stack80)
        %v76231 = vunpack.i.l.bf16 %v76230 (stack81)
        %v76235 = vadd.f32 %v76231, -1.0 (stack82)
        %v76239 = vmul.f32 %v76235, 2.0 (stack83)
        %v76243 = vadd.f32 %v76239, -0.99609375 (stack82)
        %v76247 = vmax.f32 -0.99609375, %v76243 (stack84)
        %v76249 = vand.u32 2147483647, %v76247 (stack85)
        %vm76252 = vcmp.eq.f32.partialorder %v76249, 1.0 (stack86)
        %v76257 = vmul.f32 %v76247, inf (stack83)
        %v76259 = vxor.u32 %v76247, 2147483648 (stack87)
        %v76262 = vmul.f32 %v76247, %v76259 (stack83)
        %v76264 = vadd.f32 %v76262, 1.0 (stack88)
        %v76265 = vlog2.pop %v76264 (stack89)
        %v76266 = vmul.f32 %v76265, 0.6931472 (stack90)
        %v76267 = vmul.f32 -0.5, %v76262 (stack91)
        %v76268 = vadd.f32 %v76267, 1.0 (stack92)
        %v76269 = vmul.f32 %v76268, %v76262 (stack93)
        %v76270 = vand.u32 2147483647, %v76262 (stack94)
        %vm76271 = vcmp.lt.f32.partialorder %v76270, 0.0004427343 (stack95)
        %v76272 = vsel /*vm=*/%vm76271, /*on_true_vy=*/%v76269, /*on_false_vx=*/%v76266 (stack96)
        %v76273 = vxor.u32 %v76272, 2147483648 (stack87)
        %vm76276 = vcmp.lt.f32.partialorder %v76273, 5.0 (stack86)
        %v76281 = vsel /*vm=*/%vm76276, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v76285 = vsel /*vm=*/%vm76276, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v76289 = vsel /*vm=*/%vm76276, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v76293 = vsel /*vm=*/%vm76276, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v76297 = vsel /*vm=*/%vm76276, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v76301 = vsel /*vm=*/%vm76276, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v76305 = vsel /*vm=*/%vm76276, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v76309 = vsel /*vm=*/%vm76276, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v76313 = vsel /*vm=*/%vm76276, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v76317 = vadd.f32 %v76273, -2.5 (stack82)
        %v76319 = vrsqrt.pop %v76273 (stack97)
        %v76320 = vmul.f32 %v76273, %v76319 (stack98)
        %vm76321 = vcmp.eq.f32.partialorder %v76273, inf (stack99)
        %v76322 = vsel /*vm=*/%vm76321, /*on_true_vy=*/%v76273, /*on_false_vx=*/%v76320 (stack100)
        %vm76323 = vcmp.eq.f32.partialorder %v76273, 0.0 (stack101)
        %v76324 = vand.u32 %v76273, 2147483648 (stack102)
        %v76325 = vsel /*vm=*/%vm76323, /*on_true_vy=*/%v76324, /*on_false_vx=*/%v76322 (stack103)
        %v76328 = vadd.f32 %v76325, -3.0 (stack82)
        %v76332 = vsel /*vm=*/%vm76276, /*on_true_vy=*/%v76317, /*on_false_vx=*/%v76328 (stack72)
        %v76336 = vmul.f32 %v76313, %v76332 (stack83)
        %v76340 = vadd.f32 %v76309, %v76336 (stack82)
        %v76344 = vmul.f32 %v76340, %v76332 (stack83)
        %v76348 = vadd.f32 %v76305, %v76344 (stack82)
        %v76352 = vmul.f32 %v76348, %v76332 (stack83)
        %v76356 = vadd.f32 %v76301, %v76352 (stack82)
        %v76360 = vmul.f32 %v76356, %v76332 (stack83)
        %v76364 = vadd.f32 %v76297, %v76360 (stack82)
        %v76368 = vmul.f32 %v76364, %v76332 (stack83)
        %v76372 = vadd.f32 %v76293, %v76368 (stack82)
        %v76376 = vmul.f32 %v76372, %v76332 (stack83)
        %v76380 = vadd.f32 %v76289, %v76376 (stack82)
        %v76384 = vmul.f32 %v76380, %v76332 (stack83)
        %v76388 = vadd.f32 %v76285, %v76384 (stack82)
        %v76392 = vmul.f32 %v76388, %v76332 (stack83)
        %v76396 = vadd.f32 %v76281, %v76392 (stack82)
        %v76400 = vmul.f32 %v76396, %v76247 (stack83)
        %v76404 = vsel /*vm=*/%vm76252, /*on_true_vy=*/%v76257, /*on_false_vx=*/%v76400 (stack72)
        %v76408 = vmul.f32 %v76404, 1.4140625 (stack83)
        %s76410 = scalar_lea.vmem %s280, 336 [#allocation0] (stack107)
        %v76411 = vpack.c.bf16 0.0, %v76408 (stack104)
        %76412 = vst [vmem:[%s76410] sm:$0xf] /*vst_source=*/%v76411 (stack105)
        %v76415 = vadd.s32 %v1868, %v75029 (stack65)
        %s76417 = smul.u32 128, %s27 (stack66)
        %v76418 = vlaneseq (stack67)
        %v76419 = vand.u32 %v76418, 127 (stack68)
        %v76420 = vstv %s76417 (stack69)
        %v76421 = vadd.s32 %v76419, %v76420 (stack70)
        %v76425 = vadd.s32 %v76415, %v76421 (stack65)
        %vm76429 = vcmp.lt.u32.totalorder %v76425, %v76415 (stack71)
        %vm76434 = vcmp.lt.u32.totalorder %v76415, %v1868 (stack71)
        %v76439 = vadd.s32 %v1855, %v75012 (stack65)
        %v76443 = vadd.s32 %v76439, 1 (stack65)
        %v76447 = vsel /*vm=*/%vm76434, /*on_true_vy=*/%v76443, /*on_false_vx=*/%v76439 (stack72)
        %v76451 = vadd.s32 %v76447, 1 (stack65)
        %v76455 = vsel /*vm=*/%vm76429, /*on_true_vy=*/%v76451, /*on_false_vx=*/%v76447 (stack72)
        %v76460 = vadd.s32 %v76455, %v10 (stack65)
        %v76464 = vadd.s32 %v76425, %v9 (stack65)
        %v76468 = vadd.s32 %v76460, %v76464 (stack65)
        %v76470 = vshll.u32 %v76464, 13 (stack73)
        %v76471 = vshrl.u32 %v76464, 19 (stack74)
        %v76472 = vor.u32 %v76470, %v76471 (stack75)
        %v76473 = vxor.u32 %v76468, %v76472 (stack76)
        %v76476 = vadd.s32 %v76468, %v76473 (stack65)
        %v76478 = vshll.u32 %v76473, 15 (stack73)
        %v76479 = vshrl.u32 %v76473, 17 (stack74)
        %v76480 = vor.u32 %v76478, %v76479 (stack75)
        %v76481 = vxor.u32 %v76476, %v76480 (stack76)
        %v76484 = vadd.s32 %v76476, %v76481 (stack65)
        %v76486 = vshll.u32 %v76481, 26 (stack73)
        %v76487 = vshrl.u32 %v76481, 6 (stack74)
        %v76488 = vor.u32 %v76486, %v76487 (stack75)
        %v76489 = vxor.u32 %v76484, %v76488 (stack76)
        %v76492 = vadd.s32 %v76484, %v76489 (stack65)
        %v76496 = vadd.s32 %v76492, %v9 (stack65)
        %v76498 = vshll.u32 %v76489, 6 (stack73)
        %v76499 = vshrl.u32 %v76489, 26 (stack74)
        %v76500 = vor.u32 %v76498, %v76499 (stack75)
        %v76501 = vxor.u32 %v76492, %v76500 (stack76)
        %v76504 = vadd.s32 %v76501, %v8 (stack65)
        %v76508 = vadd.s32 %v76504, 1 (stack65)
        %v76512 = vadd.s32 %v76496, %v76508 (stack65)
        %v76514 = vshll.u32 %v76508, 17 (stack73)
        %v76515 = vshrl.u32 %v76508, 15 (stack74)
        %v76516 = vor.u32 %v76514, %v76515 (stack75)
        %v76517 = vxor.u32 %v76512, %v76516 (stack76)
        %v76520 = vadd.s32 %v76512, %v76517 (stack65)
        %v76522 = vshll.u32 %v76517, 29 (stack73)
        %v76523 = vshrl.u32 %v76517, 3 (stack74)
        %v76524 = vor.u32 %v76522, %v76523 (stack75)
        %v76525 = vxor.u32 %v76520, %v76524 (stack76)
        %v76528 = vadd.s32 %v76520, %v76525 (stack65)
        %v76530 = vshll.u32 %v76525, 16 (stack73)
        %v76531 = vshrl.u32 %v76525, 16 (stack74)
        %v76532 = vor.u32 %v76530, %v76531 (stack75)
        %v76533 = vxor.u32 %v76528, %v76532 (stack76)
        %v76536 = vadd.s32 %v76528, %v76533 (stack65)
        %v76540 = vadd.s32 %v76536, %v8 (stack65)
        %v76542 = vshll.u32 %v76533, 24 (stack73)
        %v76543 = vshrl.u32 %v76533, 8 (stack74)
        %v76544 = vor.u32 %v76542, %v76543 (stack75)
        %v76545 = vxor.u32 %v76536, %v76544 (stack76)
        %v76548 = vadd.s32 %v76545, %v10 (stack65)
        %v76552 = vadd.s32 %v76548, 2 (stack65)
        %v76556 = vadd.s32 %v76540, %v76552 (stack65)
        %v76558 = vshll.u32 %v76552, 13 (stack73)
        %v76559 = vshrl.u32 %v76552, 19 (stack74)
        %v76560 = vor.u32 %v76558, %v76559 (stack75)
        %v76561 = vxor.u32 %v76556, %v76560 (stack76)
        %v76564 = vadd.s32 %v76556, %v76561 (stack65)
        %v76566 = vshll.u32 %v76561, 15 (stack73)
        %v76567 = vshrl.u32 %v76561, 17 (stack74)
        %v76568 = vor.u32 %v76566, %v76567 (stack75)
        %v76569 = vxor.u32 %v76564, %v76568 (stack76)
        %v76572 = vadd.s32 %v76564, %v76569 (stack65)
        %v76574 = vshll.u32 %v76569, 26 (stack73)
        %v76575 = vshrl.u32 %v76569, 6 (stack74)
        %v76576 = vor.u32 %v76574, %v76575 (stack75)
        %v76577 = vxor.u32 %v76572, %v76576 (stack76)
        %v76580 = vadd.s32 %v76572, %v76577 (stack65)
        %v76584 = vadd.s32 %v76580, %v10 (stack65)
        %v76586 = vshll.u32 %v76577, 6 (stack73)
        %v76587 = vshrl.u32 %v76577, 26 (stack74)
        %v76588 = vor.u32 %v76586, %v76587 (stack75)
        %v76589 = vxor.u32 %v76580, %v76588 (stack76)
        %v76592 = vadd.s32 %v76589, %v9 (stack65)
        %v76596 = vadd.s32 %v76592, 3 (stack65)
        %v76600 = vadd.s32 %v76584, %v76596 (stack65)
        %v76602 = vshll.u32 %v76596, 17 (stack73)
        %v76603 = vshrl.u32 %v76596, 15 (stack74)
        %v76604 = vor.u32 %v76602, %v76603 (stack75)
        %v76605 = vxor.u32 %v76600, %v76604 (stack76)
        %v76608 = vadd.s32 %v76600, %v76605 (stack65)
        %v76610 = vshll.u32 %v76605, 29 (stack73)
        %v76611 = vshrl.u32 %v76605, 3 (stack74)
        %v76612 = vor.u32 %v76610, %v76611 (stack75)
        %v76613 = vxor.u32 %v76608, %v76612 (stack76)
        %v76616 = vadd.s32 %v76608, %v76613 (stack65)
        %v76618 = vshll.u32 %v76613, 16 (stack73)
        %v76619 = vshrl.u32 %v76613, 16 (stack74)
        %v76620 = vor.u32 %v76618, %v76619 (stack75)
        %v76621 = vxor.u32 %v76616, %v76620 (stack76)
        %v76624 = vadd.s32 %v76616, %v76621 (stack65)
        %v76628 = vadd.s32 %v76624, %v9 (stack65)
        %v76630 = vshll.u32 %v76621, 24 (stack73)
        %v76631 = vshrl.u32 %v76621, 8 (stack74)
        %v76632 = vor.u32 %v76630, %v76631 (stack75)
        %v76633 = vxor.u32 %v76624, %v76632 (stack76)
        %v76636 = vadd.s32 %v76633, %v8 (stack65)
        %v76640 = vadd.s32 %v76636, 4 (stack65)
        %v76644 = vadd.s32 %v76628, %v76640 (stack65)
        %v76646 = vshll.u32 %v76640, 13 (stack73)
        %v76647 = vshrl.u32 %v76640, 19 (stack74)
        %v76648 = vor.u32 %v76646, %v76647 (stack75)
        %v76649 = vxor.u32 %v76644, %v76648 (stack76)
        %v76652 = vadd.s32 %v76644, %v76649 (stack65)
        %v76654 = vshll.u32 %v76649, 15 (stack73)
        %v76655 = vshrl.u32 %v76649, 17 (stack74)
        %v76656 = vor.u32 %v76654, %v76655 (stack75)
        %v76657 = vxor.u32 %v76652, %v76656 (stack76)
        %v76660 = vadd.s32 %v76652, %v76657 (stack65)
        %v76662 = vshll.u32 %v76657, 26 (stack73)
        %v76663 = vshrl.u32 %v76657, 6 (stack74)
        %v76664 = vor.u32 %v76662, %v76663 (stack75)
        %v76665 = vxor.u32 %v76660, %v76664 (stack76)
        %v76668 = vadd.s32 %v76660, %v76665 (stack65)
        %v76672 = vadd.s32 %v76668, %v8 (stack65)
        %v76674 = vshll.u32 %v76665, 6 (stack73)
        %v76675 = vshrl.u32 %v76665, 26 (stack74)
        %v76676 = vor.u32 %v76674, %v76675 (stack75)
        %v76677 = vxor.u32 %v76668, %v76676 (stack76)
        %v76680 = vadd.s32 %v76677, %v10 (stack65)
        %v76684 = vadd.s32 %v76680, 5 (stack65)
        %v76686 = vxor.u32 %v76672, %v76684 (stack76)
        %v76687 = vand.u32.u8 %v76686, 255 (stack77)
        %v76688 = vand.u32 %v76687, 65535 (stack78)
        %v76689 = vshrl.u32 %v76688, 1 (stack79)
        %v76690 = vor.u32 %v76689, 16256 (stack75)
        %v76691 = vand.u32.u16 %v76690, 65535 (stack80)
        %v76692 = vunpack.i.l.bf16 %v76691 (stack81)
        %v76696 = vadd.f32 %v76692, -1.0 (stack82)
        %v76700 = vmul.f32 %v76696, 2.0 (stack83)
        %v76704 = vadd.f32 %v76700, -0.99609375 (stack82)
        %v76708 = vmax.f32 -0.99609375, %v76704 (stack84)
        %v76710 = vand.u32 2147483647, %v76708 (stack85)
        %vm76713 = vcmp.eq.f32.partialorder %v76710, 1.0 (stack86)
        %v76718 = vmul.f32 %v76708, inf (stack83)
        %v76720 = vxor.u32 %v76708, 2147483648 (stack87)
        %v76723 = vmul.f32 %v76708, %v76720 (stack83)
        %v76725 = vadd.f32 %v76723, 1.0 (stack88)
        %v76726 = vlog2.pop %v76725 (stack89)
        %v76727 = vmul.f32 %v76726, 0.6931472 (stack90)
        %v76728 = vmul.f32 -0.5, %v76723 (stack91)
        %v76729 = vadd.f32 %v76728, 1.0 (stack92)
        %v76730 = vmul.f32 %v76729, %v76723 (stack93)
        %v76731 = vand.u32 2147483647, %v76723 (stack94)
        %vm76732 = vcmp.lt.f32.partialorder %v76731, 0.0004427343 (stack95)
        %v76733 = vsel /*vm=*/%vm76732, /*on_true_vy=*/%v76730, /*on_false_vx=*/%v76727 (stack96)
        %v76734 = vxor.u32 %v76733, 2147483648 (stack87)
        %vm76737 = vcmp.lt.f32.partialorder %v76734, 5.0 (stack86)
        %v76742 = vsel /*vm=*/%vm76737, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v76746 = vsel /*vm=*/%vm76737, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v76750 = vsel /*vm=*/%vm76737, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v76754 = vsel /*vm=*/%vm76737, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v76758 = vsel /*vm=*/%vm76737, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v76762 = vsel /*vm=*/%vm76737, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v76766 = vsel /*vm=*/%vm76737, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v76770 = vsel /*vm=*/%vm76737, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v76774 = vsel /*vm=*/%vm76737, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v76778 = vadd.f32 %v76734, -2.5 (stack82)
        %v76780 = vrsqrt.pop %v76734 (stack97)
        %v76781 = vmul.f32 %v76734, %v76780 (stack98)
        %vm76782 = vcmp.eq.f32.partialorder %v76734, inf (stack99)
        %v76783 = vsel /*vm=*/%vm76782, /*on_true_vy=*/%v76734, /*on_false_vx=*/%v76781 (stack100)
        %vm76784 = vcmp.eq.f32.partialorder %v76734, 0.0 (stack101)
        %v76785 = vand.u32 %v76734, 2147483648 (stack102)
        %v76786 = vsel /*vm=*/%vm76784, /*on_true_vy=*/%v76785, /*on_false_vx=*/%v76783 (stack103)
        %v76789 = vadd.f32 %v76786, -3.0 (stack82)
        %v76793 = vsel /*vm=*/%vm76737, /*on_true_vy=*/%v76778, /*on_false_vx=*/%v76789 (stack72)
        %v76797 = vmul.f32 %v76774, %v76793 (stack83)
        %v76801 = vadd.f32 %v76770, %v76797 (stack82)
        %v76805 = vmul.f32 %v76801, %v76793 (stack83)
        %v76809 = vadd.f32 %v76766, %v76805 (stack82)
        %v76813 = vmul.f32 %v76809, %v76793 (stack83)
        %v76817 = vadd.f32 %v76762, %v76813 (stack82)
        %v76821 = vmul.f32 %v76817, %v76793 (stack83)
        %v76825 = vadd.f32 %v76758, %v76821 (stack82)
        %v76829 = vmul.f32 %v76825, %v76793 (stack83)
        %v76833 = vadd.f32 %v76754, %v76829 (stack82)
        %v76837 = vmul.f32 %v76833, %v76793 (stack83)
        %v76841 = vadd.f32 %v76750, %v76837 (stack82)
        %v76845 = vmul.f32 %v76841, %v76793 (stack83)
        %v76849 = vadd.f32 %v76746, %v76845 (stack82)
        %v76853 = vmul.f32 %v76849, %v76793 (stack83)
        %v76857 = vadd.f32 %v76742, %v76853 (stack82)
        %v76861 = vmul.f32 %v76857, %v76708 (stack83)
        %v76865 = vsel /*vm=*/%vm76713, /*on_true_vy=*/%v76718, /*on_false_vx=*/%v76861 (stack72)
        %v76869 = vmul.f32 %v76865, 1.4140625 (stack83)
        %s76871 = scalar_lea.vmem %s280, 464 [#allocation0] (stack107)
        %v76872 = vpack.c.bf16 0.0, %v76869 (stack104)
        %76873 = vst [vmem:[%s76871] sm:$0xf] /*vst_source=*/%v76872 (stack105)
        %v76876 = vadd.s32 %v2355, %v75029 (stack65)
        %s76878 = smul.u32 128, %s27 (stack66)
        %v76879 = vlaneseq (stack67)
        %v76880 = vand.u32 %v76879, 127 (stack68)
        %v76881 = vstv %s76878 (stack69)
        %v76882 = vadd.s32 %v76880, %v76881 (stack70)
        %v76886 = vadd.s32 %v76876, %v76882 (stack65)
        %vm76890 = vcmp.lt.u32.totalorder %v76886, %v76876 (stack71)
        %vm76895 = vcmp.lt.u32.totalorder %v76876, %v2355 (stack71)
        %v76900 = vadd.s32 %v2342, %v75012 (stack65)
        %v76904 = vadd.s32 %v76900, 1 (stack65)
        %v76908 = vsel /*vm=*/%vm76895, /*on_true_vy=*/%v76904, /*on_false_vx=*/%v76900 (stack72)
        %v76912 = vadd.s32 %v76908, 1 (stack65)
        %v76916 = vsel /*vm=*/%vm76890, /*on_true_vy=*/%v76912, /*on_false_vx=*/%v76908 (stack72)
        %v76921 = vadd.s32 %v76916, %v10 (stack65)
        %v76925 = vadd.s32 %v76886, %v9 (stack65)
        %v76929 = vadd.s32 %v76921, %v76925 (stack65)
        %v76931 = vshll.u32 %v76925, 13 (stack73)
        %v76932 = vshrl.u32 %v76925, 19 (stack74)
        %v76933 = vor.u32 %v76931, %v76932 (stack75)
        %v76934 = vxor.u32 %v76929, %v76933 (stack76)
        %v76937 = vadd.s32 %v76929, %v76934 (stack65)
        %v76939 = vshll.u32 %v76934, 15 (stack73)
        %v76940 = vshrl.u32 %v76934, 17 (stack74)
        %v76941 = vor.u32 %v76939, %v76940 (stack75)
        %v76942 = vxor.u32 %v76937, %v76941 (stack76)
        %v76945 = vadd.s32 %v76937, %v76942 (stack65)
        %v76947 = vshll.u32 %v76942, 26 (stack73)
        %v76948 = vshrl.u32 %v76942, 6 (stack74)
        %v76949 = vor.u32 %v76947, %v76948 (stack75)
        %v76950 = vxor.u32 %v76945, %v76949 (stack76)
        %v76953 = vadd.s32 %v76945, %v76950 (stack65)
        %v76957 = vadd.s32 %v76953, %v9 (stack65)
        %v76959 = vshll.u32 %v76950, 6 (stack73)
        %v76960 = vshrl.u32 %v76950, 26 (stack74)
        %v76961 = vor.u32 %v76959, %v76960 (stack75)
        %v76962 = vxor.u32 %v76953, %v76961 (stack76)
        %v76965 = vadd.s32 %v76962, %v8 (stack65)
        %v76969 = vadd.s32 %v76965, 1 (stack65)
        %v76973 = vadd.s32 %v76957, %v76969 (stack65)
        %v76975 = vshll.u32 %v76969, 17 (stack73)
        %v76976 = vshrl.u32 %v76969, 15 (stack74)
        %v76977 = vor.u32 %v76975, %v76976 (stack75)
        %v76978 = vxor.u32 %v76973, %v76977 (stack76)
        %v76981 = vadd.s32 %v76973, %v76978 (stack65)
        %v76983 = vshll.u32 %v76978, 29 (stack73)
        %v76984 = vshrl.u32 %v76978, 3 (stack74)
        %v76985 = vor.u32 %v76983, %v76984 (stack75)
        %v76986 = vxor.u32 %v76981, %v76985 (stack76)
        %v76989 = vadd.s32 %v76981, %v76986 (stack65)
        %v76991 = vshll.u32 %v76986, 16 (stack73)
        %v76992 = vshrl.u32 %v76986, 16 (stack74)
        %v76993 = vor.u32 %v76991, %v76992 (stack75)
        %v76994 = vxor.u32 %v76989, %v76993 (stack76)
        %v76997 = vadd.s32 %v76989, %v76994 (stack65)
        %v77001 = vadd.s32 %v76997, %v8 (stack65)
        %v77003 = vshll.u32 %v76994, 24 (stack73)
        %v77004 = vshrl.u32 %v76994, 8 (stack74)
        %v77005 = vor.u32 %v77003, %v77004 (stack75)
        %v77006 = vxor.u32 %v76997, %v77005 (stack76)
        %v77009 = vadd.s32 %v77006, %v10 (stack65)
        %v77013 = vadd.s32 %v77009, 2 (stack65)
        %v77017 = vadd.s32 %v77001, %v77013 (stack65)
        %v77019 = vshll.u32 %v77013, 13 (stack73)
        %v77020 = vshrl.u32 %v77013, 19 (stack74)
        %v77021 = vor.u32 %v77019, %v77020 (stack75)
        %v77022 = vxor.u32 %v77017, %v77021 (stack76)
        %v77025 = vadd.s32 %v77017, %v77022 (stack65)
        %v77027 = vshll.u32 %v77022, 15 (stack73)
        %v77028 = vshrl.u32 %v77022, 17 (stack74)
        %v77029 = vor.u32 %v77027, %v77028 (stack75)
        %v77030 = vxor.u32 %v77025, %v77029 (stack76)
        %v77033 = vadd.s32 %v77025, %v77030 (stack65)
        %v77035 = vshll.u32 %v77030, 26 (stack73)
        %v77036 = vshrl.u32 %v77030, 6 (stack74)
        %v77037 = vor.u32 %v77035, %v77036 (stack75)
        %v77038 = vxor.u32 %v77033, %v77037 (stack76)
        %v77041 = vadd.s32 %v77033, %v77038 (stack65)
        %v77045 = vadd.s32 %v77041, %v10 (stack65)
        %v77047 = vshll.u32 %v77038, 6 (stack73)
        %v77048 = vshrl.u32 %v77038, 26 (stack74)
        %v77049 = vor.u32 %v77047, %v77048 (stack75)
        %v77050 = vxor.u32 %v77041, %v77049 (stack76)
        %v77053 = vadd.s32 %v77050, %v9 (stack65)
        %v77057 = vadd.s32 %v77053, 3 (stack65)
        %v77061 = vadd.s32 %v77045, %v77057 (stack65)
        %v77063 = vshll.u32 %v77057, 17 (stack73)
        %v77064 = vshrl.u32 %v77057, 15 (stack74)
        %v77065 = vor.u32 %v77063, %v77064 (stack75)
        %v77066 = vxor.u32 %v77061, %v77065 (stack76)
        %v77069 = vadd.s32 %v77061, %v77066 (stack65)
        %v77071 = vshll.u32 %v77066, 29 (stack73)
        %v77072 = vshrl.u32 %v77066, 3 (stack74)
        %v77073 = vor.u32 %v77071, %v77072 (stack75)
        %v77074 = vxor.u32 %v77069, %v77073 (stack76)
        %v77077 = vadd.s32 %v77069, %v77074 (stack65)
        %v77079 = vshll.u32 %v77074, 16 (stack73)
        %v77080 = vshrl.u32 %v77074, 16 (stack74)
        %v77081 = vor.u32 %v77079, %v77080 (stack75)
        %v77082 = vxor.u32 %v77077, %v77081 (stack76)
        %v77085 = vadd.s32 %v77077, %v77082 (stack65)
        %v77089 = vadd.s32 %v77085, %v9 (stack65)
        %v77091 = vshll.u32 %v77082, 24 (stack73)
        %v77092 = vshrl.u32 %v77082, 8 (stack74)
        %v77093 = vor.u32 %v77091, %v77092 (stack75)
        %v77094 = vxor.u32 %v77085, %v77093 (stack76)
        %v77097 = vadd.s32 %v77094, %v8 (stack65)
        %v77101 = vadd.s32 %v77097, 4 (stack65)
        %v77105 = vadd.s32 %v77089, %v77101 (stack65)
        %v77107 = vshll.u32 %v77101, 13 (stack73)
        %v77108 = vshrl.u32 %v77101, 19 (stack74)
        %v77109 = vor.u32 %v77107, %v77108 (stack75)
        %v77110 = vxor.u32 %v77105, %v77109 (stack76)
        %v77113 = vadd.s32 %v77105, %v77110 (stack65)
        %v77115 = vshll.u32 %v77110, 15 (stack73)
        %v77116 = vshrl.u32 %v77110, 17 (stack74)
        %v77117 = vor.u32 %v77115, %v77116 (stack75)
        %v77118 = vxor.u32 %v77113, %v77117 (stack76)
        %v77121 = vadd.s32 %v77113, %v77118 (stack65)
        %v77123 = vshll.u32 %v77118, 26 (stack73)
        %v77124 = vshrl.u32 %v77118, 6 (stack74)
        %v77125 = vor.u32 %v77123, %v77124 (stack75)
        %v77126 = vxor.u32 %v77121, %v77125 (stack76)
        %v77129 = vadd.s32 %v77121, %v77126 (stack65)
        %v77133 = vadd.s32 %v77129, %v8 (stack65)
        %v77135 = vshll.u32 %v77126, 6 (stack73)
        %v77136 = vshrl.u32 %v77126, 26 (stack74)
        %v77137 = vor.u32 %v77135, %v77136 (stack75)
        %v77138 = vxor.u32 %v77129, %v77137 (stack76)
        %v77141 = vadd.s32 %v77138, %v10 (stack65)
        %v77145 = vadd.s32 %v77141, 5 (stack65)
        %v77147 = vxor.u32 %v77133, %v77145 (stack76)
        %v77148 = vand.u32.u8 %v77147, 255 (stack77)
        %v77149 = vand.u32 %v77148, 65535 (stack78)
        %v77150 = vshrl.u32 %v77149, 1 (stack79)
        %v77151 = vor.u32 %v77150, 16256 (stack75)
        %v77152 = vand.u32.u16 %v77151, 65535 (stack80)
        %v77153 = vunpack.i.l.bf16 %v77152 (stack81)
        %v77157 = vadd.f32 %v77153, -1.0 (stack82)
        %v77161 = vmul.f32 %v77157, 2.0 (stack83)
        %v77165 = vadd.f32 %v77161, -0.99609375 (stack82)
        %v77169 = vmax.f32 -0.99609375, %v77165 (stack84)
        %v77171 = vand.u32 2147483647, %v77169 (stack85)
        %vm77174 = vcmp.eq.f32.partialorder %v77171, 1.0 (stack86)
        %v77179 = vmul.f32 %v77169, inf (stack83)
        %v77181 = vxor.u32 %v77169, 2147483648 (stack87)
        %v77184 = vmul.f32 %v77169, %v77181 (stack83)
        %v77186 = vadd.f32 %v77184, 1.0 (stack88)
        %v77187 = vlog2.pop %v77186 (stack89)
        %v77188 = vmul.f32 %v77187, 0.6931472 (stack90)
        %v77189 = vmul.f32 -0.5, %v77184 (stack91)
        %v77190 = vadd.f32 %v77189, 1.0 (stack92)
        %v77191 = vmul.f32 %v77190, %v77184 (stack93)
        %v77192 = vand.u32 2147483647, %v77184 (stack94)
        %vm77193 = vcmp.lt.f32.partialorder %v77192, 0.0004427343 (stack95)
        %v77194 = vsel /*vm=*/%vm77193, /*on_true_vy=*/%v77191, /*on_false_vx=*/%v77188 (stack96)
        %v77195 = vxor.u32 %v77194, 2147483648 (stack87)
        %vm77198 = vcmp.lt.f32.partialorder %v77195, 5.0 (stack86)
        %v77203 = vsel /*vm=*/%vm77198, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v77207 = vsel /*vm=*/%vm77198, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v77211 = vsel /*vm=*/%vm77198, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v77215 = vsel /*vm=*/%vm77198, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v77219 = vsel /*vm=*/%vm77198, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v77223 = vsel /*vm=*/%vm77198, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v77227 = vsel /*vm=*/%vm77198, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v77231 = vsel /*vm=*/%vm77198, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v77235 = vsel /*vm=*/%vm77198, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v77239 = vadd.f32 %v77195, -2.5 (stack82)
        %v77241 = vrsqrt.pop %v77195 (stack97)
        %v77242 = vmul.f32 %v77195, %v77241 (stack98)
        %vm77243 = vcmp.eq.f32.partialorder %v77195, inf (stack99)
        %v77244 = vsel /*vm=*/%vm77243, /*on_true_vy=*/%v77195, /*on_false_vx=*/%v77242 (stack100)
        %vm77245 = vcmp.eq.f32.partialorder %v77195, 0.0 (stack101)
        %v77246 = vand.u32 %v77195, 2147483648 (stack102)
        %v77247 = vsel /*vm=*/%vm77245, /*on_true_vy=*/%v77246, /*on_false_vx=*/%v77244 (stack103)
        %v77250 = vadd.f32 %v77247, -3.0 (stack82)
        %v77254 = vsel /*vm=*/%vm77198, /*on_true_vy=*/%v77239, /*on_false_vx=*/%v77250 (stack72)
        %v77258 = vmul.f32 %v77235, %v77254 (stack83)
        %v77262 = vadd.f32 %v77231, %v77258 (stack82)
        %v77266 = vmul.f32 %v77262, %v77254 (stack83)
        %v77270 = vadd.f32 %v77227, %v77266 (stack82)
        %v77274 = vmul.f32 %v77270, %v77254 (stack83)
        %v77278 = vadd.f32 %v77223, %v77274 (stack82)
        %v77282 = vmul.f32 %v77278, %v77254 (stack83)
        %v77286 = vadd.f32 %v77219, %v77282 (stack82)
        %v77290 = vmul.f32 %v77286, %v77254 (stack83)
        %v77294 = vadd.f32 %v77215, %v77290 (stack82)
        %v77298 = vmul.f32 %v77294, %v77254 (stack83)
        %v77302 = vadd.f32 %v77211, %v77298 (stack82)
        %v77306 = vmul.f32 %v77302, %v77254 (stack83)
        %v77310 = vadd.f32 %v77207, %v77306 (stack82)
        %v77314 = vmul.f32 %v77310, %v77254 (stack83)
        %v77318 = vadd.f32 %v77203, %v77314 (stack82)
        %v77322 = vmul.f32 %v77318, %v77169 (stack83)
        %v77326 = vsel /*vm=*/%vm77174, /*on_true_vy=*/%v77179, /*on_false_vx=*/%v77322 (stack72)
        %v77330 = vmul.f32 %v77326, 1.4140625 (stack83)
        %s77332 = scalar_lea.vmem %s280, 592 [#allocation0] (stack107)
        %v77333 = vpack.c.bf16 0.0, %v77330 (stack104)
        %77334 = vst [vmem:[%s77332] sm:$0xf] /*vst_source=*/%v77333 (stack105)
        %v77337 = vadd.s32 %v2842, %v75029 (stack65)
        %s77339 = smul.u32 128, %s27 (stack66)
        %v77340 = vlaneseq (stack67)
        %v77341 = vand.u32 %v77340, 127 (stack68)
        %v77342 = vstv %s77339 (stack69)
        %v77343 = vadd.s32 %v77341, %v77342 (stack70)
        %v77347 = vadd.s32 %v77337, %v77343 (stack65)
        %vm77351 = vcmp.lt.u32.totalorder %v77347, %v77337 (stack71)
        %vm77356 = vcmp.lt.u32.totalorder %v77337, %v2842 (stack71)
        %v77361 = vadd.s32 %v2829, %v75012 (stack65)
        %v77365 = vadd.s32 %v77361, 1 (stack65)
        %v77369 = vsel /*vm=*/%vm77356, /*on_true_vy=*/%v77365, /*on_false_vx=*/%v77361 (stack72)
        %v77373 = vadd.s32 %v77369, 1 (stack65)
        %v77377 = vsel /*vm=*/%vm77351, /*on_true_vy=*/%v77373, /*on_false_vx=*/%v77369 (stack72)
        %v77382 = vadd.s32 %v77377, %v10 (stack65)
        %v77386 = vadd.s32 %v77347, %v9 (stack65)
        %v77390 = vadd.s32 %v77382, %v77386 (stack65)
        %v77392 = vshll.u32 %v77386, 13 (stack73)
        %v77393 = vshrl.u32 %v77386, 19 (stack74)
        %v77394 = vor.u32 %v77392, %v77393 (stack75)
        %v77395 = vxor.u32 %v77390, %v77394 (stack76)
        %v77398 = vadd.s32 %v77390, %v77395 (stack65)
        %v77400 = vshll.u32 %v77395, 15 (stack73)
        %v77401 = vshrl.u32 %v77395, 17 (stack74)
        %v77402 = vor.u32 %v77400, %v77401 (stack75)
        %v77403 = vxor.u32 %v77398, %v77402 (stack76)
        %v77406 = vadd.s32 %v77398, %v77403 (stack65)
        %v77408 = vshll.u32 %v77403, 26 (stack73)
        %v77409 = vshrl.u32 %v77403, 6 (stack74)
        %v77410 = vor.u32 %v77408, %v77409 (stack75)
        %v77411 = vxor.u32 %v77406, %v77410 (stack76)
        %v77414 = vadd.s32 %v77406, %v77411 (stack65)
        %v77418 = vadd.s32 %v77414, %v9 (stack65)
        %v77420 = vshll.u32 %v77411, 6 (stack73)
        %v77421 = vshrl.u32 %v77411, 26 (stack74)
        %v77422 = vor.u32 %v77420, %v77421 (stack75)
        %v77423 = vxor.u32 %v77414, %v77422 (stack76)
        %v77426 = vadd.s32 %v77423, %v8 (stack65)
        %v77430 = vadd.s32 %v77426, 1 (stack65)
        %v77434 = vadd.s32 %v77418, %v77430 (stack65)
        %v77436 = vshll.u32 %v77430, 17 (stack73)
        %v77437 = vshrl.u32 %v77430, 15 (stack74)
        %v77438 = vor.u32 %v77436, %v77437 (stack75)
        %v77439 = vxor.u32 %v77434, %v77438 (stack76)
        %v77442 = vadd.s32 %v77434, %v77439 (stack65)
        %v77444 = vshll.u32 %v77439, 29 (stack73)
        %v77445 = vshrl.u32 %v77439, 3 (stack74)
        %v77446 = vor.u32 %v77444, %v77445 (stack75)
        %v77447 = vxor.u32 %v77442, %v77446 (stack76)
        %v77450 = vadd.s32 %v77442, %v77447 (stack65)
        %v77452 = vshll.u32 %v77447, 16 (stack73)
        %v77453 = vshrl.u32 %v77447, 16 (stack74)
        %v77454 = vor.u32 %v77452, %v77453 (stack75)
        %v77455 = vxor.u32 %v77450, %v77454 (stack76)
        %v77458 = vadd.s32 %v77450, %v77455 (stack65)
        %v77462 = vadd.s32 %v77458, %v8 (stack65)
        %v77464 = vshll.u32 %v77455, 24 (stack73)
        %v77465 = vshrl.u32 %v77455, 8 (stack74)
        %v77466 = vor.u32 %v77464, %v77465 (stack75)
        %v77467 = vxor.u32 %v77458, %v77466 (stack76)
        %v77470 = vadd.s32 %v77467, %v10 (stack65)
        %v77474 = vadd.s32 %v77470, 2 (stack65)
        %v77478 = vadd.s32 %v77462, %v77474 (stack65)
        %v77480 = vshll.u32 %v77474, 13 (stack73)
        %v77481 = vshrl.u32 %v77474, 19 (stack74)
        %v77482 = vor.u32 %v77480, %v77481 (stack75)
        %v77483 = vxor.u32 %v77478, %v77482 (stack76)
        %v77486 = vadd.s32 %v77478, %v77483 (stack65)
        %v77488 = vshll.u32 %v77483, 15 (stack73)
        %v77489 = vshrl.u32 %v77483, 17 (stack74)
        %v77490 = vor.u32 %v77488, %v77489 (stack75)
        %v77491 = vxor.u32 %v77486, %v77490 (stack76)
        %v77494 = vadd.s32 %v77486, %v77491 (stack65)
        %v77496 = vshll.u32 %v77491, 26 (stack73)
        %v77497 = vshrl.u32 %v77491, 6 (stack74)
        %v77498 = vor.u32 %v77496, %v77497 (stack75)
        %v77499 = vxor.u32 %v77494, %v77498 (stack76)
        %v77502 = vadd.s32 %v77494, %v77499 (stack65)
        %v77506 = vadd.s32 %v77502, %v10 (stack65)
        %v77508 = vshll.u32 %v77499, 6 (stack73)
        %v77509 = vshrl.u32 %v77499, 26 (stack74)
        %v77510 = vor.u32 %v77508, %v77509 (stack75)
        %v77511 = vxor.u32 %v77502, %v77510 (stack76)
        %v77514 = vadd.s32 %v77511, %v9 (stack65)
        %v77518 = vadd.s32 %v77514, 3 (stack65)
        %v77522 = vadd.s32 %v77506, %v77518 (stack65)
        %v77524 = vshll.u32 %v77518, 17 (stack73)
        %v77525 = vshrl.u32 %v77518, 15 (stack74)
        %v77526 = vor.u32 %v77524, %v77525 (stack75)
        %v77527 = vxor.u32 %v77522, %v77526 (stack76)
        %v77530 = vadd.s32 %v77522, %v77527 (stack65)
        %v77532 = vshll.u32 %v77527, 29 (stack73)
        %v77533 = vshrl.u32 %v77527, 3 (stack74)
        %v77534 = vor.u32 %v77532, %v77533 (stack75)
        %v77535 = vxor.u32 %v77530, %v77534 (stack76)
        %v77538 = vadd.s32 %v77530, %v77535 (stack65)
        %v77540 = vshll.u32 %v77535, 16 (stack73)
        %v77541 = vshrl.u32 %v77535, 16 (stack74)
        %v77542 = vor.u32 %v77540, %v77541 (stack75)
        %v77543 = vxor.u32 %v77538, %v77542 (stack76)
        %v77546 = vadd.s32 %v77538, %v77543 (stack65)
        %v77550 = vadd.s32 %v77546, %v9 (stack65)
        %v77552 = vshll.u32 %v77543, 24 (stack73)
        %v77553 = vshrl.u32 %v77543, 8 (stack74)
        %v77554 = vor.u32 %v77552, %v77553 (stack75)
        %v77555 = vxor.u32 %v77546, %v77554 (stack76)
        %v77558 = vadd.s32 %v77555, %v8 (stack65)
        %v77562 = vadd.s32 %v77558, 4 (stack65)
        %v77566 = vadd.s32 %v77550, %v77562 (stack65)
        %v77568 = vshll.u32 %v77562, 13 (stack73)
        %v77569 = vshrl.u32 %v77562, 19 (stack74)
        %v77570 = vor.u32 %v77568, %v77569 (stack75)
        %v77571 = vxor.u32 %v77566, %v77570 (stack76)
        %v77574 = vadd.s32 %v77566, %v77571 (stack65)
        %v77576 = vshll.u32 %v77571, 15 (stack73)
        %v77577 = vshrl.u32 %v77571, 17 (stack74)
        %v77578 = vor.u32 %v77576, %v77577 (stack75)
        %v77579 = vxor.u32 %v77574, %v77578 (stack76)
        %v77582 = vadd.s32 %v77574, %v77579 (stack65)
        %v77584 = vshll.u32 %v77579, 26 (stack73)
        %v77585 = vshrl.u32 %v77579, 6 (stack74)
        %v77586 = vor.u32 %v77584, %v77585 (stack75)
        %v77587 = vxor.u32 %v77582, %v77586 (stack76)
        %v77590 = vadd.s32 %v77582, %v77587 (stack65)
        %v77594 = vadd.s32 %v77590, %v8 (stack65)
        %v77596 = vshll.u32 %v77587, 6 (stack73)
        %v77597 = vshrl.u32 %v77587, 26 (stack74)
        %v77598 = vor.u32 %v77596, %v77597 (stack75)
        %v77599 = vxor.u32 %v77590, %v77598 (stack76)
        %v77602 = vadd.s32 %v77599, %v10 (stack65)
        %v77606 = vadd.s32 %v77602, 5 (stack65)
        %v77608 = vxor.u32 %v77594, %v77606 (stack76)
        %v77609 = vand.u32.u8 %v77608, 255 (stack77)
        %v77610 = vand.u32 %v77609, 65535 (stack78)
        %v77611 = vshrl.u32 %v77610, 1 (stack79)
        %v77612 = vor.u32 %v77611, 16256 (stack75)
        %v77613 = vand.u32.u16 %v77612, 65535 (stack80)
        %v77614 = vunpack.i.l.bf16 %v77613 (stack81)
        %v77618 = vadd.f32 %v77614, -1.0 (stack82)
        %v77622 = vmul.f32 %v77618, 2.0 (stack83)
        %v77626 = vadd.f32 %v77622, -0.99609375 (stack82)
        %v77630 = vmax.f32 -0.99609375, %v77626 (stack84)
        %v77632 = vand.u32 2147483647, %v77630 (stack85)
        %vm77635 = vcmp.eq.f32.partialorder %v77632, 1.0 (stack86)
        %v77640 = vmul.f32 %v77630, inf (stack83)
        %v77642 = vxor.u32 %v77630, 2147483648 (stack87)
        %v77645 = vmul.f32 %v77630, %v77642 (stack83)
        %v77647 = vadd.f32 %v77645, 1.0 (stack88)
        %v77648 = vlog2.pop %v77647 (stack89)
        %v77649 = vmul.f32 %v77648, 0.6931472 (stack90)
        %v77650 = vmul.f32 -0.5, %v77645 (stack91)
        %v77651 = vadd.f32 %v77650, 1.0 (stack92)
        %v77652 = vmul.f32 %v77651, %v77645 (stack93)
        %v77653 = vand.u32 2147483647, %v77645 (stack94)
        %vm77654 = vcmp.lt.f32.partialorder %v77653, 0.0004427343 (stack95)
        %v77655 = vsel /*vm=*/%vm77654, /*on_true_vy=*/%v77652, /*on_false_vx=*/%v77649 (stack96)
        %v77656 = vxor.u32 %v77655, 2147483648 (stack87)
        %vm77659 = vcmp.lt.f32.partialorder %v77656, 5.0 (stack86)
        %v77664 = vsel /*vm=*/%vm77659, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v77668 = vsel /*vm=*/%vm77659, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v77672 = vsel /*vm=*/%vm77659, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v77676 = vsel /*vm=*/%vm77659, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v77680 = vsel /*vm=*/%vm77659, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v77684 = vsel /*vm=*/%vm77659, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v77688 = vsel /*vm=*/%vm77659, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v77692 = vsel /*vm=*/%vm77659, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v77696 = vsel /*vm=*/%vm77659, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v77700 = vadd.f32 %v77656, -2.5 (stack82)
        %v77702 = vrsqrt.pop %v77656 (stack97)
        %v77703 = vmul.f32 %v77656, %v77702 (stack98)
        %vm77704 = vcmp.eq.f32.partialorder %v77656, inf (stack99)
        %v77705 = vsel /*vm=*/%vm77704, /*on_true_vy=*/%v77656, /*on_false_vx=*/%v77703 (stack100)
        %vm77706 = vcmp.eq.f32.partialorder %v77656, 0.0 (stack101)
        %v77707 = vand.u32 %v77656, 2147483648 (stack102)
        %v77708 = vsel /*vm=*/%vm77706, /*on_true_vy=*/%v77707, /*on_false_vx=*/%v77705 (stack103)
        %v77711 = vadd.f32 %v77708, -3.0 (stack82)
        %v77715 = vsel /*vm=*/%vm77659, /*on_true_vy=*/%v77700, /*on_false_vx=*/%v77711 (stack72)
        %v77719 = vmul.f32 %v77696, %v77715 (stack83)
        %v77723 = vadd.f32 %v77692, %v77719 (stack82)
        %v77727 = vmul.f32 %v77723, %v77715 (stack83)
        %v77731 = vadd.f32 %v77688, %v77727 (stack82)
        %v77735 = vmul.f32 %v77731, %v77715 (stack83)
        %v77739 = vadd.f32 %v77684, %v77735 (stack82)
        %v77743 = vmul.f32 %v77739, %v77715 (stack83)
        %v77747 = vadd.f32 %v77680, %v77743 (stack82)
        %v77751 = vmul.f32 %v77747, %v77715 (stack83)
        %v77755 = vadd.f32 %v77676, %v77751 (stack82)
        %v77759 = vmul.f32 %v77755, %v77715 (stack83)
        %v77763 = vadd.f32 %v77672, %v77759 (stack82)
        %v77767 = vmul.f32 %v77763, %v77715 (stack83)
        %v77771 = vadd.f32 %v77668, %v77767 (stack82)
        %v77775 = vmul.f32 %v77771, %v77715 (stack83)
        %v77779 = vadd.f32 %v77664, %v77775 (stack82)
        %v77783 = vmul.f32 %v77779, %v77630 (stack83)
        %v77787 = vsel /*vm=*/%vm77635, /*on_true_vy=*/%v77640, /*on_false_vx=*/%v77783 (stack72)
        %v77791 = vmul.f32 %v77787, 1.4140625 (stack83)
        %s77793 = scalar_lea.vmem %s280, 720 [#allocation0] (stack107)
        %v77794 = vpack.c.bf16 0.0, %v77791 (stack104)
        %77795 = vst [vmem:[%s77793] sm:$0xf] /*vst_source=*/%v77794 (stack105)
        %v77798 = vadd.s32 %v3329, %v75029 (stack65)
        %s77800 = smul.u32 128, %s27 (stack66)
        %v77801 = vlaneseq (stack67)
        %v77802 = vand.u32 %v77801, 127 (stack68)
        %v77803 = vstv %s77800 (stack69)
        %v77804 = vadd.s32 %v77802, %v77803 (stack70)
        %v77808 = vadd.s32 %v77798, %v77804 (stack65)
        %vm77812 = vcmp.lt.u32.totalorder %v77808, %v77798 (stack71)
        %vm77817 = vcmp.lt.u32.totalorder %v77798, %v3329 (stack71)
        %v77822 = vadd.s32 %v3316, %v75012 (stack65)
        %v77826 = vadd.s32 %v77822, 1 (stack65)
        %v77830 = vsel /*vm=*/%vm77817, /*on_true_vy=*/%v77826, /*on_false_vx=*/%v77822 (stack72)
        %v77834 = vadd.s32 %v77830, 1 (stack65)
        %v77838 = vsel /*vm=*/%vm77812, /*on_true_vy=*/%v77834, /*on_false_vx=*/%v77830 (stack72)
        %v77843 = vadd.s32 %v77838, %v10 (stack65)
        %v77847 = vadd.s32 %v77808, %v9 (stack65)
        %v77851 = vadd.s32 %v77843, %v77847 (stack65)
        %v77853 = vshll.u32 %v77847, 13 (stack73)
        %v77854 = vshrl.u32 %v77847, 19 (stack74)
        %v77855 = vor.u32 %v77853, %v77854 (stack75)
        %v77856 = vxor.u32 %v77851, %v77855 (stack76)
        %v77859 = vadd.s32 %v77851, %v77856 (stack65)
        %v77861 = vshll.u32 %v77856, 15 (stack73)
        %v77862 = vshrl.u32 %v77856, 17 (stack74)
        %v77863 = vor.u32 %v77861, %v77862 (stack75)
        %v77864 = vxor.u32 %v77859, %v77863 (stack76)
        %v77867 = vadd.s32 %v77859, %v77864 (stack65)
        %v77869 = vshll.u32 %v77864, 26 (stack73)
        %v77870 = vshrl.u32 %v77864, 6 (stack74)
        %v77871 = vor.u32 %v77869, %v77870 (stack75)
        %v77872 = vxor.u32 %v77867, %v77871 (stack76)
        %v77875 = vadd.s32 %v77867, %v77872 (stack65)
        %v77879 = vadd.s32 %v77875, %v9 (stack65)
        %v77881 = vshll.u32 %v77872, 6 (stack73)
        %v77882 = vshrl.u32 %v77872, 26 (stack74)
        %v77883 = vor.u32 %v77881, %v77882 (stack75)
        %v77884 = vxor.u32 %v77875, %v77883 (stack76)
        %v77887 = vadd.s32 %v77884, %v8 (stack65)
        %v77891 = vadd.s32 %v77887, 1 (stack65)
        %v77895 = vadd.s32 %v77879, %v77891 (stack65)
        %v77897 = vshll.u32 %v77891, 17 (stack73)
        %v77898 = vshrl.u32 %v77891, 15 (stack74)
        %v77899 = vor.u32 %v77897, %v77898 (stack75)
        %v77900 = vxor.u32 %v77895, %v77899 (stack76)
        %v77903 = vadd.s32 %v77895, %v77900 (stack65)
        %v77905 = vshll.u32 %v77900, 29 (stack73)
        %v77906 = vshrl.u32 %v77900, 3 (stack74)
        %v77907 = vor.u32 %v77905, %v77906 (stack75)
        %v77908 = vxor.u32 %v77903, %v77907 (stack76)
        %v77911 = vadd.s32 %v77903, %v77908 (stack65)
        %v77913 = vshll.u32 %v77908, 16 (stack73)
        %v77914 = vshrl.u32 %v77908, 16 (stack74)
        %v77915 = vor.u32 %v77913, %v77914 (stack75)
        %v77916 = vxor.u32 %v77911, %v77915 (stack76)
        %v77919 = vadd.s32 %v77911, %v77916 (stack65)
        %v77923 = vadd.s32 %v77919, %v8 (stack65)
        %v77925 = vshll.u32 %v77916, 24 (stack73)
        %v77926 = vshrl.u32 %v77916, 8 (stack74)
        %v77927 = vor.u32 %v77925, %v77926 (stack75)
        %v77928 = vxor.u32 %v77919, %v77927 (stack76)
        %v77931 = vadd.s32 %v77928, %v10 (stack65)
        %v77935 = vadd.s32 %v77931, 2 (stack65)
        %v77939 = vadd.s32 %v77923, %v77935 (stack65)
        %v77941 = vshll.u32 %v77935, 13 (stack73)
        %v77942 = vshrl.u32 %v77935, 19 (stack74)
        %v77943 = vor.u32 %v77941, %v77942 (stack75)
        %v77944 = vxor.u32 %v77939, %v77943 (stack76)
        %v77947 = vadd.s32 %v77939, %v77944 (stack65)
        %v77949 = vshll.u32 %v77944, 15 (stack73)
        %v77950 = vshrl.u32 %v77944, 17 (stack74)
        %v77951 = vor.u32 %v77949, %v77950 (stack75)
        %v77952 = vxor.u32 %v77947, %v77951 (stack76)
        %v77955 = vadd.s32 %v77947, %v77952 (stack65)
        %v77957 = vshll.u32 %v77952, 26 (stack73)
        %v77958 = vshrl.u32 %v77952, 6 (stack74)
        %v77959 = vor.u32 %v77957, %v77958 (stack75)
        %v77960 = vxor.u32 %v77955, %v77959 (stack76)
        %v77963 = vadd.s32 %v77955, %v77960 (stack65)
        %v77967 = vadd.s32 %v77963, %v10 (stack65)
        %v77969 = vshll.u32 %v77960, 6 (stack73)
        %v77970 = vshrl.u32 %v77960, 26 (stack74)
        %v77971 = vor.u32 %v77969, %v77970 (stack75)
        %v77972 = vxor.u32 %v77963, %v77971 (stack76)
        %v77975 = vadd.s32 %v77972, %v9 (stack65)
        %v77979 = vadd.s32 %v77975, 3 (stack65)
        %v77983 = vadd.s32 %v77967, %v77979 (stack65)
        %v77985 = vshll.u32 %v77979, 17 (stack73)
        %v77986 = vshrl.u32 %v77979, 15 (stack74)
        %v77987 = vor.u32 %v77985, %v77986 (stack75)
        %v77988 = vxor.u32 %v77983, %v77987 (stack76)
        %v77991 = vadd.s32 %v77983, %v77988 (stack65)
        %v77993 = vshll.u32 %v77988, 29 (stack73)
        %v77994 = vshrl.u32 %v77988, 3 (stack74)
        %v77995 = vor.u32 %v77993, %v77994 (stack75)
        %v77996 = vxor.u32 %v77991, %v77995 (stack76)
        %v77999 = vadd.s32 %v77991, %v77996 (stack65)
        %v78001 = vshll.u32 %v77996, 16 (stack73)
        %v78002 = vshrl.u32 %v77996, 16 (stack74)
        %v78003 = vor.u32 %v78001, %v78002 (stack75)
        %v78004 = vxor.u32 %v77999, %v78003 (stack76)
        %v78007 = vadd.s32 %v77999, %v78004 (stack65)
        %v78011 = vadd.s32 %v78007, %v9 (stack65)
        %v78013 = vshll.u32 %v78004, 24 (stack73)
        %v78014 = vshrl.u32 %v78004, 8 (stack74)
        %v78015 = vor.u32 %v78013, %v78014 (stack75)
        %v78016 = vxor.u32 %v78007, %v78015 (stack76)
        %v78019 = vadd.s32 %v78016, %v8 (stack65)
        %v78023 = vadd.s32 %v78019, 4 (stack65)
        %v78027 = vadd.s32 %v78011, %v78023 (stack65)
        %v78029 = vshll.u32 %v78023, 13 (stack73)
        %v78030 = vshrl.u32 %v78023, 19 (stack74)
        %v78031 = vor.u32 %v78029, %v78030 (stack75)
        %v78032 = vxor.u32 %v78027, %v78031 (stack76)
        %v78035 = vadd.s32 %v78027, %v78032 (stack65)
        %v78037 = vshll.u32 %v78032, 15 (stack73)
        %v78038 = vshrl.u32 %v78032, 17 (stack74)
        %v78039 = vor.u32 %v78037, %v78038 (stack75)
        %v78040 = vxor.u32 %v78035, %v78039 (stack76)
        %v78043 = vadd.s32 %v78035, %v78040 (stack65)
        %v78045 = vshll.u32 %v78040, 26 (stack73)
        %v78046 = vshrl.u32 %v78040, 6 (stack74)
        %v78047 = vor.u32 %v78045, %v78046 (stack75)
        %v78048 = vxor.u32 %v78043, %v78047 (stack76)
        %v78051 = vadd.s32 %v78043, %v78048 (stack65)
        %v78055 = vadd.s32 %v78051, %v8 (stack65)
        %v78057 = vshll.u32 %v78048, 6 (stack73)
        %v78058 = vshrl.u32 %v78048, 26 (stack74)
        %v78059 = vor.u32 %v78057, %v78058 (stack75)
        %v78060 = vxor.u32 %v78051, %v78059 (stack76)
        %v78063 = vadd.s32 %v78060, %v10 (stack65)
        %v78067 = vadd.s32 %v78063, 5 (stack65)
        %v78069 = vxor.u32 %v78055, %v78067 (stack76)
        %v78070 = vand.u32.u8 %v78069, 255 (stack77)
        %v78071 = vand.u32 %v78070, 65535 (stack78)
        %v78072 = vshrl.u32 %v78071, 1 (stack79)
        %v78073 = vor.u32 %v78072, 16256 (stack75)
        %v78074 = vand.u32.u16 %v78073, 65535 (stack80)
        %v78075 = vunpack.i.l.bf16 %v78074 (stack81)
        %v78079 = vadd.f32 %v78075, -1.0 (stack82)
        %v78083 = vmul.f32 %v78079, 2.0 (stack83)
        %v78087 = vadd.f32 %v78083, -0.99609375 (stack82)
        %v78091 = vmax.f32 -0.99609375, %v78087 (stack84)
        %v78093 = vand.u32 2147483647, %v78091 (stack85)
        %vm78096 = vcmp.eq.f32.partialorder %v78093, 1.0 (stack86)
        %v78101 = vmul.f32 %v78091, inf (stack83)
        %v78103 = vxor.u32 %v78091, 2147483648 (stack87)
        %v78106 = vmul.f32 %v78091, %v78103 (stack83)
        %v78108 = vadd.f32 %v78106, 1.0 (stack88)
        %v78109 = vlog2.pop %v78108 (stack89)
        %v78110 = vmul.f32 %v78109, 0.6931472 (stack90)
        %v78111 = vmul.f32 -0.5, %v78106 (stack91)
        %v78112 = vadd.f32 %v78111, 1.0 (stack92)
        %v78113 = vmul.f32 %v78112, %v78106 (stack93)
        %v78114 = vand.u32 2147483647, %v78106 (stack94)
        %vm78115 = vcmp.lt.f32.partialorder %v78114, 0.0004427343 (stack95)
        %v78116 = vsel /*vm=*/%vm78115, /*on_true_vy=*/%v78113, /*on_false_vx=*/%v78110 (stack96)
        %v78117 = vxor.u32 %v78116, 2147483648 (stack87)
        %vm78120 = vcmp.lt.f32.partialorder %v78117, 5.0 (stack86)
        %v78125 = vsel /*vm=*/%vm78120, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v78129 = vsel /*vm=*/%vm78120, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v78133 = vsel /*vm=*/%vm78120, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v78137 = vsel /*vm=*/%vm78120, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v78141 = vsel /*vm=*/%vm78120, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v78145 = vsel /*vm=*/%vm78120, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v78149 = vsel /*vm=*/%vm78120, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v78153 = vsel /*vm=*/%vm78120, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v78157 = vsel /*vm=*/%vm78120, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v78161 = vadd.f32 %v78117, -2.5 (stack82)
        %v78163 = vrsqrt.pop %v78117 (stack97)
        %v78164 = vmul.f32 %v78117, %v78163 (stack98)
        %vm78165 = vcmp.eq.f32.partialorder %v78117, inf (stack99)
        %v78166 = vsel /*vm=*/%vm78165, /*on_true_vy=*/%v78117, /*on_false_vx=*/%v78164 (stack100)
        %vm78167 = vcmp.eq.f32.partialorder %v78117, 0.0 (stack101)
        %v78168 = vand.u32 %v78117, 2147483648 (stack102)
        %v78169 = vsel /*vm=*/%vm78167, /*on_true_vy=*/%v78168, /*on_false_vx=*/%v78166 (stack103)
        %v78172 = vadd.f32 %v78169, -3.0 (stack82)
        %v78176 = vsel /*vm=*/%vm78120, /*on_true_vy=*/%v78161, /*on_false_vx=*/%v78172 (stack72)
        %v78180 = vmul.f32 %v78157, %v78176 (stack83)
        %v78184 = vadd.f32 %v78153, %v78180 (stack82)
        %v78188 = vmul.f32 %v78184, %v78176 (stack83)
        %v78192 = vadd.f32 %v78149, %v78188 (stack82)
        %v78196 = vmul.f32 %v78192, %v78176 (stack83)
        %v78200 = vadd.f32 %v78145, %v78196 (stack82)
        %v78204 = vmul.f32 %v78200, %v78176 (stack83)
        %v78208 = vadd.f32 %v78141, %v78204 (stack82)
        %v78212 = vmul.f32 %v78208, %v78176 (stack83)
        %v78216 = vadd.f32 %v78137, %v78212 (stack82)
        %v78220 = vmul.f32 %v78216, %v78176 (stack83)
        %v78224 = vadd.f32 %v78133, %v78220 (stack82)
        %v78228 = vmul.f32 %v78224, %v78176 (stack83)
        %v78232 = vadd.f32 %v78129, %v78228 (stack82)
        %v78236 = vmul.f32 %v78232, %v78176 (stack83)
        %v78240 = vadd.f32 %v78125, %v78236 (stack82)
        %v78244 = vmul.f32 %v78240, %v78091 (stack83)
        %v78248 = vsel /*vm=*/%vm78096, /*on_true_vy=*/%v78101, /*on_false_vx=*/%v78244 (stack72)
        %v78252 = vmul.f32 %v78248, 1.4140625 (stack83)
        %s78254 = scalar_lea.vmem %s280, 848 [#allocation0] (stack107)
        %v78255 = vpack.c.bf16 0.0, %v78252 (stack104)
        %78256 = vst [vmem:[%s78254] sm:$0xf] /*vst_source=*/%v78255 (stack105)
        %v78259 = vadd.s32 %v3816, %v75029 (stack65)
        %s78261 = smul.u32 128, %s27 (stack66)
        %v78262 = vlaneseq (stack67)
        %v78263 = vand.u32 %v78262, 127 (stack68)
        %v78264 = vstv %s78261 (stack69)
        %v78265 = vadd.s32 %v78263, %v78264 (stack70)
        %v78269 = vadd.s32 %v78259, %v78265 (stack65)
        %vm78273 = vcmp.lt.u32.totalorder %v78269, %v78259 (stack71)
        %vm78278 = vcmp.lt.u32.totalorder %v78259, %v3816 (stack71)
        %v78283 = vadd.s32 %v3803, %v75012 (stack65)
        %v78287 = vadd.s32 %v78283, 1 (stack65)
        %v78291 = vsel /*vm=*/%vm78278, /*on_true_vy=*/%v78287, /*on_false_vx=*/%v78283 (stack72)
        %v78295 = vadd.s32 %v78291, 1 (stack65)
        %v78299 = vsel /*vm=*/%vm78273, /*on_true_vy=*/%v78295, /*on_false_vx=*/%v78291 (stack72)
        %v78304 = vadd.s32 %v78299, %v10 (stack65)
        %v78308 = vadd.s32 %v78269, %v9 (stack65)
        %v78312 = vadd.s32 %v78304, %v78308 (stack65)
        %v78314 = vshll.u32 %v78308, 13 (stack73)
        %v78315 = vshrl.u32 %v78308, 19 (stack74)
        %v78316 = vor.u32 %v78314, %v78315 (stack75)
        %v78317 = vxor.u32 %v78312, %v78316 (stack76)
        %v78320 = vadd.s32 %v78312, %v78317 (stack65)
        %v78322 = vshll.u32 %v78317, 15 (stack73)
        %v78323 = vshrl.u32 %v78317, 17 (stack74)
        %v78324 = vor.u32 %v78322, %v78323 (stack75)
        %v78325 = vxor.u32 %v78320, %v78324 (stack76)
        %v78328 = vadd.s32 %v78320, %v78325 (stack65)
        %v78330 = vshll.u32 %v78325, 26 (stack73)
        %v78331 = vshrl.u32 %v78325, 6 (stack74)
        %v78332 = vor.u32 %v78330, %v78331 (stack75)
        %v78333 = vxor.u32 %v78328, %v78332 (stack76)
        %v78336 = vadd.s32 %v78328, %v78333 (stack65)
        %v78340 = vadd.s32 %v78336, %v9 (stack65)
        %v78342 = vshll.u32 %v78333, 6 (stack73)
        %v78343 = vshrl.u32 %v78333, 26 (stack74)
        %v78344 = vor.u32 %v78342, %v78343 (stack75)
        %v78345 = vxor.u32 %v78336, %v78344 (stack76)
        %v78348 = vadd.s32 %v78345, %v8 (stack65)
        %v78352 = vadd.s32 %v78348, 1 (stack65)
        %v78356 = vadd.s32 %v78340, %v78352 (stack65)
        %v78358 = vshll.u32 %v78352, 17 (stack73)
        %v78359 = vshrl.u32 %v78352, 15 (stack74)
        %v78360 = vor.u32 %v78358, %v78359 (stack75)
        %v78361 = vxor.u32 %v78356, %v78360 (stack76)
        %v78364 = vadd.s32 %v78356, %v78361 (stack65)
        %v78366 = vshll.u32 %v78361, 29 (stack73)
        %v78367 = vshrl.u32 %v78361, 3 (stack74)
        %v78368 = vor.u32 %v78366, %v78367 (stack75)
        %v78369 = vxor.u32 %v78364, %v78368 (stack76)
        %v78372 = vadd.s32 %v78364, %v78369 (stack65)
        %v78374 = vshll.u32 %v78369, 16 (stack73)
        %v78375 = vshrl.u32 %v78369, 16 (stack74)
        %v78376 = vor.u32 %v78374, %v78375 (stack75)
        %v78377 = vxor.u32 %v78372, %v78376 (stack76)
        %v78380 = vadd.s32 %v78372, %v78377 (stack65)
        %v78384 = vadd.s32 %v78380, %v8 (stack65)
        %v78386 = vshll.u32 %v78377, 24 (stack73)
        %v78387 = vshrl.u32 %v78377, 8 (stack74)
        %v78388 = vor.u32 %v78386, %v78387 (stack75)
        %v78389 = vxor.u32 %v78380, %v78388 (stack76)
        %v78392 = vadd.s32 %v78389, %v10 (stack65)
        %v78396 = vadd.s32 %v78392, 2 (stack65)
        %v78400 = vadd.s32 %v78384, %v78396 (stack65)
        %v78402 = vshll.u32 %v78396, 13 (stack73)
        %v78403 = vshrl.u32 %v78396, 19 (stack74)
        %v78404 = vor.u32 %v78402, %v78403 (stack75)
        %v78405 = vxor.u32 %v78400, %v78404 (stack76)
        %v78408 = vadd.s32 %v78400, %v78405 (stack65)
        %v78410 = vshll.u32 %v78405, 15 (stack73)
        %v78411 = vshrl.u32 %v78405, 17 (stack74)
        %v78412 = vor.u32 %v78410, %v78411 (stack75)
        %v78413 = vxor.u32 %v78408, %v78412 (stack76)
        %v78416 = vadd.s32 %v78408, %v78413 (stack65)
        %v78418 = vshll.u32 %v78413, 26 (stack73)
        %v78419 = vshrl.u32 %v78413, 6 (stack74)
        %v78420 = vor.u32 %v78418, %v78419 (stack75)
        %v78421 = vxor.u32 %v78416, %v78420 (stack76)
        %v78424 = vadd.s32 %v78416, %v78421 (stack65)
        %v78428 = vadd.s32 %v78424, %v10 (stack65)
        %v78430 = vshll.u32 %v78421, 6 (stack73)
        %v78431 = vshrl.u32 %v78421, 26 (stack74)
        %v78432 = vor.u32 %v78430, %v78431 (stack75)
        %v78433 = vxor.u32 %v78424, %v78432 (stack76)
        %v78436 = vadd.s32 %v78433, %v9 (stack65)
        %v78440 = vadd.s32 %v78436, 3 (stack65)
        %v78444 = vadd.s32 %v78428, %v78440 (stack65)
        %v78446 = vshll.u32 %v78440, 17 (stack73)
        %v78447 = vshrl.u32 %v78440, 15 (stack74)
        %v78448 = vor.u32 %v78446, %v78447 (stack75)
        %v78449 = vxor.u32 %v78444, %v78448 (stack76)
        %v78452 = vadd.s32 %v78444, %v78449 (stack65)
        %v78454 = vshll.u32 %v78449, 29 (stack73)
        %v78455 = vshrl.u32 %v78449, 3 (stack74)
        %v78456 = vor.u32 %v78454, %v78455 (stack75)
        %v78457 = vxor.u32 %v78452, %v78456 (stack76)
        %v78460 = vadd.s32 %v78452, %v78457 (stack65)
        %v78462 = vshll.u32 %v78457, 16 (stack73)
        %v78463 = vshrl.u32 %v78457, 16 (stack74)
        %v78464 = vor.u32 %v78462, %v78463 (stack75)
        %v78465 = vxor.u32 %v78460, %v78464 (stack76)
        %v78468 = vadd.s32 %v78460, %v78465 (stack65)
        %v78472 = vadd.s32 %v78468, %v9 (stack65)
        %v78474 = vshll.u32 %v78465, 24 (stack73)
        %v78475 = vshrl.u32 %v78465, 8 (stack74)
        %v78476 = vor.u32 %v78474, %v78475 (stack75)
        %v78477 = vxor.u32 %v78468, %v78476 (stack76)
        %v78480 = vadd.s32 %v78477, %v8 (stack65)
        %v78484 = vadd.s32 %v78480, 4 (stack65)
        %v78488 = vadd.s32 %v78472, %v78484 (stack65)
        %v78490 = vshll.u32 %v78484, 13 (stack73)
        %v78491 = vshrl.u32 %v78484, 19 (stack74)
        %v78492 = vor.u32 %v78490, %v78491 (stack75)
        %v78493 = vxor.u32 %v78488, %v78492 (stack76)
        %v78496 = vadd.s32 %v78488, %v78493 (stack65)
        %v78498 = vshll.u32 %v78493, 15 (stack73)
        %v78499 = vshrl.u32 %v78493, 17 (stack74)
        %v78500 = vor.u32 %v78498, %v78499 (stack75)
        %v78501 = vxor.u32 %v78496, %v78500 (stack76)
        %v78504 = vadd.s32 %v78496, %v78501 (stack65)
        %v78506 = vshll.u32 %v78501, 26 (stack73)
        %v78507 = vshrl.u32 %v78501, 6 (stack74)
        %v78508 = vor.u32 %v78506, %v78507 (stack75)
        %v78509 = vxor.u32 %v78504, %v78508 (stack76)
        %v78512 = vadd.s32 %v78504, %v78509 (stack65)
        %v78516 = vadd.s32 %v78512, %v8 (stack65)
        %v78518 = vshll.u32 %v78509, 6 (stack73)
        %v78519 = vshrl.u32 %v78509, 26 (stack74)
        %v78520 = vor.u32 %v78518, %v78519 (stack75)
        %v78521 = vxor.u32 %v78512, %v78520 (stack76)
        %v78524 = vadd.s32 %v78521, %v10 (stack65)
        %v78528 = vadd.s32 %v78524, 5 (stack65)
        %v78530 = vxor.u32 %v78516, %v78528 (stack76)
        %v78531 = vand.u32.u8 %v78530, 255 (stack77)
        %v78532 = vand.u32 %v78531, 65535 (stack78)
        %v78533 = vshrl.u32 %v78532, 1 (stack79)
        %v78534 = vor.u32 %v78533, 16256 (stack75)
        %v78535 = vand.u32.u16 %v78534, 65535 (stack80)
        %v78536 = vunpack.i.l.bf16 %v78535 (stack81)
        %v78540 = vadd.f32 %v78536, -1.0 (stack82)
        %v78544 = vmul.f32 %v78540, 2.0 (stack83)
        %v78548 = vadd.f32 %v78544, -0.99609375 (stack82)
        %v78552 = vmax.f32 -0.99609375, %v78548 (stack84)
        %v78554 = vand.u32 2147483647, %v78552 (stack85)
        %vm78557 = vcmp.eq.f32.partialorder %v78554, 1.0 (stack86)
        %v78562 = vmul.f32 %v78552, inf (stack83)
        %v78564 = vxor.u32 %v78552, 2147483648 (stack87)
        %v78567 = vmul.f32 %v78552, %v78564 (stack83)
        %v78569 = vadd.f32 %v78567, 1.0 (stack88)
        %v78570 = vlog2.pop %v78569 (stack89)
        %v78571 = vmul.f32 %v78570, 0.6931472 (stack90)
        %v78572 = vmul.f32 -0.5, %v78567 (stack91)
        %v78573 = vadd.f32 %v78572, 1.0 (stack92)
        %v78574 = vmul.f32 %v78573, %v78567 (stack93)
        %v78575 = vand.u32 2147483647, %v78567 (stack94)
        %vm78576 = vcmp.lt.f32.partialorder %v78575, 0.0004427343 (stack95)
        %v78577 = vsel /*vm=*/%vm78576, /*on_true_vy=*/%v78574, /*on_false_vx=*/%v78571 (stack96)
        %v78578 = vxor.u32 %v78577, 2147483648 (stack87)
        %vm78581 = vcmp.lt.f32.partialorder %v78578, 5.0 (stack86)
        %v78586 = vsel /*vm=*/%vm78581, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v78590 = vsel /*vm=*/%vm78581, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v78594 = vsel /*vm=*/%vm78581, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v78598 = vsel /*vm=*/%vm78581, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v78602 = vsel /*vm=*/%vm78581, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v78606 = vsel /*vm=*/%vm78581, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v78610 = vsel /*vm=*/%vm78581, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v78614 = vsel /*vm=*/%vm78581, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v78618 = vsel /*vm=*/%vm78581, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v78622 = vadd.f32 %v78578, -2.5 (stack82)
        %v78624 = vrsqrt.pop %v78578 (stack97)
        %v78625 = vmul.f32 %v78578, %v78624 (stack98)
        %vm78626 = vcmp.eq.f32.partialorder %v78578, inf (stack99)
        %v78627 = vsel /*vm=*/%vm78626, /*on_true_vy=*/%v78578, /*on_false_vx=*/%v78625 (stack100)
        %vm78628 = vcmp.eq.f32.partialorder %v78578, 0.0 (stack101)
        %v78629 = vand.u32 %v78578, 2147483648 (stack102)
        %v78630 = vsel /*vm=*/%vm78628, /*on_true_vy=*/%v78629, /*on_false_vx=*/%v78627 (stack103)
        %v78633 = vadd.f32 %v78630, -3.0 (stack82)
        %v78637 = vsel /*vm=*/%vm78581, /*on_true_vy=*/%v78622, /*on_false_vx=*/%v78633 (stack72)
        %v78641 = vmul.f32 %v78618, %v78637 (stack83)
        %v78645 = vadd.f32 %v78614, %v78641 (stack82)
        %v78649 = vmul.f32 %v78645, %v78637 (stack83)
        %v78653 = vadd.f32 %v78610, %v78649 (stack82)
        %v78657 = vmul.f32 %v78653, %v78637 (stack83)
        %v78661 = vadd.f32 %v78606, %v78657 (stack82)
        %v78665 = vmul.f32 %v78661, %v78637 (stack83)
        %v78669 = vadd.f32 %v78602, %v78665 (stack82)
        %v78673 = vmul.f32 %v78669, %v78637 (stack83)
        %v78677 = vadd.f32 %v78598, %v78673 (stack82)
        %v78681 = vmul.f32 %v78677, %v78637 (stack83)
        %v78685 = vadd.f32 %v78594, %v78681 (stack82)
        %v78689 = vmul.f32 %v78685, %v78637 (stack83)
        %v78693 = vadd.f32 %v78590, %v78689 (stack82)
        %v78697 = vmul.f32 %v78693, %v78637 (stack83)
        %v78701 = vadd.f32 %v78586, %v78697 (stack82)
        %v78705 = vmul.f32 %v78701, %v78552 (stack83)
        %v78709 = vsel /*vm=*/%vm78557, /*on_true_vy=*/%v78562, /*on_false_vx=*/%v78705 (stack72)
        %v78713 = vmul.f32 %v78709, 1.4140625 (stack83)
        %s78715 = scalar_lea.vmem %s280, 976 [#allocation0] (stack107)
        %v78716 = vpack.c.bf16 0.0, %v78713 (stack104)
        %78717 = vst [vmem:[%s78715] sm:$0xf] /*vst_source=*/%v78716 (stack105)
        %s78718 = sadd.s32 %s339, 168 (stack106)
        %s78719 = sshrl.u32 %s78718, 10 (stack49)
        %p78720 = scmp.lt.s32.totalorder 1, %s78719 (stack50)
        %s78721 = scalar_select /*predicate=*/%p78720, /*on_true=*/1, /*on_false=*/%s78719 (stack51)
        %s78722 = sand.u32 %s78718, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s78723 = sshrl.u32 %s78722, 7 (stack53)
        %s78724 = sand.u32 %s78722, 127 /* smod.u32 w/div 128 */ (stack54)
        %s78725 = smul.addr %s78721, 8 (stack55)
        %s78726 = scalar_lea.vmem %s3, %s78725 (stack56)
        %s78728 = scalar_lea.vmem %s78726, %s78723 (stack57)
        %v78729 = vld [vmem:[%s78728] ss:$0 sm:$0xff] (stack58)
        %s78730 = sand.u32 %s78724, 255 (stack59)
        %s78732 = sor.u32 256, %s78730 (stack60)
        %78733 = vbcast.lane.b32.xlu0 %v78729, %s78732 (stack61)
        %v78734 = vpop.permute.xlu0 %78733 (stack62)
        %s78735 = sadd.s32 %s347, 168 (stack106)
        %s78736 = sshrl.u32 %s78735, 10 (stack49)
        %p78737 = scmp.lt.s32.totalorder 1, %s78736 (stack50)
        %s78738 = scalar_select /*predicate=*/%p78737, /*on_true=*/1, /*on_false=*/%s78736 (stack51)
        %s78739 = sand.u32 %s78735, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s78740 = sshrl.u32 %s78739, 7 (stack53)
        %s78741 = sand.u32 %s78739, 127 /* smod.u32 w/div 128 */ (stack54)
        %s78742 = smul.addr %s78738, 8 (stack55)
        %s78743 = scalar_lea.vmem %s5, %s78742 (stack56)
        %s78745 = scalar_lea.vmem %s78743, %s78740 (stack57)
        %v78746 = vld [vmem:[%s78745] ss:$0 sm:$0xff] (stack58)
        %s78747 = sand.u32 %s78741, 255 (stack59)
        %s78749 = sor.u32 256, %s78747 (stack60)
        %78750 = vbcast.lane.b32.xlu0 %v78746, %s78749 (stack61)
        %v78751 = vpop.permute.xlu0 %78750 (stack62)
        %v78754 = vadd.s32 %v408, %v78751 (stack65)
        %s78756 = smul.u32 128, %s27 (stack66)
        %v78757 = vlaneseq (stack67)
        %v78758 = vand.u32 %v78757, 127 (stack68)
        %v78759 = vstv %s78756 (stack69)
        %v78760 = vadd.s32 %v78758, %v78759 (stack70)
        %v78764 = vadd.s32 %v78754, %v78760 (stack65)
        %vm78768 = vcmp.lt.u32.totalorder %v78764, %v78754 (stack71)
        %vm78773 = vcmp.lt.u32.totalorder %v78754, %v408 (stack71)
        %v78778 = vadd.s32 %v380, %v78734 (stack65)
        %v78782 = vadd.s32 %v78778, 1 (stack65)
        %v78786 = vsel /*vm=*/%vm78773, /*on_true_vy=*/%v78782, /*on_false_vx=*/%v78778 (stack72)
        %v78790 = vadd.s32 %v78786, 1 (stack65)
        %v78794 = vsel /*vm=*/%vm78768, /*on_true_vy=*/%v78790, /*on_false_vx=*/%v78786 (stack72)
        %v78799 = vadd.s32 %v78794, %v10 (stack65)
        %v78803 = vadd.s32 %v78764, %v9 (stack65)
        %v78807 = vadd.s32 %v78799, %v78803 (stack65)
        %v78809 = vshll.u32 %v78803, 13 (stack73)
        %v78810 = vshrl.u32 %v78803, 19 (stack74)
        %v78811 = vor.u32 %v78809, %v78810 (stack75)
        %v78812 = vxor.u32 %v78807, %v78811 (stack76)
        %v78815 = vadd.s32 %v78807, %v78812 (stack65)
        %v78817 = vshll.u32 %v78812, 15 (stack73)
        %v78818 = vshrl.u32 %v78812, 17 (stack74)
        %v78819 = vor.u32 %v78817, %v78818 (stack75)
        %v78820 = vxor.u32 %v78815, %v78819 (stack76)
        %v78823 = vadd.s32 %v78815, %v78820 (stack65)
        %v78825 = vshll.u32 %v78820, 26 (stack73)
        %v78826 = vshrl.u32 %v78820, 6 (stack74)
        %v78827 = vor.u32 %v78825, %v78826 (stack75)
        %v78828 = vxor.u32 %v78823, %v78827 (stack76)
        %v78831 = vadd.s32 %v78823, %v78828 (stack65)
        %v78835 = vadd.s32 %v78831, %v9 (stack65)
        %v78837 = vshll.u32 %v78828, 6 (stack73)
        %v78838 = vshrl.u32 %v78828, 26 (stack74)
        %v78839 = vor.u32 %v78837, %v78838 (stack75)
        %v78840 = vxor.u32 %v78831, %v78839 (stack76)
        %v78843 = vadd.s32 %v78840, %v8 (stack65)
        %v78847 = vadd.s32 %v78843, 1 (stack65)
        %v78851 = vadd.s32 %v78835, %v78847 (stack65)
        %v78853 = vshll.u32 %v78847, 17 (stack73)
        %v78854 = vshrl.u32 %v78847, 15 (stack74)
        %v78855 = vor.u32 %v78853, %v78854 (stack75)
        %v78856 = vxor.u32 %v78851, %v78855 (stack76)
        %v78859 = vadd.s32 %v78851, %v78856 (stack65)
        %v78861 = vshll.u32 %v78856, 29 (stack73)
        %v78862 = vshrl.u32 %v78856, 3 (stack74)
        %v78863 = vor.u32 %v78861, %v78862 (stack75)
        %v78864 = vxor.u32 %v78859, %v78863 (stack76)
        %v78867 = vadd.s32 %v78859, %v78864 (stack65)
        %v78869 = vshll.u32 %v78864, 16 (stack73)
        %v78870 = vshrl.u32 %v78864, 16 (stack74)
        %v78871 = vor.u32 %v78869, %v78870 (stack75)
        %v78872 = vxor.u32 %v78867, %v78871 (stack76)
        %v78875 = vadd.s32 %v78867, %v78872 (stack65)
        %v78879 = vadd.s32 %v78875, %v8 (stack65)
        %v78881 = vshll.u32 %v78872, 24 (stack73)
        %v78882 = vshrl.u32 %v78872, 8 (stack74)
        %v78883 = vor.u32 %v78881, %v78882 (stack75)
        %v78884 = vxor.u32 %v78875, %v78883 (stack76)
        %v78887 = vadd.s32 %v78884, %v10 (stack65)
        %v78891 = vadd.s32 %v78887, 2 (stack65)
        %v78895 = vadd.s32 %v78879, %v78891 (stack65)
        %v78897 = vshll.u32 %v78891, 13 (stack73)
        %v78898 = vshrl.u32 %v78891, 19 (stack74)
        %v78899 = vor.u32 %v78897, %v78898 (stack75)
        %v78900 = vxor.u32 %v78895, %v78899 (stack76)
        %v78903 = vadd.s32 %v78895, %v78900 (stack65)
        %v78905 = vshll.u32 %v78900, 15 (stack73)
        %v78906 = vshrl.u32 %v78900, 17 (stack74)
        %v78907 = vor.u32 %v78905, %v78906 (stack75)
        %v78908 = vxor.u32 %v78903, %v78907 (stack76)
        %v78911 = vadd.s32 %v78903, %v78908 (stack65)
        %v78913 = vshll.u32 %v78908, 26 (stack73)
        %v78914 = vshrl.u32 %v78908, 6 (stack74)
        %v78915 = vor.u32 %v78913, %v78914 (stack75)
        %v78916 = vxor.u32 %v78911, %v78915 (stack76)
        %v78919 = vadd.s32 %v78911, %v78916 (stack65)
        %v78923 = vadd.s32 %v78919, %v10 (stack65)
        %v78925 = vshll.u32 %v78916, 6 (stack73)
        %v78926 = vshrl.u32 %v78916, 26 (stack74)
        %v78927 = vor.u32 %v78925, %v78926 (stack75)
        %v78928 = vxor.u32 %v78919, %v78927 (stack76)
        %v78931 = vadd.s32 %v78928, %v9 (stack65)
        %v78935 = vadd.s32 %v78931, 3 (stack65)
        %v78939 = vadd.s32 %v78923, %v78935 (stack65)
        %v78941 = vshll.u32 %v78935, 17 (stack73)
        %v78942 = vshrl.u32 %v78935, 15 (stack74)
        %v78943 = vor.u32 %v78941, %v78942 (stack75)
        %v78944 = vxor.u32 %v78939, %v78943 (stack76)
        %v78947 = vadd.s32 %v78939, %v78944 (stack65)
        %v78949 = vshll.u32 %v78944, 29 (stack73)
        %v78950 = vshrl.u32 %v78944, 3 (stack74)
        %v78951 = vor.u32 %v78949, %v78950 (stack75)
        %v78952 = vxor.u32 %v78947, %v78951 (stack76)
        %v78955 = vadd.s32 %v78947, %v78952 (stack65)
        %v78957 = vshll.u32 %v78952, 16 (stack73)
        %v78958 = vshrl.u32 %v78952, 16 (stack74)
        %v78959 = vor.u32 %v78957, %v78958 (stack75)
        %v78960 = vxor.u32 %v78955, %v78959 (stack76)
        %v78963 = vadd.s32 %v78955, %v78960 (stack65)
        %v78967 = vadd.s32 %v78963, %v9 (stack65)
        %v78969 = vshll.u32 %v78960, 24 (stack73)
        %v78970 = vshrl.u32 %v78960, 8 (stack74)
        %v78971 = vor.u32 %v78969, %v78970 (stack75)
        %v78972 = vxor.u32 %v78963, %v78971 (stack76)
        %v78975 = vadd.s32 %v78972, %v8 (stack65)
        %v78979 = vadd.s32 %v78975, 4 (stack65)
        %v78983 = vadd.s32 %v78967, %v78979 (stack65)
        %v78985 = vshll.u32 %v78979, 13 (stack73)
        %v78986 = vshrl.u32 %v78979, 19 (stack74)
        %v78987 = vor.u32 %v78985, %v78986 (stack75)
        %v78988 = vxor.u32 %v78983, %v78987 (stack76)
        %v78991 = vadd.s32 %v78983, %v78988 (stack65)
        %v78993 = vshll.u32 %v78988, 15 (stack73)
        %v78994 = vshrl.u32 %v78988, 17 (stack74)
        %v78995 = vor.u32 %v78993, %v78994 (stack75)
        %v78996 = vxor.u32 %v78991, %v78995 (stack76)
        %v78999 = vadd.s32 %v78991, %v78996 (stack65)
        %v79001 = vshll.u32 %v78996, 26 (stack73)
        %v79002 = vshrl.u32 %v78996, 6 (stack74)
        %v79003 = vor.u32 %v79001, %v79002 (stack75)
        %v79004 = vxor.u32 %v78999, %v79003 (stack76)
        %v79007 = vadd.s32 %v78999, %v79004 (stack65)
        %v79011 = vadd.s32 %v79007, %v8 (stack65)
        %v79013 = vshll.u32 %v79004, 6 (stack73)
        %v79014 = vshrl.u32 %v79004, 26 (stack74)
        %v79015 = vor.u32 %v79013, %v79014 (stack75)
        %v79016 = vxor.u32 %v79007, %v79015 (stack76)
        %v79019 = vadd.s32 %v79016, %v10 (stack65)
        %v79023 = vadd.s32 %v79019, 5 (stack65)
        %v79025 = vxor.u32 %v79011, %v79023 (stack76)
        %v79026 = vand.u32.u8 %v79025, 255 (stack77)
        %v79027 = vand.u32 %v79026, 65535 (stack78)
        %v79028 = vshrl.u32 %v79027, 1 (stack79)
        %v79029 = vor.u32 %v79028, 16256 (stack75)
        %v79030 = vand.u32.u16 %v79029, 65535 (stack80)
        %v79031 = vunpack.i.l.bf16 %v79030 (stack81)
        %v79035 = vadd.f32 %v79031, -1.0 (stack82)
        %v79039 = vmul.f32 %v79035, 2.0 (stack83)
        %v79043 = vadd.f32 %v79039, -0.99609375 (stack82)
        %v79047 = vmax.f32 -0.99609375, %v79043 (stack84)
        %v79049 = vand.u32 2147483647, %v79047 (stack85)
        %vm79052 = vcmp.eq.f32.partialorder %v79049, 1.0 (stack86)
        %v79057 = vmul.f32 %v79047, inf (stack83)
        %v79059 = vxor.u32 %v79047, 2147483648 (stack87)
        %v79062 = vmul.f32 %v79047, %v79059 (stack83)
        %v79064 = vadd.f32 %v79062, 1.0 (stack88)
        %v79065 = vlog2.pop %v79064 (stack89)
        %v79066 = vmul.f32 %v79065, 0.6931472 (stack90)
        %v79067 = vmul.f32 -0.5, %v79062 (stack91)
        %v79068 = vadd.f32 %v79067, 1.0 (stack92)
        %v79069 = vmul.f32 %v79068, %v79062 (stack93)
        %v79070 = vand.u32 2147483647, %v79062 (stack94)
        %vm79071 = vcmp.lt.f32.partialorder %v79070, 0.0004427343 (stack95)
        %v79072 = vsel /*vm=*/%vm79071, /*on_true_vy=*/%v79069, /*on_false_vx=*/%v79066 (stack96)
        %v79073 = vxor.u32 %v79072, 2147483648 (stack87)
        %vm79076 = vcmp.lt.f32.partialorder %v79073, 5.0 (stack86)
        %v79081 = vsel /*vm=*/%vm79076, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v79085 = vsel /*vm=*/%vm79076, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v79089 = vsel /*vm=*/%vm79076, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v79093 = vsel /*vm=*/%vm79076, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v79097 = vsel /*vm=*/%vm79076, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v79101 = vsel /*vm=*/%vm79076, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v79105 = vsel /*vm=*/%vm79076, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v79109 = vsel /*vm=*/%vm79076, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v79113 = vsel /*vm=*/%vm79076, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v79117 = vadd.f32 %v79073, -2.5 (stack82)
        %v79119 = vrsqrt.pop %v79073 (stack97)
        %v79120 = vmul.f32 %v79073, %v79119 (stack98)
        %vm79121 = vcmp.eq.f32.partialorder %v79073, inf (stack99)
        %v79122 = vsel /*vm=*/%vm79121, /*on_true_vy=*/%v79073, /*on_false_vx=*/%v79120 (stack100)
        %vm79123 = vcmp.eq.f32.partialorder %v79073, 0.0 (stack101)
        %v79124 = vand.u32 %v79073, 2147483648 (stack102)
        %v79125 = vsel /*vm=*/%vm79123, /*on_true_vy=*/%v79124, /*on_false_vx=*/%v79122 (stack103)
        %v79128 = vadd.f32 %v79125, -3.0 (stack82)
        %v79132 = vsel /*vm=*/%vm79076, /*on_true_vy=*/%v79117, /*on_false_vx=*/%v79128 (stack72)
        %v79136 = vmul.f32 %v79113, %v79132 (stack83)
        %v79140 = vadd.f32 %v79109, %v79136 (stack82)
        %v79144 = vmul.f32 %v79140, %v79132 (stack83)
        %v79148 = vadd.f32 %v79105, %v79144 (stack82)
        %v79152 = vmul.f32 %v79148, %v79132 (stack83)
        %v79156 = vadd.f32 %v79101, %v79152 (stack82)
        %v79160 = vmul.f32 %v79156, %v79132 (stack83)
        %v79164 = vadd.f32 %v79097, %v79160 (stack82)
        %v79168 = vmul.f32 %v79164, %v79132 (stack83)
        %v79172 = vadd.f32 %v79093, %v79168 (stack82)
        %v79176 = vmul.f32 %v79172, %v79132 (stack83)
        %v79180 = vadd.f32 %v79089, %v79176 (stack82)
        %v79184 = vmul.f32 %v79180, %v79132 (stack83)
        %v79188 = vadd.f32 %v79085, %v79184 (stack82)
        %v79192 = vmul.f32 %v79188, %v79132 (stack83)
        %v79196 = vadd.f32 %v79081, %v79192 (stack82)
        %v79200 = vmul.f32 %v79196, %v79047 (stack83)
        %v79204 = vsel /*vm=*/%vm79052, /*on_true_vy=*/%v79057, /*on_false_vx=*/%v79200 (stack72)
        %v79208 = vmul.f32 %v79204, 1.4140625 (stack83)
        %s79210 = scalar_lea.vmem %s280, 84 [#allocation0] (stack107)
        %v79211 = vpack.c.bf16 0.0, %v79208 (stack104)
        %79212 = vst [vmem:[%s79210] sm:$0xf] /*vst_source=*/%v79211 (stack105)
        %v79215 = vadd.s32 %v894, %v78751 (stack65)
        %s79217 = smul.u32 128, %s27 (stack66)
        %v79218 = vlaneseq (stack67)
        %v79219 = vand.u32 %v79218, 127 (stack68)
        %v79220 = vstv %s79217 (stack69)
        %v79221 = vadd.s32 %v79219, %v79220 (stack70)
        %v79225 = vadd.s32 %v79215, %v79221 (stack65)
        %vm79229 = vcmp.lt.u32.totalorder %v79225, %v79215 (stack71)
        %vm79234 = vcmp.lt.u32.totalorder %v79215, %v894 (stack71)
        %v79239 = vadd.s32 %v881, %v78734 (stack65)
        %v79243 = vadd.s32 %v79239, 1 (stack65)
        %v79247 = vsel /*vm=*/%vm79234, /*on_true_vy=*/%v79243, /*on_false_vx=*/%v79239 (stack72)
        %v79251 = vadd.s32 %v79247, 1 (stack65)
        %v79255 = vsel /*vm=*/%vm79229, /*on_true_vy=*/%v79251, /*on_false_vx=*/%v79247 (stack72)
        %v79260 = vadd.s32 %v79255, %v10 (stack65)
        %v79264 = vadd.s32 %v79225, %v9 (stack65)
        %v79268 = vadd.s32 %v79260, %v79264 (stack65)
        %v79270 = vshll.u32 %v79264, 13 (stack73)
        %v79271 = vshrl.u32 %v79264, 19 (stack74)
        %v79272 = vor.u32 %v79270, %v79271 (stack75)
        %v79273 = vxor.u32 %v79268, %v79272 (stack76)
        %v79276 = vadd.s32 %v79268, %v79273 (stack65)
        %v79278 = vshll.u32 %v79273, 15 (stack73)
        %v79279 = vshrl.u32 %v79273, 17 (stack74)
        %v79280 = vor.u32 %v79278, %v79279 (stack75)
        %v79281 = vxor.u32 %v79276, %v79280 (stack76)
        %v79284 = vadd.s32 %v79276, %v79281 (stack65)
        %v79286 = vshll.u32 %v79281, 26 (stack73)
        %v79287 = vshrl.u32 %v79281, 6 (stack74)
        %v79288 = vor.u32 %v79286, %v79287 (stack75)
        %v79289 = vxor.u32 %v79284, %v79288 (stack76)
        %v79292 = vadd.s32 %v79284, %v79289 (stack65)
        %v79296 = vadd.s32 %v79292, %v9 (stack65)
        %v79298 = vshll.u32 %v79289, 6 (stack73)
        %v79299 = vshrl.u32 %v79289, 26 (stack74)
        %v79300 = vor.u32 %v79298, %v79299 (stack75)
        %v79301 = vxor.u32 %v79292, %v79300 (stack76)
        %v79304 = vadd.s32 %v79301, %v8 (stack65)
        %v79308 = vadd.s32 %v79304, 1 (stack65)
        %v79312 = vadd.s32 %v79296, %v79308 (stack65)
        %v79314 = vshll.u32 %v79308, 17 (stack73)
        %v79315 = vshrl.u32 %v79308, 15 (stack74)
        %v79316 = vor.u32 %v79314, %v79315 (stack75)
        %v79317 = vxor.u32 %v79312, %v79316 (stack76)
        %v79320 = vadd.s32 %v79312, %v79317 (stack65)
        %v79322 = vshll.u32 %v79317, 29 (stack73)
        %v79323 = vshrl.u32 %v79317, 3 (stack74)
        %v79324 = vor.u32 %v79322, %v79323 (stack75)
        %v79325 = vxor.u32 %v79320, %v79324 (stack76)
        %v79328 = vadd.s32 %v79320, %v79325 (stack65)
        %v79330 = vshll.u32 %v79325, 16 (stack73)
        %v79331 = vshrl.u32 %v79325, 16 (stack74)
        %v79332 = vor.u32 %v79330, %v79331 (stack75)
        %v79333 = vxor.u32 %v79328, %v79332 (stack76)
        %v79336 = vadd.s32 %v79328, %v79333 (stack65)
        %v79340 = vadd.s32 %v79336, %v8 (stack65)
        %v79342 = vshll.u32 %v79333, 24 (stack73)
        %v79343 = vshrl.u32 %v79333, 8 (stack74)
        %v79344 = vor.u32 %v79342, %v79343 (stack75)
        %v79345 = vxor.u32 %v79336, %v79344 (stack76)
        %v79348 = vadd.s32 %v79345, %v10 (stack65)
        %v79352 = vadd.s32 %v79348, 2 (stack65)
        %v79356 = vadd.s32 %v79340, %v79352 (stack65)
        %v79358 = vshll.u32 %v79352, 13 (stack73)
        %v79359 = vshrl.u32 %v79352, 19 (stack74)
        %v79360 = vor.u32 %v79358, %v79359 (stack75)
        %v79361 = vxor.u32 %v79356, %v79360 (stack76)
        %v79364 = vadd.s32 %v79356, %v79361 (stack65)
        %v79366 = vshll.u32 %v79361, 15 (stack73)
        %v79367 = vshrl.u32 %v79361, 17 (stack74)
        %v79368 = vor.u32 %v79366, %v79367 (stack75)
        %v79369 = vxor.u32 %v79364, %v79368 (stack76)
        %v79372 = vadd.s32 %v79364, %v79369 (stack65)
        %v79374 = vshll.u32 %v79369, 26 (stack73)
        %v79375 = vshrl.u32 %v79369, 6 (stack74)
        %v79376 = vor.u32 %v79374, %v79375 (stack75)
        %v79377 = vxor.u32 %v79372, %v79376 (stack76)
        %v79380 = vadd.s32 %v79372, %v79377 (stack65)
        %v79384 = vadd.s32 %v79380, %v10 (stack65)
        %v79386 = vshll.u32 %v79377, 6 (stack73)
        %v79387 = vshrl.u32 %v79377, 26 (stack74)
        %v79388 = vor.u32 %v79386, %v79387 (stack75)
        %v79389 = vxor.u32 %v79380, %v79388 (stack76)
        %v79392 = vadd.s32 %v79389, %v9 (stack65)
        %v79396 = vadd.s32 %v79392, 3 (stack65)
        %v79400 = vadd.s32 %v79384, %v79396 (stack65)
        %v79402 = vshll.u32 %v79396, 17 (stack73)
        %v79403 = vshrl.u32 %v79396, 15 (stack74)
        %v79404 = vor.u32 %v79402, %v79403 (stack75)
        %v79405 = vxor.u32 %v79400, %v79404 (stack76)
        %v79408 = vadd.s32 %v79400, %v79405 (stack65)
        %v79410 = vshll.u32 %v79405, 29 (stack73)
        %v79411 = vshrl.u32 %v79405, 3 (stack74)
        %v79412 = vor.u32 %v79410, %v79411 (stack75)
        %v79413 = vxor.u32 %v79408, %v79412 (stack76)
        %v79416 = vadd.s32 %v79408, %v79413 (stack65)
        %v79418 = vshll.u32 %v79413, 16 (stack73)
        %v79419 = vshrl.u32 %v79413, 16 (stack74)
        %v79420 = vor.u32 %v79418, %v79419 (stack75)
        %v79421 = vxor.u32 %v79416, %v79420 (stack76)
        %v79424 = vadd.s32 %v79416, %v79421 (stack65)
        %v79428 = vadd.s32 %v79424, %v9 (stack65)
        %v79430 = vshll.u32 %v79421, 24 (stack73)
        %v79431 = vshrl.u32 %v79421, 8 (stack74)
        %v79432 = vor.u32 %v79430, %v79431 (stack75)
        %v79433 = vxor.u32 %v79424, %v79432 (stack76)
        %v79436 = vadd.s32 %v79433, %v8 (stack65)
        %v79440 = vadd.s32 %v79436, 4 (stack65)
        %v79444 = vadd.s32 %v79428, %v79440 (stack65)
        %v79446 = vshll.u32 %v79440, 13 (stack73)
        %v79447 = vshrl.u32 %v79440, 19 (stack74)
        %v79448 = vor.u32 %v79446, %v79447 (stack75)
        %v79449 = vxor.u32 %v79444, %v79448 (stack76)
        %v79452 = vadd.s32 %v79444, %v79449 (stack65)
        %v79454 = vshll.u32 %v79449, 15 (stack73)
        %v79455 = vshrl.u32 %v79449, 17 (stack74)
        %v79456 = vor.u32 %v79454, %v79455 (stack75)
        %v79457 = vxor.u32 %v79452, %v79456 (stack76)
        %v79460 = vadd.s32 %v79452, %v79457 (stack65)
        %v79462 = vshll.u32 %v79457, 26 (stack73)
        %v79463 = vshrl.u32 %v79457, 6 (stack74)
        %v79464 = vor.u32 %v79462, %v79463 (stack75)
        %v79465 = vxor.u32 %v79460, %v79464 (stack76)
        %v79468 = vadd.s32 %v79460, %v79465 (stack65)
        %v79472 = vadd.s32 %v79468, %v8 (stack65)
        %v79474 = vshll.u32 %v79465, 6 (stack73)
        %v79475 = vshrl.u32 %v79465, 26 (stack74)
        %v79476 = vor.u32 %v79474, %v79475 (stack75)
        %v79477 = vxor.u32 %v79468, %v79476 (stack76)
        %v79480 = vadd.s32 %v79477, %v10 (stack65)
        %v79484 = vadd.s32 %v79480, 5 (stack65)
        %v79486 = vxor.u32 %v79472, %v79484 (stack76)
        %v79487 = vand.u32.u8 %v79486, 255 (stack77)
        %v79488 = vand.u32 %v79487, 65535 (stack78)
        %v79489 = vshrl.u32 %v79488, 1 (stack79)
        %v79490 = vor.u32 %v79489, 16256 (stack75)
        %v79491 = vand.u32.u16 %v79490, 65535 (stack80)
        %v79492 = vunpack.i.l.bf16 %v79491 (stack81)
        %v79496 = vadd.f32 %v79492, -1.0 (stack82)
        %v79500 = vmul.f32 %v79496, 2.0 (stack83)
        %v79504 = vadd.f32 %v79500, -0.99609375 (stack82)
        %v79508 = vmax.f32 -0.99609375, %v79504 (stack84)
        %v79510 = vand.u32 2147483647, %v79508 (stack85)
        %vm79513 = vcmp.eq.f32.partialorder %v79510, 1.0 (stack86)
        %v79518 = vmul.f32 %v79508, inf (stack83)
        %v79520 = vxor.u32 %v79508, 2147483648 (stack87)
        %v79523 = vmul.f32 %v79508, %v79520 (stack83)
        %v79525 = vadd.f32 %v79523, 1.0 (stack88)
        %v79526 = vlog2.pop %v79525 (stack89)
        %v79527 = vmul.f32 %v79526, 0.6931472 (stack90)
        %v79528 = vmul.f32 -0.5, %v79523 (stack91)
        %v79529 = vadd.f32 %v79528, 1.0 (stack92)
        %v79530 = vmul.f32 %v79529, %v79523 (stack93)
        %v79531 = vand.u32 2147483647, %v79523 (stack94)
        %vm79532 = vcmp.lt.f32.partialorder %v79531, 0.0004427343 (stack95)
        %v79533 = vsel /*vm=*/%vm79532, /*on_true_vy=*/%v79530, /*on_false_vx=*/%v79527 (stack96)
        %v79534 = vxor.u32 %v79533, 2147483648 (stack87)
        %vm79537 = vcmp.lt.f32.partialorder %v79534, 5.0 (stack86)
        %v79542 = vsel /*vm=*/%vm79537, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v79546 = vsel /*vm=*/%vm79537, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v79550 = vsel /*vm=*/%vm79537, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v79554 = vsel /*vm=*/%vm79537, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v79558 = vsel /*vm=*/%vm79537, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v79562 = vsel /*vm=*/%vm79537, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v79566 = vsel /*vm=*/%vm79537, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v79570 = vsel /*vm=*/%vm79537, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v79574 = vsel /*vm=*/%vm79537, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v79578 = vadd.f32 %v79534, -2.5 (stack82)
        %v79580 = vrsqrt.pop %v79534 (stack97)
        %v79581 = vmul.f32 %v79534, %v79580 (stack98)
        %vm79582 = vcmp.eq.f32.partialorder %v79534, inf (stack99)
        %v79583 = vsel /*vm=*/%vm79582, /*on_true_vy=*/%v79534, /*on_false_vx=*/%v79581 (stack100)
        %vm79584 = vcmp.eq.f32.partialorder %v79534, 0.0 (stack101)
        %v79585 = vand.u32 %v79534, 2147483648 (stack102)
        %v79586 = vsel /*vm=*/%vm79584, /*on_true_vy=*/%v79585, /*on_false_vx=*/%v79583 (stack103)
        %v79589 = vadd.f32 %v79586, -3.0 (stack82)
        %v79593 = vsel /*vm=*/%vm79537, /*on_true_vy=*/%v79578, /*on_false_vx=*/%v79589 (stack72)
        %v79597 = vmul.f32 %v79574, %v79593 (stack83)
        %v79601 = vadd.f32 %v79570, %v79597 (stack82)
        %v79605 = vmul.f32 %v79601, %v79593 (stack83)
        %v79609 = vadd.f32 %v79566, %v79605 (stack82)
        %v79613 = vmul.f32 %v79609, %v79593 (stack83)
        %v79617 = vadd.f32 %v79562, %v79613 (stack82)
        %v79621 = vmul.f32 %v79617, %v79593 (stack83)
        %v79625 = vadd.f32 %v79558, %v79621 (stack82)
        %v79629 = vmul.f32 %v79625, %v79593 (stack83)
        %v79633 = vadd.f32 %v79554, %v79629 (stack82)
        %v79637 = vmul.f32 %v79633, %v79593 (stack83)
        %v79641 = vadd.f32 %v79550, %v79637 (stack82)
        %v79645 = vmul.f32 %v79641, %v79593 (stack83)
        %v79649 = vadd.f32 %v79546, %v79645 (stack82)
        %v79653 = vmul.f32 %v79649, %v79593 (stack83)
        %v79657 = vadd.f32 %v79542, %v79653 (stack82)
        %v79661 = vmul.f32 %v79657, %v79508 (stack83)
        %v79665 = vsel /*vm=*/%vm79513, /*on_true_vy=*/%v79518, /*on_false_vx=*/%v79661 (stack72)
        %v79669 = vmul.f32 %v79665, 1.4140625 (stack83)
        %s79671 = scalar_lea.vmem %s280, 212 [#allocation0] (stack107)
        %v79672 = vpack.c.bf16 0.0, %v79669 (stack104)
        %79673 = vst [vmem:[%s79671] sm:$0xf] /*vst_source=*/%v79672 (stack105)
        %v79676 = vadd.s32 %v1381, %v78751 (stack65)
        %s79678 = smul.u32 128, %s27 (stack66)
        %v79679 = vlaneseq (stack67)
        %v79680 = vand.u32 %v79679, 127 (stack68)
        %v79681 = vstv %s79678 (stack69)
        %v79682 = vadd.s32 %v79680, %v79681 (stack70)
        %v79686 = vadd.s32 %v79676, %v79682 (stack65)
        %vm79690 = vcmp.lt.u32.totalorder %v79686, %v79676 (stack71)
        %vm79695 = vcmp.lt.u32.totalorder %v79676, %v1381 (stack71)
        %v79700 = vadd.s32 %v1368, %v78734 (stack65)
        %v79704 = vadd.s32 %v79700, 1 (stack65)
        %v79708 = vsel /*vm=*/%vm79695, /*on_true_vy=*/%v79704, /*on_false_vx=*/%v79700 (stack72)
        %v79712 = vadd.s32 %v79708, 1 (stack65)
        %v79716 = vsel /*vm=*/%vm79690, /*on_true_vy=*/%v79712, /*on_false_vx=*/%v79708 (stack72)
        %v79721 = vadd.s32 %v79716, %v10 (stack65)
        %v79725 = vadd.s32 %v79686, %v9 (stack65)
        %v79729 = vadd.s32 %v79721, %v79725 (stack65)
        %v79731 = vshll.u32 %v79725, 13 (stack73)
        %v79732 = vshrl.u32 %v79725, 19 (stack74)
        %v79733 = vor.u32 %v79731, %v79732 (stack75)
        %v79734 = vxor.u32 %v79729, %v79733 (stack76)
        %v79737 = vadd.s32 %v79729, %v79734 (stack65)
        %v79739 = vshll.u32 %v79734, 15 (stack73)
        %v79740 = vshrl.u32 %v79734, 17 (stack74)
        %v79741 = vor.u32 %v79739, %v79740 (stack75)
        %v79742 = vxor.u32 %v79737, %v79741 (stack76)
        %v79745 = vadd.s32 %v79737, %v79742 (stack65)
        %v79747 = vshll.u32 %v79742, 26 (stack73)
        %v79748 = vshrl.u32 %v79742, 6 (stack74)
        %v79749 = vor.u32 %v79747, %v79748 (stack75)
        %v79750 = vxor.u32 %v79745, %v79749 (stack76)
        %v79753 = vadd.s32 %v79745, %v79750 (stack65)
        %v79757 = vadd.s32 %v79753, %v9 (stack65)
        %v79759 = vshll.u32 %v79750, 6 (stack73)
        %v79760 = vshrl.u32 %v79750, 26 (stack74)
        %v79761 = vor.u32 %v79759, %v79760 (stack75)
        %v79762 = vxor.u32 %v79753, %v79761 (stack76)
        %v79765 = vadd.s32 %v79762, %v8 (stack65)
        %v79769 = vadd.s32 %v79765, 1 (stack65)
        %v79773 = vadd.s32 %v79757, %v79769 (stack65)
        %v79775 = vshll.u32 %v79769, 17 (stack73)
        %v79776 = vshrl.u32 %v79769, 15 (stack74)
        %v79777 = vor.u32 %v79775, %v79776 (stack75)
        %v79778 = vxor.u32 %v79773, %v79777 (stack76)
        %v79781 = vadd.s32 %v79773, %v79778 (stack65)
        %v79783 = vshll.u32 %v79778, 29 (stack73)
        %v79784 = vshrl.u32 %v79778, 3 (stack74)
        %v79785 = vor.u32 %v79783, %v79784 (stack75)
        %v79786 = vxor.u32 %v79781, %v79785 (stack76)
        %v79789 = vadd.s32 %v79781, %v79786 (stack65)
        %v79791 = vshll.u32 %v79786, 16 (stack73)
        %v79792 = vshrl.u32 %v79786, 16 (stack74)
        %v79793 = vor.u32 %v79791, %v79792 (stack75)
        %v79794 = vxor.u32 %v79789, %v79793 (stack76)
        %v79797 = vadd.s32 %v79789, %v79794 (stack65)
        %v79801 = vadd.s32 %v79797, %v8 (stack65)
        %v79803 = vshll.u32 %v79794, 24 (stack73)
        %v79804 = vshrl.u32 %v79794, 8 (stack74)
        %v79805 = vor.u32 %v79803, %v79804 (stack75)
        %v79806 = vxor.u32 %v79797, %v79805 (stack76)
        %v79809 = vadd.s32 %v79806, %v10 (stack65)
        %v79813 = vadd.s32 %v79809, 2 (stack65)
        %v79817 = vadd.s32 %v79801, %v79813 (stack65)
        %v79819 = vshll.u32 %v79813, 13 (stack73)
        %v79820 = vshrl.u32 %v79813, 19 (stack74)
        %v79821 = vor.u32 %v79819, %v79820 (stack75)
        %v79822 = vxor.u32 %v79817, %v79821 (stack76)
        %v79825 = vadd.s32 %v79817, %v79822 (stack65)
        %v79827 = vshll.u32 %v79822, 15 (stack73)
        %v79828 = vshrl.u32 %v79822, 17 (stack74)
        %v79829 = vor.u32 %v79827, %v79828 (stack75)
        %v79830 = vxor.u32 %v79825, %v79829 (stack76)
        %v79833 = vadd.s32 %v79825, %v79830 (stack65)
        %v79835 = vshll.u32 %v79830, 26 (stack73)
        %v79836 = vshrl.u32 %v79830, 6 (stack74)
        %v79837 = vor.u32 %v79835, %v79836 (stack75)
        %v79838 = vxor.u32 %v79833, %v79837 (stack76)
        %v79841 = vadd.s32 %v79833, %v79838 (stack65)
        %v79845 = vadd.s32 %v79841, %v10 (stack65)
        %v79847 = vshll.u32 %v79838, 6 (stack73)
        %v79848 = vshrl.u32 %v79838, 26 (stack74)
        %v79849 = vor.u32 %v79847, %v79848 (stack75)
        %v79850 = vxor.u32 %v79841, %v79849 (stack76)
        %v79853 = vadd.s32 %v79850, %v9 (stack65)
        %v79857 = vadd.s32 %v79853, 3 (stack65)
        %v79861 = vadd.s32 %v79845, %v79857 (stack65)
        %v79863 = vshll.u32 %v79857, 17 (stack73)
        %v79864 = vshrl.u32 %v79857, 15 (stack74)
        %v79865 = vor.u32 %v79863, %v79864 (stack75)
        %v79866 = vxor.u32 %v79861, %v79865 (stack76)
        %v79869 = vadd.s32 %v79861, %v79866 (stack65)
        %v79871 = vshll.u32 %v79866, 29 (stack73)
        %v79872 = vshrl.u32 %v79866, 3 (stack74)
        %v79873 = vor.u32 %v79871, %v79872 (stack75)
        %v79874 = vxor.u32 %v79869, %v79873 (stack76)
        %v79877 = vadd.s32 %v79869, %v79874 (stack65)
        %v79879 = vshll.u32 %v79874, 16 (stack73)
        %v79880 = vshrl.u32 %v79874, 16 (stack74)
        %v79881 = vor.u32 %v79879, %v79880 (stack75)
        %v79882 = vxor.u32 %v79877, %v79881 (stack76)
        %v79885 = vadd.s32 %v79877, %v79882 (stack65)
        %v79889 = vadd.s32 %v79885, %v9 (stack65)
        %v79891 = vshll.u32 %v79882, 24 (stack73)
        %v79892 = vshrl.u32 %v79882, 8 (stack74)
        %v79893 = vor.u32 %v79891, %v79892 (stack75)
        %v79894 = vxor.u32 %v79885, %v79893 (stack76)
        %v79897 = vadd.s32 %v79894, %v8 (stack65)
        %v79901 = vadd.s32 %v79897, 4 (stack65)
        %v79905 = vadd.s32 %v79889, %v79901 (stack65)
        %v79907 = vshll.u32 %v79901, 13 (stack73)
        %v79908 = vshrl.u32 %v79901, 19 (stack74)
        %v79909 = vor.u32 %v79907, %v79908 (stack75)
        %v79910 = vxor.u32 %v79905, %v79909 (stack76)
        %v79913 = vadd.s32 %v79905, %v79910 (stack65)
        %v79915 = vshll.u32 %v79910, 15 (stack73)
        %v79916 = vshrl.u32 %v79910, 17 (stack74)
        %v79917 = vor.u32 %v79915, %v79916 (stack75)
        %v79918 = vxor.u32 %v79913, %v79917 (stack76)
        %v79921 = vadd.s32 %v79913, %v79918 (stack65)
        %v79923 = vshll.u32 %v79918, 26 (stack73)
        %v79924 = vshrl.u32 %v79918, 6 (stack74)
        %v79925 = vor.u32 %v79923, %v79924 (stack75)
        %v79926 = vxor.u32 %v79921, %v79925 (stack76)
        %v79929 = vadd.s32 %v79921, %v79926 (stack65)
        %v79933 = vadd.s32 %v79929, %v8 (stack65)
        %v79935 = vshll.u32 %v79926, 6 (stack73)
        %v79936 = vshrl.u32 %v79926, 26 (stack74)
        %v79937 = vor.u32 %v79935, %v79936 (stack75)
        %v79938 = vxor.u32 %v79929, %v79937 (stack76)
        %v79941 = vadd.s32 %v79938, %v10 (stack65)
        %v79945 = vadd.s32 %v79941, 5 (stack65)
        %v79947 = vxor.u32 %v79933, %v79945 (stack76)
        %v79948 = vand.u32.u8 %v79947, 255 (stack77)
        %v79949 = vand.u32 %v79948, 65535 (stack78)
        %v79950 = vshrl.u32 %v79949, 1 (stack79)
        %v79951 = vor.u32 %v79950, 16256 (stack75)
        %v79952 = vand.u32.u16 %v79951, 65535 (stack80)
        %v79953 = vunpack.i.l.bf16 %v79952 (stack81)
        %v79957 = vadd.f32 %v79953, -1.0 (stack82)
        %v79961 = vmul.f32 %v79957, 2.0 (stack83)
        %v79965 = vadd.f32 %v79961, -0.99609375 (stack82)
        %v79969 = vmax.f32 -0.99609375, %v79965 (stack84)
        %v79971 = vand.u32 2147483647, %v79969 (stack85)
        %vm79974 = vcmp.eq.f32.partialorder %v79971, 1.0 (stack86)
        %v79979 = vmul.f32 %v79969, inf (stack83)
        %v79981 = vxor.u32 %v79969, 2147483648 (stack87)
        %v79984 = vmul.f32 %v79969, %v79981 (stack83)
        %v79986 = vadd.f32 %v79984, 1.0 (stack88)
        %v79987 = vlog2.pop %v79986 (stack89)
        %v79988 = vmul.f32 %v79987, 0.6931472 (stack90)
        %v79989 = vmul.f32 -0.5, %v79984 (stack91)
        %v79990 = vadd.f32 %v79989, 1.0 (stack92)
        %v79991 = vmul.f32 %v79990, %v79984 (stack93)
        %v79992 = vand.u32 2147483647, %v79984 (stack94)
        %vm79993 = vcmp.lt.f32.partialorder %v79992, 0.0004427343 (stack95)
        %v79994 = vsel /*vm=*/%vm79993, /*on_true_vy=*/%v79991, /*on_false_vx=*/%v79988 (stack96)
        %v79995 = vxor.u32 %v79994, 2147483648 (stack87)
        %vm79998 = vcmp.lt.f32.partialorder %v79995, 5.0 (stack86)
        %v80003 = vsel /*vm=*/%vm79998, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v80007 = vsel /*vm=*/%vm79998, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v80011 = vsel /*vm=*/%vm79998, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v80015 = vsel /*vm=*/%vm79998, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v80019 = vsel /*vm=*/%vm79998, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v80023 = vsel /*vm=*/%vm79998, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v80027 = vsel /*vm=*/%vm79998, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v80031 = vsel /*vm=*/%vm79998, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v80035 = vsel /*vm=*/%vm79998, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v80039 = vadd.f32 %v79995, -2.5 (stack82)
        %v80041 = vrsqrt.pop %v79995 (stack97)
        %v80042 = vmul.f32 %v79995, %v80041 (stack98)
        %vm80043 = vcmp.eq.f32.partialorder %v79995, inf (stack99)
        %v80044 = vsel /*vm=*/%vm80043, /*on_true_vy=*/%v79995, /*on_false_vx=*/%v80042 (stack100)
        %vm80045 = vcmp.eq.f32.partialorder %v79995, 0.0 (stack101)
        %v80046 = vand.u32 %v79995, 2147483648 (stack102)
        %v80047 = vsel /*vm=*/%vm80045, /*on_true_vy=*/%v80046, /*on_false_vx=*/%v80044 (stack103)
        %v80050 = vadd.f32 %v80047, -3.0 (stack82)
        %v80054 = vsel /*vm=*/%vm79998, /*on_true_vy=*/%v80039, /*on_false_vx=*/%v80050 (stack72)
        %v80058 = vmul.f32 %v80035, %v80054 (stack83)
        %v80062 = vadd.f32 %v80031, %v80058 (stack82)
        %v80066 = vmul.f32 %v80062, %v80054 (stack83)
        %v80070 = vadd.f32 %v80027, %v80066 (stack82)
        %v80074 = vmul.f32 %v80070, %v80054 (stack83)
        %v80078 = vadd.f32 %v80023, %v80074 (stack82)
        %v80082 = vmul.f32 %v80078, %v80054 (stack83)
        %v80086 = vadd.f32 %v80019, %v80082 (stack82)
        %v80090 = vmul.f32 %v80086, %v80054 (stack83)
        %v80094 = vadd.f32 %v80015, %v80090 (stack82)
        %v80098 = vmul.f32 %v80094, %v80054 (stack83)
        %v80102 = vadd.f32 %v80011, %v80098 (stack82)
        %v80106 = vmul.f32 %v80102, %v80054 (stack83)
        %v80110 = vadd.f32 %v80007, %v80106 (stack82)
        %v80114 = vmul.f32 %v80110, %v80054 (stack83)
        %v80118 = vadd.f32 %v80003, %v80114 (stack82)
        %v80122 = vmul.f32 %v80118, %v79969 (stack83)
        %v80126 = vsel /*vm=*/%vm79974, /*on_true_vy=*/%v79979, /*on_false_vx=*/%v80122 (stack72)
        %v80130 = vmul.f32 %v80126, 1.4140625 (stack83)
        %s80132 = scalar_lea.vmem %s280, 340 [#allocation0] (stack107)
        %v80133 = vpack.c.bf16 0.0, %v80130 (stack104)
        %80134 = vst [vmem:[%s80132] sm:$0xf] /*vst_source=*/%v80133 (stack105)
        %v80137 = vadd.s32 %v1868, %v78751 (stack65)
        %s80139 = smul.u32 128, %s27 (stack66)
        %v80140 = vlaneseq (stack67)
        %v80141 = vand.u32 %v80140, 127 (stack68)
        %v80142 = vstv %s80139 (stack69)
        %v80143 = vadd.s32 %v80141, %v80142 (stack70)
        %v80147 = vadd.s32 %v80137, %v80143 (stack65)
        %vm80151 = vcmp.lt.u32.totalorder %v80147, %v80137 (stack71)
        %vm80156 = vcmp.lt.u32.totalorder %v80137, %v1868 (stack71)
        %v80161 = vadd.s32 %v1855, %v78734 (stack65)
        %v80165 = vadd.s32 %v80161, 1 (stack65)
        %v80169 = vsel /*vm=*/%vm80156, /*on_true_vy=*/%v80165, /*on_false_vx=*/%v80161 (stack72)
        %v80173 = vadd.s32 %v80169, 1 (stack65)
        %v80177 = vsel /*vm=*/%vm80151, /*on_true_vy=*/%v80173, /*on_false_vx=*/%v80169 (stack72)
        %v80182 = vadd.s32 %v80177, %v10 (stack65)
        %v80186 = vadd.s32 %v80147, %v9 (stack65)
        %v80190 = vadd.s32 %v80182, %v80186 (stack65)
        %v80192 = vshll.u32 %v80186, 13 (stack73)
        %v80193 = vshrl.u32 %v80186, 19 (stack74)
        %v80194 = vor.u32 %v80192, %v80193 (stack75)
        %v80195 = vxor.u32 %v80190, %v80194 (stack76)
        %v80198 = vadd.s32 %v80190, %v80195 (stack65)
        %v80200 = vshll.u32 %v80195, 15 (stack73)
        %v80201 = vshrl.u32 %v80195, 17 (stack74)
        %v80202 = vor.u32 %v80200, %v80201 (stack75)
        %v80203 = vxor.u32 %v80198, %v80202 (stack76)
        %v80206 = vadd.s32 %v80198, %v80203 (stack65)
        %v80208 = vshll.u32 %v80203, 26 (stack73)
        %v80209 = vshrl.u32 %v80203, 6 (stack74)
        %v80210 = vor.u32 %v80208, %v80209 (stack75)
        %v80211 = vxor.u32 %v80206, %v80210 (stack76)
        %v80214 = vadd.s32 %v80206, %v80211 (stack65)
        %v80218 = vadd.s32 %v80214, %v9 (stack65)
        %v80220 = vshll.u32 %v80211, 6 (stack73)
        %v80221 = vshrl.u32 %v80211, 26 (stack74)
        %v80222 = vor.u32 %v80220, %v80221 (stack75)
        %v80223 = vxor.u32 %v80214, %v80222 (stack76)
        %v80226 = vadd.s32 %v80223, %v8 (stack65)
        %v80230 = vadd.s32 %v80226, 1 (stack65)
        %v80234 = vadd.s32 %v80218, %v80230 (stack65)
        %v80236 = vshll.u32 %v80230, 17 (stack73)
        %v80237 = vshrl.u32 %v80230, 15 (stack74)
        %v80238 = vor.u32 %v80236, %v80237 (stack75)
        %v80239 = vxor.u32 %v80234, %v80238 (stack76)
        %v80242 = vadd.s32 %v80234, %v80239 (stack65)
        %v80244 = vshll.u32 %v80239, 29 (stack73)
        %v80245 = vshrl.u32 %v80239, 3 (stack74)
        %v80246 = vor.u32 %v80244, %v80245 (stack75)
        %v80247 = vxor.u32 %v80242, %v80246 (stack76)
        %v80250 = vadd.s32 %v80242, %v80247 (stack65)
        %v80252 = vshll.u32 %v80247, 16 (stack73)
        %v80253 = vshrl.u32 %v80247, 16 (stack74)
        %v80254 = vor.u32 %v80252, %v80253 (stack75)
        %v80255 = vxor.u32 %v80250, %v80254 (stack76)
        %v80258 = vadd.s32 %v80250, %v80255 (stack65)
        %v80262 = vadd.s32 %v80258, %v8 (stack65)
        %v80264 = vshll.u32 %v80255, 24 (stack73)
        %v80265 = vshrl.u32 %v80255, 8 (stack74)
        %v80266 = vor.u32 %v80264, %v80265 (stack75)
        %v80267 = vxor.u32 %v80258, %v80266 (stack76)
        %v80270 = vadd.s32 %v80267, %v10 (stack65)
        %v80274 = vadd.s32 %v80270, 2 (stack65)
        %v80278 = vadd.s32 %v80262, %v80274 (stack65)
        %v80280 = vshll.u32 %v80274, 13 (stack73)
        %v80281 = vshrl.u32 %v80274, 19 (stack74)
        %v80282 = vor.u32 %v80280, %v80281 (stack75)
        %v80283 = vxor.u32 %v80278, %v80282 (stack76)
        %v80286 = vadd.s32 %v80278, %v80283 (stack65)
        %v80288 = vshll.u32 %v80283, 15 (stack73)
        %v80289 = vshrl.u32 %v80283, 17 (stack74)
        %v80290 = vor.u32 %v80288, %v80289 (stack75)
        %v80291 = vxor.u32 %v80286, %v80290 (stack76)
        %v80294 = vadd.s32 %v80286, %v80291 (stack65)
        %v80296 = vshll.u32 %v80291, 26 (stack73)
        %v80297 = vshrl.u32 %v80291, 6 (stack74)
        %v80298 = vor.u32 %v80296, %v80297 (stack75)
        %v80299 = vxor.u32 %v80294, %v80298 (stack76)
        %v80302 = vadd.s32 %v80294, %v80299 (stack65)
        %v80306 = vadd.s32 %v80302, %v10 (stack65)
        %v80308 = vshll.u32 %v80299, 6 (stack73)
        %v80309 = vshrl.u32 %v80299, 26 (stack74)
        %v80310 = vor.u32 %v80308, %v80309 (stack75)
        %v80311 = vxor.u32 %v80302, %v80310 (stack76)
        %v80314 = vadd.s32 %v80311, %v9 (stack65)
        %v80318 = vadd.s32 %v80314, 3 (stack65)
        %v80322 = vadd.s32 %v80306, %v80318 (stack65)
        %v80324 = vshll.u32 %v80318, 17 (stack73)
        %v80325 = vshrl.u32 %v80318, 15 (stack74)
        %v80326 = vor.u32 %v80324, %v80325 (stack75)
        %v80327 = vxor.u32 %v80322, %v80326 (stack76)
        %v80330 = vadd.s32 %v80322, %v80327 (stack65)
        %v80332 = vshll.u32 %v80327, 29 (stack73)
        %v80333 = vshrl.u32 %v80327, 3 (stack74)
        %v80334 = vor.u32 %v80332, %v80333 (stack75)
        %v80335 = vxor.u32 %v80330, %v80334 (stack76)
        %v80338 = vadd.s32 %v80330, %v80335 (stack65)
        %v80340 = vshll.u32 %v80335, 16 (stack73)
        %v80341 = vshrl.u32 %v80335, 16 (stack74)
        %v80342 = vor.u32 %v80340, %v80341 (stack75)
        %v80343 = vxor.u32 %v80338, %v80342 (stack76)
        %v80346 = vadd.s32 %v80338, %v80343 (stack65)
        %v80350 = vadd.s32 %v80346, %v9 (stack65)
        %v80352 = vshll.u32 %v80343, 24 (stack73)
        %v80353 = vshrl.u32 %v80343, 8 (stack74)
        %v80354 = vor.u32 %v80352, %v80353 (stack75)
        %v80355 = vxor.u32 %v80346, %v80354 (stack76)
        %v80358 = vadd.s32 %v80355, %v8 (stack65)
        %v80362 = vadd.s32 %v80358, 4 (stack65)
        %v80366 = vadd.s32 %v80350, %v80362 (stack65)
        %v80368 = vshll.u32 %v80362, 13 (stack73)
        %v80369 = vshrl.u32 %v80362, 19 (stack74)
        %v80370 = vor.u32 %v80368, %v80369 (stack75)
        %v80371 = vxor.u32 %v80366, %v80370 (stack76)
        %v80374 = vadd.s32 %v80366, %v80371 (stack65)
        %v80376 = vshll.u32 %v80371, 15 (stack73)
        %v80377 = vshrl.u32 %v80371, 17 (stack74)
        %v80378 = vor.u32 %v80376, %v80377 (stack75)
        %v80379 = vxor.u32 %v80374, %v80378 (stack76)
        %v80382 = vadd.s32 %v80374, %v80379 (stack65)
        %v80384 = vshll.u32 %v80379, 26 (stack73)
        %v80385 = vshrl.u32 %v80379, 6 (stack74)
        %v80386 = vor.u32 %v80384, %v80385 (stack75)
        %v80387 = vxor.u32 %v80382, %v80386 (stack76)
        %v80390 = vadd.s32 %v80382, %v80387 (stack65)
        %v80394 = vadd.s32 %v80390, %v8 (stack65)
        %v80396 = vshll.u32 %v80387, 6 (stack73)
        %v80397 = vshrl.u32 %v80387, 26 (stack74)
        %v80398 = vor.u32 %v80396, %v80397 (stack75)
        %v80399 = vxor.u32 %v80390, %v80398 (stack76)
        %v80402 = vadd.s32 %v80399, %v10 (stack65)
        %v80406 = vadd.s32 %v80402, 5 (stack65)
        %v80408 = vxor.u32 %v80394, %v80406 (stack76)
        %v80409 = vand.u32.u8 %v80408, 255 (stack77)
        %v80410 = vand.u32 %v80409, 65535 (stack78)
        %v80411 = vshrl.u32 %v80410, 1 (stack79)
        %v80412 = vor.u32 %v80411, 16256 (stack75)
        %v80413 = vand.u32.u16 %v80412, 65535 (stack80)
        %v80414 = vunpack.i.l.bf16 %v80413 (stack81)
        %v80418 = vadd.f32 %v80414, -1.0 (stack82)
        %v80422 = vmul.f32 %v80418, 2.0 (stack83)
        %v80426 = vadd.f32 %v80422, -0.99609375 (stack82)
        %v80430 = vmax.f32 -0.99609375, %v80426 (stack84)
        %v80432 = vand.u32 2147483647, %v80430 (stack85)
        %vm80435 = vcmp.eq.f32.partialorder %v80432, 1.0 (stack86)
        %v80440 = vmul.f32 %v80430, inf (stack83)
        %v80442 = vxor.u32 %v80430, 2147483648 (stack87)
        %v80445 = vmul.f32 %v80430, %v80442 (stack83)
        %v80447 = vadd.f32 %v80445, 1.0 (stack88)
        %v80448 = vlog2.pop %v80447 (stack89)
        %v80449 = vmul.f32 %v80448, 0.6931472 (stack90)
        %v80450 = vmul.f32 -0.5, %v80445 (stack91)
        %v80451 = vadd.f32 %v80450, 1.0 (stack92)
        %v80452 = vmul.f32 %v80451, %v80445 (stack93)
        %v80453 = vand.u32 2147483647, %v80445 (stack94)
        %vm80454 = vcmp.lt.f32.partialorder %v80453, 0.0004427343 (stack95)
        %v80455 = vsel /*vm=*/%vm80454, /*on_true_vy=*/%v80452, /*on_false_vx=*/%v80449 (stack96)
        %v80456 = vxor.u32 %v80455, 2147483648 (stack87)
        %vm80459 = vcmp.lt.f32.partialorder %v80456, 5.0 (stack86)
        %v80464 = vsel /*vm=*/%vm80459, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v80468 = vsel /*vm=*/%vm80459, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v80472 = vsel /*vm=*/%vm80459, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v80476 = vsel /*vm=*/%vm80459, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v80480 = vsel /*vm=*/%vm80459, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v80484 = vsel /*vm=*/%vm80459, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v80488 = vsel /*vm=*/%vm80459, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v80492 = vsel /*vm=*/%vm80459, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v80496 = vsel /*vm=*/%vm80459, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v80500 = vadd.f32 %v80456, -2.5 (stack82)
        %v80502 = vrsqrt.pop %v80456 (stack97)
        %v80503 = vmul.f32 %v80456, %v80502 (stack98)
        %vm80504 = vcmp.eq.f32.partialorder %v80456, inf (stack99)
        %v80505 = vsel /*vm=*/%vm80504, /*on_true_vy=*/%v80456, /*on_false_vx=*/%v80503 (stack100)
        %vm80506 = vcmp.eq.f32.partialorder %v80456, 0.0 (stack101)
        %v80507 = vand.u32 %v80456, 2147483648 (stack102)
        %v80508 = vsel /*vm=*/%vm80506, /*on_true_vy=*/%v80507, /*on_false_vx=*/%v80505 (stack103)
        %v80511 = vadd.f32 %v80508, -3.0 (stack82)
        %v80515 = vsel /*vm=*/%vm80459, /*on_true_vy=*/%v80500, /*on_false_vx=*/%v80511 (stack72)
        %v80519 = vmul.f32 %v80496, %v80515 (stack83)
        %v80523 = vadd.f32 %v80492, %v80519 (stack82)
        %v80527 = vmul.f32 %v80523, %v80515 (stack83)
        %v80531 = vadd.f32 %v80488, %v80527 (stack82)
        %v80535 = vmul.f32 %v80531, %v80515 (stack83)
        %v80539 = vadd.f32 %v80484, %v80535 (stack82)
        %v80543 = vmul.f32 %v80539, %v80515 (stack83)
        %v80547 = vadd.f32 %v80480, %v80543 (stack82)
        %v80551 = vmul.f32 %v80547, %v80515 (stack83)
        %v80555 = vadd.f32 %v80476, %v80551 (stack82)
        %v80559 = vmul.f32 %v80555, %v80515 (stack83)
        %v80563 = vadd.f32 %v80472, %v80559 (stack82)
        %v80567 = vmul.f32 %v80563, %v80515 (stack83)
        %v80571 = vadd.f32 %v80468, %v80567 (stack82)
        %v80575 = vmul.f32 %v80571, %v80515 (stack83)
        %v80579 = vadd.f32 %v80464, %v80575 (stack82)
        %v80583 = vmul.f32 %v80579, %v80430 (stack83)
        %v80587 = vsel /*vm=*/%vm80435, /*on_true_vy=*/%v80440, /*on_false_vx=*/%v80583 (stack72)
        %v80591 = vmul.f32 %v80587, 1.4140625 (stack83)
        %s80593 = scalar_lea.vmem %s280, 468 [#allocation0] (stack107)
        %v80594 = vpack.c.bf16 0.0, %v80591 (stack104)
        %80595 = vst [vmem:[%s80593] sm:$0xf] /*vst_source=*/%v80594 (stack105)
        %v80598 = vadd.s32 %v2355, %v78751 (stack65)
        %s80600 = smul.u32 128, %s27 (stack66)
        %v80601 = vlaneseq (stack67)
        %v80602 = vand.u32 %v80601, 127 (stack68)
        %v80603 = vstv %s80600 (stack69)
        %v80604 = vadd.s32 %v80602, %v80603 (stack70)
        %v80608 = vadd.s32 %v80598, %v80604 (stack65)
        %vm80612 = vcmp.lt.u32.totalorder %v80608, %v80598 (stack71)
        %vm80617 = vcmp.lt.u32.totalorder %v80598, %v2355 (stack71)
        %v80622 = vadd.s32 %v2342, %v78734 (stack65)
        %v80626 = vadd.s32 %v80622, 1 (stack65)
        %v80630 = vsel /*vm=*/%vm80617, /*on_true_vy=*/%v80626, /*on_false_vx=*/%v80622 (stack72)
        %v80634 = vadd.s32 %v80630, 1 (stack65)
        %v80638 = vsel /*vm=*/%vm80612, /*on_true_vy=*/%v80634, /*on_false_vx=*/%v80630 (stack72)
        %v80643 = vadd.s32 %v80638, %v10 (stack65)
        %v80647 = vadd.s32 %v80608, %v9 (stack65)
        %v80651 = vadd.s32 %v80643, %v80647 (stack65)
        %v80653 = vshll.u32 %v80647, 13 (stack73)
        %v80654 = vshrl.u32 %v80647, 19 (stack74)
        %v80655 = vor.u32 %v80653, %v80654 (stack75)
        %v80656 = vxor.u32 %v80651, %v80655 (stack76)
        %v80659 = vadd.s32 %v80651, %v80656 (stack65)
        %v80661 = vshll.u32 %v80656, 15 (stack73)
        %v80662 = vshrl.u32 %v80656, 17 (stack74)
        %v80663 = vor.u32 %v80661, %v80662 (stack75)
        %v80664 = vxor.u32 %v80659, %v80663 (stack76)
        %v80667 = vadd.s32 %v80659, %v80664 (stack65)
        %v80669 = vshll.u32 %v80664, 26 (stack73)
        %v80670 = vshrl.u32 %v80664, 6 (stack74)
        %v80671 = vor.u32 %v80669, %v80670 (stack75)
        %v80672 = vxor.u32 %v80667, %v80671 (stack76)
        %v80675 = vadd.s32 %v80667, %v80672 (stack65)
        %v80679 = vadd.s32 %v80675, %v9 (stack65)
        %v80681 = vshll.u32 %v80672, 6 (stack73)
        %v80682 = vshrl.u32 %v80672, 26 (stack74)
        %v80683 = vor.u32 %v80681, %v80682 (stack75)
        %v80684 = vxor.u32 %v80675, %v80683 (stack76)
        %v80687 = vadd.s32 %v80684, %v8 (stack65)
        %v80691 = vadd.s32 %v80687, 1 (stack65)
        %v80695 = vadd.s32 %v80679, %v80691 (stack65)
        %v80697 = vshll.u32 %v80691, 17 (stack73)
        %v80698 = vshrl.u32 %v80691, 15 (stack74)
        %v80699 = vor.u32 %v80697, %v80698 (stack75)
        %v80700 = vxor.u32 %v80695, %v80699 (stack76)
        %v80703 = vadd.s32 %v80695, %v80700 (stack65)
        %v80705 = vshll.u32 %v80700, 29 (stack73)
        %v80706 = vshrl.u32 %v80700, 3 (stack74)
        %v80707 = vor.u32 %v80705, %v80706 (stack75)
        %v80708 = vxor.u32 %v80703, %v80707 (stack76)
        %v80711 = vadd.s32 %v80703, %v80708 (stack65)
        %v80713 = vshll.u32 %v80708, 16 (stack73)
        %v80714 = vshrl.u32 %v80708, 16 (stack74)
        %v80715 = vor.u32 %v80713, %v80714 (stack75)
        %v80716 = vxor.u32 %v80711, %v80715 (stack76)
        %v80719 = vadd.s32 %v80711, %v80716 (stack65)
        %v80723 = vadd.s32 %v80719, %v8 (stack65)
        %v80725 = vshll.u32 %v80716, 24 (stack73)
        %v80726 = vshrl.u32 %v80716, 8 (stack74)
        %v80727 = vor.u32 %v80725, %v80726 (stack75)
        %v80728 = vxor.u32 %v80719, %v80727 (stack76)
        %v80731 = vadd.s32 %v80728, %v10 (stack65)
        %v80735 = vadd.s32 %v80731, 2 (stack65)
        %v80739 = vadd.s32 %v80723, %v80735 (stack65)
        %v80741 = vshll.u32 %v80735, 13 (stack73)
        %v80742 = vshrl.u32 %v80735, 19 (stack74)
        %v80743 = vor.u32 %v80741, %v80742 (stack75)
        %v80744 = vxor.u32 %v80739, %v80743 (stack76)
        %v80747 = vadd.s32 %v80739, %v80744 (stack65)
        %v80749 = vshll.u32 %v80744, 15 (stack73)
        %v80750 = vshrl.u32 %v80744, 17 (stack74)
        %v80751 = vor.u32 %v80749, %v80750 (stack75)
        %v80752 = vxor.u32 %v80747, %v80751 (stack76)
        %v80755 = vadd.s32 %v80747, %v80752 (stack65)
        %v80757 = vshll.u32 %v80752, 26 (stack73)
        %v80758 = vshrl.u32 %v80752, 6 (stack74)
        %v80759 = vor.u32 %v80757, %v80758 (stack75)
        %v80760 = vxor.u32 %v80755, %v80759 (stack76)
        %v80763 = vadd.s32 %v80755, %v80760 (stack65)
        %v80767 = vadd.s32 %v80763, %v10 (stack65)
        %v80769 = vshll.u32 %v80760, 6 (stack73)
        %v80770 = vshrl.u32 %v80760, 26 (stack74)
        %v80771 = vor.u32 %v80769, %v80770 (stack75)
        %v80772 = vxor.u32 %v80763, %v80771 (stack76)
        %v80775 = vadd.s32 %v80772, %v9 (stack65)
        %v80779 = vadd.s32 %v80775, 3 (stack65)
        %v80783 = vadd.s32 %v80767, %v80779 (stack65)
        %v80785 = vshll.u32 %v80779, 17 (stack73)
        %v80786 = vshrl.u32 %v80779, 15 (stack74)
        %v80787 = vor.u32 %v80785, %v80786 (stack75)
        %v80788 = vxor.u32 %v80783, %v80787 (stack76)
        %v80791 = vadd.s32 %v80783, %v80788 (stack65)
        %v80793 = vshll.u32 %v80788, 29 (stack73)
        %v80794 = vshrl.u32 %v80788, 3 (stack74)
        %v80795 = vor.u32 %v80793, %v80794 (stack75)
        %v80796 = vxor.u32 %v80791, %v80795 (stack76)
        %v80799 = vadd.s32 %v80791, %v80796 (stack65)
        %v80801 = vshll.u32 %v80796, 16 (stack73)
        %v80802 = vshrl.u32 %v80796, 16 (stack74)
        %v80803 = vor.u32 %v80801, %v80802 (stack75)
        %v80804 = vxor.u32 %v80799, %v80803 (stack76)
        %v80807 = vadd.s32 %v80799, %v80804 (stack65)
        %v80811 = vadd.s32 %v80807, %v9 (stack65)
        %v80813 = vshll.u32 %v80804, 24 (stack73)
        %v80814 = vshrl.u32 %v80804, 8 (stack74)
        %v80815 = vor.u32 %v80813, %v80814 (stack75)
        %v80816 = vxor.u32 %v80807, %v80815 (stack76)
        %v80819 = vadd.s32 %v80816, %v8 (stack65)
        %v80823 = vadd.s32 %v80819, 4 (stack65)
        %v80827 = vadd.s32 %v80811, %v80823 (stack65)
        %v80829 = vshll.u32 %v80823, 13 (stack73)
        %v80830 = vshrl.u32 %v80823, 19 (stack74)
        %v80831 = vor.u32 %v80829, %v80830 (stack75)
        %v80832 = vxor.u32 %v80827, %v80831 (stack76)
        %v80835 = vadd.s32 %v80827, %v80832 (stack65)
        %v80837 = vshll.u32 %v80832, 15 (stack73)
        %v80838 = vshrl.u32 %v80832, 17 (stack74)
        %v80839 = vor.u32 %v80837, %v80838 (stack75)
        %v80840 = vxor.u32 %v80835, %v80839 (stack76)
        %v80843 = vadd.s32 %v80835, %v80840 (stack65)
        %v80845 = vshll.u32 %v80840, 26 (stack73)
        %v80846 = vshrl.u32 %v80840, 6 (stack74)
        %v80847 = vor.u32 %v80845, %v80846 (stack75)
        %v80848 = vxor.u32 %v80843, %v80847 (stack76)
        %v80851 = vadd.s32 %v80843, %v80848 (stack65)
        %v80855 = vadd.s32 %v80851, %v8 (stack65)
        %v80857 = vshll.u32 %v80848, 6 (stack73)
        %v80858 = vshrl.u32 %v80848, 26 (stack74)
        %v80859 = vor.u32 %v80857, %v80858 (stack75)
        %v80860 = vxor.u32 %v80851, %v80859 (stack76)
        %v80863 = vadd.s32 %v80860, %v10 (stack65)
        %v80867 = vadd.s32 %v80863, 5 (stack65)
        %v80869 = vxor.u32 %v80855, %v80867 (stack76)
        %v80870 = vand.u32.u8 %v80869, 255 (stack77)
        %v80871 = vand.u32 %v80870, 65535 (stack78)
        %v80872 = vshrl.u32 %v80871, 1 (stack79)
        %v80873 = vor.u32 %v80872, 16256 (stack75)
        %v80874 = vand.u32.u16 %v80873, 65535 (stack80)
        %v80875 = vunpack.i.l.bf16 %v80874 (stack81)
        %v80879 = vadd.f32 %v80875, -1.0 (stack82)
        %v80883 = vmul.f32 %v80879, 2.0 (stack83)
        %v80887 = vadd.f32 %v80883, -0.99609375 (stack82)
        %v80891 = vmax.f32 -0.99609375, %v80887 (stack84)
        %v80893 = vand.u32 2147483647, %v80891 (stack85)
        %vm80896 = vcmp.eq.f32.partialorder %v80893, 1.0 (stack86)
        %v80901 = vmul.f32 %v80891, inf (stack83)
        %v80903 = vxor.u32 %v80891, 2147483648 (stack87)
        %v80906 = vmul.f32 %v80891, %v80903 (stack83)
        %v80908 = vadd.f32 %v80906, 1.0 (stack88)
        %v80909 = vlog2.pop %v80908 (stack89)
        %v80910 = vmul.f32 %v80909, 0.6931472 (stack90)
        %v80911 = vmul.f32 -0.5, %v80906 (stack91)
        %v80912 = vadd.f32 %v80911, 1.0 (stack92)
        %v80913 = vmul.f32 %v80912, %v80906 (stack93)
        %v80914 = vand.u32 2147483647, %v80906 (stack94)
        %vm80915 = vcmp.lt.f32.partialorder %v80914, 0.0004427343 (stack95)
        %v80916 = vsel /*vm=*/%vm80915, /*on_true_vy=*/%v80913, /*on_false_vx=*/%v80910 (stack96)
        %v80917 = vxor.u32 %v80916, 2147483648 (stack87)
        %vm80920 = vcmp.lt.f32.partialorder %v80917, 5.0 (stack86)
        %v80925 = vsel /*vm=*/%vm80920, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v80929 = vsel /*vm=*/%vm80920, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v80933 = vsel /*vm=*/%vm80920, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v80937 = vsel /*vm=*/%vm80920, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v80941 = vsel /*vm=*/%vm80920, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v80945 = vsel /*vm=*/%vm80920, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v80949 = vsel /*vm=*/%vm80920, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v80953 = vsel /*vm=*/%vm80920, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v80957 = vsel /*vm=*/%vm80920, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v80961 = vadd.f32 %v80917, -2.5 (stack82)
        %v80963 = vrsqrt.pop %v80917 (stack97)
        %v80964 = vmul.f32 %v80917, %v80963 (stack98)
        %vm80965 = vcmp.eq.f32.partialorder %v80917, inf (stack99)
        %v80966 = vsel /*vm=*/%vm80965, /*on_true_vy=*/%v80917, /*on_false_vx=*/%v80964 (stack100)
        %vm80967 = vcmp.eq.f32.partialorder %v80917, 0.0 (stack101)
        %v80968 = vand.u32 %v80917, 2147483648 (stack102)
        %v80969 = vsel /*vm=*/%vm80967, /*on_true_vy=*/%v80968, /*on_false_vx=*/%v80966 (stack103)
        %v80972 = vadd.f32 %v80969, -3.0 (stack82)
        %v80976 = vsel /*vm=*/%vm80920, /*on_true_vy=*/%v80961, /*on_false_vx=*/%v80972 (stack72)
        %v80980 = vmul.f32 %v80957, %v80976 (stack83)
        %v80984 = vadd.f32 %v80953, %v80980 (stack82)
        %v80988 = vmul.f32 %v80984, %v80976 (stack83)
        %v80992 = vadd.f32 %v80949, %v80988 (stack82)
        %v80996 = vmul.f32 %v80992, %v80976 (stack83)
        %v81000 = vadd.f32 %v80945, %v80996 (stack82)
        %v81004 = vmul.f32 %v81000, %v80976 (stack83)
        %v81008 = vadd.f32 %v80941, %v81004 (stack82)
        %v81012 = vmul.f32 %v81008, %v80976 (stack83)
        %v81016 = vadd.f32 %v80937, %v81012 (stack82)
        %v81020 = vmul.f32 %v81016, %v80976 (stack83)
        %v81024 = vadd.f32 %v80933, %v81020 (stack82)
        %v81028 = vmul.f32 %v81024, %v80976 (stack83)
        %v81032 = vadd.f32 %v80929, %v81028 (stack82)
        %v81036 = vmul.f32 %v81032, %v80976 (stack83)
        %v81040 = vadd.f32 %v80925, %v81036 (stack82)
        %v81044 = vmul.f32 %v81040, %v80891 (stack83)
        %v81048 = vsel /*vm=*/%vm80896, /*on_true_vy=*/%v80901, /*on_false_vx=*/%v81044 (stack72)
        %v81052 = vmul.f32 %v81048, 1.4140625 (stack83)
        %s81054 = scalar_lea.vmem %s280, 596 [#allocation0] (stack107)
        %v81055 = vpack.c.bf16 0.0, %v81052 (stack104)
        %81056 = vst [vmem:[%s81054] sm:$0xf] /*vst_source=*/%v81055 (stack105)
        %v81059 = vadd.s32 %v2842, %v78751 (stack65)
        %s81061 = smul.u32 128, %s27 (stack66)
        %v81062 = vlaneseq (stack67)
        %v81063 = vand.u32 %v81062, 127 (stack68)
        %v81064 = vstv %s81061 (stack69)
        %v81065 = vadd.s32 %v81063, %v81064 (stack70)
        %v81069 = vadd.s32 %v81059, %v81065 (stack65)
        %vm81073 = vcmp.lt.u32.totalorder %v81069, %v81059 (stack71)
        %vm81078 = vcmp.lt.u32.totalorder %v81059, %v2842 (stack71)
        %v81083 = vadd.s32 %v2829, %v78734 (stack65)
        %v81087 = vadd.s32 %v81083, 1 (stack65)
        %v81091 = vsel /*vm=*/%vm81078, /*on_true_vy=*/%v81087, /*on_false_vx=*/%v81083 (stack72)
        %v81095 = vadd.s32 %v81091, 1 (stack65)
        %v81099 = vsel /*vm=*/%vm81073, /*on_true_vy=*/%v81095, /*on_false_vx=*/%v81091 (stack72)
        %v81104 = vadd.s32 %v81099, %v10 (stack65)
        %v81108 = vadd.s32 %v81069, %v9 (stack65)
        %v81112 = vadd.s32 %v81104, %v81108 (stack65)
        %v81114 = vshll.u32 %v81108, 13 (stack73)
        %v81115 = vshrl.u32 %v81108, 19 (stack74)
        %v81116 = vor.u32 %v81114, %v81115 (stack75)
        %v81117 = vxor.u32 %v81112, %v81116 (stack76)
        %v81120 = vadd.s32 %v81112, %v81117 (stack65)
        %v81122 = vshll.u32 %v81117, 15 (stack73)
        %v81123 = vshrl.u32 %v81117, 17 (stack74)
        %v81124 = vor.u32 %v81122, %v81123 (stack75)
        %v81125 = vxor.u32 %v81120, %v81124 (stack76)
        %v81128 = vadd.s32 %v81120, %v81125 (stack65)
        %v81130 = vshll.u32 %v81125, 26 (stack73)
        %v81131 = vshrl.u32 %v81125, 6 (stack74)
        %v81132 = vor.u32 %v81130, %v81131 (stack75)
        %v81133 = vxor.u32 %v81128, %v81132 (stack76)
        %v81136 = vadd.s32 %v81128, %v81133 (stack65)
        %v81140 = vadd.s32 %v81136, %v9 (stack65)
        %v81142 = vshll.u32 %v81133, 6 (stack73)
        %v81143 = vshrl.u32 %v81133, 26 (stack74)
        %v81144 = vor.u32 %v81142, %v81143 (stack75)
        %v81145 = vxor.u32 %v81136, %v81144 (stack76)
        %v81148 = vadd.s32 %v81145, %v8 (stack65)
        %v81152 = vadd.s32 %v81148, 1 (stack65)
        %v81156 = vadd.s32 %v81140, %v81152 (stack65)
        %v81158 = vshll.u32 %v81152, 17 (stack73)
        %v81159 = vshrl.u32 %v81152, 15 (stack74)
        %v81160 = vor.u32 %v81158, %v81159 (stack75)
        %v81161 = vxor.u32 %v81156, %v81160 (stack76)
        %v81164 = vadd.s32 %v81156, %v81161 (stack65)
        %v81166 = vshll.u32 %v81161, 29 (stack73)
        %v81167 = vshrl.u32 %v81161, 3 (stack74)
        %v81168 = vor.u32 %v81166, %v81167 (stack75)
        %v81169 = vxor.u32 %v81164, %v81168 (stack76)
        %v81172 = vadd.s32 %v81164, %v81169 (stack65)
        %v81174 = vshll.u32 %v81169, 16 (stack73)
        %v81175 = vshrl.u32 %v81169, 16 (stack74)
        %v81176 = vor.u32 %v81174, %v81175 (stack75)
        %v81177 = vxor.u32 %v81172, %v81176 (stack76)
        %v81180 = vadd.s32 %v81172, %v81177 (stack65)
        %v81184 = vadd.s32 %v81180, %v8 (stack65)
        %v81186 = vshll.u32 %v81177, 24 (stack73)
        %v81187 = vshrl.u32 %v81177, 8 (stack74)
        %v81188 = vor.u32 %v81186, %v81187 (stack75)
        %v81189 = vxor.u32 %v81180, %v81188 (stack76)
        %v81192 = vadd.s32 %v81189, %v10 (stack65)
        %v81196 = vadd.s32 %v81192, 2 (stack65)
        %v81200 = vadd.s32 %v81184, %v81196 (stack65)
        %v81202 = vshll.u32 %v81196, 13 (stack73)
        %v81203 = vshrl.u32 %v81196, 19 (stack74)
        %v81204 = vor.u32 %v81202, %v81203 (stack75)
        %v81205 = vxor.u32 %v81200, %v81204 (stack76)
        %v81208 = vadd.s32 %v81200, %v81205 (stack65)
        %v81210 = vshll.u32 %v81205, 15 (stack73)
        %v81211 = vshrl.u32 %v81205, 17 (stack74)
        %v81212 = vor.u32 %v81210, %v81211 (stack75)
        %v81213 = vxor.u32 %v81208, %v81212 (stack76)
        %v81216 = vadd.s32 %v81208, %v81213 (stack65)
        %v81218 = vshll.u32 %v81213, 26 (stack73)
        %v81219 = vshrl.u32 %v81213, 6 (stack74)
        %v81220 = vor.u32 %v81218, %v81219 (stack75)
        %v81221 = vxor.u32 %v81216, %v81220 (stack76)
        %v81224 = vadd.s32 %v81216, %v81221 (stack65)
        %v81228 = vadd.s32 %v81224, %v10 (stack65)
        %v81230 = vshll.u32 %v81221, 6 (stack73)
        %v81231 = vshrl.u32 %v81221, 26 (stack74)
        %v81232 = vor.u32 %v81230, %v81231 (stack75)
        %v81233 = vxor.u32 %v81224, %v81232 (stack76)
        %v81236 = vadd.s32 %v81233, %v9 (stack65)
        %v81240 = vadd.s32 %v81236, 3 (stack65)
        %v81244 = vadd.s32 %v81228, %v81240 (stack65)
        %v81246 = vshll.u32 %v81240, 17 (stack73)
        %v81247 = vshrl.u32 %v81240, 15 (stack74)
        %v81248 = vor.u32 %v81246, %v81247 (stack75)
        %v81249 = vxor.u32 %v81244, %v81248 (stack76)
        %v81252 = vadd.s32 %v81244, %v81249 (stack65)
        %v81254 = vshll.u32 %v81249, 29 (stack73)
        %v81255 = vshrl.u32 %v81249, 3 (stack74)
        %v81256 = vor.u32 %v81254, %v81255 (stack75)
        %v81257 = vxor.u32 %v81252, %v81256 (stack76)
        %v81260 = vadd.s32 %v81252, %v81257 (stack65)
        %v81262 = vshll.u32 %v81257, 16 (stack73)
        %v81263 = vshrl.u32 %v81257, 16 (stack74)
        %v81264 = vor.u32 %v81262, %v81263 (stack75)
        %v81265 = vxor.u32 %v81260, %v81264 (stack76)
        %v81268 = vadd.s32 %v81260, %v81265 (stack65)
        %v81272 = vadd.s32 %v81268, %v9 (stack65)
        %v81274 = vshll.u32 %v81265, 24 (stack73)
        %v81275 = vshrl.u32 %v81265, 8 (stack74)
        %v81276 = vor.u32 %v81274, %v81275 (stack75)
        %v81277 = vxor.u32 %v81268, %v81276 (stack76)
        %v81280 = vadd.s32 %v81277, %v8 (stack65)
        %v81284 = vadd.s32 %v81280, 4 (stack65)
        %v81288 = vadd.s32 %v81272, %v81284 (stack65)
        %v81290 = vshll.u32 %v81284, 13 (stack73)
        %v81291 = vshrl.u32 %v81284, 19 (stack74)
        %v81292 = vor.u32 %v81290, %v81291 (stack75)
        %v81293 = vxor.u32 %v81288, %v81292 (stack76)
        %v81296 = vadd.s32 %v81288, %v81293 (stack65)
        %v81298 = vshll.u32 %v81293, 15 (stack73)
        %v81299 = vshrl.u32 %v81293, 17 (stack74)
        %v81300 = vor.u32 %v81298, %v81299 (stack75)
        %v81301 = vxor.u32 %v81296, %v81300 (stack76)
        %v81304 = vadd.s32 %v81296, %v81301 (stack65)
        %v81306 = vshll.u32 %v81301, 26 (stack73)
        %v81307 = vshrl.u32 %v81301, 6 (stack74)
        %v81308 = vor.u32 %v81306, %v81307 (stack75)
        %v81309 = vxor.u32 %v81304, %v81308 (stack76)
        %v81312 = vadd.s32 %v81304, %v81309 (stack65)
        %v81316 = vadd.s32 %v81312, %v8 (stack65)
        %v81318 = vshll.u32 %v81309, 6 (stack73)
        %v81319 = vshrl.u32 %v81309, 26 (stack74)
        %v81320 = vor.u32 %v81318, %v81319 (stack75)
        %v81321 = vxor.u32 %v81312, %v81320 (stack76)
        %v81324 = vadd.s32 %v81321, %v10 (stack65)
        %v81328 = vadd.s32 %v81324, 5 (stack65)
        %v81330 = vxor.u32 %v81316, %v81328 (stack76)
        %v81331 = vand.u32.u8 %v81330, 255 (stack77)
        %v81332 = vand.u32 %v81331, 65535 (stack78)
        %v81333 = vshrl.u32 %v81332, 1 (stack79)
        %v81334 = vor.u32 %v81333, 16256 (stack75)
        %v81335 = vand.u32.u16 %v81334, 65535 (stack80)
        %v81336 = vunpack.i.l.bf16 %v81335 (stack81)
        %v81340 = vadd.f32 %v81336, -1.0 (stack82)
        %v81344 = vmul.f32 %v81340, 2.0 (stack83)
        %v81348 = vadd.f32 %v81344, -0.99609375 (stack82)
        %v81352 = vmax.f32 -0.99609375, %v81348 (stack84)
        %v81354 = vand.u32 2147483647, %v81352 (stack85)
        %vm81357 = vcmp.eq.f32.partialorder %v81354, 1.0 (stack86)
        %v81362 = vmul.f32 %v81352, inf (stack83)
        %v81364 = vxor.u32 %v81352, 2147483648 (stack87)
        %v81367 = vmul.f32 %v81352, %v81364 (stack83)
        %v81369 = vadd.f32 %v81367, 1.0 (stack88)
        %v81370 = vlog2.pop %v81369 (stack89)
        %v81371 = vmul.f32 %v81370, 0.6931472 (stack90)
        %v81372 = vmul.f32 -0.5, %v81367 (stack91)
        %v81373 = vadd.f32 %v81372, 1.0 (stack92)
        %v81374 = vmul.f32 %v81373, %v81367 (stack93)
        %v81375 = vand.u32 2147483647, %v81367 (stack94)
        %vm81376 = vcmp.lt.f32.partialorder %v81375, 0.0004427343 (stack95)
        %v81377 = vsel /*vm=*/%vm81376, /*on_true_vy=*/%v81374, /*on_false_vx=*/%v81371 (stack96)
        %v81378 = vxor.u32 %v81377, 2147483648 (stack87)
        %vm81381 = vcmp.lt.f32.partialorder %v81378, 5.0 (stack86)
        %v81386 = vsel /*vm=*/%vm81381, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v81390 = vsel /*vm=*/%vm81381, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v81394 = vsel /*vm=*/%vm81381, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v81398 = vsel /*vm=*/%vm81381, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v81402 = vsel /*vm=*/%vm81381, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v81406 = vsel /*vm=*/%vm81381, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v81410 = vsel /*vm=*/%vm81381, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v81414 = vsel /*vm=*/%vm81381, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v81418 = vsel /*vm=*/%vm81381, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v81422 = vadd.f32 %v81378, -2.5 (stack82)
        %v81424 = vrsqrt.pop %v81378 (stack97)
        %v81425 = vmul.f32 %v81378, %v81424 (stack98)
        %vm81426 = vcmp.eq.f32.partialorder %v81378, inf (stack99)
        %v81427 = vsel /*vm=*/%vm81426, /*on_true_vy=*/%v81378, /*on_false_vx=*/%v81425 (stack100)
        %vm81428 = vcmp.eq.f32.partialorder %v81378, 0.0 (stack101)
        %v81429 = vand.u32 %v81378, 2147483648 (stack102)
        %v81430 = vsel /*vm=*/%vm81428, /*on_true_vy=*/%v81429, /*on_false_vx=*/%v81427 (stack103)
        %v81433 = vadd.f32 %v81430, -3.0 (stack82)
        %v81437 = vsel /*vm=*/%vm81381, /*on_true_vy=*/%v81422, /*on_false_vx=*/%v81433 (stack72)
        %v81441 = vmul.f32 %v81418, %v81437 (stack83)
        %v81445 = vadd.f32 %v81414, %v81441 (stack82)
        %v81449 = vmul.f32 %v81445, %v81437 (stack83)
        %v81453 = vadd.f32 %v81410, %v81449 (stack82)
        %v81457 = vmul.f32 %v81453, %v81437 (stack83)
        %v81461 = vadd.f32 %v81406, %v81457 (stack82)
        %v81465 = vmul.f32 %v81461, %v81437 (stack83)
        %v81469 = vadd.f32 %v81402, %v81465 (stack82)
        %v81473 = vmul.f32 %v81469, %v81437 (stack83)
        %v81477 = vadd.f32 %v81398, %v81473 (stack82)
        %v81481 = vmul.f32 %v81477, %v81437 (stack83)
        %v81485 = vadd.f32 %v81394, %v81481 (stack82)
        %v81489 = vmul.f32 %v81485, %v81437 (stack83)
        %v81493 = vadd.f32 %v81390, %v81489 (stack82)
        %v81497 = vmul.f32 %v81493, %v81437 (stack83)
        %v81501 = vadd.f32 %v81386, %v81497 (stack82)
        %v81505 = vmul.f32 %v81501, %v81352 (stack83)
        %v81509 = vsel /*vm=*/%vm81357, /*on_true_vy=*/%v81362, /*on_false_vx=*/%v81505 (stack72)
        %v81513 = vmul.f32 %v81509, 1.4140625 (stack83)
        %s81515 = scalar_lea.vmem %s280, 724 [#allocation0] (stack107)
        %v81516 = vpack.c.bf16 0.0, %v81513 (stack104)
        %81517 = vst [vmem:[%s81515] sm:$0xf] /*vst_source=*/%v81516 (stack105)
        %v81520 = vadd.s32 %v3329, %v78751 (stack65)
        %s81522 = smul.u32 128, %s27 (stack66)
        %v81523 = vlaneseq (stack67)
        %v81524 = vand.u32 %v81523, 127 (stack68)
        %v81525 = vstv %s81522 (stack69)
        %v81526 = vadd.s32 %v81524, %v81525 (stack70)
        %v81530 = vadd.s32 %v81520, %v81526 (stack65)
        %vm81534 = vcmp.lt.u32.totalorder %v81530, %v81520 (stack71)
        %vm81539 = vcmp.lt.u32.totalorder %v81520, %v3329 (stack71)
        %v81544 = vadd.s32 %v3316, %v78734 (stack65)
        %v81548 = vadd.s32 %v81544, 1 (stack65)
        %v81552 = vsel /*vm=*/%vm81539, /*on_true_vy=*/%v81548, /*on_false_vx=*/%v81544 (stack72)
        %v81556 = vadd.s32 %v81552, 1 (stack65)
        %v81560 = vsel /*vm=*/%vm81534, /*on_true_vy=*/%v81556, /*on_false_vx=*/%v81552 (stack72)
        %v81565 = vadd.s32 %v81560, %v10 (stack65)
        %v81569 = vadd.s32 %v81530, %v9 (stack65)
        %v81573 = vadd.s32 %v81565, %v81569 (stack65)
        %v81575 = vshll.u32 %v81569, 13 (stack73)
        %v81576 = vshrl.u32 %v81569, 19 (stack74)
        %v81577 = vor.u32 %v81575, %v81576 (stack75)
        %v81578 = vxor.u32 %v81573, %v81577 (stack76)
        %v81581 = vadd.s32 %v81573, %v81578 (stack65)
        %v81583 = vshll.u32 %v81578, 15 (stack73)
        %v81584 = vshrl.u32 %v81578, 17 (stack74)
        %v81585 = vor.u32 %v81583, %v81584 (stack75)
        %v81586 = vxor.u32 %v81581, %v81585 (stack76)
        %v81589 = vadd.s32 %v81581, %v81586 (stack65)
        %v81591 = vshll.u32 %v81586, 26 (stack73)
        %v81592 = vshrl.u32 %v81586, 6 (stack74)
        %v81593 = vor.u32 %v81591, %v81592 (stack75)
        %v81594 = vxor.u32 %v81589, %v81593 (stack76)
        %v81597 = vadd.s32 %v81589, %v81594 (stack65)
        %v81601 = vadd.s32 %v81597, %v9 (stack65)
        %v81603 = vshll.u32 %v81594, 6 (stack73)
        %v81604 = vshrl.u32 %v81594, 26 (stack74)
        %v81605 = vor.u32 %v81603, %v81604 (stack75)
        %v81606 = vxor.u32 %v81597, %v81605 (stack76)
        %v81609 = vadd.s32 %v81606, %v8 (stack65)
        %v81613 = vadd.s32 %v81609, 1 (stack65)
        %v81617 = vadd.s32 %v81601, %v81613 (stack65)
        %v81619 = vshll.u32 %v81613, 17 (stack73)
        %v81620 = vshrl.u32 %v81613, 15 (stack74)
        %v81621 = vor.u32 %v81619, %v81620 (stack75)
        %v81622 = vxor.u32 %v81617, %v81621 (stack76)
        %v81625 = vadd.s32 %v81617, %v81622 (stack65)
        %v81627 = vshll.u32 %v81622, 29 (stack73)
        %v81628 = vshrl.u32 %v81622, 3 (stack74)
        %v81629 = vor.u32 %v81627, %v81628 (stack75)
        %v81630 = vxor.u32 %v81625, %v81629 (stack76)
        %v81633 = vadd.s32 %v81625, %v81630 (stack65)
        %v81635 = vshll.u32 %v81630, 16 (stack73)
        %v81636 = vshrl.u32 %v81630, 16 (stack74)
        %v81637 = vor.u32 %v81635, %v81636 (stack75)
        %v81638 = vxor.u32 %v81633, %v81637 (stack76)
        %v81641 = vadd.s32 %v81633, %v81638 (stack65)
        %v81645 = vadd.s32 %v81641, %v8 (stack65)
        %v81647 = vshll.u32 %v81638, 24 (stack73)
        %v81648 = vshrl.u32 %v81638, 8 (stack74)
        %v81649 = vor.u32 %v81647, %v81648 (stack75)
        %v81650 = vxor.u32 %v81641, %v81649 (stack76)
        %v81653 = vadd.s32 %v81650, %v10 (stack65)
        %v81657 = vadd.s32 %v81653, 2 (stack65)
        %v81661 = vadd.s32 %v81645, %v81657 (stack65)
        %v81663 = vshll.u32 %v81657, 13 (stack73)
        %v81664 = vshrl.u32 %v81657, 19 (stack74)
        %v81665 = vor.u32 %v81663, %v81664 (stack75)
        %v81666 = vxor.u32 %v81661, %v81665 (stack76)
        %v81669 = vadd.s32 %v81661, %v81666 (stack65)
        %v81671 = vshll.u32 %v81666, 15 (stack73)
        %v81672 = vshrl.u32 %v81666, 17 (stack74)
        %v81673 = vor.u32 %v81671, %v81672 (stack75)
        %v81674 = vxor.u32 %v81669, %v81673 (stack76)
        %v81677 = vadd.s32 %v81669, %v81674 (stack65)
        %v81679 = vshll.u32 %v81674, 26 (stack73)
        %v81680 = vshrl.u32 %v81674, 6 (stack74)
        %v81681 = vor.u32 %v81679, %v81680 (stack75)
        %v81682 = vxor.u32 %v81677, %v81681 (stack76)
        %v81685 = vadd.s32 %v81677, %v81682 (stack65)
        %v81689 = vadd.s32 %v81685, %v10 (stack65)
        %v81691 = vshll.u32 %v81682, 6 (stack73)
        %v81692 = vshrl.u32 %v81682, 26 (stack74)
        %v81693 = vor.u32 %v81691, %v81692 (stack75)
        %v81694 = vxor.u32 %v81685, %v81693 (stack76)
        %v81697 = vadd.s32 %v81694, %v9 (stack65)
        %v81701 = vadd.s32 %v81697, 3 (stack65)
        %v81705 = vadd.s32 %v81689, %v81701 (stack65)
        %v81707 = vshll.u32 %v81701, 17 (stack73)
        %v81708 = vshrl.u32 %v81701, 15 (stack74)
        %v81709 = vor.u32 %v81707, %v81708 (stack75)
        %v81710 = vxor.u32 %v81705, %v81709 (stack76)
        %v81713 = vadd.s32 %v81705, %v81710 (stack65)
        %v81715 = vshll.u32 %v81710, 29 (stack73)
        %v81716 = vshrl.u32 %v81710, 3 (stack74)
        %v81717 = vor.u32 %v81715, %v81716 (stack75)
        %v81718 = vxor.u32 %v81713, %v81717 (stack76)
        %v81721 = vadd.s32 %v81713, %v81718 (stack65)
        %v81723 = vshll.u32 %v81718, 16 (stack73)
        %v81724 = vshrl.u32 %v81718, 16 (stack74)
        %v81725 = vor.u32 %v81723, %v81724 (stack75)
        %v81726 = vxor.u32 %v81721, %v81725 (stack76)
        %v81729 = vadd.s32 %v81721, %v81726 (stack65)
        %v81733 = vadd.s32 %v81729, %v9 (stack65)
        %v81735 = vshll.u32 %v81726, 24 (stack73)
        %v81736 = vshrl.u32 %v81726, 8 (stack74)
        %v81737 = vor.u32 %v81735, %v81736 (stack75)
        %v81738 = vxor.u32 %v81729, %v81737 (stack76)
        %v81741 = vadd.s32 %v81738, %v8 (stack65)
        %v81745 = vadd.s32 %v81741, 4 (stack65)
        %v81749 = vadd.s32 %v81733, %v81745 (stack65)
        %v81751 = vshll.u32 %v81745, 13 (stack73)
        %v81752 = vshrl.u32 %v81745, 19 (stack74)
        %v81753 = vor.u32 %v81751, %v81752 (stack75)
        %v81754 = vxor.u32 %v81749, %v81753 (stack76)
        %v81757 = vadd.s32 %v81749, %v81754 (stack65)
        %v81759 = vshll.u32 %v81754, 15 (stack73)
        %v81760 = vshrl.u32 %v81754, 17 (stack74)
        %v81761 = vor.u32 %v81759, %v81760 (stack75)
        %v81762 = vxor.u32 %v81757, %v81761 (stack76)
        %v81765 = vadd.s32 %v81757, %v81762 (stack65)
        %v81767 = vshll.u32 %v81762, 26 (stack73)
        %v81768 = vshrl.u32 %v81762, 6 (stack74)
        %v81769 = vor.u32 %v81767, %v81768 (stack75)
        %v81770 = vxor.u32 %v81765, %v81769 (stack76)
        %v81773 = vadd.s32 %v81765, %v81770 (stack65)
        %v81777 = vadd.s32 %v81773, %v8 (stack65)
        %v81779 = vshll.u32 %v81770, 6 (stack73)
        %v81780 = vshrl.u32 %v81770, 26 (stack74)
        %v81781 = vor.u32 %v81779, %v81780 (stack75)
        %v81782 = vxor.u32 %v81773, %v81781 (stack76)
        %v81785 = vadd.s32 %v81782, %v10 (stack65)
        %v81789 = vadd.s32 %v81785, 5 (stack65)
        %v81791 = vxor.u32 %v81777, %v81789 (stack76)
        %v81792 = vand.u32.u8 %v81791, 255 (stack77)
        %v81793 = vand.u32 %v81792, 65535 (stack78)
        %v81794 = vshrl.u32 %v81793, 1 (stack79)
        %v81795 = vor.u32 %v81794, 16256 (stack75)
        %v81796 = vand.u32.u16 %v81795, 65535 (stack80)
        %v81797 = vunpack.i.l.bf16 %v81796 (stack81)
        %v81801 = vadd.f32 %v81797, -1.0 (stack82)
        %v81805 = vmul.f32 %v81801, 2.0 (stack83)
        %v81809 = vadd.f32 %v81805, -0.99609375 (stack82)
        %v81813 = vmax.f32 -0.99609375, %v81809 (stack84)
        %v81815 = vand.u32 2147483647, %v81813 (stack85)
        %vm81818 = vcmp.eq.f32.partialorder %v81815, 1.0 (stack86)
        %v81823 = vmul.f32 %v81813, inf (stack83)
        %v81825 = vxor.u32 %v81813, 2147483648 (stack87)
        %v81828 = vmul.f32 %v81813, %v81825 (stack83)
        %v81830 = vadd.f32 %v81828, 1.0 (stack88)
        %v81831 = vlog2.pop %v81830 (stack89)
        %v81832 = vmul.f32 %v81831, 0.6931472 (stack90)
        %v81833 = vmul.f32 -0.5, %v81828 (stack91)
        %v81834 = vadd.f32 %v81833, 1.0 (stack92)
        %v81835 = vmul.f32 %v81834, %v81828 (stack93)
        %v81836 = vand.u32 2147483647, %v81828 (stack94)
        %vm81837 = vcmp.lt.f32.partialorder %v81836, 0.0004427343 (stack95)
        %v81838 = vsel /*vm=*/%vm81837, /*on_true_vy=*/%v81835, /*on_false_vx=*/%v81832 (stack96)
        %v81839 = vxor.u32 %v81838, 2147483648 (stack87)
        %vm81842 = vcmp.lt.f32.partialorder %v81839, 5.0 (stack86)
        %v81847 = vsel /*vm=*/%vm81842, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v81851 = vsel /*vm=*/%vm81842, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v81855 = vsel /*vm=*/%vm81842, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v81859 = vsel /*vm=*/%vm81842, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v81863 = vsel /*vm=*/%vm81842, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v81867 = vsel /*vm=*/%vm81842, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v81871 = vsel /*vm=*/%vm81842, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v81875 = vsel /*vm=*/%vm81842, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v81879 = vsel /*vm=*/%vm81842, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v81883 = vadd.f32 %v81839, -2.5 (stack82)
        %v81885 = vrsqrt.pop %v81839 (stack97)
        %v81886 = vmul.f32 %v81839, %v81885 (stack98)
        %vm81887 = vcmp.eq.f32.partialorder %v81839, inf (stack99)
        %v81888 = vsel /*vm=*/%vm81887, /*on_true_vy=*/%v81839, /*on_false_vx=*/%v81886 (stack100)
        %vm81889 = vcmp.eq.f32.partialorder %v81839, 0.0 (stack101)
        %v81890 = vand.u32 %v81839, 2147483648 (stack102)
        %v81891 = vsel /*vm=*/%vm81889, /*on_true_vy=*/%v81890, /*on_false_vx=*/%v81888 (stack103)
        %v81894 = vadd.f32 %v81891, -3.0 (stack82)
        %v81898 = vsel /*vm=*/%vm81842, /*on_true_vy=*/%v81883, /*on_false_vx=*/%v81894 (stack72)
        %v81902 = vmul.f32 %v81879, %v81898 (stack83)
        %v81906 = vadd.f32 %v81875, %v81902 (stack82)
        %v81910 = vmul.f32 %v81906, %v81898 (stack83)
        %v81914 = vadd.f32 %v81871, %v81910 (stack82)
        %v81918 = vmul.f32 %v81914, %v81898 (stack83)
        %v81922 = vadd.f32 %v81867, %v81918 (stack82)
        %v81926 = vmul.f32 %v81922, %v81898 (stack83)
        %v81930 = vadd.f32 %v81863, %v81926 (stack82)
        %v81934 = vmul.f32 %v81930, %v81898 (stack83)
        %v81938 = vadd.f32 %v81859, %v81934 (stack82)
        %v81942 = vmul.f32 %v81938, %v81898 (stack83)
        %v81946 = vadd.f32 %v81855, %v81942 (stack82)
        %v81950 = vmul.f32 %v81946, %v81898 (stack83)
        %v81954 = vadd.f32 %v81851, %v81950 (stack82)
        %v81958 = vmul.f32 %v81954, %v81898 (stack83)
        %v81962 = vadd.f32 %v81847, %v81958 (stack82)
        %v81966 = vmul.f32 %v81962, %v81813 (stack83)
        %v81970 = vsel /*vm=*/%vm81818, /*on_true_vy=*/%v81823, /*on_false_vx=*/%v81966 (stack72)
        %v81974 = vmul.f32 %v81970, 1.4140625 (stack83)
        %s81976 = scalar_lea.vmem %s280, 852 [#allocation0] (stack107)
        %v81977 = vpack.c.bf16 0.0, %v81974 (stack104)
        %81978 = vst [vmem:[%s81976] sm:$0xf] /*vst_source=*/%v81977 (stack105)
        %v81981 = vadd.s32 %v3816, %v78751 (stack65)
        %s81983 = smul.u32 128, %s27 (stack66)
        %v81984 = vlaneseq (stack67)
        %v81985 = vand.u32 %v81984, 127 (stack68)
        %v81986 = vstv %s81983 (stack69)
        %v81987 = vadd.s32 %v81985, %v81986 (stack70)
        %v81991 = vadd.s32 %v81981, %v81987 (stack65)
        %vm81995 = vcmp.lt.u32.totalorder %v81991, %v81981 (stack71)
        %vm82000 = vcmp.lt.u32.totalorder %v81981, %v3816 (stack71)
        %v82005 = vadd.s32 %v3803, %v78734 (stack65)
        %v82009 = vadd.s32 %v82005, 1 (stack65)
        %v82013 = vsel /*vm=*/%vm82000, /*on_true_vy=*/%v82009, /*on_false_vx=*/%v82005 (stack72)
        %v82017 = vadd.s32 %v82013, 1 (stack65)
        %v82021 = vsel /*vm=*/%vm81995, /*on_true_vy=*/%v82017, /*on_false_vx=*/%v82013 (stack72)
        %v82026 = vadd.s32 %v82021, %v10 (stack65)
        %v82030 = vadd.s32 %v81991, %v9 (stack65)
        %v82034 = vadd.s32 %v82026, %v82030 (stack65)
        %v82036 = vshll.u32 %v82030, 13 (stack73)
        %v82037 = vshrl.u32 %v82030, 19 (stack74)
        %v82038 = vor.u32 %v82036, %v82037 (stack75)
        %v82039 = vxor.u32 %v82034, %v82038 (stack76)
        %v82042 = vadd.s32 %v82034, %v82039 (stack65)
        %v82044 = vshll.u32 %v82039, 15 (stack73)
        %v82045 = vshrl.u32 %v82039, 17 (stack74)
        %v82046 = vor.u32 %v82044, %v82045 (stack75)
        %v82047 = vxor.u32 %v82042, %v82046 (stack76)
        %v82050 = vadd.s32 %v82042, %v82047 (stack65)
        %v82052 = vshll.u32 %v82047, 26 (stack73)
        %v82053 = vshrl.u32 %v82047, 6 (stack74)
        %v82054 = vor.u32 %v82052, %v82053 (stack75)
        %v82055 = vxor.u32 %v82050, %v82054 (stack76)
        %v82058 = vadd.s32 %v82050, %v82055 (stack65)
        %v82062 = vadd.s32 %v82058, %v9 (stack65)
        %v82064 = vshll.u32 %v82055, 6 (stack73)
        %v82065 = vshrl.u32 %v82055, 26 (stack74)
        %v82066 = vor.u32 %v82064, %v82065 (stack75)
        %v82067 = vxor.u32 %v82058, %v82066 (stack76)
        %v82070 = vadd.s32 %v82067, %v8 (stack65)
        %v82074 = vadd.s32 %v82070, 1 (stack65)
        %v82078 = vadd.s32 %v82062, %v82074 (stack65)
        %v82080 = vshll.u32 %v82074, 17 (stack73)
        %v82081 = vshrl.u32 %v82074, 15 (stack74)
        %v82082 = vor.u32 %v82080, %v82081 (stack75)
        %v82083 = vxor.u32 %v82078, %v82082 (stack76)
        %v82086 = vadd.s32 %v82078, %v82083 (stack65)
        %v82088 = vshll.u32 %v82083, 29 (stack73)
        %v82089 = vshrl.u32 %v82083, 3 (stack74)
        %v82090 = vor.u32 %v82088, %v82089 (stack75)
        %v82091 = vxor.u32 %v82086, %v82090 (stack76)
        %v82094 = vadd.s32 %v82086, %v82091 (stack65)
        %v82096 = vshll.u32 %v82091, 16 (stack73)
        %v82097 = vshrl.u32 %v82091, 16 (stack74)
        %v82098 = vor.u32 %v82096, %v82097 (stack75)
        %v82099 = vxor.u32 %v82094, %v82098 (stack76)
        %v82102 = vadd.s32 %v82094, %v82099 (stack65)
        %v82106 = vadd.s32 %v82102, %v8 (stack65)
        %v82108 = vshll.u32 %v82099, 24 (stack73)
        %v82109 = vshrl.u32 %v82099, 8 (stack74)
        %v82110 = vor.u32 %v82108, %v82109 (stack75)
        %v82111 = vxor.u32 %v82102, %v82110 (stack76)
        %v82114 = vadd.s32 %v82111, %v10 (stack65)
        %v82118 = vadd.s32 %v82114, 2 (stack65)
        %v82122 = vadd.s32 %v82106, %v82118 (stack65)
        %v82124 = vshll.u32 %v82118, 13 (stack73)
        %v82125 = vshrl.u32 %v82118, 19 (stack74)
        %v82126 = vor.u32 %v82124, %v82125 (stack75)
        %v82127 = vxor.u32 %v82122, %v82126 (stack76)
        %v82130 = vadd.s32 %v82122, %v82127 (stack65)
        %v82132 = vshll.u32 %v82127, 15 (stack73)
        %v82133 = vshrl.u32 %v82127, 17 (stack74)
        %v82134 = vor.u32 %v82132, %v82133 (stack75)
        %v82135 = vxor.u32 %v82130, %v82134 (stack76)
        %v82138 = vadd.s32 %v82130, %v82135 (stack65)
        %v82140 = vshll.u32 %v82135, 26 (stack73)
        %v82141 = vshrl.u32 %v82135, 6 (stack74)
        %v82142 = vor.u32 %v82140, %v82141 (stack75)
        %v82143 = vxor.u32 %v82138, %v82142 (stack76)
        %v82146 = vadd.s32 %v82138, %v82143 (stack65)
        %v82150 = vadd.s32 %v82146, %v10 (stack65)
        %v82152 = vshll.u32 %v82143, 6 (stack73)
        %v82153 = vshrl.u32 %v82143, 26 (stack74)
        %v82154 = vor.u32 %v82152, %v82153 (stack75)
        %v82155 = vxor.u32 %v82146, %v82154 (stack76)
        %v82158 = vadd.s32 %v82155, %v9 (stack65)
        %v82162 = vadd.s32 %v82158, 3 (stack65)
        %v82166 = vadd.s32 %v82150, %v82162 (stack65)
        %v82168 = vshll.u32 %v82162, 17 (stack73)
        %v82169 = vshrl.u32 %v82162, 15 (stack74)
        %v82170 = vor.u32 %v82168, %v82169 (stack75)
        %v82171 = vxor.u32 %v82166, %v82170 (stack76)
        %v82174 = vadd.s32 %v82166, %v82171 (stack65)
        %v82176 = vshll.u32 %v82171, 29 (stack73)
        %v82177 = vshrl.u32 %v82171, 3 (stack74)
        %v82178 = vor.u32 %v82176, %v82177 (stack75)
        %v82179 = vxor.u32 %v82174, %v82178 (stack76)
        %v82182 = vadd.s32 %v82174, %v82179 (stack65)
        %v82184 = vshll.u32 %v82179, 16 (stack73)
        %v82185 = vshrl.u32 %v82179, 16 (stack74)
        %v82186 = vor.u32 %v82184, %v82185 (stack75)
        %v82187 = vxor.u32 %v82182, %v82186 (stack76)
        %v82190 = vadd.s32 %v82182, %v82187 (stack65)
        %v82194 = vadd.s32 %v82190, %v9 (stack65)
        %v82196 = vshll.u32 %v82187, 24 (stack73)
        %v82197 = vshrl.u32 %v82187, 8 (stack74)
        %v82198 = vor.u32 %v82196, %v82197 (stack75)
        %v82199 = vxor.u32 %v82190, %v82198 (stack76)
        %v82202 = vadd.s32 %v82199, %v8 (stack65)
        %v82206 = vadd.s32 %v82202, 4 (stack65)
        %v82210 = vadd.s32 %v82194, %v82206 (stack65)
        %v82212 = vshll.u32 %v82206, 13 (stack73)
        %v82213 = vshrl.u32 %v82206, 19 (stack74)
        %v82214 = vor.u32 %v82212, %v82213 (stack75)
        %v82215 = vxor.u32 %v82210, %v82214 (stack76)
        %v82218 = vadd.s32 %v82210, %v82215 (stack65)
        %v82220 = vshll.u32 %v82215, 15 (stack73)
        %v82221 = vshrl.u32 %v82215, 17 (stack74)
        %v82222 = vor.u32 %v82220, %v82221 (stack75)
        %v82223 = vxor.u32 %v82218, %v82222 (stack76)
        %v82226 = vadd.s32 %v82218, %v82223 (stack65)
        %v82228 = vshll.u32 %v82223, 26 (stack73)
        %v82229 = vshrl.u32 %v82223, 6 (stack74)
        %v82230 = vor.u32 %v82228, %v82229 (stack75)
        %v82231 = vxor.u32 %v82226, %v82230 (stack76)
        %v82234 = vadd.s32 %v82226, %v82231 (stack65)
        %v82238 = vadd.s32 %v82234, %v8 (stack65)
        %v82240 = vshll.u32 %v82231, 6 (stack73)
        %v82241 = vshrl.u32 %v82231, 26 (stack74)
        %v82242 = vor.u32 %v82240, %v82241 (stack75)
        %v82243 = vxor.u32 %v82234, %v82242 (stack76)
        %v82246 = vadd.s32 %v82243, %v10 (stack65)
        %v82250 = vadd.s32 %v82246, 5 (stack65)
        %v82252 = vxor.u32 %v82238, %v82250 (stack76)
        %v82253 = vand.u32.u8 %v82252, 255 (stack77)
        %v82254 = vand.u32 %v82253, 65535 (stack78)
        %v82255 = vshrl.u32 %v82254, 1 (stack79)
        %v82256 = vor.u32 %v82255, 16256 (stack75)
        %v82257 = vand.u32.u16 %v82256, 65535 (stack80)
        %v82258 = vunpack.i.l.bf16 %v82257 (stack81)
        %v82262 = vadd.f32 %v82258, -1.0 (stack82)
        %v82266 = vmul.f32 %v82262, 2.0 (stack83)
        %v82270 = vadd.f32 %v82266, -0.99609375 (stack82)
        %v82274 = vmax.f32 -0.99609375, %v82270 (stack84)
        %v82276 = vand.u32 2147483647, %v82274 (stack85)
        %vm82279 = vcmp.eq.f32.partialorder %v82276, 1.0 (stack86)
        %v82284 = vmul.f32 %v82274, inf (stack83)
        %v82286 = vxor.u32 %v82274, 2147483648 (stack87)
        %v82289 = vmul.f32 %v82274, %v82286 (stack83)
        %v82291 = vadd.f32 %v82289, 1.0 (stack88)
        %v82292 = vlog2.pop %v82291 (stack89)
        %v82293 = vmul.f32 %v82292, 0.6931472 (stack90)
        %v82294 = vmul.f32 -0.5, %v82289 (stack91)
        %v82295 = vadd.f32 %v82294, 1.0 (stack92)
        %v82296 = vmul.f32 %v82295, %v82289 (stack93)
        %v82297 = vand.u32 2147483647, %v82289 (stack94)
        %vm82298 = vcmp.lt.f32.partialorder %v82297, 0.0004427343 (stack95)
        %v82299 = vsel /*vm=*/%vm82298, /*on_true_vy=*/%v82296, /*on_false_vx=*/%v82293 (stack96)
        %v82300 = vxor.u32 %v82299, 2147483648 (stack87)
        %vm82303 = vcmp.lt.f32.partialorder %v82300, 5.0 (stack86)
        %v82308 = vsel /*vm=*/%vm82303, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v82312 = vsel /*vm=*/%vm82303, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v82316 = vsel /*vm=*/%vm82303, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v82320 = vsel /*vm=*/%vm82303, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v82324 = vsel /*vm=*/%vm82303, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v82328 = vsel /*vm=*/%vm82303, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v82332 = vsel /*vm=*/%vm82303, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v82336 = vsel /*vm=*/%vm82303, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v82340 = vsel /*vm=*/%vm82303, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v82344 = vadd.f32 %v82300, -2.5 (stack82)
        %v82346 = vrsqrt.pop %v82300 (stack97)
        %v82347 = vmul.f32 %v82300, %v82346 (stack98)
        %vm82348 = vcmp.eq.f32.partialorder %v82300, inf (stack99)
        %v82349 = vsel /*vm=*/%vm82348, /*on_true_vy=*/%v82300, /*on_false_vx=*/%v82347 (stack100)
        %vm82350 = vcmp.eq.f32.partialorder %v82300, 0.0 (stack101)
        %v82351 = vand.u32 %v82300, 2147483648 (stack102)
        %v82352 = vsel /*vm=*/%vm82350, /*on_true_vy=*/%v82351, /*on_false_vx=*/%v82349 (stack103)
        %v82355 = vadd.f32 %v82352, -3.0 (stack82)
        %v82359 = vsel /*vm=*/%vm82303, /*on_true_vy=*/%v82344, /*on_false_vx=*/%v82355 (stack72)
        %v82363 = vmul.f32 %v82340, %v82359 (stack83)
        %v82367 = vadd.f32 %v82336, %v82363 (stack82)
        %v82371 = vmul.f32 %v82367, %v82359 (stack83)
        %v82375 = vadd.f32 %v82332, %v82371 (stack82)
        %v82379 = vmul.f32 %v82375, %v82359 (stack83)
        %v82383 = vadd.f32 %v82328, %v82379 (stack82)
        %v82387 = vmul.f32 %v82383, %v82359 (stack83)
        %v82391 = vadd.f32 %v82324, %v82387 (stack82)
        %v82395 = vmul.f32 %v82391, %v82359 (stack83)
        %v82399 = vadd.f32 %v82320, %v82395 (stack82)
        %v82403 = vmul.f32 %v82399, %v82359 (stack83)
        %v82407 = vadd.f32 %v82316, %v82403 (stack82)
        %v82411 = vmul.f32 %v82407, %v82359 (stack83)
        %v82415 = vadd.f32 %v82312, %v82411 (stack82)
        %v82419 = vmul.f32 %v82415, %v82359 (stack83)
        %v82423 = vadd.f32 %v82308, %v82419 (stack82)
        %v82427 = vmul.f32 %v82423, %v82274 (stack83)
        %v82431 = vsel /*vm=*/%vm82279, /*on_true_vy=*/%v82284, /*on_false_vx=*/%v82427 (stack72)
        %v82435 = vmul.f32 %v82431, 1.4140625 (stack83)
        %s82437 = scalar_lea.vmem %s280, 980 [#allocation0] (stack107)
        %v82438 = vpack.c.bf16 0.0, %v82435 (stack104)
        %82439 = vst [vmem:[%s82437] sm:$0xf] /*vst_source=*/%v82438 (stack105)
        %s82440 = sadd.s32 %s339, 176 (stack106)
        %s82441 = sshrl.u32 %s82440, 10 (stack49)
        %p82442 = scmp.lt.s32.totalorder 1, %s82441 (stack50)
        %s82443 = scalar_select /*predicate=*/%p82442, /*on_true=*/1, /*on_false=*/%s82441 (stack51)
        %s82444 = sand.u32 %s82440, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s82445 = sshrl.u32 %s82444, 7 (stack53)
        %s82446 = sand.u32 %s82444, 127 /* smod.u32 w/div 128 */ (stack54)
        %s82447 = smul.addr %s82443, 8 (stack55)
        %s82448 = scalar_lea.vmem %s3, %s82447 (stack56)
        %s82450 = scalar_lea.vmem %s82448, %s82445 (stack57)
        %v82451 = vld [vmem:[%s82450] ss:$0 sm:$0xff] (stack58)
        %s82452 = sand.u32 %s82446, 255 (stack59)
        %s82454 = sor.u32 256, %s82452 (stack60)
        %82455 = vbcast.lane.b32.xlu0 %v82451, %s82454 (stack61)
        %v82456 = vpop.permute.xlu0 %82455 (stack62)
        %s82457 = sadd.s32 %s347, 176 (stack106)
        %s82458 = sshrl.u32 %s82457, 10 (stack49)
        %p82459 = scmp.lt.s32.totalorder 1, %s82458 (stack50)
        %s82460 = scalar_select /*predicate=*/%p82459, /*on_true=*/1, /*on_false=*/%s82458 (stack51)
        %s82461 = sand.u32 %s82457, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s82462 = sshrl.u32 %s82461, 7 (stack53)
        %s82463 = sand.u32 %s82461, 127 /* smod.u32 w/div 128 */ (stack54)
        %s82464 = smul.addr %s82460, 8 (stack55)
        %s82465 = scalar_lea.vmem %s5, %s82464 (stack56)
        %s82467 = scalar_lea.vmem %s82465, %s82462 (stack57)
        %v82468 = vld [vmem:[%s82467] ss:$0 sm:$0xff] (stack58)
        %s82469 = sand.u32 %s82463, 255 (stack59)
        %s82471 = sor.u32 256, %s82469 (stack60)
        %82472 = vbcast.lane.b32.xlu0 %v82468, %s82471 (stack61)
        %v82473 = vpop.permute.xlu0 %82472 (stack62)
        %v82476 = vadd.s32 %v408, %v82473 (stack65)
        %s82478 = smul.u32 128, %s27 (stack66)
        %v82479 = vlaneseq (stack67)
        %v82480 = vand.u32 %v82479, 127 (stack68)
        %v82481 = vstv %s82478 (stack69)
        %v82482 = vadd.s32 %v82480, %v82481 (stack70)
        %v82486 = vadd.s32 %v82476, %v82482 (stack65)
        %vm82490 = vcmp.lt.u32.totalorder %v82486, %v82476 (stack71)
        %vm82495 = vcmp.lt.u32.totalorder %v82476, %v408 (stack71)
        %v82500 = vadd.s32 %v380, %v82456 (stack65)
        %v82504 = vadd.s32 %v82500, 1 (stack65)
        %v82508 = vsel /*vm=*/%vm82495, /*on_true_vy=*/%v82504, /*on_false_vx=*/%v82500 (stack72)
        %v82512 = vadd.s32 %v82508, 1 (stack65)
        %v82516 = vsel /*vm=*/%vm82490, /*on_true_vy=*/%v82512, /*on_false_vx=*/%v82508 (stack72)
        %v82521 = vadd.s32 %v82516, %v10 (stack65)
        %v82525 = vadd.s32 %v82486, %v9 (stack65)
        %v82529 = vadd.s32 %v82521, %v82525 (stack65)
        %v82531 = vshll.u32 %v82525, 13 (stack73)
        %v82532 = vshrl.u32 %v82525, 19 (stack74)
        %v82533 = vor.u32 %v82531, %v82532 (stack75)
        %v82534 = vxor.u32 %v82529, %v82533 (stack76)
        %v82537 = vadd.s32 %v82529, %v82534 (stack65)
        %v82539 = vshll.u32 %v82534, 15 (stack73)
        %v82540 = vshrl.u32 %v82534, 17 (stack74)
        %v82541 = vor.u32 %v82539, %v82540 (stack75)
        %v82542 = vxor.u32 %v82537, %v82541 (stack76)
        %v82545 = vadd.s32 %v82537, %v82542 (stack65)
        %v82547 = vshll.u32 %v82542, 26 (stack73)
        %v82548 = vshrl.u32 %v82542, 6 (stack74)
        %v82549 = vor.u32 %v82547, %v82548 (stack75)
        %v82550 = vxor.u32 %v82545, %v82549 (stack76)
        %v82553 = vadd.s32 %v82545, %v82550 (stack65)
        %v82557 = vadd.s32 %v82553, %v9 (stack65)
        %v82559 = vshll.u32 %v82550, 6 (stack73)
        %v82560 = vshrl.u32 %v82550, 26 (stack74)
        %v82561 = vor.u32 %v82559, %v82560 (stack75)
        %v82562 = vxor.u32 %v82553, %v82561 (stack76)
        %v82565 = vadd.s32 %v82562, %v8 (stack65)
        %v82569 = vadd.s32 %v82565, 1 (stack65)
        %v82573 = vadd.s32 %v82557, %v82569 (stack65)
        %v82575 = vshll.u32 %v82569, 17 (stack73)
        %v82576 = vshrl.u32 %v82569, 15 (stack74)
        %v82577 = vor.u32 %v82575, %v82576 (stack75)
        %v82578 = vxor.u32 %v82573, %v82577 (stack76)
        %v82581 = vadd.s32 %v82573, %v82578 (stack65)
        %v82583 = vshll.u32 %v82578, 29 (stack73)
        %v82584 = vshrl.u32 %v82578, 3 (stack74)
        %v82585 = vor.u32 %v82583, %v82584 (stack75)
        %v82586 = vxor.u32 %v82581, %v82585 (stack76)
        %v82589 = vadd.s32 %v82581, %v82586 (stack65)
        %v82591 = vshll.u32 %v82586, 16 (stack73)
        %v82592 = vshrl.u32 %v82586, 16 (stack74)
        %v82593 = vor.u32 %v82591, %v82592 (stack75)
        %v82594 = vxor.u32 %v82589, %v82593 (stack76)
        %v82597 = vadd.s32 %v82589, %v82594 (stack65)
        %v82601 = vadd.s32 %v82597, %v8 (stack65)
        %v82603 = vshll.u32 %v82594, 24 (stack73)
        %v82604 = vshrl.u32 %v82594, 8 (stack74)
        %v82605 = vor.u32 %v82603, %v82604 (stack75)
        %v82606 = vxor.u32 %v82597, %v82605 (stack76)
        %v82609 = vadd.s32 %v82606, %v10 (stack65)
        %v82613 = vadd.s32 %v82609, 2 (stack65)
        %v82617 = vadd.s32 %v82601, %v82613 (stack65)
        %v82619 = vshll.u32 %v82613, 13 (stack73)
        %v82620 = vshrl.u32 %v82613, 19 (stack74)
        %v82621 = vor.u32 %v82619, %v82620 (stack75)
        %v82622 = vxor.u32 %v82617, %v82621 (stack76)
        %v82625 = vadd.s32 %v82617, %v82622 (stack65)
        %v82627 = vshll.u32 %v82622, 15 (stack73)
        %v82628 = vshrl.u32 %v82622, 17 (stack74)
        %v82629 = vor.u32 %v82627, %v82628 (stack75)
        %v82630 = vxor.u32 %v82625, %v82629 (stack76)
        %v82633 = vadd.s32 %v82625, %v82630 (stack65)
        %v82635 = vshll.u32 %v82630, 26 (stack73)
        %v82636 = vshrl.u32 %v82630, 6 (stack74)
        %v82637 = vor.u32 %v82635, %v82636 (stack75)
        %v82638 = vxor.u32 %v82633, %v82637 (stack76)
        %v82641 = vadd.s32 %v82633, %v82638 (stack65)
        %v82645 = vadd.s32 %v82641, %v10 (stack65)
        %v82647 = vshll.u32 %v82638, 6 (stack73)
        %v82648 = vshrl.u32 %v82638, 26 (stack74)
        %v82649 = vor.u32 %v82647, %v82648 (stack75)
        %v82650 = vxor.u32 %v82641, %v82649 (stack76)
        %v82653 = vadd.s32 %v82650, %v9 (stack65)
        %v82657 = vadd.s32 %v82653, 3 (stack65)
        %v82661 = vadd.s32 %v82645, %v82657 (stack65)
        %v82663 = vshll.u32 %v82657, 17 (stack73)
        %v82664 = vshrl.u32 %v82657, 15 (stack74)
        %v82665 = vor.u32 %v82663, %v82664 (stack75)
        %v82666 = vxor.u32 %v82661, %v82665 (stack76)
        %v82669 = vadd.s32 %v82661, %v82666 (stack65)
        %v82671 = vshll.u32 %v82666, 29 (stack73)
        %v82672 = vshrl.u32 %v82666, 3 (stack74)
        %v82673 = vor.u32 %v82671, %v82672 (stack75)
        %v82674 = vxor.u32 %v82669, %v82673 (stack76)
        %v82677 = vadd.s32 %v82669, %v82674 (stack65)
        %v82679 = vshll.u32 %v82674, 16 (stack73)
        %v82680 = vshrl.u32 %v82674, 16 (stack74)
        %v82681 = vor.u32 %v82679, %v82680 (stack75)
        %v82682 = vxor.u32 %v82677, %v82681 (stack76)
        %v82685 = vadd.s32 %v82677, %v82682 (stack65)
        %v82689 = vadd.s32 %v82685, %v9 (stack65)
        %v82691 = vshll.u32 %v82682, 24 (stack73)
        %v82692 = vshrl.u32 %v82682, 8 (stack74)
        %v82693 = vor.u32 %v82691, %v82692 (stack75)
        %v82694 = vxor.u32 %v82685, %v82693 (stack76)
        %v82697 = vadd.s32 %v82694, %v8 (stack65)
        %v82701 = vadd.s32 %v82697, 4 (stack65)
        %v82705 = vadd.s32 %v82689, %v82701 (stack65)
        %v82707 = vshll.u32 %v82701, 13 (stack73)
        %v82708 = vshrl.u32 %v82701, 19 (stack74)
        %v82709 = vor.u32 %v82707, %v82708 (stack75)
        %v82710 = vxor.u32 %v82705, %v82709 (stack76)
        %v82713 = vadd.s32 %v82705, %v82710 (stack65)
        %v82715 = vshll.u32 %v82710, 15 (stack73)
        %v82716 = vshrl.u32 %v82710, 17 (stack74)
        %v82717 = vor.u32 %v82715, %v82716 (stack75)
        %v82718 = vxor.u32 %v82713, %v82717 (stack76)
        %v82721 = vadd.s32 %v82713, %v82718 (stack65)
        %v82723 = vshll.u32 %v82718, 26 (stack73)
        %v82724 = vshrl.u32 %v82718, 6 (stack74)
        %v82725 = vor.u32 %v82723, %v82724 (stack75)
        %v82726 = vxor.u32 %v82721, %v82725 (stack76)
        %v82729 = vadd.s32 %v82721, %v82726 (stack65)
        %v82733 = vadd.s32 %v82729, %v8 (stack65)
        %v82735 = vshll.u32 %v82726, 6 (stack73)
        %v82736 = vshrl.u32 %v82726, 26 (stack74)
        %v82737 = vor.u32 %v82735, %v82736 (stack75)
        %v82738 = vxor.u32 %v82729, %v82737 (stack76)
        %v82741 = vadd.s32 %v82738, %v10 (stack65)
        %v82745 = vadd.s32 %v82741, 5 (stack65)
        %v82747 = vxor.u32 %v82733, %v82745 (stack76)
        %v82748 = vand.u32.u8 %v82747, 255 (stack77)
        %v82749 = vand.u32 %v82748, 65535 (stack78)
        %v82750 = vshrl.u32 %v82749, 1 (stack79)
        %v82751 = vor.u32 %v82750, 16256 (stack75)
        %v82752 = vand.u32.u16 %v82751, 65535 (stack80)
        %v82753 = vunpack.i.l.bf16 %v82752 (stack81)
        %v82757 = vadd.f32 %v82753, -1.0 (stack82)
        %v82761 = vmul.f32 %v82757, 2.0 (stack83)
        %v82765 = vadd.f32 %v82761, -0.99609375 (stack82)
        %v82769 = vmax.f32 -0.99609375, %v82765 (stack84)
        %v82771 = vand.u32 2147483647, %v82769 (stack85)
        %vm82774 = vcmp.eq.f32.partialorder %v82771, 1.0 (stack86)
        %v82779 = vmul.f32 %v82769, inf (stack83)
        %v82781 = vxor.u32 %v82769, 2147483648 (stack87)
        %v82784 = vmul.f32 %v82769, %v82781 (stack83)
        %v82786 = vadd.f32 %v82784, 1.0 (stack88)
        %v82787 = vlog2.pop %v82786 (stack89)
        %v82788 = vmul.f32 %v82787, 0.6931472 (stack90)
        %v82789 = vmul.f32 -0.5, %v82784 (stack91)
        %v82790 = vadd.f32 %v82789, 1.0 (stack92)
        %v82791 = vmul.f32 %v82790, %v82784 (stack93)
        %v82792 = vand.u32 2147483647, %v82784 (stack94)
        %vm82793 = vcmp.lt.f32.partialorder %v82792, 0.0004427343 (stack95)
        %v82794 = vsel /*vm=*/%vm82793, /*on_true_vy=*/%v82791, /*on_false_vx=*/%v82788 (stack96)
        %v82795 = vxor.u32 %v82794, 2147483648 (stack87)
        %vm82798 = vcmp.lt.f32.partialorder %v82795, 5.0 (stack86)
        %v82803 = vsel /*vm=*/%vm82798, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v82807 = vsel /*vm=*/%vm82798, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v82811 = vsel /*vm=*/%vm82798, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v82815 = vsel /*vm=*/%vm82798, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v82819 = vsel /*vm=*/%vm82798, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v82823 = vsel /*vm=*/%vm82798, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v82827 = vsel /*vm=*/%vm82798, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v82831 = vsel /*vm=*/%vm82798, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v82835 = vsel /*vm=*/%vm82798, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v82839 = vadd.f32 %v82795, -2.5 (stack82)
        %v82841 = vrsqrt.pop %v82795 (stack97)
        %v82842 = vmul.f32 %v82795, %v82841 (stack98)
        %vm82843 = vcmp.eq.f32.partialorder %v82795, inf (stack99)
        %v82844 = vsel /*vm=*/%vm82843, /*on_true_vy=*/%v82795, /*on_false_vx=*/%v82842 (stack100)
        %vm82845 = vcmp.eq.f32.partialorder %v82795, 0.0 (stack101)
        %v82846 = vand.u32 %v82795, 2147483648 (stack102)
        %v82847 = vsel /*vm=*/%vm82845, /*on_true_vy=*/%v82846, /*on_false_vx=*/%v82844 (stack103)
        %v82850 = vadd.f32 %v82847, -3.0 (stack82)
        %v82854 = vsel /*vm=*/%vm82798, /*on_true_vy=*/%v82839, /*on_false_vx=*/%v82850 (stack72)
        %v82858 = vmul.f32 %v82835, %v82854 (stack83)
        %v82862 = vadd.f32 %v82831, %v82858 (stack82)
        %v82866 = vmul.f32 %v82862, %v82854 (stack83)
        %v82870 = vadd.f32 %v82827, %v82866 (stack82)
        %v82874 = vmul.f32 %v82870, %v82854 (stack83)
        %v82878 = vadd.f32 %v82823, %v82874 (stack82)
        %v82882 = vmul.f32 %v82878, %v82854 (stack83)
        %v82886 = vadd.f32 %v82819, %v82882 (stack82)
        %v82890 = vmul.f32 %v82886, %v82854 (stack83)
        %v82894 = vadd.f32 %v82815, %v82890 (stack82)
        %v82898 = vmul.f32 %v82894, %v82854 (stack83)
        %v82902 = vadd.f32 %v82811, %v82898 (stack82)
        %v82906 = vmul.f32 %v82902, %v82854 (stack83)
        %v82910 = vadd.f32 %v82807, %v82906 (stack82)
        %v82914 = vmul.f32 %v82910, %v82854 (stack83)
        %v82918 = vadd.f32 %v82803, %v82914 (stack82)
        %v82922 = vmul.f32 %v82918, %v82769 (stack83)
        %v82926 = vsel /*vm=*/%vm82774, /*on_true_vy=*/%v82779, /*on_false_vx=*/%v82922 (stack72)
        %v82930 = vmul.f32 %v82926, 1.4140625 (stack83)
        %s82932 = scalar_lea.vmem %s280, 88 [#allocation0] (stack107)
        %v82933 = vpack.c.bf16 0.0, %v82930 (stack104)
        %82934 = vst [vmem:[%s82932] sm:$0xf] /*vst_source=*/%v82933 (stack105)
        %v82937 = vadd.s32 %v894, %v82473 (stack65)
        %s82939 = smul.u32 128, %s27 (stack66)
        %v82940 = vlaneseq (stack67)
        %v82941 = vand.u32 %v82940, 127 (stack68)
        %v82942 = vstv %s82939 (stack69)
        %v82943 = vadd.s32 %v82941, %v82942 (stack70)
        %v82947 = vadd.s32 %v82937, %v82943 (stack65)
        %vm82951 = vcmp.lt.u32.totalorder %v82947, %v82937 (stack71)
        %vm82956 = vcmp.lt.u32.totalorder %v82937, %v894 (stack71)
        %v82961 = vadd.s32 %v881, %v82456 (stack65)
        %v82965 = vadd.s32 %v82961, 1 (stack65)
        %v82969 = vsel /*vm=*/%vm82956, /*on_true_vy=*/%v82965, /*on_false_vx=*/%v82961 (stack72)
        %v82973 = vadd.s32 %v82969, 1 (stack65)
        %v82977 = vsel /*vm=*/%vm82951, /*on_true_vy=*/%v82973, /*on_false_vx=*/%v82969 (stack72)
        %v82982 = vadd.s32 %v82977, %v10 (stack65)
        %v82986 = vadd.s32 %v82947, %v9 (stack65)
        %v82990 = vadd.s32 %v82982, %v82986 (stack65)
        %v82992 = vshll.u32 %v82986, 13 (stack73)
        %v82993 = vshrl.u32 %v82986, 19 (stack74)
        %v82994 = vor.u32 %v82992, %v82993 (stack75)
        %v82995 = vxor.u32 %v82990, %v82994 (stack76)
        %v82998 = vadd.s32 %v82990, %v82995 (stack65)
        %v83000 = vshll.u32 %v82995, 15 (stack73)
        %v83001 = vshrl.u32 %v82995, 17 (stack74)
        %v83002 = vor.u32 %v83000, %v83001 (stack75)
        %v83003 = vxor.u32 %v82998, %v83002 (stack76)
        %v83006 = vadd.s32 %v82998, %v83003 (stack65)
        %v83008 = vshll.u32 %v83003, 26 (stack73)
        %v83009 = vshrl.u32 %v83003, 6 (stack74)
        %v83010 = vor.u32 %v83008, %v83009 (stack75)
        %v83011 = vxor.u32 %v83006, %v83010 (stack76)
        %v83014 = vadd.s32 %v83006, %v83011 (stack65)
        %v83018 = vadd.s32 %v83014, %v9 (stack65)
        %v83020 = vshll.u32 %v83011, 6 (stack73)
        %v83021 = vshrl.u32 %v83011, 26 (stack74)
        %v83022 = vor.u32 %v83020, %v83021 (stack75)
        %v83023 = vxor.u32 %v83014, %v83022 (stack76)
        %v83026 = vadd.s32 %v83023, %v8 (stack65)
        %v83030 = vadd.s32 %v83026, 1 (stack65)
        %v83034 = vadd.s32 %v83018, %v83030 (stack65)
        %v83036 = vshll.u32 %v83030, 17 (stack73)
        %v83037 = vshrl.u32 %v83030, 15 (stack74)
        %v83038 = vor.u32 %v83036, %v83037 (stack75)
        %v83039 = vxor.u32 %v83034, %v83038 (stack76)
        %v83042 = vadd.s32 %v83034, %v83039 (stack65)
        %v83044 = vshll.u32 %v83039, 29 (stack73)
        %v83045 = vshrl.u32 %v83039, 3 (stack74)
        %v83046 = vor.u32 %v83044, %v83045 (stack75)
        %v83047 = vxor.u32 %v83042, %v83046 (stack76)
        %v83050 = vadd.s32 %v83042, %v83047 (stack65)
        %v83052 = vshll.u32 %v83047, 16 (stack73)
        %v83053 = vshrl.u32 %v83047, 16 (stack74)
        %v83054 = vor.u32 %v83052, %v83053 (stack75)
        %v83055 = vxor.u32 %v83050, %v83054 (stack76)
        %v83058 = vadd.s32 %v83050, %v83055 (stack65)
        %v83062 = vadd.s32 %v83058, %v8 (stack65)
        %v83064 = vshll.u32 %v83055, 24 (stack73)
        %v83065 = vshrl.u32 %v83055, 8 (stack74)
        %v83066 = vor.u32 %v83064, %v83065 (stack75)
        %v83067 = vxor.u32 %v83058, %v83066 (stack76)
        %v83070 = vadd.s32 %v83067, %v10 (stack65)
        %v83074 = vadd.s32 %v83070, 2 (stack65)
        %v83078 = vadd.s32 %v83062, %v83074 (stack65)
        %v83080 = vshll.u32 %v83074, 13 (stack73)
        %v83081 = vshrl.u32 %v83074, 19 (stack74)
        %v83082 = vor.u32 %v83080, %v83081 (stack75)
        %v83083 = vxor.u32 %v83078, %v83082 (stack76)
        %v83086 = vadd.s32 %v83078, %v83083 (stack65)
        %v83088 = vshll.u32 %v83083, 15 (stack73)
        %v83089 = vshrl.u32 %v83083, 17 (stack74)
        %v83090 = vor.u32 %v83088, %v83089 (stack75)
        %v83091 = vxor.u32 %v83086, %v83090 (stack76)
        %v83094 = vadd.s32 %v83086, %v83091 (stack65)
        %v83096 = vshll.u32 %v83091, 26 (stack73)
        %v83097 = vshrl.u32 %v83091, 6 (stack74)
        %v83098 = vor.u32 %v83096, %v83097 (stack75)
        %v83099 = vxor.u32 %v83094, %v83098 (stack76)
        %v83102 = vadd.s32 %v83094, %v83099 (stack65)
        %v83106 = vadd.s32 %v83102, %v10 (stack65)
        %v83108 = vshll.u32 %v83099, 6 (stack73)
        %v83109 = vshrl.u32 %v83099, 26 (stack74)
        %v83110 = vor.u32 %v83108, %v83109 (stack75)
        %v83111 = vxor.u32 %v83102, %v83110 (stack76)
        %v83114 = vadd.s32 %v83111, %v9 (stack65)
        %v83118 = vadd.s32 %v83114, 3 (stack65)
        %v83122 = vadd.s32 %v83106, %v83118 (stack65)
        %v83124 = vshll.u32 %v83118, 17 (stack73)
        %v83125 = vshrl.u32 %v83118, 15 (stack74)
        %v83126 = vor.u32 %v83124, %v83125 (stack75)
        %v83127 = vxor.u32 %v83122, %v83126 (stack76)
        %v83130 = vadd.s32 %v83122, %v83127 (stack65)
        %v83132 = vshll.u32 %v83127, 29 (stack73)
        %v83133 = vshrl.u32 %v83127, 3 (stack74)
        %v83134 = vor.u32 %v83132, %v83133 (stack75)
        %v83135 = vxor.u32 %v83130, %v83134 (stack76)
        %v83138 = vadd.s32 %v83130, %v83135 (stack65)
        %v83140 = vshll.u32 %v83135, 16 (stack73)
        %v83141 = vshrl.u32 %v83135, 16 (stack74)
        %v83142 = vor.u32 %v83140, %v83141 (stack75)
        %v83143 = vxor.u32 %v83138, %v83142 (stack76)
        %v83146 = vadd.s32 %v83138, %v83143 (stack65)
        %v83150 = vadd.s32 %v83146, %v9 (stack65)
        %v83152 = vshll.u32 %v83143, 24 (stack73)
        %v83153 = vshrl.u32 %v83143, 8 (stack74)
        %v83154 = vor.u32 %v83152, %v83153 (stack75)
        %v83155 = vxor.u32 %v83146, %v83154 (stack76)
        %v83158 = vadd.s32 %v83155, %v8 (stack65)
        %v83162 = vadd.s32 %v83158, 4 (stack65)
        %v83166 = vadd.s32 %v83150, %v83162 (stack65)
        %v83168 = vshll.u32 %v83162, 13 (stack73)
        %v83169 = vshrl.u32 %v83162, 19 (stack74)
        %v83170 = vor.u32 %v83168, %v83169 (stack75)
        %v83171 = vxor.u32 %v83166, %v83170 (stack76)
        %v83174 = vadd.s32 %v83166, %v83171 (stack65)
        %v83176 = vshll.u32 %v83171, 15 (stack73)
        %v83177 = vshrl.u32 %v83171, 17 (stack74)
        %v83178 = vor.u32 %v83176, %v83177 (stack75)
        %v83179 = vxor.u32 %v83174, %v83178 (stack76)
        %v83182 = vadd.s32 %v83174, %v83179 (stack65)
        %v83184 = vshll.u32 %v83179, 26 (stack73)
        %v83185 = vshrl.u32 %v83179, 6 (stack74)
        %v83186 = vor.u32 %v83184, %v83185 (stack75)
        %v83187 = vxor.u32 %v83182, %v83186 (stack76)
        %v83190 = vadd.s32 %v83182, %v83187 (stack65)
        %v83194 = vadd.s32 %v83190, %v8 (stack65)
        %v83196 = vshll.u32 %v83187, 6 (stack73)
        %v83197 = vshrl.u32 %v83187, 26 (stack74)
        %v83198 = vor.u32 %v83196, %v83197 (stack75)
        %v83199 = vxor.u32 %v83190, %v83198 (stack76)
        %v83202 = vadd.s32 %v83199, %v10 (stack65)
        %v83206 = vadd.s32 %v83202, 5 (stack65)
        %v83208 = vxor.u32 %v83194, %v83206 (stack76)
        %v83209 = vand.u32.u8 %v83208, 255 (stack77)
        %v83210 = vand.u32 %v83209, 65535 (stack78)
        %v83211 = vshrl.u32 %v83210, 1 (stack79)
        %v83212 = vor.u32 %v83211, 16256 (stack75)
        %v83213 = vand.u32.u16 %v83212, 65535 (stack80)
        %v83214 = vunpack.i.l.bf16 %v83213 (stack81)
        %v83218 = vadd.f32 %v83214, -1.0 (stack82)
        %v83222 = vmul.f32 %v83218, 2.0 (stack83)
        %v83226 = vadd.f32 %v83222, -0.99609375 (stack82)
        %v83230 = vmax.f32 -0.99609375, %v83226 (stack84)
        %v83232 = vand.u32 2147483647, %v83230 (stack85)
        %vm83235 = vcmp.eq.f32.partialorder %v83232, 1.0 (stack86)
        %v83240 = vmul.f32 %v83230, inf (stack83)
        %v83242 = vxor.u32 %v83230, 2147483648 (stack87)
        %v83245 = vmul.f32 %v83230, %v83242 (stack83)
        %v83247 = vadd.f32 %v83245, 1.0 (stack88)
        %v83248 = vlog2.pop %v83247 (stack89)
        %v83249 = vmul.f32 %v83248, 0.6931472 (stack90)
        %v83250 = vmul.f32 -0.5, %v83245 (stack91)
        %v83251 = vadd.f32 %v83250, 1.0 (stack92)
        %v83252 = vmul.f32 %v83251, %v83245 (stack93)
        %v83253 = vand.u32 2147483647, %v83245 (stack94)
        %vm83254 = vcmp.lt.f32.partialorder %v83253, 0.0004427343 (stack95)
        %v83255 = vsel /*vm=*/%vm83254, /*on_true_vy=*/%v83252, /*on_false_vx=*/%v83249 (stack96)
        %v83256 = vxor.u32 %v83255, 2147483648 (stack87)
        %vm83259 = vcmp.lt.f32.partialorder %v83256, 5.0 (stack86)
        %v83264 = vsel /*vm=*/%vm83259, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v83268 = vsel /*vm=*/%vm83259, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v83272 = vsel /*vm=*/%vm83259, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v83276 = vsel /*vm=*/%vm83259, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v83280 = vsel /*vm=*/%vm83259, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v83284 = vsel /*vm=*/%vm83259, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v83288 = vsel /*vm=*/%vm83259, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v83292 = vsel /*vm=*/%vm83259, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v83296 = vsel /*vm=*/%vm83259, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v83300 = vadd.f32 %v83256, -2.5 (stack82)
        %v83302 = vrsqrt.pop %v83256 (stack97)
        %v83303 = vmul.f32 %v83256, %v83302 (stack98)
        %vm83304 = vcmp.eq.f32.partialorder %v83256, inf (stack99)
        %v83305 = vsel /*vm=*/%vm83304, /*on_true_vy=*/%v83256, /*on_false_vx=*/%v83303 (stack100)
        %vm83306 = vcmp.eq.f32.partialorder %v83256, 0.0 (stack101)
        %v83307 = vand.u32 %v83256, 2147483648 (stack102)
        %v83308 = vsel /*vm=*/%vm83306, /*on_true_vy=*/%v83307, /*on_false_vx=*/%v83305 (stack103)
        %v83311 = vadd.f32 %v83308, -3.0 (stack82)
        %v83315 = vsel /*vm=*/%vm83259, /*on_true_vy=*/%v83300, /*on_false_vx=*/%v83311 (stack72)
        %v83319 = vmul.f32 %v83296, %v83315 (stack83)
        %v83323 = vadd.f32 %v83292, %v83319 (stack82)
        %v83327 = vmul.f32 %v83323, %v83315 (stack83)
        %v83331 = vadd.f32 %v83288, %v83327 (stack82)
        %v83335 = vmul.f32 %v83331, %v83315 (stack83)
        %v83339 = vadd.f32 %v83284, %v83335 (stack82)
        %v83343 = vmul.f32 %v83339, %v83315 (stack83)
        %v83347 = vadd.f32 %v83280, %v83343 (stack82)
        %v83351 = vmul.f32 %v83347, %v83315 (stack83)
        %v83355 = vadd.f32 %v83276, %v83351 (stack82)
        %v83359 = vmul.f32 %v83355, %v83315 (stack83)
        %v83363 = vadd.f32 %v83272, %v83359 (stack82)
        %v83367 = vmul.f32 %v83363, %v83315 (stack83)
        %v83371 = vadd.f32 %v83268, %v83367 (stack82)
        %v83375 = vmul.f32 %v83371, %v83315 (stack83)
        %v83379 = vadd.f32 %v83264, %v83375 (stack82)
        %v83383 = vmul.f32 %v83379, %v83230 (stack83)
        %v83387 = vsel /*vm=*/%vm83235, /*on_true_vy=*/%v83240, /*on_false_vx=*/%v83383 (stack72)
        %v83391 = vmul.f32 %v83387, 1.4140625 (stack83)
        %s83393 = scalar_lea.vmem %s280, 216 [#allocation0] (stack107)
        %v83394 = vpack.c.bf16 0.0, %v83391 (stack104)
        %83395 = vst [vmem:[%s83393] sm:$0xf] /*vst_source=*/%v83394 (stack105)
        %v83398 = vadd.s32 %v1381, %v82473 (stack65)
        %s83400 = smul.u32 128, %s27 (stack66)
        %v83401 = vlaneseq (stack67)
        %v83402 = vand.u32 %v83401, 127 (stack68)
        %v83403 = vstv %s83400 (stack69)
        %v83404 = vadd.s32 %v83402, %v83403 (stack70)
        %v83408 = vadd.s32 %v83398, %v83404 (stack65)
        %vm83412 = vcmp.lt.u32.totalorder %v83408, %v83398 (stack71)
        %vm83417 = vcmp.lt.u32.totalorder %v83398, %v1381 (stack71)
        %v83422 = vadd.s32 %v1368, %v82456 (stack65)
        %v83426 = vadd.s32 %v83422, 1 (stack65)
        %v83430 = vsel /*vm=*/%vm83417, /*on_true_vy=*/%v83426, /*on_false_vx=*/%v83422 (stack72)
        %v83434 = vadd.s32 %v83430, 1 (stack65)
        %v83438 = vsel /*vm=*/%vm83412, /*on_true_vy=*/%v83434, /*on_false_vx=*/%v83430 (stack72)
        %v83443 = vadd.s32 %v83438, %v10 (stack65)
        %v83447 = vadd.s32 %v83408, %v9 (stack65)
        %v83451 = vadd.s32 %v83443, %v83447 (stack65)
        %v83453 = vshll.u32 %v83447, 13 (stack73)
        %v83454 = vshrl.u32 %v83447, 19 (stack74)
        %v83455 = vor.u32 %v83453, %v83454 (stack75)
        %v83456 = vxor.u32 %v83451, %v83455 (stack76)
        %v83459 = vadd.s32 %v83451, %v83456 (stack65)
        %v83461 = vshll.u32 %v83456, 15 (stack73)
        %v83462 = vshrl.u32 %v83456, 17 (stack74)
        %v83463 = vor.u32 %v83461, %v83462 (stack75)
        %v83464 = vxor.u32 %v83459, %v83463 (stack76)
        %v83467 = vadd.s32 %v83459, %v83464 (stack65)
        %v83469 = vshll.u32 %v83464, 26 (stack73)
        %v83470 = vshrl.u32 %v83464, 6 (stack74)
        %v83471 = vor.u32 %v83469, %v83470 (stack75)
        %v83472 = vxor.u32 %v83467, %v83471 (stack76)
        %v83475 = vadd.s32 %v83467, %v83472 (stack65)
        %v83479 = vadd.s32 %v83475, %v9 (stack65)
        %v83481 = vshll.u32 %v83472, 6 (stack73)
        %v83482 = vshrl.u32 %v83472, 26 (stack74)
        %v83483 = vor.u32 %v83481, %v83482 (stack75)
        %v83484 = vxor.u32 %v83475, %v83483 (stack76)
        %v83487 = vadd.s32 %v83484, %v8 (stack65)
        %v83491 = vadd.s32 %v83487, 1 (stack65)
        %v83495 = vadd.s32 %v83479, %v83491 (stack65)
        %v83497 = vshll.u32 %v83491, 17 (stack73)
        %v83498 = vshrl.u32 %v83491, 15 (stack74)
        %v83499 = vor.u32 %v83497, %v83498 (stack75)
        %v83500 = vxor.u32 %v83495, %v83499 (stack76)
        %v83503 = vadd.s32 %v83495, %v83500 (stack65)
        %v83505 = vshll.u32 %v83500, 29 (stack73)
        %v83506 = vshrl.u32 %v83500, 3 (stack74)
        %v83507 = vor.u32 %v83505, %v83506 (stack75)
        %v83508 = vxor.u32 %v83503, %v83507 (stack76)
        %v83511 = vadd.s32 %v83503, %v83508 (stack65)
        %v83513 = vshll.u32 %v83508, 16 (stack73)
        %v83514 = vshrl.u32 %v83508, 16 (stack74)
        %v83515 = vor.u32 %v83513, %v83514 (stack75)
        %v83516 = vxor.u32 %v83511, %v83515 (stack76)
        %v83519 = vadd.s32 %v83511, %v83516 (stack65)
        %v83523 = vadd.s32 %v83519, %v8 (stack65)
        %v83525 = vshll.u32 %v83516, 24 (stack73)
        %v83526 = vshrl.u32 %v83516, 8 (stack74)
        %v83527 = vor.u32 %v83525, %v83526 (stack75)
        %v83528 = vxor.u32 %v83519, %v83527 (stack76)
        %v83531 = vadd.s32 %v83528, %v10 (stack65)
        %v83535 = vadd.s32 %v83531, 2 (stack65)
        %v83539 = vadd.s32 %v83523, %v83535 (stack65)
        %v83541 = vshll.u32 %v83535, 13 (stack73)
        %v83542 = vshrl.u32 %v83535, 19 (stack74)
        %v83543 = vor.u32 %v83541, %v83542 (stack75)
        %v83544 = vxor.u32 %v83539, %v83543 (stack76)
        %v83547 = vadd.s32 %v83539, %v83544 (stack65)
        %v83549 = vshll.u32 %v83544, 15 (stack73)
        %v83550 = vshrl.u32 %v83544, 17 (stack74)
        %v83551 = vor.u32 %v83549, %v83550 (stack75)
        %v83552 = vxor.u32 %v83547, %v83551 (stack76)
        %v83555 = vadd.s32 %v83547, %v83552 (stack65)
        %v83557 = vshll.u32 %v83552, 26 (stack73)
        %v83558 = vshrl.u32 %v83552, 6 (stack74)
        %v83559 = vor.u32 %v83557, %v83558 (stack75)
        %v83560 = vxor.u32 %v83555, %v83559 (stack76)
        %v83563 = vadd.s32 %v83555, %v83560 (stack65)
        %v83567 = vadd.s32 %v83563, %v10 (stack65)
        %v83569 = vshll.u32 %v83560, 6 (stack73)
        %v83570 = vshrl.u32 %v83560, 26 (stack74)
        %v83571 = vor.u32 %v83569, %v83570 (stack75)
        %v83572 = vxor.u32 %v83563, %v83571 (stack76)
        %v83575 = vadd.s32 %v83572, %v9 (stack65)
        %v83579 = vadd.s32 %v83575, 3 (stack65)
        %v83583 = vadd.s32 %v83567, %v83579 (stack65)
        %v83585 = vshll.u32 %v83579, 17 (stack73)
        %v83586 = vshrl.u32 %v83579, 15 (stack74)
        %v83587 = vor.u32 %v83585, %v83586 (stack75)
        %v83588 = vxor.u32 %v83583, %v83587 (stack76)
        %v83591 = vadd.s32 %v83583, %v83588 (stack65)
        %v83593 = vshll.u32 %v83588, 29 (stack73)
        %v83594 = vshrl.u32 %v83588, 3 (stack74)
        %v83595 = vor.u32 %v83593, %v83594 (stack75)
        %v83596 = vxor.u32 %v83591, %v83595 (stack76)
        %v83599 = vadd.s32 %v83591, %v83596 (stack65)
        %v83601 = vshll.u32 %v83596, 16 (stack73)
        %v83602 = vshrl.u32 %v83596, 16 (stack74)
        %v83603 = vor.u32 %v83601, %v83602 (stack75)
        %v83604 = vxor.u32 %v83599, %v83603 (stack76)
        %v83607 = vadd.s32 %v83599, %v83604 (stack65)
        %v83611 = vadd.s32 %v83607, %v9 (stack65)
        %v83613 = vshll.u32 %v83604, 24 (stack73)
        %v83614 = vshrl.u32 %v83604, 8 (stack74)
        %v83615 = vor.u32 %v83613, %v83614 (stack75)
        %v83616 = vxor.u32 %v83607, %v83615 (stack76)
        %v83619 = vadd.s32 %v83616, %v8 (stack65)
        %v83623 = vadd.s32 %v83619, 4 (stack65)
        %v83627 = vadd.s32 %v83611, %v83623 (stack65)
        %v83629 = vshll.u32 %v83623, 13 (stack73)
        %v83630 = vshrl.u32 %v83623, 19 (stack74)
        %v83631 = vor.u32 %v83629, %v83630 (stack75)
        %v83632 = vxor.u32 %v83627, %v83631 (stack76)
        %v83635 = vadd.s32 %v83627, %v83632 (stack65)
        %v83637 = vshll.u32 %v83632, 15 (stack73)
        %v83638 = vshrl.u32 %v83632, 17 (stack74)
        %v83639 = vor.u32 %v83637, %v83638 (stack75)
        %v83640 = vxor.u32 %v83635, %v83639 (stack76)
        %v83643 = vadd.s32 %v83635, %v83640 (stack65)
        %v83645 = vshll.u32 %v83640, 26 (stack73)
        %v83646 = vshrl.u32 %v83640, 6 (stack74)
        %v83647 = vor.u32 %v83645, %v83646 (stack75)
        %v83648 = vxor.u32 %v83643, %v83647 (stack76)
        %v83651 = vadd.s32 %v83643, %v83648 (stack65)
        %v83655 = vadd.s32 %v83651, %v8 (stack65)
        %v83657 = vshll.u32 %v83648, 6 (stack73)
        %v83658 = vshrl.u32 %v83648, 26 (stack74)
        %v83659 = vor.u32 %v83657, %v83658 (stack75)
        %v83660 = vxor.u32 %v83651, %v83659 (stack76)
        %v83663 = vadd.s32 %v83660, %v10 (stack65)
        %v83667 = vadd.s32 %v83663, 5 (stack65)
        %v83669 = vxor.u32 %v83655, %v83667 (stack76)
        %v83670 = vand.u32.u8 %v83669, 255 (stack77)
        %v83671 = vand.u32 %v83670, 65535 (stack78)
        %v83672 = vshrl.u32 %v83671, 1 (stack79)
        %v83673 = vor.u32 %v83672, 16256 (stack75)
        %v83674 = vand.u32.u16 %v83673, 65535 (stack80)
        %v83675 = vunpack.i.l.bf16 %v83674 (stack81)
        %v83679 = vadd.f32 %v83675, -1.0 (stack82)
        %v83683 = vmul.f32 %v83679, 2.0 (stack83)
        %v83687 = vadd.f32 %v83683, -0.99609375 (stack82)
        %v83691 = vmax.f32 -0.99609375, %v83687 (stack84)
        %v83693 = vand.u32 2147483647, %v83691 (stack85)
        %vm83696 = vcmp.eq.f32.partialorder %v83693, 1.0 (stack86)
        %v83701 = vmul.f32 %v83691, inf (stack83)
        %v83703 = vxor.u32 %v83691, 2147483648 (stack87)
        %v83706 = vmul.f32 %v83691, %v83703 (stack83)
        %v83708 = vadd.f32 %v83706, 1.0 (stack88)
        %v83709 = vlog2.pop %v83708 (stack89)
        %v83710 = vmul.f32 %v83709, 0.6931472 (stack90)
        %v83711 = vmul.f32 -0.5, %v83706 (stack91)
        %v83712 = vadd.f32 %v83711, 1.0 (stack92)
        %v83713 = vmul.f32 %v83712, %v83706 (stack93)
        %v83714 = vand.u32 2147483647, %v83706 (stack94)
        %vm83715 = vcmp.lt.f32.partialorder %v83714, 0.0004427343 (stack95)
        %v83716 = vsel /*vm=*/%vm83715, /*on_true_vy=*/%v83713, /*on_false_vx=*/%v83710 (stack96)
        %v83717 = vxor.u32 %v83716, 2147483648 (stack87)
        %vm83720 = vcmp.lt.f32.partialorder %v83717, 5.0 (stack86)
        %v83725 = vsel /*vm=*/%vm83720, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v83729 = vsel /*vm=*/%vm83720, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v83733 = vsel /*vm=*/%vm83720, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v83737 = vsel /*vm=*/%vm83720, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v83741 = vsel /*vm=*/%vm83720, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v83745 = vsel /*vm=*/%vm83720, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v83749 = vsel /*vm=*/%vm83720, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v83753 = vsel /*vm=*/%vm83720, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v83757 = vsel /*vm=*/%vm83720, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v83761 = vadd.f32 %v83717, -2.5 (stack82)
        %v83763 = vrsqrt.pop %v83717 (stack97)
        %v83764 = vmul.f32 %v83717, %v83763 (stack98)
        %vm83765 = vcmp.eq.f32.partialorder %v83717, inf (stack99)
        %v83766 = vsel /*vm=*/%vm83765, /*on_true_vy=*/%v83717, /*on_false_vx=*/%v83764 (stack100)
        %vm83767 = vcmp.eq.f32.partialorder %v83717, 0.0 (stack101)
        %v83768 = vand.u32 %v83717, 2147483648 (stack102)
        %v83769 = vsel /*vm=*/%vm83767, /*on_true_vy=*/%v83768, /*on_false_vx=*/%v83766 (stack103)
        %v83772 = vadd.f32 %v83769, -3.0 (stack82)
        %v83776 = vsel /*vm=*/%vm83720, /*on_true_vy=*/%v83761, /*on_false_vx=*/%v83772 (stack72)
        %v83780 = vmul.f32 %v83757, %v83776 (stack83)
        %v83784 = vadd.f32 %v83753, %v83780 (stack82)
        %v83788 = vmul.f32 %v83784, %v83776 (stack83)
        %v83792 = vadd.f32 %v83749, %v83788 (stack82)
        %v83796 = vmul.f32 %v83792, %v83776 (stack83)
        %v83800 = vadd.f32 %v83745, %v83796 (stack82)
        %v83804 = vmul.f32 %v83800, %v83776 (stack83)
        %v83808 = vadd.f32 %v83741, %v83804 (stack82)
        %v83812 = vmul.f32 %v83808, %v83776 (stack83)
        %v83816 = vadd.f32 %v83737, %v83812 (stack82)
        %v83820 = vmul.f32 %v83816, %v83776 (stack83)
        %v83824 = vadd.f32 %v83733, %v83820 (stack82)
        %v83828 = vmul.f32 %v83824, %v83776 (stack83)
        %v83832 = vadd.f32 %v83729, %v83828 (stack82)
        %v83836 = vmul.f32 %v83832, %v83776 (stack83)
        %v83840 = vadd.f32 %v83725, %v83836 (stack82)
        %v83844 = vmul.f32 %v83840, %v83691 (stack83)
        %v83848 = vsel /*vm=*/%vm83696, /*on_true_vy=*/%v83701, /*on_false_vx=*/%v83844 (stack72)
        %v83852 = vmul.f32 %v83848, 1.4140625 (stack83)
        %s83854 = scalar_lea.vmem %s280, 344 [#allocation0] (stack107)
        %v83855 = vpack.c.bf16 0.0, %v83852 (stack104)
        %83856 = vst [vmem:[%s83854] sm:$0xf] /*vst_source=*/%v83855 (stack105)
        %v83859 = vadd.s32 %v1868, %v82473 (stack65)
        %s83861 = smul.u32 128, %s27 (stack66)
        %v83862 = vlaneseq (stack67)
        %v83863 = vand.u32 %v83862, 127 (stack68)
        %v83864 = vstv %s83861 (stack69)
        %v83865 = vadd.s32 %v83863, %v83864 (stack70)
        %v83869 = vadd.s32 %v83859, %v83865 (stack65)
        %vm83873 = vcmp.lt.u32.totalorder %v83869, %v83859 (stack71)
        %vm83878 = vcmp.lt.u32.totalorder %v83859, %v1868 (stack71)
        %v83883 = vadd.s32 %v1855, %v82456 (stack65)
        %v83887 = vadd.s32 %v83883, 1 (stack65)
        %v83891 = vsel /*vm=*/%vm83878, /*on_true_vy=*/%v83887, /*on_false_vx=*/%v83883 (stack72)
        %v83895 = vadd.s32 %v83891, 1 (stack65)
        %v83899 = vsel /*vm=*/%vm83873, /*on_true_vy=*/%v83895, /*on_false_vx=*/%v83891 (stack72)
        %v83904 = vadd.s32 %v83899, %v10 (stack65)
        %v83908 = vadd.s32 %v83869, %v9 (stack65)
        %v83912 = vadd.s32 %v83904, %v83908 (stack65)
        %v83914 = vshll.u32 %v83908, 13 (stack73)
        %v83915 = vshrl.u32 %v83908, 19 (stack74)
        %v83916 = vor.u32 %v83914, %v83915 (stack75)
        %v83917 = vxor.u32 %v83912, %v83916 (stack76)
        %v83920 = vadd.s32 %v83912, %v83917 (stack65)
        %v83922 = vshll.u32 %v83917, 15 (stack73)
        %v83923 = vshrl.u32 %v83917, 17 (stack74)
        %v83924 = vor.u32 %v83922, %v83923 (stack75)
        %v83925 = vxor.u32 %v83920, %v83924 (stack76)
        %v83928 = vadd.s32 %v83920, %v83925 (stack65)
        %v83930 = vshll.u32 %v83925, 26 (stack73)
        %v83931 = vshrl.u32 %v83925, 6 (stack74)
        %v83932 = vor.u32 %v83930, %v83931 (stack75)
        %v83933 = vxor.u32 %v83928, %v83932 (stack76)
        %v83936 = vadd.s32 %v83928, %v83933 (stack65)
        %v83940 = vadd.s32 %v83936, %v9 (stack65)
        %v83942 = vshll.u32 %v83933, 6 (stack73)
        %v83943 = vshrl.u32 %v83933, 26 (stack74)
        %v83944 = vor.u32 %v83942, %v83943 (stack75)
        %v83945 = vxor.u32 %v83936, %v83944 (stack76)
        %v83948 = vadd.s32 %v83945, %v8 (stack65)
        %v83952 = vadd.s32 %v83948, 1 (stack65)
        %v83956 = vadd.s32 %v83940, %v83952 (stack65)
        %v83958 = vshll.u32 %v83952, 17 (stack73)
        %v83959 = vshrl.u32 %v83952, 15 (stack74)
        %v83960 = vor.u32 %v83958, %v83959 (stack75)
        %v83961 = vxor.u32 %v83956, %v83960 (stack76)
        %v83964 = vadd.s32 %v83956, %v83961 (stack65)
        %v83966 = vshll.u32 %v83961, 29 (stack73)
        %v83967 = vshrl.u32 %v83961, 3 (stack74)
        %v83968 = vor.u32 %v83966, %v83967 (stack75)
        %v83969 = vxor.u32 %v83964, %v83968 (stack76)
        %v83972 = vadd.s32 %v83964, %v83969 (stack65)
        %v83974 = vshll.u32 %v83969, 16 (stack73)
        %v83975 = vshrl.u32 %v83969, 16 (stack74)
        %v83976 = vor.u32 %v83974, %v83975 (stack75)
        %v83977 = vxor.u32 %v83972, %v83976 (stack76)
        %v83980 = vadd.s32 %v83972, %v83977 (stack65)
        %v83984 = vadd.s32 %v83980, %v8 (stack65)
        %v83986 = vshll.u32 %v83977, 24 (stack73)
        %v83987 = vshrl.u32 %v83977, 8 (stack74)
        %v83988 = vor.u32 %v83986, %v83987 (stack75)
        %v83989 = vxor.u32 %v83980, %v83988 (stack76)
        %v83992 = vadd.s32 %v83989, %v10 (stack65)
        %v83996 = vadd.s32 %v83992, 2 (stack65)
        %v84000 = vadd.s32 %v83984, %v83996 (stack65)
        %v84002 = vshll.u32 %v83996, 13 (stack73)
        %v84003 = vshrl.u32 %v83996, 19 (stack74)
        %v84004 = vor.u32 %v84002, %v84003 (stack75)
        %v84005 = vxor.u32 %v84000, %v84004 (stack76)
        %v84008 = vadd.s32 %v84000, %v84005 (stack65)
        %v84010 = vshll.u32 %v84005, 15 (stack73)
        %v84011 = vshrl.u32 %v84005, 17 (stack74)
        %v84012 = vor.u32 %v84010, %v84011 (stack75)
        %v84013 = vxor.u32 %v84008, %v84012 (stack76)
        %v84016 = vadd.s32 %v84008, %v84013 (stack65)
        %v84018 = vshll.u32 %v84013, 26 (stack73)
        %v84019 = vshrl.u32 %v84013, 6 (stack74)
        %v84020 = vor.u32 %v84018, %v84019 (stack75)
        %v84021 = vxor.u32 %v84016, %v84020 (stack76)
        %v84024 = vadd.s32 %v84016, %v84021 (stack65)
        %v84028 = vadd.s32 %v84024, %v10 (stack65)
        %v84030 = vshll.u32 %v84021, 6 (stack73)
        %v84031 = vshrl.u32 %v84021, 26 (stack74)
        %v84032 = vor.u32 %v84030, %v84031 (stack75)
        %v84033 = vxor.u32 %v84024, %v84032 (stack76)
        %v84036 = vadd.s32 %v84033, %v9 (stack65)
        %v84040 = vadd.s32 %v84036, 3 (stack65)
        %v84044 = vadd.s32 %v84028, %v84040 (stack65)
        %v84046 = vshll.u32 %v84040, 17 (stack73)
        %v84047 = vshrl.u32 %v84040, 15 (stack74)
        %v84048 = vor.u32 %v84046, %v84047 (stack75)
        %v84049 = vxor.u32 %v84044, %v84048 (stack76)
        %v84052 = vadd.s32 %v84044, %v84049 (stack65)
        %v84054 = vshll.u32 %v84049, 29 (stack73)
        %v84055 = vshrl.u32 %v84049, 3 (stack74)
        %v84056 = vor.u32 %v84054, %v84055 (stack75)
        %v84057 = vxor.u32 %v84052, %v84056 (stack76)
        %v84060 = vadd.s32 %v84052, %v84057 (stack65)
        %v84062 = vshll.u32 %v84057, 16 (stack73)
        %v84063 = vshrl.u32 %v84057, 16 (stack74)
        %v84064 = vor.u32 %v84062, %v84063 (stack75)
        %v84065 = vxor.u32 %v84060, %v84064 (stack76)
        %v84068 = vadd.s32 %v84060, %v84065 (stack65)
        %v84072 = vadd.s32 %v84068, %v9 (stack65)
        %v84074 = vshll.u32 %v84065, 24 (stack73)
        %v84075 = vshrl.u32 %v84065, 8 (stack74)
        %v84076 = vor.u32 %v84074, %v84075 (stack75)
        %v84077 = vxor.u32 %v84068, %v84076 (stack76)
        %v84080 = vadd.s32 %v84077, %v8 (stack65)
        %v84084 = vadd.s32 %v84080, 4 (stack65)
        %v84088 = vadd.s32 %v84072, %v84084 (stack65)
        %v84090 = vshll.u32 %v84084, 13 (stack73)
        %v84091 = vshrl.u32 %v84084, 19 (stack74)
        %v84092 = vor.u32 %v84090, %v84091 (stack75)
        %v84093 = vxor.u32 %v84088, %v84092 (stack76)
        %v84096 = vadd.s32 %v84088, %v84093 (stack65)
        %v84098 = vshll.u32 %v84093, 15 (stack73)
        %v84099 = vshrl.u32 %v84093, 17 (stack74)
        %v84100 = vor.u32 %v84098, %v84099 (stack75)
        %v84101 = vxor.u32 %v84096, %v84100 (stack76)
        %v84104 = vadd.s32 %v84096, %v84101 (stack65)
        %v84106 = vshll.u32 %v84101, 26 (stack73)
        %v84107 = vshrl.u32 %v84101, 6 (stack74)
        %v84108 = vor.u32 %v84106, %v84107 (stack75)
        %v84109 = vxor.u32 %v84104, %v84108 (stack76)
        %v84112 = vadd.s32 %v84104, %v84109 (stack65)
        %v84116 = vadd.s32 %v84112, %v8 (stack65)
        %v84118 = vshll.u32 %v84109, 6 (stack73)
        %v84119 = vshrl.u32 %v84109, 26 (stack74)
        %v84120 = vor.u32 %v84118, %v84119 (stack75)
        %v84121 = vxor.u32 %v84112, %v84120 (stack76)
        %v84124 = vadd.s32 %v84121, %v10 (stack65)
        %v84128 = vadd.s32 %v84124, 5 (stack65)
        %v84130 = vxor.u32 %v84116, %v84128 (stack76)
        %v84131 = vand.u32.u8 %v84130, 255 (stack77)
        %v84132 = vand.u32 %v84131, 65535 (stack78)
        %v84133 = vshrl.u32 %v84132, 1 (stack79)
        %v84134 = vor.u32 %v84133, 16256 (stack75)
        %v84135 = vand.u32.u16 %v84134, 65535 (stack80)
        %v84136 = vunpack.i.l.bf16 %v84135 (stack81)
        %v84140 = vadd.f32 %v84136, -1.0 (stack82)
        %v84144 = vmul.f32 %v84140, 2.0 (stack83)
        %v84148 = vadd.f32 %v84144, -0.99609375 (stack82)
        %v84152 = vmax.f32 -0.99609375, %v84148 (stack84)
        %v84154 = vand.u32 2147483647, %v84152 (stack85)
        %vm84157 = vcmp.eq.f32.partialorder %v84154, 1.0 (stack86)
        %v84162 = vmul.f32 %v84152, inf (stack83)
        %v84164 = vxor.u32 %v84152, 2147483648 (stack87)
        %v84167 = vmul.f32 %v84152, %v84164 (stack83)
        %v84169 = vadd.f32 %v84167, 1.0 (stack88)
        %v84170 = vlog2.pop %v84169 (stack89)
        %v84171 = vmul.f32 %v84170, 0.6931472 (stack90)
        %v84172 = vmul.f32 -0.5, %v84167 (stack91)
        %v84173 = vadd.f32 %v84172, 1.0 (stack92)
        %v84174 = vmul.f32 %v84173, %v84167 (stack93)
        %v84175 = vand.u32 2147483647, %v84167 (stack94)
        %vm84176 = vcmp.lt.f32.partialorder %v84175, 0.0004427343 (stack95)
        %v84177 = vsel /*vm=*/%vm84176, /*on_true_vy=*/%v84174, /*on_false_vx=*/%v84171 (stack96)
        %v84178 = vxor.u32 %v84177, 2147483648 (stack87)
        %vm84181 = vcmp.lt.f32.partialorder %v84178, 5.0 (stack86)
        %v84186 = vsel /*vm=*/%vm84181, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v84190 = vsel /*vm=*/%vm84181, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v84194 = vsel /*vm=*/%vm84181, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v84198 = vsel /*vm=*/%vm84181, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v84202 = vsel /*vm=*/%vm84181, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v84206 = vsel /*vm=*/%vm84181, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v84210 = vsel /*vm=*/%vm84181, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v84214 = vsel /*vm=*/%vm84181, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v84218 = vsel /*vm=*/%vm84181, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v84222 = vadd.f32 %v84178, -2.5 (stack82)
        %v84224 = vrsqrt.pop %v84178 (stack97)
        %v84225 = vmul.f32 %v84178, %v84224 (stack98)
        %vm84226 = vcmp.eq.f32.partialorder %v84178, inf (stack99)
        %v84227 = vsel /*vm=*/%vm84226, /*on_true_vy=*/%v84178, /*on_false_vx=*/%v84225 (stack100)
        %vm84228 = vcmp.eq.f32.partialorder %v84178, 0.0 (stack101)
        %v84229 = vand.u32 %v84178, 2147483648 (stack102)
        %v84230 = vsel /*vm=*/%vm84228, /*on_true_vy=*/%v84229, /*on_false_vx=*/%v84227 (stack103)
        %v84233 = vadd.f32 %v84230, -3.0 (stack82)
        %v84237 = vsel /*vm=*/%vm84181, /*on_true_vy=*/%v84222, /*on_false_vx=*/%v84233 (stack72)
        %v84241 = vmul.f32 %v84218, %v84237 (stack83)
        %v84245 = vadd.f32 %v84214, %v84241 (stack82)
        %v84249 = vmul.f32 %v84245, %v84237 (stack83)
        %v84253 = vadd.f32 %v84210, %v84249 (stack82)
        %v84257 = vmul.f32 %v84253, %v84237 (stack83)
        %v84261 = vadd.f32 %v84206, %v84257 (stack82)
        %v84265 = vmul.f32 %v84261, %v84237 (stack83)
        %v84269 = vadd.f32 %v84202, %v84265 (stack82)
        %v84273 = vmul.f32 %v84269, %v84237 (stack83)
        %v84277 = vadd.f32 %v84198, %v84273 (stack82)
        %v84281 = vmul.f32 %v84277, %v84237 (stack83)
        %v84285 = vadd.f32 %v84194, %v84281 (stack82)
        %v84289 = vmul.f32 %v84285, %v84237 (stack83)
        %v84293 = vadd.f32 %v84190, %v84289 (stack82)
        %v84297 = vmul.f32 %v84293, %v84237 (stack83)
        %v84301 = vadd.f32 %v84186, %v84297 (stack82)
        %v84305 = vmul.f32 %v84301, %v84152 (stack83)
        %v84309 = vsel /*vm=*/%vm84157, /*on_true_vy=*/%v84162, /*on_false_vx=*/%v84305 (stack72)
        %v84313 = vmul.f32 %v84309, 1.4140625 (stack83)
        %s84315 = scalar_lea.vmem %s280, 472 [#allocation0] (stack107)
        %v84316 = vpack.c.bf16 0.0, %v84313 (stack104)
        %84317 = vst [vmem:[%s84315] sm:$0xf] /*vst_source=*/%v84316 (stack105)
        %v84320 = vadd.s32 %v2355, %v82473 (stack65)
        %s84322 = smul.u32 128, %s27 (stack66)
        %v84323 = vlaneseq (stack67)
        %v84324 = vand.u32 %v84323, 127 (stack68)
        %v84325 = vstv %s84322 (stack69)
        %v84326 = vadd.s32 %v84324, %v84325 (stack70)
        %v84330 = vadd.s32 %v84320, %v84326 (stack65)
        %vm84334 = vcmp.lt.u32.totalorder %v84330, %v84320 (stack71)
        %vm84339 = vcmp.lt.u32.totalorder %v84320, %v2355 (stack71)
        %v84344 = vadd.s32 %v2342, %v82456 (stack65)
        %v84348 = vadd.s32 %v84344, 1 (stack65)
        %v84352 = vsel /*vm=*/%vm84339, /*on_true_vy=*/%v84348, /*on_false_vx=*/%v84344 (stack72)
        %v84356 = vadd.s32 %v84352, 1 (stack65)
        %v84360 = vsel /*vm=*/%vm84334, /*on_true_vy=*/%v84356, /*on_false_vx=*/%v84352 (stack72)
        %v84365 = vadd.s32 %v84360, %v10 (stack65)
        %v84369 = vadd.s32 %v84330, %v9 (stack65)
        %v84373 = vadd.s32 %v84365, %v84369 (stack65)
        %v84375 = vshll.u32 %v84369, 13 (stack73)
        %v84376 = vshrl.u32 %v84369, 19 (stack74)
        %v84377 = vor.u32 %v84375, %v84376 (stack75)
        %v84378 = vxor.u32 %v84373, %v84377 (stack76)
        %v84381 = vadd.s32 %v84373, %v84378 (stack65)
        %v84383 = vshll.u32 %v84378, 15 (stack73)
        %v84384 = vshrl.u32 %v84378, 17 (stack74)
        %v84385 = vor.u32 %v84383, %v84384 (stack75)
        %v84386 = vxor.u32 %v84381, %v84385 (stack76)
        %v84389 = vadd.s32 %v84381, %v84386 (stack65)
        %v84391 = vshll.u32 %v84386, 26 (stack73)
        %v84392 = vshrl.u32 %v84386, 6 (stack74)
        %v84393 = vor.u32 %v84391, %v84392 (stack75)
        %v84394 = vxor.u32 %v84389, %v84393 (stack76)
        %v84397 = vadd.s32 %v84389, %v84394 (stack65)
        %v84401 = vadd.s32 %v84397, %v9 (stack65)
        %v84403 = vshll.u32 %v84394, 6 (stack73)
        %v84404 = vshrl.u32 %v84394, 26 (stack74)
        %v84405 = vor.u32 %v84403, %v84404 (stack75)
        %v84406 = vxor.u32 %v84397, %v84405 (stack76)
        %v84409 = vadd.s32 %v84406, %v8 (stack65)
        %v84413 = vadd.s32 %v84409, 1 (stack65)
        %v84417 = vadd.s32 %v84401, %v84413 (stack65)
        %v84419 = vshll.u32 %v84413, 17 (stack73)
        %v84420 = vshrl.u32 %v84413, 15 (stack74)
        %v84421 = vor.u32 %v84419, %v84420 (stack75)
        %v84422 = vxor.u32 %v84417, %v84421 (stack76)
        %v84425 = vadd.s32 %v84417, %v84422 (stack65)
        %v84427 = vshll.u32 %v84422, 29 (stack73)
        %v84428 = vshrl.u32 %v84422, 3 (stack74)
        %v84429 = vor.u32 %v84427, %v84428 (stack75)
        %v84430 = vxor.u32 %v84425, %v84429 (stack76)
        %v84433 = vadd.s32 %v84425, %v84430 (stack65)
        %v84435 = vshll.u32 %v84430, 16 (stack73)
        %v84436 = vshrl.u32 %v84430, 16 (stack74)
        %v84437 = vor.u32 %v84435, %v84436 (stack75)
        %v84438 = vxor.u32 %v84433, %v84437 (stack76)
        %v84441 = vadd.s32 %v84433, %v84438 (stack65)
        %v84445 = vadd.s32 %v84441, %v8 (stack65)
        %v84447 = vshll.u32 %v84438, 24 (stack73)
        %v84448 = vshrl.u32 %v84438, 8 (stack74)
        %v84449 = vor.u32 %v84447, %v84448 (stack75)
        %v84450 = vxor.u32 %v84441, %v84449 (stack76)
        %v84453 = vadd.s32 %v84450, %v10 (stack65)
        %v84457 = vadd.s32 %v84453, 2 (stack65)
        %v84461 = vadd.s32 %v84445, %v84457 (stack65)
        %v84463 = vshll.u32 %v84457, 13 (stack73)
        %v84464 = vshrl.u32 %v84457, 19 (stack74)
        %v84465 = vor.u32 %v84463, %v84464 (stack75)
        %v84466 = vxor.u32 %v84461, %v84465 (stack76)
        %v84469 = vadd.s32 %v84461, %v84466 (stack65)
        %v84471 = vshll.u32 %v84466, 15 (stack73)
        %v84472 = vshrl.u32 %v84466, 17 (stack74)
        %v84473 = vor.u32 %v84471, %v84472 (stack75)
        %v84474 = vxor.u32 %v84469, %v84473 (stack76)
        %v84477 = vadd.s32 %v84469, %v84474 (stack65)
        %v84479 = vshll.u32 %v84474, 26 (stack73)
        %v84480 = vshrl.u32 %v84474, 6 (stack74)
        %v84481 = vor.u32 %v84479, %v84480 (stack75)
        %v84482 = vxor.u32 %v84477, %v84481 (stack76)
        %v84485 = vadd.s32 %v84477, %v84482 (stack65)
        %v84489 = vadd.s32 %v84485, %v10 (stack65)
        %v84491 = vshll.u32 %v84482, 6 (stack73)
        %v84492 = vshrl.u32 %v84482, 26 (stack74)
        %v84493 = vor.u32 %v84491, %v84492 (stack75)
        %v84494 = vxor.u32 %v84485, %v84493 (stack76)
        %v84497 = vadd.s32 %v84494, %v9 (stack65)
        %v84501 = vadd.s32 %v84497, 3 (stack65)
        %v84505 = vadd.s32 %v84489, %v84501 (stack65)
        %v84507 = vshll.u32 %v84501, 17 (stack73)
        %v84508 = vshrl.u32 %v84501, 15 (stack74)
        %v84509 = vor.u32 %v84507, %v84508 (stack75)
        %v84510 = vxor.u32 %v84505, %v84509 (stack76)
        %v84513 = vadd.s32 %v84505, %v84510 (stack65)
        %v84515 = vshll.u32 %v84510, 29 (stack73)
        %v84516 = vshrl.u32 %v84510, 3 (stack74)
        %v84517 = vor.u32 %v84515, %v84516 (stack75)
        %v84518 = vxor.u32 %v84513, %v84517 (stack76)
        %v84521 = vadd.s32 %v84513, %v84518 (stack65)
        %v84523 = vshll.u32 %v84518, 16 (stack73)
        %v84524 = vshrl.u32 %v84518, 16 (stack74)
        %v84525 = vor.u32 %v84523, %v84524 (stack75)
        %v84526 = vxor.u32 %v84521, %v84525 (stack76)
        %v84529 = vadd.s32 %v84521, %v84526 (stack65)
        %v84533 = vadd.s32 %v84529, %v9 (stack65)
        %v84535 = vshll.u32 %v84526, 24 (stack73)
        %v84536 = vshrl.u32 %v84526, 8 (stack74)
        %v84537 = vor.u32 %v84535, %v84536 (stack75)
        %v84538 = vxor.u32 %v84529, %v84537 (stack76)
        %v84541 = vadd.s32 %v84538, %v8 (stack65)
        %v84545 = vadd.s32 %v84541, 4 (stack65)
        %v84549 = vadd.s32 %v84533, %v84545 (stack65)
        %v84551 = vshll.u32 %v84545, 13 (stack73)
        %v84552 = vshrl.u32 %v84545, 19 (stack74)
        %v84553 = vor.u32 %v84551, %v84552 (stack75)
        %v84554 = vxor.u32 %v84549, %v84553 (stack76)
        %v84557 = vadd.s32 %v84549, %v84554 (stack65)
        %v84559 = vshll.u32 %v84554, 15 (stack73)
        %v84560 = vshrl.u32 %v84554, 17 (stack74)
        %v84561 = vor.u32 %v84559, %v84560 (stack75)
        %v84562 = vxor.u32 %v84557, %v84561 (stack76)
        %v84565 = vadd.s32 %v84557, %v84562 (stack65)
        %v84567 = vshll.u32 %v84562, 26 (stack73)
        %v84568 = vshrl.u32 %v84562, 6 (stack74)
        %v84569 = vor.u32 %v84567, %v84568 (stack75)
        %v84570 = vxor.u32 %v84565, %v84569 (stack76)
        %v84573 = vadd.s32 %v84565, %v84570 (stack65)
        %v84577 = vadd.s32 %v84573, %v8 (stack65)
        %v84579 = vshll.u32 %v84570, 6 (stack73)
        %v84580 = vshrl.u32 %v84570, 26 (stack74)
        %v84581 = vor.u32 %v84579, %v84580 (stack75)
        %v84582 = vxor.u32 %v84573, %v84581 (stack76)
        %v84585 = vadd.s32 %v84582, %v10 (stack65)
        %v84589 = vadd.s32 %v84585, 5 (stack65)
        %v84591 = vxor.u32 %v84577, %v84589 (stack76)
        %v84592 = vand.u32.u8 %v84591, 255 (stack77)
        %v84593 = vand.u32 %v84592, 65535 (stack78)
        %v84594 = vshrl.u32 %v84593, 1 (stack79)
        %v84595 = vor.u32 %v84594, 16256 (stack75)
        %v84596 = vand.u32.u16 %v84595, 65535 (stack80)
        %v84597 = vunpack.i.l.bf16 %v84596 (stack81)
        %v84601 = vadd.f32 %v84597, -1.0 (stack82)
        %v84605 = vmul.f32 %v84601, 2.0 (stack83)
        %v84609 = vadd.f32 %v84605, -0.99609375 (stack82)
        %v84613 = vmax.f32 -0.99609375, %v84609 (stack84)
        %v84615 = vand.u32 2147483647, %v84613 (stack85)
        %vm84618 = vcmp.eq.f32.partialorder %v84615, 1.0 (stack86)
        %v84623 = vmul.f32 %v84613, inf (stack83)
        %v84625 = vxor.u32 %v84613, 2147483648 (stack87)
        %v84628 = vmul.f32 %v84613, %v84625 (stack83)
        %v84630 = vadd.f32 %v84628, 1.0 (stack88)
        %v84631 = vlog2.pop %v84630 (stack89)
        %v84632 = vmul.f32 %v84631, 0.6931472 (stack90)
        %v84633 = vmul.f32 -0.5, %v84628 (stack91)
        %v84634 = vadd.f32 %v84633, 1.0 (stack92)
        %v84635 = vmul.f32 %v84634, %v84628 (stack93)
        %v84636 = vand.u32 2147483647, %v84628 (stack94)
        %vm84637 = vcmp.lt.f32.partialorder %v84636, 0.0004427343 (stack95)
        %v84638 = vsel /*vm=*/%vm84637, /*on_true_vy=*/%v84635, /*on_false_vx=*/%v84632 (stack96)
        %v84639 = vxor.u32 %v84638, 2147483648 (stack87)
        %vm84642 = vcmp.lt.f32.partialorder %v84639, 5.0 (stack86)
        %v84647 = vsel /*vm=*/%vm84642, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v84651 = vsel /*vm=*/%vm84642, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v84655 = vsel /*vm=*/%vm84642, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v84659 = vsel /*vm=*/%vm84642, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v84663 = vsel /*vm=*/%vm84642, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v84667 = vsel /*vm=*/%vm84642, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v84671 = vsel /*vm=*/%vm84642, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v84675 = vsel /*vm=*/%vm84642, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v84679 = vsel /*vm=*/%vm84642, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v84683 = vadd.f32 %v84639, -2.5 (stack82)
        %v84685 = vrsqrt.pop %v84639 (stack97)
        %v84686 = vmul.f32 %v84639, %v84685 (stack98)
        %vm84687 = vcmp.eq.f32.partialorder %v84639, inf (stack99)
        %v84688 = vsel /*vm=*/%vm84687, /*on_true_vy=*/%v84639, /*on_false_vx=*/%v84686 (stack100)
        %vm84689 = vcmp.eq.f32.partialorder %v84639, 0.0 (stack101)
        %v84690 = vand.u32 %v84639, 2147483648 (stack102)
        %v84691 = vsel /*vm=*/%vm84689, /*on_true_vy=*/%v84690, /*on_false_vx=*/%v84688 (stack103)
        %v84694 = vadd.f32 %v84691, -3.0 (stack82)
        %v84698 = vsel /*vm=*/%vm84642, /*on_true_vy=*/%v84683, /*on_false_vx=*/%v84694 (stack72)
        %v84702 = vmul.f32 %v84679, %v84698 (stack83)
        %v84706 = vadd.f32 %v84675, %v84702 (stack82)
        %v84710 = vmul.f32 %v84706, %v84698 (stack83)
        %v84714 = vadd.f32 %v84671, %v84710 (stack82)
        %v84718 = vmul.f32 %v84714, %v84698 (stack83)
        %v84722 = vadd.f32 %v84667, %v84718 (stack82)
        %v84726 = vmul.f32 %v84722, %v84698 (stack83)
        %v84730 = vadd.f32 %v84663, %v84726 (stack82)
        %v84734 = vmul.f32 %v84730, %v84698 (stack83)
        %v84738 = vadd.f32 %v84659, %v84734 (stack82)
        %v84742 = vmul.f32 %v84738, %v84698 (stack83)
        %v84746 = vadd.f32 %v84655, %v84742 (stack82)
        %v84750 = vmul.f32 %v84746, %v84698 (stack83)
        %v84754 = vadd.f32 %v84651, %v84750 (stack82)
        %v84758 = vmul.f32 %v84754, %v84698 (stack83)
        %v84762 = vadd.f32 %v84647, %v84758 (stack82)
        %v84766 = vmul.f32 %v84762, %v84613 (stack83)
        %v84770 = vsel /*vm=*/%vm84618, /*on_true_vy=*/%v84623, /*on_false_vx=*/%v84766 (stack72)
        %v84774 = vmul.f32 %v84770, 1.4140625 (stack83)
        %s84776 = scalar_lea.vmem %s280, 600 [#allocation0] (stack107)
        %v84777 = vpack.c.bf16 0.0, %v84774 (stack104)
        %84778 = vst [vmem:[%s84776] sm:$0xf] /*vst_source=*/%v84777 (stack105)
        %v84781 = vadd.s32 %v2842, %v82473 (stack65)
        %s84783 = smul.u32 128, %s27 (stack66)
        %v84784 = vlaneseq (stack67)
        %v84785 = vand.u32 %v84784, 127 (stack68)
        %v84786 = vstv %s84783 (stack69)
        %v84787 = vadd.s32 %v84785, %v84786 (stack70)
        %v84791 = vadd.s32 %v84781, %v84787 (stack65)
        %vm84795 = vcmp.lt.u32.totalorder %v84791, %v84781 (stack71)
        %vm84800 = vcmp.lt.u32.totalorder %v84781, %v2842 (stack71)
        %v84805 = vadd.s32 %v2829, %v82456 (stack65)
        %v84809 = vadd.s32 %v84805, 1 (stack65)
        %v84813 = vsel /*vm=*/%vm84800, /*on_true_vy=*/%v84809, /*on_false_vx=*/%v84805 (stack72)
        %v84817 = vadd.s32 %v84813, 1 (stack65)
        %v84821 = vsel /*vm=*/%vm84795, /*on_true_vy=*/%v84817, /*on_false_vx=*/%v84813 (stack72)
        %v84826 = vadd.s32 %v84821, %v10 (stack65)
        %v84830 = vadd.s32 %v84791, %v9 (stack65)
        %v84834 = vadd.s32 %v84826, %v84830 (stack65)
        %v84836 = vshll.u32 %v84830, 13 (stack73)
        %v84837 = vshrl.u32 %v84830, 19 (stack74)
        %v84838 = vor.u32 %v84836, %v84837 (stack75)
        %v84839 = vxor.u32 %v84834, %v84838 (stack76)
        %v84842 = vadd.s32 %v84834, %v84839 (stack65)
        %v84844 = vshll.u32 %v84839, 15 (stack73)
        %v84845 = vshrl.u32 %v84839, 17 (stack74)
        %v84846 = vor.u32 %v84844, %v84845 (stack75)
        %v84847 = vxor.u32 %v84842, %v84846 (stack76)
        %v84850 = vadd.s32 %v84842, %v84847 (stack65)
        %v84852 = vshll.u32 %v84847, 26 (stack73)
        %v84853 = vshrl.u32 %v84847, 6 (stack74)
        %v84854 = vor.u32 %v84852, %v84853 (stack75)
        %v84855 = vxor.u32 %v84850, %v84854 (stack76)
        %v84858 = vadd.s32 %v84850, %v84855 (stack65)
        %v84862 = vadd.s32 %v84858, %v9 (stack65)
        %v84864 = vshll.u32 %v84855, 6 (stack73)
        %v84865 = vshrl.u32 %v84855, 26 (stack74)
        %v84866 = vor.u32 %v84864, %v84865 (stack75)
        %v84867 = vxor.u32 %v84858, %v84866 (stack76)
        %v84870 = vadd.s32 %v84867, %v8 (stack65)
        %v84874 = vadd.s32 %v84870, 1 (stack65)
        %v84878 = vadd.s32 %v84862, %v84874 (stack65)
        %v84880 = vshll.u32 %v84874, 17 (stack73)
        %v84881 = vshrl.u32 %v84874, 15 (stack74)
        %v84882 = vor.u32 %v84880, %v84881 (stack75)
        %v84883 = vxor.u32 %v84878, %v84882 (stack76)
        %v84886 = vadd.s32 %v84878, %v84883 (stack65)
        %v84888 = vshll.u32 %v84883, 29 (stack73)
        %v84889 = vshrl.u32 %v84883, 3 (stack74)
        %v84890 = vor.u32 %v84888, %v84889 (stack75)
        %v84891 = vxor.u32 %v84886, %v84890 (stack76)
        %v84894 = vadd.s32 %v84886, %v84891 (stack65)
        %v84896 = vshll.u32 %v84891, 16 (stack73)
        %v84897 = vshrl.u32 %v84891, 16 (stack74)
        %v84898 = vor.u32 %v84896, %v84897 (stack75)
        %v84899 = vxor.u32 %v84894, %v84898 (stack76)
        %v84902 = vadd.s32 %v84894, %v84899 (stack65)
        %v84906 = vadd.s32 %v84902, %v8 (stack65)
        %v84908 = vshll.u32 %v84899, 24 (stack73)
        %v84909 = vshrl.u32 %v84899, 8 (stack74)
        %v84910 = vor.u32 %v84908, %v84909 (stack75)
        %v84911 = vxor.u32 %v84902, %v84910 (stack76)
        %v84914 = vadd.s32 %v84911, %v10 (stack65)
        %v84918 = vadd.s32 %v84914, 2 (stack65)
        %v84922 = vadd.s32 %v84906, %v84918 (stack65)
        %v84924 = vshll.u32 %v84918, 13 (stack73)
        %v84925 = vshrl.u32 %v84918, 19 (stack74)
        %v84926 = vor.u32 %v84924, %v84925 (stack75)
        %v84927 = vxor.u32 %v84922, %v84926 (stack76)
        %v84930 = vadd.s32 %v84922, %v84927 (stack65)
        %v84932 = vshll.u32 %v84927, 15 (stack73)
        %v84933 = vshrl.u32 %v84927, 17 (stack74)
        %v84934 = vor.u32 %v84932, %v84933 (stack75)
        %v84935 = vxor.u32 %v84930, %v84934 (stack76)
        %v84938 = vadd.s32 %v84930, %v84935 (stack65)
        %v84940 = vshll.u32 %v84935, 26 (stack73)
        %v84941 = vshrl.u32 %v84935, 6 (stack74)
        %v84942 = vor.u32 %v84940, %v84941 (stack75)
        %v84943 = vxor.u32 %v84938, %v84942 (stack76)
        %v84946 = vadd.s32 %v84938, %v84943 (stack65)
        %v84950 = vadd.s32 %v84946, %v10 (stack65)
        %v84952 = vshll.u32 %v84943, 6 (stack73)
        %v84953 = vshrl.u32 %v84943, 26 (stack74)
        %v84954 = vor.u32 %v84952, %v84953 (stack75)
        %v84955 = vxor.u32 %v84946, %v84954 (stack76)
        %v84958 = vadd.s32 %v84955, %v9 (stack65)
        %v84962 = vadd.s32 %v84958, 3 (stack65)
        %v84966 = vadd.s32 %v84950, %v84962 (stack65)
        %v84968 = vshll.u32 %v84962, 17 (stack73)
        %v84969 = vshrl.u32 %v84962, 15 (stack74)
        %v84970 = vor.u32 %v84968, %v84969 (stack75)
        %v84971 = vxor.u32 %v84966, %v84970 (stack76)
        %v84974 = vadd.s32 %v84966, %v84971 (stack65)
        %v84976 = vshll.u32 %v84971, 29 (stack73)
        %v84977 = vshrl.u32 %v84971, 3 (stack74)
        %v84978 = vor.u32 %v84976, %v84977 (stack75)
        %v84979 = vxor.u32 %v84974, %v84978 (stack76)
        %v84982 = vadd.s32 %v84974, %v84979 (stack65)
        %v84984 = vshll.u32 %v84979, 16 (stack73)
        %v84985 = vshrl.u32 %v84979, 16 (stack74)
        %v84986 = vor.u32 %v84984, %v84985 (stack75)
        %v84987 = vxor.u32 %v84982, %v84986 (stack76)
        %v84990 = vadd.s32 %v84982, %v84987 (stack65)
        %v84994 = vadd.s32 %v84990, %v9 (stack65)
        %v84996 = vshll.u32 %v84987, 24 (stack73)
        %v84997 = vshrl.u32 %v84987, 8 (stack74)
        %v84998 = vor.u32 %v84996, %v84997 (stack75)
        %v84999 = vxor.u32 %v84990, %v84998 (stack76)
        %v85002 = vadd.s32 %v84999, %v8 (stack65)
        %v85006 = vadd.s32 %v85002, 4 (stack65)
        %v85010 = vadd.s32 %v84994, %v85006 (stack65)
        %v85012 = vshll.u32 %v85006, 13 (stack73)
        %v85013 = vshrl.u32 %v85006, 19 (stack74)
        %v85014 = vor.u32 %v85012, %v85013 (stack75)
        %v85015 = vxor.u32 %v85010, %v85014 (stack76)
        %v85018 = vadd.s32 %v85010, %v85015 (stack65)
        %v85020 = vshll.u32 %v85015, 15 (stack73)
        %v85021 = vshrl.u32 %v85015, 17 (stack74)
        %v85022 = vor.u32 %v85020, %v85021 (stack75)
        %v85023 = vxor.u32 %v85018, %v85022 (stack76)
        %v85026 = vadd.s32 %v85018, %v85023 (stack65)
        %v85028 = vshll.u32 %v85023, 26 (stack73)
        %v85029 = vshrl.u32 %v85023, 6 (stack74)
        %v85030 = vor.u32 %v85028, %v85029 (stack75)
        %v85031 = vxor.u32 %v85026, %v85030 (stack76)
        %v85034 = vadd.s32 %v85026, %v85031 (stack65)
        %v85038 = vadd.s32 %v85034, %v8 (stack65)
        %v85040 = vshll.u32 %v85031, 6 (stack73)
        %v85041 = vshrl.u32 %v85031, 26 (stack74)
        %v85042 = vor.u32 %v85040, %v85041 (stack75)
        %v85043 = vxor.u32 %v85034, %v85042 (stack76)
        %v85046 = vadd.s32 %v85043, %v10 (stack65)
        %v85050 = vadd.s32 %v85046, 5 (stack65)
        %v85052 = vxor.u32 %v85038, %v85050 (stack76)
        %v85053 = vand.u32.u8 %v85052, 255 (stack77)
        %v85054 = vand.u32 %v85053, 65535 (stack78)
        %v85055 = vshrl.u32 %v85054, 1 (stack79)
        %v85056 = vor.u32 %v85055, 16256 (stack75)
        %v85057 = vand.u32.u16 %v85056, 65535 (stack80)
        %v85058 = vunpack.i.l.bf16 %v85057 (stack81)
        %v85062 = vadd.f32 %v85058, -1.0 (stack82)
        %v85066 = vmul.f32 %v85062, 2.0 (stack83)
        %v85070 = vadd.f32 %v85066, -0.99609375 (stack82)
        %v85074 = vmax.f32 -0.99609375, %v85070 (stack84)
        %v85076 = vand.u32 2147483647, %v85074 (stack85)
        %vm85079 = vcmp.eq.f32.partialorder %v85076, 1.0 (stack86)
        %v85084 = vmul.f32 %v85074, inf (stack83)
        %v85086 = vxor.u32 %v85074, 2147483648 (stack87)
        %v85089 = vmul.f32 %v85074, %v85086 (stack83)
        %v85091 = vadd.f32 %v85089, 1.0 (stack88)
        %v85092 = vlog2.pop %v85091 (stack89)
        %v85093 = vmul.f32 %v85092, 0.6931472 (stack90)
        %v85094 = vmul.f32 -0.5, %v85089 (stack91)
        %v85095 = vadd.f32 %v85094, 1.0 (stack92)
        %v85096 = vmul.f32 %v85095, %v85089 (stack93)
        %v85097 = vand.u32 2147483647, %v85089 (stack94)
        %vm85098 = vcmp.lt.f32.partialorder %v85097, 0.0004427343 (stack95)
        %v85099 = vsel /*vm=*/%vm85098, /*on_true_vy=*/%v85096, /*on_false_vx=*/%v85093 (stack96)
        %v85100 = vxor.u32 %v85099, 2147483648 (stack87)
        %vm85103 = vcmp.lt.f32.partialorder %v85100, 5.0 (stack86)
        %v85108 = vsel /*vm=*/%vm85103, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v85112 = vsel /*vm=*/%vm85103, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v85116 = vsel /*vm=*/%vm85103, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v85120 = vsel /*vm=*/%vm85103, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v85124 = vsel /*vm=*/%vm85103, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v85128 = vsel /*vm=*/%vm85103, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v85132 = vsel /*vm=*/%vm85103, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v85136 = vsel /*vm=*/%vm85103, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v85140 = vsel /*vm=*/%vm85103, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v85144 = vadd.f32 %v85100, -2.5 (stack82)
        %v85146 = vrsqrt.pop %v85100 (stack97)
        %v85147 = vmul.f32 %v85100, %v85146 (stack98)
        %vm85148 = vcmp.eq.f32.partialorder %v85100, inf (stack99)
        %v85149 = vsel /*vm=*/%vm85148, /*on_true_vy=*/%v85100, /*on_false_vx=*/%v85147 (stack100)
        %vm85150 = vcmp.eq.f32.partialorder %v85100, 0.0 (stack101)
        %v85151 = vand.u32 %v85100, 2147483648 (stack102)
        %v85152 = vsel /*vm=*/%vm85150, /*on_true_vy=*/%v85151, /*on_false_vx=*/%v85149 (stack103)
        %v85155 = vadd.f32 %v85152, -3.0 (stack82)
        %v85159 = vsel /*vm=*/%vm85103, /*on_true_vy=*/%v85144, /*on_false_vx=*/%v85155 (stack72)
        %v85163 = vmul.f32 %v85140, %v85159 (stack83)
        %v85167 = vadd.f32 %v85136, %v85163 (stack82)
        %v85171 = vmul.f32 %v85167, %v85159 (stack83)
        %v85175 = vadd.f32 %v85132, %v85171 (stack82)
        %v85179 = vmul.f32 %v85175, %v85159 (stack83)
        %v85183 = vadd.f32 %v85128, %v85179 (stack82)
        %v85187 = vmul.f32 %v85183, %v85159 (stack83)
        %v85191 = vadd.f32 %v85124, %v85187 (stack82)
        %v85195 = vmul.f32 %v85191, %v85159 (stack83)
        %v85199 = vadd.f32 %v85120, %v85195 (stack82)
        %v85203 = vmul.f32 %v85199, %v85159 (stack83)
        %v85207 = vadd.f32 %v85116, %v85203 (stack82)
        %v85211 = vmul.f32 %v85207, %v85159 (stack83)
        %v85215 = vadd.f32 %v85112, %v85211 (stack82)
        %v85219 = vmul.f32 %v85215, %v85159 (stack83)
        %v85223 = vadd.f32 %v85108, %v85219 (stack82)
        %v85227 = vmul.f32 %v85223, %v85074 (stack83)
        %v85231 = vsel /*vm=*/%vm85079, /*on_true_vy=*/%v85084, /*on_false_vx=*/%v85227 (stack72)
        %v85235 = vmul.f32 %v85231, 1.4140625 (stack83)
        %s85237 = scalar_lea.vmem %s280, 728 [#allocation0] (stack107)
        %v85238 = vpack.c.bf16 0.0, %v85235 (stack104)
        %85239 = vst [vmem:[%s85237] sm:$0xf] /*vst_source=*/%v85238 (stack105)
        %v85242 = vadd.s32 %v3329, %v82473 (stack65)
        %s85244 = smul.u32 128, %s27 (stack66)
        %v85245 = vlaneseq (stack67)
        %v85246 = vand.u32 %v85245, 127 (stack68)
        %v85247 = vstv %s85244 (stack69)
        %v85248 = vadd.s32 %v85246, %v85247 (stack70)
        %v85252 = vadd.s32 %v85242, %v85248 (stack65)
        %vm85256 = vcmp.lt.u32.totalorder %v85252, %v85242 (stack71)
        %vm85261 = vcmp.lt.u32.totalorder %v85242, %v3329 (stack71)
        %v85266 = vadd.s32 %v3316, %v82456 (stack65)
        %v85270 = vadd.s32 %v85266, 1 (stack65)
        %v85274 = vsel /*vm=*/%vm85261, /*on_true_vy=*/%v85270, /*on_false_vx=*/%v85266 (stack72)
        %v85278 = vadd.s32 %v85274, 1 (stack65)
        %v85282 = vsel /*vm=*/%vm85256, /*on_true_vy=*/%v85278, /*on_false_vx=*/%v85274 (stack72)
        %v85287 = vadd.s32 %v85282, %v10 (stack65)
        %v85291 = vadd.s32 %v85252, %v9 (stack65)
        %v85295 = vadd.s32 %v85287, %v85291 (stack65)
        %v85297 = vshll.u32 %v85291, 13 (stack73)
        %v85298 = vshrl.u32 %v85291, 19 (stack74)
        %v85299 = vor.u32 %v85297, %v85298 (stack75)
        %v85300 = vxor.u32 %v85295, %v85299 (stack76)
        %v85303 = vadd.s32 %v85295, %v85300 (stack65)
        %v85305 = vshll.u32 %v85300, 15 (stack73)
        %v85306 = vshrl.u32 %v85300, 17 (stack74)
        %v85307 = vor.u32 %v85305, %v85306 (stack75)
        %v85308 = vxor.u32 %v85303, %v85307 (stack76)
        %v85311 = vadd.s32 %v85303, %v85308 (stack65)
        %v85313 = vshll.u32 %v85308, 26 (stack73)
        %v85314 = vshrl.u32 %v85308, 6 (stack74)
        %v85315 = vor.u32 %v85313, %v85314 (stack75)
        %v85316 = vxor.u32 %v85311, %v85315 (stack76)
        %v85319 = vadd.s32 %v85311, %v85316 (stack65)
        %v85323 = vadd.s32 %v85319, %v9 (stack65)
        %v85325 = vshll.u32 %v85316, 6 (stack73)
        %v85326 = vshrl.u32 %v85316, 26 (stack74)
        %v85327 = vor.u32 %v85325, %v85326 (stack75)
        %v85328 = vxor.u32 %v85319, %v85327 (stack76)
        %v85331 = vadd.s32 %v85328, %v8 (stack65)
        %v85335 = vadd.s32 %v85331, 1 (stack65)
        %v85339 = vadd.s32 %v85323, %v85335 (stack65)
        %v85341 = vshll.u32 %v85335, 17 (stack73)
        %v85342 = vshrl.u32 %v85335, 15 (stack74)
        %v85343 = vor.u32 %v85341, %v85342 (stack75)
        %v85344 = vxor.u32 %v85339, %v85343 (stack76)
        %v85347 = vadd.s32 %v85339, %v85344 (stack65)
        %v85349 = vshll.u32 %v85344, 29 (stack73)
        %v85350 = vshrl.u32 %v85344, 3 (stack74)
        %v85351 = vor.u32 %v85349, %v85350 (stack75)
        %v85352 = vxor.u32 %v85347, %v85351 (stack76)
        %v85355 = vadd.s32 %v85347, %v85352 (stack65)
        %v85357 = vshll.u32 %v85352, 16 (stack73)
        %v85358 = vshrl.u32 %v85352, 16 (stack74)
        %v85359 = vor.u32 %v85357, %v85358 (stack75)
        %v85360 = vxor.u32 %v85355, %v85359 (stack76)
        %v85363 = vadd.s32 %v85355, %v85360 (stack65)
        %v85367 = vadd.s32 %v85363, %v8 (stack65)
        %v85369 = vshll.u32 %v85360, 24 (stack73)
        %v85370 = vshrl.u32 %v85360, 8 (stack74)
        %v85371 = vor.u32 %v85369, %v85370 (stack75)
        %v85372 = vxor.u32 %v85363, %v85371 (stack76)
        %v85375 = vadd.s32 %v85372, %v10 (stack65)
        %v85379 = vadd.s32 %v85375, 2 (stack65)
        %v85383 = vadd.s32 %v85367, %v85379 (stack65)
        %v85385 = vshll.u32 %v85379, 13 (stack73)
        %v85386 = vshrl.u32 %v85379, 19 (stack74)
        %v85387 = vor.u32 %v85385, %v85386 (stack75)
        %v85388 = vxor.u32 %v85383, %v85387 (stack76)
        %v85391 = vadd.s32 %v85383, %v85388 (stack65)
        %v85393 = vshll.u32 %v85388, 15 (stack73)
        %v85394 = vshrl.u32 %v85388, 17 (stack74)
        %v85395 = vor.u32 %v85393, %v85394 (stack75)
        %v85396 = vxor.u32 %v85391, %v85395 (stack76)
        %v85399 = vadd.s32 %v85391, %v85396 (stack65)
        %v85401 = vshll.u32 %v85396, 26 (stack73)
        %v85402 = vshrl.u32 %v85396, 6 (stack74)
        %v85403 = vor.u32 %v85401, %v85402 (stack75)
        %v85404 = vxor.u32 %v85399, %v85403 (stack76)
        %v85407 = vadd.s32 %v85399, %v85404 (stack65)
        %v85411 = vadd.s32 %v85407, %v10 (stack65)
        %v85413 = vshll.u32 %v85404, 6 (stack73)
        %v85414 = vshrl.u32 %v85404, 26 (stack74)
        %v85415 = vor.u32 %v85413, %v85414 (stack75)
        %v85416 = vxor.u32 %v85407, %v85415 (stack76)
        %v85419 = vadd.s32 %v85416, %v9 (stack65)
        %v85423 = vadd.s32 %v85419, 3 (stack65)
        %v85427 = vadd.s32 %v85411, %v85423 (stack65)
        %v85429 = vshll.u32 %v85423, 17 (stack73)
        %v85430 = vshrl.u32 %v85423, 15 (stack74)
        %v85431 = vor.u32 %v85429, %v85430 (stack75)
        %v85432 = vxor.u32 %v85427, %v85431 (stack76)
        %v85435 = vadd.s32 %v85427, %v85432 (stack65)
        %v85437 = vshll.u32 %v85432, 29 (stack73)
        %v85438 = vshrl.u32 %v85432, 3 (stack74)
        %v85439 = vor.u32 %v85437, %v85438 (stack75)
        %v85440 = vxor.u32 %v85435, %v85439 (stack76)
        %v85443 = vadd.s32 %v85435, %v85440 (stack65)
        %v85445 = vshll.u32 %v85440, 16 (stack73)
        %v85446 = vshrl.u32 %v85440, 16 (stack74)
        %v85447 = vor.u32 %v85445, %v85446 (stack75)
        %v85448 = vxor.u32 %v85443, %v85447 (stack76)
        %v85451 = vadd.s32 %v85443, %v85448 (stack65)
        %v85455 = vadd.s32 %v85451, %v9 (stack65)
        %v85457 = vshll.u32 %v85448, 24 (stack73)
        %v85458 = vshrl.u32 %v85448, 8 (stack74)
        %v85459 = vor.u32 %v85457, %v85458 (stack75)
        %v85460 = vxor.u32 %v85451, %v85459 (stack76)
        %v85463 = vadd.s32 %v85460, %v8 (stack65)
        %v85467 = vadd.s32 %v85463, 4 (stack65)
        %v85471 = vadd.s32 %v85455, %v85467 (stack65)
        %v85473 = vshll.u32 %v85467, 13 (stack73)
        %v85474 = vshrl.u32 %v85467, 19 (stack74)
        %v85475 = vor.u32 %v85473, %v85474 (stack75)
        %v85476 = vxor.u32 %v85471, %v85475 (stack76)
        %v85479 = vadd.s32 %v85471, %v85476 (stack65)
        %v85481 = vshll.u32 %v85476, 15 (stack73)
        %v85482 = vshrl.u32 %v85476, 17 (stack74)
        %v85483 = vor.u32 %v85481, %v85482 (stack75)
        %v85484 = vxor.u32 %v85479, %v85483 (stack76)
        %v85487 = vadd.s32 %v85479, %v85484 (stack65)
        %v85489 = vshll.u32 %v85484, 26 (stack73)
        %v85490 = vshrl.u32 %v85484, 6 (stack74)
        %v85491 = vor.u32 %v85489, %v85490 (stack75)
        %v85492 = vxor.u32 %v85487, %v85491 (stack76)
        %v85495 = vadd.s32 %v85487, %v85492 (stack65)
        %v85499 = vadd.s32 %v85495, %v8 (stack65)
        %v85501 = vshll.u32 %v85492, 6 (stack73)
        %v85502 = vshrl.u32 %v85492, 26 (stack74)
        %v85503 = vor.u32 %v85501, %v85502 (stack75)
        %v85504 = vxor.u32 %v85495, %v85503 (stack76)
        %v85507 = vadd.s32 %v85504, %v10 (stack65)
        %v85511 = vadd.s32 %v85507, 5 (stack65)
        %v85513 = vxor.u32 %v85499, %v85511 (stack76)
        %v85514 = vand.u32.u8 %v85513, 255 (stack77)
        %v85515 = vand.u32 %v85514, 65535 (stack78)
        %v85516 = vshrl.u32 %v85515, 1 (stack79)
        %v85517 = vor.u32 %v85516, 16256 (stack75)
        %v85518 = vand.u32.u16 %v85517, 65535 (stack80)
        %v85519 = vunpack.i.l.bf16 %v85518 (stack81)
        %v85523 = vadd.f32 %v85519, -1.0 (stack82)
        %v85527 = vmul.f32 %v85523, 2.0 (stack83)
        %v85531 = vadd.f32 %v85527, -0.99609375 (stack82)
        %v85535 = vmax.f32 -0.99609375, %v85531 (stack84)
        %v85537 = vand.u32 2147483647, %v85535 (stack85)
        %vm85540 = vcmp.eq.f32.partialorder %v85537, 1.0 (stack86)
        %v85545 = vmul.f32 %v85535, inf (stack83)
        %v85547 = vxor.u32 %v85535, 2147483648 (stack87)
        %v85550 = vmul.f32 %v85535, %v85547 (stack83)
        %v85552 = vadd.f32 %v85550, 1.0 (stack88)
        %v85553 = vlog2.pop %v85552 (stack89)
        %v85554 = vmul.f32 %v85553, 0.6931472 (stack90)
        %v85555 = vmul.f32 -0.5, %v85550 (stack91)
        %v85556 = vadd.f32 %v85555, 1.0 (stack92)
        %v85557 = vmul.f32 %v85556, %v85550 (stack93)
        %v85558 = vand.u32 2147483647, %v85550 (stack94)
        %vm85559 = vcmp.lt.f32.partialorder %v85558, 0.0004427343 (stack95)
        %v85560 = vsel /*vm=*/%vm85559, /*on_true_vy=*/%v85557, /*on_false_vx=*/%v85554 (stack96)
        %v85561 = vxor.u32 %v85560, 2147483648 (stack87)
        %vm85564 = vcmp.lt.f32.partialorder %v85561, 5.0 (stack86)
        %v85569 = vsel /*vm=*/%vm85564, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v85573 = vsel /*vm=*/%vm85564, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v85577 = vsel /*vm=*/%vm85564, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v85581 = vsel /*vm=*/%vm85564, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v85585 = vsel /*vm=*/%vm85564, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v85589 = vsel /*vm=*/%vm85564, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v85593 = vsel /*vm=*/%vm85564, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v85597 = vsel /*vm=*/%vm85564, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v85601 = vsel /*vm=*/%vm85564, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v85605 = vadd.f32 %v85561, -2.5 (stack82)
        %v85607 = vrsqrt.pop %v85561 (stack97)
        %v85608 = vmul.f32 %v85561, %v85607 (stack98)
        %vm85609 = vcmp.eq.f32.partialorder %v85561, inf (stack99)
        %v85610 = vsel /*vm=*/%vm85609, /*on_true_vy=*/%v85561, /*on_false_vx=*/%v85608 (stack100)
        %vm85611 = vcmp.eq.f32.partialorder %v85561, 0.0 (stack101)
        %v85612 = vand.u32 %v85561, 2147483648 (stack102)
        %v85613 = vsel /*vm=*/%vm85611, /*on_true_vy=*/%v85612, /*on_false_vx=*/%v85610 (stack103)
        %v85616 = vadd.f32 %v85613, -3.0 (stack82)
        %v85620 = vsel /*vm=*/%vm85564, /*on_true_vy=*/%v85605, /*on_false_vx=*/%v85616 (stack72)
        %v85624 = vmul.f32 %v85601, %v85620 (stack83)
        %v85628 = vadd.f32 %v85597, %v85624 (stack82)
        %v85632 = vmul.f32 %v85628, %v85620 (stack83)
        %v85636 = vadd.f32 %v85593, %v85632 (stack82)
        %v85640 = vmul.f32 %v85636, %v85620 (stack83)
        %v85644 = vadd.f32 %v85589, %v85640 (stack82)
        %v85648 = vmul.f32 %v85644, %v85620 (stack83)
        %v85652 = vadd.f32 %v85585, %v85648 (stack82)
        %v85656 = vmul.f32 %v85652, %v85620 (stack83)
        %v85660 = vadd.f32 %v85581, %v85656 (stack82)
        %v85664 = vmul.f32 %v85660, %v85620 (stack83)
        %v85668 = vadd.f32 %v85577, %v85664 (stack82)
        %v85672 = vmul.f32 %v85668, %v85620 (stack83)
        %v85676 = vadd.f32 %v85573, %v85672 (stack82)
        %v85680 = vmul.f32 %v85676, %v85620 (stack83)
        %v85684 = vadd.f32 %v85569, %v85680 (stack82)
        %v85688 = vmul.f32 %v85684, %v85535 (stack83)
        %v85692 = vsel /*vm=*/%vm85540, /*on_true_vy=*/%v85545, /*on_false_vx=*/%v85688 (stack72)
        %v85696 = vmul.f32 %v85692, 1.4140625 (stack83)
        %s85698 = scalar_lea.vmem %s280, 856 [#allocation0] (stack107)
        %v85699 = vpack.c.bf16 0.0, %v85696 (stack104)
        %85700 = vst [vmem:[%s85698] sm:$0xf] /*vst_source=*/%v85699 (stack105)
        %v85703 = vadd.s32 %v3816, %v82473 (stack65)
        %s85705 = smul.u32 128, %s27 (stack66)
        %v85706 = vlaneseq (stack67)
        %v85707 = vand.u32 %v85706, 127 (stack68)
        %v85708 = vstv %s85705 (stack69)
        %v85709 = vadd.s32 %v85707, %v85708 (stack70)
        %v85713 = vadd.s32 %v85703, %v85709 (stack65)
        %vm85717 = vcmp.lt.u32.totalorder %v85713, %v85703 (stack71)
        %vm85722 = vcmp.lt.u32.totalorder %v85703, %v3816 (stack71)
        %v85727 = vadd.s32 %v3803, %v82456 (stack65)
        %v85731 = vadd.s32 %v85727, 1 (stack65)
        %v85735 = vsel /*vm=*/%vm85722, /*on_true_vy=*/%v85731, /*on_false_vx=*/%v85727 (stack72)
        %v85739 = vadd.s32 %v85735, 1 (stack65)
        %v85743 = vsel /*vm=*/%vm85717, /*on_true_vy=*/%v85739, /*on_false_vx=*/%v85735 (stack72)
        %v85748 = vadd.s32 %v85743, %v10 (stack65)
        %v85752 = vadd.s32 %v85713, %v9 (stack65)
        %v85756 = vadd.s32 %v85748, %v85752 (stack65)
        %v85758 = vshll.u32 %v85752, 13 (stack73)
        %v85759 = vshrl.u32 %v85752, 19 (stack74)
        %v85760 = vor.u32 %v85758, %v85759 (stack75)
        %v85761 = vxor.u32 %v85756, %v85760 (stack76)
        %v85764 = vadd.s32 %v85756, %v85761 (stack65)
        %v85766 = vshll.u32 %v85761, 15 (stack73)
        %v85767 = vshrl.u32 %v85761, 17 (stack74)
        %v85768 = vor.u32 %v85766, %v85767 (stack75)
        %v85769 = vxor.u32 %v85764, %v85768 (stack76)
        %v85772 = vadd.s32 %v85764, %v85769 (stack65)
        %v85774 = vshll.u32 %v85769, 26 (stack73)
        %v85775 = vshrl.u32 %v85769, 6 (stack74)
        %v85776 = vor.u32 %v85774, %v85775 (stack75)
        %v85777 = vxor.u32 %v85772, %v85776 (stack76)
        %v85780 = vadd.s32 %v85772, %v85777 (stack65)
        %v85784 = vadd.s32 %v85780, %v9 (stack65)
        %v85786 = vshll.u32 %v85777, 6 (stack73)
        %v85787 = vshrl.u32 %v85777, 26 (stack74)
        %v85788 = vor.u32 %v85786, %v85787 (stack75)
        %v85789 = vxor.u32 %v85780, %v85788 (stack76)
        %v85792 = vadd.s32 %v85789, %v8 (stack65)
        %v85796 = vadd.s32 %v85792, 1 (stack65)
        %v85800 = vadd.s32 %v85784, %v85796 (stack65)
        %v85802 = vshll.u32 %v85796, 17 (stack73)
        %v85803 = vshrl.u32 %v85796, 15 (stack74)
        %v85804 = vor.u32 %v85802, %v85803 (stack75)
        %v85805 = vxor.u32 %v85800, %v85804 (stack76)
        %v85808 = vadd.s32 %v85800, %v85805 (stack65)
        %v85810 = vshll.u32 %v85805, 29 (stack73)
        %v85811 = vshrl.u32 %v85805, 3 (stack74)
        %v85812 = vor.u32 %v85810, %v85811 (stack75)
        %v85813 = vxor.u32 %v85808, %v85812 (stack76)
        %v85816 = vadd.s32 %v85808, %v85813 (stack65)
        %v85818 = vshll.u32 %v85813, 16 (stack73)
        %v85819 = vshrl.u32 %v85813, 16 (stack74)
        %v85820 = vor.u32 %v85818, %v85819 (stack75)
        %v85821 = vxor.u32 %v85816, %v85820 (stack76)
        %v85824 = vadd.s32 %v85816, %v85821 (stack65)
        %v85828 = vadd.s32 %v85824, %v8 (stack65)
        %v85830 = vshll.u32 %v85821, 24 (stack73)
        %v85831 = vshrl.u32 %v85821, 8 (stack74)
        %v85832 = vor.u32 %v85830, %v85831 (stack75)
        %v85833 = vxor.u32 %v85824, %v85832 (stack76)
        %v85836 = vadd.s32 %v85833, %v10 (stack65)
        %v85840 = vadd.s32 %v85836, 2 (stack65)
        %v85844 = vadd.s32 %v85828, %v85840 (stack65)
        %v85846 = vshll.u32 %v85840, 13 (stack73)
        %v85847 = vshrl.u32 %v85840, 19 (stack74)
        %v85848 = vor.u32 %v85846, %v85847 (stack75)
        %v85849 = vxor.u32 %v85844, %v85848 (stack76)
        %v85852 = vadd.s32 %v85844, %v85849 (stack65)
        %v85854 = vshll.u32 %v85849, 15 (stack73)
        %v85855 = vshrl.u32 %v85849, 17 (stack74)
        %v85856 = vor.u32 %v85854, %v85855 (stack75)
        %v85857 = vxor.u32 %v85852, %v85856 (stack76)
        %v85860 = vadd.s32 %v85852, %v85857 (stack65)
        %v85862 = vshll.u32 %v85857, 26 (stack73)
        %v85863 = vshrl.u32 %v85857, 6 (stack74)
        %v85864 = vor.u32 %v85862, %v85863 (stack75)
        %v85865 = vxor.u32 %v85860, %v85864 (stack76)
        %v85868 = vadd.s32 %v85860, %v85865 (stack65)
        %v85872 = vadd.s32 %v85868, %v10 (stack65)
        %v85874 = vshll.u32 %v85865, 6 (stack73)
        %v85875 = vshrl.u32 %v85865, 26 (stack74)
        %v85876 = vor.u32 %v85874, %v85875 (stack75)
        %v85877 = vxor.u32 %v85868, %v85876 (stack76)
        %v85880 = vadd.s32 %v85877, %v9 (stack65)
        %v85884 = vadd.s32 %v85880, 3 (stack65)
        %v85888 = vadd.s32 %v85872, %v85884 (stack65)
        %v85890 = vshll.u32 %v85884, 17 (stack73)
        %v85891 = vshrl.u32 %v85884, 15 (stack74)
        %v85892 = vor.u32 %v85890, %v85891 (stack75)
        %v85893 = vxor.u32 %v85888, %v85892 (stack76)
        %v85896 = vadd.s32 %v85888, %v85893 (stack65)
        %v85898 = vshll.u32 %v85893, 29 (stack73)
        %v85899 = vshrl.u32 %v85893, 3 (stack74)
        %v85900 = vor.u32 %v85898, %v85899 (stack75)
        %v85901 = vxor.u32 %v85896, %v85900 (stack76)
        %v85904 = vadd.s32 %v85896, %v85901 (stack65)
        %v85906 = vshll.u32 %v85901, 16 (stack73)
        %v85907 = vshrl.u32 %v85901, 16 (stack74)
        %v85908 = vor.u32 %v85906, %v85907 (stack75)
        %v85909 = vxor.u32 %v85904, %v85908 (stack76)
        %v85912 = vadd.s32 %v85904, %v85909 (stack65)
        %v85916 = vadd.s32 %v85912, %v9 (stack65)
        %v85918 = vshll.u32 %v85909, 24 (stack73)
        %v85919 = vshrl.u32 %v85909, 8 (stack74)
        %v85920 = vor.u32 %v85918, %v85919 (stack75)
        %v85921 = vxor.u32 %v85912, %v85920 (stack76)
        %v85924 = vadd.s32 %v85921, %v8 (stack65)
        %v85928 = vadd.s32 %v85924, 4 (stack65)
        %v85932 = vadd.s32 %v85916, %v85928 (stack65)
        %v85934 = vshll.u32 %v85928, 13 (stack73)
        %v85935 = vshrl.u32 %v85928, 19 (stack74)
        %v85936 = vor.u32 %v85934, %v85935 (stack75)
        %v85937 = vxor.u32 %v85932, %v85936 (stack76)
        %v85940 = vadd.s32 %v85932, %v85937 (stack65)
        %v85942 = vshll.u32 %v85937, 15 (stack73)
        %v85943 = vshrl.u32 %v85937, 17 (stack74)
        %v85944 = vor.u32 %v85942, %v85943 (stack75)
        %v85945 = vxor.u32 %v85940, %v85944 (stack76)
        %v85948 = vadd.s32 %v85940, %v85945 (stack65)
        %v85950 = vshll.u32 %v85945, 26 (stack73)
        %v85951 = vshrl.u32 %v85945, 6 (stack74)
        %v85952 = vor.u32 %v85950, %v85951 (stack75)
        %v85953 = vxor.u32 %v85948, %v85952 (stack76)
        %v85956 = vadd.s32 %v85948, %v85953 (stack65)
        %v85960 = vadd.s32 %v85956, %v8 (stack65)
        %v85962 = vshll.u32 %v85953, 6 (stack73)
        %v85963 = vshrl.u32 %v85953, 26 (stack74)
        %v85964 = vor.u32 %v85962, %v85963 (stack75)
        %v85965 = vxor.u32 %v85956, %v85964 (stack76)
        %v85968 = vadd.s32 %v85965, %v10 (stack65)
        %v85972 = vadd.s32 %v85968, 5 (stack65)
        %v85974 = vxor.u32 %v85960, %v85972 (stack76)
        %v85975 = vand.u32.u8 %v85974, 255 (stack77)
        %v85976 = vand.u32 %v85975, 65535 (stack78)
        %v85977 = vshrl.u32 %v85976, 1 (stack79)
        %v85978 = vor.u32 %v85977, 16256 (stack75)
        %v85979 = vand.u32.u16 %v85978, 65535 (stack80)
        %v85980 = vunpack.i.l.bf16 %v85979 (stack81)
        %v85984 = vadd.f32 %v85980, -1.0 (stack82)
        %v85988 = vmul.f32 %v85984, 2.0 (stack83)
        %v85992 = vadd.f32 %v85988, -0.99609375 (stack82)
        %v85996 = vmax.f32 -0.99609375, %v85992 (stack84)
        %v85998 = vand.u32 2147483647, %v85996 (stack85)
        %vm86001 = vcmp.eq.f32.partialorder %v85998, 1.0 (stack86)
        %v86006 = vmul.f32 %v85996, inf (stack83)
        %v86008 = vxor.u32 %v85996, 2147483648 (stack87)
        %v86011 = vmul.f32 %v85996, %v86008 (stack83)
        %v86013 = vadd.f32 %v86011, 1.0 (stack88)
        %v86014 = vlog2.pop %v86013 (stack89)
        %v86015 = vmul.f32 %v86014, 0.6931472 (stack90)
        %v86016 = vmul.f32 -0.5, %v86011 (stack91)
        %v86017 = vadd.f32 %v86016, 1.0 (stack92)
        %v86018 = vmul.f32 %v86017, %v86011 (stack93)
        %v86019 = vand.u32 2147483647, %v86011 (stack94)
        %vm86020 = vcmp.lt.f32.partialorder %v86019, 0.0004427343 (stack95)
        %v86021 = vsel /*vm=*/%vm86020, /*on_true_vy=*/%v86018, /*on_false_vx=*/%v86015 (stack96)
        %v86022 = vxor.u32 %v86021, 2147483648 (stack87)
        %vm86025 = vcmp.lt.f32.partialorder %v86022, 5.0 (stack86)
        %v86030 = vsel /*vm=*/%vm86025, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v86034 = vsel /*vm=*/%vm86025, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v86038 = vsel /*vm=*/%vm86025, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v86042 = vsel /*vm=*/%vm86025, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v86046 = vsel /*vm=*/%vm86025, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v86050 = vsel /*vm=*/%vm86025, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v86054 = vsel /*vm=*/%vm86025, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v86058 = vsel /*vm=*/%vm86025, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v86062 = vsel /*vm=*/%vm86025, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v86066 = vadd.f32 %v86022, -2.5 (stack82)
        %v86068 = vrsqrt.pop %v86022 (stack97)
        %v86069 = vmul.f32 %v86022, %v86068 (stack98)
        %vm86070 = vcmp.eq.f32.partialorder %v86022, inf (stack99)
        %v86071 = vsel /*vm=*/%vm86070, /*on_true_vy=*/%v86022, /*on_false_vx=*/%v86069 (stack100)
        %vm86072 = vcmp.eq.f32.partialorder %v86022, 0.0 (stack101)
        %v86073 = vand.u32 %v86022, 2147483648 (stack102)
        %v86074 = vsel /*vm=*/%vm86072, /*on_true_vy=*/%v86073, /*on_false_vx=*/%v86071 (stack103)
        %v86077 = vadd.f32 %v86074, -3.0 (stack82)
        %v86081 = vsel /*vm=*/%vm86025, /*on_true_vy=*/%v86066, /*on_false_vx=*/%v86077 (stack72)
        %v86085 = vmul.f32 %v86062, %v86081 (stack83)
        %v86089 = vadd.f32 %v86058, %v86085 (stack82)
        %v86093 = vmul.f32 %v86089, %v86081 (stack83)
        %v86097 = vadd.f32 %v86054, %v86093 (stack82)
        %v86101 = vmul.f32 %v86097, %v86081 (stack83)
        %v86105 = vadd.f32 %v86050, %v86101 (stack82)
        %v86109 = vmul.f32 %v86105, %v86081 (stack83)
        %v86113 = vadd.f32 %v86046, %v86109 (stack82)
        %v86117 = vmul.f32 %v86113, %v86081 (stack83)
        %v86121 = vadd.f32 %v86042, %v86117 (stack82)
        %v86125 = vmul.f32 %v86121, %v86081 (stack83)
        %v86129 = vadd.f32 %v86038, %v86125 (stack82)
        %v86133 = vmul.f32 %v86129, %v86081 (stack83)
        %v86137 = vadd.f32 %v86034, %v86133 (stack82)
        %v86141 = vmul.f32 %v86137, %v86081 (stack83)
        %v86145 = vadd.f32 %v86030, %v86141 (stack82)
        %v86149 = vmul.f32 %v86145, %v85996 (stack83)
        %v86153 = vsel /*vm=*/%vm86001, /*on_true_vy=*/%v86006, /*on_false_vx=*/%v86149 (stack72)
        %v86157 = vmul.f32 %v86153, 1.4140625 (stack83)
        %s86159 = scalar_lea.vmem %s280, 984 [#allocation0] (stack107)
        %v86160 = vpack.c.bf16 0.0, %v86157 (stack104)
        %86161 = vst [vmem:[%s86159] sm:$0xf] /*vst_source=*/%v86160 (stack105)
        %s86162 = sadd.s32 %s339, 184 (stack106)
        %s86163 = sshrl.u32 %s86162, 10 (stack49)
        %p86164 = scmp.lt.s32.totalorder 1, %s86163 (stack50)
        %s86165 = scalar_select /*predicate=*/%p86164, /*on_true=*/1, /*on_false=*/%s86163 (stack51)
        %s86166 = sand.u32 %s86162, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s86167 = sshrl.u32 %s86166, 7 (stack53)
        %s86168 = sand.u32 %s86166, 127 /* smod.u32 w/div 128 */ (stack54)
        %s86169 = smul.addr %s86165, 8 (stack55)
        %s86170 = scalar_lea.vmem %s3, %s86169 (stack56)
        %s86172 = scalar_lea.vmem %s86170, %s86167 (stack57)
        %v86173 = vld [vmem:[%s86172] ss:$0 sm:$0xff] (stack58)
        %s86174 = sand.u32 %s86168, 255 (stack59)
        %s86176 = sor.u32 256, %s86174 (stack60)
        %86177 = vbcast.lane.b32.xlu0 %v86173, %s86176 (stack61)
        %v86178 = vpop.permute.xlu0 %86177 (stack62)
        %s86179 = sadd.s32 %s347, 184 (stack106)
        %s86180 = sshrl.u32 %s86179, 10 (stack49)
        %p86181 = scmp.lt.s32.totalorder 1, %s86180 (stack50)
        %s86182 = scalar_select /*predicate=*/%p86181, /*on_true=*/1, /*on_false=*/%s86180 (stack51)
        %s86183 = sand.u32 %s86179, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s86184 = sshrl.u32 %s86183, 7 (stack53)
        %s86185 = sand.u32 %s86183, 127 /* smod.u32 w/div 128 */ (stack54)
        %s86186 = smul.addr %s86182, 8 (stack55)
        %s86187 = scalar_lea.vmem %s5, %s86186 (stack56)
        %s86189 = scalar_lea.vmem %s86187, %s86184 (stack57)
        %v86190 = vld [vmem:[%s86189] ss:$0 sm:$0xff] (stack58)
        %s86191 = sand.u32 %s86185, 255 (stack59)
        %s86193 = sor.u32 256, %s86191 (stack60)
        %86194 = vbcast.lane.b32.xlu0 %v86190, %s86193 (stack61)
        %v86195 = vpop.permute.xlu0 %86194 (stack62)
        %v86198 = vadd.s32 %v408, %v86195 (stack65)
        %s86200 = smul.u32 128, %s27 (stack66)
        %v86201 = vlaneseq (stack67)
        %v86202 = vand.u32 %v86201, 127 (stack68)
        %v86203 = vstv %s86200 (stack69)
        %v86204 = vadd.s32 %v86202, %v86203 (stack70)
        %v86208 = vadd.s32 %v86198, %v86204 (stack65)
        %vm86212 = vcmp.lt.u32.totalorder %v86208, %v86198 (stack71)
        %vm86217 = vcmp.lt.u32.totalorder %v86198, %v408 (stack71)
        %v86222 = vadd.s32 %v380, %v86178 (stack65)
        %v86226 = vadd.s32 %v86222, 1 (stack65)
        %v86230 = vsel /*vm=*/%vm86217, /*on_true_vy=*/%v86226, /*on_false_vx=*/%v86222 (stack72)
        %v86234 = vadd.s32 %v86230, 1 (stack65)
        %v86238 = vsel /*vm=*/%vm86212, /*on_true_vy=*/%v86234, /*on_false_vx=*/%v86230 (stack72)
        %v86243 = vadd.s32 %v86238, %v10 (stack65)
        %v86247 = vadd.s32 %v86208, %v9 (stack65)
        %v86251 = vadd.s32 %v86243, %v86247 (stack65)
        %v86253 = vshll.u32 %v86247, 13 (stack73)
        %v86254 = vshrl.u32 %v86247, 19 (stack74)
        %v86255 = vor.u32 %v86253, %v86254 (stack75)
        %v86256 = vxor.u32 %v86251, %v86255 (stack76)
        %v86259 = vadd.s32 %v86251, %v86256 (stack65)
        %v86261 = vshll.u32 %v86256, 15 (stack73)
        %v86262 = vshrl.u32 %v86256, 17 (stack74)
        %v86263 = vor.u32 %v86261, %v86262 (stack75)
        %v86264 = vxor.u32 %v86259, %v86263 (stack76)
        %v86267 = vadd.s32 %v86259, %v86264 (stack65)
        %v86269 = vshll.u32 %v86264, 26 (stack73)
        %v86270 = vshrl.u32 %v86264, 6 (stack74)
        %v86271 = vor.u32 %v86269, %v86270 (stack75)
        %v86272 = vxor.u32 %v86267, %v86271 (stack76)
        %v86275 = vadd.s32 %v86267, %v86272 (stack65)
        %v86279 = vadd.s32 %v86275, %v9 (stack65)
        %v86281 = vshll.u32 %v86272, 6 (stack73)
        %v86282 = vshrl.u32 %v86272, 26 (stack74)
        %v86283 = vor.u32 %v86281, %v86282 (stack75)
        %v86284 = vxor.u32 %v86275, %v86283 (stack76)
        %v86287 = vadd.s32 %v86284, %v8 (stack65)
        %v86291 = vadd.s32 %v86287, 1 (stack65)
        %v86295 = vadd.s32 %v86279, %v86291 (stack65)
        %v86297 = vshll.u32 %v86291, 17 (stack73)
        %v86298 = vshrl.u32 %v86291, 15 (stack74)
        %v86299 = vor.u32 %v86297, %v86298 (stack75)
        %v86300 = vxor.u32 %v86295, %v86299 (stack76)
        %v86303 = vadd.s32 %v86295, %v86300 (stack65)
        %v86305 = vshll.u32 %v86300, 29 (stack73)
        %v86306 = vshrl.u32 %v86300, 3 (stack74)
        %v86307 = vor.u32 %v86305, %v86306 (stack75)
        %v86308 = vxor.u32 %v86303, %v86307 (stack76)
        %v86311 = vadd.s32 %v86303, %v86308 (stack65)
        %v86313 = vshll.u32 %v86308, 16 (stack73)
        %v86314 = vshrl.u32 %v86308, 16 (stack74)
        %v86315 = vor.u32 %v86313, %v86314 (stack75)
        %v86316 = vxor.u32 %v86311, %v86315 (stack76)
        %v86319 = vadd.s32 %v86311, %v86316 (stack65)
        %v86323 = vadd.s32 %v86319, %v8 (stack65)
        %v86325 = vshll.u32 %v86316, 24 (stack73)
        %v86326 = vshrl.u32 %v86316, 8 (stack74)
        %v86327 = vor.u32 %v86325, %v86326 (stack75)
        %v86328 = vxor.u32 %v86319, %v86327 (stack76)
        %v86331 = vadd.s32 %v86328, %v10 (stack65)
        %v86335 = vadd.s32 %v86331, 2 (stack65)
        %v86339 = vadd.s32 %v86323, %v86335 (stack65)
        %v86341 = vshll.u32 %v86335, 13 (stack73)
        %v86342 = vshrl.u32 %v86335, 19 (stack74)
        %v86343 = vor.u32 %v86341, %v86342 (stack75)
        %v86344 = vxor.u32 %v86339, %v86343 (stack76)
        %v86347 = vadd.s32 %v86339, %v86344 (stack65)
        %v86349 = vshll.u32 %v86344, 15 (stack73)
        %v86350 = vshrl.u32 %v86344, 17 (stack74)
        %v86351 = vor.u32 %v86349, %v86350 (stack75)
        %v86352 = vxor.u32 %v86347, %v86351 (stack76)
        %v86355 = vadd.s32 %v86347, %v86352 (stack65)
        %v86357 = vshll.u32 %v86352, 26 (stack73)
        %v86358 = vshrl.u32 %v86352, 6 (stack74)
        %v86359 = vor.u32 %v86357, %v86358 (stack75)
        %v86360 = vxor.u32 %v86355, %v86359 (stack76)
        %v86363 = vadd.s32 %v86355, %v86360 (stack65)
        %v86367 = vadd.s32 %v86363, %v10 (stack65)
        %v86369 = vshll.u32 %v86360, 6 (stack73)
        %v86370 = vshrl.u32 %v86360, 26 (stack74)
        %v86371 = vor.u32 %v86369, %v86370 (stack75)
        %v86372 = vxor.u32 %v86363, %v86371 (stack76)
        %v86375 = vadd.s32 %v86372, %v9 (stack65)
        %v86379 = vadd.s32 %v86375, 3 (stack65)
        %v86383 = vadd.s32 %v86367, %v86379 (stack65)
        %v86385 = vshll.u32 %v86379, 17 (stack73)
        %v86386 = vshrl.u32 %v86379, 15 (stack74)
        %v86387 = vor.u32 %v86385, %v86386 (stack75)
        %v86388 = vxor.u32 %v86383, %v86387 (stack76)
        %v86391 = vadd.s32 %v86383, %v86388 (stack65)
        %v86393 = vshll.u32 %v86388, 29 (stack73)
        %v86394 = vshrl.u32 %v86388, 3 (stack74)
        %v86395 = vor.u32 %v86393, %v86394 (stack75)
        %v86396 = vxor.u32 %v86391, %v86395 (stack76)
        %v86399 = vadd.s32 %v86391, %v86396 (stack65)
        %v86401 = vshll.u32 %v86396, 16 (stack73)
        %v86402 = vshrl.u32 %v86396, 16 (stack74)
        %v86403 = vor.u32 %v86401, %v86402 (stack75)
        %v86404 = vxor.u32 %v86399, %v86403 (stack76)
        %v86407 = vadd.s32 %v86399, %v86404 (stack65)
        %v86411 = vadd.s32 %v86407, %v9 (stack65)
        %v86413 = vshll.u32 %v86404, 24 (stack73)
        %v86414 = vshrl.u32 %v86404, 8 (stack74)
        %v86415 = vor.u32 %v86413, %v86414 (stack75)
        %v86416 = vxor.u32 %v86407, %v86415 (stack76)
        %v86419 = vadd.s32 %v86416, %v8 (stack65)
        %v86423 = vadd.s32 %v86419, 4 (stack65)
        %v86427 = vadd.s32 %v86411, %v86423 (stack65)
        %v86429 = vshll.u32 %v86423, 13 (stack73)
        %v86430 = vshrl.u32 %v86423, 19 (stack74)
        %v86431 = vor.u32 %v86429, %v86430 (stack75)
        %v86432 = vxor.u32 %v86427, %v86431 (stack76)
        %v86435 = vadd.s32 %v86427, %v86432 (stack65)
        %v86437 = vshll.u32 %v86432, 15 (stack73)
        %v86438 = vshrl.u32 %v86432, 17 (stack74)
        %v86439 = vor.u32 %v86437, %v86438 (stack75)
        %v86440 = vxor.u32 %v86435, %v86439 (stack76)
        %v86443 = vadd.s32 %v86435, %v86440 (stack65)
        %v86445 = vshll.u32 %v86440, 26 (stack73)
        %v86446 = vshrl.u32 %v86440, 6 (stack74)
        %v86447 = vor.u32 %v86445, %v86446 (stack75)
        %v86448 = vxor.u32 %v86443, %v86447 (stack76)
        %v86451 = vadd.s32 %v86443, %v86448 (stack65)
        %v86455 = vadd.s32 %v86451, %v8 (stack65)
        %v86457 = vshll.u32 %v86448, 6 (stack73)
        %v86458 = vshrl.u32 %v86448, 26 (stack74)
        %v86459 = vor.u32 %v86457, %v86458 (stack75)
        %v86460 = vxor.u32 %v86451, %v86459 (stack76)
        %v86463 = vadd.s32 %v86460, %v10 (stack65)
        %v86467 = vadd.s32 %v86463, 5 (stack65)
        %v86469 = vxor.u32 %v86455, %v86467 (stack76)
        %v86470 = vand.u32.u8 %v86469, 255 (stack77)
        %v86471 = vand.u32 %v86470, 65535 (stack78)
        %v86472 = vshrl.u32 %v86471, 1 (stack79)
        %v86473 = vor.u32 %v86472, 16256 (stack75)
        %v86474 = vand.u32.u16 %v86473, 65535 (stack80)
        %v86475 = vunpack.i.l.bf16 %v86474 (stack81)
        %v86479 = vadd.f32 %v86475, -1.0 (stack82)
        %v86483 = vmul.f32 %v86479, 2.0 (stack83)
        %v86487 = vadd.f32 %v86483, -0.99609375 (stack82)
        %v86491 = vmax.f32 -0.99609375, %v86487 (stack84)
        %v86493 = vand.u32 2147483647, %v86491 (stack85)
        %vm86496 = vcmp.eq.f32.partialorder %v86493, 1.0 (stack86)
        %v86501 = vmul.f32 %v86491, inf (stack83)
        %v86503 = vxor.u32 %v86491, 2147483648 (stack87)
        %v86506 = vmul.f32 %v86491, %v86503 (stack83)
        %v86508 = vadd.f32 %v86506, 1.0 (stack88)
        %v86509 = vlog2.pop %v86508 (stack89)
        %v86510 = vmul.f32 %v86509, 0.6931472 (stack90)
        %v86511 = vmul.f32 -0.5, %v86506 (stack91)
        %v86512 = vadd.f32 %v86511, 1.0 (stack92)
        %v86513 = vmul.f32 %v86512, %v86506 (stack93)
        %v86514 = vand.u32 2147483647, %v86506 (stack94)
        %vm86515 = vcmp.lt.f32.partialorder %v86514, 0.0004427343 (stack95)
        %v86516 = vsel /*vm=*/%vm86515, /*on_true_vy=*/%v86513, /*on_false_vx=*/%v86510 (stack96)
        %v86517 = vxor.u32 %v86516, 2147483648 (stack87)
        %vm86520 = vcmp.lt.f32.partialorder %v86517, 5.0 (stack86)
        %v86525 = vsel /*vm=*/%vm86520, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v86529 = vsel /*vm=*/%vm86520, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v86533 = vsel /*vm=*/%vm86520, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v86537 = vsel /*vm=*/%vm86520, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v86541 = vsel /*vm=*/%vm86520, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v86545 = vsel /*vm=*/%vm86520, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v86549 = vsel /*vm=*/%vm86520, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v86553 = vsel /*vm=*/%vm86520, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v86557 = vsel /*vm=*/%vm86520, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v86561 = vadd.f32 %v86517, -2.5 (stack82)
        %v86563 = vrsqrt.pop %v86517 (stack97)
        %v86564 = vmul.f32 %v86517, %v86563 (stack98)
        %vm86565 = vcmp.eq.f32.partialorder %v86517, inf (stack99)
        %v86566 = vsel /*vm=*/%vm86565, /*on_true_vy=*/%v86517, /*on_false_vx=*/%v86564 (stack100)
        %vm86567 = vcmp.eq.f32.partialorder %v86517, 0.0 (stack101)
        %v86568 = vand.u32 %v86517, 2147483648 (stack102)
        %v86569 = vsel /*vm=*/%vm86567, /*on_true_vy=*/%v86568, /*on_false_vx=*/%v86566 (stack103)
        %v86572 = vadd.f32 %v86569, -3.0 (stack82)
        %v86576 = vsel /*vm=*/%vm86520, /*on_true_vy=*/%v86561, /*on_false_vx=*/%v86572 (stack72)
        %v86580 = vmul.f32 %v86557, %v86576 (stack83)
        %v86584 = vadd.f32 %v86553, %v86580 (stack82)
        %v86588 = vmul.f32 %v86584, %v86576 (stack83)
        %v86592 = vadd.f32 %v86549, %v86588 (stack82)
        %v86596 = vmul.f32 %v86592, %v86576 (stack83)
        %v86600 = vadd.f32 %v86545, %v86596 (stack82)
        %v86604 = vmul.f32 %v86600, %v86576 (stack83)
        %v86608 = vadd.f32 %v86541, %v86604 (stack82)
        %v86612 = vmul.f32 %v86608, %v86576 (stack83)
        %v86616 = vadd.f32 %v86537, %v86612 (stack82)
        %v86620 = vmul.f32 %v86616, %v86576 (stack83)
        %v86624 = vadd.f32 %v86533, %v86620 (stack82)
        %v86628 = vmul.f32 %v86624, %v86576 (stack83)
        %v86632 = vadd.f32 %v86529, %v86628 (stack82)
        %v86636 = vmul.f32 %v86632, %v86576 (stack83)
        %v86640 = vadd.f32 %v86525, %v86636 (stack82)
        %v86644 = vmul.f32 %v86640, %v86491 (stack83)
        %v86648 = vsel /*vm=*/%vm86496, /*on_true_vy=*/%v86501, /*on_false_vx=*/%v86644 (stack72)
        %v86652 = vmul.f32 %v86648, 1.4140625 (stack83)
        %s86654 = scalar_lea.vmem %s280, 92 [#allocation0] (stack107)
        %v86655 = vpack.c.bf16 0.0, %v86652 (stack104)
        %86656 = vst [vmem:[%s86654] sm:$0xf] /*vst_source=*/%v86655 (stack105)
        %v86659 = vadd.s32 %v894, %v86195 (stack65)
        %s86661 = smul.u32 128, %s27 (stack66)
        %v86662 = vlaneseq (stack67)
        %v86663 = vand.u32 %v86662, 127 (stack68)
        %v86664 = vstv %s86661 (stack69)
        %v86665 = vadd.s32 %v86663, %v86664 (stack70)
        %v86669 = vadd.s32 %v86659, %v86665 (stack65)
        %vm86673 = vcmp.lt.u32.totalorder %v86669, %v86659 (stack71)
        %vm86678 = vcmp.lt.u32.totalorder %v86659, %v894 (stack71)
        %v86683 = vadd.s32 %v881, %v86178 (stack65)
        %v86687 = vadd.s32 %v86683, 1 (stack65)
        %v86691 = vsel /*vm=*/%vm86678, /*on_true_vy=*/%v86687, /*on_false_vx=*/%v86683 (stack72)
        %v86695 = vadd.s32 %v86691, 1 (stack65)
        %v86699 = vsel /*vm=*/%vm86673, /*on_true_vy=*/%v86695, /*on_false_vx=*/%v86691 (stack72)
        %v86704 = vadd.s32 %v86699, %v10 (stack65)
        %v86708 = vadd.s32 %v86669, %v9 (stack65)
        %v86712 = vadd.s32 %v86704, %v86708 (stack65)
        %v86714 = vshll.u32 %v86708, 13 (stack73)
        %v86715 = vshrl.u32 %v86708, 19 (stack74)
        %v86716 = vor.u32 %v86714, %v86715 (stack75)
        %v86717 = vxor.u32 %v86712, %v86716 (stack76)
        %v86720 = vadd.s32 %v86712, %v86717 (stack65)
        %v86722 = vshll.u32 %v86717, 15 (stack73)
        %v86723 = vshrl.u32 %v86717, 17 (stack74)
        %v86724 = vor.u32 %v86722, %v86723 (stack75)
        %v86725 = vxor.u32 %v86720, %v86724 (stack76)
        %v86728 = vadd.s32 %v86720, %v86725 (stack65)
        %v86730 = vshll.u32 %v86725, 26 (stack73)
        %v86731 = vshrl.u32 %v86725, 6 (stack74)
        %v86732 = vor.u32 %v86730, %v86731 (stack75)
        %v86733 = vxor.u32 %v86728, %v86732 (stack76)
        %v86736 = vadd.s32 %v86728, %v86733 (stack65)
        %v86740 = vadd.s32 %v86736, %v9 (stack65)
        %v86742 = vshll.u32 %v86733, 6 (stack73)
        %v86743 = vshrl.u32 %v86733, 26 (stack74)
        %v86744 = vor.u32 %v86742, %v86743 (stack75)
        %v86745 = vxor.u32 %v86736, %v86744 (stack76)
        %v86748 = vadd.s32 %v86745, %v8 (stack65)
        %v86752 = vadd.s32 %v86748, 1 (stack65)
        %v86756 = vadd.s32 %v86740, %v86752 (stack65)
        %v86758 = vshll.u32 %v86752, 17 (stack73)
        %v86759 = vshrl.u32 %v86752, 15 (stack74)
        %v86760 = vor.u32 %v86758, %v86759 (stack75)
        %v86761 = vxor.u32 %v86756, %v86760 (stack76)
        %v86764 = vadd.s32 %v86756, %v86761 (stack65)
        %v86766 = vshll.u32 %v86761, 29 (stack73)
        %v86767 = vshrl.u32 %v86761, 3 (stack74)
        %v86768 = vor.u32 %v86766, %v86767 (stack75)
        %v86769 = vxor.u32 %v86764, %v86768 (stack76)
        %v86772 = vadd.s32 %v86764, %v86769 (stack65)
        %v86774 = vshll.u32 %v86769, 16 (stack73)
        %v86775 = vshrl.u32 %v86769, 16 (stack74)
        %v86776 = vor.u32 %v86774, %v86775 (stack75)
        %v86777 = vxor.u32 %v86772, %v86776 (stack76)
        %v86780 = vadd.s32 %v86772, %v86777 (stack65)
        %v86784 = vadd.s32 %v86780, %v8 (stack65)
        %v86786 = vshll.u32 %v86777, 24 (stack73)
        %v86787 = vshrl.u32 %v86777, 8 (stack74)
        %v86788 = vor.u32 %v86786, %v86787 (stack75)
        %v86789 = vxor.u32 %v86780, %v86788 (stack76)
        %v86792 = vadd.s32 %v86789, %v10 (stack65)
        %v86796 = vadd.s32 %v86792, 2 (stack65)
        %v86800 = vadd.s32 %v86784, %v86796 (stack65)
        %v86802 = vshll.u32 %v86796, 13 (stack73)
        %v86803 = vshrl.u32 %v86796, 19 (stack74)
        %v86804 = vor.u32 %v86802, %v86803 (stack75)
        %v86805 = vxor.u32 %v86800, %v86804 (stack76)
        %v86808 = vadd.s32 %v86800, %v86805 (stack65)
        %v86810 = vshll.u32 %v86805, 15 (stack73)
        %v86811 = vshrl.u32 %v86805, 17 (stack74)
        %v86812 = vor.u32 %v86810, %v86811 (stack75)
        %v86813 = vxor.u32 %v86808, %v86812 (stack76)
        %v86816 = vadd.s32 %v86808, %v86813 (stack65)
        %v86818 = vshll.u32 %v86813, 26 (stack73)
        %v86819 = vshrl.u32 %v86813, 6 (stack74)
        %v86820 = vor.u32 %v86818, %v86819 (stack75)
        %v86821 = vxor.u32 %v86816, %v86820 (stack76)
        %v86824 = vadd.s32 %v86816, %v86821 (stack65)
        %v86828 = vadd.s32 %v86824, %v10 (stack65)
        %v86830 = vshll.u32 %v86821, 6 (stack73)
        %v86831 = vshrl.u32 %v86821, 26 (stack74)
        %v86832 = vor.u32 %v86830, %v86831 (stack75)
        %v86833 = vxor.u32 %v86824, %v86832 (stack76)
        %v86836 = vadd.s32 %v86833, %v9 (stack65)
        %v86840 = vadd.s32 %v86836, 3 (stack65)
        %v86844 = vadd.s32 %v86828, %v86840 (stack65)
        %v86846 = vshll.u32 %v86840, 17 (stack73)
        %v86847 = vshrl.u32 %v86840, 15 (stack74)
        %v86848 = vor.u32 %v86846, %v86847 (stack75)
        %v86849 = vxor.u32 %v86844, %v86848 (stack76)
        %v86852 = vadd.s32 %v86844, %v86849 (stack65)
        %v86854 = vshll.u32 %v86849, 29 (stack73)
        %v86855 = vshrl.u32 %v86849, 3 (stack74)
        %v86856 = vor.u32 %v86854, %v86855 (stack75)
        %v86857 = vxor.u32 %v86852, %v86856 (stack76)
        %v86860 = vadd.s32 %v86852, %v86857 (stack65)
        %v86862 = vshll.u32 %v86857, 16 (stack73)
        %v86863 = vshrl.u32 %v86857, 16 (stack74)
        %v86864 = vor.u32 %v86862, %v86863 (stack75)
        %v86865 = vxor.u32 %v86860, %v86864 (stack76)
        %v86868 = vadd.s32 %v86860, %v86865 (stack65)
        %v86872 = vadd.s32 %v86868, %v9 (stack65)
        %v86874 = vshll.u32 %v86865, 24 (stack73)
        %v86875 = vshrl.u32 %v86865, 8 (stack74)
        %v86876 = vor.u32 %v86874, %v86875 (stack75)
        %v86877 = vxor.u32 %v86868, %v86876 (stack76)
        %v86880 = vadd.s32 %v86877, %v8 (stack65)
        %v86884 = vadd.s32 %v86880, 4 (stack65)
        %v86888 = vadd.s32 %v86872, %v86884 (stack65)
        %v86890 = vshll.u32 %v86884, 13 (stack73)
        %v86891 = vshrl.u32 %v86884, 19 (stack74)
        %v86892 = vor.u32 %v86890, %v86891 (stack75)
        %v86893 = vxor.u32 %v86888, %v86892 (stack76)
        %v86896 = vadd.s32 %v86888, %v86893 (stack65)
        %v86898 = vshll.u32 %v86893, 15 (stack73)
        %v86899 = vshrl.u32 %v86893, 17 (stack74)
        %v86900 = vor.u32 %v86898, %v86899 (stack75)
        %v86901 = vxor.u32 %v86896, %v86900 (stack76)
        %v86904 = vadd.s32 %v86896, %v86901 (stack65)
        %v86906 = vshll.u32 %v86901, 26 (stack73)
        %v86907 = vshrl.u32 %v86901, 6 (stack74)
        %v86908 = vor.u32 %v86906, %v86907 (stack75)
        %v86909 = vxor.u32 %v86904, %v86908 (stack76)
        %v86912 = vadd.s32 %v86904, %v86909 (stack65)
        %v86916 = vadd.s32 %v86912, %v8 (stack65)
        %v86918 = vshll.u32 %v86909, 6 (stack73)
        %v86919 = vshrl.u32 %v86909, 26 (stack74)
        %v86920 = vor.u32 %v86918, %v86919 (stack75)
        %v86921 = vxor.u32 %v86912, %v86920 (stack76)
        %v86924 = vadd.s32 %v86921, %v10 (stack65)
        %v86928 = vadd.s32 %v86924, 5 (stack65)
        %v86930 = vxor.u32 %v86916, %v86928 (stack76)
        %v86931 = vand.u32.u8 %v86930, 255 (stack77)
        %v86932 = vand.u32 %v86931, 65535 (stack78)
        %v86933 = vshrl.u32 %v86932, 1 (stack79)
        %v86934 = vor.u32 %v86933, 16256 (stack75)
        %v86935 = vand.u32.u16 %v86934, 65535 (stack80)
        %v86936 = vunpack.i.l.bf16 %v86935 (stack81)
        %v86940 = vadd.f32 %v86936, -1.0 (stack82)
        %v86944 = vmul.f32 %v86940, 2.0 (stack83)
        %v86948 = vadd.f32 %v86944, -0.99609375 (stack82)
        %v86952 = vmax.f32 -0.99609375, %v86948 (stack84)
        %v86954 = vand.u32 2147483647, %v86952 (stack85)
        %vm86957 = vcmp.eq.f32.partialorder %v86954, 1.0 (stack86)
        %v86962 = vmul.f32 %v86952, inf (stack83)
        %v86964 = vxor.u32 %v86952, 2147483648 (stack87)
        %v86967 = vmul.f32 %v86952, %v86964 (stack83)
        %v86969 = vadd.f32 %v86967, 1.0 (stack88)
        %v86970 = vlog2.pop %v86969 (stack89)
        %v86971 = vmul.f32 %v86970, 0.6931472 (stack90)
        %v86972 = vmul.f32 -0.5, %v86967 (stack91)
        %v86973 = vadd.f32 %v86972, 1.0 (stack92)
        %v86974 = vmul.f32 %v86973, %v86967 (stack93)
        %v86975 = vand.u32 2147483647, %v86967 (stack94)
        %vm86976 = vcmp.lt.f32.partialorder %v86975, 0.0004427343 (stack95)
        %v86977 = vsel /*vm=*/%vm86976, /*on_true_vy=*/%v86974, /*on_false_vx=*/%v86971 (stack96)
        %v86978 = vxor.u32 %v86977, 2147483648 (stack87)
        %vm86981 = vcmp.lt.f32.partialorder %v86978, 5.0 (stack86)
        %v86986 = vsel /*vm=*/%vm86981, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v86990 = vsel /*vm=*/%vm86981, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v86994 = vsel /*vm=*/%vm86981, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v86998 = vsel /*vm=*/%vm86981, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v87002 = vsel /*vm=*/%vm86981, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v87006 = vsel /*vm=*/%vm86981, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v87010 = vsel /*vm=*/%vm86981, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v87014 = vsel /*vm=*/%vm86981, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v87018 = vsel /*vm=*/%vm86981, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v87022 = vadd.f32 %v86978, -2.5 (stack82)
        %v87024 = vrsqrt.pop %v86978 (stack97)
        %v87025 = vmul.f32 %v86978, %v87024 (stack98)
        %vm87026 = vcmp.eq.f32.partialorder %v86978, inf (stack99)
        %v87027 = vsel /*vm=*/%vm87026, /*on_true_vy=*/%v86978, /*on_false_vx=*/%v87025 (stack100)
        %vm87028 = vcmp.eq.f32.partialorder %v86978, 0.0 (stack101)
        %v87029 = vand.u32 %v86978, 2147483648 (stack102)
        %v87030 = vsel /*vm=*/%vm87028, /*on_true_vy=*/%v87029, /*on_false_vx=*/%v87027 (stack103)
        %v87033 = vadd.f32 %v87030, -3.0 (stack82)
        %v87037 = vsel /*vm=*/%vm86981, /*on_true_vy=*/%v87022, /*on_false_vx=*/%v87033 (stack72)
        %v87041 = vmul.f32 %v87018, %v87037 (stack83)
        %v87045 = vadd.f32 %v87014, %v87041 (stack82)
        %v87049 = vmul.f32 %v87045, %v87037 (stack83)
        %v87053 = vadd.f32 %v87010, %v87049 (stack82)
        %v87057 = vmul.f32 %v87053, %v87037 (stack83)
        %v87061 = vadd.f32 %v87006, %v87057 (stack82)
        %v87065 = vmul.f32 %v87061, %v87037 (stack83)
        %v87069 = vadd.f32 %v87002, %v87065 (stack82)
        %v87073 = vmul.f32 %v87069, %v87037 (stack83)
        %v87077 = vadd.f32 %v86998, %v87073 (stack82)
        %v87081 = vmul.f32 %v87077, %v87037 (stack83)
        %v87085 = vadd.f32 %v86994, %v87081 (stack82)
        %v87089 = vmul.f32 %v87085, %v87037 (stack83)
        %v87093 = vadd.f32 %v86990, %v87089 (stack82)
        %v87097 = vmul.f32 %v87093, %v87037 (stack83)
        %v87101 = vadd.f32 %v86986, %v87097 (stack82)
        %v87105 = vmul.f32 %v87101, %v86952 (stack83)
        %v87109 = vsel /*vm=*/%vm86957, /*on_true_vy=*/%v86962, /*on_false_vx=*/%v87105 (stack72)
        %v87113 = vmul.f32 %v87109, 1.4140625 (stack83)
        %s87115 = scalar_lea.vmem %s280, 220 [#allocation0] (stack107)
        %v87116 = vpack.c.bf16 0.0, %v87113 (stack104)
        %87117 = vst [vmem:[%s87115] sm:$0xf] /*vst_source=*/%v87116 (stack105)
        %v87120 = vadd.s32 %v1381, %v86195 (stack65)
        %s87122 = smul.u32 128, %s27 (stack66)
        %v87123 = vlaneseq (stack67)
        %v87124 = vand.u32 %v87123, 127 (stack68)
        %v87125 = vstv %s87122 (stack69)
        %v87126 = vadd.s32 %v87124, %v87125 (stack70)
        %v87130 = vadd.s32 %v87120, %v87126 (stack65)
        %vm87134 = vcmp.lt.u32.totalorder %v87130, %v87120 (stack71)
        %vm87139 = vcmp.lt.u32.totalorder %v87120, %v1381 (stack71)
        %v87144 = vadd.s32 %v1368, %v86178 (stack65)
        %v87148 = vadd.s32 %v87144, 1 (stack65)
        %v87152 = vsel /*vm=*/%vm87139, /*on_true_vy=*/%v87148, /*on_false_vx=*/%v87144 (stack72)
        %v87156 = vadd.s32 %v87152, 1 (stack65)
        %v87160 = vsel /*vm=*/%vm87134, /*on_true_vy=*/%v87156, /*on_false_vx=*/%v87152 (stack72)
        %v87165 = vadd.s32 %v87160, %v10 (stack65)
        %v87169 = vadd.s32 %v87130, %v9 (stack65)
        %v87173 = vadd.s32 %v87165, %v87169 (stack65)
        %v87175 = vshll.u32 %v87169, 13 (stack73)
        %v87176 = vshrl.u32 %v87169, 19 (stack74)
        %v87177 = vor.u32 %v87175, %v87176 (stack75)
        %v87178 = vxor.u32 %v87173, %v87177 (stack76)
        %v87181 = vadd.s32 %v87173, %v87178 (stack65)
        %v87183 = vshll.u32 %v87178, 15 (stack73)
        %v87184 = vshrl.u32 %v87178, 17 (stack74)
        %v87185 = vor.u32 %v87183, %v87184 (stack75)
        %v87186 = vxor.u32 %v87181, %v87185 (stack76)
        %v87189 = vadd.s32 %v87181, %v87186 (stack65)
        %v87191 = vshll.u32 %v87186, 26 (stack73)
        %v87192 = vshrl.u32 %v87186, 6 (stack74)
        %v87193 = vor.u32 %v87191, %v87192 (stack75)
        %v87194 = vxor.u32 %v87189, %v87193 (stack76)
        %v87197 = vadd.s32 %v87189, %v87194 (stack65)
        %v87201 = vadd.s32 %v87197, %v9 (stack65)
        %v87203 = vshll.u32 %v87194, 6 (stack73)
        %v87204 = vshrl.u32 %v87194, 26 (stack74)
        %v87205 = vor.u32 %v87203, %v87204 (stack75)
        %v87206 = vxor.u32 %v87197, %v87205 (stack76)
        %v87209 = vadd.s32 %v87206, %v8 (stack65)
        %v87213 = vadd.s32 %v87209, 1 (stack65)
        %v87217 = vadd.s32 %v87201, %v87213 (stack65)
        %v87219 = vshll.u32 %v87213, 17 (stack73)
        %v87220 = vshrl.u32 %v87213, 15 (stack74)
        %v87221 = vor.u32 %v87219, %v87220 (stack75)
        %v87222 = vxor.u32 %v87217, %v87221 (stack76)
        %v87225 = vadd.s32 %v87217, %v87222 (stack65)
        %v87227 = vshll.u32 %v87222, 29 (stack73)
        %v87228 = vshrl.u32 %v87222, 3 (stack74)
        %v87229 = vor.u32 %v87227, %v87228 (stack75)
        %v87230 = vxor.u32 %v87225, %v87229 (stack76)
        %v87233 = vadd.s32 %v87225, %v87230 (stack65)
        %v87235 = vshll.u32 %v87230, 16 (stack73)
        %v87236 = vshrl.u32 %v87230, 16 (stack74)
        %v87237 = vor.u32 %v87235, %v87236 (stack75)
        %v87238 = vxor.u32 %v87233, %v87237 (stack76)
        %v87241 = vadd.s32 %v87233, %v87238 (stack65)
        %v87245 = vadd.s32 %v87241, %v8 (stack65)
        %v87247 = vshll.u32 %v87238, 24 (stack73)
        %v87248 = vshrl.u32 %v87238, 8 (stack74)
        %v87249 = vor.u32 %v87247, %v87248 (stack75)
        %v87250 = vxor.u32 %v87241, %v87249 (stack76)
        %v87253 = vadd.s32 %v87250, %v10 (stack65)
        %v87257 = vadd.s32 %v87253, 2 (stack65)
        %v87261 = vadd.s32 %v87245, %v87257 (stack65)
        %v87263 = vshll.u32 %v87257, 13 (stack73)
        %v87264 = vshrl.u32 %v87257, 19 (stack74)
        %v87265 = vor.u32 %v87263, %v87264 (stack75)
        %v87266 = vxor.u32 %v87261, %v87265 (stack76)
        %v87269 = vadd.s32 %v87261, %v87266 (stack65)
        %v87271 = vshll.u32 %v87266, 15 (stack73)
        %v87272 = vshrl.u32 %v87266, 17 (stack74)
        %v87273 = vor.u32 %v87271, %v87272 (stack75)
        %v87274 = vxor.u32 %v87269, %v87273 (stack76)
        %v87277 = vadd.s32 %v87269, %v87274 (stack65)
        %v87279 = vshll.u32 %v87274, 26 (stack73)
        %v87280 = vshrl.u32 %v87274, 6 (stack74)
        %v87281 = vor.u32 %v87279, %v87280 (stack75)
        %v87282 = vxor.u32 %v87277, %v87281 (stack76)
        %v87285 = vadd.s32 %v87277, %v87282 (stack65)
        %v87289 = vadd.s32 %v87285, %v10 (stack65)
        %v87291 = vshll.u32 %v87282, 6 (stack73)
        %v87292 = vshrl.u32 %v87282, 26 (stack74)
        %v87293 = vor.u32 %v87291, %v87292 (stack75)
        %v87294 = vxor.u32 %v87285, %v87293 (stack76)
        %v87297 = vadd.s32 %v87294, %v9 (stack65)
        %v87301 = vadd.s32 %v87297, 3 (stack65)
        %v87305 = vadd.s32 %v87289, %v87301 (stack65)
        %v87307 = vshll.u32 %v87301, 17 (stack73)
        %v87308 = vshrl.u32 %v87301, 15 (stack74)
        %v87309 = vor.u32 %v87307, %v87308 (stack75)
        %v87310 = vxor.u32 %v87305, %v87309 (stack76)
        %v87313 = vadd.s32 %v87305, %v87310 (stack65)
        %v87315 = vshll.u32 %v87310, 29 (stack73)
        %v87316 = vshrl.u32 %v87310, 3 (stack74)
        %v87317 = vor.u32 %v87315, %v87316 (stack75)
        %v87318 = vxor.u32 %v87313, %v87317 (stack76)
        %v87321 = vadd.s32 %v87313, %v87318 (stack65)
        %v87323 = vshll.u32 %v87318, 16 (stack73)
        %v87324 = vshrl.u32 %v87318, 16 (stack74)
        %v87325 = vor.u32 %v87323, %v87324 (stack75)
        %v87326 = vxor.u32 %v87321, %v87325 (stack76)
        %v87329 = vadd.s32 %v87321, %v87326 (stack65)
        %v87333 = vadd.s32 %v87329, %v9 (stack65)
        %v87335 = vshll.u32 %v87326, 24 (stack73)
        %v87336 = vshrl.u32 %v87326, 8 (stack74)
        %v87337 = vor.u32 %v87335, %v87336 (stack75)
        %v87338 = vxor.u32 %v87329, %v87337 (stack76)
        %v87341 = vadd.s32 %v87338, %v8 (stack65)
        %v87345 = vadd.s32 %v87341, 4 (stack65)
        %v87349 = vadd.s32 %v87333, %v87345 (stack65)
        %v87351 = vshll.u32 %v87345, 13 (stack73)
        %v87352 = vshrl.u32 %v87345, 19 (stack74)
        %v87353 = vor.u32 %v87351, %v87352 (stack75)
        %v87354 = vxor.u32 %v87349, %v87353 (stack76)
        %v87357 = vadd.s32 %v87349, %v87354 (stack65)
        %v87359 = vshll.u32 %v87354, 15 (stack73)
        %v87360 = vshrl.u32 %v87354, 17 (stack74)
        %v87361 = vor.u32 %v87359, %v87360 (stack75)
        %v87362 = vxor.u32 %v87357, %v87361 (stack76)
        %v87365 = vadd.s32 %v87357, %v87362 (stack65)
        %v87367 = vshll.u32 %v87362, 26 (stack73)
        %v87368 = vshrl.u32 %v87362, 6 (stack74)
        %v87369 = vor.u32 %v87367, %v87368 (stack75)
        %v87370 = vxor.u32 %v87365, %v87369 (stack76)
        %v87373 = vadd.s32 %v87365, %v87370 (stack65)
        %v87377 = vadd.s32 %v87373, %v8 (stack65)
        %v87379 = vshll.u32 %v87370, 6 (stack73)
        %v87380 = vshrl.u32 %v87370, 26 (stack74)
        %v87381 = vor.u32 %v87379, %v87380 (stack75)
        %v87382 = vxor.u32 %v87373, %v87381 (stack76)
        %v87385 = vadd.s32 %v87382, %v10 (stack65)
        %v87389 = vadd.s32 %v87385, 5 (stack65)
        %v87391 = vxor.u32 %v87377, %v87389 (stack76)
        %v87392 = vand.u32.u8 %v87391, 255 (stack77)
        %v87393 = vand.u32 %v87392, 65535 (stack78)
        %v87394 = vshrl.u32 %v87393, 1 (stack79)
        %v87395 = vor.u32 %v87394, 16256 (stack75)
        %v87396 = vand.u32.u16 %v87395, 65535 (stack80)
        %v87397 = vunpack.i.l.bf16 %v87396 (stack81)
        %v87401 = vadd.f32 %v87397, -1.0 (stack82)
        %v87405 = vmul.f32 %v87401, 2.0 (stack83)
        %v87409 = vadd.f32 %v87405, -0.99609375 (stack82)
        %v87413 = vmax.f32 -0.99609375, %v87409 (stack84)
        %v87415 = vand.u32 2147483647, %v87413 (stack85)
        %vm87418 = vcmp.eq.f32.partialorder %v87415, 1.0 (stack86)
        %v87423 = vmul.f32 %v87413, inf (stack83)
        %v87425 = vxor.u32 %v87413, 2147483648 (stack87)
        %v87428 = vmul.f32 %v87413, %v87425 (stack83)
        %v87430 = vadd.f32 %v87428, 1.0 (stack88)
        %v87431 = vlog2.pop %v87430 (stack89)
        %v87432 = vmul.f32 %v87431, 0.6931472 (stack90)
        %v87433 = vmul.f32 -0.5, %v87428 (stack91)
        %v87434 = vadd.f32 %v87433, 1.0 (stack92)
        %v87435 = vmul.f32 %v87434, %v87428 (stack93)
        %v87436 = vand.u32 2147483647, %v87428 (stack94)
        %vm87437 = vcmp.lt.f32.partialorder %v87436, 0.0004427343 (stack95)
        %v87438 = vsel /*vm=*/%vm87437, /*on_true_vy=*/%v87435, /*on_false_vx=*/%v87432 (stack96)
        %v87439 = vxor.u32 %v87438, 2147483648 (stack87)
        %vm87442 = vcmp.lt.f32.partialorder %v87439, 5.0 (stack86)
        %v87447 = vsel /*vm=*/%vm87442, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v87451 = vsel /*vm=*/%vm87442, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v87455 = vsel /*vm=*/%vm87442, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v87459 = vsel /*vm=*/%vm87442, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v87463 = vsel /*vm=*/%vm87442, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v87467 = vsel /*vm=*/%vm87442, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v87471 = vsel /*vm=*/%vm87442, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v87475 = vsel /*vm=*/%vm87442, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v87479 = vsel /*vm=*/%vm87442, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v87483 = vadd.f32 %v87439, -2.5 (stack82)
        %v87485 = vrsqrt.pop %v87439 (stack97)
        %v87486 = vmul.f32 %v87439, %v87485 (stack98)
        %vm87487 = vcmp.eq.f32.partialorder %v87439, inf (stack99)
        %v87488 = vsel /*vm=*/%vm87487, /*on_true_vy=*/%v87439, /*on_false_vx=*/%v87486 (stack100)
        %vm87489 = vcmp.eq.f32.partialorder %v87439, 0.0 (stack101)
        %v87490 = vand.u32 %v87439, 2147483648 (stack102)
        %v87491 = vsel /*vm=*/%vm87489, /*on_true_vy=*/%v87490, /*on_false_vx=*/%v87488 (stack103)
        %v87494 = vadd.f32 %v87491, -3.0 (stack82)
        %v87498 = vsel /*vm=*/%vm87442, /*on_true_vy=*/%v87483, /*on_false_vx=*/%v87494 (stack72)
        %v87502 = vmul.f32 %v87479, %v87498 (stack83)
        %v87506 = vadd.f32 %v87475, %v87502 (stack82)
        %v87510 = vmul.f32 %v87506, %v87498 (stack83)
        %v87514 = vadd.f32 %v87471, %v87510 (stack82)
        %v87518 = vmul.f32 %v87514, %v87498 (stack83)
        %v87522 = vadd.f32 %v87467, %v87518 (stack82)
        %v87526 = vmul.f32 %v87522, %v87498 (stack83)
        %v87530 = vadd.f32 %v87463, %v87526 (stack82)
        %v87534 = vmul.f32 %v87530, %v87498 (stack83)
        %v87538 = vadd.f32 %v87459, %v87534 (stack82)
        %v87542 = vmul.f32 %v87538, %v87498 (stack83)
        %v87546 = vadd.f32 %v87455, %v87542 (stack82)
        %v87550 = vmul.f32 %v87546, %v87498 (stack83)
        %v87554 = vadd.f32 %v87451, %v87550 (stack82)
        %v87558 = vmul.f32 %v87554, %v87498 (stack83)
        %v87562 = vadd.f32 %v87447, %v87558 (stack82)
        %v87566 = vmul.f32 %v87562, %v87413 (stack83)
        %v87570 = vsel /*vm=*/%vm87418, /*on_true_vy=*/%v87423, /*on_false_vx=*/%v87566 (stack72)
        %v87574 = vmul.f32 %v87570, 1.4140625 (stack83)
        %s87576 = scalar_lea.vmem %s280, 348 [#allocation0] (stack107)
        %v87577 = vpack.c.bf16 0.0, %v87574 (stack104)
        %87578 = vst [vmem:[%s87576] sm:$0xf] /*vst_source=*/%v87577 (stack105)
        %v87581 = vadd.s32 %v1868, %v86195 (stack65)
        %s87583 = smul.u32 128, %s27 (stack66)
        %v87584 = vlaneseq (stack67)
        %v87585 = vand.u32 %v87584, 127 (stack68)
        %v87586 = vstv %s87583 (stack69)
        %v87587 = vadd.s32 %v87585, %v87586 (stack70)
        %v87591 = vadd.s32 %v87581, %v87587 (stack65)
        %vm87595 = vcmp.lt.u32.totalorder %v87591, %v87581 (stack71)
        %vm87600 = vcmp.lt.u32.totalorder %v87581, %v1868 (stack71)
        %v87605 = vadd.s32 %v1855, %v86178 (stack65)
        %v87609 = vadd.s32 %v87605, 1 (stack65)
        %v87613 = vsel /*vm=*/%vm87600, /*on_true_vy=*/%v87609, /*on_false_vx=*/%v87605 (stack72)
        %v87617 = vadd.s32 %v87613, 1 (stack65)
        %v87621 = vsel /*vm=*/%vm87595, /*on_true_vy=*/%v87617, /*on_false_vx=*/%v87613 (stack72)
        %v87626 = vadd.s32 %v87621, %v10 (stack65)
        %v87630 = vadd.s32 %v87591, %v9 (stack65)
        %v87634 = vadd.s32 %v87626, %v87630 (stack65)
        %v87636 = vshll.u32 %v87630, 13 (stack73)
        %v87637 = vshrl.u32 %v87630, 19 (stack74)
        %v87638 = vor.u32 %v87636, %v87637 (stack75)
        %v87639 = vxor.u32 %v87634, %v87638 (stack76)
        %v87642 = vadd.s32 %v87634, %v87639 (stack65)
        %v87644 = vshll.u32 %v87639, 15 (stack73)
        %v87645 = vshrl.u32 %v87639, 17 (stack74)
        %v87646 = vor.u32 %v87644, %v87645 (stack75)
        %v87647 = vxor.u32 %v87642, %v87646 (stack76)
        %v87650 = vadd.s32 %v87642, %v87647 (stack65)
        %v87652 = vshll.u32 %v87647, 26 (stack73)
        %v87653 = vshrl.u32 %v87647, 6 (stack74)
        %v87654 = vor.u32 %v87652, %v87653 (stack75)
        %v87655 = vxor.u32 %v87650, %v87654 (stack76)
        %v87658 = vadd.s32 %v87650, %v87655 (stack65)
        %v87662 = vadd.s32 %v87658, %v9 (stack65)
        %v87664 = vshll.u32 %v87655, 6 (stack73)
        %v87665 = vshrl.u32 %v87655, 26 (stack74)
        %v87666 = vor.u32 %v87664, %v87665 (stack75)
        %v87667 = vxor.u32 %v87658, %v87666 (stack76)
        %v87670 = vadd.s32 %v87667, %v8 (stack65)
        %v87674 = vadd.s32 %v87670, 1 (stack65)
        %v87678 = vadd.s32 %v87662, %v87674 (stack65)
        %v87680 = vshll.u32 %v87674, 17 (stack73)
        %v87681 = vshrl.u32 %v87674, 15 (stack74)
        %v87682 = vor.u32 %v87680, %v87681 (stack75)
        %v87683 = vxor.u32 %v87678, %v87682 (stack76)
        %v87686 = vadd.s32 %v87678, %v87683 (stack65)
        %v87688 = vshll.u32 %v87683, 29 (stack73)
        %v87689 = vshrl.u32 %v87683, 3 (stack74)
        %v87690 = vor.u32 %v87688, %v87689 (stack75)
        %v87691 = vxor.u32 %v87686, %v87690 (stack76)
        %v87694 = vadd.s32 %v87686, %v87691 (stack65)
        %v87696 = vshll.u32 %v87691, 16 (stack73)
        %v87697 = vshrl.u32 %v87691, 16 (stack74)
        %v87698 = vor.u32 %v87696, %v87697 (stack75)
        %v87699 = vxor.u32 %v87694, %v87698 (stack76)
        %v87702 = vadd.s32 %v87694, %v87699 (stack65)
        %v87706 = vadd.s32 %v87702, %v8 (stack65)
        %v87708 = vshll.u32 %v87699, 24 (stack73)
        %v87709 = vshrl.u32 %v87699, 8 (stack74)
        %v87710 = vor.u32 %v87708, %v87709 (stack75)
        %v87711 = vxor.u32 %v87702, %v87710 (stack76)
        %v87714 = vadd.s32 %v87711, %v10 (stack65)
        %v87718 = vadd.s32 %v87714, 2 (stack65)
        %v87722 = vadd.s32 %v87706, %v87718 (stack65)
        %v87724 = vshll.u32 %v87718, 13 (stack73)
        %v87725 = vshrl.u32 %v87718, 19 (stack74)
        %v87726 = vor.u32 %v87724, %v87725 (stack75)
        %v87727 = vxor.u32 %v87722, %v87726 (stack76)
        %v87730 = vadd.s32 %v87722, %v87727 (stack65)
        %v87732 = vshll.u32 %v87727, 15 (stack73)
        %v87733 = vshrl.u32 %v87727, 17 (stack74)
        %v87734 = vor.u32 %v87732, %v87733 (stack75)
        %v87735 = vxor.u32 %v87730, %v87734 (stack76)
        %v87738 = vadd.s32 %v87730, %v87735 (stack65)
        %v87740 = vshll.u32 %v87735, 26 (stack73)
        %v87741 = vshrl.u32 %v87735, 6 (stack74)
        %v87742 = vor.u32 %v87740, %v87741 (stack75)
        %v87743 = vxor.u32 %v87738, %v87742 (stack76)
        %v87746 = vadd.s32 %v87738, %v87743 (stack65)
        %v87750 = vadd.s32 %v87746, %v10 (stack65)
        %v87752 = vshll.u32 %v87743, 6 (stack73)
        %v87753 = vshrl.u32 %v87743, 26 (stack74)
        %v87754 = vor.u32 %v87752, %v87753 (stack75)
        %v87755 = vxor.u32 %v87746, %v87754 (stack76)
        %v87758 = vadd.s32 %v87755, %v9 (stack65)
        %v87762 = vadd.s32 %v87758, 3 (stack65)
        %v87766 = vadd.s32 %v87750, %v87762 (stack65)
        %v87768 = vshll.u32 %v87762, 17 (stack73)
        %v87769 = vshrl.u32 %v87762, 15 (stack74)
        %v87770 = vor.u32 %v87768, %v87769 (stack75)
        %v87771 = vxor.u32 %v87766, %v87770 (stack76)
        %v87774 = vadd.s32 %v87766, %v87771 (stack65)
        %v87776 = vshll.u32 %v87771, 29 (stack73)
        %v87777 = vshrl.u32 %v87771, 3 (stack74)
        %v87778 = vor.u32 %v87776, %v87777 (stack75)
        %v87779 = vxor.u32 %v87774, %v87778 (stack76)
        %v87782 = vadd.s32 %v87774, %v87779 (stack65)
        %v87784 = vshll.u32 %v87779, 16 (stack73)
        %v87785 = vshrl.u32 %v87779, 16 (stack74)
        %v87786 = vor.u32 %v87784, %v87785 (stack75)
        %v87787 = vxor.u32 %v87782, %v87786 (stack76)
        %v87790 = vadd.s32 %v87782, %v87787 (stack65)
        %v87794 = vadd.s32 %v87790, %v9 (stack65)
        %v87796 = vshll.u32 %v87787, 24 (stack73)
        %v87797 = vshrl.u32 %v87787, 8 (stack74)
        %v87798 = vor.u32 %v87796, %v87797 (stack75)
        %v87799 = vxor.u32 %v87790, %v87798 (stack76)
        %v87802 = vadd.s32 %v87799, %v8 (stack65)
        %v87806 = vadd.s32 %v87802, 4 (stack65)
        %v87810 = vadd.s32 %v87794, %v87806 (stack65)
        %v87812 = vshll.u32 %v87806, 13 (stack73)
        %v87813 = vshrl.u32 %v87806, 19 (stack74)
        %v87814 = vor.u32 %v87812, %v87813 (stack75)
        %v87815 = vxor.u32 %v87810, %v87814 (stack76)
        %v87818 = vadd.s32 %v87810, %v87815 (stack65)
        %v87820 = vshll.u32 %v87815, 15 (stack73)
        %v87821 = vshrl.u32 %v87815, 17 (stack74)
        %v87822 = vor.u32 %v87820, %v87821 (stack75)
        %v87823 = vxor.u32 %v87818, %v87822 (stack76)
        %v87826 = vadd.s32 %v87818, %v87823 (stack65)
        %v87828 = vshll.u32 %v87823, 26 (stack73)
        %v87829 = vshrl.u32 %v87823, 6 (stack74)
        %v87830 = vor.u32 %v87828, %v87829 (stack75)
        %v87831 = vxor.u32 %v87826, %v87830 (stack76)
        %v87834 = vadd.s32 %v87826, %v87831 (stack65)
        %v87838 = vadd.s32 %v87834, %v8 (stack65)
        %v87840 = vshll.u32 %v87831, 6 (stack73)
        %v87841 = vshrl.u32 %v87831, 26 (stack74)
        %v87842 = vor.u32 %v87840, %v87841 (stack75)
        %v87843 = vxor.u32 %v87834, %v87842 (stack76)
        %v87846 = vadd.s32 %v87843, %v10 (stack65)
        %v87850 = vadd.s32 %v87846, 5 (stack65)
        %v87852 = vxor.u32 %v87838, %v87850 (stack76)
        %v87853 = vand.u32.u8 %v87852, 255 (stack77)
        %v87854 = vand.u32 %v87853, 65535 (stack78)
        %v87855 = vshrl.u32 %v87854, 1 (stack79)
        %v87856 = vor.u32 %v87855, 16256 (stack75)
        %v87857 = vand.u32.u16 %v87856, 65535 (stack80)
        %v87858 = vunpack.i.l.bf16 %v87857 (stack81)
        %v87862 = vadd.f32 %v87858, -1.0 (stack82)
        %v87866 = vmul.f32 %v87862, 2.0 (stack83)
        %v87870 = vadd.f32 %v87866, -0.99609375 (stack82)
        %v87874 = vmax.f32 -0.99609375, %v87870 (stack84)
        %v87876 = vand.u32 2147483647, %v87874 (stack85)
        %vm87879 = vcmp.eq.f32.partialorder %v87876, 1.0 (stack86)
        %v87884 = vmul.f32 %v87874, inf (stack83)
        %v87886 = vxor.u32 %v87874, 2147483648 (stack87)
        %v87889 = vmul.f32 %v87874, %v87886 (stack83)
        %v87891 = vadd.f32 %v87889, 1.0 (stack88)
        %v87892 = vlog2.pop %v87891 (stack89)
        %v87893 = vmul.f32 %v87892, 0.6931472 (stack90)
        %v87894 = vmul.f32 -0.5, %v87889 (stack91)
        %v87895 = vadd.f32 %v87894, 1.0 (stack92)
        %v87896 = vmul.f32 %v87895, %v87889 (stack93)
        %v87897 = vand.u32 2147483647, %v87889 (stack94)
        %vm87898 = vcmp.lt.f32.partialorder %v87897, 0.0004427343 (stack95)
        %v87899 = vsel /*vm=*/%vm87898, /*on_true_vy=*/%v87896, /*on_false_vx=*/%v87893 (stack96)
        %v87900 = vxor.u32 %v87899, 2147483648 (stack87)
        %vm87903 = vcmp.lt.f32.partialorder %v87900, 5.0 (stack86)
        %v87908 = vsel /*vm=*/%vm87903, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v87912 = vsel /*vm=*/%vm87903, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v87916 = vsel /*vm=*/%vm87903, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v87920 = vsel /*vm=*/%vm87903, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v87924 = vsel /*vm=*/%vm87903, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v87928 = vsel /*vm=*/%vm87903, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v87932 = vsel /*vm=*/%vm87903, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v87936 = vsel /*vm=*/%vm87903, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v87940 = vsel /*vm=*/%vm87903, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v87944 = vadd.f32 %v87900, -2.5 (stack82)
        %v87946 = vrsqrt.pop %v87900 (stack97)
        %v87947 = vmul.f32 %v87900, %v87946 (stack98)
        %vm87948 = vcmp.eq.f32.partialorder %v87900, inf (stack99)
        %v87949 = vsel /*vm=*/%vm87948, /*on_true_vy=*/%v87900, /*on_false_vx=*/%v87947 (stack100)
        %vm87950 = vcmp.eq.f32.partialorder %v87900, 0.0 (stack101)
        %v87951 = vand.u32 %v87900, 2147483648 (stack102)
        %v87952 = vsel /*vm=*/%vm87950, /*on_true_vy=*/%v87951, /*on_false_vx=*/%v87949 (stack103)
        %v87955 = vadd.f32 %v87952, -3.0 (stack82)
        %v87959 = vsel /*vm=*/%vm87903, /*on_true_vy=*/%v87944, /*on_false_vx=*/%v87955 (stack72)
        %v87963 = vmul.f32 %v87940, %v87959 (stack83)
        %v87967 = vadd.f32 %v87936, %v87963 (stack82)
        %v87971 = vmul.f32 %v87967, %v87959 (stack83)
        %v87975 = vadd.f32 %v87932, %v87971 (stack82)
        %v87979 = vmul.f32 %v87975, %v87959 (stack83)
        %v87983 = vadd.f32 %v87928, %v87979 (stack82)
        %v87987 = vmul.f32 %v87983, %v87959 (stack83)
        %v87991 = vadd.f32 %v87924, %v87987 (stack82)
        %v87995 = vmul.f32 %v87991, %v87959 (stack83)
        %v87999 = vadd.f32 %v87920, %v87995 (stack82)
        %v88003 = vmul.f32 %v87999, %v87959 (stack83)
        %v88007 = vadd.f32 %v87916, %v88003 (stack82)
        %v88011 = vmul.f32 %v88007, %v87959 (stack83)
        %v88015 = vadd.f32 %v87912, %v88011 (stack82)
        %v88019 = vmul.f32 %v88015, %v87959 (stack83)
        %v88023 = vadd.f32 %v87908, %v88019 (stack82)
        %v88027 = vmul.f32 %v88023, %v87874 (stack83)
        %v88031 = vsel /*vm=*/%vm87879, /*on_true_vy=*/%v87884, /*on_false_vx=*/%v88027 (stack72)
        %v88035 = vmul.f32 %v88031, 1.4140625 (stack83)
        %s88037 = scalar_lea.vmem %s280, 476 [#allocation0] (stack107)
        %v88038 = vpack.c.bf16 0.0, %v88035 (stack104)
        %88039 = vst [vmem:[%s88037] sm:$0xf] /*vst_source=*/%v88038 (stack105)
        %v88042 = vadd.s32 %v2355, %v86195 (stack65)
        %s88044 = smul.u32 128, %s27 (stack66)
        %v88045 = vlaneseq (stack67)
        %v88046 = vand.u32 %v88045, 127 (stack68)
        %v88047 = vstv %s88044 (stack69)
        %v88048 = vadd.s32 %v88046, %v88047 (stack70)
        %v88052 = vadd.s32 %v88042, %v88048 (stack65)
        %vm88056 = vcmp.lt.u32.totalorder %v88052, %v88042 (stack71)
        %vm88061 = vcmp.lt.u32.totalorder %v88042, %v2355 (stack71)
        %v88066 = vadd.s32 %v2342, %v86178 (stack65)
        %v88070 = vadd.s32 %v88066, 1 (stack65)
        %v88074 = vsel /*vm=*/%vm88061, /*on_true_vy=*/%v88070, /*on_false_vx=*/%v88066 (stack72)
        %v88078 = vadd.s32 %v88074, 1 (stack65)
        %v88082 = vsel /*vm=*/%vm88056, /*on_true_vy=*/%v88078, /*on_false_vx=*/%v88074 (stack72)
        %v88087 = vadd.s32 %v88082, %v10 (stack65)
        %v88091 = vadd.s32 %v88052, %v9 (stack65)
        %v88095 = vadd.s32 %v88087, %v88091 (stack65)
        %v88097 = vshll.u32 %v88091, 13 (stack73)
        %v88098 = vshrl.u32 %v88091, 19 (stack74)
        %v88099 = vor.u32 %v88097, %v88098 (stack75)
        %v88100 = vxor.u32 %v88095, %v88099 (stack76)
        %v88103 = vadd.s32 %v88095, %v88100 (stack65)
        %v88105 = vshll.u32 %v88100, 15 (stack73)
        %v88106 = vshrl.u32 %v88100, 17 (stack74)
        %v88107 = vor.u32 %v88105, %v88106 (stack75)
        %v88108 = vxor.u32 %v88103, %v88107 (stack76)
        %v88111 = vadd.s32 %v88103, %v88108 (stack65)
        %v88113 = vshll.u32 %v88108, 26 (stack73)
        %v88114 = vshrl.u32 %v88108, 6 (stack74)
        %v88115 = vor.u32 %v88113, %v88114 (stack75)
        %v88116 = vxor.u32 %v88111, %v88115 (stack76)
        %v88119 = vadd.s32 %v88111, %v88116 (stack65)
        %v88123 = vadd.s32 %v88119, %v9 (stack65)
        %v88125 = vshll.u32 %v88116, 6 (stack73)
        %v88126 = vshrl.u32 %v88116, 26 (stack74)
        %v88127 = vor.u32 %v88125, %v88126 (stack75)
        %v88128 = vxor.u32 %v88119, %v88127 (stack76)
        %v88131 = vadd.s32 %v88128, %v8 (stack65)
        %v88135 = vadd.s32 %v88131, 1 (stack65)
        %v88139 = vadd.s32 %v88123, %v88135 (stack65)
        %v88141 = vshll.u32 %v88135, 17 (stack73)
        %v88142 = vshrl.u32 %v88135, 15 (stack74)
        %v88143 = vor.u32 %v88141, %v88142 (stack75)
        %v88144 = vxor.u32 %v88139, %v88143 (stack76)
        %v88147 = vadd.s32 %v88139, %v88144 (stack65)
        %v88149 = vshll.u32 %v88144, 29 (stack73)
        %v88150 = vshrl.u32 %v88144, 3 (stack74)
        %v88151 = vor.u32 %v88149, %v88150 (stack75)
        %v88152 = vxor.u32 %v88147, %v88151 (stack76)
        %v88155 = vadd.s32 %v88147, %v88152 (stack65)
        %v88157 = vshll.u32 %v88152, 16 (stack73)
        %v88158 = vshrl.u32 %v88152, 16 (stack74)
        %v88159 = vor.u32 %v88157, %v88158 (stack75)
        %v88160 = vxor.u32 %v88155, %v88159 (stack76)
        %v88163 = vadd.s32 %v88155, %v88160 (stack65)
        %v88167 = vadd.s32 %v88163, %v8 (stack65)
        %v88169 = vshll.u32 %v88160, 24 (stack73)
        %v88170 = vshrl.u32 %v88160, 8 (stack74)
        %v88171 = vor.u32 %v88169, %v88170 (stack75)
        %v88172 = vxor.u32 %v88163, %v88171 (stack76)
        %v88175 = vadd.s32 %v88172, %v10 (stack65)
        %v88179 = vadd.s32 %v88175, 2 (stack65)
        %v88183 = vadd.s32 %v88167, %v88179 (stack65)
        %v88185 = vshll.u32 %v88179, 13 (stack73)
        %v88186 = vshrl.u32 %v88179, 19 (stack74)
        %v88187 = vor.u32 %v88185, %v88186 (stack75)
        %v88188 = vxor.u32 %v88183, %v88187 (stack76)
        %v88191 = vadd.s32 %v88183, %v88188 (stack65)
        %v88193 = vshll.u32 %v88188, 15 (stack73)
        %v88194 = vshrl.u32 %v88188, 17 (stack74)
        %v88195 = vor.u32 %v88193, %v88194 (stack75)
        %v88196 = vxor.u32 %v88191, %v88195 (stack76)
        %v88199 = vadd.s32 %v88191, %v88196 (stack65)
        %v88201 = vshll.u32 %v88196, 26 (stack73)
        %v88202 = vshrl.u32 %v88196, 6 (stack74)
        %v88203 = vor.u32 %v88201, %v88202 (stack75)
        %v88204 = vxor.u32 %v88199, %v88203 (stack76)
        %v88207 = vadd.s32 %v88199, %v88204 (stack65)
        %v88211 = vadd.s32 %v88207, %v10 (stack65)
        %v88213 = vshll.u32 %v88204, 6 (stack73)
        %v88214 = vshrl.u32 %v88204, 26 (stack74)
        %v88215 = vor.u32 %v88213, %v88214 (stack75)
        %v88216 = vxor.u32 %v88207, %v88215 (stack76)
        %v88219 = vadd.s32 %v88216, %v9 (stack65)
        %v88223 = vadd.s32 %v88219, 3 (stack65)
        %v88227 = vadd.s32 %v88211, %v88223 (stack65)
        %v88229 = vshll.u32 %v88223, 17 (stack73)
        %v88230 = vshrl.u32 %v88223, 15 (stack74)
        %v88231 = vor.u32 %v88229, %v88230 (stack75)
        %v88232 = vxor.u32 %v88227, %v88231 (stack76)
        %v88235 = vadd.s32 %v88227, %v88232 (stack65)
        %v88237 = vshll.u32 %v88232, 29 (stack73)
        %v88238 = vshrl.u32 %v88232, 3 (stack74)
        %v88239 = vor.u32 %v88237, %v88238 (stack75)
        %v88240 = vxor.u32 %v88235, %v88239 (stack76)
        %v88243 = vadd.s32 %v88235, %v88240 (stack65)
        %v88245 = vshll.u32 %v88240, 16 (stack73)
        %v88246 = vshrl.u32 %v88240, 16 (stack74)
        %v88247 = vor.u32 %v88245, %v88246 (stack75)
        %v88248 = vxor.u32 %v88243, %v88247 (stack76)
        %v88251 = vadd.s32 %v88243, %v88248 (stack65)
        %v88255 = vadd.s32 %v88251, %v9 (stack65)
        %v88257 = vshll.u32 %v88248, 24 (stack73)
        %v88258 = vshrl.u32 %v88248, 8 (stack74)
        %v88259 = vor.u32 %v88257, %v88258 (stack75)
        %v88260 = vxor.u32 %v88251, %v88259 (stack76)
        %v88263 = vadd.s32 %v88260, %v8 (stack65)
        %v88267 = vadd.s32 %v88263, 4 (stack65)
        %v88271 = vadd.s32 %v88255, %v88267 (stack65)
        %v88273 = vshll.u32 %v88267, 13 (stack73)
        %v88274 = vshrl.u32 %v88267, 19 (stack74)
        %v88275 = vor.u32 %v88273, %v88274 (stack75)
        %v88276 = vxor.u32 %v88271, %v88275 (stack76)
        %v88279 = vadd.s32 %v88271, %v88276 (stack65)
        %v88281 = vshll.u32 %v88276, 15 (stack73)
        %v88282 = vshrl.u32 %v88276, 17 (stack74)
        %v88283 = vor.u32 %v88281, %v88282 (stack75)
        %v88284 = vxor.u32 %v88279, %v88283 (stack76)
        %v88287 = vadd.s32 %v88279, %v88284 (stack65)
        %v88289 = vshll.u32 %v88284, 26 (stack73)
        %v88290 = vshrl.u32 %v88284, 6 (stack74)
        %v88291 = vor.u32 %v88289, %v88290 (stack75)
        %v88292 = vxor.u32 %v88287, %v88291 (stack76)
        %v88295 = vadd.s32 %v88287, %v88292 (stack65)
        %v88299 = vadd.s32 %v88295, %v8 (stack65)
        %v88301 = vshll.u32 %v88292, 6 (stack73)
        %v88302 = vshrl.u32 %v88292, 26 (stack74)
        %v88303 = vor.u32 %v88301, %v88302 (stack75)
        %v88304 = vxor.u32 %v88295, %v88303 (stack76)
        %v88307 = vadd.s32 %v88304, %v10 (stack65)
        %v88311 = vadd.s32 %v88307, 5 (stack65)
        %v88313 = vxor.u32 %v88299, %v88311 (stack76)
        %v88314 = vand.u32.u8 %v88313, 255 (stack77)
        %v88315 = vand.u32 %v88314, 65535 (stack78)
        %v88316 = vshrl.u32 %v88315, 1 (stack79)
        %v88317 = vor.u32 %v88316, 16256 (stack75)
        %v88318 = vand.u32.u16 %v88317, 65535 (stack80)
        %v88319 = vunpack.i.l.bf16 %v88318 (stack81)
        %v88323 = vadd.f32 %v88319, -1.0 (stack82)
        %v88327 = vmul.f32 %v88323, 2.0 (stack83)
        %v88331 = vadd.f32 %v88327, -0.99609375 (stack82)
        %v88335 = vmax.f32 -0.99609375, %v88331 (stack84)
        %v88337 = vand.u32 2147483647, %v88335 (stack85)
        %vm88340 = vcmp.eq.f32.partialorder %v88337, 1.0 (stack86)
        %v88345 = vmul.f32 %v88335, inf (stack83)
        %v88347 = vxor.u32 %v88335, 2147483648 (stack87)
        %v88350 = vmul.f32 %v88335, %v88347 (stack83)
        %v88352 = vadd.f32 %v88350, 1.0 (stack88)
        %v88353 = vlog2.pop %v88352 (stack89)
        %v88354 = vmul.f32 %v88353, 0.6931472 (stack90)
        %v88355 = vmul.f32 -0.5, %v88350 (stack91)
        %v88356 = vadd.f32 %v88355, 1.0 (stack92)
        %v88357 = vmul.f32 %v88356, %v88350 (stack93)
        %v88358 = vand.u32 2147483647, %v88350 (stack94)
        %vm88359 = vcmp.lt.f32.partialorder %v88358, 0.0004427343 (stack95)
        %v88360 = vsel /*vm=*/%vm88359, /*on_true_vy=*/%v88357, /*on_false_vx=*/%v88354 (stack96)
        %v88361 = vxor.u32 %v88360, 2147483648 (stack87)
        %vm88364 = vcmp.lt.f32.partialorder %v88361, 5.0 (stack86)
        %v88369 = vsel /*vm=*/%vm88364, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v88373 = vsel /*vm=*/%vm88364, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v88377 = vsel /*vm=*/%vm88364, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v88381 = vsel /*vm=*/%vm88364, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v88385 = vsel /*vm=*/%vm88364, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v88389 = vsel /*vm=*/%vm88364, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v88393 = vsel /*vm=*/%vm88364, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v88397 = vsel /*vm=*/%vm88364, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v88401 = vsel /*vm=*/%vm88364, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v88405 = vadd.f32 %v88361, -2.5 (stack82)
        %v88407 = vrsqrt.pop %v88361 (stack97)
        %v88408 = vmul.f32 %v88361, %v88407 (stack98)
        %vm88409 = vcmp.eq.f32.partialorder %v88361, inf (stack99)
        %v88410 = vsel /*vm=*/%vm88409, /*on_true_vy=*/%v88361, /*on_false_vx=*/%v88408 (stack100)
        %vm88411 = vcmp.eq.f32.partialorder %v88361, 0.0 (stack101)
        %v88412 = vand.u32 %v88361, 2147483648 (stack102)
        %v88413 = vsel /*vm=*/%vm88411, /*on_true_vy=*/%v88412, /*on_false_vx=*/%v88410 (stack103)
        %v88416 = vadd.f32 %v88413, -3.0 (stack82)
        %v88420 = vsel /*vm=*/%vm88364, /*on_true_vy=*/%v88405, /*on_false_vx=*/%v88416 (stack72)
        %v88424 = vmul.f32 %v88401, %v88420 (stack83)
        %v88428 = vadd.f32 %v88397, %v88424 (stack82)
        %v88432 = vmul.f32 %v88428, %v88420 (stack83)
        %v88436 = vadd.f32 %v88393, %v88432 (stack82)
        %v88440 = vmul.f32 %v88436, %v88420 (stack83)
        %v88444 = vadd.f32 %v88389, %v88440 (stack82)
        %v88448 = vmul.f32 %v88444, %v88420 (stack83)
        %v88452 = vadd.f32 %v88385, %v88448 (stack82)
        %v88456 = vmul.f32 %v88452, %v88420 (stack83)
        %v88460 = vadd.f32 %v88381, %v88456 (stack82)
        %v88464 = vmul.f32 %v88460, %v88420 (stack83)
        %v88468 = vadd.f32 %v88377, %v88464 (stack82)
        %v88472 = vmul.f32 %v88468, %v88420 (stack83)
        %v88476 = vadd.f32 %v88373, %v88472 (stack82)
        %v88480 = vmul.f32 %v88476, %v88420 (stack83)
        %v88484 = vadd.f32 %v88369, %v88480 (stack82)
        %v88488 = vmul.f32 %v88484, %v88335 (stack83)
        %v88492 = vsel /*vm=*/%vm88340, /*on_true_vy=*/%v88345, /*on_false_vx=*/%v88488 (stack72)
        %v88496 = vmul.f32 %v88492, 1.4140625 (stack83)
        %s88498 = scalar_lea.vmem %s280, 604 [#allocation0] (stack107)
        %v88499 = vpack.c.bf16 0.0, %v88496 (stack104)
        %88500 = vst [vmem:[%s88498] sm:$0xf] /*vst_source=*/%v88499 (stack105)
        %v88503 = vadd.s32 %v2842, %v86195 (stack65)
        %s88505 = smul.u32 128, %s27 (stack66)
        %v88506 = vlaneseq (stack67)
        %v88507 = vand.u32 %v88506, 127 (stack68)
        %v88508 = vstv %s88505 (stack69)
        %v88509 = vadd.s32 %v88507, %v88508 (stack70)
        %v88513 = vadd.s32 %v88503, %v88509 (stack65)
        %vm88517 = vcmp.lt.u32.totalorder %v88513, %v88503 (stack71)
        %vm88522 = vcmp.lt.u32.totalorder %v88503, %v2842 (stack71)
        %v88527 = vadd.s32 %v2829, %v86178 (stack65)
        %v88531 = vadd.s32 %v88527, 1 (stack65)
        %v88535 = vsel /*vm=*/%vm88522, /*on_true_vy=*/%v88531, /*on_false_vx=*/%v88527 (stack72)
        %v88539 = vadd.s32 %v88535, 1 (stack65)
        %v88543 = vsel /*vm=*/%vm88517, /*on_true_vy=*/%v88539, /*on_false_vx=*/%v88535 (stack72)
        %v88548 = vadd.s32 %v88543, %v10 (stack65)
        %v88552 = vadd.s32 %v88513, %v9 (stack65)
        %v88556 = vadd.s32 %v88548, %v88552 (stack65)
        %v88558 = vshll.u32 %v88552, 13 (stack73)
        %v88559 = vshrl.u32 %v88552, 19 (stack74)
        %v88560 = vor.u32 %v88558, %v88559 (stack75)
        %v88561 = vxor.u32 %v88556, %v88560 (stack76)
        %v88564 = vadd.s32 %v88556, %v88561 (stack65)
        %v88566 = vshll.u32 %v88561, 15 (stack73)
        %v88567 = vshrl.u32 %v88561, 17 (stack74)
        %v88568 = vor.u32 %v88566, %v88567 (stack75)
        %v88569 = vxor.u32 %v88564, %v88568 (stack76)
        %v88572 = vadd.s32 %v88564, %v88569 (stack65)
        %v88574 = vshll.u32 %v88569, 26 (stack73)
        %v88575 = vshrl.u32 %v88569, 6 (stack74)
        %v88576 = vor.u32 %v88574, %v88575 (stack75)
        %v88577 = vxor.u32 %v88572, %v88576 (stack76)
        %v88580 = vadd.s32 %v88572, %v88577 (stack65)
        %v88584 = vadd.s32 %v88580, %v9 (stack65)
        %v88586 = vshll.u32 %v88577, 6 (stack73)
        %v88587 = vshrl.u32 %v88577, 26 (stack74)
        %v88588 = vor.u32 %v88586, %v88587 (stack75)
        %v88589 = vxor.u32 %v88580, %v88588 (stack76)
        %v88592 = vadd.s32 %v88589, %v8 (stack65)
        %v88596 = vadd.s32 %v88592, 1 (stack65)
        %v88600 = vadd.s32 %v88584, %v88596 (stack65)
        %v88602 = vshll.u32 %v88596, 17 (stack73)
        %v88603 = vshrl.u32 %v88596, 15 (stack74)
        %v88604 = vor.u32 %v88602, %v88603 (stack75)
        %v88605 = vxor.u32 %v88600, %v88604 (stack76)
        %v88608 = vadd.s32 %v88600, %v88605 (stack65)
        %v88610 = vshll.u32 %v88605, 29 (stack73)
        %v88611 = vshrl.u32 %v88605, 3 (stack74)
        %v88612 = vor.u32 %v88610, %v88611 (stack75)
        %v88613 = vxor.u32 %v88608, %v88612 (stack76)
        %v88616 = vadd.s32 %v88608, %v88613 (stack65)
        %v88618 = vshll.u32 %v88613, 16 (stack73)
        %v88619 = vshrl.u32 %v88613, 16 (stack74)
        %v88620 = vor.u32 %v88618, %v88619 (stack75)
        %v88621 = vxor.u32 %v88616, %v88620 (stack76)
        %v88624 = vadd.s32 %v88616, %v88621 (stack65)
        %v88628 = vadd.s32 %v88624, %v8 (stack65)
        %v88630 = vshll.u32 %v88621, 24 (stack73)
        %v88631 = vshrl.u32 %v88621, 8 (stack74)
        %v88632 = vor.u32 %v88630, %v88631 (stack75)
        %v88633 = vxor.u32 %v88624, %v88632 (stack76)
        %v88636 = vadd.s32 %v88633, %v10 (stack65)
        %v88640 = vadd.s32 %v88636, 2 (stack65)
        %v88644 = vadd.s32 %v88628, %v88640 (stack65)
        %v88646 = vshll.u32 %v88640, 13 (stack73)
        %v88647 = vshrl.u32 %v88640, 19 (stack74)
        %v88648 = vor.u32 %v88646, %v88647 (stack75)
        %v88649 = vxor.u32 %v88644, %v88648 (stack76)
        %v88652 = vadd.s32 %v88644, %v88649 (stack65)
        %v88654 = vshll.u32 %v88649, 15 (stack73)
        %v88655 = vshrl.u32 %v88649, 17 (stack74)
        %v88656 = vor.u32 %v88654, %v88655 (stack75)
        %v88657 = vxor.u32 %v88652, %v88656 (stack76)
        %v88660 = vadd.s32 %v88652, %v88657 (stack65)
        %v88662 = vshll.u32 %v88657, 26 (stack73)
        %v88663 = vshrl.u32 %v88657, 6 (stack74)
        %v88664 = vor.u32 %v88662, %v88663 (stack75)
        %v88665 = vxor.u32 %v88660, %v88664 (stack76)
        %v88668 = vadd.s32 %v88660, %v88665 (stack65)
        %v88672 = vadd.s32 %v88668, %v10 (stack65)
        %v88674 = vshll.u32 %v88665, 6 (stack73)
        %v88675 = vshrl.u32 %v88665, 26 (stack74)
        %v88676 = vor.u32 %v88674, %v88675 (stack75)
        %v88677 = vxor.u32 %v88668, %v88676 (stack76)
        %v88680 = vadd.s32 %v88677, %v9 (stack65)
        %v88684 = vadd.s32 %v88680, 3 (stack65)
        %v88688 = vadd.s32 %v88672, %v88684 (stack65)
        %v88690 = vshll.u32 %v88684, 17 (stack73)
        %v88691 = vshrl.u32 %v88684, 15 (stack74)
        %v88692 = vor.u32 %v88690, %v88691 (stack75)
        %v88693 = vxor.u32 %v88688, %v88692 (stack76)
        %v88696 = vadd.s32 %v88688, %v88693 (stack65)
        %v88698 = vshll.u32 %v88693, 29 (stack73)
        %v88699 = vshrl.u32 %v88693, 3 (stack74)
        %v88700 = vor.u32 %v88698, %v88699 (stack75)
        %v88701 = vxor.u32 %v88696, %v88700 (stack76)
        %v88704 = vadd.s32 %v88696, %v88701 (stack65)
        %v88706 = vshll.u32 %v88701, 16 (stack73)
        %v88707 = vshrl.u32 %v88701, 16 (stack74)
        %v88708 = vor.u32 %v88706, %v88707 (stack75)
        %v88709 = vxor.u32 %v88704, %v88708 (stack76)
        %v88712 = vadd.s32 %v88704, %v88709 (stack65)
        %v88716 = vadd.s32 %v88712, %v9 (stack65)
        %v88718 = vshll.u32 %v88709, 24 (stack73)
        %v88719 = vshrl.u32 %v88709, 8 (stack74)
        %v88720 = vor.u32 %v88718, %v88719 (stack75)
        %v88721 = vxor.u32 %v88712, %v88720 (stack76)
        %v88724 = vadd.s32 %v88721, %v8 (stack65)
        %v88728 = vadd.s32 %v88724, 4 (stack65)
        %v88732 = vadd.s32 %v88716, %v88728 (stack65)
        %v88734 = vshll.u32 %v88728, 13 (stack73)
        %v88735 = vshrl.u32 %v88728, 19 (stack74)
        %v88736 = vor.u32 %v88734, %v88735 (stack75)
        %v88737 = vxor.u32 %v88732, %v88736 (stack76)
        %v88740 = vadd.s32 %v88732, %v88737 (stack65)
        %v88742 = vshll.u32 %v88737, 15 (stack73)
        %v88743 = vshrl.u32 %v88737, 17 (stack74)
        %v88744 = vor.u32 %v88742, %v88743 (stack75)
        %v88745 = vxor.u32 %v88740, %v88744 (stack76)
        %v88748 = vadd.s32 %v88740, %v88745 (stack65)
        %v88750 = vshll.u32 %v88745, 26 (stack73)
        %v88751 = vshrl.u32 %v88745, 6 (stack74)
        %v88752 = vor.u32 %v88750, %v88751 (stack75)
        %v88753 = vxor.u32 %v88748, %v88752 (stack76)
        %v88756 = vadd.s32 %v88748, %v88753 (stack65)
        %v88760 = vadd.s32 %v88756, %v8 (stack65)
        %v88762 = vshll.u32 %v88753, 6 (stack73)
        %v88763 = vshrl.u32 %v88753, 26 (stack74)
        %v88764 = vor.u32 %v88762, %v88763 (stack75)
        %v88765 = vxor.u32 %v88756, %v88764 (stack76)
        %v88768 = vadd.s32 %v88765, %v10 (stack65)
        %v88772 = vadd.s32 %v88768, 5 (stack65)
        %v88774 = vxor.u32 %v88760, %v88772 (stack76)
        %v88775 = vand.u32.u8 %v88774, 255 (stack77)
        %v88776 = vand.u32 %v88775, 65535 (stack78)
        %v88777 = vshrl.u32 %v88776, 1 (stack79)
        %v88778 = vor.u32 %v88777, 16256 (stack75)
        %v88779 = vand.u32.u16 %v88778, 65535 (stack80)
        %v88780 = vunpack.i.l.bf16 %v88779 (stack81)
        %v88784 = vadd.f32 %v88780, -1.0 (stack82)
        %v88788 = vmul.f32 %v88784, 2.0 (stack83)
        %v88792 = vadd.f32 %v88788, -0.99609375 (stack82)
        %v88796 = vmax.f32 -0.99609375, %v88792 (stack84)
        %v88798 = vand.u32 2147483647, %v88796 (stack85)
        %vm88801 = vcmp.eq.f32.partialorder %v88798, 1.0 (stack86)
        %v88806 = vmul.f32 %v88796, inf (stack83)
        %v88808 = vxor.u32 %v88796, 2147483648 (stack87)
        %v88811 = vmul.f32 %v88796, %v88808 (stack83)
        %v88813 = vadd.f32 %v88811, 1.0 (stack88)
        %v88814 = vlog2.pop %v88813 (stack89)
        %v88815 = vmul.f32 %v88814, 0.6931472 (stack90)
        %v88816 = vmul.f32 -0.5, %v88811 (stack91)
        %v88817 = vadd.f32 %v88816, 1.0 (stack92)
        %v88818 = vmul.f32 %v88817, %v88811 (stack93)
        %v88819 = vand.u32 2147483647, %v88811 (stack94)
        %vm88820 = vcmp.lt.f32.partialorder %v88819, 0.0004427343 (stack95)
        %v88821 = vsel /*vm=*/%vm88820, /*on_true_vy=*/%v88818, /*on_false_vx=*/%v88815 (stack96)
        %v88822 = vxor.u32 %v88821, 2147483648 (stack87)
        %vm88825 = vcmp.lt.f32.partialorder %v88822, 5.0 (stack86)
        %v88830 = vsel /*vm=*/%vm88825, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v88834 = vsel /*vm=*/%vm88825, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v88838 = vsel /*vm=*/%vm88825, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v88842 = vsel /*vm=*/%vm88825, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v88846 = vsel /*vm=*/%vm88825, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v88850 = vsel /*vm=*/%vm88825, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v88854 = vsel /*vm=*/%vm88825, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v88858 = vsel /*vm=*/%vm88825, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v88862 = vsel /*vm=*/%vm88825, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v88866 = vadd.f32 %v88822, -2.5 (stack82)
        %v88868 = vrsqrt.pop %v88822 (stack97)
        %v88869 = vmul.f32 %v88822, %v88868 (stack98)
        %vm88870 = vcmp.eq.f32.partialorder %v88822, inf (stack99)
        %v88871 = vsel /*vm=*/%vm88870, /*on_true_vy=*/%v88822, /*on_false_vx=*/%v88869 (stack100)
        %vm88872 = vcmp.eq.f32.partialorder %v88822, 0.0 (stack101)
        %v88873 = vand.u32 %v88822, 2147483648 (stack102)
        %v88874 = vsel /*vm=*/%vm88872, /*on_true_vy=*/%v88873, /*on_false_vx=*/%v88871 (stack103)
        %v88877 = vadd.f32 %v88874, -3.0 (stack82)
        %v88881 = vsel /*vm=*/%vm88825, /*on_true_vy=*/%v88866, /*on_false_vx=*/%v88877 (stack72)
        %v88885 = vmul.f32 %v88862, %v88881 (stack83)
        %v88889 = vadd.f32 %v88858, %v88885 (stack82)
        %v88893 = vmul.f32 %v88889, %v88881 (stack83)
        %v88897 = vadd.f32 %v88854, %v88893 (stack82)
        %v88901 = vmul.f32 %v88897, %v88881 (stack83)
        %v88905 = vadd.f32 %v88850, %v88901 (stack82)
        %v88909 = vmul.f32 %v88905, %v88881 (stack83)
        %v88913 = vadd.f32 %v88846, %v88909 (stack82)
        %v88917 = vmul.f32 %v88913, %v88881 (stack83)
        %v88921 = vadd.f32 %v88842, %v88917 (stack82)
        %v88925 = vmul.f32 %v88921, %v88881 (stack83)
        %v88929 = vadd.f32 %v88838, %v88925 (stack82)
        %v88933 = vmul.f32 %v88929, %v88881 (stack83)
        %v88937 = vadd.f32 %v88834, %v88933 (stack82)
        %v88941 = vmul.f32 %v88937, %v88881 (stack83)
        %v88945 = vadd.f32 %v88830, %v88941 (stack82)
        %v88949 = vmul.f32 %v88945, %v88796 (stack83)
        %v88953 = vsel /*vm=*/%vm88801, /*on_true_vy=*/%v88806, /*on_false_vx=*/%v88949 (stack72)
        %v88957 = vmul.f32 %v88953, 1.4140625 (stack83)
        %s88959 = scalar_lea.vmem %s280, 732 [#allocation0] (stack107)
        %v88960 = vpack.c.bf16 0.0, %v88957 (stack104)
        %88961 = vst [vmem:[%s88959] sm:$0xf] /*vst_source=*/%v88960 (stack105)
        %v88964 = vadd.s32 %v3329, %v86195 (stack65)
        %s88966 = smul.u32 128, %s27 (stack66)
        %v88967 = vlaneseq (stack67)
        %v88968 = vand.u32 %v88967, 127 (stack68)
        %v88969 = vstv %s88966 (stack69)
        %v88970 = vadd.s32 %v88968, %v88969 (stack70)
        %v88974 = vadd.s32 %v88964, %v88970 (stack65)
        %vm88978 = vcmp.lt.u32.totalorder %v88974, %v88964 (stack71)
        %vm88983 = vcmp.lt.u32.totalorder %v88964, %v3329 (stack71)
        %v88988 = vadd.s32 %v3316, %v86178 (stack65)
        %v88992 = vadd.s32 %v88988, 1 (stack65)
        %v88996 = vsel /*vm=*/%vm88983, /*on_true_vy=*/%v88992, /*on_false_vx=*/%v88988 (stack72)
        %v89000 = vadd.s32 %v88996, 1 (stack65)
        %v89004 = vsel /*vm=*/%vm88978, /*on_true_vy=*/%v89000, /*on_false_vx=*/%v88996 (stack72)
        %v89009 = vadd.s32 %v89004, %v10 (stack65)
        %v89013 = vadd.s32 %v88974, %v9 (stack65)
        %v89017 = vadd.s32 %v89009, %v89013 (stack65)
        %v89019 = vshll.u32 %v89013, 13 (stack73)
        %v89020 = vshrl.u32 %v89013, 19 (stack74)
        %v89021 = vor.u32 %v89019, %v89020 (stack75)
        %v89022 = vxor.u32 %v89017, %v89021 (stack76)
        %v89025 = vadd.s32 %v89017, %v89022 (stack65)
        %v89027 = vshll.u32 %v89022, 15 (stack73)
        %v89028 = vshrl.u32 %v89022, 17 (stack74)
        %v89029 = vor.u32 %v89027, %v89028 (stack75)
        %v89030 = vxor.u32 %v89025, %v89029 (stack76)
        %v89033 = vadd.s32 %v89025, %v89030 (stack65)
        %v89035 = vshll.u32 %v89030, 26 (stack73)
        %v89036 = vshrl.u32 %v89030, 6 (stack74)
        %v89037 = vor.u32 %v89035, %v89036 (stack75)
        %v89038 = vxor.u32 %v89033, %v89037 (stack76)
        %v89041 = vadd.s32 %v89033, %v89038 (stack65)
        %v89045 = vadd.s32 %v89041, %v9 (stack65)
        %v89047 = vshll.u32 %v89038, 6 (stack73)
        %v89048 = vshrl.u32 %v89038, 26 (stack74)
        %v89049 = vor.u32 %v89047, %v89048 (stack75)
        %v89050 = vxor.u32 %v89041, %v89049 (stack76)
        %v89053 = vadd.s32 %v89050, %v8 (stack65)
        %v89057 = vadd.s32 %v89053, 1 (stack65)
        %v89061 = vadd.s32 %v89045, %v89057 (stack65)
        %v89063 = vshll.u32 %v89057, 17 (stack73)
        %v89064 = vshrl.u32 %v89057, 15 (stack74)
        %v89065 = vor.u32 %v89063, %v89064 (stack75)
        %v89066 = vxor.u32 %v89061, %v89065 (stack76)
        %v89069 = vadd.s32 %v89061, %v89066 (stack65)
        %v89071 = vshll.u32 %v89066, 29 (stack73)
        %v89072 = vshrl.u32 %v89066, 3 (stack74)
        %v89073 = vor.u32 %v89071, %v89072 (stack75)
        %v89074 = vxor.u32 %v89069, %v89073 (stack76)
        %v89077 = vadd.s32 %v89069, %v89074 (stack65)
        %v89079 = vshll.u32 %v89074, 16 (stack73)
        %v89080 = vshrl.u32 %v89074, 16 (stack74)
        %v89081 = vor.u32 %v89079, %v89080 (stack75)
        %v89082 = vxor.u32 %v89077, %v89081 (stack76)
        %v89085 = vadd.s32 %v89077, %v89082 (stack65)
        %v89089 = vadd.s32 %v89085, %v8 (stack65)
        %v89091 = vshll.u32 %v89082, 24 (stack73)
        %v89092 = vshrl.u32 %v89082, 8 (stack74)
        %v89093 = vor.u32 %v89091, %v89092 (stack75)
        %v89094 = vxor.u32 %v89085, %v89093 (stack76)
        %v89097 = vadd.s32 %v89094, %v10 (stack65)
        %v89101 = vadd.s32 %v89097, 2 (stack65)
        %v89105 = vadd.s32 %v89089, %v89101 (stack65)
        %v89107 = vshll.u32 %v89101, 13 (stack73)
        %v89108 = vshrl.u32 %v89101, 19 (stack74)
        %v89109 = vor.u32 %v89107, %v89108 (stack75)
        %v89110 = vxor.u32 %v89105, %v89109 (stack76)
        %v89113 = vadd.s32 %v89105, %v89110 (stack65)
        %v89115 = vshll.u32 %v89110, 15 (stack73)
        %v89116 = vshrl.u32 %v89110, 17 (stack74)
        %v89117 = vor.u32 %v89115, %v89116 (stack75)
        %v89118 = vxor.u32 %v89113, %v89117 (stack76)
        %v89121 = vadd.s32 %v89113, %v89118 (stack65)
        %v89123 = vshll.u32 %v89118, 26 (stack73)
        %v89124 = vshrl.u32 %v89118, 6 (stack74)
        %v89125 = vor.u32 %v89123, %v89124 (stack75)
        %v89126 = vxor.u32 %v89121, %v89125 (stack76)
        %v89129 = vadd.s32 %v89121, %v89126 (stack65)
        %v89133 = vadd.s32 %v89129, %v10 (stack65)
        %v89135 = vshll.u32 %v89126, 6 (stack73)
        %v89136 = vshrl.u32 %v89126, 26 (stack74)
        %v89137 = vor.u32 %v89135, %v89136 (stack75)
        %v89138 = vxor.u32 %v89129, %v89137 (stack76)
        %v89141 = vadd.s32 %v89138, %v9 (stack65)
        %v89145 = vadd.s32 %v89141, 3 (stack65)
        %v89149 = vadd.s32 %v89133, %v89145 (stack65)
        %v89151 = vshll.u32 %v89145, 17 (stack73)
        %v89152 = vshrl.u32 %v89145, 15 (stack74)
        %v89153 = vor.u32 %v89151, %v89152 (stack75)
        %v89154 = vxor.u32 %v89149, %v89153 (stack76)
        %v89157 = vadd.s32 %v89149, %v89154 (stack65)
        %v89159 = vshll.u32 %v89154, 29 (stack73)
        %v89160 = vshrl.u32 %v89154, 3 (stack74)
        %v89161 = vor.u32 %v89159, %v89160 (stack75)
        %v89162 = vxor.u32 %v89157, %v89161 (stack76)
        %v89165 = vadd.s32 %v89157, %v89162 (stack65)
        %v89167 = vshll.u32 %v89162, 16 (stack73)
        %v89168 = vshrl.u32 %v89162, 16 (stack74)
        %v89169 = vor.u32 %v89167, %v89168 (stack75)
        %v89170 = vxor.u32 %v89165, %v89169 (stack76)
        %v89173 = vadd.s32 %v89165, %v89170 (stack65)
        %v89177 = vadd.s32 %v89173, %v9 (stack65)
        %v89179 = vshll.u32 %v89170, 24 (stack73)
        %v89180 = vshrl.u32 %v89170, 8 (stack74)
        %v89181 = vor.u32 %v89179, %v89180 (stack75)
        %v89182 = vxor.u32 %v89173, %v89181 (stack76)
        %v89185 = vadd.s32 %v89182, %v8 (stack65)
        %v89189 = vadd.s32 %v89185, 4 (stack65)
        %v89193 = vadd.s32 %v89177, %v89189 (stack65)
        %v89195 = vshll.u32 %v89189, 13 (stack73)
        %v89196 = vshrl.u32 %v89189, 19 (stack74)
        %v89197 = vor.u32 %v89195, %v89196 (stack75)
        %v89198 = vxor.u32 %v89193, %v89197 (stack76)
        %v89201 = vadd.s32 %v89193, %v89198 (stack65)
        %v89203 = vshll.u32 %v89198, 15 (stack73)
        %v89204 = vshrl.u32 %v89198, 17 (stack74)
        %v89205 = vor.u32 %v89203, %v89204 (stack75)
        %v89206 = vxor.u32 %v89201, %v89205 (stack76)
        %v89209 = vadd.s32 %v89201, %v89206 (stack65)
        %v89211 = vshll.u32 %v89206, 26 (stack73)
        %v89212 = vshrl.u32 %v89206, 6 (stack74)
        %v89213 = vor.u32 %v89211, %v89212 (stack75)
        %v89214 = vxor.u32 %v89209, %v89213 (stack76)
        %v89217 = vadd.s32 %v89209, %v89214 (stack65)
        %v89221 = vadd.s32 %v89217, %v8 (stack65)
        %v89223 = vshll.u32 %v89214, 6 (stack73)
        %v89224 = vshrl.u32 %v89214, 26 (stack74)
        %v89225 = vor.u32 %v89223, %v89224 (stack75)
        %v89226 = vxor.u32 %v89217, %v89225 (stack76)
        %v89229 = vadd.s32 %v89226, %v10 (stack65)
        %v89233 = vadd.s32 %v89229, 5 (stack65)
        %v89235 = vxor.u32 %v89221, %v89233 (stack76)
        %v89236 = vand.u32.u8 %v89235, 255 (stack77)
        %v89237 = vand.u32 %v89236, 65535 (stack78)
        %v89238 = vshrl.u32 %v89237, 1 (stack79)
        %v89239 = vor.u32 %v89238, 16256 (stack75)
        %v89240 = vand.u32.u16 %v89239, 65535 (stack80)
        %v89241 = vunpack.i.l.bf16 %v89240 (stack81)
        %v89245 = vadd.f32 %v89241, -1.0 (stack82)
        %v89249 = vmul.f32 %v89245, 2.0 (stack83)
        %v89253 = vadd.f32 %v89249, -0.99609375 (stack82)
        %v89257 = vmax.f32 -0.99609375, %v89253 (stack84)
        %v89259 = vand.u32 2147483647, %v89257 (stack85)
        %vm89262 = vcmp.eq.f32.partialorder %v89259, 1.0 (stack86)
        %v89267 = vmul.f32 %v89257, inf (stack83)
        %v89269 = vxor.u32 %v89257, 2147483648 (stack87)
        %v89272 = vmul.f32 %v89257, %v89269 (stack83)
        %v89274 = vadd.f32 %v89272, 1.0 (stack88)
        %v89275 = vlog2.pop %v89274 (stack89)
        %v89276 = vmul.f32 %v89275, 0.6931472 (stack90)
        %v89277 = vmul.f32 -0.5, %v89272 (stack91)
        %v89278 = vadd.f32 %v89277, 1.0 (stack92)
        %v89279 = vmul.f32 %v89278, %v89272 (stack93)
        %v89280 = vand.u32 2147483647, %v89272 (stack94)
        %vm89281 = vcmp.lt.f32.partialorder %v89280, 0.0004427343 (stack95)
        %v89282 = vsel /*vm=*/%vm89281, /*on_true_vy=*/%v89279, /*on_false_vx=*/%v89276 (stack96)
        %v89283 = vxor.u32 %v89282, 2147483648 (stack87)
        %vm89286 = vcmp.lt.f32.partialorder %v89283, 5.0 (stack86)
        %v89291 = vsel /*vm=*/%vm89286, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v89295 = vsel /*vm=*/%vm89286, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v89299 = vsel /*vm=*/%vm89286, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v89303 = vsel /*vm=*/%vm89286, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v89307 = vsel /*vm=*/%vm89286, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v89311 = vsel /*vm=*/%vm89286, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v89315 = vsel /*vm=*/%vm89286, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v89319 = vsel /*vm=*/%vm89286, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v89323 = vsel /*vm=*/%vm89286, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v89327 = vadd.f32 %v89283, -2.5 (stack82)
        %v89329 = vrsqrt.pop %v89283 (stack97)
        %v89330 = vmul.f32 %v89283, %v89329 (stack98)
        %vm89331 = vcmp.eq.f32.partialorder %v89283, inf (stack99)
        %v89332 = vsel /*vm=*/%vm89331, /*on_true_vy=*/%v89283, /*on_false_vx=*/%v89330 (stack100)
        %vm89333 = vcmp.eq.f32.partialorder %v89283, 0.0 (stack101)
        %v89334 = vand.u32 %v89283, 2147483648 (stack102)
        %v89335 = vsel /*vm=*/%vm89333, /*on_true_vy=*/%v89334, /*on_false_vx=*/%v89332 (stack103)
        %v89338 = vadd.f32 %v89335, -3.0 (stack82)
        %v89342 = vsel /*vm=*/%vm89286, /*on_true_vy=*/%v89327, /*on_false_vx=*/%v89338 (stack72)
        %v89346 = vmul.f32 %v89323, %v89342 (stack83)
        %v89350 = vadd.f32 %v89319, %v89346 (stack82)
        %v89354 = vmul.f32 %v89350, %v89342 (stack83)
        %v89358 = vadd.f32 %v89315, %v89354 (stack82)
        %v89362 = vmul.f32 %v89358, %v89342 (stack83)
        %v89366 = vadd.f32 %v89311, %v89362 (stack82)
        %v89370 = vmul.f32 %v89366, %v89342 (stack83)
        %v89374 = vadd.f32 %v89307, %v89370 (stack82)
        %v89378 = vmul.f32 %v89374, %v89342 (stack83)
        %v89382 = vadd.f32 %v89303, %v89378 (stack82)
        %v89386 = vmul.f32 %v89382, %v89342 (stack83)
        %v89390 = vadd.f32 %v89299, %v89386 (stack82)
        %v89394 = vmul.f32 %v89390, %v89342 (stack83)
        %v89398 = vadd.f32 %v89295, %v89394 (stack82)
        %v89402 = vmul.f32 %v89398, %v89342 (stack83)
        %v89406 = vadd.f32 %v89291, %v89402 (stack82)
        %v89410 = vmul.f32 %v89406, %v89257 (stack83)
        %v89414 = vsel /*vm=*/%vm89262, /*on_true_vy=*/%v89267, /*on_false_vx=*/%v89410 (stack72)
        %v89418 = vmul.f32 %v89414, 1.4140625 (stack83)
        %s89420 = scalar_lea.vmem %s280, 860 [#allocation0] (stack107)
        %v89421 = vpack.c.bf16 0.0, %v89418 (stack104)
        %89422 = vst [vmem:[%s89420] sm:$0xf] /*vst_source=*/%v89421 (stack105)
        %v89425 = vadd.s32 %v3816, %v86195 (stack65)
        %s89427 = smul.u32 128, %s27 (stack66)
        %v89428 = vlaneseq (stack67)
        %v89429 = vand.u32 %v89428, 127 (stack68)
        %v89430 = vstv %s89427 (stack69)
        %v89431 = vadd.s32 %v89429, %v89430 (stack70)
        %v89435 = vadd.s32 %v89425, %v89431 (stack65)
        %vm89439 = vcmp.lt.u32.totalorder %v89435, %v89425 (stack71)
        %vm89444 = vcmp.lt.u32.totalorder %v89425, %v3816 (stack71)
        %v89449 = vadd.s32 %v3803, %v86178 (stack65)
        %v89453 = vadd.s32 %v89449, 1 (stack65)
        %v89457 = vsel /*vm=*/%vm89444, /*on_true_vy=*/%v89453, /*on_false_vx=*/%v89449 (stack72)
        %v89461 = vadd.s32 %v89457, 1 (stack65)
        %v89465 = vsel /*vm=*/%vm89439, /*on_true_vy=*/%v89461, /*on_false_vx=*/%v89457 (stack72)
        %v89470 = vadd.s32 %v89465, %v10 (stack65)
        %v89474 = vadd.s32 %v89435, %v9 (stack65)
        %v89478 = vadd.s32 %v89470, %v89474 (stack65)
        %v89480 = vshll.u32 %v89474, 13 (stack73)
        %v89481 = vshrl.u32 %v89474, 19 (stack74)
        %v89482 = vor.u32 %v89480, %v89481 (stack75)
        %v89483 = vxor.u32 %v89478, %v89482 (stack76)
        %v89486 = vadd.s32 %v89478, %v89483 (stack65)
        %v89488 = vshll.u32 %v89483, 15 (stack73)
        %v89489 = vshrl.u32 %v89483, 17 (stack74)
        %v89490 = vor.u32 %v89488, %v89489 (stack75)
        %v89491 = vxor.u32 %v89486, %v89490 (stack76)
        %v89494 = vadd.s32 %v89486, %v89491 (stack65)
        %v89496 = vshll.u32 %v89491, 26 (stack73)
        %v89497 = vshrl.u32 %v89491, 6 (stack74)
        %v89498 = vor.u32 %v89496, %v89497 (stack75)
        %v89499 = vxor.u32 %v89494, %v89498 (stack76)
        %v89502 = vadd.s32 %v89494, %v89499 (stack65)
        %v89506 = vadd.s32 %v89502, %v9 (stack65)
        %v89508 = vshll.u32 %v89499, 6 (stack73)
        %v89509 = vshrl.u32 %v89499, 26 (stack74)
        %v89510 = vor.u32 %v89508, %v89509 (stack75)
        %v89511 = vxor.u32 %v89502, %v89510 (stack76)
        %v89514 = vadd.s32 %v89511, %v8 (stack65)
        %v89518 = vadd.s32 %v89514, 1 (stack65)
        %v89522 = vadd.s32 %v89506, %v89518 (stack65)
        %v89524 = vshll.u32 %v89518, 17 (stack73)
        %v89525 = vshrl.u32 %v89518, 15 (stack74)
        %v89526 = vor.u32 %v89524, %v89525 (stack75)
        %v89527 = vxor.u32 %v89522, %v89526 (stack76)
        %v89530 = vadd.s32 %v89522, %v89527 (stack65)
        %v89532 = vshll.u32 %v89527, 29 (stack73)
        %v89533 = vshrl.u32 %v89527, 3 (stack74)
        %v89534 = vor.u32 %v89532, %v89533 (stack75)
        %v89535 = vxor.u32 %v89530, %v89534 (stack76)
        %v89538 = vadd.s32 %v89530, %v89535 (stack65)
        %v89540 = vshll.u32 %v89535, 16 (stack73)
        %v89541 = vshrl.u32 %v89535, 16 (stack74)
        %v89542 = vor.u32 %v89540, %v89541 (stack75)
        %v89543 = vxor.u32 %v89538, %v89542 (stack76)
        %v89546 = vadd.s32 %v89538, %v89543 (stack65)
        %v89550 = vadd.s32 %v89546, %v8 (stack65)
        %v89552 = vshll.u32 %v89543, 24 (stack73)
        %v89553 = vshrl.u32 %v89543, 8 (stack74)
        %v89554 = vor.u32 %v89552, %v89553 (stack75)
        %v89555 = vxor.u32 %v89546, %v89554 (stack76)
        %v89558 = vadd.s32 %v89555, %v10 (stack65)
        %v89562 = vadd.s32 %v89558, 2 (stack65)
        %v89566 = vadd.s32 %v89550, %v89562 (stack65)
        %v89568 = vshll.u32 %v89562, 13 (stack73)
        %v89569 = vshrl.u32 %v89562, 19 (stack74)
        %v89570 = vor.u32 %v89568, %v89569 (stack75)
        %v89571 = vxor.u32 %v89566, %v89570 (stack76)
        %v89574 = vadd.s32 %v89566, %v89571 (stack65)
        %v89576 = vshll.u32 %v89571, 15 (stack73)
        %v89577 = vshrl.u32 %v89571, 17 (stack74)
        %v89578 = vor.u32 %v89576, %v89577 (stack75)
        %v89579 = vxor.u32 %v89574, %v89578 (stack76)
        %v89582 = vadd.s32 %v89574, %v89579 (stack65)
        %v89584 = vshll.u32 %v89579, 26 (stack73)
        %v89585 = vshrl.u32 %v89579, 6 (stack74)
        %v89586 = vor.u32 %v89584, %v89585 (stack75)
        %v89587 = vxor.u32 %v89582, %v89586 (stack76)
        %v89590 = vadd.s32 %v89582, %v89587 (stack65)
        %v89594 = vadd.s32 %v89590, %v10 (stack65)
        %v89596 = vshll.u32 %v89587, 6 (stack73)
        %v89597 = vshrl.u32 %v89587, 26 (stack74)
        %v89598 = vor.u32 %v89596, %v89597 (stack75)
        %v89599 = vxor.u32 %v89590, %v89598 (stack76)
        %v89602 = vadd.s32 %v89599, %v9 (stack65)
        %v89606 = vadd.s32 %v89602, 3 (stack65)
        %v89610 = vadd.s32 %v89594, %v89606 (stack65)
        %v89612 = vshll.u32 %v89606, 17 (stack73)
        %v89613 = vshrl.u32 %v89606, 15 (stack74)
        %v89614 = vor.u32 %v89612, %v89613 (stack75)
        %v89615 = vxor.u32 %v89610, %v89614 (stack76)
        %v89618 = vadd.s32 %v89610, %v89615 (stack65)
        %v89620 = vshll.u32 %v89615, 29 (stack73)
        %v89621 = vshrl.u32 %v89615, 3 (stack74)
        %v89622 = vor.u32 %v89620, %v89621 (stack75)
        %v89623 = vxor.u32 %v89618, %v89622 (stack76)
        %v89626 = vadd.s32 %v89618, %v89623 (stack65)
        %v89628 = vshll.u32 %v89623, 16 (stack73)
        %v89629 = vshrl.u32 %v89623, 16 (stack74)
        %v89630 = vor.u32 %v89628, %v89629 (stack75)
        %v89631 = vxor.u32 %v89626, %v89630 (stack76)
        %v89634 = vadd.s32 %v89626, %v89631 (stack65)
        %v89638 = vadd.s32 %v89634, %v9 (stack65)
        %v89640 = vshll.u32 %v89631, 24 (stack73)
        %v89641 = vshrl.u32 %v89631, 8 (stack74)
        %v89642 = vor.u32 %v89640, %v89641 (stack75)
        %v89643 = vxor.u32 %v89634, %v89642 (stack76)
        %v89646 = vadd.s32 %v89643, %v8 (stack65)
        %v89650 = vadd.s32 %v89646, 4 (stack65)
        %v89654 = vadd.s32 %v89638, %v89650 (stack65)
        %v89656 = vshll.u32 %v89650, 13 (stack73)
        %v89657 = vshrl.u32 %v89650, 19 (stack74)
        %v89658 = vor.u32 %v89656, %v89657 (stack75)
        %v89659 = vxor.u32 %v89654, %v89658 (stack76)
        %v89662 = vadd.s32 %v89654, %v89659 (stack65)
        %v89664 = vshll.u32 %v89659, 15 (stack73)
        %v89665 = vshrl.u32 %v89659, 17 (stack74)
        %v89666 = vor.u32 %v89664, %v89665 (stack75)
        %v89667 = vxor.u32 %v89662, %v89666 (stack76)
        %v89670 = vadd.s32 %v89662, %v89667 (stack65)
        %v89672 = vshll.u32 %v89667, 26 (stack73)
        %v89673 = vshrl.u32 %v89667, 6 (stack74)
        %v89674 = vor.u32 %v89672, %v89673 (stack75)
        %v89675 = vxor.u32 %v89670, %v89674 (stack76)
        %v89678 = vadd.s32 %v89670, %v89675 (stack65)
        %v89682 = vadd.s32 %v89678, %v8 (stack65)
        %v89684 = vshll.u32 %v89675, 6 (stack73)
        %v89685 = vshrl.u32 %v89675, 26 (stack74)
        %v89686 = vor.u32 %v89684, %v89685 (stack75)
        %v89687 = vxor.u32 %v89678, %v89686 (stack76)
        %v89690 = vadd.s32 %v89687, %v10 (stack65)
        %v89694 = vadd.s32 %v89690, 5 (stack65)
        %v89696 = vxor.u32 %v89682, %v89694 (stack76)
        %v89697 = vand.u32.u8 %v89696, 255 (stack77)
        %v89698 = vand.u32 %v89697, 65535 (stack78)
        %v89699 = vshrl.u32 %v89698, 1 (stack79)
        %v89700 = vor.u32 %v89699, 16256 (stack75)
        %v89701 = vand.u32.u16 %v89700, 65535 (stack80)
        %v89702 = vunpack.i.l.bf16 %v89701 (stack81)
        %v89706 = vadd.f32 %v89702, -1.0 (stack82)
        %v89710 = vmul.f32 %v89706, 2.0 (stack83)
        %v89714 = vadd.f32 %v89710, -0.99609375 (stack82)
        %v89718 = vmax.f32 -0.99609375, %v89714 (stack84)
        %v89720 = vand.u32 2147483647, %v89718 (stack85)
        %vm89723 = vcmp.eq.f32.partialorder %v89720, 1.0 (stack86)
        %v89728 = vmul.f32 %v89718, inf (stack83)
        %v89730 = vxor.u32 %v89718, 2147483648 (stack87)
        %v89733 = vmul.f32 %v89718, %v89730 (stack83)
        %v89735 = vadd.f32 %v89733, 1.0 (stack88)
        %v89736 = vlog2.pop %v89735 (stack89)
        %v89737 = vmul.f32 %v89736, 0.6931472 (stack90)
        %v89738 = vmul.f32 -0.5, %v89733 (stack91)
        %v89739 = vadd.f32 %v89738, 1.0 (stack92)
        %v89740 = vmul.f32 %v89739, %v89733 (stack93)
        %v89741 = vand.u32 2147483647, %v89733 (stack94)
        %vm89742 = vcmp.lt.f32.partialorder %v89741, 0.0004427343 (stack95)
        %v89743 = vsel /*vm=*/%vm89742, /*on_true_vy=*/%v89740, /*on_false_vx=*/%v89737 (stack96)
        %v89744 = vxor.u32 %v89743, 2147483648 (stack87)
        %vm89747 = vcmp.lt.f32.partialorder %v89744, 5.0 (stack86)
        %v89752 = vsel /*vm=*/%vm89747, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v89756 = vsel /*vm=*/%vm89747, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v89760 = vsel /*vm=*/%vm89747, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v89764 = vsel /*vm=*/%vm89747, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v89768 = vsel /*vm=*/%vm89747, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v89772 = vsel /*vm=*/%vm89747, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v89776 = vsel /*vm=*/%vm89747, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v89780 = vsel /*vm=*/%vm89747, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v89784 = vsel /*vm=*/%vm89747, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v89788 = vadd.f32 %v89744, -2.5 (stack82)
        %v89790 = vrsqrt.pop %v89744 (stack97)
        %v89791 = vmul.f32 %v89744, %v89790 (stack98)
        %vm89792 = vcmp.eq.f32.partialorder %v89744, inf (stack99)
        %v89793 = vsel /*vm=*/%vm89792, /*on_true_vy=*/%v89744, /*on_false_vx=*/%v89791 (stack100)
        %vm89794 = vcmp.eq.f32.partialorder %v89744, 0.0 (stack101)
        %v89795 = vand.u32 %v89744, 2147483648 (stack102)
        %v89796 = vsel /*vm=*/%vm89794, /*on_true_vy=*/%v89795, /*on_false_vx=*/%v89793 (stack103)
        %v89799 = vadd.f32 %v89796, -3.0 (stack82)
        %v89803 = vsel /*vm=*/%vm89747, /*on_true_vy=*/%v89788, /*on_false_vx=*/%v89799 (stack72)
        %v89807 = vmul.f32 %v89784, %v89803 (stack83)
        %v89811 = vadd.f32 %v89780, %v89807 (stack82)
        %v89815 = vmul.f32 %v89811, %v89803 (stack83)
        %v89819 = vadd.f32 %v89776, %v89815 (stack82)
        %v89823 = vmul.f32 %v89819, %v89803 (stack83)
        %v89827 = vadd.f32 %v89772, %v89823 (stack82)
        %v89831 = vmul.f32 %v89827, %v89803 (stack83)
        %v89835 = vadd.f32 %v89768, %v89831 (stack82)
        %v89839 = vmul.f32 %v89835, %v89803 (stack83)
        %v89843 = vadd.f32 %v89764, %v89839 (stack82)
        %v89847 = vmul.f32 %v89843, %v89803 (stack83)
        %v89851 = vadd.f32 %v89760, %v89847 (stack82)
        %v89855 = vmul.f32 %v89851, %v89803 (stack83)
        %v89859 = vadd.f32 %v89756, %v89855 (stack82)
        %v89863 = vmul.f32 %v89859, %v89803 (stack83)
        %v89867 = vadd.f32 %v89752, %v89863 (stack82)
        %v89871 = vmul.f32 %v89867, %v89718 (stack83)
        %v89875 = vsel /*vm=*/%vm89723, /*on_true_vy=*/%v89728, /*on_false_vx=*/%v89871 (stack72)
        %v89879 = vmul.f32 %v89875, 1.4140625 (stack83)
        %s89881 = scalar_lea.vmem %s280, 988 [#allocation0] (stack107)
        %v89882 = vpack.c.bf16 0.0, %v89879 (stack104)
        %89883 = vst [vmem:[%s89881] sm:$0xf] /*vst_source=*/%v89882 (stack105)
        %s89884 = sadd.s32 %s339, 192 (stack106)
        %s89885 = sshrl.u32 %s89884, 10 (stack49)
        %p89886 = scmp.lt.s32.totalorder 1, %s89885 (stack50)
        %s89887 = scalar_select /*predicate=*/%p89886, /*on_true=*/1, /*on_false=*/%s89885 (stack51)
        %s89888 = sand.u32 %s89884, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s89889 = sshrl.u32 %s89888, 7 (stack53)
        %s89890 = sand.u32 %s89888, 127 /* smod.u32 w/div 128 */ (stack54)
        %s89891 = smul.addr %s89887, 8 (stack55)
        %s89892 = scalar_lea.vmem %s3, %s89891 (stack56)
        %s89894 = scalar_lea.vmem %s89892, %s89889 (stack57)
        %v89895 = vld [vmem:[%s89894] ss:$0 sm:$0xff] (stack58)
        %s89896 = sand.u32 %s89890, 255 (stack59)
        %s89898 = sor.u32 256, %s89896 (stack60)
        %89899 = vbcast.lane.b32.xlu0 %v89895, %s89898 (stack61)
        %v89900 = vpop.permute.xlu0 %89899 (stack62)
        %s89901 = sadd.s32 %s347, 192 (stack106)
        %s89902 = sshrl.u32 %s89901, 10 (stack49)
        %p89903 = scmp.lt.s32.totalorder 1, %s89902 (stack50)
        %s89904 = scalar_select /*predicate=*/%p89903, /*on_true=*/1, /*on_false=*/%s89902 (stack51)
        %s89905 = sand.u32 %s89901, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s89906 = sshrl.u32 %s89905, 7 (stack53)
        %s89907 = sand.u32 %s89905, 127 /* smod.u32 w/div 128 */ (stack54)
        %s89908 = smul.addr %s89904, 8 (stack55)
        %s89909 = scalar_lea.vmem %s5, %s89908 (stack56)
        %s89911 = scalar_lea.vmem %s89909, %s89906 (stack57)
        %v89912 = vld [vmem:[%s89911] ss:$0 sm:$0xff] (stack58)
        %s89913 = sand.u32 %s89907, 255 (stack59)
        %s89915 = sor.u32 256, %s89913 (stack60)
        %89916 = vbcast.lane.b32.xlu0 %v89912, %s89915 (stack61)
        %v89917 = vpop.permute.xlu0 %89916 (stack62)
        %v89920 = vadd.s32 %v408, %v89917 (stack65)
        %s89922 = smul.u32 128, %s27 (stack66)
        %v89923 = vlaneseq (stack67)
        %v89924 = vand.u32 %v89923, 127 (stack68)
        %v89925 = vstv %s89922 (stack69)
        %v89926 = vadd.s32 %v89924, %v89925 (stack70)
        %v89930 = vadd.s32 %v89920, %v89926 (stack65)
        %vm89934 = vcmp.lt.u32.totalorder %v89930, %v89920 (stack71)
        %vm89939 = vcmp.lt.u32.totalorder %v89920, %v408 (stack71)
        %v89944 = vadd.s32 %v380, %v89900 (stack65)
        %v89948 = vadd.s32 %v89944, 1 (stack65)
        %v89952 = vsel /*vm=*/%vm89939, /*on_true_vy=*/%v89948, /*on_false_vx=*/%v89944 (stack72)
        %v89956 = vadd.s32 %v89952, 1 (stack65)
        %v89960 = vsel /*vm=*/%vm89934, /*on_true_vy=*/%v89956, /*on_false_vx=*/%v89952 (stack72)
        %v89965 = vadd.s32 %v89960, %v10 (stack65)
        %v89969 = vadd.s32 %v89930, %v9 (stack65)
        %v89973 = vadd.s32 %v89965, %v89969 (stack65)
        %v89975 = vshll.u32 %v89969, 13 (stack73)
        %v89976 = vshrl.u32 %v89969, 19 (stack74)
        %v89977 = vor.u32 %v89975, %v89976 (stack75)
        %v89978 = vxor.u32 %v89973, %v89977 (stack76)
        %v89981 = vadd.s32 %v89973, %v89978 (stack65)
        %v89983 = vshll.u32 %v89978, 15 (stack73)
        %v89984 = vshrl.u32 %v89978, 17 (stack74)
        %v89985 = vor.u32 %v89983, %v89984 (stack75)
        %v89986 = vxor.u32 %v89981, %v89985 (stack76)
        %v89989 = vadd.s32 %v89981, %v89986 (stack65)
        %v89991 = vshll.u32 %v89986, 26 (stack73)
        %v89992 = vshrl.u32 %v89986, 6 (stack74)
        %v89993 = vor.u32 %v89991, %v89992 (stack75)
        %v89994 = vxor.u32 %v89989, %v89993 (stack76)
        %v89997 = vadd.s32 %v89989, %v89994 (stack65)
        %v90001 = vadd.s32 %v89997, %v9 (stack65)
        %v90003 = vshll.u32 %v89994, 6 (stack73)
        %v90004 = vshrl.u32 %v89994, 26 (stack74)
        %v90005 = vor.u32 %v90003, %v90004 (stack75)
        %v90006 = vxor.u32 %v89997, %v90005 (stack76)
        %v90009 = vadd.s32 %v90006, %v8 (stack65)
        %v90013 = vadd.s32 %v90009, 1 (stack65)
        %v90017 = vadd.s32 %v90001, %v90013 (stack65)
        %v90019 = vshll.u32 %v90013, 17 (stack73)
        %v90020 = vshrl.u32 %v90013, 15 (stack74)
        %v90021 = vor.u32 %v90019, %v90020 (stack75)
        %v90022 = vxor.u32 %v90017, %v90021 (stack76)
        %v90025 = vadd.s32 %v90017, %v90022 (stack65)
        %v90027 = vshll.u32 %v90022, 29 (stack73)
        %v90028 = vshrl.u32 %v90022, 3 (stack74)
        %v90029 = vor.u32 %v90027, %v90028 (stack75)
        %v90030 = vxor.u32 %v90025, %v90029 (stack76)
        %v90033 = vadd.s32 %v90025, %v90030 (stack65)
        %v90035 = vshll.u32 %v90030, 16 (stack73)
        %v90036 = vshrl.u32 %v90030, 16 (stack74)
        %v90037 = vor.u32 %v90035, %v90036 (stack75)
        %v90038 = vxor.u32 %v90033, %v90037 (stack76)
        %v90041 = vadd.s32 %v90033, %v90038 (stack65)
        %v90045 = vadd.s32 %v90041, %v8 (stack65)
        %v90047 = vshll.u32 %v90038, 24 (stack73)
        %v90048 = vshrl.u32 %v90038, 8 (stack74)
        %v90049 = vor.u32 %v90047, %v90048 (stack75)
        %v90050 = vxor.u32 %v90041, %v90049 (stack76)
        %v90053 = vadd.s32 %v90050, %v10 (stack65)
        %v90057 = vadd.s32 %v90053, 2 (stack65)
        %v90061 = vadd.s32 %v90045, %v90057 (stack65)
        %v90063 = vshll.u32 %v90057, 13 (stack73)
        %v90064 = vshrl.u32 %v90057, 19 (stack74)
        %v90065 = vor.u32 %v90063, %v90064 (stack75)
        %v90066 = vxor.u32 %v90061, %v90065 (stack76)
        %v90069 = vadd.s32 %v90061, %v90066 (stack65)
        %v90071 = vshll.u32 %v90066, 15 (stack73)
        %v90072 = vshrl.u32 %v90066, 17 (stack74)
        %v90073 = vor.u32 %v90071, %v90072 (stack75)
        %v90074 = vxor.u32 %v90069, %v90073 (stack76)
        %v90077 = vadd.s32 %v90069, %v90074 (stack65)
        %v90079 = vshll.u32 %v90074, 26 (stack73)
        %v90080 = vshrl.u32 %v90074, 6 (stack74)
        %v90081 = vor.u32 %v90079, %v90080 (stack75)
        %v90082 = vxor.u32 %v90077, %v90081 (stack76)
        %v90085 = vadd.s32 %v90077, %v90082 (stack65)
        %v90089 = vadd.s32 %v90085, %v10 (stack65)
        %v90091 = vshll.u32 %v90082, 6 (stack73)
        %v90092 = vshrl.u32 %v90082, 26 (stack74)
        %v90093 = vor.u32 %v90091, %v90092 (stack75)
        %v90094 = vxor.u32 %v90085, %v90093 (stack76)
        %v90097 = vadd.s32 %v90094, %v9 (stack65)
        %v90101 = vadd.s32 %v90097, 3 (stack65)
        %v90105 = vadd.s32 %v90089, %v90101 (stack65)
        %v90107 = vshll.u32 %v90101, 17 (stack73)
        %v90108 = vshrl.u32 %v90101, 15 (stack74)
        %v90109 = vor.u32 %v90107, %v90108 (stack75)
        %v90110 = vxor.u32 %v90105, %v90109 (stack76)
        %v90113 = vadd.s32 %v90105, %v90110 (stack65)
        %v90115 = vshll.u32 %v90110, 29 (stack73)
        %v90116 = vshrl.u32 %v90110, 3 (stack74)
        %v90117 = vor.u32 %v90115, %v90116 (stack75)
        %v90118 = vxor.u32 %v90113, %v90117 (stack76)
        %v90121 = vadd.s32 %v90113, %v90118 (stack65)
        %v90123 = vshll.u32 %v90118, 16 (stack73)
        %v90124 = vshrl.u32 %v90118, 16 (stack74)
        %v90125 = vor.u32 %v90123, %v90124 (stack75)
        %v90126 = vxor.u32 %v90121, %v90125 (stack76)
        %v90129 = vadd.s32 %v90121, %v90126 (stack65)
        %v90133 = vadd.s32 %v90129, %v9 (stack65)
        %v90135 = vshll.u32 %v90126, 24 (stack73)
        %v90136 = vshrl.u32 %v90126, 8 (stack74)
        %v90137 = vor.u32 %v90135, %v90136 (stack75)
        %v90138 = vxor.u32 %v90129, %v90137 (stack76)
        %v90141 = vadd.s32 %v90138, %v8 (stack65)
        %v90145 = vadd.s32 %v90141, 4 (stack65)
        %v90149 = vadd.s32 %v90133, %v90145 (stack65)
        %v90151 = vshll.u32 %v90145, 13 (stack73)
        %v90152 = vshrl.u32 %v90145, 19 (stack74)
        %v90153 = vor.u32 %v90151, %v90152 (stack75)
        %v90154 = vxor.u32 %v90149, %v90153 (stack76)
        %v90157 = vadd.s32 %v90149, %v90154 (stack65)
        %v90159 = vshll.u32 %v90154, 15 (stack73)
        %v90160 = vshrl.u32 %v90154, 17 (stack74)
        %v90161 = vor.u32 %v90159, %v90160 (stack75)
        %v90162 = vxor.u32 %v90157, %v90161 (stack76)
        %v90165 = vadd.s32 %v90157, %v90162 (stack65)
        %v90167 = vshll.u32 %v90162, 26 (stack73)
        %v90168 = vshrl.u32 %v90162, 6 (stack74)
        %v90169 = vor.u32 %v90167, %v90168 (stack75)
        %v90170 = vxor.u32 %v90165, %v90169 (stack76)
        %v90173 = vadd.s32 %v90165, %v90170 (stack65)
        %v90177 = vadd.s32 %v90173, %v8 (stack65)
        %v90179 = vshll.u32 %v90170, 6 (stack73)
        %v90180 = vshrl.u32 %v90170, 26 (stack74)
        %v90181 = vor.u32 %v90179, %v90180 (stack75)
        %v90182 = vxor.u32 %v90173, %v90181 (stack76)
        %v90185 = vadd.s32 %v90182, %v10 (stack65)
        %v90189 = vadd.s32 %v90185, 5 (stack65)
        %v90191 = vxor.u32 %v90177, %v90189 (stack76)
        %v90192 = vand.u32.u8 %v90191, 255 (stack77)
        %v90193 = vand.u32 %v90192, 65535 (stack78)
        %v90194 = vshrl.u32 %v90193, 1 (stack79)
        %v90195 = vor.u32 %v90194, 16256 (stack75)
        %v90196 = vand.u32.u16 %v90195, 65535 (stack80)
        %v90197 = vunpack.i.l.bf16 %v90196 (stack81)
        %v90201 = vadd.f32 %v90197, -1.0 (stack82)
        %v90205 = vmul.f32 %v90201, 2.0 (stack83)
        %v90209 = vadd.f32 %v90205, -0.99609375 (stack82)
        %v90213 = vmax.f32 -0.99609375, %v90209 (stack84)
        %v90215 = vand.u32 2147483647, %v90213 (stack85)
        %vm90218 = vcmp.eq.f32.partialorder %v90215, 1.0 (stack86)
        %v90223 = vmul.f32 %v90213, inf (stack83)
        %v90225 = vxor.u32 %v90213, 2147483648 (stack87)
        %v90228 = vmul.f32 %v90213, %v90225 (stack83)
        %v90230 = vadd.f32 %v90228, 1.0 (stack88)
        %v90231 = vlog2.pop %v90230 (stack89)
        %v90232 = vmul.f32 %v90231, 0.6931472 (stack90)
        %v90233 = vmul.f32 -0.5, %v90228 (stack91)
        %v90234 = vadd.f32 %v90233, 1.0 (stack92)
        %v90235 = vmul.f32 %v90234, %v90228 (stack93)
        %v90236 = vand.u32 2147483647, %v90228 (stack94)
        %vm90237 = vcmp.lt.f32.partialorder %v90236, 0.0004427343 (stack95)
        %v90238 = vsel /*vm=*/%vm90237, /*on_true_vy=*/%v90235, /*on_false_vx=*/%v90232 (stack96)
        %v90239 = vxor.u32 %v90238, 2147483648 (stack87)
        %vm90242 = vcmp.lt.f32.partialorder %v90239, 5.0 (stack86)
        %v90247 = vsel /*vm=*/%vm90242, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v90251 = vsel /*vm=*/%vm90242, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v90255 = vsel /*vm=*/%vm90242, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v90259 = vsel /*vm=*/%vm90242, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v90263 = vsel /*vm=*/%vm90242, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v90267 = vsel /*vm=*/%vm90242, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v90271 = vsel /*vm=*/%vm90242, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v90275 = vsel /*vm=*/%vm90242, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v90279 = vsel /*vm=*/%vm90242, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v90283 = vadd.f32 %v90239, -2.5 (stack82)
        %v90285 = vrsqrt.pop %v90239 (stack97)
        %v90286 = vmul.f32 %v90239, %v90285 (stack98)
        %vm90287 = vcmp.eq.f32.partialorder %v90239, inf (stack99)
        %v90288 = vsel /*vm=*/%vm90287, /*on_true_vy=*/%v90239, /*on_false_vx=*/%v90286 (stack100)
        %vm90289 = vcmp.eq.f32.partialorder %v90239, 0.0 (stack101)
        %v90290 = vand.u32 %v90239, 2147483648 (stack102)
        %v90291 = vsel /*vm=*/%vm90289, /*on_true_vy=*/%v90290, /*on_false_vx=*/%v90288 (stack103)
        %v90294 = vadd.f32 %v90291, -3.0 (stack82)
        %v90298 = vsel /*vm=*/%vm90242, /*on_true_vy=*/%v90283, /*on_false_vx=*/%v90294 (stack72)
        %v90302 = vmul.f32 %v90279, %v90298 (stack83)
        %v90306 = vadd.f32 %v90275, %v90302 (stack82)
        %v90310 = vmul.f32 %v90306, %v90298 (stack83)
        %v90314 = vadd.f32 %v90271, %v90310 (stack82)
        %v90318 = vmul.f32 %v90314, %v90298 (stack83)
        %v90322 = vadd.f32 %v90267, %v90318 (stack82)
        %v90326 = vmul.f32 %v90322, %v90298 (stack83)
        %v90330 = vadd.f32 %v90263, %v90326 (stack82)
        %v90334 = vmul.f32 %v90330, %v90298 (stack83)
        %v90338 = vadd.f32 %v90259, %v90334 (stack82)
        %v90342 = vmul.f32 %v90338, %v90298 (stack83)
        %v90346 = vadd.f32 %v90255, %v90342 (stack82)
        %v90350 = vmul.f32 %v90346, %v90298 (stack83)
        %v90354 = vadd.f32 %v90251, %v90350 (stack82)
        %v90358 = vmul.f32 %v90354, %v90298 (stack83)
        %v90362 = vadd.f32 %v90247, %v90358 (stack82)
        %v90366 = vmul.f32 %v90362, %v90213 (stack83)
        %v90370 = vsel /*vm=*/%vm90218, /*on_true_vy=*/%v90223, /*on_false_vx=*/%v90366 (stack72)
        %v90374 = vmul.f32 %v90370, 1.4140625 (stack83)
        %s90376 = scalar_lea.vmem %s280, 96 [#allocation0] (stack107)
        %v90377 = vpack.c.bf16 0.0, %v90374 (stack104)
        %90378 = vst [vmem:[%s90376] sm:$0xf] /*vst_source=*/%v90377 (stack105)
        %v90381 = vadd.s32 %v894, %v89917 (stack65)
        %s90383 = smul.u32 128, %s27 (stack66)
        %v90384 = vlaneseq (stack67)
        %v90385 = vand.u32 %v90384, 127 (stack68)
        %v90386 = vstv %s90383 (stack69)
        %v90387 = vadd.s32 %v90385, %v90386 (stack70)
        %v90391 = vadd.s32 %v90381, %v90387 (stack65)
        %vm90395 = vcmp.lt.u32.totalorder %v90391, %v90381 (stack71)
        %vm90400 = vcmp.lt.u32.totalorder %v90381, %v894 (stack71)
        %v90405 = vadd.s32 %v881, %v89900 (stack65)
        %v90409 = vadd.s32 %v90405, 1 (stack65)
        %v90413 = vsel /*vm=*/%vm90400, /*on_true_vy=*/%v90409, /*on_false_vx=*/%v90405 (stack72)
        %v90417 = vadd.s32 %v90413, 1 (stack65)
        %v90421 = vsel /*vm=*/%vm90395, /*on_true_vy=*/%v90417, /*on_false_vx=*/%v90413 (stack72)
        %v90426 = vadd.s32 %v90421, %v10 (stack65)
        %v90430 = vadd.s32 %v90391, %v9 (stack65)
        %v90434 = vadd.s32 %v90426, %v90430 (stack65)
        %v90436 = vshll.u32 %v90430, 13 (stack73)
        %v90437 = vshrl.u32 %v90430, 19 (stack74)
        %v90438 = vor.u32 %v90436, %v90437 (stack75)
        %v90439 = vxor.u32 %v90434, %v90438 (stack76)
        %v90442 = vadd.s32 %v90434, %v90439 (stack65)
        %v90444 = vshll.u32 %v90439, 15 (stack73)
        %v90445 = vshrl.u32 %v90439, 17 (stack74)
        %v90446 = vor.u32 %v90444, %v90445 (stack75)
        %v90447 = vxor.u32 %v90442, %v90446 (stack76)
        %v90450 = vadd.s32 %v90442, %v90447 (stack65)
        %v90452 = vshll.u32 %v90447, 26 (stack73)
        %v90453 = vshrl.u32 %v90447, 6 (stack74)
        %v90454 = vor.u32 %v90452, %v90453 (stack75)
        %v90455 = vxor.u32 %v90450, %v90454 (stack76)
        %v90458 = vadd.s32 %v90450, %v90455 (stack65)
        %v90462 = vadd.s32 %v90458, %v9 (stack65)
        %v90464 = vshll.u32 %v90455, 6 (stack73)
        %v90465 = vshrl.u32 %v90455, 26 (stack74)
        %v90466 = vor.u32 %v90464, %v90465 (stack75)
        %v90467 = vxor.u32 %v90458, %v90466 (stack76)
        %v90470 = vadd.s32 %v90467, %v8 (stack65)
        %v90474 = vadd.s32 %v90470, 1 (stack65)
        %v90478 = vadd.s32 %v90462, %v90474 (stack65)
        %v90480 = vshll.u32 %v90474, 17 (stack73)
        %v90481 = vshrl.u32 %v90474, 15 (stack74)
        %v90482 = vor.u32 %v90480, %v90481 (stack75)
        %v90483 = vxor.u32 %v90478, %v90482 (stack76)
        %v90486 = vadd.s32 %v90478, %v90483 (stack65)
        %v90488 = vshll.u32 %v90483, 29 (stack73)
        %v90489 = vshrl.u32 %v90483, 3 (stack74)
        %v90490 = vor.u32 %v90488, %v90489 (stack75)
        %v90491 = vxor.u32 %v90486, %v90490 (stack76)
        %v90494 = vadd.s32 %v90486, %v90491 (stack65)
        %v90496 = vshll.u32 %v90491, 16 (stack73)
        %v90497 = vshrl.u32 %v90491, 16 (stack74)
        %v90498 = vor.u32 %v90496, %v90497 (stack75)
        %v90499 = vxor.u32 %v90494, %v90498 (stack76)
        %v90502 = vadd.s32 %v90494, %v90499 (stack65)
        %v90506 = vadd.s32 %v90502, %v8 (stack65)
        %v90508 = vshll.u32 %v90499, 24 (stack73)
        %v90509 = vshrl.u32 %v90499, 8 (stack74)
        %v90510 = vor.u32 %v90508, %v90509 (stack75)
        %v90511 = vxor.u32 %v90502, %v90510 (stack76)
        %v90514 = vadd.s32 %v90511, %v10 (stack65)
        %v90518 = vadd.s32 %v90514, 2 (stack65)
        %v90522 = vadd.s32 %v90506, %v90518 (stack65)
        %v90524 = vshll.u32 %v90518, 13 (stack73)
        %v90525 = vshrl.u32 %v90518, 19 (stack74)
        %v90526 = vor.u32 %v90524, %v90525 (stack75)
        %v90527 = vxor.u32 %v90522, %v90526 (stack76)
        %v90530 = vadd.s32 %v90522, %v90527 (stack65)
        %v90532 = vshll.u32 %v90527, 15 (stack73)
        %v90533 = vshrl.u32 %v90527, 17 (stack74)
        %v90534 = vor.u32 %v90532, %v90533 (stack75)
        %v90535 = vxor.u32 %v90530, %v90534 (stack76)
        %v90538 = vadd.s32 %v90530, %v90535 (stack65)
        %v90540 = vshll.u32 %v90535, 26 (stack73)
        %v90541 = vshrl.u32 %v90535, 6 (stack74)
        %v90542 = vor.u32 %v90540, %v90541 (stack75)
        %v90543 = vxor.u32 %v90538, %v90542 (stack76)
        %v90546 = vadd.s32 %v90538, %v90543 (stack65)
        %v90550 = vadd.s32 %v90546, %v10 (stack65)
        %v90552 = vshll.u32 %v90543, 6 (stack73)
        %v90553 = vshrl.u32 %v90543, 26 (stack74)
        %v90554 = vor.u32 %v90552, %v90553 (stack75)
        %v90555 = vxor.u32 %v90546, %v90554 (stack76)
        %v90558 = vadd.s32 %v90555, %v9 (stack65)
        %v90562 = vadd.s32 %v90558, 3 (stack65)
        %v90566 = vadd.s32 %v90550, %v90562 (stack65)
        %v90568 = vshll.u32 %v90562, 17 (stack73)
        %v90569 = vshrl.u32 %v90562, 15 (stack74)
        %v90570 = vor.u32 %v90568, %v90569 (stack75)
        %v90571 = vxor.u32 %v90566, %v90570 (stack76)
        %v90574 = vadd.s32 %v90566, %v90571 (stack65)
        %v90576 = vshll.u32 %v90571, 29 (stack73)
        %v90577 = vshrl.u32 %v90571, 3 (stack74)
        %v90578 = vor.u32 %v90576, %v90577 (stack75)
        %v90579 = vxor.u32 %v90574, %v90578 (stack76)
        %v90582 = vadd.s32 %v90574, %v90579 (stack65)
        %v90584 = vshll.u32 %v90579, 16 (stack73)
        %v90585 = vshrl.u32 %v90579, 16 (stack74)
        %v90586 = vor.u32 %v90584, %v90585 (stack75)
        %v90587 = vxor.u32 %v90582, %v90586 (stack76)
        %v90590 = vadd.s32 %v90582, %v90587 (stack65)
        %v90594 = vadd.s32 %v90590, %v9 (stack65)
        %v90596 = vshll.u32 %v90587, 24 (stack73)
        %v90597 = vshrl.u32 %v90587, 8 (stack74)
        %v90598 = vor.u32 %v90596, %v90597 (stack75)
        %v90599 = vxor.u32 %v90590, %v90598 (stack76)
        %v90602 = vadd.s32 %v90599, %v8 (stack65)
        %v90606 = vadd.s32 %v90602, 4 (stack65)
        %v90610 = vadd.s32 %v90594, %v90606 (stack65)
        %v90612 = vshll.u32 %v90606, 13 (stack73)
        %v90613 = vshrl.u32 %v90606, 19 (stack74)
        %v90614 = vor.u32 %v90612, %v90613 (stack75)
        %v90615 = vxor.u32 %v90610, %v90614 (stack76)
        %v90618 = vadd.s32 %v90610, %v90615 (stack65)
        %v90620 = vshll.u32 %v90615, 15 (stack73)
        %v90621 = vshrl.u32 %v90615, 17 (stack74)
        %v90622 = vor.u32 %v90620, %v90621 (stack75)
        %v90623 = vxor.u32 %v90618, %v90622 (stack76)
        %v90626 = vadd.s32 %v90618, %v90623 (stack65)
        %v90628 = vshll.u32 %v90623, 26 (stack73)
        %v90629 = vshrl.u32 %v90623, 6 (stack74)
        %v90630 = vor.u32 %v90628, %v90629 (stack75)
        %v90631 = vxor.u32 %v90626, %v90630 (stack76)
        %v90634 = vadd.s32 %v90626, %v90631 (stack65)
        %v90638 = vadd.s32 %v90634, %v8 (stack65)
        %v90640 = vshll.u32 %v90631, 6 (stack73)
        %v90641 = vshrl.u32 %v90631, 26 (stack74)
        %v90642 = vor.u32 %v90640, %v90641 (stack75)
        %v90643 = vxor.u32 %v90634, %v90642 (stack76)
        %v90646 = vadd.s32 %v90643, %v10 (stack65)
        %v90650 = vadd.s32 %v90646, 5 (stack65)
        %v90652 = vxor.u32 %v90638, %v90650 (stack76)
        %v90653 = vand.u32.u8 %v90652, 255 (stack77)
        %v90654 = vand.u32 %v90653, 65535 (stack78)
        %v90655 = vshrl.u32 %v90654, 1 (stack79)
        %v90656 = vor.u32 %v90655, 16256 (stack75)
        %v90657 = vand.u32.u16 %v90656, 65535 (stack80)
        %v90658 = vunpack.i.l.bf16 %v90657 (stack81)
        %v90662 = vadd.f32 %v90658, -1.0 (stack82)
        %v90666 = vmul.f32 %v90662, 2.0 (stack83)
        %v90670 = vadd.f32 %v90666, -0.99609375 (stack82)
        %v90674 = vmax.f32 -0.99609375, %v90670 (stack84)
        %v90676 = vand.u32 2147483647, %v90674 (stack85)
        %vm90679 = vcmp.eq.f32.partialorder %v90676, 1.0 (stack86)
        %v90684 = vmul.f32 %v90674, inf (stack83)
        %v90686 = vxor.u32 %v90674, 2147483648 (stack87)
        %v90689 = vmul.f32 %v90674, %v90686 (stack83)
        %v90691 = vadd.f32 %v90689, 1.0 (stack88)
        %v90692 = vlog2.pop %v90691 (stack89)
        %v90693 = vmul.f32 %v90692, 0.6931472 (stack90)
        %v90694 = vmul.f32 -0.5, %v90689 (stack91)
        %v90695 = vadd.f32 %v90694, 1.0 (stack92)
        %v90696 = vmul.f32 %v90695, %v90689 (stack93)
        %v90697 = vand.u32 2147483647, %v90689 (stack94)
        %vm90698 = vcmp.lt.f32.partialorder %v90697, 0.0004427343 (stack95)
        %v90699 = vsel /*vm=*/%vm90698, /*on_true_vy=*/%v90696, /*on_false_vx=*/%v90693 (stack96)
        %v90700 = vxor.u32 %v90699, 2147483648 (stack87)
        %vm90703 = vcmp.lt.f32.partialorder %v90700, 5.0 (stack86)
        %v90708 = vsel /*vm=*/%vm90703, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v90712 = vsel /*vm=*/%vm90703, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v90716 = vsel /*vm=*/%vm90703, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v90720 = vsel /*vm=*/%vm90703, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v90724 = vsel /*vm=*/%vm90703, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v90728 = vsel /*vm=*/%vm90703, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v90732 = vsel /*vm=*/%vm90703, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v90736 = vsel /*vm=*/%vm90703, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v90740 = vsel /*vm=*/%vm90703, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v90744 = vadd.f32 %v90700, -2.5 (stack82)
        %v90746 = vrsqrt.pop %v90700 (stack97)
        %v90747 = vmul.f32 %v90700, %v90746 (stack98)
        %vm90748 = vcmp.eq.f32.partialorder %v90700, inf (stack99)
        %v90749 = vsel /*vm=*/%vm90748, /*on_true_vy=*/%v90700, /*on_false_vx=*/%v90747 (stack100)
        %vm90750 = vcmp.eq.f32.partialorder %v90700, 0.0 (stack101)
        %v90751 = vand.u32 %v90700, 2147483648 (stack102)
        %v90752 = vsel /*vm=*/%vm90750, /*on_true_vy=*/%v90751, /*on_false_vx=*/%v90749 (stack103)
        %v90755 = vadd.f32 %v90752, -3.0 (stack82)
        %v90759 = vsel /*vm=*/%vm90703, /*on_true_vy=*/%v90744, /*on_false_vx=*/%v90755 (stack72)
        %v90763 = vmul.f32 %v90740, %v90759 (stack83)
        %v90767 = vadd.f32 %v90736, %v90763 (stack82)
        %v90771 = vmul.f32 %v90767, %v90759 (stack83)
        %v90775 = vadd.f32 %v90732, %v90771 (stack82)
        %v90779 = vmul.f32 %v90775, %v90759 (stack83)
        %v90783 = vadd.f32 %v90728, %v90779 (stack82)
        %v90787 = vmul.f32 %v90783, %v90759 (stack83)
        %v90791 = vadd.f32 %v90724, %v90787 (stack82)
        %v90795 = vmul.f32 %v90791, %v90759 (stack83)
        %v90799 = vadd.f32 %v90720, %v90795 (stack82)
        %v90803 = vmul.f32 %v90799, %v90759 (stack83)
        %v90807 = vadd.f32 %v90716, %v90803 (stack82)
        %v90811 = vmul.f32 %v90807, %v90759 (stack83)
        %v90815 = vadd.f32 %v90712, %v90811 (stack82)
        %v90819 = vmul.f32 %v90815, %v90759 (stack83)
        %v90823 = vadd.f32 %v90708, %v90819 (stack82)
        %v90827 = vmul.f32 %v90823, %v90674 (stack83)
        %v90831 = vsel /*vm=*/%vm90679, /*on_true_vy=*/%v90684, /*on_false_vx=*/%v90827 (stack72)
        %v90835 = vmul.f32 %v90831, 1.4140625 (stack83)
        %s90837 = scalar_lea.vmem %s280, 224 [#allocation0] (stack107)
        %v90838 = vpack.c.bf16 0.0, %v90835 (stack104)
        %90839 = vst [vmem:[%s90837] sm:$0xf] /*vst_source=*/%v90838 (stack105)
        %v90842 = vadd.s32 %v1381, %v89917 (stack65)
        %s90844 = smul.u32 128, %s27 (stack66)
        %v90845 = vlaneseq (stack67)
        %v90846 = vand.u32 %v90845, 127 (stack68)
        %v90847 = vstv %s90844 (stack69)
        %v90848 = vadd.s32 %v90846, %v90847 (stack70)
        %v90852 = vadd.s32 %v90842, %v90848 (stack65)
        %vm90856 = vcmp.lt.u32.totalorder %v90852, %v90842 (stack71)
        %vm90861 = vcmp.lt.u32.totalorder %v90842, %v1381 (stack71)
        %v90866 = vadd.s32 %v1368, %v89900 (stack65)
        %v90870 = vadd.s32 %v90866, 1 (stack65)
        %v90874 = vsel /*vm=*/%vm90861, /*on_true_vy=*/%v90870, /*on_false_vx=*/%v90866 (stack72)
        %v90878 = vadd.s32 %v90874, 1 (stack65)
        %v90882 = vsel /*vm=*/%vm90856, /*on_true_vy=*/%v90878, /*on_false_vx=*/%v90874 (stack72)
        %v90887 = vadd.s32 %v90882, %v10 (stack65)
        %v90891 = vadd.s32 %v90852, %v9 (stack65)
        %v90895 = vadd.s32 %v90887, %v90891 (stack65)
        %v90897 = vshll.u32 %v90891, 13 (stack73)
        %v90898 = vshrl.u32 %v90891, 19 (stack74)
        %v90899 = vor.u32 %v90897, %v90898 (stack75)
        %v90900 = vxor.u32 %v90895, %v90899 (stack76)
        %v90903 = vadd.s32 %v90895, %v90900 (stack65)
        %v90905 = vshll.u32 %v90900, 15 (stack73)
        %v90906 = vshrl.u32 %v90900, 17 (stack74)
        %v90907 = vor.u32 %v90905, %v90906 (stack75)
        %v90908 = vxor.u32 %v90903, %v90907 (stack76)
        %v90911 = vadd.s32 %v90903, %v90908 (stack65)
        %v90913 = vshll.u32 %v90908, 26 (stack73)
        %v90914 = vshrl.u32 %v90908, 6 (stack74)
        %v90915 = vor.u32 %v90913, %v90914 (stack75)
        %v90916 = vxor.u32 %v90911, %v90915 (stack76)
        %v90919 = vadd.s32 %v90911, %v90916 (stack65)
        %v90923 = vadd.s32 %v90919, %v9 (stack65)
        %v90925 = vshll.u32 %v90916, 6 (stack73)
        %v90926 = vshrl.u32 %v90916, 26 (stack74)
        %v90927 = vor.u32 %v90925, %v90926 (stack75)
        %v90928 = vxor.u32 %v90919, %v90927 (stack76)
        %v90931 = vadd.s32 %v90928, %v8 (stack65)
        %v90935 = vadd.s32 %v90931, 1 (stack65)
        %v90939 = vadd.s32 %v90923, %v90935 (stack65)
        %v90941 = vshll.u32 %v90935, 17 (stack73)
        %v90942 = vshrl.u32 %v90935, 15 (stack74)
        %v90943 = vor.u32 %v90941, %v90942 (stack75)
        %v90944 = vxor.u32 %v90939, %v90943 (stack76)
        %v90947 = vadd.s32 %v90939, %v90944 (stack65)
        %v90949 = vshll.u32 %v90944, 29 (stack73)
        %v90950 = vshrl.u32 %v90944, 3 (stack74)
        %v90951 = vor.u32 %v90949, %v90950 (stack75)
        %v90952 = vxor.u32 %v90947, %v90951 (stack76)
        %v90955 = vadd.s32 %v90947, %v90952 (stack65)
        %v90957 = vshll.u32 %v90952, 16 (stack73)
        %v90958 = vshrl.u32 %v90952, 16 (stack74)
        %v90959 = vor.u32 %v90957, %v90958 (stack75)
        %v90960 = vxor.u32 %v90955, %v90959 (stack76)
        %v90963 = vadd.s32 %v90955, %v90960 (stack65)
        %v90967 = vadd.s32 %v90963, %v8 (stack65)
        %v90969 = vshll.u32 %v90960, 24 (stack73)
        %v90970 = vshrl.u32 %v90960, 8 (stack74)
        %v90971 = vor.u32 %v90969, %v90970 (stack75)
        %v90972 = vxor.u32 %v90963, %v90971 (stack76)
        %v90975 = vadd.s32 %v90972, %v10 (stack65)
        %v90979 = vadd.s32 %v90975, 2 (stack65)
        %v90983 = vadd.s32 %v90967, %v90979 (stack65)
        %v90985 = vshll.u32 %v90979, 13 (stack73)
        %v90986 = vshrl.u32 %v90979, 19 (stack74)
        %v90987 = vor.u32 %v90985, %v90986 (stack75)
        %v90988 = vxor.u32 %v90983, %v90987 (stack76)
        %v90991 = vadd.s32 %v90983, %v90988 (stack65)
        %v90993 = vshll.u32 %v90988, 15 (stack73)
        %v90994 = vshrl.u32 %v90988, 17 (stack74)
        %v90995 = vor.u32 %v90993, %v90994 (stack75)
        %v90996 = vxor.u32 %v90991, %v90995 (stack76)
        %v90999 = vadd.s32 %v90991, %v90996 (stack65)
        %v91001 = vshll.u32 %v90996, 26 (stack73)
        %v91002 = vshrl.u32 %v90996, 6 (stack74)
        %v91003 = vor.u32 %v91001, %v91002 (stack75)
        %v91004 = vxor.u32 %v90999, %v91003 (stack76)
        %v91007 = vadd.s32 %v90999, %v91004 (stack65)
        %v91011 = vadd.s32 %v91007, %v10 (stack65)
        %v91013 = vshll.u32 %v91004, 6 (stack73)
        %v91014 = vshrl.u32 %v91004, 26 (stack74)
        %v91015 = vor.u32 %v91013, %v91014 (stack75)
        %v91016 = vxor.u32 %v91007, %v91015 (stack76)
        %v91019 = vadd.s32 %v91016, %v9 (stack65)
        %v91023 = vadd.s32 %v91019, 3 (stack65)
        %v91027 = vadd.s32 %v91011, %v91023 (stack65)
        %v91029 = vshll.u32 %v91023, 17 (stack73)
        %v91030 = vshrl.u32 %v91023, 15 (stack74)
        %v91031 = vor.u32 %v91029, %v91030 (stack75)
        %v91032 = vxor.u32 %v91027, %v91031 (stack76)
        %v91035 = vadd.s32 %v91027, %v91032 (stack65)
        %v91037 = vshll.u32 %v91032, 29 (stack73)
        %v91038 = vshrl.u32 %v91032, 3 (stack74)
        %v91039 = vor.u32 %v91037, %v91038 (stack75)
        %v91040 = vxor.u32 %v91035, %v91039 (stack76)
        %v91043 = vadd.s32 %v91035, %v91040 (stack65)
        %v91045 = vshll.u32 %v91040, 16 (stack73)
        %v91046 = vshrl.u32 %v91040, 16 (stack74)
        %v91047 = vor.u32 %v91045, %v91046 (stack75)
        %v91048 = vxor.u32 %v91043, %v91047 (stack76)
        %v91051 = vadd.s32 %v91043, %v91048 (stack65)
        %v91055 = vadd.s32 %v91051, %v9 (stack65)
        %v91057 = vshll.u32 %v91048, 24 (stack73)
        %v91058 = vshrl.u32 %v91048, 8 (stack74)
        %v91059 = vor.u32 %v91057, %v91058 (stack75)
        %v91060 = vxor.u32 %v91051, %v91059 (stack76)
        %v91063 = vadd.s32 %v91060, %v8 (stack65)
        %v91067 = vadd.s32 %v91063, 4 (stack65)
        %v91071 = vadd.s32 %v91055, %v91067 (stack65)
        %v91073 = vshll.u32 %v91067, 13 (stack73)
        %v91074 = vshrl.u32 %v91067, 19 (stack74)
        %v91075 = vor.u32 %v91073, %v91074 (stack75)
        %v91076 = vxor.u32 %v91071, %v91075 (stack76)
        %v91079 = vadd.s32 %v91071, %v91076 (stack65)
        %v91081 = vshll.u32 %v91076, 15 (stack73)
        %v91082 = vshrl.u32 %v91076, 17 (stack74)
        %v91083 = vor.u32 %v91081, %v91082 (stack75)
        %v91084 = vxor.u32 %v91079, %v91083 (stack76)
        %v91087 = vadd.s32 %v91079, %v91084 (stack65)
        %v91089 = vshll.u32 %v91084, 26 (stack73)
        %v91090 = vshrl.u32 %v91084, 6 (stack74)
        %v91091 = vor.u32 %v91089, %v91090 (stack75)
        %v91092 = vxor.u32 %v91087, %v91091 (stack76)
        %v91095 = vadd.s32 %v91087, %v91092 (stack65)
        %v91099 = vadd.s32 %v91095, %v8 (stack65)
        %v91101 = vshll.u32 %v91092, 6 (stack73)
        %v91102 = vshrl.u32 %v91092, 26 (stack74)
        %v91103 = vor.u32 %v91101, %v91102 (stack75)
        %v91104 = vxor.u32 %v91095, %v91103 (stack76)
        %v91107 = vadd.s32 %v91104, %v10 (stack65)
        %v91111 = vadd.s32 %v91107, 5 (stack65)
        %v91113 = vxor.u32 %v91099, %v91111 (stack76)
        %v91114 = vand.u32.u8 %v91113, 255 (stack77)
        %v91115 = vand.u32 %v91114, 65535 (stack78)
        %v91116 = vshrl.u32 %v91115, 1 (stack79)
        %v91117 = vor.u32 %v91116, 16256 (stack75)
        %v91118 = vand.u32.u16 %v91117, 65535 (stack80)
        %v91119 = vunpack.i.l.bf16 %v91118 (stack81)
        %v91123 = vadd.f32 %v91119, -1.0 (stack82)
        %v91127 = vmul.f32 %v91123, 2.0 (stack83)
        %v91131 = vadd.f32 %v91127, -0.99609375 (stack82)
        %v91135 = vmax.f32 -0.99609375, %v91131 (stack84)
        %v91137 = vand.u32 2147483647, %v91135 (stack85)
        %vm91140 = vcmp.eq.f32.partialorder %v91137, 1.0 (stack86)
        %v91145 = vmul.f32 %v91135, inf (stack83)
        %v91147 = vxor.u32 %v91135, 2147483648 (stack87)
        %v91150 = vmul.f32 %v91135, %v91147 (stack83)
        %v91152 = vadd.f32 %v91150, 1.0 (stack88)
        %v91153 = vlog2.pop %v91152 (stack89)
        %v91154 = vmul.f32 %v91153, 0.6931472 (stack90)
        %v91155 = vmul.f32 -0.5, %v91150 (stack91)
        %v91156 = vadd.f32 %v91155, 1.0 (stack92)
        %v91157 = vmul.f32 %v91156, %v91150 (stack93)
        %v91158 = vand.u32 2147483647, %v91150 (stack94)
        %vm91159 = vcmp.lt.f32.partialorder %v91158, 0.0004427343 (stack95)
        %v91160 = vsel /*vm=*/%vm91159, /*on_true_vy=*/%v91157, /*on_false_vx=*/%v91154 (stack96)
        %v91161 = vxor.u32 %v91160, 2147483648 (stack87)
        %vm91164 = vcmp.lt.f32.partialorder %v91161, 5.0 (stack86)
        %v91169 = vsel /*vm=*/%vm91164, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v91173 = vsel /*vm=*/%vm91164, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v91177 = vsel /*vm=*/%vm91164, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v91181 = vsel /*vm=*/%vm91164, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v91185 = vsel /*vm=*/%vm91164, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v91189 = vsel /*vm=*/%vm91164, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v91193 = vsel /*vm=*/%vm91164, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v91197 = vsel /*vm=*/%vm91164, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v91201 = vsel /*vm=*/%vm91164, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v91205 = vadd.f32 %v91161, -2.5 (stack82)
        %v91207 = vrsqrt.pop %v91161 (stack97)
        %v91208 = vmul.f32 %v91161, %v91207 (stack98)
        %vm91209 = vcmp.eq.f32.partialorder %v91161, inf (stack99)
        %v91210 = vsel /*vm=*/%vm91209, /*on_true_vy=*/%v91161, /*on_false_vx=*/%v91208 (stack100)
        %vm91211 = vcmp.eq.f32.partialorder %v91161, 0.0 (stack101)
        %v91212 = vand.u32 %v91161, 2147483648 (stack102)
        %v91213 = vsel /*vm=*/%vm91211, /*on_true_vy=*/%v91212, /*on_false_vx=*/%v91210 (stack103)
        %v91216 = vadd.f32 %v91213, -3.0 (stack82)
        %v91220 = vsel /*vm=*/%vm91164, /*on_true_vy=*/%v91205, /*on_false_vx=*/%v91216 (stack72)
        %v91224 = vmul.f32 %v91201, %v91220 (stack83)
        %v91228 = vadd.f32 %v91197, %v91224 (stack82)
        %v91232 = vmul.f32 %v91228, %v91220 (stack83)
        %v91236 = vadd.f32 %v91193, %v91232 (stack82)
        %v91240 = vmul.f32 %v91236, %v91220 (stack83)
        %v91244 = vadd.f32 %v91189, %v91240 (stack82)
        %v91248 = vmul.f32 %v91244, %v91220 (stack83)
        %v91252 = vadd.f32 %v91185, %v91248 (stack82)
        %v91256 = vmul.f32 %v91252, %v91220 (stack83)
        %v91260 = vadd.f32 %v91181, %v91256 (stack82)
        %v91264 = vmul.f32 %v91260, %v91220 (stack83)
        %v91268 = vadd.f32 %v91177, %v91264 (stack82)
        %v91272 = vmul.f32 %v91268, %v91220 (stack83)
        %v91276 = vadd.f32 %v91173, %v91272 (stack82)
        %v91280 = vmul.f32 %v91276, %v91220 (stack83)
        %v91284 = vadd.f32 %v91169, %v91280 (stack82)
        %v91288 = vmul.f32 %v91284, %v91135 (stack83)
        %v91292 = vsel /*vm=*/%vm91140, /*on_true_vy=*/%v91145, /*on_false_vx=*/%v91288 (stack72)
        %v91296 = vmul.f32 %v91292, 1.4140625 (stack83)
        %s91298 = scalar_lea.vmem %s280, 352 [#allocation0] (stack107)
        %v91299 = vpack.c.bf16 0.0, %v91296 (stack104)
        %91300 = vst [vmem:[%s91298] sm:$0xf] /*vst_source=*/%v91299 (stack105)
        %v91303 = vadd.s32 %v1868, %v89917 (stack65)
        %s91305 = smul.u32 128, %s27 (stack66)
        %v91306 = vlaneseq (stack67)
        %v91307 = vand.u32 %v91306, 127 (stack68)
        %v91308 = vstv %s91305 (stack69)
        %v91309 = vadd.s32 %v91307, %v91308 (stack70)
        %v91313 = vadd.s32 %v91303, %v91309 (stack65)
        %vm91317 = vcmp.lt.u32.totalorder %v91313, %v91303 (stack71)
        %vm91322 = vcmp.lt.u32.totalorder %v91303, %v1868 (stack71)
        %v91327 = vadd.s32 %v1855, %v89900 (stack65)
        %v91331 = vadd.s32 %v91327, 1 (stack65)
        %v91335 = vsel /*vm=*/%vm91322, /*on_true_vy=*/%v91331, /*on_false_vx=*/%v91327 (stack72)
        %v91339 = vadd.s32 %v91335, 1 (stack65)
        %v91343 = vsel /*vm=*/%vm91317, /*on_true_vy=*/%v91339, /*on_false_vx=*/%v91335 (stack72)
        %v91348 = vadd.s32 %v91343, %v10 (stack65)
        %v91352 = vadd.s32 %v91313, %v9 (stack65)
        %v91356 = vadd.s32 %v91348, %v91352 (stack65)
        %v91358 = vshll.u32 %v91352, 13 (stack73)
        %v91359 = vshrl.u32 %v91352, 19 (stack74)
        %v91360 = vor.u32 %v91358, %v91359 (stack75)
        %v91361 = vxor.u32 %v91356, %v91360 (stack76)
        %v91364 = vadd.s32 %v91356, %v91361 (stack65)
        %v91366 = vshll.u32 %v91361, 15 (stack73)
        %v91367 = vshrl.u32 %v91361, 17 (stack74)
        %v91368 = vor.u32 %v91366, %v91367 (stack75)
        %v91369 = vxor.u32 %v91364, %v91368 (stack76)
        %v91372 = vadd.s32 %v91364, %v91369 (stack65)
        %v91374 = vshll.u32 %v91369, 26 (stack73)
        %v91375 = vshrl.u32 %v91369, 6 (stack74)
        %v91376 = vor.u32 %v91374, %v91375 (stack75)
        %v91377 = vxor.u32 %v91372, %v91376 (stack76)
        %v91380 = vadd.s32 %v91372, %v91377 (stack65)
        %v91384 = vadd.s32 %v91380, %v9 (stack65)
        %v91386 = vshll.u32 %v91377, 6 (stack73)
        %v91387 = vshrl.u32 %v91377, 26 (stack74)
        %v91388 = vor.u32 %v91386, %v91387 (stack75)
        %v91389 = vxor.u32 %v91380, %v91388 (stack76)
        %v91392 = vadd.s32 %v91389, %v8 (stack65)
        %v91396 = vadd.s32 %v91392, 1 (stack65)
        %v91400 = vadd.s32 %v91384, %v91396 (stack65)
        %v91402 = vshll.u32 %v91396, 17 (stack73)
        %v91403 = vshrl.u32 %v91396, 15 (stack74)
        %v91404 = vor.u32 %v91402, %v91403 (stack75)
        %v91405 = vxor.u32 %v91400, %v91404 (stack76)
        %v91408 = vadd.s32 %v91400, %v91405 (stack65)
        %v91410 = vshll.u32 %v91405, 29 (stack73)
        %v91411 = vshrl.u32 %v91405, 3 (stack74)
        %v91412 = vor.u32 %v91410, %v91411 (stack75)
        %v91413 = vxor.u32 %v91408, %v91412 (stack76)
        %v91416 = vadd.s32 %v91408, %v91413 (stack65)
        %v91418 = vshll.u32 %v91413, 16 (stack73)
        %v91419 = vshrl.u32 %v91413, 16 (stack74)
        %v91420 = vor.u32 %v91418, %v91419 (stack75)
        %v91421 = vxor.u32 %v91416, %v91420 (stack76)
        %v91424 = vadd.s32 %v91416, %v91421 (stack65)
        %v91428 = vadd.s32 %v91424, %v8 (stack65)
        %v91430 = vshll.u32 %v91421, 24 (stack73)
        %v91431 = vshrl.u32 %v91421, 8 (stack74)
        %v91432 = vor.u32 %v91430, %v91431 (stack75)
        %v91433 = vxor.u32 %v91424, %v91432 (stack76)
        %v91436 = vadd.s32 %v91433, %v10 (stack65)
        %v91440 = vadd.s32 %v91436, 2 (stack65)
        %v91444 = vadd.s32 %v91428, %v91440 (stack65)
        %v91446 = vshll.u32 %v91440, 13 (stack73)
        %v91447 = vshrl.u32 %v91440, 19 (stack74)
        %v91448 = vor.u32 %v91446, %v91447 (stack75)
        %v91449 = vxor.u32 %v91444, %v91448 (stack76)
        %v91452 = vadd.s32 %v91444, %v91449 (stack65)
        %v91454 = vshll.u32 %v91449, 15 (stack73)
        %v91455 = vshrl.u32 %v91449, 17 (stack74)
        %v91456 = vor.u32 %v91454, %v91455 (stack75)
        %v91457 = vxor.u32 %v91452, %v91456 (stack76)
        %v91460 = vadd.s32 %v91452, %v91457 (stack65)
        %v91462 = vshll.u32 %v91457, 26 (stack73)
        %v91463 = vshrl.u32 %v91457, 6 (stack74)
        %v91464 = vor.u32 %v91462, %v91463 (stack75)
        %v91465 = vxor.u32 %v91460, %v91464 (stack76)
        %v91468 = vadd.s32 %v91460, %v91465 (stack65)
        %v91472 = vadd.s32 %v91468, %v10 (stack65)
        %v91474 = vshll.u32 %v91465, 6 (stack73)
        %v91475 = vshrl.u32 %v91465, 26 (stack74)
        %v91476 = vor.u32 %v91474, %v91475 (stack75)
        %v91477 = vxor.u32 %v91468, %v91476 (stack76)
        %v91480 = vadd.s32 %v91477, %v9 (stack65)
        %v91484 = vadd.s32 %v91480, 3 (stack65)
        %v91488 = vadd.s32 %v91472, %v91484 (stack65)
        %v91490 = vshll.u32 %v91484, 17 (stack73)
        %v91491 = vshrl.u32 %v91484, 15 (stack74)
        %v91492 = vor.u32 %v91490, %v91491 (stack75)
        %v91493 = vxor.u32 %v91488, %v91492 (stack76)
        %v91496 = vadd.s32 %v91488, %v91493 (stack65)
        %v91498 = vshll.u32 %v91493, 29 (stack73)
        %v91499 = vshrl.u32 %v91493, 3 (stack74)
        %v91500 = vor.u32 %v91498, %v91499 (stack75)
        %v91501 = vxor.u32 %v91496, %v91500 (stack76)
        %v91504 = vadd.s32 %v91496, %v91501 (stack65)
        %v91506 = vshll.u32 %v91501, 16 (stack73)
        %v91507 = vshrl.u32 %v91501, 16 (stack74)
        %v91508 = vor.u32 %v91506, %v91507 (stack75)
        %v91509 = vxor.u32 %v91504, %v91508 (stack76)
        %v91512 = vadd.s32 %v91504, %v91509 (stack65)
        %v91516 = vadd.s32 %v91512, %v9 (stack65)
        %v91518 = vshll.u32 %v91509, 24 (stack73)
        %v91519 = vshrl.u32 %v91509, 8 (stack74)
        %v91520 = vor.u32 %v91518, %v91519 (stack75)
        %v91521 = vxor.u32 %v91512, %v91520 (stack76)
        %v91524 = vadd.s32 %v91521, %v8 (stack65)
        %v91528 = vadd.s32 %v91524, 4 (stack65)
        %v91532 = vadd.s32 %v91516, %v91528 (stack65)
        %v91534 = vshll.u32 %v91528, 13 (stack73)
        %v91535 = vshrl.u32 %v91528, 19 (stack74)
        %v91536 = vor.u32 %v91534, %v91535 (stack75)
        %v91537 = vxor.u32 %v91532, %v91536 (stack76)
        %v91540 = vadd.s32 %v91532, %v91537 (stack65)
        %v91542 = vshll.u32 %v91537, 15 (stack73)
        %v91543 = vshrl.u32 %v91537, 17 (stack74)
        %v91544 = vor.u32 %v91542, %v91543 (stack75)
        %v91545 = vxor.u32 %v91540, %v91544 (stack76)
        %v91548 = vadd.s32 %v91540, %v91545 (stack65)
        %v91550 = vshll.u32 %v91545, 26 (stack73)
        %v91551 = vshrl.u32 %v91545, 6 (stack74)
        %v91552 = vor.u32 %v91550, %v91551 (stack75)
        %v91553 = vxor.u32 %v91548, %v91552 (stack76)
        %v91556 = vadd.s32 %v91548, %v91553 (stack65)
        %v91560 = vadd.s32 %v91556, %v8 (stack65)
        %v91562 = vshll.u32 %v91553, 6 (stack73)
        %v91563 = vshrl.u32 %v91553, 26 (stack74)
        %v91564 = vor.u32 %v91562, %v91563 (stack75)
        %v91565 = vxor.u32 %v91556, %v91564 (stack76)
        %v91568 = vadd.s32 %v91565, %v10 (stack65)
        %v91572 = vadd.s32 %v91568, 5 (stack65)
        %v91574 = vxor.u32 %v91560, %v91572 (stack76)
        %v91575 = vand.u32.u8 %v91574, 255 (stack77)
        %v91576 = vand.u32 %v91575, 65535 (stack78)
        %v91577 = vshrl.u32 %v91576, 1 (stack79)
        %v91578 = vor.u32 %v91577, 16256 (stack75)
        %v91579 = vand.u32.u16 %v91578, 65535 (stack80)
        %v91580 = vunpack.i.l.bf16 %v91579 (stack81)
        %v91584 = vadd.f32 %v91580, -1.0 (stack82)
        %v91588 = vmul.f32 %v91584, 2.0 (stack83)
        %v91592 = vadd.f32 %v91588, -0.99609375 (stack82)
        %v91596 = vmax.f32 -0.99609375, %v91592 (stack84)
        %v91598 = vand.u32 2147483647, %v91596 (stack85)
        %vm91601 = vcmp.eq.f32.partialorder %v91598, 1.0 (stack86)
        %v91606 = vmul.f32 %v91596, inf (stack83)
        %v91608 = vxor.u32 %v91596, 2147483648 (stack87)
        %v91611 = vmul.f32 %v91596, %v91608 (stack83)
        %v91613 = vadd.f32 %v91611, 1.0 (stack88)
        %v91614 = vlog2.pop %v91613 (stack89)
        %v91615 = vmul.f32 %v91614, 0.6931472 (stack90)
        %v91616 = vmul.f32 -0.5, %v91611 (stack91)
        %v91617 = vadd.f32 %v91616, 1.0 (stack92)
        %v91618 = vmul.f32 %v91617, %v91611 (stack93)
        %v91619 = vand.u32 2147483647, %v91611 (stack94)
        %vm91620 = vcmp.lt.f32.partialorder %v91619, 0.0004427343 (stack95)
        %v91621 = vsel /*vm=*/%vm91620, /*on_true_vy=*/%v91618, /*on_false_vx=*/%v91615 (stack96)
        %v91622 = vxor.u32 %v91621, 2147483648 (stack87)
        %vm91625 = vcmp.lt.f32.partialorder %v91622, 5.0 (stack86)
        %v91630 = vsel /*vm=*/%vm91625, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v91634 = vsel /*vm=*/%vm91625, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v91638 = vsel /*vm=*/%vm91625, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v91642 = vsel /*vm=*/%vm91625, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v91646 = vsel /*vm=*/%vm91625, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v91650 = vsel /*vm=*/%vm91625, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v91654 = vsel /*vm=*/%vm91625, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v91658 = vsel /*vm=*/%vm91625, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v91662 = vsel /*vm=*/%vm91625, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v91666 = vadd.f32 %v91622, -2.5 (stack82)
        %v91668 = vrsqrt.pop %v91622 (stack97)
        %v91669 = vmul.f32 %v91622, %v91668 (stack98)
        %vm91670 = vcmp.eq.f32.partialorder %v91622, inf (stack99)
        %v91671 = vsel /*vm=*/%vm91670, /*on_true_vy=*/%v91622, /*on_false_vx=*/%v91669 (stack100)
        %vm91672 = vcmp.eq.f32.partialorder %v91622, 0.0 (stack101)
        %v91673 = vand.u32 %v91622, 2147483648 (stack102)
        %v91674 = vsel /*vm=*/%vm91672, /*on_true_vy=*/%v91673, /*on_false_vx=*/%v91671 (stack103)
        %v91677 = vadd.f32 %v91674, -3.0 (stack82)
        %v91681 = vsel /*vm=*/%vm91625, /*on_true_vy=*/%v91666, /*on_false_vx=*/%v91677 (stack72)
        %v91685 = vmul.f32 %v91662, %v91681 (stack83)
        %v91689 = vadd.f32 %v91658, %v91685 (stack82)
        %v91693 = vmul.f32 %v91689, %v91681 (stack83)
        %v91697 = vadd.f32 %v91654, %v91693 (stack82)
        %v91701 = vmul.f32 %v91697, %v91681 (stack83)
        %v91705 = vadd.f32 %v91650, %v91701 (stack82)
        %v91709 = vmul.f32 %v91705, %v91681 (stack83)
        %v91713 = vadd.f32 %v91646, %v91709 (stack82)
        %v91717 = vmul.f32 %v91713, %v91681 (stack83)
        %v91721 = vadd.f32 %v91642, %v91717 (stack82)
        %v91725 = vmul.f32 %v91721, %v91681 (stack83)
        %v91729 = vadd.f32 %v91638, %v91725 (stack82)
        %v91733 = vmul.f32 %v91729, %v91681 (stack83)
        %v91737 = vadd.f32 %v91634, %v91733 (stack82)
        %v91741 = vmul.f32 %v91737, %v91681 (stack83)
        %v91745 = vadd.f32 %v91630, %v91741 (stack82)
        %v91749 = vmul.f32 %v91745, %v91596 (stack83)
        %v91753 = vsel /*vm=*/%vm91601, /*on_true_vy=*/%v91606, /*on_false_vx=*/%v91749 (stack72)
        %v91757 = vmul.f32 %v91753, 1.4140625 (stack83)
        %s91759 = scalar_lea.vmem %s280, 480 [#allocation0] (stack107)
        %v91760 = vpack.c.bf16 0.0, %v91757 (stack104)
        %91761 = vst [vmem:[%s91759] sm:$0xf] /*vst_source=*/%v91760 (stack105)
        %v91764 = vadd.s32 %v2355, %v89917 (stack65)
        %s91766 = smul.u32 128, %s27 (stack66)
        %v91767 = vlaneseq (stack67)
        %v91768 = vand.u32 %v91767, 127 (stack68)
        %v91769 = vstv %s91766 (stack69)
        %v91770 = vadd.s32 %v91768, %v91769 (stack70)
        %v91774 = vadd.s32 %v91764, %v91770 (stack65)
        %vm91778 = vcmp.lt.u32.totalorder %v91774, %v91764 (stack71)
        %vm91783 = vcmp.lt.u32.totalorder %v91764, %v2355 (stack71)
        %v91788 = vadd.s32 %v2342, %v89900 (stack65)
        %v91792 = vadd.s32 %v91788, 1 (stack65)
        %v91796 = vsel /*vm=*/%vm91783, /*on_true_vy=*/%v91792, /*on_false_vx=*/%v91788 (stack72)
        %v91800 = vadd.s32 %v91796, 1 (stack65)
        %v91804 = vsel /*vm=*/%vm91778, /*on_true_vy=*/%v91800, /*on_false_vx=*/%v91796 (stack72)
        %v91809 = vadd.s32 %v91804, %v10 (stack65)
        %v91813 = vadd.s32 %v91774, %v9 (stack65)
        %v91817 = vadd.s32 %v91809, %v91813 (stack65)
        %v91819 = vshll.u32 %v91813, 13 (stack73)
        %v91820 = vshrl.u32 %v91813, 19 (stack74)
        %v91821 = vor.u32 %v91819, %v91820 (stack75)
        %v91822 = vxor.u32 %v91817, %v91821 (stack76)
        %v91825 = vadd.s32 %v91817, %v91822 (stack65)
        %v91827 = vshll.u32 %v91822, 15 (stack73)
        %v91828 = vshrl.u32 %v91822, 17 (stack74)
        %v91829 = vor.u32 %v91827, %v91828 (stack75)
        %v91830 = vxor.u32 %v91825, %v91829 (stack76)
        %v91833 = vadd.s32 %v91825, %v91830 (stack65)
        %v91835 = vshll.u32 %v91830, 26 (stack73)
        %v91836 = vshrl.u32 %v91830, 6 (stack74)
        %v91837 = vor.u32 %v91835, %v91836 (stack75)
        %v91838 = vxor.u32 %v91833, %v91837 (stack76)
        %v91841 = vadd.s32 %v91833, %v91838 (stack65)
        %v91845 = vadd.s32 %v91841, %v9 (stack65)
        %v91847 = vshll.u32 %v91838, 6 (stack73)
        %v91848 = vshrl.u32 %v91838, 26 (stack74)
        %v91849 = vor.u32 %v91847, %v91848 (stack75)
        %v91850 = vxor.u32 %v91841, %v91849 (stack76)
        %v91853 = vadd.s32 %v91850, %v8 (stack65)
        %v91857 = vadd.s32 %v91853, 1 (stack65)
        %v91861 = vadd.s32 %v91845, %v91857 (stack65)
        %v91863 = vshll.u32 %v91857, 17 (stack73)
        %v91864 = vshrl.u32 %v91857, 15 (stack74)
        %v91865 = vor.u32 %v91863, %v91864 (stack75)
        %v91866 = vxor.u32 %v91861, %v91865 (stack76)
        %v91869 = vadd.s32 %v91861, %v91866 (stack65)
        %v91871 = vshll.u32 %v91866, 29 (stack73)
        %v91872 = vshrl.u32 %v91866, 3 (stack74)
        %v91873 = vor.u32 %v91871, %v91872 (stack75)
        %v91874 = vxor.u32 %v91869, %v91873 (stack76)
        %v91877 = vadd.s32 %v91869, %v91874 (stack65)
        %v91879 = vshll.u32 %v91874, 16 (stack73)
        %v91880 = vshrl.u32 %v91874, 16 (stack74)
        %v91881 = vor.u32 %v91879, %v91880 (stack75)
        %v91882 = vxor.u32 %v91877, %v91881 (stack76)
        %v91885 = vadd.s32 %v91877, %v91882 (stack65)
        %v91889 = vadd.s32 %v91885, %v8 (stack65)
        %v91891 = vshll.u32 %v91882, 24 (stack73)
        %v91892 = vshrl.u32 %v91882, 8 (stack74)
        %v91893 = vor.u32 %v91891, %v91892 (stack75)
        %v91894 = vxor.u32 %v91885, %v91893 (stack76)
        %v91897 = vadd.s32 %v91894, %v10 (stack65)
        %v91901 = vadd.s32 %v91897, 2 (stack65)
        %v91905 = vadd.s32 %v91889, %v91901 (stack65)
        %v91907 = vshll.u32 %v91901, 13 (stack73)
        %v91908 = vshrl.u32 %v91901, 19 (stack74)
        %v91909 = vor.u32 %v91907, %v91908 (stack75)
        %v91910 = vxor.u32 %v91905, %v91909 (stack76)
        %v91913 = vadd.s32 %v91905, %v91910 (stack65)
        %v91915 = vshll.u32 %v91910, 15 (stack73)
        %v91916 = vshrl.u32 %v91910, 17 (stack74)
        %v91917 = vor.u32 %v91915, %v91916 (stack75)
        %v91918 = vxor.u32 %v91913, %v91917 (stack76)
        %v91921 = vadd.s32 %v91913, %v91918 (stack65)
        %v91923 = vshll.u32 %v91918, 26 (stack73)
        %v91924 = vshrl.u32 %v91918, 6 (stack74)
        %v91925 = vor.u32 %v91923, %v91924 (stack75)
        %v91926 = vxor.u32 %v91921, %v91925 (stack76)
        %v91929 = vadd.s32 %v91921, %v91926 (stack65)
        %v91933 = vadd.s32 %v91929, %v10 (stack65)
        %v91935 = vshll.u32 %v91926, 6 (stack73)
        %v91936 = vshrl.u32 %v91926, 26 (stack74)
        %v91937 = vor.u32 %v91935, %v91936 (stack75)
        %v91938 = vxor.u32 %v91929, %v91937 (stack76)
        %v91941 = vadd.s32 %v91938, %v9 (stack65)
        %v91945 = vadd.s32 %v91941, 3 (stack65)
        %v91949 = vadd.s32 %v91933, %v91945 (stack65)
        %v91951 = vshll.u32 %v91945, 17 (stack73)
        %v91952 = vshrl.u32 %v91945, 15 (stack74)
        %v91953 = vor.u32 %v91951, %v91952 (stack75)
        %v91954 = vxor.u32 %v91949, %v91953 (stack76)
        %v91957 = vadd.s32 %v91949, %v91954 (stack65)
        %v91959 = vshll.u32 %v91954, 29 (stack73)
        %v91960 = vshrl.u32 %v91954, 3 (stack74)
        %v91961 = vor.u32 %v91959, %v91960 (stack75)
        %v91962 = vxor.u32 %v91957, %v91961 (stack76)
        %v91965 = vadd.s32 %v91957, %v91962 (stack65)
        %v91967 = vshll.u32 %v91962, 16 (stack73)
        %v91968 = vshrl.u32 %v91962, 16 (stack74)
        %v91969 = vor.u32 %v91967, %v91968 (stack75)
        %v91970 = vxor.u32 %v91965, %v91969 (stack76)
        %v91973 = vadd.s32 %v91965, %v91970 (stack65)
        %v91977 = vadd.s32 %v91973, %v9 (stack65)
        %v91979 = vshll.u32 %v91970, 24 (stack73)
        %v91980 = vshrl.u32 %v91970, 8 (stack74)
        %v91981 = vor.u32 %v91979, %v91980 (stack75)
        %v91982 = vxor.u32 %v91973, %v91981 (stack76)
        %v91985 = vadd.s32 %v91982, %v8 (stack65)
        %v91989 = vadd.s32 %v91985, 4 (stack65)
        %v91993 = vadd.s32 %v91977, %v91989 (stack65)
        %v91995 = vshll.u32 %v91989, 13 (stack73)
        %v91996 = vshrl.u32 %v91989, 19 (stack74)
        %v91997 = vor.u32 %v91995, %v91996 (stack75)
        %v91998 = vxor.u32 %v91993, %v91997 (stack76)
        %v92001 = vadd.s32 %v91993, %v91998 (stack65)
        %v92003 = vshll.u32 %v91998, 15 (stack73)
        %v92004 = vshrl.u32 %v91998, 17 (stack74)
        %v92005 = vor.u32 %v92003, %v92004 (stack75)
        %v92006 = vxor.u32 %v92001, %v92005 (stack76)
        %v92009 = vadd.s32 %v92001, %v92006 (stack65)
        %v92011 = vshll.u32 %v92006, 26 (stack73)
        %v92012 = vshrl.u32 %v92006, 6 (stack74)
        %v92013 = vor.u32 %v92011, %v92012 (stack75)
        %v92014 = vxor.u32 %v92009, %v92013 (stack76)
        %v92017 = vadd.s32 %v92009, %v92014 (stack65)
        %v92021 = vadd.s32 %v92017, %v8 (stack65)
        %v92023 = vshll.u32 %v92014, 6 (stack73)
        %v92024 = vshrl.u32 %v92014, 26 (stack74)
        %v92025 = vor.u32 %v92023, %v92024 (stack75)
        %v92026 = vxor.u32 %v92017, %v92025 (stack76)
        %v92029 = vadd.s32 %v92026, %v10 (stack65)
        %v92033 = vadd.s32 %v92029, 5 (stack65)
        %v92035 = vxor.u32 %v92021, %v92033 (stack76)
        %v92036 = vand.u32.u8 %v92035, 255 (stack77)
        %v92037 = vand.u32 %v92036, 65535 (stack78)
        %v92038 = vshrl.u32 %v92037, 1 (stack79)
        %v92039 = vor.u32 %v92038, 16256 (stack75)
        %v92040 = vand.u32.u16 %v92039, 65535 (stack80)
        %v92041 = vunpack.i.l.bf16 %v92040 (stack81)
        %v92045 = vadd.f32 %v92041, -1.0 (stack82)
        %v92049 = vmul.f32 %v92045, 2.0 (stack83)
        %v92053 = vadd.f32 %v92049, -0.99609375 (stack82)
        %v92057 = vmax.f32 -0.99609375, %v92053 (stack84)
        %v92059 = vand.u32 2147483647, %v92057 (stack85)
        %vm92062 = vcmp.eq.f32.partialorder %v92059, 1.0 (stack86)
        %v92067 = vmul.f32 %v92057, inf (stack83)
        %v92069 = vxor.u32 %v92057, 2147483648 (stack87)
        %v92072 = vmul.f32 %v92057, %v92069 (stack83)
        %v92074 = vadd.f32 %v92072, 1.0 (stack88)
        %v92075 = vlog2.pop %v92074 (stack89)
        %v92076 = vmul.f32 %v92075, 0.6931472 (stack90)
        %v92077 = vmul.f32 -0.5, %v92072 (stack91)
        %v92078 = vadd.f32 %v92077, 1.0 (stack92)
        %v92079 = vmul.f32 %v92078, %v92072 (stack93)
        %v92080 = vand.u32 2147483647, %v92072 (stack94)
        %vm92081 = vcmp.lt.f32.partialorder %v92080, 0.0004427343 (stack95)
        %v92082 = vsel /*vm=*/%vm92081, /*on_true_vy=*/%v92079, /*on_false_vx=*/%v92076 (stack96)
        %v92083 = vxor.u32 %v92082, 2147483648 (stack87)
        %vm92086 = vcmp.lt.f32.partialorder %v92083, 5.0 (stack86)
        %v92091 = vsel /*vm=*/%vm92086, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v92095 = vsel /*vm=*/%vm92086, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v92099 = vsel /*vm=*/%vm92086, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v92103 = vsel /*vm=*/%vm92086, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v92107 = vsel /*vm=*/%vm92086, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v92111 = vsel /*vm=*/%vm92086, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v92115 = vsel /*vm=*/%vm92086, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v92119 = vsel /*vm=*/%vm92086, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v92123 = vsel /*vm=*/%vm92086, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v92127 = vadd.f32 %v92083, -2.5 (stack82)
        %v92129 = vrsqrt.pop %v92083 (stack97)
        %v92130 = vmul.f32 %v92083, %v92129 (stack98)
        %vm92131 = vcmp.eq.f32.partialorder %v92083, inf (stack99)
        %v92132 = vsel /*vm=*/%vm92131, /*on_true_vy=*/%v92083, /*on_false_vx=*/%v92130 (stack100)
        %vm92133 = vcmp.eq.f32.partialorder %v92083, 0.0 (stack101)
        %v92134 = vand.u32 %v92083, 2147483648 (stack102)
        %v92135 = vsel /*vm=*/%vm92133, /*on_true_vy=*/%v92134, /*on_false_vx=*/%v92132 (stack103)
        %v92138 = vadd.f32 %v92135, -3.0 (stack82)
        %v92142 = vsel /*vm=*/%vm92086, /*on_true_vy=*/%v92127, /*on_false_vx=*/%v92138 (stack72)
        %v92146 = vmul.f32 %v92123, %v92142 (stack83)
        %v92150 = vadd.f32 %v92119, %v92146 (stack82)
        %v92154 = vmul.f32 %v92150, %v92142 (stack83)
        %v92158 = vadd.f32 %v92115, %v92154 (stack82)
        %v92162 = vmul.f32 %v92158, %v92142 (stack83)
        %v92166 = vadd.f32 %v92111, %v92162 (stack82)
        %v92170 = vmul.f32 %v92166, %v92142 (stack83)
        %v92174 = vadd.f32 %v92107, %v92170 (stack82)
        %v92178 = vmul.f32 %v92174, %v92142 (stack83)
        %v92182 = vadd.f32 %v92103, %v92178 (stack82)
        %v92186 = vmul.f32 %v92182, %v92142 (stack83)
        %v92190 = vadd.f32 %v92099, %v92186 (stack82)
        %v92194 = vmul.f32 %v92190, %v92142 (stack83)
        %v92198 = vadd.f32 %v92095, %v92194 (stack82)
        %v92202 = vmul.f32 %v92198, %v92142 (stack83)
        %v92206 = vadd.f32 %v92091, %v92202 (stack82)
        %v92210 = vmul.f32 %v92206, %v92057 (stack83)
        %v92214 = vsel /*vm=*/%vm92062, /*on_true_vy=*/%v92067, /*on_false_vx=*/%v92210 (stack72)
        %v92218 = vmul.f32 %v92214, 1.4140625 (stack83)
        %s92220 = scalar_lea.vmem %s280, 608 [#allocation0] (stack107)
        %v92221 = vpack.c.bf16 0.0, %v92218 (stack104)
        %92222 = vst [vmem:[%s92220] sm:$0xf] /*vst_source=*/%v92221 (stack105)
        %v92225 = vadd.s32 %v2842, %v89917 (stack65)
        %s92227 = smul.u32 128, %s27 (stack66)
        %v92228 = vlaneseq (stack67)
        %v92229 = vand.u32 %v92228, 127 (stack68)
        %v92230 = vstv %s92227 (stack69)
        %v92231 = vadd.s32 %v92229, %v92230 (stack70)
        %v92235 = vadd.s32 %v92225, %v92231 (stack65)
        %vm92239 = vcmp.lt.u32.totalorder %v92235, %v92225 (stack71)
        %vm92244 = vcmp.lt.u32.totalorder %v92225, %v2842 (stack71)
        %v92249 = vadd.s32 %v2829, %v89900 (stack65)
        %v92253 = vadd.s32 %v92249, 1 (stack65)
        %v92257 = vsel /*vm=*/%vm92244, /*on_true_vy=*/%v92253, /*on_false_vx=*/%v92249 (stack72)
        %v92261 = vadd.s32 %v92257, 1 (stack65)
        %v92265 = vsel /*vm=*/%vm92239, /*on_true_vy=*/%v92261, /*on_false_vx=*/%v92257 (stack72)
        %v92270 = vadd.s32 %v92265, %v10 (stack65)
        %v92274 = vadd.s32 %v92235, %v9 (stack65)
        %v92278 = vadd.s32 %v92270, %v92274 (stack65)
        %v92280 = vshll.u32 %v92274, 13 (stack73)
        %v92281 = vshrl.u32 %v92274, 19 (stack74)
        %v92282 = vor.u32 %v92280, %v92281 (stack75)
        %v92283 = vxor.u32 %v92278, %v92282 (stack76)
        %v92286 = vadd.s32 %v92278, %v92283 (stack65)
        %v92288 = vshll.u32 %v92283, 15 (stack73)
        %v92289 = vshrl.u32 %v92283, 17 (stack74)
        %v92290 = vor.u32 %v92288, %v92289 (stack75)
        %v92291 = vxor.u32 %v92286, %v92290 (stack76)
        %v92294 = vadd.s32 %v92286, %v92291 (stack65)
        %v92296 = vshll.u32 %v92291, 26 (stack73)
        %v92297 = vshrl.u32 %v92291, 6 (stack74)
        %v92298 = vor.u32 %v92296, %v92297 (stack75)
        %v92299 = vxor.u32 %v92294, %v92298 (stack76)
        %v92302 = vadd.s32 %v92294, %v92299 (stack65)
        %v92306 = vadd.s32 %v92302, %v9 (stack65)
        %v92308 = vshll.u32 %v92299, 6 (stack73)
        %v92309 = vshrl.u32 %v92299, 26 (stack74)
        %v92310 = vor.u32 %v92308, %v92309 (stack75)
        %v92311 = vxor.u32 %v92302, %v92310 (stack76)
        %v92314 = vadd.s32 %v92311, %v8 (stack65)
        %v92318 = vadd.s32 %v92314, 1 (stack65)
        %v92322 = vadd.s32 %v92306, %v92318 (stack65)
        %v92324 = vshll.u32 %v92318, 17 (stack73)
        %v92325 = vshrl.u32 %v92318, 15 (stack74)
        %v92326 = vor.u32 %v92324, %v92325 (stack75)
        %v92327 = vxor.u32 %v92322, %v92326 (stack76)
        %v92330 = vadd.s32 %v92322, %v92327 (stack65)
        %v92332 = vshll.u32 %v92327, 29 (stack73)
        %v92333 = vshrl.u32 %v92327, 3 (stack74)
        %v92334 = vor.u32 %v92332, %v92333 (stack75)
        %v92335 = vxor.u32 %v92330, %v92334 (stack76)
        %v92338 = vadd.s32 %v92330, %v92335 (stack65)
        %v92340 = vshll.u32 %v92335, 16 (stack73)
        %v92341 = vshrl.u32 %v92335, 16 (stack74)
        %v92342 = vor.u32 %v92340, %v92341 (stack75)
        %v92343 = vxor.u32 %v92338, %v92342 (stack76)
        %v92346 = vadd.s32 %v92338, %v92343 (stack65)
        %v92350 = vadd.s32 %v92346, %v8 (stack65)
        %v92352 = vshll.u32 %v92343, 24 (stack73)
        %v92353 = vshrl.u32 %v92343, 8 (stack74)
        %v92354 = vor.u32 %v92352, %v92353 (stack75)
        %v92355 = vxor.u32 %v92346, %v92354 (stack76)
        %v92358 = vadd.s32 %v92355, %v10 (stack65)
        %v92362 = vadd.s32 %v92358, 2 (stack65)
        %v92366 = vadd.s32 %v92350, %v92362 (stack65)
        %v92368 = vshll.u32 %v92362, 13 (stack73)
        %v92369 = vshrl.u32 %v92362, 19 (stack74)
        %v92370 = vor.u32 %v92368, %v92369 (stack75)
        %v92371 = vxor.u32 %v92366, %v92370 (stack76)
        %v92374 = vadd.s32 %v92366, %v92371 (stack65)
        %v92376 = vshll.u32 %v92371, 15 (stack73)
        %v92377 = vshrl.u32 %v92371, 17 (stack74)
        %v92378 = vor.u32 %v92376, %v92377 (stack75)
        %v92379 = vxor.u32 %v92374, %v92378 (stack76)
        %v92382 = vadd.s32 %v92374, %v92379 (stack65)
        %v92384 = vshll.u32 %v92379, 26 (stack73)
        %v92385 = vshrl.u32 %v92379, 6 (stack74)
        %v92386 = vor.u32 %v92384, %v92385 (stack75)
        %v92387 = vxor.u32 %v92382, %v92386 (stack76)
        %v92390 = vadd.s32 %v92382, %v92387 (stack65)
        %v92394 = vadd.s32 %v92390, %v10 (stack65)
        %v92396 = vshll.u32 %v92387, 6 (stack73)
        %v92397 = vshrl.u32 %v92387, 26 (stack74)
        %v92398 = vor.u32 %v92396, %v92397 (stack75)
        %v92399 = vxor.u32 %v92390, %v92398 (stack76)
        %v92402 = vadd.s32 %v92399, %v9 (stack65)
        %v92406 = vadd.s32 %v92402, 3 (stack65)
        %v92410 = vadd.s32 %v92394, %v92406 (stack65)
        %v92412 = vshll.u32 %v92406, 17 (stack73)
        %v92413 = vshrl.u32 %v92406, 15 (stack74)
        %v92414 = vor.u32 %v92412, %v92413 (stack75)
        %v92415 = vxor.u32 %v92410, %v92414 (stack76)
        %v92418 = vadd.s32 %v92410, %v92415 (stack65)
        %v92420 = vshll.u32 %v92415, 29 (stack73)
        %v92421 = vshrl.u32 %v92415, 3 (stack74)
        %v92422 = vor.u32 %v92420, %v92421 (stack75)
        %v92423 = vxor.u32 %v92418, %v92422 (stack76)
        %v92426 = vadd.s32 %v92418, %v92423 (stack65)
        %v92428 = vshll.u32 %v92423, 16 (stack73)
        %v92429 = vshrl.u32 %v92423, 16 (stack74)
        %v92430 = vor.u32 %v92428, %v92429 (stack75)
        %v92431 = vxor.u32 %v92426, %v92430 (stack76)
        %v92434 = vadd.s32 %v92426, %v92431 (stack65)
        %v92438 = vadd.s32 %v92434, %v9 (stack65)
        %v92440 = vshll.u32 %v92431, 24 (stack73)
        %v92441 = vshrl.u32 %v92431, 8 (stack74)
        %v92442 = vor.u32 %v92440, %v92441 (stack75)
        %v92443 = vxor.u32 %v92434, %v92442 (stack76)
        %v92446 = vadd.s32 %v92443, %v8 (stack65)
        %v92450 = vadd.s32 %v92446, 4 (stack65)
        %v92454 = vadd.s32 %v92438, %v92450 (stack65)
        %v92456 = vshll.u32 %v92450, 13 (stack73)
        %v92457 = vshrl.u32 %v92450, 19 (stack74)
        %v92458 = vor.u32 %v92456, %v92457 (stack75)
        %v92459 = vxor.u32 %v92454, %v92458 (stack76)
        %v92462 = vadd.s32 %v92454, %v92459 (stack65)
        %v92464 = vshll.u32 %v92459, 15 (stack73)
        %v92465 = vshrl.u32 %v92459, 17 (stack74)
        %v92466 = vor.u32 %v92464, %v92465 (stack75)
        %v92467 = vxor.u32 %v92462, %v92466 (stack76)
        %v92470 = vadd.s32 %v92462, %v92467 (stack65)
        %v92472 = vshll.u32 %v92467, 26 (stack73)
        %v92473 = vshrl.u32 %v92467, 6 (stack74)
        %v92474 = vor.u32 %v92472, %v92473 (stack75)
        %v92475 = vxor.u32 %v92470, %v92474 (stack76)
        %v92478 = vadd.s32 %v92470, %v92475 (stack65)
        %v92482 = vadd.s32 %v92478, %v8 (stack65)
        %v92484 = vshll.u32 %v92475, 6 (stack73)
        %v92485 = vshrl.u32 %v92475, 26 (stack74)
        %v92486 = vor.u32 %v92484, %v92485 (stack75)
        %v92487 = vxor.u32 %v92478, %v92486 (stack76)
        %v92490 = vadd.s32 %v92487, %v10 (stack65)
        %v92494 = vadd.s32 %v92490, 5 (stack65)
        %v92496 = vxor.u32 %v92482, %v92494 (stack76)
        %v92497 = vand.u32.u8 %v92496, 255 (stack77)
        %v92498 = vand.u32 %v92497, 65535 (stack78)
        %v92499 = vshrl.u32 %v92498, 1 (stack79)
        %v92500 = vor.u32 %v92499, 16256 (stack75)
        %v92501 = vand.u32.u16 %v92500, 65535 (stack80)
        %v92502 = vunpack.i.l.bf16 %v92501 (stack81)
        %v92506 = vadd.f32 %v92502, -1.0 (stack82)
        %v92510 = vmul.f32 %v92506, 2.0 (stack83)
        %v92514 = vadd.f32 %v92510, -0.99609375 (stack82)
        %v92518 = vmax.f32 -0.99609375, %v92514 (stack84)
        %v92520 = vand.u32 2147483647, %v92518 (stack85)
        %vm92523 = vcmp.eq.f32.partialorder %v92520, 1.0 (stack86)
        %v92528 = vmul.f32 %v92518, inf (stack83)
        %v92530 = vxor.u32 %v92518, 2147483648 (stack87)
        %v92533 = vmul.f32 %v92518, %v92530 (stack83)
        %v92535 = vadd.f32 %v92533, 1.0 (stack88)
        %v92536 = vlog2.pop %v92535 (stack89)
        %v92537 = vmul.f32 %v92536, 0.6931472 (stack90)
        %v92538 = vmul.f32 -0.5, %v92533 (stack91)
        %v92539 = vadd.f32 %v92538, 1.0 (stack92)
        %v92540 = vmul.f32 %v92539, %v92533 (stack93)
        %v92541 = vand.u32 2147483647, %v92533 (stack94)
        %vm92542 = vcmp.lt.f32.partialorder %v92541, 0.0004427343 (stack95)
        %v92543 = vsel /*vm=*/%vm92542, /*on_true_vy=*/%v92540, /*on_false_vx=*/%v92537 (stack96)
        %v92544 = vxor.u32 %v92543, 2147483648 (stack87)
        %vm92547 = vcmp.lt.f32.partialorder %v92544, 5.0 (stack86)
        %v92552 = vsel /*vm=*/%vm92547, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v92556 = vsel /*vm=*/%vm92547, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v92560 = vsel /*vm=*/%vm92547, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v92564 = vsel /*vm=*/%vm92547, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v92568 = vsel /*vm=*/%vm92547, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v92572 = vsel /*vm=*/%vm92547, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v92576 = vsel /*vm=*/%vm92547, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v92580 = vsel /*vm=*/%vm92547, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v92584 = vsel /*vm=*/%vm92547, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v92588 = vadd.f32 %v92544, -2.5 (stack82)
        %v92590 = vrsqrt.pop %v92544 (stack97)
        %v92591 = vmul.f32 %v92544, %v92590 (stack98)
        %vm92592 = vcmp.eq.f32.partialorder %v92544, inf (stack99)
        %v92593 = vsel /*vm=*/%vm92592, /*on_true_vy=*/%v92544, /*on_false_vx=*/%v92591 (stack100)
        %vm92594 = vcmp.eq.f32.partialorder %v92544, 0.0 (stack101)
        %v92595 = vand.u32 %v92544, 2147483648 (stack102)
        %v92596 = vsel /*vm=*/%vm92594, /*on_true_vy=*/%v92595, /*on_false_vx=*/%v92593 (stack103)
        %v92599 = vadd.f32 %v92596, -3.0 (stack82)
        %v92603 = vsel /*vm=*/%vm92547, /*on_true_vy=*/%v92588, /*on_false_vx=*/%v92599 (stack72)
        %v92607 = vmul.f32 %v92584, %v92603 (stack83)
        %v92611 = vadd.f32 %v92580, %v92607 (stack82)
        %v92615 = vmul.f32 %v92611, %v92603 (stack83)
        %v92619 = vadd.f32 %v92576, %v92615 (stack82)
        %v92623 = vmul.f32 %v92619, %v92603 (stack83)
        %v92627 = vadd.f32 %v92572, %v92623 (stack82)
        %v92631 = vmul.f32 %v92627, %v92603 (stack83)
        %v92635 = vadd.f32 %v92568, %v92631 (stack82)
        %v92639 = vmul.f32 %v92635, %v92603 (stack83)
        %v92643 = vadd.f32 %v92564, %v92639 (stack82)
        %v92647 = vmul.f32 %v92643, %v92603 (stack83)
        %v92651 = vadd.f32 %v92560, %v92647 (stack82)
        %v92655 = vmul.f32 %v92651, %v92603 (stack83)
        %v92659 = vadd.f32 %v92556, %v92655 (stack82)
        %v92663 = vmul.f32 %v92659, %v92603 (stack83)
        %v92667 = vadd.f32 %v92552, %v92663 (stack82)
        %v92671 = vmul.f32 %v92667, %v92518 (stack83)
        %v92675 = vsel /*vm=*/%vm92523, /*on_true_vy=*/%v92528, /*on_false_vx=*/%v92671 (stack72)
        %v92679 = vmul.f32 %v92675, 1.4140625 (stack83)
        %s92681 = scalar_lea.vmem %s280, 736 [#allocation0] (stack107)
        %v92682 = vpack.c.bf16 0.0, %v92679 (stack104)
        %92683 = vst [vmem:[%s92681] sm:$0xf] /*vst_source=*/%v92682 (stack105)
        %v92686 = vadd.s32 %v3329, %v89917 (stack65)
        %s92688 = smul.u32 128, %s27 (stack66)
        %v92689 = vlaneseq (stack67)
        %v92690 = vand.u32 %v92689, 127 (stack68)
        %v92691 = vstv %s92688 (stack69)
        %v92692 = vadd.s32 %v92690, %v92691 (stack70)
        %v92696 = vadd.s32 %v92686, %v92692 (stack65)
        %vm92700 = vcmp.lt.u32.totalorder %v92696, %v92686 (stack71)
        %vm92705 = vcmp.lt.u32.totalorder %v92686, %v3329 (stack71)
        %v92710 = vadd.s32 %v3316, %v89900 (stack65)
        %v92714 = vadd.s32 %v92710, 1 (stack65)
        %v92718 = vsel /*vm=*/%vm92705, /*on_true_vy=*/%v92714, /*on_false_vx=*/%v92710 (stack72)
        %v92722 = vadd.s32 %v92718, 1 (stack65)
        %v92726 = vsel /*vm=*/%vm92700, /*on_true_vy=*/%v92722, /*on_false_vx=*/%v92718 (stack72)
        %v92731 = vadd.s32 %v92726, %v10 (stack65)
        %v92735 = vadd.s32 %v92696, %v9 (stack65)
        %v92739 = vadd.s32 %v92731, %v92735 (stack65)
        %v92741 = vshll.u32 %v92735, 13 (stack73)
        %v92742 = vshrl.u32 %v92735, 19 (stack74)
        %v92743 = vor.u32 %v92741, %v92742 (stack75)
        %v92744 = vxor.u32 %v92739, %v92743 (stack76)
        %v92747 = vadd.s32 %v92739, %v92744 (stack65)
        %v92749 = vshll.u32 %v92744, 15 (stack73)
        %v92750 = vshrl.u32 %v92744, 17 (stack74)
        %v92751 = vor.u32 %v92749, %v92750 (stack75)
        %v92752 = vxor.u32 %v92747, %v92751 (stack76)
        %v92755 = vadd.s32 %v92747, %v92752 (stack65)
        %v92757 = vshll.u32 %v92752, 26 (stack73)
        %v92758 = vshrl.u32 %v92752, 6 (stack74)
        %v92759 = vor.u32 %v92757, %v92758 (stack75)
        %v92760 = vxor.u32 %v92755, %v92759 (stack76)
        %v92763 = vadd.s32 %v92755, %v92760 (stack65)
        %v92767 = vadd.s32 %v92763, %v9 (stack65)
        %v92769 = vshll.u32 %v92760, 6 (stack73)
        %v92770 = vshrl.u32 %v92760, 26 (stack74)
        %v92771 = vor.u32 %v92769, %v92770 (stack75)
        %v92772 = vxor.u32 %v92763, %v92771 (stack76)
        %v92775 = vadd.s32 %v92772, %v8 (stack65)
        %v92779 = vadd.s32 %v92775, 1 (stack65)
        %v92783 = vadd.s32 %v92767, %v92779 (stack65)
        %v92785 = vshll.u32 %v92779, 17 (stack73)
        %v92786 = vshrl.u32 %v92779, 15 (stack74)
        %v92787 = vor.u32 %v92785, %v92786 (stack75)
        %v92788 = vxor.u32 %v92783, %v92787 (stack76)
        %v92791 = vadd.s32 %v92783, %v92788 (stack65)
        %v92793 = vshll.u32 %v92788, 29 (stack73)
        %v92794 = vshrl.u32 %v92788, 3 (stack74)
        %v92795 = vor.u32 %v92793, %v92794 (stack75)
        %v92796 = vxor.u32 %v92791, %v92795 (stack76)
        %v92799 = vadd.s32 %v92791, %v92796 (stack65)
        %v92801 = vshll.u32 %v92796, 16 (stack73)
        %v92802 = vshrl.u32 %v92796, 16 (stack74)
        %v92803 = vor.u32 %v92801, %v92802 (stack75)
        %v92804 = vxor.u32 %v92799, %v92803 (stack76)
        %v92807 = vadd.s32 %v92799, %v92804 (stack65)
        %v92811 = vadd.s32 %v92807, %v8 (stack65)
        %v92813 = vshll.u32 %v92804, 24 (stack73)
        %v92814 = vshrl.u32 %v92804, 8 (stack74)
        %v92815 = vor.u32 %v92813, %v92814 (stack75)
        %v92816 = vxor.u32 %v92807, %v92815 (stack76)
        %v92819 = vadd.s32 %v92816, %v10 (stack65)
        %v92823 = vadd.s32 %v92819, 2 (stack65)
        %v92827 = vadd.s32 %v92811, %v92823 (stack65)
        %v92829 = vshll.u32 %v92823, 13 (stack73)
        %v92830 = vshrl.u32 %v92823, 19 (stack74)
        %v92831 = vor.u32 %v92829, %v92830 (stack75)
        %v92832 = vxor.u32 %v92827, %v92831 (stack76)
        %v92835 = vadd.s32 %v92827, %v92832 (stack65)
        %v92837 = vshll.u32 %v92832, 15 (stack73)
        %v92838 = vshrl.u32 %v92832, 17 (stack74)
        %v92839 = vor.u32 %v92837, %v92838 (stack75)
        %v92840 = vxor.u32 %v92835, %v92839 (stack76)
        %v92843 = vadd.s32 %v92835, %v92840 (stack65)
        %v92845 = vshll.u32 %v92840, 26 (stack73)
        %v92846 = vshrl.u32 %v92840, 6 (stack74)
        %v92847 = vor.u32 %v92845, %v92846 (stack75)
        %v92848 = vxor.u32 %v92843, %v92847 (stack76)
        %v92851 = vadd.s32 %v92843, %v92848 (stack65)
        %v92855 = vadd.s32 %v92851, %v10 (stack65)
        %v92857 = vshll.u32 %v92848, 6 (stack73)
        %v92858 = vshrl.u32 %v92848, 26 (stack74)
        %v92859 = vor.u32 %v92857, %v92858 (stack75)
        %v92860 = vxor.u32 %v92851, %v92859 (stack76)
        %v92863 = vadd.s32 %v92860, %v9 (stack65)
        %v92867 = vadd.s32 %v92863, 3 (stack65)
        %v92871 = vadd.s32 %v92855, %v92867 (stack65)
        %v92873 = vshll.u32 %v92867, 17 (stack73)
        %v92874 = vshrl.u32 %v92867, 15 (stack74)
        %v92875 = vor.u32 %v92873, %v92874 (stack75)
        %v92876 = vxor.u32 %v92871, %v92875 (stack76)
        %v92879 = vadd.s32 %v92871, %v92876 (stack65)
        %v92881 = vshll.u32 %v92876, 29 (stack73)
        %v92882 = vshrl.u32 %v92876, 3 (stack74)
        %v92883 = vor.u32 %v92881, %v92882 (stack75)
        %v92884 = vxor.u32 %v92879, %v92883 (stack76)
        %v92887 = vadd.s32 %v92879, %v92884 (stack65)
        %v92889 = vshll.u32 %v92884, 16 (stack73)
        %v92890 = vshrl.u32 %v92884, 16 (stack74)
        %v92891 = vor.u32 %v92889, %v92890 (stack75)
        %v92892 = vxor.u32 %v92887, %v92891 (stack76)
        %v92895 = vadd.s32 %v92887, %v92892 (stack65)
        %v92899 = vadd.s32 %v92895, %v9 (stack65)
        %v92901 = vshll.u32 %v92892, 24 (stack73)
        %v92902 = vshrl.u32 %v92892, 8 (stack74)
        %v92903 = vor.u32 %v92901, %v92902 (stack75)
        %v92904 = vxor.u32 %v92895, %v92903 (stack76)
        %v92907 = vadd.s32 %v92904, %v8 (stack65)
        %v92911 = vadd.s32 %v92907, 4 (stack65)
        %v92915 = vadd.s32 %v92899, %v92911 (stack65)
        %v92917 = vshll.u32 %v92911, 13 (stack73)
        %v92918 = vshrl.u32 %v92911, 19 (stack74)
        %v92919 = vor.u32 %v92917, %v92918 (stack75)
        %v92920 = vxor.u32 %v92915, %v92919 (stack76)
        %v92923 = vadd.s32 %v92915, %v92920 (stack65)
        %v92925 = vshll.u32 %v92920, 15 (stack73)
        %v92926 = vshrl.u32 %v92920, 17 (stack74)
        %v92927 = vor.u32 %v92925, %v92926 (stack75)
        %v92928 = vxor.u32 %v92923, %v92927 (stack76)
        %v92931 = vadd.s32 %v92923, %v92928 (stack65)
        %v92933 = vshll.u32 %v92928, 26 (stack73)
        %v92934 = vshrl.u32 %v92928, 6 (stack74)
        %v92935 = vor.u32 %v92933, %v92934 (stack75)
        %v92936 = vxor.u32 %v92931, %v92935 (stack76)
        %v92939 = vadd.s32 %v92931, %v92936 (stack65)
        %v92943 = vadd.s32 %v92939, %v8 (stack65)
        %v92945 = vshll.u32 %v92936, 6 (stack73)
        %v92946 = vshrl.u32 %v92936, 26 (stack74)
        %v92947 = vor.u32 %v92945, %v92946 (stack75)
        %v92948 = vxor.u32 %v92939, %v92947 (stack76)
        %v92951 = vadd.s32 %v92948, %v10 (stack65)
        %v92955 = vadd.s32 %v92951, 5 (stack65)
        %v92957 = vxor.u32 %v92943, %v92955 (stack76)
        %v92958 = vand.u32.u8 %v92957, 255 (stack77)
        %v92959 = vand.u32 %v92958, 65535 (stack78)
        %v92960 = vshrl.u32 %v92959, 1 (stack79)
        %v92961 = vor.u32 %v92960, 16256 (stack75)
        %v92962 = vand.u32.u16 %v92961, 65535 (stack80)
        %v92963 = vunpack.i.l.bf16 %v92962 (stack81)
        %v92967 = vadd.f32 %v92963, -1.0 (stack82)
        %v92971 = vmul.f32 %v92967, 2.0 (stack83)
        %v92975 = vadd.f32 %v92971, -0.99609375 (stack82)
        %v92979 = vmax.f32 -0.99609375, %v92975 (stack84)
        %v92981 = vand.u32 2147483647, %v92979 (stack85)
        %vm92984 = vcmp.eq.f32.partialorder %v92981, 1.0 (stack86)
        %v92989 = vmul.f32 %v92979, inf (stack83)
        %v92991 = vxor.u32 %v92979, 2147483648 (stack87)
        %v92994 = vmul.f32 %v92979, %v92991 (stack83)
        %v92996 = vadd.f32 %v92994, 1.0 (stack88)
        %v92997 = vlog2.pop %v92996 (stack89)
        %v92998 = vmul.f32 %v92997, 0.6931472 (stack90)
        %v92999 = vmul.f32 -0.5, %v92994 (stack91)
        %v93000 = vadd.f32 %v92999, 1.0 (stack92)
        %v93001 = vmul.f32 %v93000, %v92994 (stack93)
        %v93002 = vand.u32 2147483647, %v92994 (stack94)
        %vm93003 = vcmp.lt.f32.partialorder %v93002, 0.0004427343 (stack95)
        %v93004 = vsel /*vm=*/%vm93003, /*on_true_vy=*/%v93001, /*on_false_vx=*/%v92998 (stack96)
        %v93005 = vxor.u32 %v93004, 2147483648 (stack87)
        %vm93008 = vcmp.lt.f32.partialorder %v93005, 5.0 (stack86)
        %v93013 = vsel /*vm=*/%vm93008, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v93017 = vsel /*vm=*/%vm93008, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v93021 = vsel /*vm=*/%vm93008, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v93025 = vsel /*vm=*/%vm93008, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v93029 = vsel /*vm=*/%vm93008, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v93033 = vsel /*vm=*/%vm93008, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v93037 = vsel /*vm=*/%vm93008, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v93041 = vsel /*vm=*/%vm93008, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v93045 = vsel /*vm=*/%vm93008, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v93049 = vadd.f32 %v93005, -2.5 (stack82)
        %v93051 = vrsqrt.pop %v93005 (stack97)
        %v93052 = vmul.f32 %v93005, %v93051 (stack98)
        %vm93053 = vcmp.eq.f32.partialorder %v93005, inf (stack99)
        %v93054 = vsel /*vm=*/%vm93053, /*on_true_vy=*/%v93005, /*on_false_vx=*/%v93052 (stack100)
        %vm93055 = vcmp.eq.f32.partialorder %v93005, 0.0 (stack101)
        %v93056 = vand.u32 %v93005, 2147483648 (stack102)
        %v93057 = vsel /*vm=*/%vm93055, /*on_true_vy=*/%v93056, /*on_false_vx=*/%v93054 (stack103)
        %v93060 = vadd.f32 %v93057, -3.0 (stack82)
        %v93064 = vsel /*vm=*/%vm93008, /*on_true_vy=*/%v93049, /*on_false_vx=*/%v93060 (stack72)
        %v93068 = vmul.f32 %v93045, %v93064 (stack83)
        %v93072 = vadd.f32 %v93041, %v93068 (stack82)
        %v93076 = vmul.f32 %v93072, %v93064 (stack83)
        %v93080 = vadd.f32 %v93037, %v93076 (stack82)
        %v93084 = vmul.f32 %v93080, %v93064 (stack83)
        %v93088 = vadd.f32 %v93033, %v93084 (stack82)
        %v93092 = vmul.f32 %v93088, %v93064 (stack83)
        %v93096 = vadd.f32 %v93029, %v93092 (stack82)
        %v93100 = vmul.f32 %v93096, %v93064 (stack83)
        %v93104 = vadd.f32 %v93025, %v93100 (stack82)
        %v93108 = vmul.f32 %v93104, %v93064 (stack83)
        %v93112 = vadd.f32 %v93021, %v93108 (stack82)
        %v93116 = vmul.f32 %v93112, %v93064 (stack83)
        %v93120 = vadd.f32 %v93017, %v93116 (stack82)
        %v93124 = vmul.f32 %v93120, %v93064 (stack83)
        %v93128 = vadd.f32 %v93013, %v93124 (stack82)
        %v93132 = vmul.f32 %v93128, %v92979 (stack83)
        %v93136 = vsel /*vm=*/%vm92984, /*on_true_vy=*/%v92989, /*on_false_vx=*/%v93132 (stack72)
        %v93140 = vmul.f32 %v93136, 1.4140625 (stack83)
        %s93142 = scalar_lea.vmem %s280, 864 [#allocation0] (stack107)
        %v93143 = vpack.c.bf16 0.0, %v93140 (stack104)
        %93144 = vst [vmem:[%s93142] sm:$0xf] /*vst_source=*/%v93143 (stack105)
        %v93147 = vadd.s32 %v3816, %v89917 (stack65)
        %s93149 = smul.u32 128, %s27 (stack66)
        %v93150 = vlaneseq (stack67)
        %v93151 = vand.u32 %v93150, 127 (stack68)
        %v93152 = vstv %s93149 (stack69)
        %v93153 = vadd.s32 %v93151, %v93152 (stack70)
        %v93157 = vadd.s32 %v93147, %v93153 (stack65)
        %vm93161 = vcmp.lt.u32.totalorder %v93157, %v93147 (stack71)
        %vm93166 = vcmp.lt.u32.totalorder %v93147, %v3816 (stack71)
        %v93171 = vadd.s32 %v3803, %v89900 (stack65)
        %v93175 = vadd.s32 %v93171, 1 (stack65)
        %v93179 = vsel /*vm=*/%vm93166, /*on_true_vy=*/%v93175, /*on_false_vx=*/%v93171 (stack72)
        %v93183 = vadd.s32 %v93179, 1 (stack65)
        %v93187 = vsel /*vm=*/%vm93161, /*on_true_vy=*/%v93183, /*on_false_vx=*/%v93179 (stack72)
        %v93192 = vadd.s32 %v93187, %v10 (stack65)
        %v93196 = vadd.s32 %v93157, %v9 (stack65)
        %v93200 = vadd.s32 %v93192, %v93196 (stack65)
        %v93202 = vshll.u32 %v93196, 13 (stack73)
        %v93203 = vshrl.u32 %v93196, 19 (stack74)
        %v93204 = vor.u32 %v93202, %v93203 (stack75)
        %v93205 = vxor.u32 %v93200, %v93204 (stack76)
        %v93208 = vadd.s32 %v93200, %v93205 (stack65)
        %v93210 = vshll.u32 %v93205, 15 (stack73)
        %v93211 = vshrl.u32 %v93205, 17 (stack74)
        %v93212 = vor.u32 %v93210, %v93211 (stack75)
        %v93213 = vxor.u32 %v93208, %v93212 (stack76)
        %v93216 = vadd.s32 %v93208, %v93213 (stack65)
        %v93218 = vshll.u32 %v93213, 26 (stack73)
        %v93219 = vshrl.u32 %v93213, 6 (stack74)
        %v93220 = vor.u32 %v93218, %v93219 (stack75)
        %v93221 = vxor.u32 %v93216, %v93220 (stack76)
        %v93224 = vadd.s32 %v93216, %v93221 (stack65)
        %v93228 = vadd.s32 %v93224, %v9 (stack65)
        %v93230 = vshll.u32 %v93221, 6 (stack73)
        %v93231 = vshrl.u32 %v93221, 26 (stack74)
        %v93232 = vor.u32 %v93230, %v93231 (stack75)
        %v93233 = vxor.u32 %v93224, %v93232 (stack76)
        %v93236 = vadd.s32 %v93233, %v8 (stack65)
        %v93240 = vadd.s32 %v93236, 1 (stack65)
        %v93244 = vadd.s32 %v93228, %v93240 (stack65)
        %v93246 = vshll.u32 %v93240, 17 (stack73)
        %v93247 = vshrl.u32 %v93240, 15 (stack74)
        %v93248 = vor.u32 %v93246, %v93247 (stack75)
        %v93249 = vxor.u32 %v93244, %v93248 (stack76)
        %v93252 = vadd.s32 %v93244, %v93249 (stack65)
        %v93254 = vshll.u32 %v93249, 29 (stack73)
        %v93255 = vshrl.u32 %v93249, 3 (stack74)
        %v93256 = vor.u32 %v93254, %v93255 (stack75)
        %v93257 = vxor.u32 %v93252, %v93256 (stack76)
        %v93260 = vadd.s32 %v93252, %v93257 (stack65)
        %v93262 = vshll.u32 %v93257, 16 (stack73)
        %v93263 = vshrl.u32 %v93257, 16 (stack74)
        %v93264 = vor.u32 %v93262, %v93263 (stack75)
        %v93265 = vxor.u32 %v93260, %v93264 (stack76)
        %v93268 = vadd.s32 %v93260, %v93265 (stack65)
        %v93272 = vadd.s32 %v93268, %v8 (stack65)
        %v93274 = vshll.u32 %v93265, 24 (stack73)
        %v93275 = vshrl.u32 %v93265, 8 (stack74)
        %v93276 = vor.u32 %v93274, %v93275 (stack75)
        %v93277 = vxor.u32 %v93268, %v93276 (stack76)
        %v93280 = vadd.s32 %v93277, %v10 (stack65)
        %v93284 = vadd.s32 %v93280, 2 (stack65)
        %v93288 = vadd.s32 %v93272, %v93284 (stack65)
        %v93290 = vshll.u32 %v93284, 13 (stack73)
        %v93291 = vshrl.u32 %v93284, 19 (stack74)
        %v93292 = vor.u32 %v93290, %v93291 (stack75)
        %v93293 = vxor.u32 %v93288, %v93292 (stack76)
        %v93296 = vadd.s32 %v93288, %v93293 (stack65)
        %v93298 = vshll.u32 %v93293, 15 (stack73)
        %v93299 = vshrl.u32 %v93293, 17 (stack74)
        %v93300 = vor.u32 %v93298, %v93299 (stack75)
        %v93301 = vxor.u32 %v93296, %v93300 (stack76)
        %v93304 = vadd.s32 %v93296, %v93301 (stack65)
        %v93306 = vshll.u32 %v93301, 26 (stack73)
        %v93307 = vshrl.u32 %v93301, 6 (stack74)
        %v93308 = vor.u32 %v93306, %v93307 (stack75)
        %v93309 = vxor.u32 %v93304, %v93308 (stack76)
        %v93312 = vadd.s32 %v93304, %v93309 (stack65)
        %v93316 = vadd.s32 %v93312, %v10 (stack65)
        %v93318 = vshll.u32 %v93309, 6 (stack73)
        %v93319 = vshrl.u32 %v93309, 26 (stack74)
        %v93320 = vor.u32 %v93318, %v93319 (stack75)
        %v93321 = vxor.u32 %v93312, %v93320 (stack76)
        %v93324 = vadd.s32 %v93321, %v9 (stack65)
        %v93328 = vadd.s32 %v93324, 3 (stack65)
        %v93332 = vadd.s32 %v93316, %v93328 (stack65)
        %v93334 = vshll.u32 %v93328, 17 (stack73)
        %v93335 = vshrl.u32 %v93328, 15 (stack74)
        %v93336 = vor.u32 %v93334, %v93335 (stack75)
        %v93337 = vxor.u32 %v93332, %v93336 (stack76)
        %v93340 = vadd.s32 %v93332, %v93337 (stack65)
        %v93342 = vshll.u32 %v93337, 29 (stack73)
        %v93343 = vshrl.u32 %v93337, 3 (stack74)
        %v93344 = vor.u32 %v93342, %v93343 (stack75)
        %v93345 = vxor.u32 %v93340, %v93344 (stack76)
        %v93348 = vadd.s32 %v93340, %v93345 (stack65)
        %v93350 = vshll.u32 %v93345, 16 (stack73)
        %v93351 = vshrl.u32 %v93345, 16 (stack74)
        %v93352 = vor.u32 %v93350, %v93351 (stack75)
        %v93353 = vxor.u32 %v93348, %v93352 (stack76)
        %v93356 = vadd.s32 %v93348, %v93353 (stack65)
        %v93360 = vadd.s32 %v93356, %v9 (stack65)
        %v93362 = vshll.u32 %v93353, 24 (stack73)
        %v93363 = vshrl.u32 %v93353, 8 (stack74)
        %v93364 = vor.u32 %v93362, %v93363 (stack75)
        %v93365 = vxor.u32 %v93356, %v93364 (stack76)
        %v93368 = vadd.s32 %v93365, %v8 (stack65)
        %v93372 = vadd.s32 %v93368, 4 (stack65)
        %v93376 = vadd.s32 %v93360, %v93372 (stack65)
        %v93378 = vshll.u32 %v93372, 13 (stack73)
        %v93379 = vshrl.u32 %v93372, 19 (stack74)
        %v93380 = vor.u32 %v93378, %v93379 (stack75)
        %v93381 = vxor.u32 %v93376, %v93380 (stack76)
        %v93384 = vadd.s32 %v93376, %v93381 (stack65)
        %v93386 = vshll.u32 %v93381, 15 (stack73)
        %v93387 = vshrl.u32 %v93381, 17 (stack74)
        %v93388 = vor.u32 %v93386, %v93387 (stack75)
        %v93389 = vxor.u32 %v93384, %v93388 (stack76)
        %v93392 = vadd.s32 %v93384, %v93389 (stack65)
        %v93394 = vshll.u32 %v93389, 26 (stack73)
        %v93395 = vshrl.u32 %v93389, 6 (stack74)
        %v93396 = vor.u32 %v93394, %v93395 (stack75)
        %v93397 = vxor.u32 %v93392, %v93396 (stack76)
        %v93400 = vadd.s32 %v93392, %v93397 (stack65)
        %v93404 = vadd.s32 %v93400, %v8 (stack65)
        %v93406 = vshll.u32 %v93397, 6 (stack73)
        %v93407 = vshrl.u32 %v93397, 26 (stack74)
        %v93408 = vor.u32 %v93406, %v93407 (stack75)
        %v93409 = vxor.u32 %v93400, %v93408 (stack76)
        %v93412 = vadd.s32 %v93409, %v10 (stack65)
        %v93416 = vadd.s32 %v93412, 5 (stack65)
        %v93418 = vxor.u32 %v93404, %v93416 (stack76)
        %v93419 = vand.u32.u8 %v93418, 255 (stack77)
        %v93420 = vand.u32 %v93419, 65535 (stack78)
        %v93421 = vshrl.u32 %v93420, 1 (stack79)
        %v93422 = vor.u32 %v93421, 16256 (stack75)
        %v93423 = vand.u32.u16 %v93422, 65535 (stack80)
        %v93424 = vunpack.i.l.bf16 %v93423 (stack81)
        %v93428 = vadd.f32 %v93424, -1.0 (stack82)
        %v93432 = vmul.f32 %v93428, 2.0 (stack83)
        %v93436 = vadd.f32 %v93432, -0.99609375 (stack82)
        %v93440 = vmax.f32 -0.99609375, %v93436 (stack84)
        %v93442 = vand.u32 2147483647, %v93440 (stack85)
        %vm93445 = vcmp.eq.f32.partialorder %v93442, 1.0 (stack86)
        %v93450 = vmul.f32 %v93440, inf (stack83)
        %v93452 = vxor.u32 %v93440, 2147483648 (stack87)
        %v93455 = vmul.f32 %v93440, %v93452 (stack83)
        %v93457 = vadd.f32 %v93455, 1.0 (stack88)
        %v93458 = vlog2.pop %v93457 (stack89)
        %v93459 = vmul.f32 %v93458, 0.6931472 (stack90)
        %v93460 = vmul.f32 -0.5, %v93455 (stack91)
        %v93461 = vadd.f32 %v93460, 1.0 (stack92)
        %v93462 = vmul.f32 %v93461, %v93455 (stack93)
        %v93463 = vand.u32 2147483647, %v93455 (stack94)
        %vm93464 = vcmp.lt.f32.partialorder %v93463, 0.0004427343 (stack95)
        %v93465 = vsel /*vm=*/%vm93464, /*on_true_vy=*/%v93462, /*on_false_vx=*/%v93459 (stack96)
        %v93466 = vxor.u32 %v93465, 2147483648 (stack87)
        %vm93469 = vcmp.lt.f32.partialorder %v93466, 5.0 (stack86)
        %v93474 = vsel /*vm=*/%vm93469, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v93478 = vsel /*vm=*/%vm93469, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v93482 = vsel /*vm=*/%vm93469, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v93486 = vsel /*vm=*/%vm93469, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v93490 = vsel /*vm=*/%vm93469, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v93494 = vsel /*vm=*/%vm93469, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v93498 = vsel /*vm=*/%vm93469, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v93502 = vsel /*vm=*/%vm93469, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v93506 = vsel /*vm=*/%vm93469, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v93510 = vadd.f32 %v93466, -2.5 (stack82)
        %v93512 = vrsqrt.pop %v93466 (stack97)
        %v93513 = vmul.f32 %v93466, %v93512 (stack98)
        %vm93514 = vcmp.eq.f32.partialorder %v93466, inf (stack99)
        %v93515 = vsel /*vm=*/%vm93514, /*on_true_vy=*/%v93466, /*on_false_vx=*/%v93513 (stack100)
        %vm93516 = vcmp.eq.f32.partialorder %v93466, 0.0 (stack101)
        %v93517 = vand.u32 %v93466, 2147483648 (stack102)
        %v93518 = vsel /*vm=*/%vm93516, /*on_true_vy=*/%v93517, /*on_false_vx=*/%v93515 (stack103)
        %v93521 = vadd.f32 %v93518, -3.0 (stack82)
        %v93525 = vsel /*vm=*/%vm93469, /*on_true_vy=*/%v93510, /*on_false_vx=*/%v93521 (stack72)
        %v93529 = vmul.f32 %v93506, %v93525 (stack83)
        %v93533 = vadd.f32 %v93502, %v93529 (stack82)
        %v93537 = vmul.f32 %v93533, %v93525 (stack83)
        %v93541 = vadd.f32 %v93498, %v93537 (stack82)
        %v93545 = vmul.f32 %v93541, %v93525 (stack83)
        %v93549 = vadd.f32 %v93494, %v93545 (stack82)
        %v93553 = vmul.f32 %v93549, %v93525 (stack83)
        %v93557 = vadd.f32 %v93490, %v93553 (stack82)
        %v93561 = vmul.f32 %v93557, %v93525 (stack83)
        %v93565 = vadd.f32 %v93486, %v93561 (stack82)
        %v93569 = vmul.f32 %v93565, %v93525 (stack83)
        %v93573 = vadd.f32 %v93482, %v93569 (stack82)
        %v93577 = vmul.f32 %v93573, %v93525 (stack83)
        %v93581 = vadd.f32 %v93478, %v93577 (stack82)
        %v93585 = vmul.f32 %v93581, %v93525 (stack83)
        %v93589 = vadd.f32 %v93474, %v93585 (stack82)
        %v93593 = vmul.f32 %v93589, %v93440 (stack83)
        %v93597 = vsel /*vm=*/%vm93445, /*on_true_vy=*/%v93450, /*on_false_vx=*/%v93593 (stack72)
        %v93601 = vmul.f32 %v93597, 1.4140625 (stack83)
        %s93603 = scalar_lea.vmem %s280, 992 [#allocation0] (stack107)
        %v93604 = vpack.c.bf16 0.0, %v93601 (stack104)
        %93605 = vst [vmem:[%s93603] sm:$0xf] /*vst_source=*/%v93604 (stack105)
        %s93606 = sadd.s32 %s339, 200 (stack106)
        %s93607 = sshrl.u32 %s93606, 10 (stack49)
        %p93608 = scmp.lt.s32.totalorder 1, %s93607 (stack50)
        %s93609 = scalar_select /*predicate=*/%p93608, /*on_true=*/1, /*on_false=*/%s93607 (stack51)
        %s93610 = sand.u32 %s93606, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s93611 = sshrl.u32 %s93610, 7 (stack53)
        %s93612 = sand.u32 %s93610, 127 /* smod.u32 w/div 128 */ (stack54)
        %s93613 = smul.addr %s93609, 8 (stack55)
        %s93614 = scalar_lea.vmem %s3, %s93613 (stack56)
        %s93616 = scalar_lea.vmem %s93614, %s93611 (stack57)
        %v93617 = vld [vmem:[%s93616] ss:$0 sm:$0xff] (stack58)
        %s93618 = sand.u32 %s93612, 255 (stack59)
        %s93620 = sor.u32 256, %s93618 (stack60)
        %93621 = vbcast.lane.b32.xlu0 %v93617, %s93620 (stack61)
        %v93622 = vpop.permute.xlu0 %93621 (stack62)
        %s93623 = sadd.s32 %s347, 200 (stack106)
        %s93624 = sshrl.u32 %s93623, 10 (stack49)
        %p93625 = scmp.lt.s32.totalorder 1, %s93624 (stack50)
        %s93626 = scalar_select /*predicate=*/%p93625, /*on_true=*/1, /*on_false=*/%s93624 (stack51)
        %s93627 = sand.u32 %s93623, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s93628 = sshrl.u32 %s93627, 7 (stack53)
        %s93629 = sand.u32 %s93627, 127 /* smod.u32 w/div 128 */ (stack54)
        %s93630 = smul.addr %s93626, 8 (stack55)
        %s93631 = scalar_lea.vmem %s5, %s93630 (stack56)
        %s93633 = scalar_lea.vmem %s93631, %s93628 (stack57)
        %v93634 = vld [vmem:[%s93633] ss:$0 sm:$0xff] (stack58)
        %s93635 = sand.u32 %s93629, 255 (stack59)
        %s93637 = sor.u32 256, %s93635 (stack60)
        %93638 = vbcast.lane.b32.xlu0 %v93634, %s93637 (stack61)
        %v93639 = vpop.permute.xlu0 %93638 (stack62)
        %v93642 = vadd.s32 %v408, %v93639 (stack65)
        %s93644 = smul.u32 128, %s27 (stack66)
        %v93645 = vlaneseq (stack67)
        %v93646 = vand.u32 %v93645, 127 (stack68)
        %v93647 = vstv %s93644 (stack69)
        %v93648 = vadd.s32 %v93646, %v93647 (stack70)
        %v93652 = vadd.s32 %v93642, %v93648 (stack65)
        %vm93656 = vcmp.lt.u32.totalorder %v93652, %v93642 (stack71)
        %vm93661 = vcmp.lt.u32.totalorder %v93642, %v408 (stack71)
        %v93666 = vadd.s32 %v380, %v93622 (stack65)
        %v93670 = vadd.s32 %v93666, 1 (stack65)
        %v93674 = vsel /*vm=*/%vm93661, /*on_true_vy=*/%v93670, /*on_false_vx=*/%v93666 (stack72)
        %v93678 = vadd.s32 %v93674, 1 (stack65)
        %v93682 = vsel /*vm=*/%vm93656, /*on_true_vy=*/%v93678, /*on_false_vx=*/%v93674 (stack72)
        %v93687 = vadd.s32 %v93682, %v10 (stack65)
        %v93691 = vadd.s32 %v93652, %v9 (stack65)
        %v93695 = vadd.s32 %v93687, %v93691 (stack65)
        %v93697 = vshll.u32 %v93691, 13 (stack73)
        %v93698 = vshrl.u32 %v93691, 19 (stack74)
        %v93699 = vor.u32 %v93697, %v93698 (stack75)
        %v93700 = vxor.u32 %v93695, %v93699 (stack76)
        %v93703 = vadd.s32 %v93695, %v93700 (stack65)
        %v93705 = vshll.u32 %v93700, 15 (stack73)
        %v93706 = vshrl.u32 %v93700, 17 (stack74)
        %v93707 = vor.u32 %v93705, %v93706 (stack75)
        %v93708 = vxor.u32 %v93703, %v93707 (stack76)
        %v93711 = vadd.s32 %v93703, %v93708 (stack65)
        %v93713 = vshll.u32 %v93708, 26 (stack73)
        %v93714 = vshrl.u32 %v93708, 6 (stack74)
        %v93715 = vor.u32 %v93713, %v93714 (stack75)
        %v93716 = vxor.u32 %v93711, %v93715 (stack76)
        %v93719 = vadd.s32 %v93711, %v93716 (stack65)
        %v93723 = vadd.s32 %v93719, %v9 (stack65)
        %v93725 = vshll.u32 %v93716, 6 (stack73)
        %v93726 = vshrl.u32 %v93716, 26 (stack74)
        %v93727 = vor.u32 %v93725, %v93726 (stack75)
        %v93728 = vxor.u32 %v93719, %v93727 (stack76)
        %v93731 = vadd.s32 %v93728, %v8 (stack65)
        %v93735 = vadd.s32 %v93731, 1 (stack65)
        %v93739 = vadd.s32 %v93723, %v93735 (stack65)
        %v93741 = vshll.u32 %v93735, 17 (stack73)
        %v93742 = vshrl.u32 %v93735, 15 (stack74)
        %v93743 = vor.u32 %v93741, %v93742 (stack75)
        %v93744 = vxor.u32 %v93739, %v93743 (stack76)
        %v93747 = vadd.s32 %v93739, %v93744 (stack65)
        %v93749 = vshll.u32 %v93744, 29 (stack73)
        %v93750 = vshrl.u32 %v93744, 3 (stack74)
        %v93751 = vor.u32 %v93749, %v93750 (stack75)
        %v93752 = vxor.u32 %v93747, %v93751 (stack76)
        %v93755 = vadd.s32 %v93747, %v93752 (stack65)
        %v93757 = vshll.u32 %v93752, 16 (stack73)
        %v93758 = vshrl.u32 %v93752, 16 (stack74)
        %v93759 = vor.u32 %v93757, %v93758 (stack75)
        %v93760 = vxor.u32 %v93755, %v93759 (stack76)
        %v93763 = vadd.s32 %v93755, %v93760 (stack65)
        %v93767 = vadd.s32 %v93763, %v8 (stack65)
        %v93769 = vshll.u32 %v93760, 24 (stack73)
        %v93770 = vshrl.u32 %v93760, 8 (stack74)
        %v93771 = vor.u32 %v93769, %v93770 (stack75)
        %v93772 = vxor.u32 %v93763, %v93771 (stack76)
        %v93775 = vadd.s32 %v93772, %v10 (stack65)
        %v93779 = vadd.s32 %v93775, 2 (stack65)
        %v93783 = vadd.s32 %v93767, %v93779 (stack65)
        %v93785 = vshll.u32 %v93779, 13 (stack73)
        %v93786 = vshrl.u32 %v93779, 19 (stack74)
        %v93787 = vor.u32 %v93785, %v93786 (stack75)
        %v93788 = vxor.u32 %v93783, %v93787 (stack76)
        %v93791 = vadd.s32 %v93783, %v93788 (stack65)
        %v93793 = vshll.u32 %v93788, 15 (stack73)
        %v93794 = vshrl.u32 %v93788, 17 (stack74)
        %v93795 = vor.u32 %v93793, %v93794 (stack75)
        %v93796 = vxor.u32 %v93791, %v93795 (stack76)
        %v93799 = vadd.s32 %v93791, %v93796 (stack65)
        %v93801 = vshll.u32 %v93796, 26 (stack73)
        %v93802 = vshrl.u32 %v93796, 6 (stack74)
        %v93803 = vor.u32 %v93801, %v93802 (stack75)
        %v93804 = vxor.u32 %v93799, %v93803 (stack76)
        %v93807 = vadd.s32 %v93799, %v93804 (stack65)
        %v93811 = vadd.s32 %v93807, %v10 (stack65)
        %v93813 = vshll.u32 %v93804, 6 (stack73)
        %v93814 = vshrl.u32 %v93804, 26 (stack74)
        %v93815 = vor.u32 %v93813, %v93814 (stack75)
        %v93816 = vxor.u32 %v93807, %v93815 (stack76)
        %v93819 = vadd.s32 %v93816, %v9 (stack65)
        %v93823 = vadd.s32 %v93819, 3 (stack65)
        %v93827 = vadd.s32 %v93811, %v93823 (stack65)
        %v93829 = vshll.u32 %v93823, 17 (stack73)
        %v93830 = vshrl.u32 %v93823, 15 (stack74)
        %v93831 = vor.u32 %v93829, %v93830 (stack75)
        %v93832 = vxor.u32 %v93827, %v93831 (stack76)
        %v93835 = vadd.s32 %v93827, %v93832 (stack65)
        %v93837 = vshll.u32 %v93832, 29 (stack73)
        %v93838 = vshrl.u32 %v93832, 3 (stack74)
        %v93839 = vor.u32 %v93837, %v93838 (stack75)
        %v93840 = vxor.u32 %v93835, %v93839 (stack76)
        %v93843 = vadd.s32 %v93835, %v93840 (stack65)
        %v93845 = vshll.u32 %v93840, 16 (stack73)
        %v93846 = vshrl.u32 %v93840, 16 (stack74)
        %v93847 = vor.u32 %v93845, %v93846 (stack75)
        %v93848 = vxor.u32 %v93843, %v93847 (stack76)
        %v93851 = vadd.s32 %v93843, %v93848 (stack65)
        %v93855 = vadd.s32 %v93851, %v9 (stack65)
        %v93857 = vshll.u32 %v93848, 24 (stack73)
        %v93858 = vshrl.u32 %v93848, 8 (stack74)
        %v93859 = vor.u32 %v93857, %v93858 (stack75)
        %v93860 = vxor.u32 %v93851, %v93859 (stack76)
        %v93863 = vadd.s32 %v93860, %v8 (stack65)
        %v93867 = vadd.s32 %v93863, 4 (stack65)
        %v93871 = vadd.s32 %v93855, %v93867 (stack65)
        %v93873 = vshll.u32 %v93867, 13 (stack73)
        %v93874 = vshrl.u32 %v93867, 19 (stack74)
        %v93875 = vor.u32 %v93873, %v93874 (stack75)
        %v93876 = vxor.u32 %v93871, %v93875 (stack76)
        %v93879 = vadd.s32 %v93871, %v93876 (stack65)
        %v93881 = vshll.u32 %v93876, 15 (stack73)
        %v93882 = vshrl.u32 %v93876, 17 (stack74)
        %v93883 = vor.u32 %v93881, %v93882 (stack75)
        %v93884 = vxor.u32 %v93879, %v93883 (stack76)
        %v93887 = vadd.s32 %v93879, %v93884 (stack65)
        %v93889 = vshll.u32 %v93884, 26 (stack73)
        %v93890 = vshrl.u32 %v93884, 6 (stack74)
        %v93891 = vor.u32 %v93889, %v93890 (stack75)
        %v93892 = vxor.u32 %v93887, %v93891 (stack76)
        %v93895 = vadd.s32 %v93887, %v93892 (stack65)
        %v93899 = vadd.s32 %v93895, %v8 (stack65)
        %v93901 = vshll.u32 %v93892, 6 (stack73)
        %v93902 = vshrl.u32 %v93892, 26 (stack74)
        %v93903 = vor.u32 %v93901, %v93902 (stack75)
        %v93904 = vxor.u32 %v93895, %v93903 (stack76)
        %v93907 = vadd.s32 %v93904, %v10 (stack65)
        %v93911 = vadd.s32 %v93907, 5 (stack65)
        %v93913 = vxor.u32 %v93899, %v93911 (stack76)
        %v93914 = vand.u32.u8 %v93913, 255 (stack77)
        %v93915 = vand.u32 %v93914, 65535 (stack78)
        %v93916 = vshrl.u32 %v93915, 1 (stack79)
        %v93917 = vor.u32 %v93916, 16256 (stack75)
        %v93918 = vand.u32.u16 %v93917, 65535 (stack80)
        %v93919 = vunpack.i.l.bf16 %v93918 (stack81)
        %v93923 = vadd.f32 %v93919, -1.0 (stack82)
        %v93927 = vmul.f32 %v93923, 2.0 (stack83)
        %v93931 = vadd.f32 %v93927, -0.99609375 (stack82)
        %v93935 = vmax.f32 -0.99609375, %v93931 (stack84)
        %v93937 = vand.u32 2147483647, %v93935 (stack85)
        %vm93940 = vcmp.eq.f32.partialorder %v93937, 1.0 (stack86)
        %v93945 = vmul.f32 %v93935, inf (stack83)
        %v93947 = vxor.u32 %v93935, 2147483648 (stack87)
        %v93950 = vmul.f32 %v93935, %v93947 (stack83)
        %v93952 = vadd.f32 %v93950, 1.0 (stack88)
        %v93953 = vlog2.pop %v93952 (stack89)
        %v93954 = vmul.f32 %v93953, 0.6931472 (stack90)
        %v93955 = vmul.f32 -0.5, %v93950 (stack91)
        %v93956 = vadd.f32 %v93955, 1.0 (stack92)
        %v93957 = vmul.f32 %v93956, %v93950 (stack93)
        %v93958 = vand.u32 2147483647, %v93950 (stack94)
        %vm93959 = vcmp.lt.f32.partialorder %v93958, 0.0004427343 (stack95)
        %v93960 = vsel /*vm=*/%vm93959, /*on_true_vy=*/%v93957, /*on_false_vx=*/%v93954 (stack96)
        %v93961 = vxor.u32 %v93960, 2147483648 (stack87)
        %vm93964 = vcmp.lt.f32.partialorder %v93961, 5.0 (stack86)
        %v93969 = vsel /*vm=*/%vm93964, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v93973 = vsel /*vm=*/%vm93964, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v93977 = vsel /*vm=*/%vm93964, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v93981 = vsel /*vm=*/%vm93964, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v93985 = vsel /*vm=*/%vm93964, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v93989 = vsel /*vm=*/%vm93964, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v93993 = vsel /*vm=*/%vm93964, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v93997 = vsel /*vm=*/%vm93964, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v94001 = vsel /*vm=*/%vm93964, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v94005 = vadd.f32 %v93961, -2.5 (stack82)
        %v94007 = vrsqrt.pop %v93961 (stack97)
        %v94008 = vmul.f32 %v93961, %v94007 (stack98)
        %vm94009 = vcmp.eq.f32.partialorder %v93961, inf (stack99)
        %v94010 = vsel /*vm=*/%vm94009, /*on_true_vy=*/%v93961, /*on_false_vx=*/%v94008 (stack100)
        %vm94011 = vcmp.eq.f32.partialorder %v93961, 0.0 (stack101)
        %v94012 = vand.u32 %v93961, 2147483648 (stack102)
        %v94013 = vsel /*vm=*/%vm94011, /*on_true_vy=*/%v94012, /*on_false_vx=*/%v94010 (stack103)
        %v94016 = vadd.f32 %v94013, -3.0 (stack82)
        %v94020 = vsel /*vm=*/%vm93964, /*on_true_vy=*/%v94005, /*on_false_vx=*/%v94016 (stack72)
        %v94024 = vmul.f32 %v94001, %v94020 (stack83)
        %v94028 = vadd.f32 %v93997, %v94024 (stack82)
        %v94032 = vmul.f32 %v94028, %v94020 (stack83)
        %v94036 = vadd.f32 %v93993, %v94032 (stack82)
        %v94040 = vmul.f32 %v94036, %v94020 (stack83)
        %v94044 = vadd.f32 %v93989, %v94040 (stack82)
        %v94048 = vmul.f32 %v94044, %v94020 (stack83)
        %v94052 = vadd.f32 %v93985, %v94048 (stack82)
        %v94056 = vmul.f32 %v94052, %v94020 (stack83)
        %v94060 = vadd.f32 %v93981, %v94056 (stack82)
        %v94064 = vmul.f32 %v94060, %v94020 (stack83)
        %v94068 = vadd.f32 %v93977, %v94064 (stack82)
        %v94072 = vmul.f32 %v94068, %v94020 (stack83)
        %v94076 = vadd.f32 %v93973, %v94072 (stack82)
        %v94080 = vmul.f32 %v94076, %v94020 (stack83)
        %v94084 = vadd.f32 %v93969, %v94080 (stack82)
        %v94088 = vmul.f32 %v94084, %v93935 (stack83)
        %v94092 = vsel /*vm=*/%vm93940, /*on_true_vy=*/%v93945, /*on_false_vx=*/%v94088 (stack72)
        %v94096 = vmul.f32 %v94092, 1.4140625 (stack83)
        %s94098 = scalar_lea.vmem %s280, 100 [#allocation0] (stack107)
        %v94099 = vpack.c.bf16 0.0, %v94096 (stack104)
        %94100 = vst [vmem:[%s94098] sm:$0xf] /*vst_source=*/%v94099 (stack105)
        %v94103 = vadd.s32 %v894, %v93639 (stack65)
        %s94105 = smul.u32 128, %s27 (stack66)
        %v94106 = vlaneseq (stack67)
        %v94107 = vand.u32 %v94106, 127 (stack68)
        %v94108 = vstv %s94105 (stack69)
        %v94109 = vadd.s32 %v94107, %v94108 (stack70)
        %v94113 = vadd.s32 %v94103, %v94109 (stack65)
        %vm94117 = vcmp.lt.u32.totalorder %v94113, %v94103 (stack71)
        %vm94122 = vcmp.lt.u32.totalorder %v94103, %v894 (stack71)
        %v94127 = vadd.s32 %v881, %v93622 (stack65)
        %v94131 = vadd.s32 %v94127, 1 (stack65)
        %v94135 = vsel /*vm=*/%vm94122, /*on_true_vy=*/%v94131, /*on_false_vx=*/%v94127 (stack72)
        %v94139 = vadd.s32 %v94135, 1 (stack65)
        %v94143 = vsel /*vm=*/%vm94117, /*on_true_vy=*/%v94139, /*on_false_vx=*/%v94135 (stack72)
        %v94148 = vadd.s32 %v94143, %v10 (stack65)
        %v94152 = vadd.s32 %v94113, %v9 (stack65)
        %v94156 = vadd.s32 %v94148, %v94152 (stack65)
        %v94158 = vshll.u32 %v94152, 13 (stack73)
        %v94159 = vshrl.u32 %v94152, 19 (stack74)
        %v94160 = vor.u32 %v94158, %v94159 (stack75)
        %v94161 = vxor.u32 %v94156, %v94160 (stack76)
        %v94164 = vadd.s32 %v94156, %v94161 (stack65)
        %v94166 = vshll.u32 %v94161, 15 (stack73)
        %v94167 = vshrl.u32 %v94161, 17 (stack74)
        %v94168 = vor.u32 %v94166, %v94167 (stack75)
        %v94169 = vxor.u32 %v94164, %v94168 (stack76)
        %v94172 = vadd.s32 %v94164, %v94169 (stack65)
        %v94174 = vshll.u32 %v94169, 26 (stack73)
        %v94175 = vshrl.u32 %v94169, 6 (stack74)
        %v94176 = vor.u32 %v94174, %v94175 (stack75)
        %v94177 = vxor.u32 %v94172, %v94176 (stack76)
        %v94180 = vadd.s32 %v94172, %v94177 (stack65)
        %v94184 = vadd.s32 %v94180, %v9 (stack65)
        %v94186 = vshll.u32 %v94177, 6 (stack73)
        %v94187 = vshrl.u32 %v94177, 26 (stack74)
        %v94188 = vor.u32 %v94186, %v94187 (stack75)
        %v94189 = vxor.u32 %v94180, %v94188 (stack76)
        %v94192 = vadd.s32 %v94189, %v8 (stack65)
        %v94196 = vadd.s32 %v94192, 1 (stack65)
        %v94200 = vadd.s32 %v94184, %v94196 (stack65)
        %v94202 = vshll.u32 %v94196, 17 (stack73)
        %v94203 = vshrl.u32 %v94196, 15 (stack74)
        %v94204 = vor.u32 %v94202, %v94203 (stack75)
        %v94205 = vxor.u32 %v94200, %v94204 (stack76)
        %v94208 = vadd.s32 %v94200, %v94205 (stack65)
        %v94210 = vshll.u32 %v94205, 29 (stack73)
        %v94211 = vshrl.u32 %v94205, 3 (stack74)
        %v94212 = vor.u32 %v94210, %v94211 (stack75)
        %v94213 = vxor.u32 %v94208, %v94212 (stack76)
        %v94216 = vadd.s32 %v94208, %v94213 (stack65)
        %v94218 = vshll.u32 %v94213, 16 (stack73)
        %v94219 = vshrl.u32 %v94213, 16 (stack74)
        %v94220 = vor.u32 %v94218, %v94219 (stack75)
        %v94221 = vxor.u32 %v94216, %v94220 (stack76)
        %v94224 = vadd.s32 %v94216, %v94221 (stack65)
        %v94228 = vadd.s32 %v94224, %v8 (stack65)
        %v94230 = vshll.u32 %v94221, 24 (stack73)
        %v94231 = vshrl.u32 %v94221, 8 (stack74)
        %v94232 = vor.u32 %v94230, %v94231 (stack75)
        %v94233 = vxor.u32 %v94224, %v94232 (stack76)
        %v94236 = vadd.s32 %v94233, %v10 (stack65)
        %v94240 = vadd.s32 %v94236, 2 (stack65)
        %v94244 = vadd.s32 %v94228, %v94240 (stack65)
        %v94246 = vshll.u32 %v94240, 13 (stack73)
        %v94247 = vshrl.u32 %v94240, 19 (stack74)
        %v94248 = vor.u32 %v94246, %v94247 (stack75)
        %v94249 = vxor.u32 %v94244, %v94248 (stack76)
        %v94252 = vadd.s32 %v94244, %v94249 (stack65)
        %v94254 = vshll.u32 %v94249, 15 (stack73)
        %v94255 = vshrl.u32 %v94249, 17 (stack74)
        %v94256 = vor.u32 %v94254, %v94255 (stack75)
        %v94257 = vxor.u32 %v94252, %v94256 (stack76)
        %v94260 = vadd.s32 %v94252, %v94257 (stack65)
        %v94262 = vshll.u32 %v94257, 26 (stack73)
        %v94263 = vshrl.u32 %v94257, 6 (stack74)
        %v94264 = vor.u32 %v94262, %v94263 (stack75)
        %v94265 = vxor.u32 %v94260, %v94264 (stack76)
        %v94268 = vadd.s32 %v94260, %v94265 (stack65)
        %v94272 = vadd.s32 %v94268, %v10 (stack65)
        %v94274 = vshll.u32 %v94265, 6 (stack73)
        %v94275 = vshrl.u32 %v94265, 26 (stack74)
        %v94276 = vor.u32 %v94274, %v94275 (stack75)
        %v94277 = vxor.u32 %v94268, %v94276 (stack76)
        %v94280 = vadd.s32 %v94277, %v9 (stack65)
        %v94284 = vadd.s32 %v94280, 3 (stack65)
        %v94288 = vadd.s32 %v94272, %v94284 (stack65)
        %v94290 = vshll.u32 %v94284, 17 (stack73)
        %v94291 = vshrl.u32 %v94284, 15 (stack74)
        %v94292 = vor.u32 %v94290, %v94291 (stack75)
        %v94293 = vxor.u32 %v94288, %v94292 (stack76)
        %v94296 = vadd.s32 %v94288, %v94293 (stack65)
        %v94298 = vshll.u32 %v94293, 29 (stack73)
        %v94299 = vshrl.u32 %v94293, 3 (stack74)
        %v94300 = vor.u32 %v94298, %v94299 (stack75)
        %v94301 = vxor.u32 %v94296, %v94300 (stack76)
        %v94304 = vadd.s32 %v94296, %v94301 (stack65)
        %v94306 = vshll.u32 %v94301, 16 (stack73)
        %v94307 = vshrl.u32 %v94301, 16 (stack74)
        %v94308 = vor.u32 %v94306, %v94307 (stack75)
        %v94309 = vxor.u32 %v94304, %v94308 (stack76)
        %v94312 = vadd.s32 %v94304, %v94309 (stack65)
        %v94316 = vadd.s32 %v94312, %v9 (stack65)
        %v94318 = vshll.u32 %v94309, 24 (stack73)
        %v94319 = vshrl.u32 %v94309, 8 (stack74)
        %v94320 = vor.u32 %v94318, %v94319 (stack75)
        %v94321 = vxor.u32 %v94312, %v94320 (stack76)
        %v94324 = vadd.s32 %v94321, %v8 (stack65)
        %v94328 = vadd.s32 %v94324, 4 (stack65)
        %v94332 = vadd.s32 %v94316, %v94328 (stack65)
        %v94334 = vshll.u32 %v94328, 13 (stack73)
        %v94335 = vshrl.u32 %v94328, 19 (stack74)
        %v94336 = vor.u32 %v94334, %v94335 (stack75)
        %v94337 = vxor.u32 %v94332, %v94336 (stack76)
        %v94340 = vadd.s32 %v94332, %v94337 (stack65)
        %v94342 = vshll.u32 %v94337, 15 (stack73)
        %v94343 = vshrl.u32 %v94337, 17 (stack74)
        %v94344 = vor.u32 %v94342, %v94343 (stack75)
        %v94345 = vxor.u32 %v94340, %v94344 (stack76)
        %v94348 = vadd.s32 %v94340, %v94345 (stack65)
        %v94350 = vshll.u32 %v94345, 26 (stack73)
        %v94351 = vshrl.u32 %v94345, 6 (stack74)
        %v94352 = vor.u32 %v94350, %v94351 (stack75)
        %v94353 = vxor.u32 %v94348, %v94352 (stack76)
        %v94356 = vadd.s32 %v94348, %v94353 (stack65)
        %v94360 = vadd.s32 %v94356, %v8 (stack65)
        %v94362 = vshll.u32 %v94353, 6 (stack73)
        %v94363 = vshrl.u32 %v94353, 26 (stack74)
        %v94364 = vor.u32 %v94362, %v94363 (stack75)
        %v94365 = vxor.u32 %v94356, %v94364 (stack76)
        %v94368 = vadd.s32 %v94365, %v10 (stack65)
        %v94372 = vadd.s32 %v94368, 5 (stack65)
        %v94374 = vxor.u32 %v94360, %v94372 (stack76)
        %v94375 = vand.u32.u8 %v94374, 255 (stack77)
        %v94376 = vand.u32 %v94375, 65535 (stack78)
        %v94377 = vshrl.u32 %v94376, 1 (stack79)
        %v94378 = vor.u32 %v94377, 16256 (stack75)
        %v94379 = vand.u32.u16 %v94378, 65535 (stack80)
        %v94380 = vunpack.i.l.bf16 %v94379 (stack81)
        %v94384 = vadd.f32 %v94380, -1.0 (stack82)
        %v94388 = vmul.f32 %v94384, 2.0 (stack83)
        %v94392 = vadd.f32 %v94388, -0.99609375 (stack82)
        %v94396 = vmax.f32 -0.99609375, %v94392 (stack84)
        %v94398 = vand.u32 2147483647, %v94396 (stack85)
        %vm94401 = vcmp.eq.f32.partialorder %v94398, 1.0 (stack86)
        %v94406 = vmul.f32 %v94396, inf (stack83)
        %v94408 = vxor.u32 %v94396, 2147483648 (stack87)
        %v94411 = vmul.f32 %v94396, %v94408 (stack83)
        %v94413 = vadd.f32 %v94411, 1.0 (stack88)
        %v94414 = vlog2.pop %v94413 (stack89)
        %v94415 = vmul.f32 %v94414, 0.6931472 (stack90)
        %v94416 = vmul.f32 -0.5, %v94411 (stack91)
        %v94417 = vadd.f32 %v94416, 1.0 (stack92)
        %v94418 = vmul.f32 %v94417, %v94411 (stack93)
        %v94419 = vand.u32 2147483647, %v94411 (stack94)
        %vm94420 = vcmp.lt.f32.partialorder %v94419, 0.0004427343 (stack95)
        %v94421 = vsel /*vm=*/%vm94420, /*on_true_vy=*/%v94418, /*on_false_vx=*/%v94415 (stack96)
        %v94422 = vxor.u32 %v94421, 2147483648 (stack87)
        %vm94425 = vcmp.lt.f32.partialorder %v94422, 5.0 (stack86)
        %v94430 = vsel /*vm=*/%vm94425, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v94434 = vsel /*vm=*/%vm94425, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v94438 = vsel /*vm=*/%vm94425, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v94442 = vsel /*vm=*/%vm94425, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v94446 = vsel /*vm=*/%vm94425, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v94450 = vsel /*vm=*/%vm94425, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v94454 = vsel /*vm=*/%vm94425, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v94458 = vsel /*vm=*/%vm94425, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v94462 = vsel /*vm=*/%vm94425, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v94466 = vadd.f32 %v94422, -2.5 (stack82)
        %v94468 = vrsqrt.pop %v94422 (stack97)
        %v94469 = vmul.f32 %v94422, %v94468 (stack98)
        %vm94470 = vcmp.eq.f32.partialorder %v94422, inf (stack99)
        %v94471 = vsel /*vm=*/%vm94470, /*on_true_vy=*/%v94422, /*on_false_vx=*/%v94469 (stack100)
        %vm94472 = vcmp.eq.f32.partialorder %v94422, 0.0 (stack101)
        %v94473 = vand.u32 %v94422, 2147483648 (stack102)
        %v94474 = vsel /*vm=*/%vm94472, /*on_true_vy=*/%v94473, /*on_false_vx=*/%v94471 (stack103)
        %v94477 = vadd.f32 %v94474, -3.0 (stack82)
        %v94481 = vsel /*vm=*/%vm94425, /*on_true_vy=*/%v94466, /*on_false_vx=*/%v94477 (stack72)
        %v94485 = vmul.f32 %v94462, %v94481 (stack83)
        %v94489 = vadd.f32 %v94458, %v94485 (stack82)
        %v94493 = vmul.f32 %v94489, %v94481 (stack83)
        %v94497 = vadd.f32 %v94454, %v94493 (stack82)
        %v94501 = vmul.f32 %v94497, %v94481 (stack83)
        %v94505 = vadd.f32 %v94450, %v94501 (stack82)
        %v94509 = vmul.f32 %v94505, %v94481 (stack83)
        %v94513 = vadd.f32 %v94446, %v94509 (stack82)
        %v94517 = vmul.f32 %v94513, %v94481 (stack83)
        %v94521 = vadd.f32 %v94442, %v94517 (stack82)
        %v94525 = vmul.f32 %v94521, %v94481 (stack83)
        %v94529 = vadd.f32 %v94438, %v94525 (stack82)
        %v94533 = vmul.f32 %v94529, %v94481 (stack83)
        %v94537 = vadd.f32 %v94434, %v94533 (stack82)
        %v94541 = vmul.f32 %v94537, %v94481 (stack83)
        %v94545 = vadd.f32 %v94430, %v94541 (stack82)
        %v94549 = vmul.f32 %v94545, %v94396 (stack83)
        %v94553 = vsel /*vm=*/%vm94401, /*on_true_vy=*/%v94406, /*on_false_vx=*/%v94549 (stack72)
        %v94557 = vmul.f32 %v94553, 1.4140625 (stack83)
        %s94559 = scalar_lea.vmem %s280, 228 [#allocation0] (stack107)
        %v94560 = vpack.c.bf16 0.0, %v94557 (stack104)
        %94561 = vst [vmem:[%s94559] sm:$0xf] /*vst_source=*/%v94560 (stack105)
        %v94564 = vadd.s32 %v1381, %v93639 (stack65)
        %s94566 = smul.u32 128, %s27 (stack66)
        %v94567 = vlaneseq (stack67)
        %v94568 = vand.u32 %v94567, 127 (stack68)
        %v94569 = vstv %s94566 (stack69)
        %v94570 = vadd.s32 %v94568, %v94569 (stack70)
        %v94574 = vadd.s32 %v94564, %v94570 (stack65)
        %vm94578 = vcmp.lt.u32.totalorder %v94574, %v94564 (stack71)
        %vm94583 = vcmp.lt.u32.totalorder %v94564, %v1381 (stack71)
        %v94588 = vadd.s32 %v1368, %v93622 (stack65)
        %v94592 = vadd.s32 %v94588, 1 (stack65)
        %v94596 = vsel /*vm=*/%vm94583, /*on_true_vy=*/%v94592, /*on_false_vx=*/%v94588 (stack72)
        %v94600 = vadd.s32 %v94596, 1 (stack65)
        %v94604 = vsel /*vm=*/%vm94578, /*on_true_vy=*/%v94600, /*on_false_vx=*/%v94596 (stack72)
        %v94609 = vadd.s32 %v94604, %v10 (stack65)
        %v94613 = vadd.s32 %v94574, %v9 (stack65)
        %v94617 = vadd.s32 %v94609, %v94613 (stack65)
        %v94619 = vshll.u32 %v94613, 13 (stack73)
        %v94620 = vshrl.u32 %v94613, 19 (stack74)
        %v94621 = vor.u32 %v94619, %v94620 (stack75)
        %v94622 = vxor.u32 %v94617, %v94621 (stack76)
        %v94625 = vadd.s32 %v94617, %v94622 (stack65)
        %v94627 = vshll.u32 %v94622, 15 (stack73)
        %v94628 = vshrl.u32 %v94622, 17 (stack74)
        %v94629 = vor.u32 %v94627, %v94628 (stack75)
        %v94630 = vxor.u32 %v94625, %v94629 (stack76)
        %v94633 = vadd.s32 %v94625, %v94630 (stack65)
        %v94635 = vshll.u32 %v94630, 26 (stack73)
        %v94636 = vshrl.u32 %v94630, 6 (stack74)
        %v94637 = vor.u32 %v94635, %v94636 (stack75)
        %v94638 = vxor.u32 %v94633, %v94637 (stack76)
        %v94641 = vadd.s32 %v94633, %v94638 (stack65)
        %v94645 = vadd.s32 %v94641, %v9 (stack65)
        %v94647 = vshll.u32 %v94638, 6 (stack73)
        %v94648 = vshrl.u32 %v94638, 26 (stack74)
        %v94649 = vor.u32 %v94647, %v94648 (stack75)
        %v94650 = vxor.u32 %v94641, %v94649 (stack76)
        %v94653 = vadd.s32 %v94650, %v8 (stack65)
        %v94657 = vadd.s32 %v94653, 1 (stack65)
        %v94661 = vadd.s32 %v94645, %v94657 (stack65)
        %v94663 = vshll.u32 %v94657, 17 (stack73)
        %v94664 = vshrl.u32 %v94657, 15 (stack74)
        %v94665 = vor.u32 %v94663, %v94664 (stack75)
        %v94666 = vxor.u32 %v94661, %v94665 (stack76)
        %v94669 = vadd.s32 %v94661, %v94666 (stack65)
        %v94671 = vshll.u32 %v94666, 29 (stack73)
        %v94672 = vshrl.u32 %v94666, 3 (stack74)
        %v94673 = vor.u32 %v94671, %v94672 (stack75)
        %v94674 = vxor.u32 %v94669, %v94673 (stack76)
        %v94677 = vadd.s32 %v94669, %v94674 (stack65)
        %v94679 = vshll.u32 %v94674, 16 (stack73)
        %v94680 = vshrl.u32 %v94674, 16 (stack74)
        %v94681 = vor.u32 %v94679, %v94680 (stack75)
        %v94682 = vxor.u32 %v94677, %v94681 (stack76)
        %v94685 = vadd.s32 %v94677, %v94682 (stack65)
        %v94689 = vadd.s32 %v94685, %v8 (stack65)
        %v94691 = vshll.u32 %v94682, 24 (stack73)
        %v94692 = vshrl.u32 %v94682, 8 (stack74)
        %v94693 = vor.u32 %v94691, %v94692 (stack75)
        %v94694 = vxor.u32 %v94685, %v94693 (stack76)
        %v94697 = vadd.s32 %v94694, %v10 (stack65)
        %v94701 = vadd.s32 %v94697, 2 (stack65)
        %v94705 = vadd.s32 %v94689, %v94701 (stack65)
        %v94707 = vshll.u32 %v94701, 13 (stack73)
        %v94708 = vshrl.u32 %v94701, 19 (stack74)
        %v94709 = vor.u32 %v94707, %v94708 (stack75)
        %v94710 = vxor.u32 %v94705, %v94709 (stack76)
        %v94713 = vadd.s32 %v94705, %v94710 (stack65)
        %v94715 = vshll.u32 %v94710, 15 (stack73)
        %v94716 = vshrl.u32 %v94710, 17 (stack74)
        %v94717 = vor.u32 %v94715, %v94716 (stack75)
        %v94718 = vxor.u32 %v94713, %v94717 (stack76)
        %v94721 = vadd.s32 %v94713, %v94718 (stack65)
        %v94723 = vshll.u32 %v94718, 26 (stack73)
        %v94724 = vshrl.u32 %v94718, 6 (stack74)
        %v94725 = vor.u32 %v94723, %v94724 (stack75)
        %v94726 = vxor.u32 %v94721, %v94725 (stack76)
        %v94729 = vadd.s32 %v94721, %v94726 (stack65)
        %v94733 = vadd.s32 %v94729, %v10 (stack65)
        %v94735 = vshll.u32 %v94726, 6 (stack73)
        %v94736 = vshrl.u32 %v94726, 26 (stack74)
        %v94737 = vor.u32 %v94735, %v94736 (stack75)
        %v94738 = vxor.u32 %v94729, %v94737 (stack76)
        %v94741 = vadd.s32 %v94738, %v9 (stack65)
        %v94745 = vadd.s32 %v94741, 3 (stack65)
        %v94749 = vadd.s32 %v94733, %v94745 (stack65)
        %v94751 = vshll.u32 %v94745, 17 (stack73)
        %v94752 = vshrl.u32 %v94745, 15 (stack74)
        %v94753 = vor.u32 %v94751, %v94752 (stack75)
        %v94754 = vxor.u32 %v94749, %v94753 (stack76)
        %v94757 = vadd.s32 %v94749, %v94754 (stack65)
        %v94759 = vshll.u32 %v94754, 29 (stack73)
        %v94760 = vshrl.u32 %v94754, 3 (stack74)
        %v94761 = vor.u32 %v94759, %v94760 (stack75)
        %v94762 = vxor.u32 %v94757, %v94761 (stack76)
        %v94765 = vadd.s32 %v94757, %v94762 (stack65)
        %v94767 = vshll.u32 %v94762, 16 (stack73)
        %v94768 = vshrl.u32 %v94762, 16 (stack74)
        %v94769 = vor.u32 %v94767, %v94768 (stack75)
        %v94770 = vxor.u32 %v94765, %v94769 (stack76)
        %v94773 = vadd.s32 %v94765, %v94770 (stack65)
        %v94777 = vadd.s32 %v94773, %v9 (stack65)
        %v94779 = vshll.u32 %v94770, 24 (stack73)
        %v94780 = vshrl.u32 %v94770, 8 (stack74)
        %v94781 = vor.u32 %v94779, %v94780 (stack75)
        %v94782 = vxor.u32 %v94773, %v94781 (stack76)
        %v94785 = vadd.s32 %v94782, %v8 (stack65)
        %v94789 = vadd.s32 %v94785, 4 (stack65)
        %v94793 = vadd.s32 %v94777, %v94789 (stack65)
        %v94795 = vshll.u32 %v94789, 13 (stack73)
        %v94796 = vshrl.u32 %v94789, 19 (stack74)
        %v94797 = vor.u32 %v94795, %v94796 (stack75)
        %v94798 = vxor.u32 %v94793, %v94797 (stack76)
        %v94801 = vadd.s32 %v94793, %v94798 (stack65)
        %v94803 = vshll.u32 %v94798, 15 (stack73)
        %v94804 = vshrl.u32 %v94798, 17 (stack74)
        %v94805 = vor.u32 %v94803, %v94804 (stack75)
        %v94806 = vxor.u32 %v94801, %v94805 (stack76)
        %v94809 = vadd.s32 %v94801, %v94806 (stack65)
        %v94811 = vshll.u32 %v94806, 26 (stack73)
        %v94812 = vshrl.u32 %v94806, 6 (stack74)
        %v94813 = vor.u32 %v94811, %v94812 (stack75)
        %v94814 = vxor.u32 %v94809, %v94813 (stack76)
        %v94817 = vadd.s32 %v94809, %v94814 (stack65)
        %v94821 = vadd.s32 %v94817, %v8 (stack65)
        %v94823 = vshll.u32 %v94814, 6 (stack73)
        %v94824 = vshrl.u32 %v94814, 26 (stack74)
        %v94825 = vor.u32 %v94823, %v94824 (stack75)
        %v94826 = vxor.u32 %v94817, %v94825 (stack76)
        %v94829 = vadd.s32 %v94826, %v10 (stack65)
        %v94833 = vadd.s32 %v94829, 5 (stack65)
        %v94835 = vxor.u32 %v94821, %v94833 (stack76)
        %v94836 = vand.u32.u8 %v94835, 255 (stack77)
        %v94837 = vand.u32 %v94836, 65535 (stack78)
        %v94838 = vshrl.u32 %v94837, 1 (stack79)
        %v94839 = vor.u32 %v94838, 16256 (stack75)
        %v94840 = vand.u32.u16 %v94839, 65535 (stack80)
        %v94841 = vunpack.i.l.bf16 %v94840 (stack81)
        %v94845 = vadd.f32 %v94841, -1.0 (stack82)
        %v94849 = vmul.f32 %v94845, 2.0 (stack83)
        %v94853 = vadd.f32 %v94849, -0.99609375 (stack82)
        %v94857 = vmax.f32 -0.99609375, %v94853 (stack84)
        %v94859 = vand.u32 2147483647, %v94857 (stack85)
        %vm94862 = vcmp.eq.f32.partialorder %v94859, 1.0 (stack86)
        %v94867 = vmul.f32 %v94857, inf (stack83)
        %v94869 = vxor.u32 %v94857, 2147483648 (stack87)
        %v94872 = vmul.f32 %v94857, %v94869 (stack83)
        %v94874 = vadd.f32 %v94872, 1.0 (stack88)
        %v94875 = vlog2.pop %v94874 (stack89)
        %v94876 = vmul.f32 %v94875, 0.6931472 (stack90)
        %v94877 = vmul.f32 -0.5, %v94872 (stack91)
        %v94878 = vadd.f32 %v94877, 1.0 (stack92)
        %v94879 = vmul.f32 %v94878, %v94872 (stack93)
        %v94880 = vand.u32 2147483647, %v94872 (stack94)
        %vm94881 = vcmp.lt.f32.partialorder %v94880, 0.0004427343 (stack95)
        %v94882 = vsel /*vm=*/%vm94881, /*on_true_vy=*/%v94879, /*on_false_vx=*/%v94876 (stack96)
        %v94883 = vxor.u32 %v94882, 2147483648 (stack87)
        %vm94886 = vcmp.lt.f32.partialorder %v94883, 5.0 (stack86)
        %v94891 = vsel /*vm=*/%vm94886, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v94895 = vsel /*vm=*/%vm94886, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v94899 = vsel /*vm=*/%vm94886, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v94903 = vsel /*vm=*/%vm94886, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v94907 = vsel /*vm=*/%vm94886, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v94911 = vsel /*vm=*/%vm94886, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v94915 = vsel /*vm=*/%vm94886, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v94919 = vsel /*vm=*/%vm94886, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v94923 = vsel /*vm=*/%vm94886, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v94927 = vadd.f32 %v94883, -2.5 (stack82)
        %v94929 = vrsqrt.pop %v94883 (stack97)
        %v94930 = vmul.f32 %v94883, %v94929 (stack98)
        %vm94931 = vcmp.eq.f32.partialorder %v94883, inf (stack99)
        %v94932 = vsel /*vm=*/%vm94931, /*on_true_vy=*/%v94883, /*on_false_vx=*/%v94930 (stack100)
        %vm94933 = vcmp.eq.f32.partialorder %v94883, 0.0 (stack101)
        %v94934 = vand.u32 %v94883, 2147483648 (stack102)
        %v94935 = vsel /*vm=*/%vm94933, /*on_true_vy=*/%v94934, /*on_false_vx=*/%v94932 (stack103)
        %v94938 = vadd.f32 %v94935, -3.0 (stack82)
        %v94942 = vsel /*vm=*/%vm94886, /*on_true_vy=*/%v94927, /*on_false_vx=*/%v94938 (stack72)
        %v94946 = vmul.f32 %v94923, %v94942 (stack83)
        %v94950 = vadd.f32 %v94919, %v94946 (stack82)
        %v94954 = vmul.f32 %v94950, %v94942 (stack83)
        %v94958 = vadd.f32 %v94915, %v94954 (stack82)
        %v94962 = vmul.f32 %v94958, %v94942 (stack83)
        %v94966 = vadd.f32 %v94911, %v94962 (stack82)
        %v94970 = vmul.f32 %v94966, %v94942 (stack83)
        %v94974 = vadd.f32 %v94907, %v94970 (stack82)
        %v94978 = vmul.f32 %v94974, %v94942 (stack83)
        %v94982 = vadd.f32 %v94903, %v94978 (stack82)
        %v94986 = vmul.f32 %v94982, %v94942 (stack83)
        %v94990 = vadd.f32 %v94899, %v94986 (stack82)
        %v94994 = vmul.f32 %v94990, %v94942 (stack83)
        %v94998 = vadd.f32 %v94895, %v94994 (stack82)
        %v95002 = vmul.f32 %v94998, %v94942 (stack83)
        %v95006 = vadd.f32 %v94891, %v95002 (stack82)
        %v95010 = vmul.f32 %v95006, %v94857 (stack83)
        %v95014 = vsel /*vm=*/%vm94862, /*on_true_vy=*/%v94867, /*on_false_vx=*/%v95010 (stack72)
        %v95018 = vmul.f32 %v95014, 1.4140625 (stack83)
        %s95020 = scalar_lea.vmem %s280, 356 [#allocation0] (stack107)
        %v95021 = vpack.c.bf16 0.0, %v95018 (stack104)
        %95022 = vst [vmem:[%s95020] sm:$0xf] /*vst_source=*/%v95021 (stack105)
        %v95025 = vadd.s32 %v1868, %v93639 (stack65)
        %s95027 = smul.u32 128, %s27 (stack66)
        %v95028 = vlaneseq (stack67)
        %v95029 = vand.u32 %v95028, 127 (stack68)
        %v95030 = vstv %s95027 (stack69)
        %v95031 = vadd.s32 %v95029, %v95030 (stack70)
        %v95035 = vadd.s32 %v95025, %v95031 (stack65)
        %vm95039 = vcmp.lt.u32.totalorder %v95035, %v95025 (stack71)
        %vm95044 = vcmp.lt.u32.totalorder %v95025, %v1868 (stack71)
        %v95049 = vadd.s32 %v1855, %v93622 (stack65)
        %v95053 = vadd.s32 %v95049, 1 (stack65)
        %v95057 = vsel /*vm=*/%vm95044, /*on_true_vy=*/%v95053, /*on_false_vx=*/%v95049 (stack72)
        %v95061 = vadd.s32 %v95057, 1 (stack65)
        %v95065 = vsel /*vm=*/%vm95039, /*on_true_vy=*/%v95061, /*on_false_vx=*/%v95057 (stack72)
        %v95070 = vadd.s32 %v95065, %v10 (stack65)
        %v95074 = vadd.s32 %v95035, %v9 (stack65)
        %v95078 = vadd.s32 %v95070, %v95074 (stack65)
        %v95080 = vshll.u32 %v95074, 13 (stack73)
        %v95081 = vshrl.u32 %v95074, 19 (stack74)
        %v95082 = vor.u32 %v95080, %v95081 (stack75)
        %v95083 = vxor.u32 %v95078, %v95082 (stack76)
        %v95086 = vadd.s32 %v95078, %v95083 (stack65)
        %v95088 = vshll.u32 %v95083, 15 (stack73)
        %v95089 = vshrl.u32 %v95083, 17 (stack74)
        %v95090 = vor.u32 %v95088, %v95089 (stack75)
        %v95091 = vxor.u32 %v95086, %v95090 (stack76)
        %v95094 = vadd.s32 %v95086, %v95091 (stack65)
        %v95096 = vshll.u32 %v95091, 26 (stack73)
        %v95097 = vshrl.u32 %v95091, 6 (stack74)
        %v95098 = vor.u32 %v95096, %v95097 (stack75)
        %v95099 = vxor.u32 %v95094, %v95098 (stack76)
        %v95102 = vadd.s32 %v95094, %v95099 (stack65)
        %v95106 = vadd.s32 %v95102, %v9 (stack65)
        %v95108 = vshll.u32 %v95099, 6 (stack73)
        %v95109 = vshrl.u32 %v95099, 26 (stack74)
        %v95110 = vor.u32 %v95108, %v95109 (stack75)
        %v95111 = vxor.u32 %v95102, %v95110 (stack76)
        %v95114 = vadd.s32 %v95111, %v8 (stack65)
        %v95118 = vadd.s32 %v95114, 1 (stack65)
        %v95122 = vadd.s32 %v95106, %v95118 (stack65)
        %v95124 = vshll.u32 %v95118, 17 (stack73)
        %v95125 = vshrl.u32 %v95118, 15 (stack74)
        %v95126 = vor.u32 %v95124, %v95125 (stack75)
        %v95127 = vxor.u32 %v95122, %v95126 (stack76)
        %v95130 = vadd.s32 %v95122, %v95127 (stack65)
        %v95132 = vshll.u32 %v95127, 29 (stack73)
        %v95133 = vshrl.u32 %v95127, 3 (stack74)
        %v95134 = vor.u32 %v95132, %v95133 (stack75)
        %v95135 = vxor.u32 %v95130, %v95134 (stack76)
        %v95138 = vadd.s32 %v95130, %v95135 (stack65)
        %v95140 = vshll.u32 %v95135, 16 (stack73)
        %v95141 = vshrl.u32 %v95135, 16 (stack74)
        %v95142 = vor.u32 %v95140, %v95141 (stack75)
        %v95143 = vxor.u32 %v95138, %v95142 (stack76)
        %v95146 = vadd.s32 %v95138, %v95143 (stack65)
        %v95150 = vadd.s32 %v95146, %v8 (stack65)
        %v95152 = vshll.u32 %v95143, 24 (stack73)
        %v95153 = vshrl.u32 %v95143, 8 (stack74)
        %v95154 = vor.u32 %v95152, %v95153 (stack75)
        %v95155 = vxor.u32 %v95146, %v95154 (stack76)
        %v95158 = vadd.s32 %v95155, %v10 (stack65)
        %v95162 = vadd.s32 %v95158, 2 (stack65)
        %v95166 = vadd.s32 %v95150, %v95162 (stack65)
        %v95168 = vshll.u32 %v95162, 13 (stack73)
        %v95169 = vshrl.u32 %v95162, 19 (stack74)
        %v95170 = vor.u32 %v95168, %v95169 (stack75)
        %v95171 = vxor.u32 %v95166, %v95170 (stack76)
        %v95174 = vadd.s32 %v95166, %v95171 (stack65)
        %v95176 = vshll.u32 %v95171, 15 (stack73)
        %v95177 = vshrl.u32 %v95171, 17 (stack74)
        %v95178 = vor.u32 %v95176, %v95177 (stack75)
        %v95179 = vxor.u32 %v95174, %v95178 (stack76)
        %v95182 = vadd.s32 %v95174, %v95179 (stack65)
        %v95184 = vshll.u32 %v95179, 26 (stack73)
        %v95185 = vshrl.u32 %v95179, 6 (stack74)
        %v95186 = vor.u32 %v95184, %v95185 (stack75)
        %v95187 = vxor.u32 %v95182, %v95186 (stack76)
        %v95190 = vadd.s32 %v95182, %v95187 (stack65)
        %v95194 = vadd.s32 %v95190, %v10 (stack65)
        %v95196 = vshll.u32 %v95187, 6 (stack73)
        %v95197 = vshrl.u32 %v95187, 26 (stack74)
        %v95198 = vor.u32 %v95196, %v95197 (stack75)
        %v95199 = vxor.u32 %v95190, %v95198 (stack76)
        %v95202 = vadd.s32 %v95199, %v9 (stack65)
        %v95206 = vadd.s32 %v95202, 3 (stack65)
        %v95210 = vadd.s32 %v95194, %v95206 (stack65)
        %v95212 = vshll.u32 %v95206, 17 (stack73)
        %v95213 = vshrl.u32 %v95206, 15 (stack74)
        %v95214 = vor.u32 %v95212, %v95213 (stack75)
        %v95215 = vxor.u32 %v95210, %v95214 (stack76)
        %v95218 = vadd.s32 %v95210, %v95215 (stack65)
        %v95220 = vshll.u32 %v95215, 29 (stack73)
        %v95221 = vshrl.u32 %v95215, 3 (stack74)
        %v95222 = vor.u32 %v95220, %v95221 (stack75)
        %v95223 = vxor.u32 %v95218, %v95222 (stack76)
        %v95226 = vadd.s32 %v95218, %v95223 (stack65)
        %v95228 = vshll.u32 %v95223, 16 (stack73)
        %v95229 = vshrl.u32 %v95223, 16 (stack74)
        %v95230 = vor.u32 %v95228, %v95229 (stack75)
        %v95231 = vxor.u32 %v95226, %v95230 (stack76)
        %v95234 = vadd.s32 %v95226, %v95231 (stack65)
        %v95238 = vadd.s32 %v95234, %v9 (stack65)
        %v95240 = vshll.u32 %v95231, 24 (stack73)
        %v95241 = vshrl.u32 %v95231, 8 (stack74)
        %v95242 = vor.u32 %v95240, %v95241 (stack75)
        %v95243 = vxor.u32 %v95234, %v95242 (stack76)
        %v95246 = vadd.s32 %v95243, %v8 (stack65)
        %v95250 = vadd.s32 %v95246, 4 (stack65)
        %v95254 = vadd.s32 %v95238, %v95250 (stack65)
        %v95256 = vshll.u32 %v95250, 13 (stack73)
        %v95257 = vshrl.u32 %v95250, 19 (stack74)
        %v95258 = vor.u32 %v95256, %v95257 (stack75)
        %v95259 = vxor.u32 %v95254, %v95258 (stack76)
        %v95262 = vadd.s32 %v95254, %v95259 (stack65)
        %v95264 = vshll.u32 %v95259, 15 (stack73)
        %v95265 = vshrl.u32 %v95259, 17 (stack74)
        %v95266 = vor.u32 %v95264, %v95265 (stack75)
        %v95267 = vxor.u32 %v95262, %v95266 (stack76)
        %v95270 = vadd.s32 %v95262, %v95267 (stack65)
        %v95272 = vshll.u32 %v95267, 26 (stack73)
        %v95273 = vshrl.u32 %v95267, 6 (stack74)
        %v95274 = vor.u32 %v95272, %v95273 (stack75)
        %v95275 = vxor.u32 %v95270, %v95274 (stack76)
        %v95278 = vadd.s32 %v95270, %v95275 (stack65)
        %v95282 = vadd.s32 %v95278, %v8 (stack65)
        %v95284 = vshll.u32 %v95275, 6 (stack73)
        %v95285 = vshrl.u32 %v95275, 26 (stack74)
        %v95286 = vor.u32 %v95284, %v95285 (stack75)
        %v95287 = vxor.u32 %v95278, %v95286 (stack76)
        %v95290 = vadd.s32 %v95287, %v10 (stack65)
        %v95294 = vadd.s32 %v95290, 5 (stack65)
        %v95296 = vxor.u32 %v95282, %v95294 (stack76)
        %v95297 = vand.u32.u8 %v95296, 255 (stack77)
        %v95298 = vand.u32 %v95297, 65535 (stack78)
        %v95299 = vshrl.u32 %v95298, 1 (stack79)
        %v95300 = vor.u32 %v95299, 16256 (stack75)
        %v95301 = vand.u32.u16 %v95300, 65535 (stack80)
        %v95302 = vunpack.i.l.bf16 %v95301 (stack81)
        %v95306 = vadd.f32 %v95302, -1.0 (stack82)
        %v95310 = vmul.f32 %v95306, 2.0 (stack83)
        %v95314 = vadd.f32 %v95310, -0.99609375 (stack82)
        %v95318 = vmax.f32 -0.99609375, %v95314 (stack84)
        %v95320 = vand.u32 2147483647, %v95318 (stack85)
        %vm95323 = vcmp.eq.f32.partialorder %v95320, 1.0 (stack86)
        %v95328 = vmul.f32 %v95318, inf (stack83)
        %v95330 = vxor.u32 %v95318, 2147483648 (stack87)
        %v95333 = vmul.f32 %v95318, %v95330 (stack83)
        %v95335 = vadd.f32 %v95333, 1.0 (stack88)
        %v95336 = vlog2.pop %v95335 (stack89)
        %v95337 = vmul.f32 %v95336, 0.6931472 (stack90)
        %v95338 = vmul.f32 -0.5, %v95333 (stack91)
        %v95339 = vadd.f32 %v95338, 1.0 (stack92)
        %v95340 = vmul.f32 %v95339, %v95333 (stack93)
        %v95341 = vand.u32 2147483647, %v95333 (stack94)
        %vm95342 = vcmp.lt.f32.partialorder %v95341, 0.0004427343 (stack95)
        %v95343 = vsel /*vm=*/%vm95342, /*on_true_vy=*/%v95340, /*on_false_vx=*/%v95337 (stack96)
        %v95344 = vxor.u32 %v95343, 2147483648 (stack87)
        %vm95347 = vcmp.lt.f32.partialorder %v95344, 5.0 (stack86)
        %v95352 = vsel /*vm=*/%vm95347, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v95356 = vsel /*vm=*/%vm95347, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v95360 = vsel /*vm=*/%vm95347, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v95364 = vsel /*vm=*/%vm95347, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v95368 = vsel /*vm=*/%vm95347, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v95372 = vsel /*vm=*/%vm95347, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v95376 = vsel /*vm=*/%vm95347, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v95380 = vsel /*vm=*/%vm95347, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v95384 = vsel /*vm=*/%vm95347, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v95388 = vadd.f32 %v95344, -2.5 (stack82)
        %v95390 = vrsqrt.pop %v95344 (stack97)
        %v95391 = vmul.f32 %v95344, %v95390 (stack98)
        %vm95392 = vcmp.eq.f32.partialorder %v95344, inf (stack99)
        %v95393 = vsel /*vm=*/%vm95392, /*on_true_vy=*/%v95344, /*on_false_vx=*/%v95391 (stack100)
        %vm95394 = vcmp.eq.f32.partialorder %v95344, 0.0 (stack101)
        %v95395 = vand.u32 %v95344, 2147483648 (stack102)
        %v95396 = vsel /*vm=*/%vm95394, /*on_true_vy=*/%v95395, /*on_false_vx=*/%v95393 (stack103)
        %v95399 = vadd.f32 %v95396, -3.0 (stack82)
        %v95403 = vsel /*vm=*/%vm95347, /*on_true_vy=*/%v95388, /*on_false_vx=*/%v95399 (stack72)
        %v95407 = vmul.f32 %v95384, %v95403 (stack83)
        %v95411 = vadd.f32 %v95380, %v95407 (stack82)
        %v95415 = vmul.f32 %v95411, %v95403 (stack83)
        %v95419 = vadd.f32 %v95376, %v95415 (stack82)
        %v95423 = vmul.f32 %v95419, %v95403 (stack83)
        %v95427 = vadd.f32 %v95372, %v95423 (stack82)
        %v95431 = vmul.f32 %v95427, %v95403 (stack83)
        %v95435 = vadd.f32 %v95368, %v95431 (stack82)
        %v95439 = vmul.f32 %v95435, %v95403 (stack83)
        %v95443 = vadd.f32 %v95364, %v95439 (stack82)
        %v95447 = vmul.f32 %v95443, %v95403 (stack83)
        %v95451 = vadd.f32 %v95360, %v95447 (stack82)
        %v95455 = vmul.f32 %v95451, %v95403 (stack83)
        %v95459 = vadd.f32 %v95356, %v95455 (stack82)
        %v95463 = vmul.f32 %v95459, %v95403 (stack83)
        %v95467 = vadd.f32 %v95352, %v95463 (stack82)
        %v95471 = vmul.f32 %v95467, %v95318 (stack83)
        %v95475 = vsel /*vm=*/%vm95323, /*on_true_vy=*/%v95328, /*on_false_vx=*/%v95471 (stack72)
        %v95479 = vmul.f32 %v95475, 1.4140625 (stack83)
        %s95481 = scalar_lea.vmem %s280, 484 [#allocation0] (stack107)
        %v95482 = vpack.c.bf16 0.0, %v95479 (stack104)
        %95483 = vst [vmem:[%s95481] sm:$0xf] /*vst_source=*/%v95482 (stack105)
        %v95486 = vadd.s32 %v2355, %v93639 (stack65)
        %s95488 = smul.u32 128, %s27 (stack66)
        %v95489 = vlaneseq (stack67)
        %v95490 = vand.u32 %v95489, 127 (stack68)
        %v95491 = vstv %s95488 (stack69)
        %v95492 = vadd.s32 %v95490, %v95491 (stack70)
        %v95496 = vadd.s32 %v95486, %v95492 (stack65)
        %vm95500 = vcmp.lt.u32.totalorder %v95496, %v95486 (stack71)
        %vm95505 = vcmp.lt.u32.totalorder %v95486, %v2355 (stack71)
        %v95510 = vadd.s32 %v2342, %v93622 (stack65)
        %v95514 = vadd.s32 %v95510, 1 (stack65)
        %v95518 = vsel /*vm=*/%vm95505, /*on_true_vy=*/%v95514, /*on_false_vx=*/%v95510 (stack72)
        %v95522 = vadd.s32 %v95518, 1 (stack65)
        %v95526 = vsel /*vm=*/%vm95500, /*on_true_vy=*/%v95522, /*on_false_vx=*/%v95518 (stack72)
        %v95531 = vadd.s32 %v95526, %v10 (stack65)
        %v95535 = vadd.s32 %v95496, %v9 (stack65)
        %v95539 = vadd.s32 %v95531, %v95535 (stack65)
        %v95541 = vshll.u32 %v95535, 13 (stack73)
        %v95542 = vshrl.u32 %v95535, 19 (stack74)
        %v95543 = vor.u32 %v95541, %v95542 (stack75)
        %v95544 = vxor.u32 %v95539, %v95543 (stack76)
        %v95547 = vadd.s32 %v95539, %v95544 (stack65)
        %v95549 = vshll.u32 %v95544, 15 (stack73)
        %v95550 = vshrl.u32 %v95544, 17 (stack74)
        %v95551 = vor.u32 %v95549, %v95550 (stack75)
        %v95552 = vxor.u32 %v95547, %v95551 (stack76)
        %v95555 = vadd.s32 %v95547, %v95552 (stack65)
        %v95557 = vshll.u32 %v95552, 26 (stack73)
        %v95558 = vshrl.u32 %v95552, 6 (stack74)
        %v95559 = vor.u32 %v95557, %v95558 (stack75)
        %v95560 = vxor.u32 %v95555, %v95559 (stack76)
        %v95563 = vadd.s32 %v95555, %v95560 (stack65)
        %v95567 = vadd.s32 %v95563, %v9 (stack65)
        %v95569 = vshll.u32 %v95560, 6 (stack73)
        %v95570 = vshrl.u32 %v95560, 26 (stack74)
        %v95571 = vor.u32 %v95569, %v95570 (stack75)
        %v95572 = vxor.u32 %v95563, %v95571 (stack76)
        %v95575 = vadd.s32 %v95572, %v8 (stack65)
        %v95579 = vadd.s32 %v95575, 1 (stack65)
        %v95583 = vadd.s32 %v95567, %v95579 (stack65)
        %v95585 = vshll.u32 %v95579, 17 (stack73)
        %v95586 = vshrl.u32 %v95579, 15 (stack74)
        %v95587 = vor.u32 %v95585, %v95586 (stack75)
        %v95588 = vxor.u32 %v95583, %v95587 (stack76)
        %v95591 = vadd.s32 %v95583, %v95588 (stack65)
        %v95593 = vshll.u32 %v95588, 29 (stack73)
        %v95594 = vshrl.u32 %v95588, 3 (stack74)
        %v95595 = vor.u32 %v95593, %v95594 (stack75)
        %v95596 = vxor.u32 %v95591, %v95595 (stack76)
        %v95599 = vadd.s32 %v95591, %v95596 (stack65)
        %v95601 = vshll.u32 %v95596, 16 (stack73)
        %v95602 = vshrl.u32 %v95596, 16 (stack74)
        %v95603 = vor.u32 %v95601, %v95602 (stack75)
        %v95604 = vxor.u32 %v95599, %v95603 (stack76)
        %v95607 = vadd.s32 %v95599, %v95604 (stack65)
        %v95611 = vadd.s32 %v95607, %v8 (stack65)
        %v95613 = vshll.u32 %v95604, 24 (stack73)
        %v95614 = vshrl.u32 %v95604, 8 (stack74)
        %v95615 = vor.u32 %v95613, %v95614 (stack75)
        %v95616 = vxor.u32 %v95607, %v95615 (stack76)
        %v95619 = vadd.s32 %v95616, %v10 (stack65)
        %v95623 = vadd.s32 %v95619, 2 (stack65)
        %v95627 = vadd.s32 %v95611, %v95623 (stack65)
        %v95629 = vshll.u32 %v95623, 13 (stack73)
        %v95630 = vshrl.u32 %v95623, 19 (stack74)
        %v95631 = vor.u32 %v95629, %v95630 (stack75)
        %v95632 = vxor.u32 %v95627, %v95631 (stack76)
        %v95635 = vadd.s32 %v95627, %v95632 (stack65)
        %v95637 = vshll.u32 %v95632, 15 (stack73)
        %v95638 = vshrl.u32 %v95632, 17 (stack74)
        %v95639 = vor.u32 %v95637, %v95638 (stack75)
        %v95640 = vxor.u32 %v95635, %v95639 (stack76)
        %v95643 = vadd.s32 %v95635, %v95640 (stack65)
        %v95645 = vshll.u32 %v95640, 26 (stack73)
        %v95646 = vshrl.u32 %v95640, 6 (stack74)
        %v95647 = vor.u32 %v95645, %v95646 (stack75)
        %v95648 = vxor.u32 %v95643, %v95647 (stack76)
        %v95651 = vadd.s32 %v95643, %v95648 (stack65)
        %v95655 = vadd.s32 %v95651, %v10 (stack65)
        %v95657 = vshll.u32 %v95648, 6 (stack73)
        %v95658 = vshrl.u32 %v95648, 26 (stack74)
        %v95659 = vor.u32 %v95657, %v95658 (stack75)
        %v95660 = vxor.u32 %v95651, %v95659 (stack76)
        %v95663 = vadd.s32 %v95660, %v9 (stack65)
        %v95667 = vadd.s32 %v95663, 3 (stack65)
        %v95671 = vadd.s32 %v95655, %v95667 (stack65)
        %v95673 = vshll.u32 %v95667, 17 (stack73)
        %v95674 = vshrl.u32 %v95667, 15 (stack74)
        %v95675 = vor.u32 %v95673, %v95674 (stack75)
        %v95676 = vxor.u32 %v95671, %v95675 (stack76)
        %v95679 = vadd.s32 %v95671, %v95676 (stack65)
        %v95681 = vshll.u32 %v95676, 29 (stack73)
        %v95682 = vshrl.u32 %v95676, 3 (stack74)
        %v95683 = vor.u32 %v95681, %v95682 (stack75)
        %v95684 = vxor.u32 %v95679, %v95683 (stack76)
        %v95687 = vadd.s32 %v95679, %v95684 (stack65)
        %v95689 = vshll.u32 %v95684, 16 (stack73)
        %v95690 = vshrl.u32 %v95684, 16 (stack74)
        %v95691 = vor.u32 %v95689, %v95690 (stack75)
        %v95692 = vxor.u32 %v95687, %v95691 (stack76)
        %v95695 = vadd.s32 %v95687, %v95692 (stack65)
        %v95699 = vadd.s32 %v95695, %v9 (stack65)
        %v95701 = vshll.u32 %v95692, 24 (stack73)
        %v95702 = vshrl.u32 %v95692, 8 (stack74)
        %v95703 = vor.u32 %v95701, %v95702 (stack75)
        %v95704 = vxor.u32 %v95695, %v95703 (stack76)
        %v95707 = vadd.s32 %v95704, %v8 (stack65)
        %v95711 = vadd.s32 %v95707, 4 (stack65)
        %v95715 = vadd.s32 %v95699, %v95711 (stack65)
        %v95717 = vshll.u32 %v95711, 13 (stack73)
        %v95718 = vshrl.u32 %v95711, 19 (stack74)
        %v95719 = vor.u32 %v95717, %v95718 (stack75)
        %v95720 = vxor.u32 %v95715, %v95719 (stack76)
        %v95723 = vadd.s32 %v95715, %v95720 (stack65)
        %v95725 = vshll.u32 %v95720, 15 (stack73)
        %v95726 = vshrl.u32 %v95720, 17 (stack74)
        %v95727 = vor.u32 %v95725, %v95726 (stack75)
        %v95728 = vxor.u32 %v95723, %v95727 (stack76)
        %v95731 = vadd.s32 %v95723, %v95728 (stack65)
        %v95733 = vshll.u32 %v95728, 26 (stack73)
        %v95734 = vshrl.u32 %v95728, 6 (stack74)
        %v95735 = vor.u32 %v95733, %v95734 (stack75)
        %v95736 = vxor.u32 %v95731, %v95735 (stack76)
        %v95739 = vadd.s32 %v95731, %v95736 (stack65)
        %v95743 = vadd.s32 %v95739, %v8 (stack65)
        %v95745 = vshll.u32 %v95736, 6 (stack73)
        %v95746 = vshrl.u32 %v95736, 26 (stack74)
        %v95747 = vor.u32 %v95745, %v95746 (stack75)
        %v95748 = vxor.u32 %v95739, %v95747 (stack76)
        %v95751 = vadd.s32 %v95748, %v10 (stack65)
        %v95755 = vadd.s32 %v95751, 5 (stack65)
        %v95757 = vxor.u32 %v95743, %v95755 (stack76)
        %v95758 = vand.u32.u8 %v95757, 255 (stack77)
        %v95759 = vand.u32 %v95758, 65535 (stack78)
        %v95760 = vshrl.u32 %v95759, 1 (stack79)
        %v95761 = vor.u32 %v95760, 16256 (stack75)
        %v95762 = vand.u32.u16 %v95761, 65535 (stack80)
        %v95763 = vunpack.i.l.bf16 %v95762 (stack81)
        %v95767 = vadd.f32 %v95763, -1.0 (stack82)
        %v95771 = vmul.f32 %v95767, 2.0 (stack83)
        %v95775 = vadd.f32 %v95771, -0.99609375 (stack82)
        %v95779 = vmax.f32 -0.99609375, %v95775 (stack84)
        %v95781 = vand.u32 2147483647, %v95779 (stack85)
        %vm95784 = vcmp.eq.f32.partialorder %v95781, 1.0 (stack86)
        %v95789 = vmul.f32 %v95779, inf (stack83)
        %v95791 = vxor.u32 %v95779, 2147483648 (stack87)
        %v95794 = vmul.f32 %v95779, %v95791 (stack83)
        %v95796 = vadd.f32 %v95794, 1.0 (stack88)
        %v95797 = vlog2.pop %v95796 (stack89)
        %v95798 = vmul.f32 %v95797, 0.6931472 (stack90)
        %v95799 = vmul.f32 -0.5, %v95794 (stack91)
        %v95800 = vadd.f32 %v95799, 1.0 (stack92)
        %v95801 = vmul.f32 %v95800, %v95794 (stack93)
        %v95802 = vand.u32 2147483647, %v95794 (stack94)
        %vm95803 = vcmp.lt.f32.partialorder %v95802, 0.0004427343 (stack95)
        %v95804 = vsel /*vm=*/%vm95803, /*on_true_vy=*/%v95801, /*on_false_vx=*/%v95798 (stack96)
        %v95805 = vxor.u32 %v95804, 2147483648 (stack87)
        %vm95808 = vcmp.lt.f32.partialorder %v95805, 5.0 (stack86)
        %v95813 = vsel /*vm=*/%vm95808, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v95817 = vsel /*vm=*/%vm95808, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v95821 = vsel /*vm=*/%vm95808, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v95825 = vsel /*vm=*/%vm95808, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v95829 = vsel /*vm=*/%vm95808, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v95833 = vsel /*vm=*/%vm95808, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v95837 = vsel /*vm=*/%vm95808, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v95841 = vsel /*vm=*/%vm95808, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v95845 = vsel /*vm=*/%vm95808, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v95849 = vadd.f32 %v95805, -2.5 (stack82)
        %v95851 = vrsqrt.pop %v95805 (stack97)
        %v95852 = vmul.f32 %v95805, %v95851 (stack98)
        %vm95853 = vcmp.eq.f32.partialorder %v95805, inf (stack99)
        %v95854 = vsel /*vm=*/%vm95853, /*on_true_vy=*/%v95805, /*on_false_vx=*/%v95852 (stack100)
        %vm95855 = vcmp.eq.f32.partialorder %v95805, 0.0 (stack101)
        %v95856 = vand.u32 %v95805, 2147483648 (stack102)
        %v95857 = vsel /*vm=*/%vm95855, /*on_true_vy=*/%v95856, /*on_false_vx=*/%v95854 (stack103)
        %v95860 = vadd.f32 %v95857, -3.0 (stack82)
        %v95864 = vsel /*vm=*/%vm95808, /*on_true_vy=*/%v95849, /*on_false_vx=*/%v95860 (stack72)
        %v95868 = vmul.f32 %v95845, %v95864 (stack83)
        %v95872 = vadd.f32 %v95841, %v95868 (stack82)
        %v95876 = vmul.f32 %v95872, %v95864 (stack83)
        %v95880 = vadd.f32 %v95837, %v95876 (stack82)
        %v95884 = vmul.f32 %v95880, %v95864 (stack83)
        %v95888 = vadd.f32 %v95833, %v95884 (stack82)
        %v95892 = vmul.f32 %v95888, %v95864 (stack83)
        %v95896 = vadd.f32 %v95829, %v95892 (stack82)
        %v95900 = vmul.f32 %v95896, %v95864 (stack83)
        %v95904 = vadd.f32 %v95825, %v95900 (stack82)
        %v95908 = vmul.f32 %v95904, %v95864 (stack83)
        %v95912 = vadd.f32 %v95821, %v95908 (stack82)
        %v95916 = vmul.f32 %v95912, %v95864 (stack83)
        %v95920 = vadd.f32 %v95817, %v95916 (stack82)
        %v95924 = vmul.f32 %v95920, %v95864 (stack83)
        %v95928 = vadd.f32 %v95813, %v95924 (stack82)
        %v95932 = vmul.f32 %v95928, %v95779 (stack83)
        %v95936 = vsel /*vm=*/%vm95784, /*on_true_vy=*/%v95789, /*on_false_vx=*/%v95932 (stack72)
        %v95940 = vmul.f32 %v95936, 1.4140625 (stack83)
        %s95942 = scalar_lea.vmem %s280, 612 [#allocation0] (stack107)
        %v95943 = vpack.c.bf16 0.0, %v95940 (stack104)
        %95944 = vst [vmem:[%s95942] sm:$0xf] /*vst_source=*/%v95943 (stack105)
        %v95947 = vadd.s32 %v2842, %v93639 (stack65)
        %s95949 = smul.u32 128, %s27 (stack66)
        %v95950 = vlaneseq (stack67)
        %v95951 = vand.u32 %v95950, 127 (stack68)
        %v95952 = vstv %s95949 (stack69)
        %v95953 = vadd.s32 %v95951, %v95952 (stack70)
        %v95957 = vadd.s32 %v95947, %v95953 (stack65)
        %vm95961 = vcmp.lt.u32.totalorder %v95957, %v95947 (stack71)
        %vm95966 = vcmp.lt.u32.totalorder %v95947, %v2842 (stack71)
        %v95971 = vadd.s32 %v2829, %v93622 (stack65)
        %v95975 = vadd.s32 %v95971, 1 (stack65)
        %v95979 = vsel /*vm=*/%vm95966, /*on_true_vy=*/%v95975, /*on_false_vx=*/%v95971 (stack72)
        %v95983 = vadd.s32 %v95979, 1 (stack65)
        %v95987 = vsel /*vm=*/%vm95961, /*on_true_vy=*/%v95983, /*on_false_vx=*/%v95979 (stack72)
        %v95992 = vadd.s32 %v95987, %v10 (stack65)
        %v95996 = vadd.s32 %v95957, %v9 (stack65)
        %v96000 = vadd.s32 %v95992, %v95996 (stack65)
        %v96002 = vshll.u32 %v95996, 13 (stack73)
        %v96003 = vshrl.u32 %v95996, 19 (stack74)
        %v96004 = vor.u32 %v96002, %v96003 (stack75)
        %v96005 = vxor.u32 %v96000, %v96004 (stack76)
        %v96008 = vadd.s32 %v96000, %v96005 (stack65)
        %v96010 = vshll.u32 %v96005, 15 (stack73)
        %v96011 = vshrl.u32 %v96005, 17 (stack74)
        %v96012 = vor.u32 %v96010, %v96011 (stack75)
        %v96013 = vxor.u32 %v96008, %v96012 (stack76)
        %v96016 = vadd.s32 %v96008, %v96013 (stack65)
        %v96018 = vshll.u32 %v96013, 26 (stack73)
        %v96019 = vshrl.u32 %v96013, 6 (stack74)
        %v96020 = vor.u32 %v96018, %v96019 (stack75)
        %v96021 = vxor.u32 %v96016, %v96020 (stack76)
        %v96024 = vadd.s32 %v96016, %v96021 (stack65)
        %v96028 = vadd.s32 %v96024, %v9 (stack65)
        %v96030 = vshll.u32 %v96021, 6 (stack73)
        %v96031 = vshrl.u32 %v96021, 26 (stack74)
        %v96032 = vor.u32 %v96030, %v96031 (stack75)
        %v96033 = vxor.u32 %v96024, %v96032 (stack76)
        %v96036 = vadd.s32 %v96033, %v8 (stack65)
        %v96040 = vadd.s32 %v96036, 1 (stack65)
        %v96044 = vadd.s32 %v96028, %v96040 (stack65)
        %v96046 = vshll.u32 %v96040, 17 (stack73)
        %v96047 = vshrl.u32 %v96040, 15 (stack74)
        %v96048 = vor.u32 %v96046, %v96047 (stack75)
        %v96049 = vxor.u32 %v96044, %v96048 (stack76)
        %v96052 = vadd.s32 %v96044, %v96049 (stack65)
        %v96054 = vshll.u32 %v96049, 29 (stack73)
        %v96055 = vshrl.u32 %v96049, 3 (stack74)
        %v96056 = vor.u32 %v96054, %v96055 (stack75)
        %v96057 = vxor.u32 %v96052, %v96056 (stack76)
        %v96060 = vadd.s32 %v96052, %v96057 (stack65)
        %v96062 = vshll.u32 %v96057, 16 (stack73)
        %v96063 = vshrl.u32 %v96057, 16 (stack74)
        %v96064 = vor.u32 %v96062, %v96063 (stack75)
        %v96065 = vxor.u32 %v96060, %v96064 (stack76)
        %v96068 = vadd.s32 %v96060, %v96065 (stack65)
        %v96072 = vadd.s32 %v96068, %v8 (stack65)
        %v96074 = vshll.u32 %v96065, 24 (stack73)
        %v96075 = vshrl.u32 %v96065, 8 (stack74)
        %v96076 = vor.u32 %v96074, %v96075 (stack75)
        %v96077 = vxor.u32 %v96068, %v96076 (stack76)
        %v96080 = vadd.s32 %v96077, %v10 (stack65)
        %v96084 = vadd.s32 %v96080, 2 (stack65)
        %v96088 = vadd.s32 %v96072, %v96084 (stack65)
        %v96090 = vshll.u32 %v96084, 13 (stack73)
        %v96091 = vshrl.u32 %v96084, 19 (stack74)
        %v96092 = vor.u32 %v96090, %v96091 (stack75)
        %v96093 = vxor.u32 %v96088, %v96092 (stack76)
        %v96096 = vadd.s32 %v96088, %v96093 (stack65)
        %v96098 = vshll.u32 %v96093, 15 (stack73)
        %v96099 = vshrl.u32 %v96093, 17 (stack74)
        %v96100 = vor.u32 %v96098, %v96099 (stack75)
        %v96101 = vxor.u32 %v96096, %v96100 (stack76)
        %v96104 = vadd.s32 %v96096, %v96101 (stack65)
        %v96106 = vshll.u32 %v96101, 26 (stack73)
        %v96107 = vshrl.u32 %v96101, 6 (stack74)
        %v96108 = vor.u32 %v96106, %v96107 (stack75)
        %v96109 = vxor.u32 %v96104, %v96108 (stack76)
        %v96112 = vadd.s32 %v96104, %v96109 (stack65)
        %v96116 = vadd.s32 %v96112, %v10 (stack65)
        %v96118 = vshll.u32 %v96109, 6 (stack73)
        %v96119 = vshrl.u32 %v96109, 26 (stack74)
        %v96120 = vor.u32 %v96118, %v96119 (stack75)
        %v96121 = vxor.u32 %v96112, %v96120 (stack76)
        %v96124 = vadd.s32 %v96121, %v9 (stack65)
        %v96128 = vadd.s32 %v96124, 3 (stack65)
        %v96132 = vadd.s32 %v96116, %v96128 (stack65)
        %v96134 = vshll.u32 %v96128, 17 (stack73)
        %v96135 = vshrl.u32 %v96128, 15 (stack74)
        %v96136 = vor.u32 %v96134, %v96135 (stack75)
        %v96137 = vxor.u32 %v96132, %v96136 (stack76)
        %v96140 = vadd.s32 %v96132, %v96137 (stack65)
        %v96142 = vshll.u32 %v96137, 29 (stack73)
        %v96143 = vshrl.u32 %v96137, 3 (stack74)
        %v96144 = vor.u32 %v96142, %v96143 (stack75)
        %v96145 = vxor.u32 %v96140, %v96144 (stack76)
        %v96148 = vadd.s32 %v96140, %v96145 (stack65)
        %v96150 = vshll.u32 %v96145, 16 (stack73)
        %v96151 = vshrl.u32 %v96145, 16 (stack74)
        %v96152 = vor.u32 %v96150, %v96151 (stack75)
        %v96153 = vxor.u32 %v96148, %v96152 (stack76)
        %v96156 = vadd.s32 %v96148, %v96153 (stack65)
        %v96160 = vadd.s32 %v96156, %v9 (stack65)
        %v96162 = vshll.u32 %v96153, 24 (stack73)
        %v96163 = vshrl.u32 %v96153, 8 (stack74)
        %v96164 = vor.u32 %v96162, %v96163 (stack75)
        %v96165 = vxor.u32 %v96156, %v96164 (stack76)
        %v96168 = vadd.s32 %v96165, %v8 (stack65)
        %v96172 = vadd.s32 %v96168, 4 (stack65)
        %v96176 = vadd.s32 %v96160, %v96172 (stack65)
        %v96178 = vshll.u32 %v96172, 13 (stack73)
        %v96179 = vshrl.u32 %v96172, 19 (stack74)
        %v96180 = vor.u32 %v96178, %v96179 (stack75)
        %v96181 = vxor.u32 %v96176, %v96180 (stack76)
        %v96184 = vadd.s32 %v96176, %v96181 (stack65)
        %v96186 = vshll.u32 %v96181, 15 (stack73)
        %v96187 = vshrl.u32 %v96181, 17 (stack74)
        %v96188 = vor.u32 %v96186, %v96187 (stack75)
        %v96189 = vxor.u32 %v96184, %v96188 (stack76)
        %v96192 = vadd.s32 %v96184, %v96189 (stack65)
        %v96194 = vshll.u32 %v96189, 26 (stack73)
        %v96195 = vshrl.u32 %v96189, 6 (stack74)
        %v96196 = vor.u32 %v96194, %v96195 (stack75)
        %v96197 = vxor.u32 %v96192, %v96196 (stack76)
        %v96200 = vadd.s32 %v96192, %v96197 (stack65)
        %v96204 = vadd.s32 %v96200, %v8 (stack65)
        %v96206 = vshll.u32 %v96197, 6 (stack73)
        %v96207 = vshrl.u32 %v96197, 26 (stack74)
        %v96208 = vor.u32 %v96206, %v96207 (stack75)
        %v96209 = vxor.u32 %v96200, %v96208 (stack76)
        %v96212 = vadd.s32 %v96209, %v10 (stack65)
        %v96216 = vadd.s32 %v96212, 5 (stack65)
        %v96218 = vxor.u32 %v96204, %v96216 (stack76)
        %v96219 = vand.u32.u8 %v96218, 255 (stack77)
        %v96220 = vand.u32 %v96219, 65535 (stack78)
        %v96221 = vshrl.u32 %v96220, 1 (stack79)
        %v96222 = vor.u32 %v96221, 16256 (stack75)
        %v96223 = vand.u32.u16 %v96222, 65535 (stack80)
        %v96224 = vunpack.i.l.bf16 %v96223 (stack81)
        %v96228 = vadd.f32 %v96224, -1.0 (stack82)
        %v96232 = vmul.f32 %v96228, 2.0 (stack83)
        %v96236 = vadd.f32 %v96232, -0.99609375 (stack82)
        %v96240 = vmax.f32 -0.99609375, %v96236 (stack84)
        %v96242 = vand.u32 2147483647, %v96240 (stack85)
        %vm96245 = vcmp.eq.f32.partialorder %v96242, 1.0 (stack86)
        %v96250 = vmul.f32 %v96240, inf (stack83)
        %v96252 = vxor.u32 %v96240, 2147483648 (stack87)
        %v96255 = vmul.f32 %v96240, %v96252 (stack83)
        %v96257 = vadd.f32 %v96255, 1.0 (stack88)
        %v96258 = vlog2.pop %v96257 (stack89)
        %v96259 = vmul.f32 %v96258, 0.6931472 (stack90)
        %v96260 = vmul.f32 -0.5, %v96255 (stack91)
        %v96261 = vadd.f32 %v96260, 1.0 (stack92)
        %v96262 = vmul.f32 %v96261, %v96255 (stack93)
        %v96263 = vand.u32 2147483647, %v96255 (stack94)
        %vm96264 = vcmp.lt.f32.partialorder %v96263, 0.0004427343 (stack95)
        %v96265 = vsel /*vm=*/%vm96264, /*on_true_vy=*/%v96262, /*on_false_vx=*/%v96259 (stack96)
        %v96266 = vxor.u32 %v96265, 2147483648 (stack87)
        %vm96269 = vcmp.lt.f32.partialorder %v96266, 5.0 (stack86)
        %v96274 = vsel /*vm=*/%vm96269, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v96278 = vsel /*vm=*/%vm96269, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v96282 = vsel /*vm=*/%vm96269, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v96286 = vsel /*vm=*/%vm96269, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v96290 = vsel /*vm=*/%vm96269, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v96294 = vsel /*vm=*/%vm96269, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v96298 = vsel /*vm=*/%vm96269, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v96302 = vsel /*vm=*/%vm96269, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v96306 = vsel /*vm=*/%vm96269, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v96310 = vadd.f32 %v96266, -2.5 (stack82)
        %v96312 = vrsqrt.pop %v96266 (stack97)
        %v96313 = vmul.f32 %v96266, %v96312 (stack98)
        %vm96314 = vcmp.eq.f32.partialorder %v96266, inf (stack99)
        %v96315 = vsel /*vm=*/%vm96314, /*on_true_vy=*/%v96266, /*on_false_vx=*/%v96313 (stack100)
        %vm96316 = vcmp.eq.f32.partialorder %v96266, 0.0 (stack101)
        %v96317 = vand.u32 %v96266, 2147483648 (stack102)
        %v96318 = vsel /*vm=*/%vm96316, /*on_true_vy=*/%v96317, /*on_false_vx=*/%v96315 (stack103)
        %v96321 = vadd.f32 %v96318, -3.0 (stack82)
        %v96325 = vsel /*vm=*/%vm96269, /*on_true_vy=*/%v96310, /*on_false_vx=*/%v96321 (stack72)
        %v96329 = vmul.f32 %v96306, %v96325 (stack83)
        %v96333 = vadd.f32 %v96302, %v96329 (stack82)
        %v96337 = vmul.f32 %v96333, %v96325 (stack83)
        %v96341 = vadd.f32 %v96298, %v96337 (stack82)
        %v96345 = vmul.f32 %v96341, %v96325 (stack83)
        %v96349 = vadd.f32 %v96294, %v96345 (stack82)
        %v96353 = vmul.f32 %v96349, %v96325 (stack83)
        %v96357 = vadd.f32 %v96290, %v96353 (stack82)
        %v96361 = vmul.f32 %v96357, %v96325 (stack83)
        %v96365 = vadd.f32 %v96286, %v96361 (stack82)
        %v96369 = vmul.f32 %v96365, %v96325 (stack83)
        %v96373 = vadd.f32 %v96282, %v96369 (stack82)
        %v96377 = vmul.f32 %v96373, %v96325 (stack83)
        %v96381 = vadd.f32 %v96278, %v96377 (stack82)
        %v96385 = vmul.f32 %v96381, %v96325 (stack83)
        %v96389 = vadd.f32 %v96274, %v96385 (stack82)
        %v96393 = vmul.f32 %v96389, %v96240 (stack83)
        %v96397 = vsel /*vm=*/%vm96245, /*on_true_vy=*/%v96250, /*on_false_vx=*/%v96393 (stack72)
        %v96401 = vmul.f32 %v96397, 1.4140625 (stack83)
        %s96403 = scalar_lea.vmem %s280, 740 [#allocation0] (stack107)
        %v96404 = vpack.c.bf16 0.0, %v96401 (stack104)
        %96405 = vst [vmem:[%s96403] sm:$0xf] /*vst_source=*/%v96404 (stack105)
        %v96408 = vadd.s32 %v3329, %v93639 (stack65)
        %s96410 = smul.u32 128, %s27 (stack66)
        %v96411 = vlaneseq (stack67)
        %v96412 = vand.u32 %v96411, 127 (stack68)
        %v96413 = vstv %s96410 (stack69)
        %v96414 = vadd.s32 %v96412, %v96413 (stack70)
        %v96418 = vadd.s32 %v96408, %v96414 (stack65)
        %vm96422 = vcmp.lt.u32.totalorder %v96418, %v96408 (stack71)
        %vm96427 = vcmp.lt.u32.totalorder %v96408, %v3329 (stack71)
        %v96432 = vadd.s32 %v3316, %v93622 (stack65)
        %v96436 = vadd.s32 %v96432, 1 (stack65)
        %v96440 = vsel /*vm=*/%vm96427, /*on_true_vy=*/%v96436, /*on_false_vx=*/%v96432 (stack72)
        %v96444 = vadd.s32 %v96440, 1 (stack65)
        %v96448 = vsel /*vm=*/%vm96422, /*on_true_vy=*/%v96444, /*on_false_vx=*/%v96440 (stack72)
        %v96453 = vadd.s32 %v96448, %v10 (stack65)
        %v96457 = vadd.s32 %v96418, %v9 (stack65)
        %v96461 = vadd.s32 %v96453, %v96457 (stack65)
        %v96463 = vshll.u32 %v96457, 13 (stack73)
        %v96464 = vshrl.u32 %v96457, 19 (stack74)
        %v96465 = vor.u32 %v96463, %v96464 (stack75)
        %v96466 = vxor.u32 %v96461, %v96465 (stack76)
        %v96469 = vadd.s32 %v96461, %v96466 (stack65)
        %v96471 = vshll.u32 %v96466, 15 (stack73)
        %v96472 = vshrl.u32 %v96466, 17 (stack74)
        %v96473 = vor.u32 %v96471, %v96472 (stack75)
        %v96474 = vxor.u32 %v96469, %v96473 (stack76)
        %v96477 = vadd.s32 %v96469, %v96474 (stack65)
        %v96479 = vshll.u32 %v96474, 26 (stack73)
        %v96480 = vshrl.u32 %v96474, 6 (stack74)
        %v96481 = vor.u32 %v96479, %v96480 (stack75)
        %v96482 = vxor.u32 %v96477, %v96481 (stack76)
        %v96485 = vadd.s32 %v96477, %v96482 (stack65)
        %v96489 = vadd.s32 %v96485, %v9 (stack65)
        %v96491 = vshll.u32 %v96482, 6 (stack73)
        %v96492 = vshrl.u32 %v96482, 26 (stack74)
        %v96493 = vor.u32 %v96491, %v96492 (stack75)
        %v96494 = vxor.u32 %v96485, %v96493 (stack76)
        %v96497 = vadd.s32 %v96494, %v8 (stack65)
        %v96501 = vadd.s32 %v96497, 1 (stack65)
        %v96505 = vadd.s32 %v96489, %v96501 (stack65)
        %v96507 = vshll.u32 %v96501, 17 (stack73)
        %v96508 = vshrl.u32 %v96501, 15 (stack74)
        %v96509 = vor.u32 %v96507, %v96508 (stack75)
        %v96510 = vxor.u32 %v96505, %v96509 (stack76)
        %v96513 = vadd.s32 %v96505, %v96510 (stack65)
        %v96515 = vshll.u32 %v96510, 29 (stack73)
        %v96516 = vshrl.u32 %v96510, 3 (stack74)
        %v96517 = vor.u32 %v96515, %v96516 (stack75)
        %v96518 = vxor.u32 %v96513, %v96517 (stack76)
        %v96521 = vadd.s32 %v96513, %v96518 (stack65)
        %v96523 = vshll.u32 %v96518, 16 (stack73)
        %v96524 = vshrl.u32 %v96518, 16 (stack74)
        %v96525 = vor.u32 %v96523, %v96524 (stack75)
        %v96526 = vxor.u32 %v96521, %v96525 (stack76)
        %v96529 = vadd.s32 %v96521, %v96526 (stack65)
        %v96533 = vadd.s32 %v96529, %v8 (stack65)
        %v96535 = vshll.u32 %v96526, 24 (stack73)
        %v96536 = vshrl.u32 %v96526, 8 (stack74)
        %v96537 = vor.u32 %v96535, %v96536 (stack75)
        %v96538 = vxor.u32 %v96529, %v96537 (stack76)
        %v96541 = vadd.s32 %v96538, %v10 (stack65)
        %v96545 = vadd.s32 %v96541, 2 (stack65)
        %v96549 = vadd.s32 %v96533, %v96545 (stack65)
        %v96551 = vshll.u32 %v96545, 13 (stack73)
        %v96552 = vshrl.u32 %v96545, 19 (stack74)
        %v96553 = vor.u32 %v96551, %v96552 (stack75)
        %v96554 = vxor.u32 %v96549, %v96553 (stack76)
        %v96557 = vadd.s32 %v96549, %v96554 (stack65)
        %v96559 = vshll.u32 %v96554, 15 (stack73)
        %v96560 = vshrl.u32 %v96554, 17 (stack74)
        %v96561 = vor.u32 %v96559, %v96560 (stack75)
        %v96562 = vxor.u32 %v96557, %v96561 (stack76)
        %v96565 = vadd.s32 %v96557, %v96562 (stack65)
        %v96567 = vshll.u32 %v96562, 26 (stack73)
        %v96568 = vshrl.u32 %v96562, 6 (stack74)
        %v96569 = vor.u32 %v96567, %v96568 (stack75)
        %v96570 = vxor.u32 %v96565, %v96569 (stack76)
        %v96573 = vadd.s32 %v96565, %v96570 (stack65)
        %v96577 = vadd.s32 %v96573, %v10 (stack65)
        %v96579 = vshll.u32 %v96570, 6 (stack73)
        %v96580 = vshrl.u32 %v96570, 26 (stack74)
        %v96581 = vor.u32 %v96579, %v96580 (stack75)
        %v96582 = vxor.u32 %v96573, %v96581 (stack76)
        %v96585 = vadd.s32 %v96582, %v9 (stack65)
        %v96589 = vadd.s32 %v96585, 3 (stack65)
        %v96593 = vadd.s32 %v96577, %v96589 (stack65)
        %v96595 = vshll.u32 %v96589, 17 (stack73)
        %v96596 = vshrl.u32 %v96589, 15 (stack74)
        %v96597 = vor.u32 %v96595, %v96596 (stack75)
        %v96598 = vxor.u32 %v96593, %v96597 (stack76)
        %v96601 = vadd.s32 %v96593, %v96598 (stack65)
        %v96603 = vshll.u32 %v96598, 29 (stack73)
        %v96604 = vshrl.u32 %v96598, 3 (stack74)
        %v96605 = vor.u32 %v96603, %v96604 (stack75)
        %v96606 = vxor.u32 %v96601, %v96605 (stack76)
        %v96609 = vadd.s32 %v96601, %v96606 (stack65)
        %v96611 = vshll.u32 %v96606, 16 (stack73)
        %v96612 = vshrl.u32 %v96606, 16 (stack74)
        %v96613 = vor.u32 %v96611, %v96612 (stack75)
        %v96614 = vxor.u32 %v96609, %v96613 (stack76)
        %v96617 = vadd.s32 %v96609, %v96614 (stack65)
        %v96621 = vadd.s32 %v96617, %v9 (stack65)
        %v96623 = vshll.u32 %v96614, 24 (stack73)
        %v96624 = vshrl.u32 %v96614, 8 (stack74)
        %v96625 = vor.u32 %v96623, %v96624 (stack75)
        %v96626 = vxor.u32 %v96617, %v96625 (stack76)
        %v96629 = vadd.s32 %v96626, %v8 (stack65)
        %v96633 = vadd.s32 %v96629, 4 (stack65)
        %v96637 = vadd.s32 %v96621, %v96633 (stack65)
        %v96639 = vshll.u32 %v96633, 13 (stack73)
        %v96640 = vshrl.u32 %v96633, 19 (stack74)
        %v96641 = vor.u32 %v96639, %v96640 (stack75)
        %v96642 = vxor.u32 %v96637, %v96641 (stack76)
        %v96645 = vadd.s32 %v96637, %v96642 (stack65)
        %v96647 = vshll.u32 %v96642, 15 (stack73)
        %v96648 = vshrl.u32 %v96642, 17 (stack74)
        %v96649 = vor.u32 %v96647, %v96648 (stack75)
        %v96650 = vxor.u32 %v96645, %v96649 (stack76)
        %v96653 = vadd.s32 %v96645, %v96650 (stack65)
        %v96655 = vshll.u32 %v96650, 26 (stack73)
        %v96656 = vshrl.u32 %v96650, 6 (stack74)
        %v96657 = vor.u32 %v96655, %v96656 (stack75)
        %v96658 = vxor.u32 %v96653, %v96657 (stack76)
        %v96661 = vadd.s32 %v96653, %v96658 (stack65)
        %v96665 = vadd.s32 %v96661, %v8 (stack65)
        %v96667 = vshll.u32 %v96658, 6 (stack73)
        %v96668 = vshrl.u32 %v96658, 26 (stack74)
        %v96669 = vor.u32 %v96667, %v96668 (stack75)
        %v96670 = vxor.u32 %v96661, %v96669 (stack76)
        %v96673 = vadd.s32 %v96670, %v10 (stack65)
        %v96677 = vadd.s32 %v96673, 5 (stack65)
        %v96679 = vxor.u32 %v96665, %v96677 (stack76)
        %v96680 = vand.u32.u8 %v96679, 255 (stack77)
        %v96681 = vand.u32 %v96680, 65535 (stack78)
        %v96682 = vshrl.u32 %v96681, 1 (stack79)
        %v96683 = vor.u32 %v96682, 16256 (stack75)
        %v96684 = vand.u32.u16 %v96683, 65535 (stack80)
        %v96685 = vunpack.i.l.bf16 %v96684 (stack81)
        %v96689 = vadd.f32 %v96685, -1.0 (stack82)
        %v96693 = vmul.f32 %v96689, 2.0 (stack83)
        %v96697 = vadd.f32 %v96693, -0.99609375 (stack82)
        %v96701 = vmax.f32 -0.99609375, %v96697 (stack84)
        %v96703 = vand.u32 2147483647, %v96701 (stack85)
        %vm96706 = vcmp.eq.f32.partialorder %v96703, 1.0 (stack86)
        %v96711 = vmul.f32 %v96701, inf (stack83)
        %v96713 = vxor.u32 %v96701, 2147483648 (stack87)
        %v96716 = vmul.f32 %v96701, %v96713 (stack83)
        %v96718 = vadd.f32 %v96716, 1.0 (stack88)
        %v96719 = vlog2.pop %v96718 (stack89)
        %v96720 = vmul.f32 %v96719, 0.6931472 (stack90)
        %v96721 = vmul.f32 -0.5, %v96716 (stack91)
        %v96722 = vadd.f32 %v96721, 1.0 (stack92)
        %v96723 = vmul.f32 %v96722, %v96716 (stack93)
        %v96724 = vand.u32 2147483647, %v96716 (stack94)
        %vm96725 = vcmp.lt.f32.partialorder %v96724, 0.0004427343 (stack95)
        %v96726 = vsel /*vm=*/%vm96725, /*on_true_vy=*/%v96723, /*on_false_vx=*/%v96720 (stack96)
        %v96727 = vxor.u32 %v96726, 2147483648 (stack87)
        %vm96730 = vcmp.lt.f32.partialorder %v96727, 5.0 (stack86)
        %v96735 = vsel /*vm=*/%vm96730, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v96739 = vsel /*vm=*/%vm96730, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v96743 = vsel /*vm=*/%vm96730, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v96747 = vsel /*vm=*/%vm96730, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v96751 = vsel /*vm=*/%vm96730, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v96755 = vsel /*vm=*/%vm96730, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v96759 = vsel /*vm=*/%vm96730, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v96763 = vsel /*vm=*/%vm96730, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v96767 = vsel /*vm=*/%vm96730, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v96771 = vadd.f32 %v96727, -2.5 (stack82)
        %v96773 = vrsqrt.pop %v96727 (stack97)
        %v96774 = vmul.f32 %v96727, %v96773 (stack98)
        %vm96775 = vcmp.eq.f32.partialorder %v96727, inf (stack99)
        %v96776 = vsel /*vm=*/%vm96775, /*on_true_vy=*/%v96727, /*on_false_vx=*/%v96774 (stack100)
        %vm96777 = vcmp.eq.f32.partialorder %v96727, 0.0 (stack101)
        %v96778 = vand.u32 %v96727, 2147483648 (stack102)
        %v96779 = vsel /*vm=*/%vm96777, /*on_true_vy=*/%v96778, /*on_false_vx=*/%v96776 (stack103)
        %v96782 = vadd.f32 %v96779, -3.0 (stack82)
        %v96786 = vsel /*vm=*/%vm96730, /*on_true_vy=*/%v96771, /*on_false_vx=*/%v96782 (stack72)
        %v96790 = vmul.f32 %v96767, %v96786 (stack83)
        %v96794 = vadd.f32 %v96763, %v96790 (stack82)
        %v96798 = vmul.f32 %v96794, %v96786 (stack83)
        %v96802 = vadd.f32 %v96759, %v96798 (stack82)
        %v96806 = vmul.f32 %v96802, %v96786 (stack83)
        %v96810 = vadd.f32 %v96755, %v96806 (stack82)
        %v96814 = vmul.f32 %v96810, %v96786 (stack83)
        %v96818 = vadd.f32 %v96751, %v96814 (stack82)
        %v96822 = vmul.f32 %v96818, %v96786 (stack83)
        %v96826 = vadd.f32 %v96747, %v96822 (stack82)
        %v96830 = vmul.f32 %v96826, %v96786 (stack83)
        %v96834 = vadd.f32 %v96743, %v96830 (stack82)
        %v96838 = vmul.f32 %v96834, %v96786 (stack83)
        %v96842 = vadd.f32 %v96739, %v96838 (stack82)
        %v96846 = vmul.f32 %v96842, %v96786 (stack83)
        %v96850 = vadd.f32 %v96735, %v96846 (stack82)
        %v96854 = vmul.f32 %v96850, %v96701 (stack83)
        %v96858 = vsel /*vm=*/%vm96706, /*on_true_vy=*/%v96711, /*on_false_vx=*/%v96854 (stack72)
        %v96862 = vmul.f32 %v96858, 1.4140625 (stack83)
        %s96864 = scalar_lea.vmem %s280, 868 [#allocation0] (stack107)
        %v96865 = vpack.c.bf16 0.0, %v96862 (stack104)
        %96866 = vst [vmem:[%s96864] sm:$0xf] /*vst_source=*/%v96865 (stack105)
        %v96869 = vadd.s32 %v3816, %v93639 (stack65)
        %s96871 = smul.u32 128, %s27 (stack66)
        %v96872 = vlaneseq (stack67)
        %v96873 = vand.u32 %v96872, 127 (stack68)
        %v96874 = vstv %s96871 (stack69)
        %v96875 = vadd.s32 %v96873, %v96874 (stack70)
        %v96879 = vadd.s32 %v96869, %v96875 (stack65)
        %vm96883 = vcmp.lt.u32.totalorder %v96879, %v96869 (stack71)
        %vm96888 = vcmp.lt.u32.totalorder %v96869, %v3816 (stack71)
        %v96893 = vadd.s32 %v3803, %v93622 (stack65)
        %v96897 = vadd.s32 %v96893, 1 (stack65)
        %v96901 = vsel /*vm=*/%vm96888, /*on_true_vy=*/%v96897, /*on_false_vx=*/%v96893 (stack72)
        %v96905 = vadd.s32 %v96901, 1 (stack65)
        %v96909 = vsel /*vm=*/%vm96883, /*on_true_vy=*/%v96905, /*on_false_vx=*/%v96901 (stack72)
        %v96914 = vadd.s32 %v96909, %v10 (stack65)
        %v96918 = vadd.s32 %v96879, %v9 (stack65)
        %v96922 = vadd.s32 %v96914, %v96918 (stack65)
        %v96924 = vshll.u32 %v96918, 13 (stack73)
        %v96925 = vshrl.u32 %v96918, 19 (stack74)
        %v96926 = vor.u32 %v96924, %v96925 (stack75)
        %v96927 = vxor.u32 %v96922, %v96926 (stack76)
        %v96930 = vadd.s32 %v96922, %v96927 (stack65)
        %v96932 = vshll.u32 %v96927, 15 (stack73)
        %v96933 = vshrl.u32 %v96927, 17 (stack74)
        %v96934 = vor.u32 %v96932, %v96933 (stack75)
        %v96935 = vxor.u32 %v96930, %v96934 (stack76)
        %v96938 = vadd.s32 %v96930, %v96935 (stack65)
        %v96940 = vshll.u32 %v96935, 26 (stack73)
        %v96941 = vshrl.u32 %v96935, 6 (stack74)
        %v96942 = vor.u32 %v96940, %v96941 (stack75)
        %v96943 = vxor.u32 %v96938, %v96942 (stack76)
        %v96946 = vadd.s32 %v96938, %v96943 (stack65)
        %v96950 = vadd.s32 %v96946, %v9 (stack65)
        %v96952 = vshll.u32 %v96943, 6 (stack73)
        %v96953 = vshrl.u32 %v96943, 26 (stack74)
        %v96954 = vor.u32 %v96952, %v96953 (stack75)
        %v96955 = vxor.u32 %v96946, %v96954 (stack76)
        %v96958 = vadd.s32 %v96955, %v8 (stack65)
        %v96962 = vadd.s32 %v96958, 1 (stack65)
        %v96966 = vadd.s32 %v96950, %v96962 (stack65)
        %v96968 = vshll.u32 %v96962, 17 (stack73)
        %v96969 = vshrl.u32 %v96962, 15 (stack74)
        %v96970 = vor.u32 %v96968, %v96969 (stack75)
        %v96971 = vxor.u32 %v96966, %v96970 (stack76)
        %v96974 = vadd.s32 %v96966, %v96971 (stack65)
        %v96976 = vshll.u32 %v96971, 29 (stack73)
        %v96977 = vshrl.u32 %v96971, 3 (stack74)
        %v96978 = vor.u32 %v96976, %v96977 (stack75)
        %v96979 = vxor.u32 %v96974, %v96978 (stack76)
        %v96982 = vadd.s32 %v96974, %v96979 (stack65)
        %v96984 = vshll.u32 %v96979, 16 (stack73)
        %v96985 = vshrl.u32 %v96979, 16 (stack74)
        %v96986 = vor.u32 %v96984, %v96985 (stack75)
        %v96987 = vxor.u32 %v96982, %v96986 (stack76)
        %v96990 = vadd.s32 %v96982, %v96987 (stack65)
        %v96994 = vadd.s32 %v96990, %v8 (stack65)
        %v96996 = vshll.u32 %v96987, 24 (stack73)
        %v96997 = vshrl.u32 %v96987, 8 (stack74)
        %v96998 = vor.u32 %v96996, %v96997 (stack75)
        %v96999 = vxor.u32 %v96990, %v96998 (stack76)
        %v97002 = vadd.s32 %v96999, %v10 (stack65)
        %v97006 = vadd.s32 %v97002, 2 (stack65)
        %v97010 = vadd.s32 %v96994, %v97006 (stack65)
        %v97012 = vshll.u32 %v97006, 13 (stack73)
        %v97013 = vshrl.u32 %v97006, 19 (stack74)
        %v97014 = vor.u32 %v97012, %v97013 (stack75)
        %v97015 = vxor.u32 %v97010, %v97014 (stack76)
        %v97018 = vadd.s32 %v97010, %v97015 (stack65)
        %v97020 = vshll.u32 %v97015, 15 (stack73)
        %v97021 = vshrl.u32 %v97015, 17 (stack74)
        %v97022 = vor.u32 %v97020, %v97021 (stack75)
        %v97023 = vxor.u32 %v97018, %v97022 (stack76)
        %v97026 = vadd.s32 %v97018, %v97023 (stack65)
        %v97028 = vshll.u32 %v97023, 26 (stack73)
        %v97029 = vshrl.u32 %v97023, 6 (stack74)
        %v97030 = vor.u32 %v97028, %v97029 (stack75)
        %v97031 = vxor.u32 %v97026, %v97030 (stack76)
        %v97034 = vadd.s32 %v97026, %v97031 (stack65)
        %v97038 = vadd.s32 %v97034, %v10 (stack65)
        %v97040 = vshll.u32 %v97031, 6 (stack73)
        %v97041 = vshrl.u32 %v97031, 26 (stack74)
        %v97042 = vor.u32 %v97040, %v97041 (stack75)
        %v97043 = vxor.u32 %v97034, %v97042 (stack76)
        %v97046 = vadd.s32 %v97043, %v9 (stack65)
        %v97050 = vadd.s32 %v97046, 3 (stack65)
        %v97054 = vadd.s32 %v97038, %v97050 (stack65)
        %v97056 = vshll.u32 %v97050, 17 (stack73)
        %v97057 = vshrl.u32 %v97050, 15 (stack74)
        %v97058 = vor.u32 %v97056, %v97057 (stack75)
        %v97059 = vxor.u32 %v97054, %v97058 (stack76)
        %v97062 = vadd.s32 %v97054, %v97059 (stack65)
        %v97064 = vshll.u32 %v97059, 29 (stack73)
        %v97065 = vshrl.u32 %v97059, 3 (stack74)
        %v97066 = vor.u32 %v97064, %v97065 (stack75)
        %v97067 = vxor.u32 %v97062, %v97066 (stack76)
        %v97070 = vadd.s32 %v97062, %v97067 (stack65)
        %v97072 = vshll.u32 %v97067, 16 (stack73)
        %v97073 = vshrl.u32 %v97067, 16 (stack74)
        %v97074 = vor.u32 %v97072, %v97073 (stack75)
        %v97075 = vxor.u32 %v97070, %v97074 (stack76)
        %v97078 = vadd.s32 %v97070, %v97075 (stack65)
        %v97082 = vadd.s32 %v97078, %v9 (stack65)
        %v97084 = vshll.u32 %v97075, 24 (stack73)
        %v97085 = vshrl.u32 %v97075, 8 (stack74)
        %v97086 = vor.u32 %v97084, %v97085 (stack75)
        %v97087 = vxor.u32 %v97078, %v97086 (stack76)
        %v97090 = vadd.s32 %v97087, %v8 (stack65)
        %v97094 = vadd.s32 %v97090, 4 (stack65)
        %v97098 = vadd.s32 %v97082, %v97094 (stack65)
        %v97100 = vshll.u32 %v97094, 13 (stack73)
        %v97101 = vshrl.u32 %v97094, 19 (stack74)
        %v97102 = vor.u32 %v97100, %v97101 (stack75)
        %v97103 = vxor.u32 %v97098, %v97102 (stack76)
        %v97106 = vadd.s32 %v97098, %v97103 (stack65)
        %v97108 = vshll.u32 %v97103, 15 (stack73)
        %v97109 = vshrl.u32 %v97103, 17 (stack74)
        %v97110 = vor.u32 %v97108, %v97109 (stack75)
        %v97111 = vxor.u32 %v97106, %v97110 (stack76)
        %v97114 = vadd.s32 %v97106, %v97111 (stack65)
        %v97116 = vshll.u32 %v97111, 26 (stack73)
        %v97117 = vshrl.u32 %v97111, 6 (stack74)
        %v97118 = vor.u32 %v97116, %v97117 (stack75)
        %v97119 = vxor.u32 %v97114, %v97118 (stack76)
        %v97122 = vadd.s32 %v97114, %v97119 (stack65)
        %v97126 = vadd.s32 %v97122, %v8 (stack65)
        %v97128 = vshll.u32 %v97119, 6 (stack73)
        %v97129 = vshrl.u32 %v97119, 26 (stack74)
        %v97130 = vor.u32 %v97128, %v97129 (stack75)
        %v97131 = vxor.u32 %v97122, %v97130 (stack76)
        %v97134 = vadd.s32 %v97131, %v10 (stack65)
        %v97138 = vadd.s32 %v97134, 5 (stack65)
        %v97140 = vxor.u32 %v97126, %v97138 (stack76)
        %v97141 = vand.u32.u8 %v97140, 255 (stack77)
        %v97142 = vand.u32 %v97141, 65535 (stack78)
        %v97143 = vshrl.u32 %v97142, 1 (stack79)
        %v97144 = vor.u32 %v97143, 16256 (stack75)
        %v97145 = vand.u32.u16 %v97144, 65535 (stack80)
        %v97146 = vunpack.i.l.bf16 %v97145 (stack81)
        %v97150 = vadd.f32 %v97146, -1.0 (stack82)
        %v97154 = vmul.f32 %v97150, 2.0 (stack83)
        %v97158 = vadd.f32 %v97154, -0.99609375 (stack82)
        %v97162 = vmax.f32 -0.99609375, %v97158 (stack84)
        %v97164 = vand.u32 2147483647, %v97162 (stack85)
        %vm97167 = vcmp.eq.f32.partialorder %v97164, 1.0 (stack86)
        %v97172 = vmul.f32 %v97162, inf (stack83)
        %v97174 = vxor.u32 %v97162, 2147483648 (stack87)
        %v97177 = vmul.f32 %v97162, %v97174 (stack83)
        %v97179 = vadd.f32 %v97177, 1.0 (stack88)
        %v97180 = vlog2.pop %v97179 (stack89)
        %v97181 = vmul.f32 %v97180, 0.6931472 (stack90)
        %v97182 = vmul.f32 -0.5, %v97177 (stack91)
        %v97183 = vadd.f32 %v97182, 1.0 (stack92)
        %v97184 = vmul.f32 %v97183, %v97177 (stack93)
        %v97185 = vand.u32 2147483647, %v97177 (stack94)
        %vm97186 = vcmp.lt.f32.partialorder %v97185, 0.0004427343 (stack95)
        %v97187 = vsel /*vm=*/%vm97186, /*on_true_vy=*/%v97184, /*on_false_vx=*/%v97181 (stack96)
        %v97188 = vxor.u32 %v97187, 2147483648 (stack87)
        %vm97191 = vcmp.lt.f32.partialorder %v97188, 5.0 (stack86)
        %v97196 = vsel /*vm=*/%vm97191, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v97200 = vsel /*vm=*/%vm97191, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v97204 = vsel /*vm=*/%vm97191, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v97208 = vsel /*vm=*/%vm97191, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v97212 = vsel /*vm=*/%vm97191, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v97216 = vsel /*vm=*/%vm97191, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v97220 = vsel /*vm=*/%vm97191, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v97224 = vsel /*vm=*/%vm97191, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v97228 = vsel /*vm=*/%vm97191, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v97232 = vadd.f32 %v97188, -2.5 (stack82)
        %v97234 = vrsqrt.pop %v97188 (stack97)
        %v97235 = vmul.f32 %v97188, %v97234 (stack98)
        %vm97236 = vcmp.eq.f32.partialorder %v97188, inf (stack99)
        %v97237 = vsel /*vm=*/%vm97236, /*on_true_vy=*/%v97188, /*on_false_vx=*/%v97235 (stack100)
        %vm97238 = vcmp.eq.f32.partialorder %v97188, 0.0 (stack101)
        %v97239 = vand.u32 %v97188, 2147483648 (stack102)
        %v97240 = vsel /*vm=*/%vm97238, /*on_true_vy=*/%v97239, /*on_false_vx=*/%v97237 (stack103)
        %v97243 = vadd.f32 %v97240, -3.0 (stack82)
        %v97247 = vsel /*vm=*/%vm97191, /*on_true_vy=*/%v97232, /*on_false_vx=*/%v97243 (stack72)
        %v97251 = vmul.f32 %v97228, %v97247 (stack83)
        %v97255 = vadd.f32 %v97224, %v97251 (stack82)
        %v97259 = vmul.f32 %v97255, %v97247 (stack83)
        %v97263 = vadd.f32 %v97220, %v97259 (stack82)
        %v97267 = vmul.f32 %v97263, %v97247 (stack83)
        %v97271 = vadd.f32 %v97216, %v97267 (stack82)
        %v97275 = vmul.f32 %v97271, %v97247 (stack83)
        %v97279 = vadd.f32 %v97212, %v97275 (stack82)
        %v97283 = vmul.f32 %v97279, %v97247 (stack83)
        %v97287 = vadd.f32 %v97208, %v97283 (stack82)
        %v97291 = vmul.f32 %v97287, %v97247 (stack83)
        %v97295 = vadd.f32 %v97204, %v97291 (stack82)
        %v97299 = vmul.f32 %v97295, %v97247 (stack83)
        %v97303 = vadd.f32 %v97200, %v97299 (stack82)
        %v97307 = vmul.f32 %v97303, %v97247 (stack83)
        %v97311 = vadd.f32 %v97196, %v97307 (stack82)
        %v97315 = vmul.f32 %v97311, %v97162 (stack83)
        %v97319 = vsel /*vm=*/%vm97167, /*on_true_vy=*/%v97172, /*on_false_vx=*/%v97315 (stack72)
        %v97323 = vmul.f32 %v97319, 1.4140625 (stack83)
        %s97325 = scalar_lea.vmem %s280, 996 [#allocation0] (stack107)
        %v97326 = vpack.c.bf16 0.0, %v97323 (stack104)
        %97327 = vst [vmem:[%s97325] sm:$0xf] /*vst_source=*/%v97326 (stack105)
        %s97328 = sadd.s32 %s339, 208 (stack106)
        %s97329 = sshrl.u32 %s97328, 10 (stack49)
        %p97330 = scmp.lt.s32.totalorder 1, %s97329 (stack50)
        %s97331 = scalar_select /*predicate=*/%p97330, /*on_true=*/1, /*on_false=*/%s97329 (stack51)
        %s97332 = sand.u32 %s97328, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s97333 = sshrl.u32 %s97332, 7 (stack53)
        %s97334 = sand.u32 %s97332, 127 /* smod.u32 w/div 128 */ (stack54)
        %s97335 = smul.addr %s97331, 8 (stack55)
        %s97336 = scalar_lea.vmem %s3, %s97335 (stack56)
        %s97338 = scalar_lea.vmem %s97336, %s97333 (stack57)
        %v97339 = vld [vmem:[%s97338] ss:$0 sm:$0xff] (stack58)
        %s97340 = sand.u32 %s97334, 255 (stack59)
        %s97342 = sor.u32 256, %s97340 (stack60)
        %97343 = vbcast.lane.b32.xlu0 %v97339, %s97342 (stack61)
        %v97344 = vpop.permute.xlu0 %97343 (stack62)
        %s97345 = sadd.s32 %s347, 208 (stack106)
        %s97346 = sshrl.u32 %s97345, 10 (stack49)
        %p97347 = scmp.lt.s32.totalorder 1, %s97346 (stack50)
        %s97348 = scalar_select /*predicate=*/%p97347, /*on_true=*/1, /*on_false=*/%s97346 (stack51)
        %s97349 = sand.u32 %s97345, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s97350 = sshrl.u32 %s97349, 7 (stack53)
        %s97351 = sand.u32 %s97349, 127 /* smod.u32 w/div 128 */ (stack54)
        %s97352 = smul.addr %s97348, 8 (stack55)
        %s97353 = scalar_lea.vmem %s5, %s97352 (stack56)
        %s97355 = scalar_lea.vmem %s97353, %s97350 (stack57)
        %v97356 = vld [vmem:[%s97355] ss:$0 sm:$0xff] (stack58)
        %s97357 = sand.u32 %s97351, 255 (stack59)
        %s97359 = sor.u32 256, %s97357 (stack60)
        %97360 = vbcast.lane.b32.xlu0 %v97356, %s97359 (stack61)
        %v97361 = vpop.permute.xlu0 %97360 (stack62)
        %v97364 = vadd.s32 %v408, %v97361 (stack65)
        %s97366 = smul.u32 128, %s27 (stack66)
        %v97367 = vlaneseq (stack67)
        %v97368 = vand.u32 %v97367, 127 (stack68)
        %v97369 = vstv %s97366 (stack69)
        %v97370 = vadd.s32 %v97368, %v97369 (stack70)
        %v97374 = vadd.s32 %v97364, %v97370 (stack65)
        %vm97378 = vcmp.lt.u32.totalorder %v97374, %v97364 (stack71)
        %vm97383 = vcmp.lt.u32.totalorder %v97364, %v408 (stack71)
        %v97388 = vadd.s32 %v380, %v97344 (stack65)
        %v97392 = vadd.s32 %v97388, 1 (stack65)
        %v97396 = vsel /*vm=*/%vm97383, /*on_true_vy=*/%v97392, /*on_false_vx=*/%v97388 (stack72)
        %v97400 = vadd.s32 %v97396, 1 (stack65)
        %v97404 = vsel /*vm=*/%vm97378, /*on_true_vy=*/%v97400, /*on_false_vx=*/%v97396 (stack72)
        %v97409 = vadd.s32 %v97404, %v10 (stack65)
        %v97413 = vadd.s32 %v97374, %v9 (stack65)
        %v97417 = vadd.s32 %v97409, %v97413 (stack65)
        %v97419 = vshll.u32 %v97413, 13 (stack73)
        %v97420 = vshrl.u32 %v97413, 19 (stack74)
        %v97421 = vor.u32 %v97419, %v97420 (stack75)
        %v97422 = vxor.u32 %v97417, %v97421 (stack76)
        %v97425 = vadd.s32 %v97417, %v97422 (stack65)
        %v97427 = vshll.u32 %v97422, 15 (stack73)
        %v97428 = vshrl.u32 %v97422, 17 (stack74)
        %v97429 = vor.u32 %v97427, %v97428 (stack75)
        %v97430 = vxor.u32 %v97425, %v97429 (stack76)
        %v97433 = vadd.s32 %v97425, %v97430 (stack65)
        %v97435 = vshll.u32 %v97430, 26 (stack73)
        %v97436 = vshrl.u32 %v97430, 6 (stack74)
        %v97437 = vor.u32 %v97435, %v97436 (stack75)
        %v97438 = vxor.u32 %v97433, %v97437 (stack76)
        %v97441 = vadd.s32 %v97433, %v97438 (stack65)
        %v97445 = vadd.s32 %v97441, %v9 (stack65)
        %v97447 = vshll.u32 %v97438, 6 (stack73)
        %v97448 = vshrl.u32 %v97438, 26 (stack74)
        %v97449 = vor.u32 %v97447, %v97448 (stack75)
        %v97450 = vxor.u32 %v97441, %v97449 (stack76)
        %v97453 = vadd.s32 %v97450, %v8 (stack65)
        %v97457 = vadd.s32 %v97453, 1 (stack65)
        %v97461 = vadd.s32 %v97445, %v97457 (stack65)
        %v97463 = vshll.u32 %v97457, 17 (stack73)
        %v97464 = vshrl.u32 %v97457, 15 (stack74)
        %v97465 = vor.u32 %v97463, %v97464 (stack75)
        %v97466 = vxor.u32 %v97461, %v97465 (stack76)
        %v97469 = vadd.s32 %v97461, %v97466 (stack65)
        %v97471 = vshll.u32 %v97466, 29 (stack73)
        %v97472 = vshrl.u32 %v97466, 3 (stack74)
        %v97473 = vor.u32 %v97471, %v97472 (stack75)
        %v97474 = vxor.u32 %v97469, %v97473 (stack76)
        %v97477 = vadd.s32 %v97469, %v97474 (stack65)
        %v97479 = vshll.u32 %v97474, 16 (stack73)
        %v97480 = vshrl.u32 %v97474, 16 (stack74)
        %v97481 = vor.u32 %v97479, %v97480 (stack75)
        %v97482 = vxor.u32 %v97477, %v97481 (stack76)
        %v97485 = vadd.s32 %v97477, %v97482 (stack65)
        %v97489 = vadd.s32 %v97485, %v8 (stack65)
        %v97491 = vshll.u32 %v97482, 24 (stack73)
        %v97492 = vshrl.u32 %v97482, 8 (stack74)
        %v97493 = vor.u32 %v97491, %v97492 (stack75)
        %v97494 = vxor.u32 %v97485, %v97493 (stack76)
        %v97497 = vadd.s32 %v97494, %v10 (stack65)
        %v97501 = vadd.s32 %v97497, 2 (stack65)
        %v97505 = vadd.s32 %v97489, %v97501 (stack65)
        %v97507 = vshll.u32 %v97501, 13 (stack73)
        %v97508 = vshrl.u32 %v97501, 19 (stack74)
        %v97509 = vor.u32 %v97507, %v97508 (stack75)
        %v97510 = vxor.u32 %v97505, %v97509 (stack76)
        %v97513 = vadd.s32 %v97505, %v97510 (stack65)
        %v97515 = vshll.u32 %v97510, 15 (stack73)
        %v97516 = vshrl.u32 %v97510, 17 (stack74)
        %v97517 = vor.u32 %v97515, %v97516 (stack75)
        %v97518 = vxor.u32 %v97513, %v97517 (stack76)
        %v97521 = vadd.s32 %v97513, %v97518 (stack65)
        %v97523 = vshll.u32 %v97518, 26 (stack73)
        %v97524 = vshrl.u32 %v97518, 6 (stack74)
        %v97525 = vor.u32 %v97523, %v97524 (stack75)
        %v97526 = vxor.u32 %v97521, %v97525 (stack76)
        %v97529 = vadd.s32 %v97521, %v97526 (stack65)
        %v97533 = vadd.s32 %v97529, %v10 (stack65)
        %v97535 = vshll.u32 %v97526, 6 (stack73)
        %v97536 = vshrl.u32 %v97526, 26 (stack74)
        %v97537 = vor.u32 %v97535, %v97536 (stack75)
        %v97538 = vxor.u32 %v97529, %v97537 (stack76)
        %v97541 = vadd.s32 %v97538, %v9 (stack65)
        %v97545 = vadd.s32 %v97541, 3 (stack65)
        %v97549 = vadd.s32 %v97533, %v97545 (stack65)
        %v97551 = vshll.u32 %v97545, 17 (stack73)
        %v97552 = vshrl.u32 %v97545, 15 (stack74)
        %v97553 = vor.u32 %v97551, %v97552 (stack75)
        %v97554 = vxor.u32 %v97549, %v97553 (stack76)
        %v97557 = vadd.s32 %v97549, %v97554 (stack65)
        %v97559 = vshll.u32 %v97554, 29 (stack73)
        %v97560 = vshrl.u32 %v97554, 3 (stack74)
        %v97561 = vor.u32 %v97559, %v97560 (stack75)
        %v97562 = vxor.u32 %v97557, %v97561 (stack76)
        %v97565 = vadd.s32 %v97557, %v97562 (stack65)
        %v97567 = vshll.u32 %v97562, 16 (stack73)
        %v97568 = vshrl.u32 %v97562, 16 (stack74)
        %v97569 = vor.u32 %v97567, %v97568 (stack75)
        %v97570 = vxor.u32 %v97565, %v97569 (stack76)
        %v97573 = vadd.s32 %v97565, %v97570 (stack65)
        %v97577 = vadd.s32 %v97573, %v9 (stack65)
        %v97579 = vshll.u32 %v97570, 24 (stack73)
        %v97580 = vshrl.u32 %v97570, 8 (stack74)
        %v97581 = vor.u32 %v97579, %v97580 (stack75)
        %v97582 = vxor.u32 %v97573, %v97581 (stack76)
        %v97585 = vadd.s32 %v97582, %v8 (stack65)
        %v97589 = vadd.s32 %v97585, 4 (stack65)
        %v97593 = vadd.s32 %v97577, %v97589 (stack65)
        %v97595 = vshll.u32 %v97589, 13 (stack73)
        %v97596 = vshrl.u32 %v97589, 19 (stack74)
        %v97597 = vor.u32 %v97595, %v97596 (stack75)
        %v97598 = vxor.u32 %v97593, %v97597 (stack76)
        %v97601 = vadd.s32 %v97593, %v97598 (stack65)
        %v97603 = vshll.u32 %v97598, 15 (stack73)
        %v97604 = vshrl.u32 %v97598, 17 (stack74)
        %v97605 = vor.u32 %v97603, %v97604 (stack75)
        %v97606 = vxor.u32 %v97601, %v97605 (stack76)
        %v97609 = vadd.s32 %v97601, %v97606 (stack65)
        %v97611 = vshll.u32 %v97606, 26 (stack73)
        %v97612 = vshrl.u32 %v97606, 6 (stack74)
        %v97613 = vor.u32 %v97611, %v97612 (stack75)
        %v97614 = vxor.u32 %v97609, %v97613 (stack76)
        %v97617 = vadd.s32 %v97609, %v97614 (stack65)
        %v97621 = vadd.s32 %v97617, %v8 (stack65)
        %v97623 = vshll.u32 %v97614, 6 (stack73)
        %v97624 = vshrl.u32 %v97614, 26 (stack74)
        %v97625 = vor.u32 %v97623, %v97624 (stack75)
        %v97626 = vxor.u32 %v97617, %v97625 (stack76)
        %v97629 = vadd.s32 %v97626, %v10 (stack65)
        %v97633 = vadd.s32 %v97629, 5 (stack65)
        %v97635 = vxor.u32 %v97621, %v97633 (stack76)
        %v97636 = vand.u32.u8 %v97635, 255 (stack77)
        %v97637 = vand.u32 %v97636, 65535 (stack78)
        %v97638 = vshrl.u32 %v97637, 1 (stack79)
        %v97639 = vor.u32 %v97638, 16256 (stack75)
        %v97640 = vand.u32.u16 %v97639, 65535 (stack80)
        %v97641 = vunpack.i.l.bf16 %v97640 (stack81)
        %v97645 = vadd.f32 %v97641, -1.0 (stack82)
        %v97649 = vmul.f32 %v97645, 2.0 (stack83)
        %v97653 = vadd.f32 %v97649, -0.99609375 (stack82)
        %v97657 = vmax.f32 -0.99609375, %v97653 (stack84)
        %v97659 = vand.u32 2147483647, %v97657 (stack85)
        %vm97662 = vcmp.eq.f32.partialorder %v97659, 1.0 (stack86)
        %v97667 = vmul.f32 %v97657, inf (stack83)
        %v97669 = vxor.u32 %v97657, 2147483648 (stack87)
        %v97672 = vmul.f32 %v97657, %v97669 (stack83)
        %v97674 = vadd.f32 %v97672, 1.0 (stack88)
        %v97675 = vlog2.pop %v97674 (stack89)
        %v97676 = vmul.f32 %v97675, 0.6931472 (stack90)
        %v97677 = vmul.f32 -0.5, %v97672 (stack91)
        %v97678 = vadd.f32 %v97677, 1.0 (stack92)
        %v97679 = vmul.f32 %v97678, %v97672 (stack93)
        %v97680 = vand.u32 2147483647, %v97672 (stack94)
        %vm97681 = vcmp.lt.f32.partialorder %v97680, 0.0004427343 (stack95)
        %v97682 = vsel /*vm=*/%vm97681, /*on_true_vy=*/%v97679, /*on_false_vx=*/%v97676 (stack96)
        %v97683 = vxor.u32 %v97682, 2147483648 (stack87)
        %vm97686 = vcmp.lt.f32.partialorder %v97683, 5.0 (stack86)
        %v97691 = vsel /*vm=*/%vm97686, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v97695 = vsel /*vm=*/%vm97686, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v97699 = vsel /*vm=*/%vm97686, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v97703 = vsel /*vm=*/%vm97686, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v97707 = vsel /*vm=*/%vm97686, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v97711 = vsel /*vm=*/%vm97686, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v97715 = vsel /*vm=*/%vm97686, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v97719 = vsel /*vm=*/%vm97686, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v97723 = vsel /*vm=*/%vm97686, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v97727 = vadd.f32 %v97683, -2.5 (stack82)
        %v97729 = vrsqrt.pop %v97683 (stack97)
        %v97730 = vmul.f32 %v97683, %v97729 (stack98)
        %vm97731 = vcmp.eq.f32.partialorder %v97683, inf (stack99)
        %v97732 = vsel /*vm=*/%vm97731, /*on_true_vy=*/%v97683, /*on_false_vx=*/%v97730 (stack100)
        %vm97733 = vcmp.eq.f32.partialorder %v97683, 0.0 (stack101)
        %v97734 = vand.u32 %v97683, 2147483648 (stack102)
        %v97735 = vsel /*vm=*/%vm97733, /*on_true_vy=*/%v97734, /*on_false_vx=*/%v97732 (stack103)
        %v97738 = vadd.f32 %v97735, -3.0 (stack82)
        %v97742 = vsel /*vm=*/%vm97686, /*on_true_vy=*/%v97727, /*on_false_vx=*/%v97738 (stack72)
        %v97746 = vmul.f32 %v97723, %v97742 (stack83)
        %v97750 = vadd.f32 %v97719, %v97746 (stack82)
        %v97754 = vmul.f32 %v97750, %v97742 (stack83)
        %v97758 = vadd.f32 %v97715, %v97754 (stack82)
        %v97762 = vmul.f32 %v97758, %v97742 (stack83)
        %v97766 = vadd.f32 %v97711, %v97762 (stack82)
        %v97770 = vmul.f32 %v97766, %v97742 (stack83)
        %v97774 = vadd.f32 %v97707, %v97770 (stack82)
        %v97778 = vmul.f32 %v97774, %v97742 (stack83)
        %v97782 = vadd.f32 %v97703, %v97778 (stack82)
        %v97786 = vmul.f32 %v97782, %v97742 (stack83)
        %v97790 = vadd.f32 %v97699, %v97786 (stack82)
        %v97794 = vmul.f32 %v97790, %v97742 (stack83)
        %v97798 = vadd.f32 %v97695, %v97794 (stack82)
        %v97802 = vmul.f32 %v97798, %v97742 (stack83)
        %v97806 = vadd.f32 %v97691, %v97802 (stack82)
        %v97810 = vmul.f32 %v97806, %v97657 (stack83)
        %v97814 = vsel /*vm=*/%vm97662, /*on_true_vy=*/%v97667, /*on_false_vx=*/%v97810 (stack72)
        %v97818 = vmul.f32 %v97814, 1.4140625 (stack83)
        %s97820 = scalar_lea.vmem %s280, 104 [#allocation0] (stack107)
        %v97821 = vpack.c.bf16 0.0, %v97818 (stack104)
        %97822 = vst [vmem:[%s97820] sm:$0xf] /*vst_source=*/%v97821 (stack105)
        %v97825 = vadd.s32 %v894, %v97361 (stack65)
        %s97827 = smul.u32 128, %s27 (stack66)
        %v97828 = vlaneseq (stack67)
        %v97829 = vand.u32 %v97828, 127 (stack68)
        %v97830 = vstv %s97827 (stack69)
        %v97831 = vadd.s32 %v97829, %v97830 (stack70)
        %v97835 = vadd.s32 %v97825, %v97831 (stack65)
        %vm97839 = vcmp.lt.u32.totalorder %v97835, %v97825 (stack71)
        %vm97844 = vcmp.lt.u32.totalorder %v97825, %v894 (stack71)
        %v97849 = vadd.s32 %v881, %v97344 (stack65)
        %v97853 = vadd.s32 %v97849, 1 (stack65)
        %v97857 = vsel /*vm=*/%vm97844, /*on_true_vy=*/%v97853, /*on_false_vx=*/%v97849 (stack72)
        %v97861 = vadd.s32 %v97857, 1 (stack65)
        %v97865 = vsel /*vm=*/%vm97839, /*on_true_vy=*/%v97861, /*on_false_vx=*/%v97857 (stack72)
        %v97870 = vadd.s32 %v97865, %v10 (stack65)
        %v97874 = vadd.s32 %v97835, %v9 (stack65)
        %v97878 = vadd.s32 %v97870, %v97874 (stack65)
        %v97880 = vshll.u32 %v97874, 13 (stack73)
        %v97881 = vshrl.u32 %v97874, 19 (stack74)
        %v97882 = vor.u32 %v97880, %v97881 (stack75)
        %v97883 = vxor.u32 %v97878, %v97882 (stack76)
        %v97886 = vadd.s32 %v97878, %v97883 (stack65)
        %v97888 = vshll.u32 %v97883, 15 (stack73)
        %v97889 = vshrl.u32 %v97883, 17 (stack74)
        %v97890 = vor.u32 %v97888, %v97889 (stack75)
        %v97891 = vxor.u32 %v97886, %v97890 (stack76)
        %v97894 = vadd.s32 %v97886, %v97891 (stack65)
        %v97896 = vshll.u32 %v97891, 26 (stack73)
        %v97897 = vshrl.u32 %v97891, 6 (stack74)
        %v97898 = vor.u32 %v97896, %v97897 (stack75)
        %v97899 = vxor.u32 %v97894, %v97898 (stack76)
        %v97902 = vadd.s32 %v97894, %v97899 (stack65)
        %v97906 = vadd.s32 %v97902, %v9 (stack65)
        %v97908 = vshll.u32 %v97899, 6 (stack73)
        %v97909 = vshrl.u32 %v97899, 26 (stack74)
        %v97910 = vor.u32 %v97908, %v97909 (stack75)
        %v97911 = vxor.u32 %v97902, %v97910 (stack76)
        %v97914 = vadd.s32 %v97911, %v8 (stack65)
        %v97918 = vadd.s32 %v97914, 1 (stack65)
        %v97922 = vadd.s32 %v97906, %v97918 (stack65)
        %v97924 = vshll.u32 %v97918, 17 (stack73)
        %v97925 = vshrl.u32 %v97918, 15 (stack74)
        %v97926 = vor.u32 %v97924, %v97925 (stack75)
        %v97927 = vxor.u32 %v97922, %v97926 (stack76)
        %v97930 = vadd.s32 %v97922, %v97927 (stack65)
        %v97932 = vshll.u32 %v97927, 29 (stack73)
        %v97933 = vshrl.u32 %v97927, 3 (stack74)
        %v97934 = vor.u32 %v97932, %v97933 (stack75)
        %v97935 = vxor.u32 %v97930, %v97934 (stack76)
        %v97938 = vadd.s32 %v97930, %v97935 (stack65)
        %v97940 = vshll.u32 %v97935, 16 (stack73)
        %v97941 = vshrl.u32 %v97935, 16 (stack74)
        %v97942 = vor.u32 %v97940, %v97941 (stack75)
        %v97943 = vxor.u32 %v97938, %v97942 (stack76)
        %v97946 = vadd.s32 %v97938, %v97943 (stack65)
        %v97950 = vadd.s32 %v97946, %v8 (stack65)
        %v97952 = vshll.u32 %v97943, 24 (stack73)
        %v97953 = vshrl.u32 %v97943, 8 (stack74)
        %v97954 = vor.u32 %v97952, %v97953 (stack75)
        %v97955 = vxor.u32 %v97946, %v97954 (stack76)
        %v97958 = vadd.s32 %v97955, %v10 (stack65)
        %v97962 = vadd.s32 %v97958, 2 (stack65)
        %v97966 = vadd.s32 %v97950, %v97962 (stack65)
        %v97968 = vshll.u32 %v97962, 13 (stack73)
        %v97969 = vshrl.u32 %v97962, 19 (stack74)
        %v97970 = vor.u32 %v97968, %v97969 (stack75)
        %v97971 = vxor.u32 %v97966, %v97970 (stack76)
        %v97974 = vadd.s32 %v97966, %v97971 (stack65)
        %v97976 = vshll.u32 %v97971, 15 (stack73)
        %v97977 = vshrl.u32 %v97971, 17 (stack74)
        %v97978 = vor.u32 %v97976, %v97977 (stack75)
        %v97979 = vxor.u32 %v97974, %v97978 (stack76)
        %v97982 = vadd.s32 %v97974, %v97979 (stack65)
        %v97984 = vshll.u32 %v97979, 26 (stack73)
        %v97985 = vshrl.u32 %v97979, 6 (stack74)
        %v97986 = vor.u32 %v97984, %v97985 (stack75)
        %v97987 = vxor.u32 %v97982, %v97986 (stack76)
        %v97990 = vadd.s32 %v97982, %v97987 (stack65)
        %v97994 = vadd.s32 %v97990, %v10 (stack65)
        %v97996 = vshll.u32 %v97987, 6 (stack73)
        %v97997 = vshrl.u32 %v97987, 26 (stack74)
        %v97998 = vor.u32 %v97996, %v97997 (stack75)
        %v97999 = vxor.u32 %v97990, %v97998 (stack76)
        %v98002 = vadd.s32 %v97999, %v9 (stack65)
        %v98006 = vadd.s32 %v98002, 3 (stack65)
        %v98010 = vadd.s32 %v97994, %v98006 (stack65)
        %v98012 = vshll.u32 %v98006, 17 (stack73)
        %v98013 = vshrl.u32 %v98006, 15 (stack74)
        %v98014 = vor.u32 %v98012, %v98013 (stack75)
        %v98015 = vxor.u32 %v98010, %v98014 (stack76)
        %v98018 = vadd.s32 %v98010, %v98015 (stack65)
        %v98020 = vshll.u32 %v98015, 29 (stack73)
        %v98021 = vshrl.u32 %v98015, 3 (stack74)
        %v98022 = vor.u32 %v98020, %v98021 (stack75)
        %v98023 = vxor.u32 %v98018, %v98022 (stack76)
        %v98026 = vadd.s32 %v98018, %v98023 (stack65)
        %v98028 = vshll.u32 %v98023, 16 (stack73)
        %v98029 = vshrl.u32 %v98023, 16 (stack74)
        %v98030 = vor.u32 %v98028, %v98029 (stack75)
        %v98031 = vxor.u32 %v98026, %v98030 (stack76)
        %v98034 = vadd.s32 %v98026, %v98031 (stack65)
        %v98038 = vadd.s32 %v98034, %v9 (stack65)
        %v98040 = vshll.u32 %v98031, 24 (stack73)
        %v98041 = vshrl.u32 %v98031, 8 (stack74)
        %v98042 = vor.u32 %v98040, %v98041 (stack75)
        %v98043 = vxor.u32 %v98034, %v98042 (stack76)
        %v98046 = vadd.s32 %v98043, %v8 (stack65)
        %v98050 = vadd.s32 %v98046, 4 (stack65)
        %v98054 = vadd.s32 %v98038, %v98050 (stack65)
        %v98056 = vshll.u32 %v98050, 13 (stack73)
        %v98057 = vshrl.u32 %v98050, 19 (stack74)
        %v98058 = vor.u32 %v98056, %v98057 (stack75)
        %v98059 = vxor.u32 %v98054, %v98058 (stack76)
        %v98062 = vadd.s32 %v98054, %v98059 (stack65)
        %v98064 = vshll.u32 %v98059, 15 (stack73)
        %v98065 = vshrl.u32 %v98059, 17 (stack74)
        %v98066 = vor.u32 %v98064, %v98065 (stack75)
        %v98067 = vxor.u32 %v98062, %v98066 (stack76)
        %v98070 = vadd.s32 %v98062, %v98067 (stack65)
        %v98072 = vshll.u32 %v98067, 26 (stack73)
        %v98073 = vshrl.u32 %v98067, 6 (stack74)
        %v98074 = vor.u32 %v98072, %v98073 (stack75)
        %v98075 = vxor.u32 %v98070, %v98074 (stack76)
        %v98078 = vadd.s32 %v98070, %v98075 (stack65)
        %v98082 = vadd.s32 %v98078, %v8 (stack65)
        %v98084 = vshll.u32 %v98075, 6 (stack73)
        %v98085 = vshrl.u32 %v98075, 26 (stack74)
        %v98086 = vor.u32 %v98084, %v98085 (stack75)
        %v98087 = vxor.u32 %v98078, %v98086 (stack76)
        %v98090 = vadd.s32 %v98087, %v10 (stack65)
        %v98094 = vadd.s32 %v98090, 5 (stack65)
        %v98096 = vxor.u32 %v98082, %v98094 (stack76)
        %v98097 = vand.u32.u8 %v98096, 255 (stack77)
        %v98098 = vand.u32 %v98097, 65535 (stack78)
        %v98099 = vshrl.u32 %v98098, 1 (stack79)
        %v98100 = vor.u32 %v98099, 16256 (stack75)
        %v98101 = vand.u32.u16 %v98100, 65535 (stack80)
        %v98102 = vunpack.i.l.bf16 %v98101 (stack81)
        %v98106 = vadd.f32 %v98102, -1.0 (stack82)
        %v98110 = vmul.f32 %v98106, 2.0 (stack83)
        %v98114 = vadd.f32 %v98110, -0.99609375 (stack82)
        %v98118 = vmax.f32 -0.99609375, %v98114 (stack84)
        %v98120 = vand.u32 2147483647, %v98118 (stack85)
        %vm98123 = vcmp.eq.f32.partialorder %v98120, 1.0 (stack86)
        %v98128 = vmul.f32 %v98118, inf (stack83)
        %v98130 = vxor.u32 %v98118, 2147483648 (stack87)
        %v98133 = vmul.f32 %v98118, %v98130 (stack83)
        %v98135 = vadd.f32 %v98133, 1.0 (stack88)
        %v98136 = vlog2.pop %v98135 (stack89)
        %v98137 = vmul.f32 %v98136, 0.6931472 (stack90)
        %v98138 = vmul.f32 -0.5, %v98133 (stack91)
        %v98139 = vadd.f32 %v98138, 1.0 (stack92)
        %v98140 = vmul.f32 %v98139, %v98133 (stack93)
        %v98141 = vand.u32 2147483647, %v98133 (stack94)
        %vm98142 = vcmp.lt.f32.partialorder %v98141, 0.0004427343 (stack95)
        %v98143 = vsel /*vm=*/%vm98142, /*on_true_vy=*/%v98140, /*on_false_vx=*/%v98137 (stack96)
        %v98144 = vxor.u32 %v98143, 2147483648 (stack87)
        %vm98147 = vcmp.lt.f32.partialorder %v98144, 5.0 (stack86)
        %v98152 = vsel /*vm=*/%vm98147, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v98156 = vsel /*vm=*/%vm98147, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v98160 = vsel /*vm=*/%vm98147, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v98164 = vsel /*vm=*/%vm98147, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v98168 = vsel /*vm=*/%vm98147, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v98172 = vsel /*vm=*/%vm98147, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v98176 = vsel /*vm=*/%vm98147, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v98180 = vsel /*vm=*/%vm98147, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v98184 = vsel /*vm=*/%vm98147, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v98188 = vadd.f32 %v98144, -2.5 (stack82)
        %v98190 = vrsqrt.pop %v98144 (stack97)
        %v98191 = vmul.f32 %v98144, %v98190 (stack98)
        %vm98192 = vcmp.eq.f32.partialorder %v98144, inf (stack99)
        %v98193 = vsel /*vm=*/%vm98192, /*on_true_vy=*/%v98144, /*on_false_vx=*/%v98191 (stack100)
        %vm98194 = vcmp.eq.f32.partialorder %v98144, 0.0 (stack101)
        %v98195 = vand.u32 %v98144, 2147483648 (stack102)
        %v98196 = vsel /*vm=*/%vm98194, /*on_true_vy=*/%v98195, /*on_false_vx=*/%v98193 (stack103)
        %v98199 = vadd.f32 %v98196, -3.0 (stack82)
        %v98203 = vsel /*vm=*/%vm98147, /*on_true_vy=*/%v98188, /*on_false_vx=*/%v98199 (stack72)
        %v98207 = vmul.f32 %v98184, %v98203 (stack83)
        %v98211 = vadd.f32 %v98180, %v98207 (stack82)
        %v98215 = vmul.f32 %v98211, %v98203 (stack83)
        %v98219 = vadd.f32 %v98176, %v98215 (stack82)
        %v98223 = vmul.f32 %v98219, %v98203 (stack83)
        %v98227 = vadd.f32 %v98172, %v98223 (stack82)
        %v98231 = vmul.f32 %v98227, %v98203 (stack83)
        %v98235 = vadd.f32 %v98168, %v98231 (stack82)
        %v98239 = vmul.f32 %v98235, %v98203 (stack83)
        %v98243 = vadd.f32 %v98164, %v98239 (stack82)
        %v98247 = vmul.f32 %v98243, %v98203 (stack83)
        %v98251 = vadd.f32 %v98160, %v98247 (stack82)
        %v98255 = vmul.f32 %v98251, %v98203 (stack83)
        %v98259 = vadd.f32 %v98156, %v98255 (stack82)
        %v98263 = vmul.f32 %v98259, %v98203 (stack83)
        %v98267 = vadd.f32 %v98152, %v98263 (stack82)
        %v98271 = vmul.f32 %v98267, %v98118 (stack83)
        %v98275 = vsel /*vm=*/%vm98123, /*on_true_vy=*/%v98128, /*on_false_vx=*/%v98271 (stack72)
        %v98279 = vmul.f32 %v98275, 1.4140625 (stack83)
        %s98281 = scalar_lea.vmem %s280, 232 [#allocation0] (stack107)
        %v98282 = vpack.c.bf16 0.0, %v98279 (stack104)
        %98283 = vst [vmem:[%s98281] sm:$0xf] /*vst_source=*/%v98282 (stack105)
        %v98286 = vadd.s32 %v1381, %v97361 (stack65)
        %s98288 = smul.u32 128, %s27 (stack66)
        %v98289 = vlaneseq (stack67)
        %v98290 = vand.u32 %v98289, 127 (stack68)
        %v98291 = vstv %s98288 (stack69)
        %v98292 = vadd.s32 %v98290, %v98291 (stack70)
        %v98296 = vadd.s32 %v98286, %v98292 (stack65)
        %vm98300 = vcmp.lt.u32.totalorder %v98296, %v98286 (stack71)
        %vm98305 = vcmp.lt.u32.totalorder %v98286, %v1381 (stack71)
        %v98310 = vadd.s32 %v1368, %v97344 (stack65)
        %v98314 = vadd.s32 %v98310, 1 (stack65)
        %v98318 = vsel /*vm=*/%vm98305, /*on_true_vy=*/%v98314, /*on_false_vx=*/%v98310 (stack72)
        %v98322 = vadd.s32 %v98318, 1 (stack65)
        %v98326 = vsel /*vm=*/%vm98300, /*on_true_vy=*/%v98322, /*on_false_vx=*/%v98318 (stack72)
        %v98331 = vadd.s32 %v98326, %v10 (stack65)
        %v98335 = vadd.s32 %v98296, %v9 (stack65)
        %v98339 = vadd.s32 %v98331, %v98335 (stack65)
        %v98341 = vshll.u32 %v98335, 13 (stack73)
        %v98342 = vshrl.u32 %v98335, 19 (stack74)
        %v98343 = vor.u32 %v98341, %v98342 (stack75)
        %v98344 = vxor.u32 %v98339, %v98343 (stack76)
        %v98347 = vadd.s32 %v98339, %v98344 (stack65)
        %v98349 = vshll.u32 %v98344, 15 (stack73)
        %v98350 = vshrl.u32 %v98344, 17 (stack74)
        %v98351 = vor.u32 %v98349, %v98350 (stack75)
        %v98352 = vxor.u32 %v98347, %v98351 (stack76)
        %v98355 = vadd.s32 %v98347, %v98352 (stack65)
        %v98357 = vshll.u32 %v98352, 26 (stack73)
        %v98358 = vshrl.u32 %v98352, 6 (stack74)
        %v98359 = vor.u32 %v98357, %v98358 (stack75)
        %v98360 = vxor.u32 %v98355, %v98359 (stack76)
        %v98363 = vadd.s32 %v98355, %v98360 (stack65)
        %v98367 = vadd.s32 %v98363, %v9 (stack65)
        %v98369 = vshll.u32 %v98360, 6 (stack73)
        %v98370 = vshrl.u32 %v98360, 26 (stack74)
        %v98371 = vor.u32 %v98369, %v98370 (stack75)
        %v98372 = vxor.u32 %v98363, %v98371 (stack76)
        %v98375 = vadd.s32 %v98372, %v8 (stack65)
        %v98379 = vadd.s32 %v98375, 1 (stack65)
        %v98383 = vadd.s32 %v98367, %v98379 (stack65)
        %v98385 = vshll.u32 %v98379, 17 (stack73)
        %v98386 = vshrl.u32 %v98379, 15 (stack74)
        %v98387 = vor.u32 %v98385, %v98386 (stack75)
        %v98388 = vxor.u32 %v98383, %v98387 (stack76)
        %v98391 = vadd.s32 %v98383, %v98388 (stack65)
        %v98393 = vshll.u32 %v98388, 29 (stack73)
        %v98394 = vshrl.u32 %v98388, 3 (stack74)
        %v98395 = vor.u32 %v98393, %v98394 (stack75)
        %v98396 = vxor.u32 %v98391, %v98395 (stack76)
        %v98399 = vadd.s32 %v98391, %v98396 (stack65)
        %v98401 = vshll.u32 %v98396, 16 (stack73)
        %v98402 = vshrl.u32 %v98396, 16 (stack74)
        %v98403 = vor.u32 %v98401, %v98402 (stack75)
        %v98404 = vxor.u32 %v98399, %v98403 (stack76)
        %v98407 = vadd.s32 %v98399, %v98404 (stack65)
        %v98411 = vadd.s32 %v98407, %v8 (stack65)
        %v98413 = vshll.u32 %v98404, 24 (stack73)
        %v98414 = vshrl.u32 %v98404, 8 (stack74)
        %v98415 = vor.u32 %v98413, %v98414 (stack75)
        %v98416 = vxor.u32 %v98407, %v98415 (stack76)
        %v98419 = vadd.s32 %v98416, %v10 (stack65)
        %v98423 = vadd.s32 %v98419, 2 (stack65)
        %v98427 = vadd.s32 %v98411, %v98423 (stack65)
        %v98429 = vshll.u32 %v98423, 13 (stack73)
        %v98430 = vshrl.u32 %v98423, 19 (stack74)
        %v98431 = vor.u32 %v98429, %v98430 (stack75)
        %v98432 = vxor.u32 %v98427, %v98431 (stack76)
        %v98435 = vadd.s32 %v98427, %v98432 (stack65)
        %v98437 = vshll.u32 %v98432, 15 (stack73)
        %v98438 = vshrl.u32 %v98432, 17 (stack74)
        %v98439 = vor.u32 %v98437, %v98438 (stack75)
        %v98440 = vxor.u32 %v98435, %v98439 (stack76)
        %v98443 = vadd.s32 %v98435, %v98440 (stack65)
        %v98445 = vshll.u32 %v98440, 26 (stack73)
        %v98446 = vshrl.u32 %v98440, 6 (stack74)
        %v98447 = vor.u32 %v98445, %v98446 (stack75)
        %v98448 = vxor.u32 %v98443, %v98447 (stack76)
        %v98451 = vadd.s32 %v98443, %v98448 (stack65)
        %v98455 = vadd.s32 %v98451, %v10 (stack65)
        %v98457 = vshll.u32 %v98448, 6 (stack73)
        %v98458 = vshrl.u32 %v98448, 26 (stack74)
        %v98459 = vor.u32 %v98457, %v98458 (stack75)
        %v98460 = vxor.u32 %v98451, %v98459 (stack76)
        %v98463 = vadd.s32 %v98460, %v9 (stack65)
        %v98467 = vadd.s32 %v98463, 3 (stack65)
        %v98471 = vadd.s32 %v98455, %v98467 (stack65)
        %v98473 = vshll.u32 %v98467, 17 (stack73)
        %v98474 = vshrl.u32 %v98467, 15 (stack74)
        %v98475 = vor.u32 %v98473, %v98474 (stack75)
        %v98476 = vxor.u32 %v98471, %v98475 (stack76)
        %v98479 = vadd.s32 %v98471, %v98476 (stack65)
        %v98481 = vshll.u32 %v98476, 29 (stack73)
        %v98482 = vshrl.u32 %v98476, 3 (stack74)
        %v98483 = vor.u32 %v98481, %v98482 (stack75)
        %v98484 = vxor.u32 %v98479, %v98483 (stack76)
        %v98487 = vadd.s32 %v98479, %v98484 (stack65)
        %v98489 = vshll.u32 %v98484, 16 (stack73)
        %v98490 = vshrl.u32 %v98484, 16 (stack74)
        %v98491 = vor.u32 %v98489, %v98490 (stack75)
        %v98492 = vxor.u32 %v98487, %v98491 (stack76)
        %v98495 = vadd.s32 %v98487, %v98492 (stack65)
        %v98499 = vadd.s32 %v98495, %v9 (stack65)
        %v98501 = vshll.u32 %v98492, 24 (stack73)
        %v98502 = vshrl.u32 %v98492, 8 (stack74)
        %v98503 = vor.u32 %v98501, %v98502 (stack75)
        %v98504 = vxor.u32 %v98495, %v98503 (stack76)
        %v98507 = vadd.s32 %v98504, %v8 (stack65)
        %v98511 = vadd.s32 %v98507, 4 (stack65)
        %v98515 = vadd.s32 %v98499, %v98511 (stack65)
        %v98517 = vshll.u32 %v98511, 13 (stack73)
        %v98518 = vshrl.u32 %v98511, 19 (stack74)
        %v98519 = vor.u32 %v98517, %v98518 (stack75)
        %v98520 = vxor.u32 %v98515, %v98519 (stack76)
        %v98523 = vadd.s32 %v98515, %v98520 (stack65)
        %v98525 = vshll.u32 %v98520, 15 (stack73)
        %v98526 = vshrl.u32 %v98520, 17 (stack74)
        %v98527 = vor.u32 %v98525, %v98526 (stack75)
        %v98528 = vxor.u32 %v98523, %v98527 (stack76)
        %v98531 = vadd.s32 %v98523, %v98528 (stack65)
        %v98533 = vshll.u32 %v98528, 26 (stack73)
        %v98534 = vshrl.u32 %v98528, 6 (stack74)
        %v98535 = vor.u32 %v98533, %v98534 (stack75)
        %v98536 = vxor.u32 %v98531, %v98535 (stack76)
        %v98539 = vadd.s32 %v98531, %v98536 (stack65)
        %v98543 = vadd.s32 %v98539, %v8 (stack65)
        %v98545 = vshll.u32 %v98536, 6 (stack73)
        %v98546 = vshrl.u32 %v98536, 26 (stack74)
        %v98547 = vor.u32 %v98545, %v98546 (stack75)
        %v98548 = vxor.u32 %v98539, %v98547 (stack76)
        %v98551 = vadd.s32 %v98548, %v10 (stack65)
        %v98555 = vadd.s32 %v98551, 5 (stack65)
        %v98557 = vxor.u32 %v98543, %v98555 (stack76)
        %v98558 = vand.u32.u8 %v98557, 255 (stack77)
        %v98559 = vand.u32 %v98558, 65535 (stack78)
        %v98560 = vshrl.u32 %v98559, 1 (stack79)
        %v98561 = vor.u32 %v98560, 16256 (stack75)
        %v98562 = vand.u32.u16 %v98561, 65535 (stack80)
        %v98563 = vunpack.i.l.bf16 %v98562 (stack81)
        %v98567 = vadd.f32 %v98563, -1.0 (stack82)
        %v98571 = vmul.f32 %v98567, 2.0 (stack83)
        %v98575 = vadd.f32 %v98571, -0.99609375 (stack82)
        %v98579 = vmax.f32 -0.99609375, %v98575 (stack84)
        %v98581 = vand.u32 2147483647, %v98579 (stack85)
        %vm98584 = vcmp.eq.f32.partialorder %v98581, 1.0 (stack86)
        %v98589 = vmul.f32 %v98579, inf (stack83)
        %v98591 = vxor.u32 %v98579, 2147483648 (stack87)
        %v98594 = vmul.f32 %v98579, %v98591 (stack83)
        %v98596 = vadd.f32 %v98594, 1.0 (stack88)
        %v98597 = vlog2.pop %v98596 (stack89)
        %v98598 = vmul.f32 %v98597, 0.6931472 (stack90)
        %v98599 = vmul.f32 -0.5, %v98594 (stack91)
        %v98600 = vadd.f32 %v98599, 1.0 (stack92)
        %v98601 = vmul.f32 %v98600, %v98594 (stack93)
        %v98602 = vand.u32 2147483647, %v98594 (stack94)
        %vm98603 = vcmp.lt.f32.partialorder %v98602, 0.0004427343 (stack95)
        %v98604 = vsel /*vm=*/%vm98603, /*on_true_vy=*/%v98601, /*on_false_vx=*/%v98598 (stack96)
        %v98605 = vxor.u32 %v98604, 2147483648 (stack87)
        %vm98608 = vcmp.lt.f32.partialorder %v98605, 5.0 (stack86)
        %v98613 = vsel /*vm=*/%vm98608, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v98617 = vsel /*vm=*/%vm98608, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v98621 = vsel /*vm=*/%vm98608, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v98625 = vsel /*vm=*/%vm98608, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v98629 = vsel /*vm=*/%vm98608, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v98633 = vsel /*vm=*/%vm98608, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v98637 = vsel /*vm=*/%vm98608, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v98641 = vsel /*vm=*/%vm98608, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v98645 = vsel /*vm=*/%vm98608, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v98649 = vadd.f32 %v98605, -2.5 (stack82)
        %v98651 = vrsqrt.pop %v98605 (stack97)
        %v98652 = vmul.f32 %v98605, %v98651 (stack98)
        %vm98653 = vcmp.eq.f32.partialorder %v98605, inf (stack99)
        %v98654 = vsel /*vm=*/%vm98653, /*on_true_vy=*/%v98605, /*on_false_vx=*/%v98652 (stack100)
        %vm98655 = vcmp.eq.f32.partialorder %v98605, 0.0 (stack101)
        %v98656 = vand.u32 %v98605, 2147483648 (stack102)
        %v98657 = vsel /*vm=*/%vm98655, /*on_true_vy=*/%v98656, /*on_false_vx=*/%v98654 (stack103)
        %v98660 = vadd.f32 %v98657, -3.0 (stack82)
        %v98664 = vsel /*vm=*/%vm98608, /*on_true_vy=*/%v98649, /*on_false_vx=*/%v98660 (stack72)
        %v98668 = vmul.f32 %v98645, %v98664 (stack83)
        %v98672 = vadd.f32 %v98641, %v98668 (stack82)
        %v98676 = vmul.f32 %v98672, %v98664 (stack83)
        %v98680 = vadd.f32 %v98637, %v98676 (stack82)
        %v98684 = vmul.f32 %v98680, %v98664 (stack83)
        %v98688 = vadd.f32 %v98633, %v98684 (stack82)
        %v98692 = vmul.f32 %v98688, %v98664 (stack83)
        %v98696 = vadd.f32 %v98629, %v98692 (stack82)
        %v98700 = vmul.f32 %v98696, %v98664 (stack83)
        %v98704 = vadd.f32 %v98625, %v98700 (stack82)
        %v98708 = vmul.f32 %v98704, %v98664 (stack83)
        %v98712 = vadd.f32 %v98621, %v98708 (stack82)
        %v98716 = vmul.f32 %v98712, %v98664 (stack83)
        %v98720 = vadd.f32 %v98617, %v98716 (stack82)
        %v98724 = vmul.f32 %v98720, %v98664 (stack83)
        %v98728 = vadd.f32 %v98613, %v98724 (stack82)
        %v98732 = vmul.f32 %v98728, %v98579 (stack83)
        %v98736 = vsel /*vm=*/%vm98584, /*on_true_vy=*/%v98589, /*on_false_vx=*/%v98732 (stack72)
        %v98740 = vmul.f32 %v98736, 1.4140625 (stack83)
        %s98742 = scalar_lea.vmem %s280, 360 [#allocation0] (stack107)
        %v98743 = vpack.c.bf16 0.0, %v98740 (stack104)
        %98744 = vst [vmem:[%s98742] sm:$0xf] /*vst_source=*/%v98743 (stack105)
        %v98747 = vadd.s32 %v1868, %v97361 (stack65)
        %s98749 = smul.u32 128, %s27 (stack66)
        %v98750 = vlaneseq (stack67)
        %v98751 = vand.u32 %v98750, 127 (stack68)
        %v98752 = vstv %s98749 (stack69)
        %v98753 = vadd.s32 %v98751, %v98752 (stack70)
        %v98757 = vadd.s32 %v98747, %v98753 (stack65)
        %vm98761 = vcmp.lt.u32.totalorder %v98757, %v98747 (stack71)
        %vm98766 = vcmp.lt.u32.totalorder %v98747, %v1868 (stack71)
        %v98771 = vadd.s32 %v1855, %v97344 (stack65)
        %v98775 = vadd.s32 %v98771, 1 (stack65)
        %v98779 = vsel /*vm=*/%vm98766, /*on_true_vy=*/%v98775, /*on_false_vx=*/%v98771 (stack72)
        %v98783 = vadd.s32 %v98779, 1 (stack65)
        %v98787 = vsel /*vm=*/%vm98761, /*on_true_vy=*/%v98783, /*on_false_vx=*/%v98779 (stack72)
        %v98792 = vadd.s32 %v98787, %v10 (stack65)
        %v98796 = vadd.s32 %v98757, %v9 (stack65)
        %v98800 = vadd.s32 %v98792, %v98796 (stack65)
        %v98802 = vshll.u32 %v98796, 13 (stack73)
        %v98803 = vshrl.u32 %v98796, 19 (stack74)
        %v98804 = vor.u32 %v98802, %v98803 (stack75)
        %v98805 = vxor.u32 %v98800, %v98804 (stack76)
        %v98808 = vadd.s32 %v98800, %v98805 (stack65)
        %v98810 = vshll.u32 %v98805, 15 (stack73)
        %v98811 = vshrl.u32 %v98805, 17 (stack74)
        %v98812 = vor.u32 %v98810, %v98811 (stack75)
        %v98813 = vxor.u32 %v98808, %v98812 (stack76)
        %v98816 = vadd.s32 %v98808, %v98813 (stack65)
        %v98818 = vshll.u32 %v98813, 26 (stack73)
        %v98819 = vshrl.u32 %v98813, 6 (stack74)
        %v98820 = vor.u32 %v98818, %v98819 (stack75)
        %v98821 = vxor.u32 %v98816, %v98820 (stack76)
        %v98824 = vadd.s32 %v98816, %v98821 (stack65)
        %v98828 = vadd.s32 %v98824, %v9 (stack65)
        %v98830 = vshll.u32 %v98821, 6 (stack73)
        %v98831 = vshrl.u32 %v98821, 26 (stack74)
        %v98832 = vor.u32 %v98830, %v98831 (stack75)
        %v98833 = vxor.u32 %v98824, %v98832 (stack76)
        %v98836 = vadd.s32 %v98833, %v8 (stack65)
        %v98840 = vadd.s32 %v98836, 1 (stack65)
        %v98844 = vadd.s32 %v98828, %v98840 (stack65)
        %v98846 = vshll.u32 %v98840, 17 (stack73)
        %v98847 = vshrl.u32 %v98840, 15 (stack74)
        %v98848 = vor.u32 %v98846, %v98847 (stack75)
        %v98849 = vxor.u32 %v98844, %v98848 (stack76)
        %v98852 = vadd.s32 %v98844, %v98849 (stack65)
        %v98854 = vshll.u32 %v98849, 29 (stack73)
        %v98855 = vshrl.u32 %v98849, 3 (stack74)
        %v98856 = vor.u32 %v98854, %v98855 (stack75)
        %v98857 = vxor.u32 %v98852, %v98856 (stack76)
        %v98860 = vadd.s32 %v98852, %v98857 (stack65)
        %v98862 = vshll.u32 %v98857, 16 (stack73)
        %v98863 = vshrl.u32 %v98857, 16 (stack74)
        %v98864 = vor.u32 %v98862, %v98863 (stack75)
        %v98865 = vxor.u32 %v98860, %v98864 (stack76)
        %v98868 = vadd.s32 %v98860, %v98865 (stack65)
        %v98872 = vadd.s32 %v98868, %v8 (stack65)
        %v98874 = vshll.u32 %v98865, 24 (stack73)
        %v98875 = vshrl.u32 %v98865, 8 (stack74)
        %v98876 = vor.u32 %v98874, %v98875 (stack75)
        %v98877 = vxor.u32 %v98868, %v98876 (stack76)
        %v98880 = vadd.s32 %v98877, %v10 (stack65)
        %v98884 = vadd.s32 %v98880, 2 (stack65)
        %v98888 = vadd.s32 %v98872, %v98884 (stack65)
        %v98890 = vshll.u32 %v98884, 13 (stack73)
        %v98891 = vshrl.u32 %v98884, 19 (stack74)
        %v98892 = vor.u32 %v98890, %v98891 (stack75)
        %v98893 = vxor.u32 %v98888, %v98892 (stack76)
        %v98896 = vadd.s32 %v98888, %v98893 (stack65)
        %v98898 = vshll.u32 %v98893, 15 (stack73)
        %v98899 = vshrl.u32 %v98893, 17 (stack74)
        %v98900 = vor.u32 %v98898, %v98899 (stack75)
        %v98901 = vxor.u32 %v98896, %v98900 (stack76)
        %v98904 = vadd.s32 %v98896, %v98901 (stack65)
        %v98906 = vshll.u32 %v98901, 26 (stack73)
        %v98907 = vshrl.u32 %v98901, 6 (stack74)
        %v98908 = vor.u32 %v98906, %v98907 (stack75)
        %v98909 = vxor.u32 %v98904, %v98908 (stack76)
        %v98912 = vadd.s32 %v98904, %v98909 (stack65)
        %v98916 = vadd.s32 %v98912, %v10 (stack65)
        %v98918 = vshll.u32 %v98909, 6 (stack73)
        %v98919 = vshrl.u32 %v98909, 26 (stack74)
        %v98920 = vor.u32 %v98918, %v98919 (stack75)
        %v98921 = vxor.u32 %v98912, %v98920 (stack76)
        %v98924 = vadd.s32 %v98921, %v9 (stack65)
        %v98928 = vadd.s32 %v98924, 3 (stack65)
        %v98932 = vadd.s32 %v98916, %v98928 (stack65)
        %v98934 = vshll.u32 %v98928, 17 (stack73)
        %v98935 = vshrl.u32 %v98928, 15 (stack74)
        %v98936 = vor.u32 %v98934, %v98935 (stack75)
        %v98937 = vxor.u32 %v98932, %v98936 (stack76)
        %v98940 = vadd.s32 %v98932, %v98937 (stack65)
        %v98942 = vshll.u32 %v98937, 29 (stack73)
        %v98943 = vshrl.u32 %v98937, 3 (stack74)
        %v98944 = vor.u32 %v98942, %v98943 (stack75)
        %v98945 = vxor.u32 %v98940, %v98944 (stack76)
        %v98948 = vadd.s32 %v98940, %v98945 (stack65)
        %v98950 = vshll.u32 %v98945, 16 (stack73)
        %v98951 = vshrl.u32 %v98945, 16 (stack74)
        %v98952 = vor.u32 %v98950, %v98951 (stack75)
        %v98953 = vxor.u32 %v98948, %v98952 (stack76)
        %v98956 = vadd.s32 %v98948, %v98953 (stack65)
        %v98960 = vadd.s32 %v98956, %v9 (stack65)
        %v98962 = vshll.u32 %v98953, 24 (stack73)
        %v98963 = vshrl.u32 %v98953, 8 (stack74)
        %v98964 = vor.u32 %v98962, %v98963 (stack75)
        %v98965 = vxor.u32 %v98956, %v98964 (stack76)
        %v98968 = vadd.s32 %v98965, %v8 (stack65)
        %v98972 = vadd.s32 %v98968, 4 (stack65)
        %v98976 = vadd.s32 %v98960, %v98972 (stack65)
        %v98978 = vshll.u32 %v98972, 13 (stack73)
        %v98979 = vshrl.u32 %v98972, 19 (stack74)
        %v98980 = vor.u32 %v98978, %v98979 (stack75)
        %v98981 = vxor.u32 %v98976, %v98980 (stack76)
        %v98984 = vadd.s32 %v98976, %v98981 (stack65)
        %v98986 = vshll.u32 %v98981, 15 (stack73)
        %v98987 = vshrl.u32 %v98981, 17 (stack74)
        %v98988 = vor.u32 %v98986, %v98987 (stack75)
        %v98989 = vxor.u32 %v98984, %v98988 (stack76)
        %v98992 = vadd.s32 %v98984, %v98989 (stack65)
        %v98994 = vshll.u32 %v98989, 26 (stack73)
        %v98995 = vshrl.u32 %v98989, 6 (stack74)
        %v98996 = vor.u32 %v98994, %v98995 (stack75)
        %v98997 = vxor.u32 %v98992, %v98996 (stack76)
        %v99000 = vadd.s32 %v98992, %v98997 (stack65)
        %v99004 = vadd.s32 %v99000, %v8 (stack65)
        %v99006 = vshll.u32 %v98997, 6 (stack73)
        %v99007 = vshrl.u32 %v98997, 26 (stack74)
        %v99008 = vor.u32 %v99006, %v99007 (stack75)
        %v99009 = vxor.u32 %v99000, %v99008 (stack76)
        %v99012 = vadd.s32 %v99009, %v10 (stack65)
        %v99016 = vadd.s32 %v99012, 5 (stack65)
        %v99018 = vxor.u32 %v99004, %v99016 (stack76)
        %v99019 = vand.u32.u8 %v99018, 255 (stack77)
        %v99020 = vand.u32 %v99019, 65535 (stack78)
        %v99021 = vshrl.u32 %v99020, 1 (stack79)
        %v99022 = vor.u32 %v99021, 16256 (stack75)
        %v99023 = vand.u32.u16 %v99022, 65535 (stack80)
        %v99024 = vunpack.i.l.bf16 %v99023 (stack81)
        %v99028 = vadd.f32 %v99024, -1.0 (stack82)
        %v99032 = vmul.f32 %v99028, 2.0 (stack83)
        %v99036 = vadd.f32 %v99032, -0.99609375 (stack82)
        %v99040 = vmax.f32 -0.99609375, %v99036 (stack84)
        %v99042 = vand.u32 2147483647, %v99040 (stack85)
        %vm99045 = vcmp.eq.f32.partialorder %v99042, 1.0 (stack86)
        %v99050 = vmul.f32 %v99040, inf (stack83)
        %v99052 = vxor.u32 %v99040, 2147483648 (stack87)
        %v99055 = vmul.f32 %v99040, %v99052 (stack83)
        %v99057 = vadd.f32 %v99055, 1.0 (stack88)
        %v99058 = vlog2.pop %v99057 (stack89)
        %v99059 = vmul.f32 %v99058, 0.6931472 (stack90)
        %v99060 = vmul.f32 -0.5, %v99055 (stack91)
        %v99061 = vadd.f32 %v99060, 1.0 (stack92)
        %v99062 = vmul.f32 %v99061, %v99055 (stack93)
        %v99063 = vand.u32 2147483647, %v99055 (stack94)
        %vm99064 = vcmp.lt.f32.partialorder %v99063, 0.0004427343 (stack95)
        %v99065 = vsel /*vm=*/%vm99064, /*on_true_vy=*/%v99062, /*on_false_vx=*/%v99059 (stack96)
        %v99066 = vxor.u32 %v99065, 2147483648 (stack87)
        %vm99069 = vcmp.lt.f32.partialorder %v99066, 5.0 (stack86)
        %v99074 = vsel /*vm=*/%vm99069, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v99078 = vsel /*vm=*/%vm99069, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v99082 = vsel /*vm=*/%vm99069, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v99086 = vsel /*vm=*/%vm99069, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v99090 = vsel /*vm=*/%vm99069, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v99094 = vsel /*vm=*/%vm99069, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v99098 = vsel /*vm=*/%vm99069, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v99102 = vsel /*vm=*/%vm99069, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v99106 = vsel /*vm=*/%vm99069, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v99110 = vadd.f32 %v99066, -2.5 (stack82)
        %v99112 = vrsqrt.pop %v99066 (stack97)
        %v99113 = vmul.f32 %v99066, %v99112 (stack98)
        %vm99114 = vcmp.eq.f32.partialorder %v99066, inf (stack99)
        %v99115 = vsel /*vm=*/%vm99114, /*on_true_vy=*/%v99066, /*on_false_vx=*/%v99113 (stack100)
        %vm99116 = vcmp.eq.f32.partialorder %v99066, 0.0 (stack101)
        %v99117 = vand.u32 %v99066, 2147483648 (stack102)
        %v99118 = vsel /*vm=*/%vm99116, /*on_true_vy=*/%v99117, /*on_false_vx=*/%v99115 (stack103)
        %v99121 = vadd.f32 %v99118, -3.0 (stack82)
        %v99125 = vsel /*vm=*/%vm99069, /*on_true_vy=*/%v99110, /*on_false_vx=*/%v99121 (stack72)
        %v99129 = vmul.f32 %v99106, %v99125 (stack83)
        %v99133 = vadd.f32 %v99102, %v99129 (stack82)
        %v99137 = vmul.f32 %v99133, %v99125 (stack83)
        %v99141 = vadd.f32 %v99098, %v99137 (stack82)
        %v99145 = vmul.f32 %v99141, %v99125 (stack83)
        %v99149 = vadd.f32 %v99094, %v99145 (stack82)
        %v99153 = vmul.f32 %v99149, %v99125 (stack83)
        %v99157 = vadd.f32 %v99090, %v99153 (stack82)
        %v99161 = vmul.f32 %v99157, %v99125 (stack83)
        %v99165 = vadd.f32 %v99086, %v99161 (stack82)
        %v99169 = vmul.f32 %v99165, %v99125 (stack83)
        %v99173 = vadd.f32 %v99082, %v99169 (stack82)
        %v99177 = vmul.f32 %v99173, %v99125 (stack83)
        %v99181 = vadd.f32 %v99078, %v99177 (stack82)
        %v99185 = vmul.f32 %v99181, %v99125 (stack83)
        %v99189 = vadd.f32 %v99074, %v99185 (stack82)
        %v99193 = vmul.f32 %v99189, %v99040 (stack83)
        %v99197 = vsel /*vm=*/%vm99045, /*on_true_vy=*/%v99050, /*on_false_vx=*/%v99193 (stack72)
        %v99201 = vmul.f32 %v99197, 1.4140625 (stack83)
        %s99203 = scalar_lea.vmem %s280, 488 [#allocation0] (stack107)
        %v99204 = vpack.c.bf16 0.0, %v99201 (stack104)
        %99205 = vst [vmem:[%s99203] sm:$0xf] /*vst_source=*/%v99204 (stack105)
        %v99208 = vadd.s32 %v2355, %v97361 (stack65)
        %s99210 = smul.u32 128, %s27 (stack66)
        %v99211 = vlaneseq (stack67)
        %v99212 = vand.u32 %v99211, 127 (stack68)
        %v99213 = vstv %s99210 (stack69)
        %v99214 = vadd.s32 %v99212, %v99213 (stack70)
        %v99218 = vadd.s32 %v99208, %v99214 (stack65)
        %vm99222 = vcmp.lt.u32.totalorder %v99218, %v99208 (stack71)
        %vm99227 = vcmp.lt.u32.totalorder %v99208, %v2355 (stack71)
        %v99232 = vadd.s32 %v2342, %v97344 (stack65)
        %v99236 = vadd.s32 %v99232, 1 (stack65)
        %v99240 = vsel /*vm=*/%vm99227, /*on_true_vy=*/%v99236, /*on_false_vx=*/%v99232 (stack72)
        %v99244 = vadd.s32 %v99240, 1 (stack65)
        %v99248 = vsel /*vm=*/%vm99222, /*on_true_vy=*/%v99244, /*on_false_vx=*/%v99240 (stack72)
        %v99253 = vadd.s32 %v99248, %v10 (stack65)
        %v99257 = vadd.s32 %v99218, %v9 (stack65)
        %v99261 = vadd.s32 %v99253, %v99257 (stack65)
        %v99263 = vshll.u32 %v99257, 13 (stack73)
        %v99264 = vshrl.u32 %v99257, 19 (stack74)
        %v99265 = vor.u32 %v99263, %v99264 (stack75)
        %v99266 = vxor.u32 %v99261, %v99265 (stack76)
        %v99269 = vadd.s32 %v99261, %v99266 (stack65)
        %v99271 = vshll.u32 %v99266, 15 (stack73)
        %v99272 = vshrl.u32 %v99266, 17 (stack74)
        %v99273 = vor.u32 %v99271, %v99272 (stack75)
        %v99274 = vxor.u32 %v99269, %v99273 (stack76)
        %v99277 = vadd.s32 %v99269, %v99274 (stack65)
        %v99279 = vshll.u32 %v99274, 26 (stack73)
        %v99280 = vshrl.u32 %v99274, 6 (stack74)
        %v99281 = vor.u32 %v99279, %v99280 (stack75)
        %v99282 = vxor.u32 %v99277, %v99281 (stack76)
        %v99285 = vadd.s32 %v99277, %v99282 (stack65)
        %v99289 = vadd.s32 %v99285, %v9 (stack65)
        %v99291 = vshll.u32 %v99282, 6 (stack73)
        %v99292 = vshrl.u32 %v99282, 26 (stack74)
        %v99293 = vor.u32 %v99291, %v99292 (stack75)
        %v99294 = vxor.u32 %v99285, %v99293 (stack76)
        %v99297 = vadd.s32 %v99294, %v8 (stack65)
        %v99301 = vadd.s32 %v99297, 1 (stack65)
        %v99305 = vadd.s32 %v99289, %v99301 (stack65)
        %v99307 = vshll.u32 %v99301, 17 (stack73)
        %v99308 = vshrl.u32 %v99301, 15 (stack74)
        %v99309 = vor.u32 %v99307, %v99308 (stack75)
        %v99310 = vxor.u32 %v99305, %v99309 (stack76)
        %v99313 = vadd.s32 %v99305, %v99310 (stack65)
        %v99315 = vshll.u32 %v99310, 29 (stack73)
        %v99316 = vshrl.u32 %v99310, 3 (stack74)
        %v99317 = vor.u32 %v99315, %v99316 (stack75)
        %v99318 = vxor.u32 %v99313, %v99317 (stack76)
        %v99321 = vadd.s32 %v99313, %v99318 (stack65)
        %v99323 = vshll.u32 %v99318, 16 (stack73)
        %v99324 = vshrl.u32 %v99318, 16 (stack74)
        %v99325 = vor.u32 %v99323, %v99324 (stack75)
        %v99326 = vxor.u32 %v99321, %v99325 (stack76)
        %v99329 = vadd.s32 %v99321, %v99326 (stack65)
        %v99333 = vadd.s32 %v99329, %v8 (stack65)
        %v99335 = vshll.u32 %v99326, 24 (stack73)
        %v99336 = vshrl.u32 %v99326, 8 (stack74)
        %v99337 = vor.u32 %v99335, %v99336 (stack75)
        %v99338 = vxor.u32 %v99329, %v99337 (stack76)
        %v99341 = vadd.s32 %v99338, %v10 (stack65)
        %v99345 = vadd.s32 %v99341, 2 (stack65)
        %v99349 = vadd.s32 %v99333, %v99345 (stack65)
        %v99351 = vshll.u32 %v99345, 13 (stack73)
        %v99352 = vshrl.u32 %v99345, 19 (stack74)
        %v99353 = vor.u32 %v99351, %v99352 (stack75)
        %v99354 = vxor.u32 %v99349, %v99353 (stack76)
        %v99357 = vadd.s32 %v99349, %v99354 (stack65)
        %v99359 = vshll.u32 %v99354, 15 (stack73)
        %v99360 = vshrl.u32 %v99354, 17 (stack74)
        %v99361 = vor.u32 %v99359, %v99360 (stack75)
        %v99362 = vxor.u32 %v99357, %v99361 (stack76)
        %v99365 = vadd.s32 %v99357, %v99362 (stack65)
        %v99367 = vshll.u32 %v99362, 26 (stack73)
        %v99368 = vshrl.u32 %v99362, 6 (stack74)
        %v99369 = vor.u32 %v99367, %v99368 (stack75)
        %v99370 = vxor.u32 %v99365, %v99369 (stack76)
        %v99373 = vadd.s32 %v99365, %v99370 (stack65)
        %v99377 = vadd.s32 %v99373, %v10 (stack65)
        %v99379 = vshll.u32 %v99370, 6 (stack73)
        %v99380 = vshrl.u32 %v99370, 26 (stack74)
        %v99381 = vor.u32 %v99379, %v99380 (stack75)
        %v99382 = vxor.u32 %v99373, %v99381 (stack76)
        %v99385 = vadd.s32 %v99382, %v9 (stack65)
        %v99389 = vadd.s32 %v99385, 3 (stack65)
        %v99393 = vadd.s32 %v99377, %v99389 (stack65)
        %v99395 = vshll.u32 %v99389, 17 (stack73)
        %v99396 = vshrl.u32 %v99389, 15 (stack74)
        %v99397 = vor.u32 %v99395, %v99396 (stack75)
        %v99398 = vxor.u32 %v99393, %v99397 (stack76)
        %v99401 = vadd.s32 %v99393, %v99398 (stack65)
        %v99403 = vshll.u32 %v99398, 29 (stack73)
        %v99404 = vshrl.u32 %v99398, 3 (stack74)
        %v99405 = vor.u32 %v99403, %v99404 (stack75)
        %v99406 = vxor.u32 %v99401, %v99405 (stack76)
        %v99409 = vadd.s32 %v99401, %v99406 (stack65)
        %v99411 = vshll.u32 %v99406, 16 (stack73)
        %v99412 = vshrl.u32 %v99406, 16 (stack74)
        %v99413 = vor.u32 %v99411, %v99412 (stack75)
        %v99414 = vxor.u32 %v99409, %v99413 (stack76)
        %v99417 = vadd.s32 %v99409, %v99414 (stack65)
        %v99421 = vadd.s32 %v99417, %v9 (stack65)
        %v99423 = vshll.u32 %v99414, 24 (stack73)
        %v99424 = vshrl.u32 %v99414, 8 (stack74)
        %v99425 = vor.u32 %v99423, %v99424 (stack75)
        %v99426 = vxor.u32 %v99417, %v99425 (stack76)
        %v99429 = vadd.s32 %v99426, %v8 (stack65)
        %v99433 = vadd.s32 %v99429, 4 (stack65)
        %v99437 = vadd.s32 %v99421, %v99433 (stack65)
        %v99439 = vshll.u32 %v99433, 13 (stack73)
        %v99440 = vshrl.u32 %v99433, 19 (stack74)
        %v99441 = vor.u32 %v99439, %v99440 (stack75)
        %v99442 = vxor.u32 %v99437, %v99441 (stack76)
        %v99445 = vadd.s32 %v99437, %v99442 (stack65)
        %v99447 = vshll.u32 %v99442, 15 (stack73)
        %v99448 = vshrl.u32 %v99442, 17 (stack74)
        %v99449 = vor.u32 %v99447, %v99448 (stack75)
        %v99450 = vxor.u32 %v99445, %v99449 (stack76)
        %v99453 = vadd.s32 %v99445, %v99450 (stack65)
        %v99455 = vshll.u32 %v99450, 26 (stack73)
        %v99456 = vshrl.u32 %v99450, 6 (stack74)
        %v99457 = vor.u32 %v99455, %v99456 (stack75)
        %v99458 = vxor.u32 %v99453, %v99457 (stack76)
        %v99461 = vadd.s32 %v99453, %v99458 (stack65)
        %v99465 = vadd.s32 %v99461, %v8 (stack65)
        %v99467 = vshll.u32 %v99458, 6 (stack73)
        %v99468 = vshrl.u32 %v99458, 26 (stack74)
        %v99469 = vor.u32 %v99467, %v99468 (stack75)
        %v99470 = vxor.u32 %v99461, %v99469 (stack76)
        %v99473 = vadd.s32 %v99470, %v10 (stack65)
        %v99477 = vadd.s32 %v99473, 5 (stack65)
        %v99479 = vxor.u32 %v99465, %v99477 (stack76)
        %v99480 = vand.u32.u8 %v99479, 255 (stack77)
        %v99481 = vand.u32 %v99480, 65535 (stack78)
        %v99482 = vshrl.u32 %v99481, 1 (stack79)
        %v99483 = vor.u32 %v99482, 16256 (stack75)
        %v99484 = vand.u32.u16 %v99483, 65535 (stack80)
        %v99485 = vunpack.i.l.bf16 %v99484 (stack81)
        %v99489 = vadd.f32 %v99485, -1.0 (stack82)
        %v99493 = vmul.f32 %v99489, 2.0 (stack83)
        %v99497 = vadd.f32 %v99493, -0.99609375 (stack82)
        %v99501 = vmax.f32 -0.99609375, %v99497 (stack84)
        %v99503 = vand.u32 2147483647, %v99501 (stack85)
        %vm99506 = vcmp.eq.f32.partialorder %v99503, 1.0 (stack86)
        %v99511 = vmul.f32 %v99501, inf (stack83)
        %v99513 = vxor.u32 %v99501, 2147483648 (stack87)
        %v99516 = vmul.f32 %v99501, %v99513 (stack83)
        %v99518 = vadd.f32 %v99516, 1.0 (stack88)
        %v99519 = vlog2.pop %v99518 (stack89)
        %v99520 = vmul.f32 %v99519, 0.6931472 (stack90)
        %v99521 = vmul.f32 -0.5, %v99516 (stack91)
        %v99522 = vadd.f32 %v99521, 1.0 (stack92)
        %v99523 = vmul.f32 %v99522, %v99516 (stack93)
        %v99524 = vand.u32 2147483647, %v99516 (stack94)
        %vm99525 = vcmp.lt.f32.partialorder %v99524, 0.0004427343 (stack95)
        %v99526 = vsel /*vm=*/%vm99525, /*on_true_vy=*/%v99523, /*on_false_vx=*/%v99520 (stack96)
        %v99527 = vxor.u32 %v99526, 2147483648 (stack87)
        %vm99530 = vcmp.lt.f32.partialorder %v99527, 5.0 (stack86)
        %v99535 = vsel /*vm=*/%vm99530, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v99539 = vsel /*vm=*/%vm99530, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v99543 = vsel /*vm=*/%vm99530, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v99547 = vsel /*vm=*/%vm99530, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v99551 = vsel /*vm=*/%vm99530, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v99555 = vsel /*vm=*/%vm99530, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v99559 = vsel /*vm=*/%vm99530, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v99563 = vsel /*vm=*/%vm99530, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v99567 = vsel /*vm=*/%vm99530, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v99571 = vadd.f32 %v99527, -2.5 (stack82)
        %v99573 = vrsqrt.pop %v99527 (stack97)
        %v99574 = vmul.f32 %v99527, %v99573 (stack98)
        %vm99575 = vcmp.eq.f32.partialorder %v99527, inf (stack99)
        %v99576 = vsel /*vm=*/%vm99575, /*on_true_vy=*/%v99527, /*on_false_vx=*/%v99574 (stack100)
        %vm99577 = vcmp.eq.f32.partialorder %v99527, 0.0 (stack101)
        %v99578 = vand.u32 %v99527, 2147483648 (stack102)
        %v99579 = vsel /*vm=*/%vm99577, /*on_true_vy=*/%v99578, /*on_false_vx=*/%v99576 (stack103)
        %v99582 = vadd.f32 %v99579, -3.0 (stack82)
        %v99586 = vsel /*vm=*/%vm99530, /*on_true_vy=*/%v99571, /*on_false_vx=*/%v99582 (stack72)
        %v99590 = vmul.f32 %v99567, %v99586 (stack83)
        %v99594 = vadd.f32 %v99563, %v99590 (stack82)
        %v99598 = vmul.f32 %v99594, %v99586 (stack83)
        %v99602 = vadd.f32 %v99559, %v99598 (stack82)
        %v99606 = vmul.f32 %v99602, %v99586 (stack83)
        %v99610 = vadd.f32 %v99555, %v99606 (stack82)
        %v99614 = vmul.f32 %v99610, %v99586 (stack83)
        %v99618 = vadd.f32 %v99551, %v99614 (stack82)
        %v99622 = vmul.f32 %v99618, %v99586 (stack83)
        %v99626 = vadd.f32 %v99547, %v99622 (stack82)
        %v99630 = vmul.f32 %v99626, %v99586 (stack83)
        %v99634 = vadd.f32 %v99543, %v99630 (stack82)
        %v99638 = vmul.f32 %v99634, %v99586 (stack83)
        %v99642 = vadd.f32 %v99539, %v99638 (stack82)
        %v99646 = vmul.f32 %v99642, %v99586 (stack83)
        %v99650 = vadd.f32 %v99535, %v99646 (stack82)
        %v99654 = vmul.f32 %v99650, %v99501 (stack83)
        %v99658 = vsel /*vm=*/%vm99506, /*on_true_vy=*/%v99511, /*on_false_vx=*/%v99654 (stack72)
        %v99662 = vmul.f32 %v99658, 1.4140625 (stack83)
        %s99664 = scalar_lea.vmem %s280, 616 [#allocation0] (stack107)
        %v99665 = vpack.c.bf16 0.0, %v99662 (stack104)
        %99666 = vst [vmem:[%s99664] sm:$0xf] /*vst_source=*/%v99665 (stack105)
        %v99669 = vadd.s32 %v2842, %v97361 (stack65)
        %s99671 = smul.u32 128, %s27 (stack66)
        %v99672 = vlaneseq (stack67)
        %v99673 = vand.u32 %v99672, 127 (stack68)
        %v99674 = vstv %s99671 (stack69)
        %v99675 = vadd.s32 %v99673, %v99674 (stack70)
        %v99679 = vadd.s32 %v99669, %v99675 (stack65)
        %vm99683 = vcmp.lt.u32.totalorder %v99679, %v99669 (stack71)
        %vm99688 = vcmp.lt.u32.totalorder %v99669, %v2842 (stack71)
        %v99693 = vadd.s32 %v2829, %v97344 (stack65)
        %v99697 = vadd.s32 %v99693, 1 (stack65)
        %v99701 = vsel /*vm=*/%vm99688, /*on_true_vy=*/%v99697, /*on_false_vx=*/%v99693 (stack72)
        %v99705 = vadd.s32 %v99701, 1 (stack65)
        %v99709 = vsel /*vm=*/%vm99683, /*on_true_vy=*/%v99705, /*on_false_vx=*/%v99701 (stack72)
        %v99714 = vadd.s32 %v99709, %v10 (stack65)
        %v99718 = vadd.s32 %v99679, %v9 (stack65)
        %v99722 = vadd.s32 %v99714, %v99718 (stack65)
        %v99724 = vshll.u32 %v99718, 13 (stack73)
        %v99725 = vshrl.u32 %v99718, 19 (stack74)
        %v99726 = vor.u32 %v99724, %v99725 (stack75)
        %v99727 = vxor.u32 %v99722, %v99726 (stack76)
        %v99730 = vadd.s32 %v99722, %v99727 (stack65)
        %v99732 = vshll.u32 %v99727, 15 (stack73)
        %v99733 = vshrl.u32 %v99727, 17 (stack74)
        %v99734 = vor.u32 %v99732, %v99733 (stack75)
        %v99735 = vxor.u32 %v99730, %v99734 (stack76)
        %v99738 = vadd.s32 %v99730, %v99735 (stack65)
        %v99740 = vshll.u32 %v99735, 26 (stack73)
        %v99741 = vshrl.u32 %v99735, 6 (stack74)
        %v99742 = vor.u32 %v99740, %v99741 (stack75)
        %v99743 = vxor.u32 %v99738, %v99742 (stack76)
        %v99746 = vadd.s32 %v99738, %v99743 (stack65)
        %v99750 = vadd.s32 %v99746, %v9 (stack65)
        %v99752 = vshll.u32 %v99743, 6 (stack73)
        %v99753 = vshrl.u32 %v99743, 26 (stack74)
        %v99754 = vor.u32 %v99752, %v99753 (stack75)
        %v99755 = vxor.u32 %v99746, %v99754 (stack76)
        %v99758 = vadd.s32 %v99755, %v8 (stack65)
        %v99762 = vadd.s32 %v99758, 1 (stack65)
        %v99766 = vadd.s32 %v99750, %v99762 (stack65)
        %v99768 = vshll.u32 %v99762, 17 (stack73)
        %v99769 = vshrl.u32 %v99762, 15 (stack74)
        %v99770 = vor.u32 %v99768, %v99769 (stack75)
        %v99771 = vxor.u32 %v99766, %v99770 (stack76)
        %v99774 = vadd.s32 %v99766, %v99771 (stack65)
        %v99776 = vshll.u32 %v99771, 29 (stack73)
        %v99777 = vshrl.u32 %v99771, 3 (stack74)
        %v99778 = vor.u32 %v99776, %v99777 (stack75)
        %v99779 = vxor.u32 %v99774, %v99778 (stack76)
        %v99782 = vadd.s32 %v99774, %v99779 (stack65)
        %v99784 = vshll.u32 %v99779, 16 (stack73)
        %v99785 = vshrl.u32 %v99779, 16 (stack74)
        %v99786 = vor.u32 %v99784, %v99785 (stack75)
        %v99787 = vxor.u32 %v99782, %v99786 (stack76)
        %v99790 = vadd.s32 %v99782, %v99787 (stack65)
        %v99794 = vadd.s32 %v99790, %v8 (stack65)
        %v99796 = vshll.u32 %v99787, 24 (stack73)
        %v99797 = vshrl.u32 %v99787, 8 (stack74)
        %v99798 = vor.u32 %v99796, %v99797 (stack75)
        %v99799 = vxor.u32 %v99790, %v99798 (stack76)
        %v99802 = vadd.s32 %v99799, %v10 (stack65)
        %v99806 = vadd.s32 %v99802, 2 (stack65)
        %v99810 = vadd.s32 %v99794, %v99806 (stack65)
        %v99812 = vshll.u32 %v99806, 13 (stack73)
        %v99813 = vshrl.u32 %v99806, 19 (stack74)
        %v99814 = vor.u32 %v99812, %v99813 (stack75)
        %v99815 = vxor.u32 %v99810, %v99814 (stack76)
        %v99818 = vadd.s32 %v99810, %v99815 (stack65)
        %v99820 = vshll.u32 %v99815, 15 (stack73)
        %v99821 = vshrl.u32 %v99815, 17 (stack74)
        %v99822 = vor.u32 %v99820, %v99821 (stack75)
        %v99823 = vxor.u32 %v99818, %v99822 (stack76)
        %v99826 = vadd.s32 %v99818, %v99823 (stack65)
        %v99828 = vshll.u32 %v99823, 26 (stack73)
        %v99829 = vshrl.u32 %v99823, 6 (stack74)
        %v99830 = vor.u32 %v99828, %v99829 (stack75)
        %v99831 = vxor.u32 %v99826, %v99830 (stack76)
        %v99834 = vadd.s32 %v99826, %v99831 (stack65)
        %v99838 = vadd.s32 %v99834, %v10 (stack65)
        %v99840 = vshll.u32 %v99831, 6 (stack73)
        %v99841 = vshrl.u32 %v99831, 26 (stack74)
        %v99842 = vor.u32 %v99840, %v99841 (stack75)
        %v99843 = vxor.u32 %v99834, %v99842 (stack76)
        %v99846 = vadd.s32 %v99843, %v9 (stack65)
        %v99850 = vadd.s32 %v99846, 3 (stack65)
        %v99854 = vadd.s32 %v99838, %v99850 (stack65)
        %v99856 = vshll.u32 %v99850, 17 (stack73)
        %v99857 = vshrl.u32 %v99850, 15 (stack74)
        %v99858 = vor.u32 %v99856, %v99857 (stack75)
        %v99859 = vxor.u32 %v99854, %v99858 (stack76)
        %v99862 = vadd.s32 %v99854, %v99859 (stack65)
        %v99864 = vshll.u32 %v99859, 29 (stack73)
        %v99865 = vshrl.u32 %v99859, 3 (stack74)
        %v99866 = vor.u32 %v99864, %v99865 (stack75)
        %v99867 = vxor.u32 %v99862, %v99866 (stack76)
        %v99870 = vadd.s32 %v99862, %v99867 (stack65)
        %v99872 = vshll.u32 %v99867, 16 (stack73)
        %v99873 = vshrl.u32 %v99867, 16 (stack74)
        %v99874 = vor.u32 %v99872, %v99873 (stack75)
        %v99875 = vxor.u32 %v99870, %v99874 (stack76)
        %v99878 = vadd.s32 %v99870, %v99875 (stack65)
        %v99882 = vadd.s32 %v99878, %v9 (stack65)
        %v99884 = vshll.u32 %v99875, 24 (stack73)
        %v99885 = vshrl.u32 %v99875, 8 (stack74)
        %v99886 = vor.u32 %v99884, %v99885 (stack75)
        %v99887 = vxor.u32 %v99878, %v99886 (stack76)
        %v99890 = vadd.s32 %v99887, %v8 (stack65)
        %v99894 = vadd.s32 %v99890, 4 (stack65)
        %v99898 = vadd.s32 %v99882, %v99894 (stack65)
        %v99900 = vshll.u32 %v99894, 13 (stack73)
        %v99901 = vshrl.u32 %v99894, 19 (stack74)
        %v99902 = vor.u32 %v99900, %v99901 (stack75)
        %v99903 = vxor.u32 %v99898, %v99902 (stack76)
        %v99906 = vadd.s32 %v99898, %v99903 (stack65)
        %v99908 = vshll.u32 %v99903, 15 (stack73)
        %v99909 = vshrl.u32 %v99903, 17 (stack74)
        %v99910 = vor.u32 %v99908, %v99909 (stack75)
        %v99911 = vxor.u32 %v99906, %v99910 (stack76)
        %v99914 = vadd.s32 %v99906, %v99911 (stack65)
        %v99916 = vshll.u32 %v99911, 26 (stack73)
        %v99917 = vshrl.u32 %v99911, 6 (stack74)
        %v99918 = vor.u32 %v99916, %v99917 (stack75)
        %v99919 = vxor.u32 %v99914, %v99918 (stack76)
        %v99922 = vadd.s32 %v99914, %v99919 (stack65)
        %v99926 = vadd.s32 %v99922, %v8 (stack65)
        %v99928 = vshll.u32 %v99919, 6 (stack73)
        %v99929 = vshrl.u32 %v99919, 26 (stack74)
        %v99930 = vor.u32 %v99928, %v99929 (stack75)
        %v99931 = vxor.u32 %v99922, %v99930 (stack76)
        %v99934 = vadd.s32 %v99931, %v10 (stack65)
        %v99938 = vadd.s32 %v99934, 5 (stack65)
        %v99940 = vxor.u32 %v99926, %v99938 (stack76)
        %v99941 = vand.u32.u8 %v99940, 255 (stack77)
        %v99942 = vand.u32 %v99941, 65535 (stack78)
        %v99943 = vshrl.u32 %v99942, 1 (stack79)
        %v99944 = vor.u32 %v99943, 16256 (stack75)
        %v99945 = vand.u32.u16 %v99944, 65535 (stack80)
        %v99946 = vunpack.i.l.bf16 %v99945 (stack81)
        %v99950 = vadd.f32 %v99946, -1.0 (stack82)
        %v99954 = vmul.f32 %v99950, 2.0 (stack83)
        %v99958 = vadd.f32 %v99954, -0.99609375 (stack82)
        %v99962 = vmax.f32 -0.99609375, %v99958 (stack84)
        %v99964 = vand.u32 2147483647, %v99962 (stack85)
        %vm99967 = vcmp.eq.f32.partialorder %v99964, 1.0 (stack86)
        %v99972 = vmul.f32 %v99962, inf (stack83)
        %v99974 = vxor.u32 %v99962, 2147483648 (stack87)
        %v99977 = vmul.f32 %v99962, %v99974 (stack83)
        %v99979 = vadd.f32 %v99977, 1.0 (stack88)
        %v99980 = vlog2.pop %v99979 (stack89)
        %v99981 = vmul.f32 %v99980, 0.6931472 (stack90)
        %v99982 = vmul.f32 -0.5, %v99977 (stack91)
        %v99983 = vadd.f32 %v99982, 1.0 (stack92)
        %v99984 = vmul.f32 %v99983, %v99977 (stack93)
        %v99985 = vand.u32 2147483647, %v99977 (stack94)
        %vm99986 = vcmp.lt.f32.partialorder %v99985, 0.0004427343 (stack95)
        %v99987 = vsel /*vm=*/%vm99986, /*on_true_vy=*/%v99984, /*on_false_vx=*/%v99981 (stack96)
        %v99988 = vxor.u32 %v99987, 2147483648 (stack87)
        %vm99991 = vcmp.lt.f32.partialorder %v99988, 5.0 (stack86)
        %v99996 = vsel /*vm=*/%vm99991, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v100000 = vsel /*vm=*/%vm99991, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v100004 = vsel /*vm=*/%vm99991, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v100008 = vsel /*vm=*/%vm99991, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v100012 = vsel /*vm=*/%vm99991, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v100016 = vsel /*vm=*/%vm99991, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v100020 = vsel /*vm=*/%vm99991, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v100024 = vsel /*vm=*/%vm99991, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v100028 = vsel /*vm=*/%vm99991, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v100032 = vadd.f32 %v99988, -2.5 (stack82)
        %v100034 = vrsqrt.pop %v99988 (stack97)
        %v100035 = vmul.f32 %v99988, %v100034 (stack98)
        %vm100036 = vcmp.eq.f32.partialorder %v99988, inf (stack99)
        %v100037 = vsel /*vm=*/%vm100036, /*on_true_vy=*/%v99988, /*on_false_vx=*/%v100035 (stack100)
        %vm100038 = vcmp.eq.f32.partialorder %v99988, 0.0 (stack101)
        %v100039 = vand.u32 %v99988, 2147483648 (stack102)
        %v100040 = vsel /*vm=*/%vm100038, /*on_true_vy=*/%v100039, /*on_false_vx=*/%v100037 (stack103)
        %v100043 = vadd.f32 %v100040, -3.0 (stack82)
        %v100047 = vsel /*vm=*/%vm99991, /*on_true_vy=*/%v100032, /*on_false_vx=*/%v100043 (stack72)
        %v100051 = vmul.f32 %v100028, %v100047 (stack83)
        %v100055 = vadd.f32 %v100024, %v100051 (stack82)
        %v100059 = vmul.f32 %v100055, %v100047 (stack83)
        %v100063 = vadd.f32 %v100020, %v100059 (stack82)
        %v100067 = vmul.f32 %v100063, %v100047 (stack83)
        %v100071 = vadd.f32 %v100016, %v100067 (stack82)
        %v100075 = vmul.f32 %v100071, %v100047 (stack83)
        %v100079 = vadd.f32 %v100012, %v100075 (stack82)
        %v100083 = vmul.f32 %v100079, %v100047 (stack83)
        %v100087 = vadd.f32 %v100008, %v100083 (stack82)
        %v100091 = vmul.f32 %v100087, %v100047 (stack83)
        %v100095 = vadd.f32 %v100004, %v100091 (stack82)
        %v100099 = vmul.f32 %v100095, %v100047 (stack83)
        %v100103 = vadd.f32 %v100000, %v100099 (stack82)
        %v100107 = vmul.f32 %v100103, %v100047 (stack83)
        %v100111 = vadd.f32 %v99996, %v100107 (stack82)
        %v100115 = vmul.f32 %v100111, %v99962 (stack83)
        %v100119 = vsel /*vm=*/%vm99967, /*on_true_vy=*/%v99972, /*on_false_vx=*/%v100115 (stack72)
        %v100123 = vmul.f32 %v100119, 1.4140625 (stack83)
        %s100125 = scalar_lea.vmem %s280, 744 [#allocation0] (stack107)
        %v100126 = vpack.c.bf16 0.0, %v100123 (stack104)
        %100127 = vst [vmem:[%s100125] sm:$0xf] /*vst_source=*/%v100126 (stack105)
        %v100130 = vadd.s32 %v3329, %v97361 (stack65)
        %s100132 = smul.u32 128, %s27 (stack66)
        %v100133 = vlaneseq (stack67)
        %v100134 = vand.u32 %v100133, 127 (stack68)
        %v100135 = vstv %s100132 (stack69)
        %v100136 = vadd.s32 %v100134, %v100135 (stack70)
        %v100140 = vadd.s32 %v100130, %v100136 (stack65)
        %vm100144 = vcmp.lt.u32.totalorder %v100140, %v100130 (stack71)
        %vm100149 = vcmp.lt.u32.totalorder %v100130, %v3329 (stack71)
        %v100154 = vadd.s32 %v3316, %v97344 (stack65)
        %v100158 = vadd.s32 %v100154, 1 (stack65)
        %v100162 = vsel /*vm=*/%vm100149, /*on_true_vy=*/%v100158, /*on_false_vx=*/%v100154 (stack72)
        %v100166 = vadd.s32 %v100162, 1 (stack65)
        %v100170 = vsel /*vm=*/%vm100144, /*on_true_vy=*/%v100166, /*on_false_vx=*/%v100162 (stack72)
        %v100175 = vadd.s32 %v100170, %v10 (stack65)
        %v100179 = vadd.s32 %v100140, %v9 (stack65)
        %v100183 = vadd.s32 %v100175, %v100179 (stack65)
        %v100185 = vshll.u32 %v100179, 13 (stack73)
        %v100186 = vshrl.u32 %v100179, 19 (stack74)
        %v100187 = vor.u32 %v100185, %v100186 (stack75)
        %v100188 = vxor.u32 %v100183, %v100187 (stack76)
        %v100191 = vadd.s32 %v100183, %v100188 (stack65)
        %v100193 = vshll.u32 %v100188, 15 (stack73)
        %v100194 = vshrl.u32 %v100188, 17 (stack74)
        %v100195 = vor.u32 %v100193, %v100194 (stack75)
        %v100196 = vxor.u32 %v100191, %v100195 (stack76)
        %v100199 = vadd.s32 %v100191, %v100196 (stack65)
        %v100201 = vshll.u32 %v100196, 26 (stack73)
        %v100202 = vshrl.u32 %v100196, 6 (stack74)
        %v100203 = vor.u32 %v100201, %v100202 (stack75)
        %v100204 = vxor.u32 %v100199, %v100203 (stack76)
        %v100207 = vadd.s32 %v100199, %v100204 (stack65)
        %v100211 = vadd.s32 %v100207, %v9 (stack65)
        %v100213 = vshll.u32 %v100204, 6 (stack73)
        %v100214 = vshrl.u32 %v100204, 26 (stack74)
        %v100215 = vor.u32 %v100213, %v100214 (stack75)
        %v100216 = vxor.u32 %v100207, %v100215 (stack76)
        %v100219 = vadd.s32 %v100216, %v8 (stack65)
        %v100223 = vadd.s32 %v100219, 1 (stack65)
        %v100227 = vadd.s32 %v100211, %v100223 (stack65)
        %v100229 = vshll.u32 %v100223, 17 (stack73)
        %v100230 = vshrl.u32 %v100223, 15 (stack74)
        %v100231 = vor.u32 %v100229, %v100230 (stack75)
        %v100232 = vxor.u32 %v100227, %v100231 (stack76)
        %v100235 = vadd.s32 %v100227, %v100232 (stack65)
        %v100237 = vshll.u32 %v100232, 29 (stack73)
        %v100238 = vshrl.u32 %v100232, 3 (stack74)
        %v100239 = vor.u32 %v100237, %v100238 (stack75)
        %v100240 = vxor.u32 %v100235, %v100239 (stack76)
        %v100243 = vadd.s32 %v100235, %v100240 (stack65)
        %v100245 = vshll.u32 %v100240, 16 (stack73)
        %v100246 = vshrl.u32 %v100240, 16 (stack74)
        %v100247 = vor.u32 %v100245, %v100246 (stack75)
        %v100248 = vxor.u32 %v100243, %v100247 (stack76)
        %v100251 = vadd.s32 %v100243, %v100248 (stack65)
        %v100255 = vadd.s32 %v100251, %v8 (stack65)
        %v100257 = vshll.u32 %v100248, 24 (stack73)
        %v100258 = vshrl.u32 %v100248, 8 (stack74)
        %v100259 = vor.u32 %v100257, %v100258 (stack75)
        %v100260 = vxor.u32 %v100251, %v100259 (stack76)
        %v100263 = vadd.s32 %v100260, %v10 (stack65)
        %v100267 = vadd.s32 %v100263, 2 (stack65)
        %v100271 = vadd.s32 %v100255, %v100267 (stack65)
        %v100273 = vshll.u32 %v100267, 13 (stack73)
        %v100274 = vshrl.u32 %v100267, 19 (stack74)
        %v100275 = vor.u32 %v100273, %v100274 (stack75)
        %v100276 = vxor.u32 %v100271, %v100275 (stack76)
        %v100279 = vadd.s32 %v100271, %v100276 (stack65)
        %v100281 = vshll.u32 %v100276, 15 (stack73)
        %v100282 = vshrl.u32 %v100276, 17 (stack74)
        %v100283 = vor.u32 %v100281, %v100282 (stack75)
        %v100284 = vxor.u32 %v100279, %v100283 (stack76)
        %v100287 = vadd.s32 %v100279, %v100284 (stack65)
        %v100289 = vshll.u32 %v100284, 26 (stack73)
        %v100290 = vshrl.u32 %v100284, 6 (stack74)
        %v100291 = vor.u32 %v100289, %v100290 (stack75)
        %v100292 = vxor.u32 %v100287, %v100291 (stack76)
        %v100295 = vadd.s32 %v100287, %v100292 (stack65)
        %v100299 = vadd.s32 %v100295, %v10 (stack65)
        %v100301 = vshll.u32 %v100292, 6 (stack73)
        %v100302 = vshrl.u32 %v100292, 26 (stack74)
        %v100303 = vor.u32 %v100301, %v100302 (stack75)
        %v100304 = vxor.u32 %v100295, %v100303 (stack76)
        %v100307 = vadd.s32 %v100304, %v9 (stack65)
        %v100311 = vadd.s32 %v100307, 3 (stack65)
        %v100315 = vadd.s32 %v100299, %v100311 (stack65)
        %v100317 = vshll.u32 %v100311, 17 (stack73)
        %v100318 = vshrl.u32 %v100311, 15 (stack74)
        %v100319 = vor.u32 %v100317, %v100318 (stack75)
        %v100320 = vxor.u32 %v100315, %v100319 (stack76)
        %v100323 = vadd.s32 %v100315, %v100320 (stack65)
        %v100325 = vshll.u32 %v100320, 29 (stack73)
        %v100326 = vshrl.u32 %v100320, 3 (stack74)
        %v100327 = vor.u32 %v100325, %v100326 (stack75)
        %v100328 = vxor.u32 %v100323, %v100327 (stack76)
        %v100331 = vadd.s32 %v100323, %v100328 (stack65)
        %v100333 = vshll.u32 %v100328, 16 (stack73)
        %v100334 = vshrl.u32 %v100328, 16 (stack74)
        %v100335 = vor.u32 %v100333, %v100334 (stack75)
        %v100336 = vxor.u32 %v100331, %v100335 (stack76)
        %v100339 = vadd.s32 %v100331, %v100336 (stack65)
        %v100343 = vadd.s32 %v100339, %v9 (stack65)
        %v100345 = vshll.u32 %v100336, 24 (stack73)
        %v100346 = vshrl.u32 %v100336, 8 (stack74)
        %v100347 = vor.u32 %v100345, %v100346 (stack75)
        %v100348 = vxor.u32 %v100339, %v100347 (stack76)
        %v100351 = vadd.s32 %v100348, %v8 (stack65)
        %v100355 = vadd.s32 %v100351, 4 (stack65)
        %v100359 = vadd.s32 %v100343, %v100355 (stack65)
        %v100361 = vshll.u32 %v100355, 13 (stack73)
        %v100362 = vshrl.u32 %v100355, 19 (stack74)
        %v100363 = vor.u32 %v100361, %v100362 (stack75)
        %v100364 = vxor.u32 %v100359, %v100363 (stack76)
        %v100367 = vadd.s32 %v100359, %v100364 (stack65)
        %v100369 = vshll.u32 %v100364, 15 (stack73)
        %v100370 = vshrl.u32 %v100364, 17 (stack74)
        %v100371 = vor.u32 %v100369, %v100370 (stack75)
        %v100372 = vxor.u32 %v100367, %v100371 (stack76)
        %v100375 = vadd.s32 %v100367, %v100372 (stack65)
        %v100377 = vshll.u32 %v100372, 26 (stack73)
        %v100378 = vshrl.u32 %v100372, 6 (stack74)
        %v100379 = vor.u32 %v100377, %v100378 (stack75)
        %v100380 = vxor.u32 %v100375, %v100379 (stack76)
        %v100383 = vadd.s32 %v100375, %v100380 (stack65)
        %v100387 = vadd.s32 %v100383, %v8 (stack65)
        %v100389 = vshll.u32 %v100380, 6 (stack73)
        %v100390 = vshrl.u32 %v100380, 26 (stack74)
        %v100391 = vor.u32 %v100389, %v100390 (stack75)
        %v100392 = vxor.u32 %v100383, %v100391 (stack76)
        %v100395 = vadd.s32 %v100392, %v10 (stack65)
        %v100399 = vadd.s32 %v100395, 5 (stack65)
        %v100401 = vxor.u32 %v100387, %v100399 (stack76)
        %v100402 = vand.u32.u8 %v100401, 255 (stack77)
        %v100403 = vand.u32 %v100402, 65535 (stack78)
        %v100404 = vshrl.u32 %v100403, 1 (stack79)
        %v100405 = vor.u32 %v100404, 16256 (stack75)
        %v100406 = vand.u32.u16 %v100405, 65535 (stack80)
        %v100407 = vunpack.i.l.bf16 %v100406 (stack81)
        %v100411 = vadd.f32 %v100407, -1.0 (stack82)
        %v100415 = vmul.f32 %v100411, 2.0 (stack83)
        %v100419 = vadd.f32 %v100415, -0.99609375 (stack82)
        %v100423 = vmax.f32 -0.99609375, %v100419 (stack84)
        %v100425 = vand.u32 2147483647, %v100423 (stack85)
        %vm100428 = vcmp.eq.f32.partialorder %v100425, 1.0 (stack86)
        %v100433 = vmul.f32 %v100423, inf (stack83)
        %v100435 = vxor.u32 %v100423, 2147483648 (stack87)
        %v100438 = vmul.f32 %v100423, %v100435 (stack83)
        %v100440 = vadd.f32 %v100438, 1.0 (stack88)
        %v100441 = vlog2.pop %v100440 (stack89)
        %v100442 = vmul.f32 %v100441, 0.6931472 (stack90)
        %v100443 = vmul.f32 -0.5, %v100438 (stack91)
        %v100444 = vadd.f32 %v100443, 1.0 (stack92)
        %v100445 = vmul.f32 %v100444, %v100438 (stack93)
        %v100446 = vand.u32 2147483647, %v100438 (stack94)
        %vm100447 = vcmp.lt.f32.partialorder %v100446, 0.0004427343 (stack95)
        %v100448 = vsel /*vm=*/%vm100447, /*on_true_vy=*/%v100445, /*on_false_vx=*/%v100442 (stack96)
        %v100449 = vxor.u32 %v100448, 2147483648 (stack87)
        %vm100452 = vcmp.lt.f32.partialorder %v100449, 5.0 (stack86)
        %v100457 = vsel /*vm=*/%vm100452, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v100461 = vsel /*vm=*/%vm100452, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v100465 = vsel /*vm=*/%vm100452, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v100469 = vsel /*vm=*/%vm100452, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v100473 = vsel /*vm=*/%vm100452, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v100477 = vsel /*vm=*/%vm100452, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v100481 = vsel /*vm=*/%vm100452, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v100485 = vsel /*vm=*/%vm100452, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v100489 = vsel /*vm=*/%vm100452, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v100493 = vadd.f32 %v100449, -2.5 (stack82)
        %v100495 = vrsqrt.pop %v100449 (stack97)
        %v100496 = vmul.f32 %v100449, %v100495 (stack98)
        %vm100497 = vcmp.eq.f32.partialorder %v100449, inf (stack99)
        %v100498 = vsel /*vm=*/%vm100497, /*on_true_vy=*/%v100449, /*on_false_vx=*/%v100496 (stack100)
        %vm100499 = vcmp.eq.f32.partialorder %v100449, 0.0 (stack101)
        %v100500 = vand.u32 %v100449, 2147483648 (stack102)
        %v100501 = vsel /*vm=*/%vm100499, /*on_true_vy=*/%v100500, /*on_false_vx=*/%v100498 (stack103)
        %v100504 = vadd.f32 %v100501, -3.0 (stack82)
        %v100508 = vsel /*vm=*/%vm100452, /*on_true_vy=*/%v100493, /*on_false_vx=*/%v100504 (stack72)
        %v100512 = vmul.f32 %v100489, %v100508 (stack83)
        %v100516 = vadd.f32 %v100485, %v100512 (stack82)
        %v100520 = vmul.f32 %v100516, %v100508 (stack83)
        %v100524 = vadd.f32 %v100481, %v100520 (stack82)
        %v100528 = vmul.f32 %v100524, %v100508 (stack83)
        %v100532 = vadd.f32 %v100477, %v100528 (stack82)
        %v100536 = vmul.f32 %v100532, %v100508 (stack83)
        %v100540 = vadd.f32 %v100473, %v100536 (stack82)
        %v100544 = vmul.f32 %v100540, %v100508 (stack83)
        %v100548 = vadd.f32 %v100469, %v100544 (stack82)
        %v100552 = vmul.f32 %v100548, %v100508 (stack83)
        %v100556 = vadd.f32 %v100465, %v100552 (stack82)
        %v100560 = vmul.f32 %v100556, %v100508 (stack83)
        %v100564 = vadd.f32 %v100461, %v100560 (stack82)
        %v100568 = vmul.f32 %v100564, %v100508 (stack83)
        %v100572 = vadd.f32 %v100457, %v100568 (stack82)
        %v100576 = vmul.f32 %v100572, %v100423 (stack83)
        %v100580 = vsel /*vm=*/%vm100428, /*on_true_vy=*/%v100433, /*on_false_vx=*/%v100576 (stack72)
        %v100584 = vmul.f32 %v100580, 1.4140625 (stack83)
        %s100586 = scalar_lea.vmem %s280, 872 [#allocation0] (stack107)
        %v100587 = vpack.c.bf16 0.0, %v100584 (stack104)
        %100588 = vst [vmem:[%s100586] sm:$0xf] /*vst_source=*/%v100587 (stack105)
        %v100591 = vadd.s32 %v3816, %v97361 (stack65)
        %s100593 = smul.u32 128, %s27 (stack66)
        %v100594 = vlaneseq (stack67)
        %v100595 = vand.u32 %v100594, 127 (stack68)
        %v100596 = vstv %s100593 (stack69)
        %v100597 = vadd.s32 %v100595, %v100596 (stack70)
        %v100601 = vadd.s32 %v100591, %v100597 (stack65)
        %vm100605 = vcmp.lt.u32.totalorder %v100601, %v100591 (stack71)
        %vm100610 = vcmp.lt.u32.totalorder %v100591, %v3816 (stack71)
        %v100615 = vadd.s32 %v3803, %v97344 (stack65)
        %v100619 = vadd.s32 %v100615, 1 (stack65)
        %v100623 = vsel /*vm=*/%vm100610, /*on_true_vy=*/%v100619, /*on_false_vx=*/%v100615 (stack72)
        %v100627 = vadd.s32 %v100623, 1 (stack65)
        %v100631 = vsel /*vm=*/%vm100605, /*on_true_vy=*/%v100627, /*on_false_vx=*/%v100623 (stack72)
        %v100636 = vadd.s32 %v100631, %v10 (stack65)
        %v100640 = vadd.s32 %v100601, %v9 (stack65)
        %v100644 = vadd.s32 %v100636, %v100640 (stack65)
        %v100646 = vshll.u32 %v100640, 13 (stack73)
        %v100647 = vshrl.u32 %v100640, 19 (stack74)
        %v100648 = vor.u32 %v100646, %v100647 (stack75)
        %v100649 = vxor.u32 %v100644, %v100648 (stack76)
        %v100652 = vadd.s32 %v100644, %v100649 (stack65)
        %v100654 = vshll.u32 %v100649, 15 (stack73)
        %v100655 = vshrl.u32 %v100649, 17 (stack74)
        %v100656 = vor.u32 %v100654, %v100655 (stack75)
        %v100657 = vxor.u32 %v100652, %v100656 (stack76)
        %v100660 = vadd.s32 %v100652, %v100657 (stack65)
        %v100662 = vshll.u32 %v100657, 26 (stack73)
        %v100663 = vshrl.u32 %v100657, 6 (stack74)
        %v100664 = vor.u32 %v100662, %v100663 (stack75)
        %v100665 = vxor.u32 %v100660, %v100664 (stack76)
        %v100668 = vadd.s32 %v100660, %v100665 (stack65)
        %v100672 = vadd.s32 %v100668, %v9 (stack65)
        %v100674 = vshll.u32 %v100665, 6 (stack73)
        %v100675 = vshrl.u32 %v100665, 26 (stack74)
        %v100676 = vor.u32 %v100674, %v100675 (stack75)
        %v100677 = vxor.u32 %v100668, %v100676 (stack76)
        %v100680 = vadd.s32 %v100677, %v8 (stack65)
        %v100684 = vadd.s32 %v100680, 1 (stack65)
        %v100688 = vadd.s32 %v100672, %v100684 (stack65)
        %v100690 = vshll.u32 %v100684, 17 (stack73)
        %v100691 = vshrl.u32 %v100684, 15 (stack74)
        %v100692 = vor.u32 %v100690, %v100691 (stack75)
        %v100693 = vxor.u32 %v100688, %v100692 (stack76)
        %v100696 = vadd.s32 %v100688, %v100693 (stack65)
        %v100698 = vshll.u32 %v100693, 29 (stack73)
        %v100699 = vshrl.u32 %v100693, 3 (stack74)
        %v100700 = vor.u32 %v100698, %v100699 (stack75)
        %v100701 = vxor.u32 %v100696, %v100700 (stack76)
        %v100704 = vadd.s32 %v100696, %v100701 (stack65)
        %v100706 = vshll.u32 %v100701, 16 (stack73)
        %v100707 = vshrl.u32 %v100701, 16 (stack74)
        %v100708 = vor.u32 %v100706, %v100707 (stack75)
        %v100709 = vxor.u32 %v100704, %v100708 (stack76)
        %v100712 = vadd.s32 %v100704, %v100709 (stack65)
        %v100716 = vadd.s32 %v100712, %v8 (stack65)
        %v100718 = vshll.u32 %v100709, 24 (stack73)
        %v100719 = vshrl.u32 %v100709, 8 (stack74)
        %v100720 = vor.u32 %v100718, %v100719 (stack75)
        %v100721 = vxor.u32 %v100712, %v100720 (stack76)
        %v100724 = vadd.s32 %v100721, %v10 (stack65)
        %v100728 = vadd.s32 %v100724, 2 (stack65)
        %v100732 = vadd.s32 %v100716, %v100728 (stack65)
        %v100734 = vshll.u32 %v100728, 13 (stack73)
        %v100735 = vshrl.u32 %v100728, 19 (stack74)
        %v100736 = vor.u32 %v100734, %v100735 (stack75)
        %v100737 = vxor.u32 %v100732, %v100736 (stack76)
        %v100740 = vadd.s32 %v100732, %v100737 (stack65)
        %v100742 = vshll.u32 %v100737, 15 (stack73)
        %v100743 = vshrl.u32 %v100737, 17 (stack74)
        %v100744 = vor.u32 %v100742, %v100743 (stack75)
        %v100745 = vxor.u32 %v100740, %v100744 (stack76)
        %v100748 = vadd.s32 %v100740, %v100745 (stack65)
        %v100750 = vshll.u32 %v100745, 26 (stack73)
        %v100751 = vshrl.u32 %v100745, 6 (stack74)
        %v100752 = vor.u32 %v100750, %v100751 (stack75)
        %v100753 = vxor.u32 %v100748, %v100752 (stack76)
        %v100756 = vadd.s32 %v100748, %v100753 (stack65)
        %v100760 = vadd.s32 %v100756, %v10 (stack65)
        %v100762 = vshll.u32 %v100753, 6 (stack73)
        %v100763 = vshrl.u32 %v100753, 26 (stack74)
        %v100764 = vor.u32 %v100762, %v100763 (stack75)
        %v100765 = vxor.u32 %v100756, %v100764 (stack76)
        %v100768 = vadd.s32 %v100765, %v9 (stack65)
        %v100772 = vadd.s32 %v100768, 3 (stack65)
        %v100776 = vadd.s32 %v100760, %v100772 (stack65)
        %v100778 = vshll.u32 %v100772, 17 (stack73)
        %v100779 = vshrl.u32 %v100772, 15 (stack74)
        %v100780 = vor.u32 %v100778, %v100779 (stack75)
        %v100781 = vxor.u32 %v100776, %v100780 (stack76)
        %v100784 = vadd.s32 %v100776, %v100781 (stack65)
        %v100786 = vshll.u32 %v100781, 29 (stack73)
        %v100787 = vshrl.u32 %v100781, 3 (stack74)
        %v100788 = vor.u32 %v100786, %v100787 (stack75)
        %v100789 = vxor.u32 %v100784, %v100788 (stack76)
        %v100792 = vadd.s32 %v100784, %v100789 (stack65)
        %v100794 = vshll.u32 %v100789, 16 (stack73)
        %v100795 = vshrl.u32 %v100789, 16 (stack74)
        %v100796 = vor.u32 %v100794, %v100795 (stack75)
        %v100797 = vxor.u32 %v100792, %v100796 (stack76)
        %v100800 = vadd.s32 %v100792, %v100797 (stack65)
        %v100804 = vadd.s32 %v100800, %v9 (stack65)
        %v100806 = vshll.u32 %v100797, 24 (stack73)
        %v100807 = vshrl.u32 %v100797, 8 (stack74)
        %v100808 = vor.u32 %v100806, %v100807 (stack75)
        %v100809 = vxor.u32 %v100800, %v100808 (stack76)
        %v100812 = vadd.s32 %v100809, %v8 (stack65)
        %v100816 = vadd.s32 %v100812, 4 (stack65)
        %v100820 = vadd.s32 %v100804, %v100816 (stack65)
        %v100822 = vshll.u32 %v100816, 13 (stack73)
        %v100823 = vshrl.u32 %v100816, 19 (stack74)
        %v100824 = vor.u32 %v100822, %v100823 (stack75)
        %v100825 = vxor.u32 %v100820, %v100824 (stack76)
        %v100828 = vadd.s32 %v100820, %v100825 (stack65)
        %v100830 = vshll.u32 %v100825, 15 (stack73)
        %v100831 = vshrl.u32 %v100825, 17 (stack74)
        %v100832 = vor.u32 %v100830, %v100831 (stack75)
        %v100833 = vxor.u32 %v100828, %v100832 (stack76)
        %v100836 = vadd.s32 %v100828, %v100833 (stack65)
        %v100838 = vshll.u32 %v100833, 26 (stack73)
        %v100839 = vshrl.u32 %v100833, 6 (stack74)
        %v100840 = vor.u32 %v100838, %v100839 (stack75)
        %v100841 = vxor.u32 %v100836, %v100840 (stack76)
        %v100844 = vadd.s32 %v100836, %v100841 (stack65)
        %v100848 = vadd.s32 %v100844, %v8 (stack65)
        %v100850 = vshll.u32 %v100841, 6 (stack73)
        %v100851 = vshrl.u32 %v100841, 26 (stack74)
        %v100852 = vor.u32 %v100850, %v100851 (stack75)
        %v100853 = vxor.u32 %v100844, %v100852 (stack76)
        %v100856 = vadd.s32 %v100853, %v10 (stack65)
        %v100860 = vadd.s32 %v100856, 5 (stack65)
        %v100862 = vxor.u32 %v100848, %v100860 (stack76)
        %v100863 = vand.u32.u8 %v100862, 255 (stack77)
        %v100864 = vand.u32 %v100863, 65535 (stack78)
        %v100865 = vshrl.u32 %v100864, 1 (stack79)
        %v100866 = vor.u32 %v100865, 16256 (stack75)
        %v100867 = vand.u32.u16 %v100866, 65535 (stack80)
        %v100868 = vunpack.i.l.bf16 %v100867 (stack81)
        %v100872 = vadd.f32 %v100868, -1.0 (stack82)
        %v100876 = vmul.f32 %v100872, 2.0 (stack83)
        %v100880 = vadd.f32 %v100876, -0.99609375 (stack82)
        %v100884 = vmax.f32 -0.99609375, %v100880 (stack84)
        %v100886 = vand.u32 2147483647, %v100884 (stack85)
        %vm100889 = vcmp.eq.f32.partialorder %v100886, 1.0 (stack86)
        %v100894 = vmul.f32 %v100884, inf (stack83)
        %v100896 = vxor.u32 %v100884, 2147483648 (stack87)
        %v100899 = vmul.f32 %v100884, %v100896 (stack83)
        %v100901 = vadd.f32 %v100899, 1.0 (stack88)
        %v100902 = vlog2.pop %v100901 (stack89)
        %v100903 = vmul.f32 %v100902, 0.6931472 (stack90)
        %v100904 = vmul.f32 -0.5, %v100899 (stack91)
        %v100905 = vadd.f32 %v100904, 1.0 (stack92)
        %v100906 = vmul.f32 %v100905, %v100899 (stack93)
        %v100907 = vand.u32 2147483647, %v100899 (stack94)
        %vm100908 = vcmp.lt.f32.partialorder %v100907, 0.0004427343 (stack95)
        %v100909 = vsel /*vm=*/%vm100908, /*on_true_vy=*/%v100906, /*on_false_vx=*/%v100903 (stack96)
        %v100910 = vxor.u32 %v100909, 2147483648 (stack87)
        %vm100913 = vcmp.lt.f32.partialorder %v100910, 5.0 (stack86)
        %v100918 = vsel /*vm=*/%vm100913, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v100922 = vsel /*vm=*/%vm100913, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v100926 = vsel /*vm=*/%vm100913, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v100930 = vsel /*vm=*/%vm100913, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v100934 = vsel /*vm=*/%vm100913, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v100938 = vsel /*vm=*/%vm100913, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v100942 = vsel /*vm=*/%vm100913, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v100946 = vsel /*vm=*/%vm100913, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v100950 = vsel /*vm=*/%vm100913, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v100954 = vadd.f32 %v100910, -2.5 (stack82)
        %v100956 = vrsqrt.pop %v100910 (stack97)
        %v100957 = vmul.f32 %v100910, %v100956 (stack98)
        %vm100958 = vcmp.eq.f32.partialorder %v100910, inf (stack99)
        %v100959 = vsel /*vm=*/%vm100958, /*on_true_vy=*/%v100910, /*on_false_vx=*/%v100957 (stack100)
        %vm100960 = vcmp.eq.f32.partialorder %v100910, 0.0 (stack101)
        %v100961 = vand.u32 %v100910, 2147483648 (stack102)
        %v100962 = vsel /*vm=*/%vm100960, /*on_true_vy=*/%v100961, /*on_false_vx=*/%v100959 (stack103)
        %v100965 = vadd.f32 %v100962, -3.0 (stack82)
        %v100969 = vsel /*vm=*/%vm100913, /*on_true_vy=*/%v100954, /*on_false_vx=*/%v100965 (stack72)
        %v100973 = vmul.f32 %v100950, %v100969 (stack83)
        %v100977 = vadd.f32 %v100946, %v100973 (stack82)
        %v100981 = vmul.f32 %v100977, %v100969 (stack83)
        %v100985 = vadd.f32 %v100942, %v100981 (stack82)
        %v100989 = vmul.f32 %v100985, %v100969 (stack83)
        %v100993 = vadd.f32 %v100938, %v100989 (stack82)
        %v100997 = vmul.f32 %v100993, %v100969 (stack83)
        %v101001 = vadd.f32 %v100934, %v100997 (stack82)
        %v101005 = vmul.f32 %v101001, %v100969 (stack83)
        %v101009 = vadd.f32 %v100930, %v101005 (stack82)
        %v101013 = vmul.f32 %v101009, %v100969 (stack83)
        %v101017 = vadd.f32 %v100926, %v101013 (stack82)
        %v101021 = vmul.f32 %v101017, %v100969 (stack83)
        %v101025 = vadd.f32 %v100922, %v101021 (stack82)
        %v101029 = vmul.f32 %v101025, %v100969 (stack83)
        %v101033 = vadd.f32 %v100918, %v101029 (stack82)
        %v101037 = vmul.f32 %v101033, %v100884 (stack83)
        %v101041 = vsel /*vm=*/%vm100889, /*on_true_vy=*/%v100894, /*on_false_vx=*/%v101037 (stack72)
        %v101045 = vmul.f32 %v101041, 1.4140625 (stack83)
        %s101047 = scalar_lea.vmem %s280, 1000 [#allocation0] (stack107)
        %v101048 = vpack.c.bf16 0.0, %v101045 (stack104)
        %101049 = vst [vmem:[%s101047] sm:$0xf] /*vst_source=*/%v101048 (stack105)
        %s101050 = sadd.s32 %s339, 216 (stack106)
        %s101051 = sshrl.u32 %s101050, 10 (stack49)
        %p101052 = scmp.lt.s32.totalorder 1, %s101051 (stack50)
        %s101053 = scalar_select /*predicate=*/%p101052, /*on_true=*/1, /*on_false=*/%s101051 (stack51)
        %s101054 = sand.u32 %s101050, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s101055 = sshrl.u32 %s101054, 7 (stack53)
        %s101056 = sand.u32 %s101054, 127 /* smod.u32 w/div 128 */ (stack54)
        %s101057 = smul.addr %s101053, 8 (stack55)
        %s101058 = scalar_lea.vmem %s3, %s101057 (stack56)
        %s101060 = scalar_lea.vmem %s101058, %s101055 (stack57)
        %v101061 = vld [vmem:[%s101060] ss:$0 sm:$0xff] (stack58)
        %s101062 = sand.u32 %s101056, 255 (stack59)
        %s101064 = sor.u32 256, %s101062 (stack60)
        %101065 = vbcast.lane.b32.xlu0 %v101061, %s101064 (stack61)
        %v101066 = vpop.permute.xlu0 %101065 (stack62)
        %s101067 = sadd.s32 %s347, 216 (stack106)
        %s101068 = sshrl.u32 %s101067, 10 (stack49)
        %p101069 = scmp.lt.s32.totalorder 1, %s101068 (stack50)
        %s101070 = scalar_select /*predicate=*/%p101069, /*on_true=*/1, /*on_false=*/%s101068 (stack51)
        %s101071 = sand.u32 %s101067, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s101072 = sshrl.u32 %s101071, 7 (stack53)
        %s101073 = sand.u32 %s101071, 127 /* smod.u32 w/div 128 */ (stack54)
        %s101074 = smul.addr %s101070, 8 (stack55)
        %s101075 = scalar_lea.vmem %s5, %s101074 (stack56)
        %s101077 = scalar_lea.vmem %s101075, %s101072 (stack57)
        %v101078 = vld [vmem:[%s101077] ss:$0 sm:$0xff] (stack58)
        %s101079 = sand.u32 %s101073, 255 (stack59)
        %s101081 = sor.u32 256, %s101079 (stack60)
        %101082 = vbcast.lane.b32.xlu0 %v101078, %s101081 (stack61)
        %v101083 = vpop.permute.xlu0 %101082 (stack62)
        %v101086 = vadd.s32 %v408, %v101083 (stack65)
        %s101088 = smul.u32 128, %s27 (stack66)
        %v101089 = vlaneseq (stack67)
        %v101090 = vand.u32 %v101089, 127 (stack68)
        %v101091 = vstv %s101088 (stack69)
        %v101092 = vadd.s32 %v101090, %v101091 (stack70)
        %v101096 = vadd.s32 %v101086, %v101092 (stack65)
        %vm101100 = vcmp.lt.u32.totalorder %v101096, %v101086 (stack71)
        %vm101105 = vcmp.lt.u32.totalorder %v101086, %v408 (stack71)
        %v101110 = vadd.s32 %v380, %v101066 (stack65)
        %v101114 = vadd.s32 %v101110, 1 (stack65)
        %v101118 = vsel /*vm=*/%vm101105, /*on_true_vy=*/%v101114, /*on_false_vx=*/%v101110 (stack72)
        %v101122 = vadd.s32 %v101118, 1 (stack65)
        %v101126 = vsel /*vm=*/%vm101100, /*on_true_vy=*/%v101122, /*on_false_vx=*/%v101118 (stack72)
        %v101131 = vadd.s32 %v101126, %v10 (stack65)
        %v101135 = vadd.s32 %v101096, %v9 (stack65)
        %v101139 = vadd.s32 %v101131, %v101135 (stack65)
        %v101141 = vshll.u32 %v101135, 13 (stack73)
        %v101142 = vshrl.u32 %v101135, 19 (stack74)
        %v101143 = vor.u32 %v101141, %v101142 (stack75)
        %v101144 = vxor.u32 %v101139, %v101143 (stack76)
        %v101147 = vadd.s32 %v101139, %v101144 (stack65)
        %v101149 = vshll.u32 %v101144, 15 (stack73)
        %v101150 = vshrl.u32 %v101144, 17 (stack74)
        %v101151 = vor.u32 %v101149, %v101150 (stack75)
        %v101152 = vxor.u32 %v101147, %v101151 (stack76)
        %v101155 = vadd.s32 %v101147, %v101152 (stack65)
        %v101157 = vshll.u32 %v101152, 26 (stack73)
        %v101158 = vshrl.u32 %v101152, 6 (stack74)
        %v101159 = vor.u32 %v101157, %v101158 (stack75)
        %v101160 = vxor.u32 %v101155, %v101159 (stack76)
        %v101163 = vadd.s32 %v101155, %v101160 (stack65)
        %v101167 = vadd.s32 %v101163, %v9 (stack65)
        %v101169 = vshll.u32 %v101160, 6 (stack73)
        %v101170 = vshrl.u32 %v101160, 26 (stack74)
        %v101171 = vor.u32 %v101169, %v101170 (stack75)
        %v101172 = vxor.u32 %v101163, %v101171 (stack76)
        %v101175 = vadd.s32 %v101172, %v8 (stack65)
        %v101179 = vadd.s32 %v101175, 1 (stack65)
        %v101183 = vadd.s32 %v101167, %v101179 (stack65)
        %v101185 = vshll.u32 %v101179, 17 (stack73)
        %v101186 = vshrl.u32 %v101179, 15 (stack74)
        %v101187 = vor.u32 %v101185, %v101186 (stack75)
        %v101188 = vxor.u32 %v101183, %v101187 (stack76)
        %v101191 = vadd.s32 %v101183, %v101188 (stack65)
        %v101193 = vshll.u32 %v101188, 29 (stack73)
        %v101194 = vshrl.u32 %v101188, 3 (stack74)
        %v101195 = vor.u32 %v101193, %v101194 (stack75)
        %v101196 = vxor.u32 %v101191, %v101195 (stack76)
        %v101199 = vadd.s32 %v101191, %v101196 (stack65)
        %v101201 = vshll.u32 %v101196, 16 (stack73)
        %v101202 = vshrl.u32 %v101196, 16 (stack74)
        %v101203 = vor.u32 %v101201, %v101202 (stack75)
        %v101204 = vxor.u32 %v101199, %v101203 (stack76)
        %v101207 = vadd.s32 %v101199, %v101204 (stack65)
        %v101211 = vadd.s32 %v101207, %v8 (stack65)
        %v101213 = vshll.u32 %v101204, 24 (stack73)
        %v101214 = vshrl.u32 %v101204, 8 (stack74)
        %v101215 = vor.u32 %v101213, %v101214 (stack75)
        %v101216 = vxor.u32 %v101207, %v101215 (stack76)
        %v101219 = vadd.s32 %v101216, %v10 (stack65)
        %v101223 = vadd.s32 %v101219, 2 (stack65)
        %v101227 = vadd.s32 %v101211, %v101223 (stack65)
        %v101229 = vshll.u32 %v101223, 13 (stack73)
        %v101230 = vshrl.u32 %v101223, 19 (stack74)
        %v101231 = vor.u32 %v101229, %v101230 (stack75)
        %v101232 = vxor.u32 %v101227, %v101231 (stack76)
        %v101235 = vadd.s32 %v101227, %v101232 (stack65)
        %v101237 = vshll.u32 %v101232, 15 (stack73)
        %v101238 = vshrl.u32 %v101232, 17 (stack74)
        %v101239 = vor.u32 %v101237, %v101238 (stack75)
        %v101240 = vxor.u32 %v101235, %v101239 (stack76)
        %v101243 = vadd.s32 %v101235, %v101240 (stack65)
        %v101245 = vshll.u32 %v101240, 26 (stack73)
        %v101246 = vshrl.u32 %v101240, 6 (stack74)
        %v101247 = vor.u32 %v101245, %v101246 (stack75)
        %v101248 = vxor.u32 %v101243, %v101247 (stack76)
        %v101251 = vadd.s32 %v101243, %v101248 (stack65)
        %v101255 = vadd.s32 %v101251, %v10 (stack65)
        %v101257 = vshll.u32 %v101248, 6 (stack73)
        %v101258 = vshrl.u32 %v101248, 26 (stack74)
        %v101259 = vor.u32 %v101257, %v101258 (stack75)
        %v101260 = vxor.u32 %v101251, %v101259 (stack76)
        %v101263 = vadd.s32 %v101260, %v9 (stack65)
        %v101267 = vadd.s32 %v101263, 3 (stack65)
        %v101271 = vadd.s32 %v101255, %v101267 (stack65)
        %v101273 = vshll.u32 %v101267, 17 (stack73)
        %v101274 = vshrl.u32 %v101267, 15 (stack74)
        %v101275 = vor.u32 %v101273, %v101274 (stack75)
        %v101276 = vxor.u32 %v101271, %v101275 (stack76)
        %v101279 = vadd.s32 %v101271, %v101276 (stack65)
        %v101281 = vshll.u32 %v101276, 29 (stack73)
        %v101282 = vshrl.u32 %v101276, 3 (stack74)
        %v101283 = vor.u32 %v101281, %v101282 (stack75)
        %v101284 = vxor.u32 %v101279, %v101283 (stack76)
        %v101287 = vadd.s32 %v101279, %v101284 (stack65)
        %v101289 = vshll.u32 %v101284, 16 (stack73)
        %v101290 = vshrl.u32 %v101284, 16 (stack74)
        %v101291 = vor.u32 %v101289, %v101290 (stack75)
        %v101292 = vxor.u32 %v101287, %v101291 (stack76)
        %v101295 = vadd.s32 %v101287, %v101292 (stack65)
        %v101299 = vadd.s32 %v101295, %v9 (stack65)
        %v101301 = vshll.u32 %v101292, 24 (stack73)
        %v101302 = vshrl.u32 %v101292, 8 (stack74)
        %v101303 = vor.u32 %v101301, %v101302 (stack75)
        %v101304 = vxor.u32 %v101295, %v101303 (stack76)
        %v101307 = vadd.s32 %v101304, %v8 (stack65)
        %v101311 = vadd.s32 %v101307, 4 (stack65)
        %v101315 = vadd.s32 %v101299, %v101311 (stack65)
        %v101317 = vshll.u32 %v101311, 13 (stack73)
        %v101318 = vshrl.u32 %v101311, 19 (stack74)
        %v101319 = vor.u32 %v101317, %v101318 (stack75)
        %v101320 = vxor.u32 %v101315, %v101319 (stack76)
        %v101323 = vadd.s32 %v101315, %v101320 (stack65)
        %v101325 = vshll.u32 %v101320, 15 (stack73)
        %v101326 = vshrl.u32 %v101320, 17 (stack74)
        %v101327 = vor.u32 %v101325, %v101326 (stack75)
        %v101328 = vxor.u32 %v101323, %v101327 (stack76)
        %v101331 = vadd.s32 %v101323, %v101328 (stack65)
        %v101333 = vshll.u32 %v101328, 26 (stack73)
        %v101334 = vshrl.u32 %v101328, 6 (stack74)
        %v101335 = vor.u32 %v101333, %v101334 (stack75)
        %v101336 = vxor.u32 %v101331, %v101335 (stack76)
        %v101339 = vadd.s32 %v101331, %v101336 (stack65)
        %v101343 = vadd.s32 %v101339, %v8 (stack65)
        %v101345 = vshll.u32 %v101336, 6 (stack73)
        %v101346 = vshrl.u32 %v101336, 26 (stack74)
        %v101347 = vor.u32 %v101345, %v101346 (stack75)
        %v101348 = vxor.u32 %v101339, %v101347 (stack76)
        %v101351 = vadd.s32 %v101348, %v10 (stack65)
        %v101355 = vadd.s32 %v101351, 5 (stack65)
        %v101357 = vxor.u32 %v101343, %v101355 (stack76)
        %v101358 = vand.u32.u8 %v101357, 255 (stack77)
        %v101359 = vand.u32 %v101358, 65535 (stack78)
        %v101360 = vshrl.u32 %v101359, 1 (stack79)
        %v101361 = vor.u32 %v101360, 16256 (stack75)
        %v101362 = vand.u32.u16 %v101361, 65535 (stack80)
        %v101363 = vunpack.i.l.bf16 %v101362 (stack81)
        %v101367 = vadd.f32 %v101363, -1.0 (stack82)
        %v101371 = vmul.f32 %v101367, 2.0 (stack83)
        %v101375 = vadd.f32 %v101371, -0.99609375 (stack82)
        %v101379 = vmax.f32 -0.99609375, %v101375 (stack84)
        %v101381 = vand.u32 2147483647, %v101379 (stack85)
        %vm101384 = vcmp.eq.f32.partialorder %v101381, 1.0 (stack86)
        %v101389 = vmul.f32 %v101379, inf (stack83)
        %v101391 = vxor.u32 %v101379, 2147483648 (stack87)
        %v101394 = vmul.f32 %v101379, %v101391 (stack83)
        %v101396 = vadd.f32 %v101394, 1.0 (stack88)
        %v101397 = vlog2.pop %v101396 (stack89)
        %v101398 = vmul.f32 %v101397, 0.6931472 (stack90)
        %v101399 = vmul.f32 -0.5, %v101394 (stack91)
        %v101400 = vadd.f32 %v101399, 1.0 (stack92)
        %v101401 = vmul.f32 %v101400, %v101394 (stack93)
        %v101402 = vand.u32 2147483647, %v101394 (stack94)
        %vm101403 = vcmp.lt.f32.partialorder %v101402, 0.0004427343 (stack95)
        %v101404 = vsel /*vm=*/%vm101403, /*on_true_vy=*/%v101401, /*on_false_vx=*/%v101398 (stack96)
        %v101405 = vxor.u32 %v101404, 2147483648 (stack87)
        %vm101408 = vcmp.lt.f32.partialorder %v101405, 5.0 (stack86)
        %v101413 = vsel /*vm=*/%vm101408, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v101417 = vsel /*vm=*/%vm101408, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v101421 = vsel /*vm=*/%vm101408, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v101425 = vsel /*vm=*/%vm101408, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v101429 = vsel /*vm=*/%vm101408, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v101433 = vsel /*vm=*/%vm101408, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v101437 = vsel /*vm=*/%vm101408, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v101441 = vsel /*vm=*/%vm101408, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v101445 = vsel /*vm=*/%vm101408, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v101449 = vadd.f32 %v101405, -2.5 (stack82)
        %v101451 = vrsqrt.pop %v101405 (stack97)
        %v101452 = vmul.f32 %v101405, %v101451 (stack98)
        %vm101453 = vcmp.eq.f32.partialorder %v101405, inf (stack99)
        %v101454 = vsel /*vm=*/%vm101453, /*on_true_vy=*/%v101405, /*on_false_vx=*/%v101452 (stack100)
        %vm101455 = vcmp.eq.f32.partialorder %v101405, 0.0 (stack101)
        %v101456 = vand.u32 %v101405, 2147483648 (stack102)
        %v101457 = vsel /*vm=*/%vm101455, /*on_true_vy=*/%v101456, /*on_false_vx=*/%v101454 (stack103)
        %v101460 = vadd.f32 %v101457, -3.0 (stack82)
        %v101464 = vsel /*vm=*/%vm101408, /*on_true_vy=*/%v101449, /*on_false_vx=*/%v101460 (stack72)
        %v101468 = vmul.f32 %v101445, %v101464 (stack83)
        %v101472 = vadd.f32 %v101441, %v101468 (stack82)
        %v101476 = vmul.f32 %v101472, %v101464 (stack83)
        %v101480 = vadd.f32 %v101437, %v101476 (stack82)
        %v101484 = vmul.f32 %v101480, %v101464 (stack83)
        %v101488 = vadd.f32 %v101433, %v101484 (stack82)
        %v101492 = vmul.f32 %v101488, %v101464 (stack83)
        %v101496 = vadd.f32 %v101429, %v101492 (stack82)
        %v101500 = vmul.f32 %v101496, %v101464 (stack83)
        %v101504 = vadd.f32 %v101425, %v101500 (stack82)
        %v101508 = vmul.f32 %v101504, %v101464 (stack83)
        %v101512 = vadd.f32 %v101421, %v101508 (stack82)
        %v101516 = vmul.f32 %v101512, %v101464 (stack83)
        %v101520 = vadd.f32 %v101417, %v101516 (stack82)
        %v101524 = vmul.f32 %v101520, %v101464 (stack83)
        %v101528 = vadd.f32 %v101413, %v101524 (stack82)
        %v101532 = vmul.f32 %v101528, %v101379 (stack83)
        %v101536 = vsel /*vm=*/%vm101384, /*on_true_vy=*/%v101389, /*on_false_vx=*/%v101532 (stack72)
        %v101540 = vmul.f32 %v101536, 1.4140625 (stack83)
        %s101542 = scalar_lea.vmem %s280, 108 [#allocation0] (stack107)
        %v101543 = vpack.c.bf16 0.0, %v101540 (stack104)
        %101544 = vst [vmem:[%s101542] sm:$0xf] /*vst_source=*/%v101543 (stack105)
        %v101547 = vadd.s32 %v894, %v101083 (stack65)
        %s101549 = smul.u32 128, %s27 (stack66)
        %v101550 = vlaneseq (stack67)
        %v101551 = vand.u32 %v101550, 127 (stack68)
        %v101552 = vstv %s101549 (stack69)
        %v101553 = vadd.s32 %v101551, %v101552 (stack70)
        %v101557 = vadd.s32 %v101547, %v101553 (stack65)
        %vm101561 = vcmp.lt.u32.totalorder %v101557, %v101547 (stack71)
        %vm101566 = vcmp.lt.u32.totalorder %v101547, %v894 (stack71)
        %v101571 = vadd.s32 %v881, %v101066 (stack65)
        %v101575 = vadd.s32 %v101571, 1 (stack65)
        %v101579 = vsel /*vm=*/%vm101566, /*on_true_vy=*/%v101575, /*on_false_vx=*/%v101571 (stack72)
        %v101583 = vadd.s32 %v101579, 1 (stack65)
        %v101587 = vsel /*vm=*/%vm101561, /*on_true_vy=*/%v101583, /*on_false_vx=*/%v101579 (stack72)
        %v101592 = vadd.s32 %v101587, %v10 (stack65)
        %v101596 = vadd.s32 %v101557, %v9 (stack65)
        %v101600 = vadd.s32 %v101592, %v101596 (stack65)
        %v101602 = vshll.u32 %v101596, 13 (stack73)
        %v101603 = vshrl.u32 %v101596, 19 (stack74)
        %v101604 = vor.u32 %v101602, %v101603 (stack75)
        %v101605 = vxor.u32 %v101600, %v101604 (stack76)
        %v101608 = vadd.s32 %v101600, %v101605 (stack65)
        %v101610 = vshll.u32 %v101605, 15 (stack73)
        %v101611 = vshrl.u32 %v101605, 17 (stack74)
        %v101612 = vor.u32 %v101610, %v101611 (stack75)
        %v101613 = vxor.u32 %v101608, %v101612 (stack76)
        %v101616 = vadd.s32 %v101608, %v101613 (stack65)
        %v101618 = vshll.u32 %v101613, 26 (stack73)
        %v101619 = vshrl.u32 %v101613, 6 (stack74)
        %v101620 = vor.u32 %v101618, %v101619 (stack75)
        %v101621 = vxor.u32 %v101616, %v101620 (stack76)
        %v101624 = vadd.s32 %v101616, %v101621 (stack65)
        %v101628 = vadd.s32 %v101624, %v9 (stack65)
        %v101630 = vshll.u32 %v101621, 6 (stack73)
        %v101631 = vshrl.u32 %v101621, 26 (stack74)
        %v101632 = vor.u32 %v101630, %v101631 (stack75)
        %v101633 = vxor.u32 %v101624, %v101632 (stack76)
        %v101636 = vadd.s32 %v101633, %v8 (stack65)
        %v101640 = vadd.s32 %v101636, 1 (stack65)
        %v101644 = vadd.s32 %v101628, %v101640 (stack65)
        %v101646 = vshll.u32 %v101640, 17 (stack73)
        %v101647 = vshrl.u32 %v101640, 15 (stack74)
        %v101648 = vor.u32 %v101646, %v101647 (stack75)
        %v101649 = vxor.u32 %v101644, %v101648 (stack76)
        %v101652 = vadd.s32 %v101644, %v101649 (stack65)
        %v101654 = vshll.u32 %v101649, 29 (stack73)
        %v101655 = vshrl.u32 %v101649, 3 (stack74)
        %v101656 = vor.u32 %v101654, %v101655 (stack75)
        %v101657 = vxor.u32 %v101652, %v101656 (stack76)
        %v101660 = vadd.s32 %v101652, %v101657 (stack65)
        %v101662 = vshll.u32 %v101657, 16 (stack73)
        %v101663 = vshrl.u32 %v101657, 16 (stack74)
        %v101664 = vor.u32 %v101662, %v101663 (stack75)
        %v101665 = vxor.u32 %v101660, %v101664 (stack76)
        %v101668 = vadd.s32 %v101660, %v101665 (stack65)
        %v101672 = vadd.s32 %v101668, %v8 (stack65)
        %v101674 = vshll.u32 %v101665, 24 (stack73)
        %v101675 = vshrl.u32 %v101665, 8 (stack74)
        %v101676 = vor.u32 %v101674, %v101675 (stack75)
        %v101677 = vxor.u32 %v101668, %v101676 (stack76)
        %v101680 = vadd.s32 %v101677, %v10 (stack65)
        %v101684 = vadd.s32 %v101680, 2 (stack65)
        %v101688 = vadd.s32 %v101672, %v101684 (stack65)
        %v101690 = vshll.u32 %v101684, 13 (stack73)
        %v101691 = vshrl.u32 %v101684, 19 (stack74)
        %v101692 = vor.u32 %v101690, %v101691 (stack75)
        %v101693 = vxor.u32 %v101688, %v101692 (stack76)
        %v101696 = vadd.s32 %v101688, %v101693 (stack65)
        %v101698 = vshll.u32 %v101693, 15 (stack73)
        %v101699 = vshrl.u32 %v101693, 17 (stack74)
        %v101700 = vor.u32 %v101698, %v101699 (stack75)
        %v101701 = vxor.u32 %v101696, %v101700 (stack76)
        %v101704 = vadd.s32 %v101696, %v101701 (stack65)
        %v101706 = vshll.u32 %v101701, 26 (stack73)
        %v101707 = vshrl.u32 %v101701, 6 (stack74)
        %v101708 = vor.u32 %v101706, %v101707 (stack75)
        %v101709 = vxor.u32 %v101704, %v101708 (stack76)
        %v101712 = vadd.s32 %v101704, %v101709 (stack65)
        %v101716 = vadd.s32 %v101712, %v10 (stack65)
        %v101718 = vshll.u32 %v101709, 6 (stack73)
        %v101719 = vshrl.u32 %v101709, 26 (stack74)
        %v101720 = vor.u32 %v101718, %v101719 (stack75)
        %v101721 = vxor.u32 %v101712, %v101720 (stack76)
        %v101724 = vadd.s32 %v101721, %v9 (stack65)
        %v101728 = vadd.s32 %v101724, 3 (stack65)
        %v101732 = vadd.s32 %v101716, %v101728 (stack65)
        %v101734 = vshll.u32 %v101728, 17 (stack73)
        %v101735 = vshrl.u32 %v101728, 15 (stack74)
        %v101736 = vor.u32 %v101734, %v101735 (stack75)
        %v101737 = vxor.u32 %v101732, %v101736 (stack76)
        %v101740 = vadd.s32 %v101732, %v101737 (stack65)
        %v101742 = vshll.u32 %v101737, 29 (stack73)
        %v101743 = vshrl.u32 %v101737, 3 (stack74)
        %v101744 = vor.u32 %v101742, %v101743 (stack75)
        %v101745 = vxor.u32 %v101740, %v101744 (stack76)
        %v101748 = vadd.s32 %v101740, %v101745 (stack65)
        %v101750 = vshll.u32 %v101745, 16 (stack73)
        %v101751 = vshrl.u32 %v101745, 16 (stack74)
        %v101752 = vor.u32 %v101750, %v101751 (stack75)
        %v101753 = vxor.u32 %v101748, %v101752 (stack76)
        %v101756 = vadd.s32 %v101748, %v101753 (stack65)
        %v101760 = vadd.s32 %v101756, %v9 (stack65)
        %v101762 = vshll.u32 %v101753, 24 (stack73)
        %v101763 = vshrl.u32 %v101753, 8 (stack74)
        %v101764 = vor.u32 %v101762, %v101763 (stack75)
        %v101765 = vxor.u32 %v101756, %v101764 (stack76)
        %v101768 = vadd.s32 %v101765, %v8 (stack65)
        %v101772 = vadd.s32 %v101768, 4 (stack65)
        %v101776 = vadd.s32 %v101760, %v101772 (stack65)
        %v101778 = vshll.u32 %v101772, 13 (stack73)
        %v101779 = vshrl.u32 %v101772, 19 (stack74)
        %v101780 = vor.u32 %v101778, %v101779 (stack75)
        %v101781 = vxor.u32 %v101776, %v101780 (stack76)
        %v101784 = vadd.s32 %v101776, %v101781 (stack65)
        %v101786 = vshll.u32 %v101781, 15 (stack73)
        %v101787 = vshrl.u32 %v101781, 17 (stack74)
        %v101788 = vor.u32 %v101786, %v101787 (stack75)
        %v101789 = vxor.u32 %v101784, %v101788 (stack76)
        %v101792 = vadd.s32 %v101784, %v101789 (stack65)
        %v101794 = vshll.u32 %v101789, 26 (stack73)
        %v101795 = vshrl.u32 %v101789, 6 (stack74)
        %v101796 = vor.u32 %v101794, %v101795 (stack75)
        %v101797 = vxor.u32 %v101792, %v101796 (stack76)
        %v101800 = vadd.s32 %v101792, %v101797 (stack65)
        %v101804 = vadd.s32 %v101800, %v8 (stack65)
        %v101806 = vshll.u32 %v101797, 6 (stack73)
        %v101807 = vshrl.u32 %v101797, 26 (stack74)
        %v101808 = vor.u32 %v101806, %v101807 (stack75)
        %v101809 = vxor.u32 %v101800, %v101808 (stack76)
        %v101812 = vadd.s32 %v101809, %v10 (stack65)
        %v101816 = vadd.s32 %v101812, 5 (stack65)
        %v101818 = vxor.u32 %v101804, %v101816 (stack76)
        %v101819 = vand.u32.u8 %v101818, 255 (stack77)
        %v101820 = vand.u32 %v101819, 65535 (stack78)
        %v101821 = vshrl.u32 %v101820, 1 (stack79)
        %v101822 = vor.u32 %v101821, 16256 (stack75)
        %v101823 = vand.u32.u16 %v101822, 65535 (stack80)
        %v101824 = vunpack.i.l.bf16 %v101823 (stack81)
        %v101828 = vadd.f32 %v101824, -1.0 (stack82)
        %v101832 = vmul.f32 %v101828, 2.0 (stack83)
        %v101836 = vadd.f32 %v101832, -0.99609375 (stack82)
        %v101840 = vmax.f32 -0.99609375, %v101836 (stack84)
        %v101842 = vand.u32 2147483647, %v101840 (stack85)
        %vm101845 = vcmp.eq.f32.partialorder %v101842, 1.0 (stack86)
        %v101850 = vmul.f32 %v101840, inf (stack83)
        %v101852 = vxor.u32 %v101840, 2147483648 (stack87)
        %v101855 = vmul.f32 %v101840, %v101852 (stack83)
        %v101857 = vadd.f32 %v101855, 1.0 (stack88)
        %v101858 = vlog2.pop %v101857 (stack89)
        %v101859 = vmul.f32 %v101858, 0.6931472 (stack90)
        %v101860 = vmul.f32 -0.5, %v101855 (stack91)
        %v101861 = vadd.f32 %v101860, 1.0 (stack92)
        %v101862 = vmul.f32 %v101861, %v101855 (stack93)
        %v101863 = vand.u32 2147483647, %v101855 (stack94)
        %vm101864 = vcmp.lt.f32.partialorder %v101863, 0.0004427343 (stack95)
        %v101865 = vsel /*vm=*/%vm101864, /*on_true_vy=*/%v101862, /*on_false_vx=*/%v101859 (stack96)
        %v101866 = vxor.u32 %v101865, 2147483648 (stack87)
        %vm101869 = vcmp.lt.f32.partialorder %v101866, 5.0 (stack86)
        %v101874 = vsel /*vm=*/%vm101869, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v101878 = vsel /*vm=*/%vm101869, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v101882 = vsel /*vm=*/%vm101869, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v101886 = vsel /*vm=*/%vm101869, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v101890 = vsel /*vm=*/%vm101869, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v101894 = vsel /*vm=*/%vm101869, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v101898 = vsel /*vm=*/%vm101869, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v101902 = vsel /*vm=*/%vm101869, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v101906 = vsel /*vm=*/%vm101869, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v101910 = vadd.f32 %v101866, -2.5 (stack82)
        %v101912 = vrsqrt.pop %v101866 (stack97)
        %v101913 = vmul.f32 %v101866, %v101912 (stack98)
        %vm101914 = vcmp.eq.f32.partialorder %v101866, inf (stack99)
        %v101915 = vsel /*vm=*/%vm101914, /*on_true_vy=*/%v101866, /*on_false_vx=*/%v101913 (stack100)
        %vm101916 = vcmp.eq.f32.partialorder %v101866, 0.0 (stack101)
        %v101917 = vand.u32 %v101866, 2147483648 (stack102)
        %v101918 = vsel /*vm=*/%vm101916, /*on_true_vy=*/%v101917, /*on_false_vx=*/%v101915 (stack103)
        %v101921 = vadd.f32 %v101918, -3.0 (stack82)
        %v101925 = vsel /*vm=*/%vm101869, /*on_true_vy=*/%v101910, /*on_false_vx=*/%v101921 (stack72)
        %v101929 = vmul.f32 %v101906, %v101925 (stack83)
        %v101933 = vadd.f32 %v101902, %v101929 (stack82)
        %v101937 = vmul.f32 %v101933, %v101925 (stack83)
        %v101941 = vadd.f32 %v101898, %v101937 (stack82)
        %v101945 = vmul.f32 %v101941, %v101925 (stack83)
        %v101949 = vadd.f32 %v101894, %v101945 (stack82)
        %v101953 = vmul.f32 %v101949, %v101925 (stack83)
        %v101957 = vadd.f32 %v101890, %v101953 (stack82)
        %v101961 = vmul.f32 %v101957, %v101925 (stack83)
        %v101965 = vadd.f32 %v101886, %v101961 (stack82)
        %v101969 = vmul.f32 %v101965, %v101925 (stack83)
        %v101973 = vadd.f32 %v101882, %v101969 (stack82)
        %v101977 = vmul.f32 %v101973, %v101925 (stack83)
        %v101981 = vadd.f32 %v101878, %v101977 (stack82)
        %v101985 = vmul.f32 %v101981, %v101925 (stack83)
        %v101989 = vadd.f32 %v101874, %v101985 (stack82)
        %v101993 = vmul.f32 %v101989, %v101840 (stack83)
        %v101997 = vsel /*vm=*/%vm101845, /*on_true_vy=*/%v101850, /*on_false_vx=*/%v101993 (stack72)
        %v102001 = vmul.f32 %v101997, 1.4140625 (stack83)
        %s102003 = scalar_lea.vmem %s280, 236 [#allocation0] (stack107)
        %v102004 = vpack.c.bf16 0.0, %v102001 (stack104)
        %102005 = vst [vmem:[%s102003] sm:$0xf] /*vst_source=*/%v102004 (stack105)
        %v102008 = vadd.s32 %v1381, %v101083 (stack65)
        %s102010 = smul.u32 128, %s27 (stack66)
        %v102011 = vlaneseq (stack67)
        %v102012 = vand.u32 %v102011, 127 (stack68)
        %v102013 = vstv %s102010 (stack69)
        %v102014 = vadd.s32 %v102012, %v102013 (stack70)
        %v102018 = vadd.s32 %v102008, %v102014 (stack65)
        %vm102022 = vcmp.lt.u32.totalorder %v102018, %v102008 (stack71)
        %vm102027 = vcmp.lt.u32.totalorder %v102008, %v1381 (stack71)
        %v102032 = vadd.s32 %v1368, %v101066 (stack65)
        %v102036 = vadd.s32 %v102032, 1 (stack65)
        %v102040 = vsel /*vm=*/%vm102027, /*on_true_vy=*/%v102036, /*on_false_vx=*/%v102032 (stack72)
        %v102044 = vadd.s32 %v102040, 1 (stack65)
        %v102048 = vsel /*vm=*/%vm102022, /*on_true_vy=*/%v102044, /*on_false_vx=*/%v102040 (stack72)
        %v102053 = vadd.s32 %v102048, %v10 (stack65)
        %v102057 = vadd.s32 %v102018, %v9 (stack65)
        %v102061 = vadd.s32 %v102053, %v102057 (stack65)
        %v102063 = vshll.u32 %v102057, 13 (stack73)
        %v102064 = vshrl.u32 %v102057, 19 (stack74)
        %v102065 = vor.u32 %v102063, %v102064 (stack75)
        %v102066 = vxor.u32 %v102061, %v102065 (stack76)
        %v102069 = vadd.s32 %v102061, %v102066 (stack65)
        %v102071 = vshll.u32 %v102066, 15 (stack73)
        %v102072 = vshrl.u32 %v102066, 17 (stack74)
        %v102073 = vor.u32 %v102071, %v102072 (stack75)
        %v102074 = vxor.u32 %v102069, %v102073 (stack76)
        %v102077 = vadd.s32 %v102069, %v102074 (stack65)
        %v102079 = vshll.u32 %v102074, 26 (stack73)
        %v102080 = vshrl.u32 %v102074, 6 (stack74)
        %v102081 = vor.u32 %v102079, %v102080 (stack75)
        %v102082 = vxor.u32 %v102077, %v102081 (stack76)
        %v102085 = vadd.s32 %v102077, %v102082 (stack65)
        %v102089 = vadd.s32 %v102085, %v9 (stack65)
        %v102091 = vshll.u32 %v102082, 6 (stack73)
        %v102092 = vshrl.u32 %v102082, 26 (stack74)
        %v102093 = vor.u32 %v102091, %v102092 (stack75)
        %v102094 = vxor.u32 %v102085, %v102093 (stack76)
        %v102097 = vadd.s32 %v102094, %v8 (stack65)
        %v102101 = vadd.s32 %v102097, 1 (stack65)
        %v102105 = vadd.s32 %v102089, %v102101 (stack65)
        %v102107 = vshll.u32 %v102101, 17 (stack73)
        %v102108 = vshrl.u32 %v102101, 15 (stack74)
        %v102109 = vor.u32 %v102107, %v102108 (stack75)
        %v102110 = vxor.u32 %v102105, %v102109 (stack76)
        %v102113 = vadd.s32 %v102105, %v102110 (stack65)
        %v102115 = vshll.u32 %v102110, 29 (stack73)
        %v102116 = vshrl.u32 %v102110, 3 (stack74)
        %v102117 = vor.u32 %v102115, %v102116 (stack75)
        %v102118 = vxor.u32 %v102113, %v102117 (stack76)
        %v102121 = vadd.s32 %v102113, %v102118 (stack65)
        %v102123 = vshll.u32 %v102118, 16 (stack73)
        %v102124 = vshrl.u32 %v102118, 16 (stack74)
        %v102125 = vor.u32 %v102123, %v102124 (stack75)
        %v102126 = vxor.u32 %v102121, %v102125 (stack76)
        %v102129 = vadd.s32 %v102121, %v102126 (stack65)
        %v102133 = vadd.s32 %v102129, %v8 (stack65)
        %v102135 = vshll.u32 %v102126, 24 (stack73)
        %v102136 = vshrl.u32 %v102126, 8 (stack74)
        %v102137 = vor.u32 %v102135, %v102136 (stack75)
        %v102138 = vxor.u32 %v102129, %v102137 (stack76)
        %v102141 = vadd.s32 %v102138, %v10 (stack65)
        %v102145 = vadd.s32 %v102141, 2 (stack65)
        %v102149 = vadd.s32 %v102133, %v102145 (stack65)
        %v102151 = vshll.u32 %v102145, 13 (stack73)
        %v102152 = vshrl.u32 %v102145, 19 (stack74)
        %v102153 = vor.u32 %v102151, %v102152 (stack75)
        %v102154 = vxor.u32 %v102149, %v102153 (stack76)
        %v102157 = vadd.s32 %v102149, %v102154 (stack65)
        %v102159 = vshll.u32 %v102154, 15 (stack73)
        %v102160 = vshrl.u32 %v102154, 17 (stack74)
        %v102161 = vor.u32 %v102159, %v102160 (stack75)
        %v102162 = vxor.u32 %v102157, %v102161 (stack76)
        %v102165 = vadd.s32 %v102157, %v102162 (stack65)
        %v102167 = vshll.u32 %v102162, 26 (stack73)
        %v102168 = vshrl.u32 %v102162, 6 (stack74)
        %v102169 = vor.u32 %v102167, %v102168 (stack75)
        %v102170 = vxor.u32 %v102165, %v102169 (stack76)
        %v102173 = vadd.s32 %v102165, %v102170 (stack65)
        %v102177 = vadd.s32 %v102173, %v10 (stack65)
        %v102179 = vshll.u32 %v102170, 6 (stack73)
        %v102180 = vshrl.u32 %v102170, 26 (stack74)
        %v102181 = vor.u32 %v102179, %v102180 (stack75)
        %v102182 = vxor.u32 %v102173, %v102181 (stack76)
        %v102185 = vadd.s32 %v102182, %v9 (stack65)
        %v102189 = vadd.s32 %v102185, 3 (stack65)
        %v102193 = vadd.s32 %v102177, %v102189 (stack65)
        %v102195 = vshll.u32 %v102189, 17 (stack73)
        %v102196 = vshrl.u32 %v102189, 15 (stack74)
        %v102197 = vor.u32 %v102195, %v102196 (stack75)
        %v102198 = vxor.u32 %v102193, %v102197 (stack76)
        %v102201 = vadd.s32 %v102193, %v102198 (stack65)
        %v102203 = vshll.u32 %v102198, 29 (stack73)
        %v102204 = vshrl.u32 %v102198, 3 (stack74)
        %v102205 = vor.u32 %v102203, %v102204 (stack75)
        %v102206 = vxor.u32 %v102201, %v102205 (stack76)
        %v102209 = vadd.s32 %v102201, %v102206 (stack65)
        %v102211 = vshll.u32 %v102206, 16 (stack73)
        %v102212 = vshrl.u32 %v102206, 16 (stack74)
        %v102213 = vor.u32 %v102211, %v102212 (stack75)
        %v102214 = vxor.u32 %v102209, %v102213 (stack76)
        %v102217 = vadd.s32 %v102209, %v102214 (stack65)
        %v102221 = vadd.s32 %v102217, %v9 (stack65)
        %v102223 = vshll.u32 %v102214, 24 (stack73)
        %v102224 = vshrl.u32 %v102214, 8 (stack74)
        %v102225 = vor.u32 %v102223, %v102224 (stack75)
        %v102226 = vxor.u32 %v102217, %v102225 (stack76)
        %v102229 = vadd.s32 %v102226, %v8 (stack65)
        %v102233 = vadd.s32 %v102229, 4 (stack65)
        %v102237 = vadd.s32 %v102221, %v102233 (stack65)
        %v102239 = vshll.u32 %v102233, 13 (stack73)
        %v102240 = vshrl.u32 %v102233, 19 (stack74)
        %v102241 = vor.u32 %v102239, %v102240 (stack75)
        %v102242 = vxor.u32 %v102237, %v102241 (stack76)
        %v102245 = vadd.s32 %v102237, %v102242 (stack65)
        %v102247 = vshll.u32 %v102242, 15 (stack73)
        %v102248 = vshrl.u32 %v102242, 17 (stack74)
        %v102249 = vor.u32 %v102247, %v102248 (stack75)
        %v102250 = vxor.u32 %v102245, %v102249 (stack76)
        %v102253 = vadd.s32 %v102245, %v102250 (stack65)
        %v102255 = vshll.u32 %v102250, 26 (stack73)
        %v102256 = vshrl.u32 %v102250, 6 (stack74)
        %v102257 = vor.u32 %v102255, %v102256 (stack75)
        %v102258 = vxor.u32 %v102253, %v102257 (stack76)
        %v102261 = vadd.s32 %v102253, %v102258 (stack65)
        %v102265 = vadd.s32 %v102261, %v8 (stack65)
        %v102267 = vshll.u32 %v102258, 6 (stack73)
        %v102268 = vshrl.u32 %v102258, 26 (stack74)
        %v102269 = vor.u32 %v102267, %v102268 (stack75)
        %v102270 = vxor.u32 %v102261, %v102269 (stack76)
        %v102273 = vadd.s32 %v102270, %v10 (stack65)
        %v102277 = vadd.s32 %v102273, 5 (stack65)
        %v102279 = vxor.u32 %v102265, %v102277 (stack76)
        %v102280 = vand.u32.u8 %v102279, 255 (stack77)
        %v102281 = vand.u32 %v102280, 65535 (stack78)
        %v102282 = vshrl.u32 %v102281, 1 (stack79)
        %v102283 = vor.u32 %v102282, 16256 (stack75)
        %v102284 = vand.u32.u16 %v102283, 65535 (stack80)
        %v102285 = vunpack.i.l.bf16 %v102284 (stack81)
        %v102289 = vadd.f32 %v102285, -1.0 (stack82)
        %v102293 = vmul.f32 %v102289, 2.0 (stack83)
        %v102297 = vadd.f32 %v102293, -0.99609375 (stack82)
        %v102301 = vmax.f32 -0.99609375, %v102297 (stack84)
        %v102303 = vand.u32 2147483647, %v102301 (stack85)
        %vm102306 = vcmp.eq.f32.partialorder %v102303, 1.0 (stack86)
        %v102311 = vmul.f32 %v102301, inf (stack83)
        %v102313 = vxor.u32 %v102301, 2147483648 (stack87)
        %v102316 = vmul.f32 %v102301, %v102313 (stack83)
        %v102318 = vadd.f32 %v102316, 1.0 (stack88)
        %v102319 = vlog2.pop %v102318 (stack89)
        %v102320 = vmul.f32 %v102319, 0.6931472 (stack90)
        %v102321 = vmul.f32 -0.5, %v102316 (stack91)
        %v102322 = vadd.f32 %v102321, 1.0 (stack92)
        %v102323 = vmul.f32 %v102322, %v102316 (stack93)
        %v102324 = vand.u32 2147483647, %v102316 (stack94)
        %vm102325 = vcmp.lt.f32.partialorder %v102324, 0.0004427343 (stack95)
        %v102326 = vsel /*vm=*/%vm102325, /*on_true_vy=*/%v102323, /*on_false_vx=*/%v102320 (stack96)
        %v102327 = vxor.u32 %v102326, 2147483648 (stack87)
        %vm102330 = vcmp.lt.f32.partialorder %v102327, 5.0 (stack86)
        %v102335 = vsel /*vm=*/%vm102330, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v102339 = vsel /*vm=*/%vm102330, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v102343 = vsel /*vm=*/%vm102330, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v102347 = vsel /*vm=*/%vm102330, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v102351 = vsel /*vm=*/%vm102330, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v102355 = vsel /*vm=*/%vm102330, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v102359 = vsel /*vm=*/%vm102330, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v102363 = vsel /*vm=*/%vm102330, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v102367 = vsel /*vm=*/%vm102330, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v102371 = vadd.f32 %v102327, -2.5 (stack82)
        %v102373 = vrsqrt.pop %v102327 (stack97)
        %v102374 = vmul.f32 %v102327, %v102373 (stack98)
        %vm102375 = vcmp.eq.f32.partialorder %v102327, inf (stack99)
        %v102376 = vsel /*vm=*/%vm102375, /*on_true_vy=*/%v102327, /*on_false_vx=*/%v102374 (stack100)
        %vm102377 = vcmp.eq.f32.partialorder %v102327, 0.0 (stack101)
        %v102378 = vand.u32 %v102327, 2147483648 (stack102)
        %v102379 = vsel /*vm=*/%vm102377, /*on_true_vy=*/%v102378, /*on_false_vx=*/%v102376 (stack103)
        %v102382 = vadd.f32 %v102379, -3.0 (stack82)
        %v102386 = vsel /*vm=*/%vm102330, /*on_true_vy=*/%v102371, /*on_false_vx=*/%v102382 (stack72)
        %v102390 = vmul.f32 %v102367, %v102386 (stack83)
        %v102394 = vadd.f32 %v102363, %v102390 (stack82)
        %v102398 = vmul.f32 %v102394, %v102386 (stack83)
        %v102402 = vadd.f32 %v102359, %v102398 (stack82)
        %v102406 = vmul.f32 %v102402, %v102386 (stack83)
        %v102410 = vadd.f32 %v102355, %v102406 (stack82)
        %v102414 = vmul.f32 %v102410, %v102386 (stack83)
        %v102418 = vadd.f32 %v102351, %v102414 (stack82)
        %v102422 = vmul.f32 %v102418, %v102386 (stack83)
        %v102426 = vadd.f32 %v102347, %v102422 (stack82)
        %v102430 = vmul.f32 %v102426, %v102386 (stack83)
        %v102434 = vadd.f32 %v102343, %v102430 (stack82)
        %v102438 = vmul.f32 %v102434, %v102386 (stack83)
        %v102442 = vadd.f32 %v102339, %v102438 (stack82)
        %v102446 = vmul.f32 %v102442, %v102386 (stack83)
        %v102450 = vadd.f32 %v102335, %v102446 (stack82)
        %v102454 = vmul.f32 %v102450, %v102301 (stack83)
        %v102458 = vsel /*vm=*/%vm102306, /*on_true_vy=*/%v102311, /*on_false_vx=*/%v102454 (stack72)
        %v102462 = vmul.f32 %v102458, 1.4140625 (stack83)
        %s102464 = scalar_lea.vmem %s280, 364 [#allocation0] (stack107)
        %v102465 = vpack.c.bf16 0.0, %v102462 (stack104)
        %102466 = vst [vmem:[%s102464] sm:$0xf] /*vst_source=*/%v102465 (stack105)
        %v102469 = vadd.s32 %v1868, %v101083 (stack65)
        %s102471 = smul.u32 128, %s27 (stack66)
        %v102472 = vlaneseq (stack67)
        %v102473 = vand.u32 %v102472, 127 (stack68)
        %v102474 = vstv %s102471 (stack69)
        %v102475 = vadd.s32 %v102473, %v102474 (stack70)
        %v102479 = vadd.s32 %v102469, %v102475 (stack65)
        %vm102483 = vcmp.lt.u32.totalorder %v102479, %v102469 (stack71)
        %vm102488 = vcmp.lt.u32.totalorder %v102469, %v1868 (stack71)
        %v102493 = vadd.s32 %v1855, %v101066 (stack65)
        %v102497 = vadd.s32 %v102493, 1 (stack65)
        %v102501 = vsel /*vm=*/%vm102488, /*on_true_vy=*/%v102497, /*on_false_vx=*/%v102493 (stack72)
        %v102505 = vadd.s32 %v102501, 1 (stack65)
        %v102509 = vsel /*vm=*/%vm102483, /*on_true_vy=*/%v102505, /*on_false_vx=*/%v102501 (stack72)
        %v102514 = vadd.s32 %v102509, %v10 (stack65)
        %v102518 = vadd.s32 %v102479, %v9 (stack65)
        %v102522 = vadd.s32 %v102514, %v102518 (stack65)
        %v102524 = vshll.u32 %v102518, 13 (stack73)
        %v102525 = vshrl.u32 %v102518, 19 (stack74)
        %v102526 = vor.u32 %v102524, %v102525 (stack75)
        %v102527 = vxor.u32 %v102522, %v102526 (stack76)
        %v102530 = vadd.s32 %v102522, %v102527 (stack65)
        %v102532 = vshll.u32 %v102527, 15 (stack73)
        %v102533 = vshrl.u32 %v102527, 17 (stack74)
        %v102534 = vor.u32 %v102532, %v102533 (stack75)
        %v102535 = vxor.u32 %v102530, %v102534 (stack76)
        %v102538 = vadd.s32 %v102530, %v102535 (stack65)
        %v102540 = vshll.u32 %v102535, 26 (stack73)
        %v102541 = vshrl.u32 %v102535, 6 (stack74)
        %v102542 = vor.u32 %v102540, %v102541 (stack75)
        %v102543 = vxor.u32 %v102538, %v102542 (stack76)
        %v102546 = vadd.s32 %v102538, %v102543 (stack65)
        %v102550 = vadd.s32 %v102546, %v9 (stack65)
        %v102552 = vshll.u32 %v102543, 6 (stack73)
        %v102553 = vshrl.u32 %v102543, 26 (stack74)
        %v102554 = vor.u32 %v102552, %v102553 (stack75)
        %v102555 = vxor.u32 %v102546, %v102554 (stack76)
        %v102558 = vadd.s32 %v102555, %v8 (stack65)
        %v102562 = vadd.s32 %v102558, 1 (stack65)
        %v102566 = vadd.s32 %v102550, %v102562 (stack65)
        %v102568 = vshll.u32 %v102562, 17 (stack73)
        %v102569 = vshrl.u32 %v102562, 15 (stack74)
        %v102570 = vor.u32 %v102568, %v102569 (stack75)
        %v102571 = vxor.u32 %v102566, %v102570 (stack76)
        %v102574 = vadd.s32 %v102566, %v102571 (stack65)
        %v102576 = vshll.u32 %v102571, 29 (stack73)
        %v102577 = vshrl.u32 %v102571, 3 (stack74)
        %v102578 = vor.u32 %v102576, %v102577 (stack75)
        %v102579 = vxor.u32 %v102574, %v102578 (stack76)
        %v102582 = vadd.s32 %v102574, %v102579 (stack65)
        %v102584 = vshll.u32 %v102579, 16 (stack73)
        %v102585 = vshrl.u32 %v102579, 16 (stack74)
        %v102586 = vor.u32 %v102584, %v102585 (stack75)
        %v102587 = vxor.u32 %v102582, %v102586 (stack76)
        %v102590 = vadd.s32 %v102582, %v102587 (stack65)
        %v102594 = vadd.s32 %v102590, %v8 (stack65)
        %v102596 = vshll.u32 %v102587, 24 (stack73)
        %v102597 = vshrl.u32 %v102587, 8 (stack74)
        %v102598 = vor.u32 %v102596, %v102597 (stack75)
        %v102599 = vxor.u32 %v102590, %v102598 (stack76)
        %v102602 = vadd.s32 %v102599, %v10 (stack65)
        %v102606 = vadd.s32 %v102602, 2 (stack65)
        %v102610 = vadd.s32 %v102594, %v102606 (stack65)
        %v102612 = vshll.u32 %v102606, 13 (stack73)
        %v102613 = vshrl.u32 %v102606, 19 (stack74)
        %v102614 = vor.u32 %v102612, %v102613 (stack75)
        %v102615 = vxor.u32 %v102610, %v102614 (stack76)
        %v102618 = vadd.s32 %v102610, %v102615 (stack65)
        %v102620 = vshll.u32 %v102615, 15 (stack73)
        %v102621 = vshrl.u32 %v102615, 17 (stack74)
        %v102622 = vor.u32 %v102620, %v102621 (stack75)
        %v102623 = vxor.u32 %v102618, %v102622 (stack76)
        %v102626 = vadd.s32 %v102618, %v102623 (stack65)
        %v102628 = vshll.u32 %v102623, 26 (stack73)
        %v102629 = vshrl.u32 %v102623, 6 (stack74)
        %v102630 = vor.u32 %v102628, %v102629 (stack75)
        %v102631 = vxor.u32 %v102626, %v102630 (stack76)
        %v102634 = vadd.s32 %v102626, %v102631 (stack65)
        %v102638 = vadd.s32 %v102634, %v10 (stack65)
        %v102640 = vshll.u32 %v102631, 6 (stack73)
        %v102641 = vshrl.u32 %v102631, 26 (stack74)
        %v102642 = vor.u32 %v102640, %v102641 (stack75)
        %v102643 = vxor.u32 %v102634, %v102642 (stack76)
        %v102646 = vadd.s32 %v102643, %v9 (stack65)
        %v102650 = vadd.s32 %v102646, 3 (stack65)
        %v102654 = vadd.s32 %v102638, %v102650 (stack65)
        %v102656 = vshll.u32 %v102650, 17 (stack73)
        %v102657 = vshrl.u32 %v102650, 15 (stack74)
        %v102658 = vor.u32 %v102656, %v102657 (stack75)
        %v102659 = vxor.u32 %v102654, %v102658 (stack76)
        %v102662 = vadd.s32 %v102654, %v102659 (stack65)
        %v102664 = vshll.u32 %v102659, 29 (stack73)
        %v102665 = vshrl.u32 %v102659, 3 (stack74)
        %v102666 = vor.u32 %v102664, %v102665 (stack75)
        %v102667 = vxor.u32 %v102662, %v102666 (stack76)
        %v102670 = vadd.s32 %v102662, %v102667 (stack65)
        %v102672 = vshll.u32 %v102667, 16 (stack73)
        %v102673 = vshrl.u32 %v102667, 16 (stack74)
        %v102674 = vor.u32 %v102672, %v102673 (stack75)
        %v102675 = vxor.u32 %v102670, %v102674 (stack76)
        %v102678 = vadd.s32 %v102670, %v102675 (stack65)
        %v102682 = vadd.s32 %v102678, %v9 (stack65)
        %v102684 = vshll.u32 %v102675, 24 (stack73)
        %v102685 = vshrl.u32 %v102675, 8 (stack74)
        %v102686 = vor.u32 %v102684, %v102685 (stack75)
        %v102687 = vxor.u32 %v102678, %v102686 (stack76)
        %v102690 = vadd.s32 %v102687, %v8 (stack65)
        %v102694 = vadd.s32 %v102690, 4 (stack65)
        %v102698 = vadd.s32 %v102682, %v102694 (stack65)
        %v102700 = vshll.u32 %v102694, 13 (stack73)
        %v102701 = vshrl.u32 %v102694, 19 (stack74)
        %v102702 = vor.u32 %v102700, %v102701 (stack75)
        %v102703 = vxor.u32 %v102698, %v102702 (stack76)
        %v102706 = vadd.s32 %v102698, %v102703 (stack65)
        %v102708 = vshll.u32 %v102703, 15 (stack73)
        %v102709 = vshrl.u32 %v102703, 17 (stack74)
        %v102710 = vor.u32 %v102708, %v102709 (stack75)
        %v102711 = vxor.u32 %v102706, %v102710 (stack76)
        %v102714 = vadd.s32 %v102706, %v102711 (stack65)
        %v102716 = vshll.u32 %v102711, 26 (stack73)
        %v102717 = vshrl.u32 %v102711, 6 (stack74)
        %v102718 = vor.u32 %v102716, %v102717 (stack75)
        %v102719 = vxor.u32 %v102714, %v102718 (stack76)
        %v102722 = vadd.s32 %v102714, %v102719 (stack65)
        %v102726 = vadd.s32 %v102722, %v8 (stack65)
        %v102728 = vshll.u32 %v102719, 6 (stack73)
        %v102729 = vshrl.u32 %v102719, 26 (stack74)
        %v102730 = vor.u32 %v102728, %v102729 (stack75)
        %v102731 = vxor.u32 %v102722, %v102730 (stack76)
        %v102734 = vadd.s32 %v102731, %v10 (stack65)
        %v102738 = vadd.s32 %v102734, 5 (stack65)
        %v102740 = vxor.u32 %v102726, %v102738 (stack76)
        %v102741 = vand.u32.u8 %v102740, 255 (stack77)
        %v102742 = vand.u32 %v102741, 65535 (stack78)
        %v102743 = vshrl.u32 %v102742, 1 (stack79)
        %v102744 = vor.u32 %v102743, 16256 (stack75)
        %v102745 = vand.u32.u16 %v102744, 65535 (stack80)
        %v102746 = vunpack.i.l.bf16 %v102745 (stack81)
        %v102750 = vadd.f32 %v102746, -1.0 (stack82)
        %v102754 = vmul.f32 %v102750, 2.0 (stack83)
        %v102758 = vadd.f32 %v102754, -0.99609375 (stack82)
        %v102762 = vmax.f32 -0.99609375, %v102758 (stack84)
        %v102764 = vand.u32 2147483647, %v102762 (stack85)
        %vm102767 = vcmp.eq.f32.partialorder %v102764, 1.0 (stack86)
        %v102772 = vmul.f32 %v102762, inf (stack83)
        %v102774 = vxor.u32 %v102762, 2147483648 (stack87)
        %v102777 = vmul.f32 %v102762, %v102774 (stack83)
        %v102779 = vadd.f32 %v102777, 1.0 (stack88)
        %v102780 = vlog2.pop %v102779 (stack89)
        %v102781 = vmul.f32 %v102780, 0.6931472 (stack90)
        %v102782 = vmul.f32 -0.5, %v102777 (stack91)
        %v102783 = vadd.f32 %v102782, 1.0 (stack92)
        %v102784 = vmul.f32 %v102783, %v102777 (stack93)
        %v102785 = vand.u32 2147483647, %v102777 (stack94)
        %vm102786 = vcmp.lt.f32.partialorder %v102785, 0.0004427343 (stack95)
        %v102787 = vsel /*vm=*/%vm102786, /*on_true_vy=*/%v102784, /*on_false_vx=*/%v102781 (stack96)
        %v102788 = vxor.u32 %v102787, 2147483648 (stack87)
        %vm102791 = vcmp.lt.f32.partialorder %v102788, 5.0 (stack86)
        %v102796 = vsel /*vm=*/%vm102791, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v102800 = vsel /*vm=*/%vm102791, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v102804 = vsel /*vm=*/%vm102791, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v102808 = vsel /*vm=*/%vm102791, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v102812 = vsel /*vm=*/%vm102791, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v102816 = vsel /*vm=*/%vm102791, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v102820 = vsel /*vm=*/%vm102791, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v102824 = vsel /*vm=*/%vm102791, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v102828 = vsel /*vm=*/%vm102791, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v102832 = vadd.f32 %v102788, -2.5 (stack82)
        %v102834 = vrsqrt.pop %v102788 (stack97)
        %v102835 = vmul.f32 %v102788, %v102834 (stack98)
        %vm102836 = vcmp.eq.f32.partialorder %v102788, inf (stack99)
        %v102837 = vsel /*vm=*/%vm102836, /*on_true_vy=*/%v102788, /*on_false_vx=*/%v102835 (stack100)
        %vm102838 = vcmp.eq.f32.partialorder %v102788, 0.0 (stack101)
        %v102839 = vand.u32 %v102788, 2147483648 (stack102)
        %v102840 = vsel /*vm=*/%vm102838, /*on_true_vy=*/%v102839, /*on_false_vx=*/%v102837 (stack103)
        %v102843 = vadd.f32 %v102840, -3.0 (stack82)
        %v102847 = vsel /*vm=*/%vm102791, /*on_true_vy=*/%v102832, /*on_false_vx=*/%v102843 (stack72)
        %v102851 = vmul.f32 %v102828, %v102847 (stack83)
        %v102855 = vadd.f32 %v102824, %v102851 (stack82)
        %v102859 = vmul.f32 %v102855, %v102847 (stack83)
        %v102863 = vadd.f32 %v102820, %v102859 (stack82)
        %v102867 = vmul.f32 %v102863, %v102847 (stack83)
        %v102871 = vadd.f32 %v102816, %v102867 (stack82)
        %v102875 = vmul.f32 %v102871, %v102847 (stack83)
        %v102879 = vadd.f32 %v102812, %v102875 (stack82)
        %v102883 = vmul.f32 %v102879, %v102847 (stack83)
        %v102887 = vadd.f32 %v102808, %v102883 (stack82)
        %v102891 = vmul.f32 %v102887, %v102847 (stack83)
        %v102895 = vadd.f32 %v102804, %v102891 (stack82)
        %v102899 = vmul.f32 %v102895, %v102847 (stack83)
        %v102903 = vadd.f32 %v102800, %v102899 (stack82)
        %v102907 = vmul.f32 %v102903, %v102847 (stack83)
        %v102911 = vadd.f32 %v102796, %v102907 (stack82)
        %v102915 = vmul.f32 %v102911, %v102762 (stack83)
        %v102919 = vsel /*vm=*/%vm102767, /*on_true_vy=*/%v102772, /*on_false_vx=*/%v102915 (stack72)
        %v102923 = vmul.f32 %v102919, 1.4140625 (stack83)
        %s102925 = scalar_lea.vmem %s280, 492 [#allocation0] (stack107)
        %v102926 = vpack.c.bf16 0.0, %v102923 (stack104)
        %102927 = vst [vmem:[%s102925] sm:$0xf] /*vst_source=*/%v102926 (stack105)
        %v102930 = vadd.s32 %v2355, %v101083 (stack65)
        %s102932 = smul.u32 128, %s27 (stack66)
        %v102933 = vlaneseq (stack67)
        %v102934 = vand.u32 %v102933, 127 (stack68)
        %v102935 = vstv %s102932 (stack69)
        %v102936 = vadd.s32 %v102934, %v102935 (stack70)
        %v102940 = vadd.s32 %v102930, %v102936 (stack65)
        %vm102944 = vcmp.lt.u32.totalorder %v102940, %v102930 (stack71)
        %vm102949 = vcmp.lt.u32.totalorder %v102930, %v2355 (stack71)
        %v102954 = vadd.s32 %v2342, %v101066 (stack65)
        %v102958 = vadd.s32 %v102954, 1 (stack65)
        %v102962 = vsel /*vm=*/%vm102949, /*on_true_vy=*/%v102958, /*on_false_vx=*/%v102954 (stack72)
        %v102966 = vadd.s32 %v102962, 1 (stack65)
        %v102970 = vsel /*vm=*/%vm102944, /*on_true_vy=*/%v102966, /*on_false_vx=*/%v102962 (stack72)
        %v102975 = vadd.s32 %v102970, %v10 (stack65)
        %v102979 = vadd.s32 %v102940, %v9 (stack65)
        %v102983 = vadd.s32 %v102975, %v102979 (stack65)
        %v102985 = vshll.u32 %v102979, 13 (stack73)
        %v102986 = vshrl.u32 %v102979, 19 (stack74)
        %v102987 = vor.u32 %v102985, %v102986 (stack75)
        %v102988 = vxor.u32 %v102983, %v102987 (stack76)
        %v102991 = vadd.s32 %v102983, %v102988 (stack65)
        %v102993 = vshll.u32 %v102988, 15 (stack73)
        %v102994 = vshrl.u32 %v102988, 17 (stack74)
        %v102995 = vor.u32 %v102993, %v102994 (stack75)
        %v102996 = vxor.u32 %v102991, %v102995 (stack76)
        %v102999 = vadd.s32 %v102991, %v102996 (stack65)
        %v103001 = vshll.u32 %v102996, 26 (stack73)
        %v103002 = vshrl.u32 %v102996, 6 (stack74)
        %v103003 = vor.u32 %v103001, %v103002 (stack75)
        %v103004 = vxor.u32 %v102999, %v103003 (stack76)
        %v103007 = vadd.s32 %v102999, %v103004 (stack65)
        %v103011 = vadd.s32 %v103007, %v9 (stack65)
        %v103013 = vshll.u32 %v103004, 6 (stack73)
        %v103014 = vshrl.u32 %v103004, 26 (stack74)
        %v103015 = vor.u32 %v103013, %v103014 (stack75)
        %v103016 = vxor.u32 %v103007, %v103015 (stack76)
        %v103019 = vadd.s32 %v103016, %v8 (stack65)
        %v103023 = vadd.s32 %v103019, 1 (stack65)
        %v103027 = vadd.s32 %v103011, %v103023 (stack65)
        %v103029 = vshll.u32 %v103023, 17 (stack73)
        %v103030 = vshrl.u32 %v103023, 15 (stack74)
        %v103031 = vor.u32 %v103029, %v103030 (stack75)
        %v103032 = vxor.u32 %v103027, %v103031 (stack76)
        %v103035 = vadd.s32 %v103027, %v103032 (stack65)
        %v103037 = vshll.u32 %v103032, 29 (stack73)
        %v103038 = vshrl.u32 %v103032, 3 (stack74)
        %v103039 = vor.u32 %v103037, %v103038 (stack75)
        %v103040 = vxor.u32 %v103035, %v103039 (stack76)
        %v103043 = vadd.s32 %v103035, %v103040 (stack65)
        %v103045 = vshll.u32 %v103040, 16 (stack73)
        %v103046 = vshrl.u32 %v103040, 16 (stack74)
        %v103047 = vor.u32 %v103045, %v103046 (stack75)
        %v103048 = vxor.u32 %v103043, %v103047 (stack76)
        %v103051 = vadd.s32 %v103043, %v103048 (stack65)
        %v103055 = vadd.s32 %v103051, %v8 (stack65)
        %v103057 = vshll.u32 %v103048, 24 (stack73)
        %v103058 = vshrl.u32 %v103048, 8 (stack74)
        %v103059 = vor.u32 %v103057, %v103058 (stack75)
        %v103060 = vxor.u32 %v103051, %v103059 (stack76)
        %v103063 = vadd.s32 %v103060, %v10 (stack65)
        %v103067 = vadd.s32 %v103063, 2 (stack65)
        %v103071 = vadd.s32 %v103055, %v103067 (stack65)
        %v103073 = vshll.u32 %v103067, 13 (stack73)
        %v103074 = vshrl.u32 %v103067, 19 (stack74)
        %v103075 = vor.u32 %v103073, %v103074 (stack75)
        %v103076 = vxor.u32 %v103071, %v103075 (stack76)
        %v103079 = vadd.s32 %v103071, %v103076 (stack65)
        %v103081 = vshll.u32 %v103076, 15 (stack73)
        %v103082 = vshrl.u32 %v103076, 17 (stack74)
        %v103083 = vor.u32 %v103081, %v103082 (stack75)
        %v103084 = vxor.u32 %v103079, %v103083 (stack76)
        %v103087 = vadd.s32 %v103079, %v103084 (stack65)
        %v103089 = vshll.u32 %v103084, 26 (stack73)
        %v103090 = vshrl.u32 %v103084, 6 (stack74)
        %v103091 = vor.u32 %v103089, %v103090 (stack75)
        %v103092 = vxor.u32 %v103087, %v103091 (stack76)
        %v103095 = vadd.s32 %v103087, %v103092 (stack65)
        %v103099 = vadd.s32 %v103095, %v10 (stack65)
        %v103101 = vshll.u32 %v103092, 6 (stack73)
        %v103102 = vshrl.u32 %v103092, 26 (stack74)
        %v103103 = vor.u32 %v103101, %v103102 (stack75)
        %v103104 = vxor.u32 %v103095, %v103103 (stack76)
        %v103107 = vadd.s32 %v103104, %v9 (stack65)
        %v103111 = vadd.s32 %v103107, 3 (stack65)
        %v103115 = vadd.s32 %v103099, %v103111 (stack65)
        %v103117 = vshll.u32 %v103111, 17 (stack73)
        %v103118 = vshrl.u32 %v103111, 15 (stack74)
        %v103119 = vor.u32 %v103117, %v103118 (stack75)
        %v103120 = vxor.u32 %v103115, %v103119 (stack76)
        %v103123 = vadd.s32 %v103115, %v103120 (stack65)
        %v103125 = vshll.u32 %v103120, 29 (stack73)
        %v103126 = vshrl.u32 %v103120, 3 (stack74)
        %v103127 = vor.u32 %v103125, %v103126 (stack75)
        %v103128 = vxor.u32 %v103123, %v103127 (stack76)
        %v103131 = vadd.s32 %v103123, %v103128 (stack65)
        %v103133 = vshll.u32 %v103128, 16 (stack73)
        %v103134 = vshrl.u32 %v103128, 16 (stack74)
        %v103135 = vor.u32 %v103133, %v103134 (stack75)
        %v103136 = vxor.u32 %v103131, %v103135 (stack76)
        %v103139 = vadd.s32 %v103131, %v103136 (stack65)
        %v103143 = vadd.s32 %v103139, %v9 (stack65)
        %v103145 = vshll.u32 %v103136, 24 (stack73)
        %v103146 = vshrl.u32 %v103136, 8 (stack74)
        %v103147 = vor.u32 %v103145, %v103146 (stack75)
        %v103148 = vxor.u32 %v103139, %v103147 (stack76)
        %v103151 = vadd.s32 %v103148, %v8 (stack65)
        %v103155 = vadd.s32 %v103151, 4 (stack65)
        %v103159 = vadd.s32 %v103143, %v103155 (stack65)
        %v103161 = vshll.u32 %v103155, 13 (stack73)
        %v103162 = vshrl.u32 %v103155, 19 (stack74)
        %v103163 = vor.u32 %v103161, %v103162 (stack75)
        %v103164 = vxor.u32 %v103159, %v103163 (stack76)
        %v103167 = vadd.s32 %v103159, %v103164 (stack65)
        %v103169 = vshll.u32 %v103164, 15 (stack73)
        %v103170 = vshrl.u32 %v103164, 17 (stack74)
        %v103171 = vor.u32 %v103169, %v103170 (stack75)
        %v103172 = vxor.u32 %v103167, %v103171 (stack76)
        %v103175 = vadd.s32 %v103167, %v103172 (stack65)
        %v103177 = vshll.u32 %v103172, 26 (stack73)
        %v103178 = vshrl.u32 %v103172, 6 (stack74)
        %v103179 = vor.u32 %v103177, %v103178 (stack75)
        %v103180 = vxor.u32 %v103175, %v103179 (stack76)
        %v103183 = vadd.s32 %v103175, %v103180 (stack65)
        %v103187 = vadd.s32 %v103183, %v8 (stack65)
        %v103189 = vshll.u32 %v103180, 6 (stack73)
        %v103190 = vshrl.u32 %v103180, 26 (stack74)
        %v103191 = vor.u32 %v103189, %v103190 (stack75)
        %v103192 = vxor.u32 %v103183, %v103191 (stack76)
        %v103195 = vadd.s32 %v103192, %v10 (stack65)
        %v103199 = vadd.s32 %v103195, 5 (stack65)
        %v103201 = vxor.u32 %v103187, %v103199 (stack76)
        %v103202 = vand.u32.u8 %v103201, 255 (stack77)
        %v103203 = vand.u32 %v103202, 65535 (stack78)
        %v103204 = vshrl.u32 %v103203, 1 (stack79)
        %v103205 = vor.u32 %v103204, 16256 (stack75)
        %v103206 = vand.u32.u16 %v103205, 65535 (stack80)
        %v103207 = vunpack.i.l.bf16 %v103206 (stack81)
        %v103211 = vadd.f32 %v103207, -1.0 (stack82)
        %v103215 = vmul.f32 %v103211, 2.0 (stack83)
        %v103219 = vadd.f32 %v103215, -0.99609375 (stack82)
        %v103223 = vmax.f32 -0.99609375, %v103219 (stack84)
        %v103225 = vand.u32 2147483647, %v103223 (stack85)
        %vm103228 = vcmp.eq.f32.partialorder %v103225, 1.0 (stack86)
        %v103233 = vmul.f32 %v103223, inf (stack83)
        %v103235 = vxor.u32 %v103223, 2147483648 (stack87)
        %v103238 = vmul.f32 %v103223, %v103235 (stack83)
        %v103240 = vadd.f32 %v103238, 1.0 (stack88)
        %v103241 = vlog2.pop %v103240 (stack89)
        %v103242 = vmul.f32 %v103241, 0.6931472 (stack90)
        %v103243 = vmul.f32 -0.5, %v103238 (stack91)
        %v103244 = vadd.f32 %v103243, 1.0 (stack92)
        %v103245 = vmul.f32 %v103244, %v103238 (stack93)
        %v103246 = vand.u32 2147483647, %v103238 (stack94)
        %vm103247 = vcmp.lt.f32.partialorder %v103246, 0.0004427343 (stack95)
        %v103248 = vsel /*vm=*/%vm103247, /*on_true_vy=*/%v103245, /*on_false_vx=*/%v103242 (stack96)
        %v103249 = vxor.u32 %v103248, 2147483648 (stack87)
        %vm103252 = vcmp.lt.f32.partialorder %v103249, 5.0 (stack86)
        %v103257 = vsel /*vm=*/%vm103252, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v103261 = vsel /*vm=*/%vm103252, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v103265 = vsel /*vm=*/%vm103252, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v103269 = vsel /*vm=*/%vm103252, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v103273 = vsel /*vm=*/%vm103252, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v103277 = vsel /*vm=*/%vm103252, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v103281 = vsel /*vm=*/%vm103252, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v103285 = vsel /*vm=*/%vm103252, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v103289 = vsel /*vm=*/%vm103252, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v103293 = vadd.f32 %v103249, -2.5 (stack82)
        %v103295 = vrsqrt.pop %v103249 (stack97)
        %v103296 = vmul.f32 %v103249, %v103295 (stack98)
        %vm103297 = vcmp.eq.f32.partialorder %v103249, inf (stack99)
        %v103298 = vsel /*vm=*/%vm103297, /*on_true_vy=*/%v103249, /*on_false_vx=*/%v103296 (stack100)
        %vm103299 = vcmp.eq.f32.partialorder %v103249, 0.0 (stack101)
        %v103300 = vand.u32 %v103249, 2147483648 (stack102)
        %v103301 = vsel /*vm=*/%vm103299, /*on_true_vy=*/%v103300, /*on_false_vx=*/%v103298 (stack103)
        %v103304 = vadd.f32 %v103301, -3.0 (stack82)
        %v103308 = vsel /*vm=*/%vm103252, /*on_true_vy=*/%v103293, /*on_false_vx=*/%v103304 (stack72)
        %v103312 = vmul.f32 %v103289, %v103308 (stack83)
        %v103316 = vadd.f32 %v103285, %v103312 (stack82)
        %v103320 = vmul.f32 %v103316, %v103308 (stack83)
        %v103324 = vadd.f32 %v103281, %v103320 (stack82)
        %v103328 = vmul.f32 %v103324, %v103308 (stack83)
        %v103332 = vadd.f32 %v103277, %v103328 (stack82)
        %v103336 = vmul.f32 %v103332, %v103308 (stack83)
        %v103340 = vadd.f32 %v103273, %v103336 (stack82)
        %v103344 = vmul.f32 %v103340, %v103308 (stack83)
        %v103348 = vadd.f32 %v103269, %v103344 (stack82)
        %v103352 = vmul.f32 %v103348, %v103308 (stack83)
        %v103356 = vadd.f32 %v103265, %v103352 (stack82)
        %v103360 = vmul.f32 %v103356, %v103308 (stack83)
        %v103364 = vadd.f32 %v103261, %v103360 (stack82)
        %v103368 = vmul.f32 %v103364, %v103308 (stack83)
        %v103372 = vadd.f32 %v103257, %v103368 (stack82)
        %v103376 = vmul.f32 %v103372, %v103223 (stack83)
        %v103380 = vsel /*vm=*/%vm103228, /*on_true_vy=*/%v103233, /*on_false_vx=*/%v103376 (stack72)
        %v103384 = vmul.f32 %v103380, 1.4140625 (stack83)
        %s103386 = scalar_lea.vmem %s280, 620 [#allocation0] (stack107)
        %v103387 = vpack.c.bf16 0.0, %v103384 (stack104)
        %103388 = vst [vmem:[%s103386] sm:$0xf] /*vst_source=*/%v103387 (stack105)
        %v103391 = vadd.s32 %v2842, %v101083 (stack65)
        %s103393 = smul.u32 128, %s27 (stack66)
        %v103394 = vlaneseq (stack67)
        %v103395 = vand.u32 %v103394, 127 (stack68)
        %v103396 = vstv %s103393 (stack69)
        %v103397 = vadd.s32 %v103395, %v103396 (stack70)
        %v103401 = vadd.s32 %v103391, %v103397 (stack65)
        %vm103405 = vcmp.lt.u32.totalorder %v103401, %v103391 (stack71)
        %vm103410 = vcmp.lt.u32.totalorder %v103391, %v2842 (stack71)
        %v103415 = vadd.s32 %v2829, %v101066 (stack65)
        %v103419 = vadd.s32 %v103415, 1 (stack65)
        %v103423 = vsel /*vm=*/%vm103410, /*on_true_vy=*/%v103419, /*on_false_vx=*/%v103415 (stack72)
        %v103427 = vadd.s32 %v103423, 1 (stack65)
        %v103431 = vsel /*vm=*/%vm103405, /*on_true_vy=*/%v103427, /*on_false_vx=*/%v103423 (stack72)
        %v103436 = vadd.s32 %v103431, %v10 (stack65)
        %v103440 = vadd.s32 %v103401, %v9 (stack65)
        %v103444 = vadd.s32 %v103436, %v103440 (stack65)
        %v103446 = vshll.u32 %v103440, 13 (stack73)
        %v103447 = vshrl.u32 %v103440, 19 (stack74)
        %v103448 = vor.u32 %v103446, %v103447 (stack75)
        %v103449 = vxor.u32 %v103444, %v103448 (stack76)
        %v103452 = vadd.s32 %v103444, %v103449 (stack65)
        %v103454 = vshll.u32 %v103449, 15 (stack73)
        %v103455 = vshrl.u32 %v103449, 17 (stack74)
        %v103456 = vor.u32 %v103454, %v103455 (stack75)
        %v103457 = vxor.u32 %v103452, %v103456 (stack76)
        %v103460 = vadd.s32 %v103452, %v103457 (stack65)
        %v103462 = vshll.u32 %v103457, 26 (stack73)
        %v103463 = vshrl.u32 %v103457, 6 (stack74)
        %v103464 = vor.u32 %v103462, %v103463 (stack75)
        %v103465 = vxor.u32 %v103460, %v103464 (stack76)
        %v103468 = vadd.s32 %v103460, %v103465 (stack65)
        %v103472 = vadd.s32 %v103468, %v9 (stack65)
        %v103474 = vshll.u32 %v103465, 6 (stack73)
        %v103475 = vshrl.u32 %v103465, 26 (stack74)
        %v103476 = vor.u32 %v103474, %v103475 (stack75)
        %v103477 = vxor.u32 %v103468, %v103476 (stack76)
        %v103480 = vadd.s32 %v103477, %v8 (stack65)
        %v103484 = vadd.s32 %v103480, 1 (stack65)
        %v103488 = vadd.s32 %v103472, %v103484 (stack65)
        %v103490 = vshll.u32 %v103484, 17 (stack73)
        %v103491 = vshrl.u32 %v103484, 15 (stack74)
        %v103492 = vor.u32 %v103490, %v103491 (stack75)
        %v103493 = vxor.u32 %v103488, %v103492 (stack76)
        %v103496 = vadd.s32 %v103488, %v103493 (stack65)
        %v103498 = vshll.u32 %v103493, 29 (stack73)
        %v103499 = vshrl.u32 %v103493, 3 (stack74)
        %v103500 = vor.u32 %v103498, %v103499 (stack75)
        %v103501 = vxor.u32 %v103496, %v103500 (stack76)
        %v103504 = vadd.s32 %v103496, %v103501 (stack65)
        %v103506 = vshll.u32 %v103501, 16 (stack73)
        %v103507 = vshrl.u32 %v103501, 16 (stack74)
        %v103508 = vor.u32 %v103506, %v103507 (stack75)
        %v103509 = vxor.u32 %v103504, %v103508 (stack76)
        %v103512 = vadd.s32 %v103504, %v103509 (stack65)
        %v103516 = vadd.s32 %v103512, %v8 (stack65)
        %v103518 = vshll.u32 %v103509, 24 (stack73)
        %v103519 = vshrl.u32 %v103509, 8 (stack74)
        %v103520 = vor.u32 %v103518, %v103519 (stack75)
        %v103521 = vxor.u32 %v103512, %v103520 (stack76)
        %v103524 = vadd.s32 %v103521, %v10 (stack65)
        %v103528 = vadd.s32 %v103524, 2 (stack65)
        %v103532 = vadd.s32 %v103516, %v103528 (stack65)
        %v103534 = vshll.u32 %v103528, 13 (stack73)
        %v103535 = vshrl.u32 %v103528, 19 (stack74)
        %v103536 = vor.u32 %v103534, %v103535 (stack75)
        %v103537 = vxor.u32 %v103532, %v103536 (stack76)
        %v103540 = vadd.s32 %v103532, %v103537 (stack65)
        %v103542 = vshll.u32 %v103537, 15 (stack73)
        %v103543 = vshrl.u32 %v103537, 17 (stack74)
        %v103544 = vor.u32 %v103542, %v103543 (stack75)
        %v103545 = vxor.u32 %v103540, %v103544 (stack76)
        %v103548 = vadd.s32 %v103540, %v103545 (stack65)
        %v103550 = vshll.u32 %v103545, 26 (stack73)
        %v103551 = vshrl.u32 %v103545, 6 (stack74)
        %v103552 = vor.u32 %v103550, %v103551 (stack75)
        %v103553 = vxor.u32 %v103548, %v103552 (stack76)
        %v103556 = vadd.s32 %v103548, %v103553 (stack65)
        %v103560 = vadd.s32 %v103556, %v10 (stack65)
        %v103562 = vshll.u32 %v103553, 6 (stack73)
        %v103563 = vshrl.u32 %v103553, 26 (stack74)
        %v103564 = vor.u32 %v103562, %v103563 (stack75)
        %v103565 = vxor.u32 %v103556, %v103564 (stack76)
        %v103568 = vadd.s32 %v103565, %v9 (stack65)
        %v103572 = vadd.s32 %v103568, 3 (stack65)
        %v103576 = vadd.s32 %v103560, %v103572 (stack65)
        %v103578 = vshll.u32 %v103572, 17 (stack73)
        %v103579 = vshrl.u32 %v103572, 15 (stack74)
        %v103580 = vor.u32 %v103578, %v103579 (stack75)
        %v103581 = vxor.u32 %v103576, %v103580 (stack76)
        %v103584 = vadd.s32 %v103576, %v103581 (stack65)
        %v103586 = vshll.u32 %v103581, 29 (stack73)
        %v103587 = vshrl.u32 %v103581, 3 (stack74)
        %v103588 = vor.u32 %v103586, %v103587 (stack75)
        %v103589 = vxor.u32 %v103584, %v103588 (stack76)
        %v103592 = vadd.s32 %v103584, %v103589 (stack65)
        %v103594 = vshll.u32 %v103589, 16 (stack73)
        %v103595 = vshrl.u32 %v103589, 16 (stack74)
        %v103596 = vor.u32 %v103594, %v103595 (stack75)
        %v103597 = vxor.u32 %v103592, %v103596 (stack76)
        %v103600 = vadd.s32 %v103592, %v103597 (stack65)
        %v103604 = vadd.s32 %v103600, %v9 (stack65)
        %v103606 = vshll.u32 %v103597, 24 (stack73)
        %v103607 = vshrl.u32 %v103597, 8 (stack74)
        %v103608 = vor.u32 %v103606, %v103607 (stack75)
        %v103609 = vxor.u32 %v103600, %v103608 (stack76)
        %v103612 = vadd.s32 %v103609, %v8 (stack65)
        %v103616 = vadd.s32 %v103612, 4 (stack65)
        %v103620 = vadd.s32 %v103604, %v103616 (stack65)
        %v103622 = vshll.u32 %v103616, 13 (stack73)
        %v103623 = vshrl.u32 %v103616, 19 (stack74)
        %v103624 = vor.u32 %v103622, %v103623 (stack75)
        %v103625 = vxor.u32 %v103620, %v103624 (stack76)
        %v103628 = vadd.s32 %v103620, %v103625 (stack65)
        %v103630 = vshll.u32 %v103625, 15 (stack73)
        %v103631 = vshrl.u32 %v103625, 17 (stack74)
        %v103632 = vor.u32 %v103630, %v103631 (stack75)
        %v103633 = vxor.u32 %v103628, %v103632 (stack76)
        %v103636 = vadd.s32 %v103628, %v103633 (stack65)
        %v103638 = vshll.u32 %v103633, 26 (stack73)
        %v103639 = vshrl.u32 %v103633, 6 (stack74)
        %v103640 = vor.u32 %v103638, %v103639 (stack75)
        %v103641 = vxor.u32 %v103636, %v103640 (stack76)
        %v103644 = vadd.s32 %v103636, %v103641 (stack65)
        %v103648 = vadd.s32 %v103644, %v8 (stack65)
        %v103650 = vshll.u32 %v103641, 6 (stack73)
        %v103651 = vshrl.u32 %v103641, 26 (stack74)
        %v103652 = vor.u32 %v103650, %v103651 (stack75)
        %v103653 = vxor.u32 %v103644, %v103652 (stack76)
        %v103656 = vadd.s32 %v103653, %v10 (stack65)
        %v103660 = vadd.s32 %v103656, 5 (stack65)
        %v103662 = vxor.u32 %v103648, %v103660 (stack76)
        %v103663 = vand.u32.u8 %v103662, 255 (stack77)
        %v103664 = vand.u32 %v103663, 65535 (stack78)
        %v103665 = vshrl.u32 %v103664, 1 (stack79)
        %v103666 = vor.u32 %v103665, 16256 (stack75)
        %v103667 = vand.u32.u16 %v103666, 65535 (stack80)
        %v103668 = vunpack.i.l.bf16 %v103667 (stack81)
        %v103672 = vadd.f32 %v103668, -1.0 (stack82)
        %v103676 = vmul.f32 %v103672, 2.0 (stack83)
        %v103680 = vadd.f32 %v103676, -0.99609375 (stack82)
        %v103684 = vmax.f32 -0.99609375, %v103680 (stack84)
        %v103686 = vand.u32 2147483647, %v103684 (stack85)
        %vm103689 = vcmp.eq.f32.partialorder %v103686, 1.0 (stack86)
        %v103694 = vmul.f32 %v103684, inf (stack83)
        %v103696 = vxor.u32 %v103684, 2147483648 (stack87)
        %v103699 = vmul.f32 %v103684, %v103696 (stack83)
        %v103701 = vadd.f32 %v103699, 1.0 (stack88)
        %v103702 = vlog2.pop %v103701 (stack89)
        %v103703 = vmul.f32 %v103702, 0.6931472 (stack90)
        %v103704 = vmul.f32 -0.5, %v103699 (stack91)
        %v103705 = vadd.f32 %v103704, 1.0 (stack92)
        %v103706 = vmul.f32 %v103705, %v103699 (stack93)
        %v103707 = vand.u32 2147483647, %v103699 (stack94)
        %vm103708 = vcmp.lt.f32.partialorder %v103707, 0.0004427343 (stack95)
        %v103709 = vsel /*vm=*/%vm103708, /*on_true_vy=*/%v103706, /*on_false_vx=*/%v103703 (stack96)
        %v103710 = vxor.u32 %v103709, 2147483648 (stack87)
        %vm103713 = vcmp.lt.f32.partialorder %v103710, 5.0 (stack86)
        %v103718 = vsel /*vm=*/%vm103713, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v103722 = vsel /*vm=*/%vm103713, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v103726 = vsel /*vm=*/%vm103713, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v103730 = vsel /*vm=*/%vm103713, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v103734 = vsel /*vm=*/%vm103713, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v103738 = vsel /*vm=*/%vm103713, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v103742 = vsel /*vm=*/%vm103713, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v103746 = vsel /*vm=*/%vm103713, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v103750 = vsel /*vm=*/%vm103713, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v103754 = vadd.f32 %v103710, -2.5 (stack82)
        %v103756 = vrsqrt.pop %v103710 (stack97)
        %v103757 = vmul.f32 %v103710, %v103756 (stack98)
        %vm103758 = vcmp.eq.f32.partialorder %v103710, inf (stack99)
        %v103759 = vsel /*vm=*/%vm103758, /*on_true_vy=*/%v103710, /*on_false_vx=*/%v103757 (stack100)
        %vm103760 = vcmp.eq.f32.partialorder %v103710, 0.0 (stack101)
        %v103761 = vand.u32 %v103710, 2147483648 (stack102)
        %v103762 = vsel /*vm=*/%vm103760, /*on_true_vy=*/%v103761, /*on_false_vx=*/%v103759 (stack103)
        %v103765 = vadd.f32 %v103762, -3.0 (stack82)
        %v103769 = vsel /*vm=*/%vm103713, /*on_true_vy=*/%v103754, /*on_false_vx=*/%v103765 (stack72)
        %v103773 = vmul.f32 %v103750, %v103769 (stack83)
        %v103777 = vadd.f32 %v103746, %v103773 (stack82)
        %v103781 = vmul.f32 %v103777, %v103769 (stack83)
        %v103785 = vadd.f32 %v103742, %v103781 (stack82)
        %v103789 = vmul.f32 %v103785, %v103769 (stack83)
        %v103793 = vadd.f32 %v103738, %v103789 (stack82)
        %v103797 = vmul.f32 %v103793, %v103769 (stack83)
        %v103801 = vadd.f32 %v103734, %v103797 (stack82)
        %v103805 = vmul.f32 %v103801, %v103769 (stack83)
        %v103809 = vadd.f32 %v103730, %v103805 (stack82)
        %v103813 = vmul.f32 %v103809, %v103769 (stack83)
        %v103817 = vadd.f32 %v103726, %v103813 (stack82)
        %v103821 = vmul.f32 %v103817, %v103769 (stack83)
        %v103825 = vadd.f32 %v103722, %v103821 (stack82)
        %v103829 = vmul.f32 %v103825, %v103769 (stack83)
        %v103833 = vadd.f32 %v103718, %v103829 (stack82)
        %v103837 = vmul.f32 %v103833, %v103684 (stack83)
        %v103841 = vsel /*vm=*/%vm103689, /*on_true_vy=*/%v103694, /*on_false_vx=*/%v103837 (stack72)
        %v103845 = vmul.f32 %v103841, 1.4140625 (stack83)
        %s103847 = scalar_lea.vmem %s280, 748 [#allocation0] (stack107)
        %v103848 = vpack.c.bf16 0.0, %v103845 (stack104)
        %103849 = vst [vmem:[%s103847] sm:$0xf] /*vst_source=*/%v103848 (stack105)
        %v103852 = vadd.s32 %v3329, %v101083 (stack65)
        %s103854 = smul.u32 128, %s27 (stack66)
        %v103855 = vlaneseq (stack67)
        %v103856 = vand.u32 %v103855, 127 (stack68)
        %v103857 = vstv %s103854 (stack69)
        %v103858 = vadd.s32 %v103856, %v103857 (stack70)
        %v103862 = vadd.s32 %v103852, %v103858 (stack65)
        %vm103866 = vcmp.lt.u32.totalorder %v103862, %v103852 (stack71)
        %vm103871 = vcmp.lt.u32.totalorder %v103852, %v3329 (stack71)
        %v103876 = vadd.s32 %v3316, %v101066 (stack65)
        %v103880 = vadd.s32 %v103876, 1 (stack65)
        %v103884 = vsel /*vm=*/%vm103871, /*on_true_vy=*/%v103880, /*on_false_vx=*/%v103876 (stack72)
        %v103888 = vadd.s32 %v103884, 1 (stack65)
        %v103892 = vsel /*vm=*/%vm103866, /*on_true_vy=*/%v103888, /*on_false_vx=*/%v103884 (stack72)
        %v103897 = vadd.s32 %v103892, %v10 (stack65)
        %v103901 = vadd.s32 %v103862, %v9 (stack65)
        %v103905 = vadd.s32 %v103897, %v103901 (stack65)
        %v103907 = vshll.u32 %v103901, 13 (stack73)
        %v103908 = vshrl.u32 %v103901, 19 (stack74)
        %v103909 = vor.u32 %v103907, %v103908 (stack75)
        %v103910 = vxor.u32 %v103905, %v103909 (stack76)
        %v103913 = vadd.s32 %v103905, %v103910 (stack65)
        %v103915 = vshll.u32 %v103910, 15 (stack73)
        %v103916 = vshrl.u32 %v103910, 17 (stack74)
        %v103917 = vor.u32 %v103915, %v103916 (stack75)
        %v103918 = vxor.u32 %v103913, %v103917 (stack76)
        %v103921 = vadd.s32 %v103913, %v103918 (stack65)
        %v103923 = vshll.u32 %v103918, 26 (stack73)
        %v103924 = vshrl.u32 %v103918, 6 (stack74)
        %v103925 = vor.u32 %v103923, %v103924 (stack75)
        %v103926 = vxor.u32 %v103921, %v103925 (stack76)
        %v103929 = vadd.s32 %v103921, %v103926 (stack65)
        %v103933 = vadd.s32 %v103929, %v9 (stack65)
        %v103935 = vshll.u32 %v103926, 6 (stack73)
        %v103936 = vshrl.u32 %v103926, 26 (stack74)
        %v103937 = vor.u32 %v103935, %v103936 (stack75)
        %v103938 = vxor.u32 %v103929, %v103937 (stack76)
        %v103941 = vadd.s32 %v103938, %v8 (stack65)
        %v103945 = vadd.s32 %v103941, 1 (stack65)
        %v103949 = vadd.s32 %v103933, %v103945 (stack65)
        %v103951 = vshll.u32 %v103945, 17 (stack73)
        %v103952 = vshrl.u32 %v103945, 15 (stack74)
        %v103953 = vor.u32 %v103951, %v103952 (stack75)
        %v103954 = vxor.u32 %v103949, %v103953 (stack76)
        %v103957 = vadd.s32 %v103949, %v103954 (stack65)
        %v103959 = vshll.u32 %v103954, 29 (stack73)
        %v103960 = vshrl.u32 %v103954, 3 (stack74)
        %v103961 = vor.u32 %v103959, %v103960 (stack75)
        %v103962 = vxor.u32 %v103957, %v103961 (stack76)
        %v103965 = vadd.s32 %v103957, %v103962 (stack65)
        %v103967 = vshll.u32 %v103962, 16 (stack73)
        %v103968 = vshrl.u32 %v103962, 16 (stack74)
        %v103969 = vor.u32 %v103967, %v103968 (stack75)
        %v103970 = vxor.u32 %v103965, %v103969 (stack76)
        %v103973 = vadd.s32 %v103965, %v103970 (stack65)
        %v103977 = vadd.s32 %v103973, %v8 (stack65)
        %v103979 = vshll.u32 %v103970, 24 (stack73)
        %v103980 = vshrl.u32 %v103970, 8 (stack74)
        %v103981 = vor.u32 %v103979, %v103980 (stack75)
        %v103982 = vxor.u32 %v103973, %v103981 (stack76)
        %v103985 = vadd.s32 %v103982, %v10 (stack65)
        %v103989 = vadd.s32 %v103985, 2 (stack65)
        %v103993 = vadd.s32 %v103977, %v103989 (stack65)
        %v103995 = vshll.u32 %v103989, 13 (stack73)
        %v103996 = vshrl.u32 %v103989, 19 (stack74)
        %v103997 = vor.u32 %v103995, %v103996 (stack75)
        %v103998 = vxor.u32 %v103993, %v103997 (stack76)
        %v104001 = vadd.s32 %v103993, %v103998 (stack65)
        %v104003 = vshll.u32 %v103998, 15 (stack73)
        %v104004 = vshrl.u32 %v103998, 17 (stack74)
        %v104005 = vor.u32 %v104003, %v104004 (stack75)
        %v104006 = vxor.u32 %v104001, %v104005 (stack76)
        %v104009 = vadd.s32 %v104001, %v104006 (stack65)
        %v104011 = vshll.u32 %v104006, 26 (stack73)
        %v104012 = vshrl.u32 %v104006, 6 (stack74)
        %v104013 = vor.u32 %v104011, %v104012 (stack75)
        %v104014 = vxor.u32 %v104009, %v104013 (stack76)
        %v104017 = vadd.s32 %v104009, %v104014 (stack65)
        %v104021 = vadd.s32 %v104017, %v10 (stack65)
        %v104023 = vshll.u32 %v104014, 6 (stack73)
        %v104024 = vshrl.u32 %v104014, 26 (stack74)
        %v104025 = vor.u32 %v104023, %v104024 (stack75)
        %v104026 = vxor.u32 %v104017, %v104025 (stack76)
        %v104029 = vadd.s32 %v104026, %v9 (stack65)
        %v104033 = vadd.s32 %v104029, 3 (stack65)
        %v104037 = vadd.s32 %v104021, %v104033 (stack65)
        %v104039 = vshll.u32 %v104033, 17 (stack73)
        %v104040 = vshrl.u32 %v104033, 15 (stack74)
        %v104041 = vor.u32 %v104039, %v104040 (stack75)
        %v104042 = vxor.u32 %v104037, %v104041 (stack76)
        %v104045 = vadd.s32 %v104037, %v104042 (stack65)
        %v104047 = vshll.u32 %v104042, 29 (stack73)
        %v104048 = vshrl.u32 %v104042, 3 (stack74)
        %v104049 = vor.u32 %v104047, %v104048 (stack75)
        %v104050 = vxor.u32 %v104045, %v104049 (stack76)
        %v104053 = vadd.s32 %v104045, %v104050 (stack65)
        %v104055 = vshll.u32 %v104050, 16 (stack73)
        %v104056 = vshrl.u32 %v104050, 16 (stack74)
        %v104057 = vor.u32 %v104055, %v104056 (stack75)
        %v104058 = vxor.u32 %v104053, %v104057 (stack76)
        %v104061 = vadd.s32 %v104053, %v104058 (stack65)
        %v104065 = vadd.s32 %v104061, %v9 (stack65)
        %v104067 = vshll.u32 %v104058, 24 (stack73)
        %v104068 = vshrl.u32 %v104058, 8 (stack74)
        %v104069 = vor.u32 %v104067, %v104068 (stack75)
        %v104070 = vxor.u32 %v104061, %v104069 (stack76)
        %v104073 = vadd.s32 %v104070, %v8 (stack65)
        %v104077 = vadd.s32 %v104073, 4 (stack65)
        %v104081 = vadd.s32 %v104065, %v104077 (stack65)
        %v104083 = vshll.u32 %v104077, 13 (stack73)
        %v104084 = vshrl.u32 %v104077, 19 (stack74)
        %v104085 = vor.u32 %v104083, %v104084 (stack75)
        %v104086 = vxor.u32 %v104081, %v104085 (stack76)
        %v104089 = vadd.s32 %v104081, %v104086 (stack65)
        %v104091 = vshll.u32 %v104086, 15 (stack73)
        %v104092 = vshrl.u32 %v104086, 17 (stack74)
        %v104093 = vor.u32 %v104091, %v104092 (stack75)
        %v104094 = vxor.u32 %v104089, %v104093 (stack76)
        %v104097 = vadd.s32 %v104089, %v104094 (stack65)
        %v104099 = vshll.u32 %v104094, 26 (stack73)
        %v104100 = vshrl.u32 %v104094, 6 (stack74)
        %v104101 = vor.u32 %v104099, %v104100 (stack75)
        %v104102 = vxor.u32 %v104097, %v104101 (stack76)
        %v104105 = vadd.s32 %v104097, %v104102 (stack65)
        %v104109 = vadd.s32 %v104105, %v8 (stack65)
        %v104111 = vshll.u32 %v104102, 6 (stack73)
        %v104112 = vshrl.u32 %v104102, 26 (stack74)
        %v104113 = vor.u32 %v104111, %v104112 (stack75)
        %v104114 = vxor.u32 %v104105, %v104113 (stack76)
        %v104117 = vadd.s32 %v104114, %v10 (stack65)
        %v104121 = vadd.s32 %v104117, 5 (stack65)
        %v104123 = vxor.u32 %v104109, %v104121 (stack76)
        %v104124 = vand.u32.u8 %v104123, 255 (stack77)
        %v104125 = vand.u32 %v104124, 65535 (stack78)
        %v104126 = vshrl.u32 %v104125, 1 (stack79)
        %v104127 = vor.u32 %v104126, 16256 (stack75)
        %v104128 = vand.u32.u16 %v104127, 65535 (stack80)
        %v104129 = vunpack.i.l.bf16 %v104128 (stack81)
        %v104133 = vadd.f32 %v104129, -1.0 (stack82)
        %v104137 = vmul.f32 %v104133, 2.0 (stack83)
        %v104141 = vadd.f32 %v104137, -0.99609375 (stack82)
        %v104145 = vmax.f32 -0.99609375, %v104141 (stack84)
        %v104147 = vand.u32 2147483647, %v104145 (stack85)
        %vm104150 = vcmp.eq.f32.partialorder %v104147, 1.0 (stack86)
        %v104155 = vmul.f32 %v104145, inf (stack83)
        %v104157 = vxor.u32 %v104145, 2147483648 (stack87)
        %v104160 = vmul.f32 %v104145, %v104157 (stack83)
        %v104162 = vadd.f32 %v104160, 1.0 (stack88)
        %v104163 = vlog2.pop %v104162 (stack89)
        %v104164 = vmul.f32 %v104163, 0.6931472 (stack90)
        %v104165 = vmul.f32 -0.5, %v104160 (stack91)
        %v104166 = vadd.f32 %v104165, 1.0 (stack92)
        %v104167 = vmul.f32 %v104166, %v104160 (stack93)
        %v104168 = vand.u32 2147483647, %v104160 (stack94)
        %vm104169 = vcmp.lt.f32.partialorder %v104168, 0.0004427343 (stack95)
        %v104170 = vsel /*vm=*/%vm104169, /*on_true_vy=*/%v104167, /*on_false_vx=*/%v104164 (stack96)
        %v104171 = vxor.u32 %v104170, 2147483648 (stack87)
        %vm104174 = vcmp.lt.f32.partialorder %v104171, 5.0 (stack86)
        %v104179 = vsel /*vm=*/%vm104174, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v104183 = vsel /*vm=*/%vm104174, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v104187 = vsel /*vm=*/%vm104174, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v104191 = vsel /*vm=*/%vm104174, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v104195 = vsel /*vm=*/%vm104174, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v104199 = vsel /*vm=*/%vm104174, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v104203 = vsel /*vm=*/%vm104174, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v104207 = vsel /*vm=*/%vm104174, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v104211 = vsel /*vm=*/%vm104174, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v104215 = vadd.f32 %v104171, -2.5 (stack82)
        %v104217 = vrsqrt.pop %v104171 (stack97)
        %v104218 = vmul.f32 %v104171, %v104217 (stack98)
        %vm104219 = vcmp.eq.f32.partialorder %v104171, inf (stack99)
        %v104220 = vsel /*vm=*/%vm104219, /*on_true_vy=*/%v104171, /*on_false_vx=*/%v104218 (stack100)
        %vm104221 = vcmp.eq.f32.partialorder %v104171, 0.0 (stack101)
        %v104222 = vand.u32 %v104171, 2147483648 (stack102)
        %v104223 = vsel /*vm=*/%vm104221, /*on_true_vy=*/%v104222, /*on_false_vx=*/%v104220 (stack103)
        %v104226 = vadd.f32 %v104223, -3.0 (stack82)
        %v104230 = vsel /*vm=*/%vm104174, /*on_true_vy=*/%v104215, /*on_false_vx=*/%v104226 (stack72)
        %v104234 = vmul.f32 %v104211, %v104230 (stack83)
        %v104238 = vadd.f32 %v104207, %v104234 (stack82)
        %v104242 = vmul.f32 %v104238, %v104230 (stack83)
        %v104246 = vadd.f32 %v104203, %v104242 (stack82)
        %v104250 = vmul.f32 %v104246, %v104230 (stack83)
        %v104254 = vadd.f32 %v104199, %v104250 (stack82)
        %v104258 = vmul.f32 %v104254, %v104230 (stack83)
        %v104262 = vadd.f32 %v104195, %v104258 (stack82)
        %v104266 = vmul.f32 %v104262, %v104230 (stack83)
        %v104270 = vadd.f32 %v104191, %v104266 (stack82)
        %v104274 = vmul.f32 %v104270, %v104230 (stack83)
        %v104278 = vadd.f32 %v104187, %v104274 (stack82)
        %v104282 = vmul.f32 %v104278, %v104230 (stack83)
        %v104286 = vadd.f32 %v104183, %v104282 (stack82)
        %v104290 = vmul.f32 %v104286, %v104230 (stack83)
        %v104294 = vadd.f32 %v104179, %v104290 (stack82)
        %v104298 = vmul.f32 %v104294, %v104145 (stack83)
        %v104302 = vsel /*vm=*/%vm104150, /*on_true_vy=*/%v104155, /*on_false_vx=*/%v104298 (stack72)
        %v104306 = vmul.f32 %v104302, 1.4140625 (stack83)
        %s104308 = scalar_lea.vmem %s280, 876 [#allocation0] (stack107)
        %v104309 = vpack.c.bf16 0.0, %v104306 (stack104)
        %104310 = vst [vmem:[%s104308] sm:$0xf] /*vst_source=*/%v104309 (stack105)
        %v104313 = vadd.s32 %v3816, %v101083 (stack65)
        %s104315 = smul.u32 128, %s27 (stack66)
        %v104316 = vlaneseq (stack67)
        %v104317 = vand.u32 %v104316, 127 (stack68)
        %v104318 = vstv %s104315 (stack69)
        %v104319 = vadd.s32 %v104317, %v104318 (stack70)
        %v104323 = vadd.s32 %v104313, %v104319 (stack65)
        %vm104327 = vcmp.lt.u32.totalorder %v104323, %v104313 (stack71)
        %vm104332 = vcmp.lt.u32.totalorder %v104313, %v3816 (stack71)
        %v104337 = vadd.s32 %v3803, %v101066 (stack65)
        %v104341 = vadd.s32 %v104337, 1 (stack65)
        %v104345 = vsel /*vm=*/%vm104332, /*on_true_vy=*/%v104341, /*on_false_vx=*/%v104337 (stack72)
        %v104349 = vadd.s32 %v104345, 1 (stack65)
        %v104353 = vsel /*vm=*/%vm104327, /*on_true_vy=*/%v104349, /*on_false_vx=*/%v104345 (stack72)
        %v104358 = vadd.s32 %v104353, %v10 (stack65)
        %v104362 = vadd.s32 %v104323, %v9 (stack65)
        %v104366 = vadd.s32 %v104358, %v104362 (stack65)
        %v104368 = vshll.u32 %v104362, 13 (stack73)
        %v104369 = vshrl.u32 %v104362, 19 (stack74)
        %v104370 = vor.u32 %v104368, %v104369 (stack75)
        %v104371 = vxor.u32 %v104366, %v104370 (stack76)
        %v104374 = vadd.s32 %v104366, %v104371 (stack65)
        %v104376 = vshll.u32 %v104371, 15 (stack73)
        %v104377 = vshrl.u32 %v104371, 17 (stack74)
        %v104378 = vor.u32 %v104376, %v104377 (stack75)
        %v104379 = vxor.u32 %v104374, %v104378 (stack76)
        %v104382 = vadd.s32 %v104374, %v104379 (stack65)
        %v104384 = vshll.u32 %v104379, 26 (stack73)
        %v104385 = vshrl.u32 %v104379, 6 (stack74)
        %v104386 = vor.u32 %v104384, %v104385 (stack75)
        %v104387 = vxor.u32 %v104382, %v104386 (stack76)
        %v104390 = vadd.s32 %v104382, %v104387 (stack65)
        %v104394 = vadd.s32 %v104390, %v9 (stack65)
        %v104396 = vshll.u32 %v104387, 6 (stack73)
        %v104397 = vshrl.u32 %v104387, 26 (stack74)
        %v104398 = vor.u32 %v104396, %v104397 (stack75)
        %v104399 = vxor.u32 %v104390, %v104398 (stack76)
        %v104402 = vadd.s32 %v104399, %v8 (stack65)
        %v104406 = vadd.s32 %v104402, 1 (stack65)
        %v104410 = vadd.s32 %v104394, %v104406 (stack65)
        %v104412 = vshll.u32 %v104406, 17 (stack73)
        %v104413 = vshrl.u32 %v104406, 15 (stack74)
        %v104414 = vor.u32 %v104412, %v104413 (stack75)
        %v104415 = vxor.u32 %v104410, %v104414 (stack76)
        %v104418 = vadd.s32 %v104410, %v104415 (stack65)
        %v104420 = vshll.u32 %v104415, 29 (stack73)
        %v104421 = vshrl.u32 %v104415, 3 (stack74)
        %v104422 = vor.u32 %v104420, %v104421 (stack75)
        %v104423 = vxor.u32 %v104418, %v104422 (stack76)
        %v104426 = vadd.s32 %v104418, %v104423 (stack65)
        %v104428 = vshll.u32 %v104423, 16 (stack73)
        %v104429 = vshrl.u32 %v104423, 16 (stack74)
        %v104430 = vor.u32 %v104428, %v104429 (stack75)
        %v104431 = vxor.u32 %v104426, %v104430 (stack76)
        %v104434 = vadd.s32 %v104426, %v104431 (stack65)
        %v104438 = vadd.s32 %v104434, %v8 (stack65)
        %v104440 = vshll.u32 %v104431, 24 (stack73)
        %v104441 = vshrl.u32 %v104431, 8 (stack74)
        %v104442 = vor.u32 %v104440, %v104441 (stack75)
        %v104443 = vxor.u32 %v104434, %v104442 (stack76)
        %v104446 = vadd.s32 %v104443, %v10 (stack65)
        %v104450 = vadd.s32 %v104446, 2 (stack65)
        %v104454 = vadd.s32 %v104438, %v104450 (stack65)
        %v104456 = vshll.u32 %v104450, 13 (stack73)
        %v104457 = vshrl.u32 %v104450, 19 (stack74)
        %v104458 = vor.u32 %v104456, %v104457 (stack75)
        %v104459 = vxor.u32 %v104454, %v104458 (stack76)
        %v104462 = vadd.s32 %v104454, %v104459 (stack65)
        %v104464 = vshll.u32 %v104459, 15 (stack73)
        %v104465 = vshrl.u32 %v104459, 17 (stack74)
        %v104466 = vor.u32 %v104464, %v104465 (stack75)
        %v104467 = vxor.u32 %v104462, %v104466 (stack76)
        %v104470 = vadd.s32 %v104462, %v104467 (stack65)
        %v104472 = vshll.u32 %v104467, 26 (stack73)
        %v104473 = vshrl.u32 %v104467, 6 (stack74)
        %v104474 = vor.u32 %v104472, %v104473 (stack75)
        %v104475 = vxor.u32 %v104470, %v104474 (stack76)
        %v104478 = vadd.s32 %v104470, %v104475 (stack65)
        %v104482 = vadd.s32 %v104478, %v10 (stack65)
        %v104484 = vshll.u32 %v104475, 6 (stack73)
        %v104485 = vshrl.u32 %v104475, 26 (stack74)
        %v104486 = vor.u32 %v104484, %v104485 (stack75)
        %v104487 = vxor.u32 %v104478, %v104486 (stack76)
        %v104490 = vadd.s32 %v104487, %v9 (stack65)
        %v104494 = vadd.s32 %v104490, 3 (stack65)
        %v104498 = vadd.s32 %v104482, %v104494 (stack65)
        %v104500 = vshll.u32 %v104494, 17 (stack73)
        %v104501 = vshrl.u32 %v104494, 15 (stack74)
        %v104502 = vor.u32 %v104500, %v104501 (stack75)
        %v104503 = vxor.u32 %v104498, %v104502 (stack76)
        %v104506 = vadd.s32 %v104498, %v104503 (stack65)
        %v104508 = vshll.u32 %v104503, 29 (stack73)
        %v104509 = vshrl.u32 %v104503, 3 (stack74)
        %v104510 = vor.u32 %v104508, %v104509 (stack75)
        %v104511 = vxor.u32 %v104506, %v104510 (stack76)
        %v104514 = vadd.s32 %v104506, %v104511 (stack65)
        %v104516 = vshll.u32 %v104511, 16 (stack73)
        %v104517 = vshrl.u32 %v104511, 16 (stack74)
        %v104518 = vor.u32 %v104516, %v104517 (stack75)
        %v104519 = vxor.u32 %v104514, %v104518 (stack76)
        %v104522 = vadd.s32 %v104514, %v104519 (stack65)
        %v104526 = vadd.s32 %v104522, %v9 (stack65)
        %v104528 = vshll.u32 %v104519, 24 (stack73)
        %v104529 = vshrl.u32 %v104519, 8 (stack74)
        %v104530 = vor.u32 %v104528, %v104529 (stack75)
        %v104531 = vxor.u32 %v104522, %v104530 (stack76)
        %v104534 = vadd.s32 %v104531, %v8 (stack65)
        %v104538 = vadd.s32 %v104534, 4 (stack65)
        %v104542 = vadd.s32 %v104526, %v104538 (stack65)
        %v104544 = vshll.u32 %v104538, 13 (stack73)
        %v104545 = vshrl.u32 %v104538, 19 (stack74)
        %v104546 = vor.u32 %v104544, %v104545 (stack75)
        %v104547 = vxor.u32 %v104542, %v104546 (stack76)
        %v104550 = vadd.s32 %v104542, %v104547 (stack65)
        %v104552 = vshll.u32 %v104547, 15 (stack73)
        %v104553 = vshrl.u32 %v104547, 17 (stack74)
        %v104554 = vor.u32 %v104552, %v104553 (stack75)
        %v104555 = vxor.u32 %v104550, %v104554 (stack76)
        %v104558 = vadd.s32 %v104550, %v104555 (stack65)
        %v104560 = vshll.u32 %v104555, 26 (stack73)
        %v104561 = vshrl.u32 %v104555, 6 (stack74)
        %v104562 = vor.u32 %v104560, %v104561 (stack75)
        %v104563 = vxor.u32 %v104558, %v104562 (stack76)
        %v104566 = vadd.s32 %v104558, %v104563 (stack65)
        %v104570 = vadd.s32 %v104566, %v8 (stack65)
        %v104572 = vshll.u32 %v104563, 6 (stack73)
        %v104573 = vshrl.u32 %v104563, 26 (stack74)
        %v104574 = vor.u32 %v104572, %v104573 (stack75)
        %v104575 = vxor.u32 %v104566, %v104574 (stack76)
        %v104578 = vadd.s32 %v104575, %v10 (stack65)
        %v104582 = vadd.s32 %v104578, 5 (stack65)
        %v104584 = vxor.u32 %v104570, %v104582 (stack76)
        %v104585 = vand.u32.u8 %v104584, 255 (stack77)
        %v104586 = vand.u32 %v104585, 65535 (stack78)
        %v104587 = vshrl.u32 %v104586, 1 (stack79)
        %v104588 = vor.u32 %v104587, 16256 (stack75)
        %v104589 = vand.u32.u16 %v104588, 65535 (stack80)
        %v104590 = vunpack.i.l.bf16 %v104589 (stack81)
        %v104594 = vadd.f32 %v104590, -1.0 (stack82)
        %v104598 = vmul.f32 %v104594, 2.0 (stack83)
        %v104602 = vadd.f32 %v104598, -0.99609375 (stack82)
        %v104606 = vmax.f32 -0.99609375, %v104602 (stack84)
        %v104608 = vand.u32 2147483647, %v104606 (stack85)
        %vm104611 = vcmp.eq.f32.partialorder %v104608, 1.0 (stack86)
        %v104616 = vmul.f32 %v104606, inf (stack83)
        %v104618 = vxor.u32 %v104606, 2147483648 (stack87)
        %v104621 = vmul.f32 %v104606, %v104618 (stack83)
        %v104623 = vadd.f32 %v104621, 1.0 (stack88)
        %v104624 = vlog2.pop %v104623 (stack89)
        %v104625 = vmul.f32 %v104624, 0.6931472 (stack90)
        %v104626 = vmul.f32 -0.5, %v104621 (stack91)
        %v104627 = vadd.f32 %v104626, 1.0 (stack92)
        %v104628 = vmul.f32 %v104627, %v104621 (stack93)
        %v104629 = vand.u32 2147483647, %v104621 (stack94)
        %vm104630 = vcmp.lt.f32.partialorder %v104629, 0.0004427343 (stack95)
        %v104631 = vsel /*vm=*/%vm104630, /*on_true_vy=*/%v104628, /*on_false_vx=*/%v104625 (stack96)
        %v104632 = vxor.u32 %v104631, 2147483648 (stack87)
        %vm104635 = vcmp.lt.f32.partialorder %v104632, 5.0 (stack86)
        %v104640 = vsel /*vm=*/%vm104635, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v104644 = vsel /*vm=*/%vm104635, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v104648 = vsel /*vm=*/%vm104635, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v104652 = vsel /*vm=*/%vm104635, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v104656 = vsel /*vm=*/%vm104635, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v104660 = vsel /*vm=*/%vm104635, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v104664 = vsel /*vm=*/%vm104635, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v104668 = vsel /*vm=*/%vm104635, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v104672 = vsel /*vm=*/%vm104635, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v104676 = vadd.f32 %v104632, -2.5 (stack82)
        %v104678 = vrsqrt.pop %v104632 (stack97)
        %v104679 = vmul.f32 %v104632, %v104678 (stack98)
        %vm104680 = vcmp.eq.f32.partialorder %v104632, inf (stack99)
        %v104681 = vsel /*vm=*/%vm104680, /*on_true_vy=*/%v104632, /*on_false_vx=*/%v104679 (stack100)
        %vm104682 = vcmp.eq.f32.partialorder %v104632, 0.0 (stack101)
        %v104683 = vand.u32 %v104632, 2147483648 (stack102)
        %v104684 = vsel /*vm=*/%vm104682, /*on_true_vy=*/%v104683, /*on_false_vx=*/%v104681 (stack103)
        %v104687 = vadd.f32 %v104684, -3.0 (stack82)
        %v104691 = vsel /*vm=*/%vm104635, /*on_true_vy=*/%v104676, /*on_false_vx=*/%v104687 (stack72)
        %v104695 = vmul.f32 %v104672, %v104691 (stack83)
        %v104699 = vadd.f32 %v104668, %v104695 (stack82)
        %v104703 = vmul.f32 %v104699, %v104691 (stack83)
        %v104707 = vadd.f32 %v104664, %v104703 (stack82)
        %v104711 = vmul.f32 %v104707, %v104691 (stack83)
        %v104715 = vadd.f32 %v104660, %v104711 (stack82)
        %v104719 = vmul.f32 %v104715, %v104691 (stack83)
        %v104723 = vadd.f32 %v104656, %v104719 (stack82)
        %v104727 = vmul.f32 %v104723, %v104691 (stack83)
        %v104731 = vadd.f32 %v104652, %v104727 (stack82)
        %v104735 = vmul.f32 %v104731, %v104691 (stack83)
        %v104739 = vadd.f32 %v104648, %v104735 (stack82)
        %v104743 = vmul.f32 %v104739, %v104691 (stack83)
        %v104747 = vadd.f32 %v104644, %v104743 (stack82)
        %v104751 = vmul.f32 %v104747, %v104691 (stack83)
        %v104755 = vadd.f32 %v104640, %v104751 (stack82)
        %v104759 = vmul.f32 %v104755, %v104606 (stack83)
        %v104763 = vsel /*vm=*/%vm104611, /*on_true_vy=*/%v104616, /*on_false_vx=*/%v104759 (stack72)
        %v104767 = vmul.f32 %v104763, 1.4140625 (stack83)
        %s104769 = scalar_lea.vmem %s280, 1004 [#allocation0] (stack107)
        %v104770 = vpack.c.bf16 0.0, %v104767 (stack104)
        %104771 = vst [vmem:[%s104769] sm:$0xf] /*vst_source=*/%v104770 (stack105)
        %s104772 = sadd.s32 %s339, 224 (stack106)
        %s104773 = sshrl.u32 %s104772, 10 (stack49)
        %p104774 = scmp.lt.s32.totalorder 1, %s104773 (stack50)
        %s104775 = scalar_select /*predicate=*/%p104774, /*on_true=*/1, /*on_false=*/%s104773 (stack51)
        %s104776 = sand.u32 %s104772, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s104777 = sshrl.u32 %s104776, 7 (stack53)
        %s104778 = sand.u32 %s104776, 127 /* smod.u32 w/div 128 */ (stack54)
        %s104779 = smul.addr %s104775, 8 (stack55)
        %s104780 = scalar_lea.vmem %s3, %s104779 (stack56)
        %s104782 = scalar_lea.vmem %s104780, %s104777 (stack57)
        %v104783 = vld [vmem:[%s104782] ss:$0 sm:$0xff] (stack58)
        %s104784 = sand.u32 %s104778, 255 (stack59)
        %s104786 = sor.u32 256, %s104784 (stack60)
        %104787 = vbcast.lane.b32.xlu0 %v104783, %s104786 (stack61)
        %v104788 = vpop.permute.xlu0 %104787 (stack62)
        %s104789 = sadd.s32 %s347, 224 (stack106)
        %s104790 = sshrl.u32 %s104789, 10 (stack49)
        %p104791 = scmp.lt.s32.totalorder 1, %s104790 (stack50)
        %s104792 = scalar_select /*predicate=*/%p104791, /*on_true=*/1, /*on_false=*/%s104790 (stack51)
        %s104793 = sand.u32 %s104789, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s104794 = sshrl.u32 %s104793, 7 (stack53)
        %s104795 = sand.u32 %s104793, 127 /* smod.u32 w/div 128 */ (stack54)
        %s104796 = smul.addr %s104792, 8 (stack55)
        %s104797 = scalar_lea.vmem %s5, %s104796 (stack56)
        %s104799 = scalar_lea.vmem %s104797, %s104794 (stack57)
        %v104800 = vld [vmem:[%s104799] ss:$0 sm:$0xff] (stack58)
        %s104801 = sand.u32 %s104795, 255 (stack59)
        %s104803 = sor.u32 256, %s104801 (stack60)
        %104804 = vbcast.lane.b32.xlu0 %v104800, %s104803 (stack61)
        %v104805 = vpop.permute.xlu0 %104804 (stack62)
        %v104808 = vadd.s32 %v408, %v104805 (stack65)
        %s104810 = smul.u32 128, %s27 (stack66)
        %v104811 = vlaneseq (stack67)
        %v104812 = vand.u32 %v104811, 127 (stack68)
        %v104813 = vstv %s104810 (stack69)
        %v104814 = vadd.s32 %v104812, %v104813 (stack70)
        %v104818 = vadd.s32 %v104808, %v104814 (stack65)
        %vm104822 = vcmp.lt.u32.totalorder %v104818, %v104808 (stack71)
        %vm104827 = vcmp.lt.u32.totalorder %v104808, %v408 (stack71)
        %v104832 = vadd.s32 %v380, %v104788 (stack65)
        %v104836 = vadd.s32 %v104832, 1 (stack65)
        %v104840 = vsel /*vm=*/%vm104827, /*on_true_vy=*/%v104836, /*on_false_vx=*/%v104832 (stack72)
        %v104844 = vadd.s32 %v104840, 1 (stack65)
        %v104848 = vsel /*vm=*/%vm104822, /*on_true_vy=*/%v104844, /*on_false_vx=*/%v104840 (stack72)
        %v104853 = vadd.s32 %v104848, %v10 (stack65)
        %v104857 = vadd.s32 %v104818, %v9 (stack65)
        %v104861 = vadd.s32 %v104853, %v104857 (stack65)
        %v104863 = vshll.u32 %v104857, 13 (stack73)
        %v104864 = vshrl.u32 %v104857, 19 (stack74)
        %v104865 = vor.u32 %v104863, %v104864 (stack75)
        %v104866 = vxor.u32 %v104861, %v104865 (stack76)
        %v104869 = vadd.s32 %v104861, %v104866 (stack65)
        %v104871 = vshll.u32 %v104866, 15 (stack73)
        %v104872 = vshrl.u32 %v104866, 17 (stack74)
        %v104873 = vor.u32 %v104871, %v104872 (stack75)
        %v104874 = vxor.u32 %v104869, %v104873 (stack76)
        %v104877 = vadd.s32 %v104869, %v104874 (stack65)
        %v104879 = vshll.u32 %v104874, 26 (stack73)
        %v104880 = vshrl.u32 %v104874, 6 (stack74)
        %v104881 = vor.u32 %v104879, %v104880 (stack75)
        %v104882 = vxor.u32 %v104877, %v104881 (stack76)
        %v104885 = vadd.s32 %v104877, %v104882 (stack65)
        %v104889 = vadd.s32 %v104885, %v9 (stack65)
        %v104891 = vshll.u32 %v104882, 6 (stack73)
        %v104892 = vshrl.u32 %v104882, 26 (stack74)
        %v104893 = vor.u32 %v104891, %v104892 (stack75)
        %v104894 = vxor.u32 %v104885, %v104893 (stack76)
        %v104897 = vadd.s32 %v104894, %v8 (stack65)
        %v104901 = vadd.s32 %v104897, 1 (stack65)
        %v104905 = vadd.s32 %v104889, %v104901 (stack65)
        %v104907 = vshll.u32 %v104901, 17 (stack73)
        %v104908 = vshrl.u32 %v104901, 15 (stack74)
        %v104909 = vor.u32 %v104907, %v104908 (stack75)
        %v104910 = vxor.u32 %v104905, %v104909 (stack76)
        %v104913 = vadd.s32 %v104905, %v104910 (stack65)
        %v104915 = vshll.u32 %v104910, 29 (stack73)
        %v104916 = vshrl.u32 %v104910, 3 (stack74)
        %v104917 = vor.u32 %v104915, %v104916 (stack75)
        %v104918 = vxor.u32 %v104913, %v104917 (stack76)
        %v104921 = vadd.s32 %v104913, %v104918 (stack65)
        %v104923 = vshll.u32 %v104918, 16 (stack73)
        %v104924 = vshrl.u32 %v104918, 16 (stack74)
        %v104925 = vor.u32 %v104923, %v104924 (stack75)
        %v104926 = vxor.u32 %v104921, %v104925 (stack76)
        %v104929 = vadd.s32 %v104921, %v104926 (stack65)
        %v104933 = vadd.s32 %v104929, %v8 (stack65)
        %v104935 = vshll.u32 %v104926, 24 (stack73)
        %v104936 = vshrl.u32 %v104926, 8 (stack74)
        %v104937 = vor.u32 %v104935, %v104936 (stack75)
        %v104938 = vxor.u32 %v104929, %v104937 (stack76)
        %v104941 = vadd.s32 %v104938, %v10 (stack65)
        %v104945 = vadd.s32 %v104941, 2 (stack65)
        %v104949 = vadd.s32 %v104933, %v104945 (stack65)
        %v104951 = vshll.u32 %v104945, 13 (stack73)
        %v104952 = vshrl.u32 %v104945, 19 (stack74)
        %v104953 = vor.u32 %v104951, %v104952 (stack75)
        %v104954 = vxor.u32 %v104949, %v104953 (stack76)
        %v104957 = vadd.s32 %v104949, %v104954 (stack65)
        %v104959 = vshll.u32 %v104954, 15 (stack73)
        %v104960 = vshrl.u32 %v104954, 17 (stack74)
        %v104961 = vor.u32 %v104959, %v104960 (stack75)
        %v104962 = vxor.u32 %v104957, %v104961 (stack76)
        %v104965 = vadd.s32 %v104957, %v104962 (stack65)
        %v104967 = vshll.u32 %v104962, 26 (stack73)
        %v104968 = vshrl.u32 %v104962, 6 (stack74)
        %v104969 = vor.u32 %v104967, %v104968 (stack75)
        %v104970 = vxor.u32 %v104965, %v104969 (stack76)
        %v104973 = vadd.s32 %v104965, %v104970 (stack65)
        %v104977 = vadd.s32 %v104973, %v10 (stack65)
        %v104979 = vshll.u32 %v104970, 6 (stack73)
        %v104980 = vshrl.u32 %v104970, 26 (stack74)
        %v104981 = vor.u32 %v104979, %v104980 (stack75)
        %v104982 = vxor.u32 %v104973, %v104981 (stack76)
        %v104985 = vadd.s32 %v104982, %v9 (stack65)
        %v104989 = vadd.s32 %v104985, 3 (stack65)
        %v104993 = vadd.s32 %v104977, %v104989 (stack65)
        %v104995 = vshll.u32 %v104989, 17 (stack73)
        %v104996 = vshrl.u32 %v104989, 15 (stack74)
        %v104997 = vor.u32 %v104995, %v104996 (stack75)
        %v104998 = vxor.u32 %v104993, %v104997 (stack76)
        %v105001 = vadd.s32 %v104993, %v104998 (stack65)
        %v105003 = vshll.u32 %v104998, 29 (stack73)
        %v105004 = vshrl.u32 %v104998, 3 (stack74)
        %v105005 = vor.u32 %v105003, %v105004 (stack75)
        %v105006 = vxor.u32 %v105001, %v105005 (stack76)
        %v105009 = vadd.s32 %v105001, %v105006 (stack65)
        %v105011 = vshll.u32 %v105006, 16 (stack73)
        %v105012 = vshrl.u32 %v105006, 16 (stack74)
        %v105013 = vor.u32 %v105011, %v105012 (stack75)
        %v105014 = vxor.u32 %v105009, %v105013 (stack76)
        %v105017 = vadd.s32 %v105009, %v105014 (stack65)
        %v105021 = vadd.s32 %v105017, %v9 (stack65)
        %v105023 = vshll.u32 %v105014, 24 (stack73)
        %v105024 = vshrl.u32 %v105014, 8 (stack74)
        %v105025 = vor.u32 %v105023, %v105024 (stack75)
        %v105026 = vxor.u32 %v105017, %v105025 (stack76)
        %v105029 = vadd.s32 %v105026, %v8 (stack65)
        %v105033 = vadd.s32 %v105029, 4 (stack65)
        %v105037 = vadd.s32 %v105021, %v105033 (stack65)
        %v105039 = vshll.u32 %v105033, 13 (stack73)
        %v105040 = vshrl.u32 %v105033, 19 (stack74)
        %v105041 = vor.u32 %v105039, %v105040 (stack75)
        %v105042 = vxor.u32 %v105037, %v105041 (stack76)
        %v105045 = vadd.s32 %v105037, %v105042 (stack65)
        %v105047 = vshll.u32 %v105042, 15 (stack73)
        %v105048 = vshrl.u32 %v105042, 17 (stack74)
        %v105049 = vor.u32 %v105047, %v105048 (stack75)
        %v105050 = vxor.u32 %v105045, %v105049 (stack76)
        %v105053 = vadd.s32 %v105045, %v105050 (stack65)
        %v105055 = vshll.u32 %v105050, 26 (stack73)
        %v105056 = vshrl.u32 %v105050, 6 (stack74)
        %v105057 = vor.u32 %v105055, %v105056 (stack75)
        %v105058 = vxor.u32 %v105053, %v105057 (stack76)
        %v105061 = vadd.s32 %v105053, %v105058 (stack65)
        %v105065 = vadd.s32 %v105061, %v8 (stack65)
        %v105067 = vshll.u32 %v105058, 6 (stack73)
        %v105068 = vshrl.u32 %v105058, 26 (stack74)
        %v105069 = vor.u32 %v105067, %v105068 (stack75)
        %v105070 = vxor.u32 %v105061, %v105069 (stack76)
        %v105073 = vadd.s32 %v105070, %v10 (stack65)
        %v105077 = vadd.s32 %v105073, 5 (stack65)
        %v105079 = vxor.u32 %v105065, %v105077 (stack76)
        %v105080 = vand.u32.u8 %v105079, 255 (stack77)
        %v105081 = vand.u32 %v105080, 65535 (stack78)
        %v105082 = vshrl.u32 %v105081, 1 (stack79)
        %v105083 = vor.u32 %v105082, 16256 (stack75)
        %v105084 = vand.u32.u16 %v105083, 65535 (stack80)
        %v105085 = vunpack.i.l.bf16 %v105084 (stack81)
        %v105089 = vadd.f32 %v105085, -1.0 (stack82)
        %v105093 = vmul.f32 %v105089, 2.0 (stack83)
        %v105097 = vadd.f32 %v105093, -0.99609375 (stack82)
        %v105101 = vmax.f32 -0.99609375, %v105097 (stack84)
        %v105103 = vand.u32 2147483647, %v105101 (stack85)
        %vm105106 = vcmp.eq.f32.partialorder %v105103, 1.0 (stack86)
        %v105111 = vmul.f32 %v105101, inf (stack83)
        %v105113 = vxor.u32 %v105101, 2147483648 (stack87)
        %v105116 = vmul.f32 %v105101, %v105113 (stack83)
        %v105118 = vadd.f32 %v105116, 1.0 (stack88)
        %v105119 = vlog2.pop %v105118 (stack89)
        %v105120 = vmul.f32 %v105119, 0.6931472 (stack90)
        %v105121 = vmul.f32 -0.5, %v105116 (stack91)
        %v105122 = vadd.f32 %v105121, 1.0 (stack92)
        %v105123 = vmul.f32 %v105122, %v105116 (stack93)
        %v105124 = vand.u32 2147483647, %v105116 (stack94)
        %vm105125 = vcmp.lt.f32.partialorder %v105124, 0.0004427343 (stack95)
        %v105126 = vsel /*vm=*/%vm105125, /*on_true_vy=*/%v105123, /*on_false_vx=*/%v105120 (stack96)
        %v105127 = vxor.u32 %v105126, 2147483648 (stack87)
        %vm105130 = vcmp.lt.f32.partialorder %v105127, 5.0 (stack86)
        %v105135 = vsel /*vm=*/%vm105130, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v105139 = vsel /*vm=*/%vm105130, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v105143 = vsel /*vm=*/%vm105130, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v105147 = vsel /*vm=*/%vm105130, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v105151 = vsel /*vm=*/%vm105130, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v105155 = vsel /*vm=*/%vm105130, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v105159 = vsel /*vm=*/%vm105130, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v105163 = vsel /*vm=*/%vm105130, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v105167 = vsel /*vm=*/%vm105130, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v105171 = vadd.f32 %v105127, -2.5 (stack82)
        %v105173 = vrsqrt.pop %v105127 (stack97)
        %v105174 = vmul.f32 %v105127, %v105173 (stack98)
        %vm105175 = vcmp.eq.f32.partialorder %v105127, inf (stack99)
        %v105176 = vsel /*vm=*/%vm105175, /*on_true_vy=*/%v105127, /*on_false_vx=*/%v105174 (stack100)
        %vm105177 = vcmp.eq.f32.partialorder %v105127, 0.0 (stack101)
        %v105178 = vand.u32 %v105127, 2147483648 (stack102)
        %v105179 = vsel /*vm=*/%vm105177, /*on_true_vy=*/%v105178, /*on_false_vx=*/%v105176 (stack103)
        %v105182 = vadd.f32 %v105179, -3.0 (stack82)
        %v105186 = vsel /*vm=*/%vm105130, /*on_true_vy=*/%v105171, /*on_false_vx=*/%v105182 (stack72)
        %v105190 = vmul.f32 %v105167, %v105186 (stack83)
        %v105194 = vadd.f32 %v105163, %v105190 (stack82)
        %v105198 = vmul.f32 %v105194, %v105186 (stack83)
        %v105202 = vadd.f32 %v105159, %v105198 (stack82)
        %v105206 = vmul.f32 %v105202, %v105186 (stack83)
        %v105210 = vadd.f32 %v105155, %v105206 (stack82)
        %v105214 = vmul.f32 %v105210, %v105186 (stack83)
        %v105218 = vadd.f32 %v105151, %v105214 (stack82)
        %v105222 = vmul.f32 %v105218, %v105186 (stack83)
        %v105226 = vadd.f32 %v105147, %v105222 (stack82)
        %v105230 = vmul.f32 %v105226, %v105186 (stack83)
        %v105234 = vadd.f32 %v105143, %v105230 (stack82)
        %v105238 = vmul.f32 %v105234, %v105186 (stack83)
        %v105242 = vadd.f32 %v105139, %v105238 (stack82)
        %v105246 = vmul.f32 %v105242, %v105186 (stack83)
        %v105250 = vadd.f32 %v105135, %v105246 (stack82)
        %v105254 = vmul.f32 %v105250, %v105101 (stack83)
        %v105258 = vsel /*vm=*/%vm105106, /*on_true_vy=*/%v105111, /*on_false_vx=*/%v105254 (stack72)
        %v105262 = vmul.f32 %v105258, 1.4140625 (stack83)
        %s105264 = scalar_lea.vmem %s280, 112 [#allocation0] (stack107)
        %v105265 = vpack.c.bf16 0.0, %v105262 (stack104)
        %105266 = vst [vmem:[%s105264] sm:$0xf] /*vst_source=*/%v105265 (stack105)
        %v105269 = vadd.s32 %v894, %v104805 (stack65)
        %s105271 = smul.u32 128, %s27 (stack66)
        %v105272 = vlaneseq (stack67)
        %v105273 = vand.u32 %v105272, 127 (stack68)
        %v105274 = vstv %s105271 (stack69)
        %v105275 = vadd.s32 %v105273, %v105274 (stack70)
        %v105279 = vadd.s32 %v105269, %v105275 (stack65)
        %vm105283 = vcmp.lt.u32.totalorder %v105279, %v105269 (stack71)
        %vm105288 = vcmp.lt.u32.totalorder %v105269, %v894 (stack71)
        %v105293 = vadd.s32 %v881, %v104788 (stack65)
        %v105297 = vadd.s32 %v105293, 1 (stack65)
        %v105301 = vsel /*vm=*/%vm105288, /*on_true_vy=*/%v105297, /*on_false_vx=*/%v105293 (stack72)
        %v105305 = vadd.s32 %v105301, 1 (stack65)
        %v105309 = vsel /*vm=*/%vm105283, /*on_true_vy=*/%v105305, /*on_false_vx=*/%v105301 (stack72)
        %v105314 = vadd.s32 %v105309, %v10 (stack65)
        %v105318 = vadd.s32 %v105279, %v9 (stack65)
        %v105322 = vadd.s32 %v105314, %v105318 (stack65)
        %v105324 = vshll.u32 %v105318, 13 (stack73)
        %v105325 = vshrl.u32 %v105318, 19 (stack74)
        %v105326 = vor.u32 %v105324, %v105325 (stack75)
        %v105327 = vxor.u32 %v105322, %v105326 (stack76)
        %v105330 = vadd.s32 %v105322, %v105327 (stack65)
        %v105332 = vshll.u32 %v105327, 15 (stack73)
        %v105333 = vshrl.u32 %v105327, 17 (stack74)
        %v105334 = vor.u32 %v105332, %v105333 (stack75)
        %v105335 = vxor.u32 %v105330, %v105334 (stack76)
        %v105338 = vadd.s32 %v105330, %v105335 (stack65)
        %v105340 = vshll.u32 %v105335, 26 (stack73)
        %v105341 = vshrl.u32 %v105335, 6 (stack74)
        %v105342 = vor.u32 %v105340, %v105341 (stack75)
        %v105343 = vxor.u32 %v105338, %v105342 (stack76)
        %v105346 = vadd.s32 %v105338, %v105343 (stack65)
        %v105350 = vadd.s32 %v105346, %v9 (stack65)
        %v105352 = vshll.u32 %v105343, 6 (stack73)
        %v105353 = vshrl.u32 %v105343, 26 (stack74)
        %v105354 = vor.u32 %v105352, %v105353 (stack75)
        %v105355 = vxor.u32 %v105346, %v105354 (stack76)
        %v105358 = vadd.s32 %v105355, %v8 (stack65)
        %v105362 = vadd.s32 %v105358, 1 (stack65)
        %v105366 = vadd.s32 %v105350, %v105362 (stack65)
        %v105368 = vshll.u32 %v105362, 17 (stack73)
        %v105369 = vshrl.u32 %v105362, 15 (stack74)
        %v105370 = vor.u32 %v105368, %v105369 (stack75)
        %v105371 = vxor.u32 %v105366, %v105370 (stack76)
        %v105374 = vadd.s32 %v105366, %v105371 (stack65)
        %v105376 = vshll.u32 %v105371, 29 (stack73)
        %v105377 = vshrl.u32 %v105371, 3 (stack74)
        %v105378 = vor.u32 %v105376, %v105377 (stack75)
        %v105379 = vxor.u32 %v105374, %v105378 (stack76)
        %v105382 = vadd.s32 %v105374, %v105379 (stack65)
        %v105384 = vshll.u32 %v105379, 16 (stack73)
        %v105385 = vshrl.u32 %v105379, 16 (stack74)
        %v105386 = vor.u32 %v105384, %v105385 (stack75)
        %v105387 = vxor.u32 %v105382, %v105386 (stack76)
        %v105390 = vadd.s32 %v105382, %v105387 (stack65)
        %v105394 = vadd.s32 %v105390, %v8 (stack65)
        %v105396 = vshll.u32 %v105387, 24 (stack73)
        %v105397 = vshrl.u32 %v105387, 8 (stack74)
        %v105398 = vor.u32 %v105396, %v105397 (stack75)
        %v105399 = vxor.u32 %v105390, %v105398 (stack76)
        %v105402 = vadd.s32 %v105399, %v10 (stack65)
        %v105406 = vadd.s32 %v105402, 2 (stack65)
        %v105410 = vadd.s32 %v105394, %v105406 (stack65)
        %v105412 = vshll.u32 %v105406, 13 (stack73)
        %v105413 = vshrl.u32 %v105406, 19 (stack74)
        %v105414 = vor.u32 %v105412, %v105413 (stack75)
        %v105415 = vxor.u32 %v105410, %v105414 (stack76)
        %v105418 = vadd.s32 %v105410, %v105415 (stack65)
        %v105420 = vshll.u32 %v105415, 15 (stack73)
        %v105421 = vshrl.u32 %v105415, 17 (stack74)
        %v105422 = vor.u32 %v105420, %v105421 (stack75)
        %v105423 = vxor.u32 %v105418, %v105422 (stack76)
        %v105426 = vadd.s32 %v105418, %v105423 (stack65)
        %v105428 = vshll.u32 %v105423, 26 (stack73)
        %v105429 = vshrl.u32 %v105423, 6 (stack74)
        %v105430 = vor.u32 %v105428, %v105429 (stack75)
        %v105431 = vxor.u32 %v105426, %v105430 (stack76)
        %v105434 = vadd.s32 %v105426, %v105431 (stack65)
        %v105438 = vadd.s32 %v105434, %v10 (stack65)
        %v105440 = vshll.u32 %v105431, 6 (stack73)
        %v105441 = vshrl.u32 %v105431, 26 (stack74)
        %v105442 = vor.u32 %v105440, %v105441 (stack75)
        %v105443 = vxor.u32 %v105434, %v105442 (stack76)
        %v105446 = vadd.s32 %v105443, %v9 (stack65)
        %v105450 = vadd.s32 %v105446, 3 (stack65)
        %v105454 = vadd.s32 %v105438, %v105450 (stack65)
        %v105456 = vshll.u32 %v105450, 17 (stack73)
        %v105457 = vshrl.u32 %v105450, 15 (stack74)
        %v105458 = vor.u32 %v105456, %v105457 (stack75)
        %v105459 = vxor.u32 %v105454, %v105458 (stack76)
        %v105462 = vadd.s32 %v105454, %v105459 (stack65)
        %v105464 = vshll.u32 %v105459, 29 (stack73)
        %v105465 = vshrl.u32 %v105459, 3 (stack74)
        %v105466 = vor.u32 %v105464, %v105465 (stack75)
        %v105467 = vxor.u32 %v105462, %v105466 (stack76)
        %v105470 = vadd.s32 %v105462, %v105467 (stack65)
        %v105472 = vshll.u32 %v105467, 16 (stack73)
        %v105473 = vshrl.u32 %v105467, 16 (stack74)
        %v105474 = vor.u32 %v105472, %v105473 (stack75)
        %v105475 = vxor.u32 %v105470, %v105474 (stack76)
        %v105478 = vadd.s32 %v105470, %v105475 (stack65)
        %v105482 = vadd.s32 %v105478, %v9 (stack65)
        %v105484 = vshll.u32 %v105475, 24 (stack73)
        %v105485 = vshrl.u32 %v105475, 8 (stack74)
        %v105486 = vor.u32 %v105484, %v105485 (stack75)
        %v105487 = vxor.u32 %v105478, %v105486 (stack76)
        %v105490 = vadd.s32 %v105487, %v8 (stack65)
        %v105494 = vadd.s32 %v105490, 4 (stack65)
        %v105498 = vadd.s32 %v105482, %v105494 (stack65)
        %v105500 = vshll.u32 %v105494, 13 (stack73)
        %v105501 = vshrl.u32 %v105494, 19 (stack74)
        %v105502 = vor.u32 %v105500, %v105501 (stack75)
        %v105503 = vxor.u32 %v105498, %v105502 (stack76)
        %v105506 = vadd.s32 %v105498, %v105503 (stack65)
        %v105508 = vshll.u32 %v105503, 15 (stack73)
        %v105509 = vshrl.u32 %v105503, 17 (stack74)
        %v105510 = vor.u32 %v105508, %v105509 (stack75)
        %v105511 = vxor.u32 %v105506, %v105510 (stack76)
        %v105514 = vadd.s32 %v105506, %v105511 (stack65)
        %v105516 = vshll.u32 %v105511, 26 (stack73)
        %v105517 = vshrl.u32 %v105511, 6 (stack74)
        %v105518 = vor.u32 %v105516, %v105517 (stack75)
        %v105519 = vxor.u32 %v105514, %v105518 (stack76)
        %v105522 = vadd.s32 %v105514, %v105519 (stack65)
        %v105526 = vadd.s32 %v105522, %v8 (stack65)
        %v105528 = vshll.u32 %v105519, 6 (stack73)
        %v105529 = vshrl.u32 %v105519, 26 (stack74)
        %v105530 = vor.u32 %v105528, %v105529 (stack75)
        %v105531 = vxor.u32 %v105522, %v105530 (stack76)
        %v105534 = vadd.s32 %v105531, %v10 (stack65)
        %v105538 = vadd.s32 %v105534, 5 (stack65)
        %v105540 = vxor.u32 %v105526, %v105538 (stack76)
        %v105541 = vand.u32.u8 %v105540, 255 (stack77)
        %v105542 = vand.u32 %v105541, 65535 (stack78)
        %v105543 = vshrl.u32 %v105542, 1 (stack79)
        %v105544 = vor.u32 %v105543, 16256 (stack75)
        %v105545 = vand.u32.u16 %v105544, 65535 (stack80)
        %v105546 = vunpack.i.l.bf16 %v105545 (stack81)
        %v105550 = vadd.f32 %v105546, -1.0 (stack82)
        %v105554 = vmul.f32 %v105550, 2.0 (stack83)
        %v105558 = vadd.f32 %v105554, -0.99609375 (stack82)
        %v105562 = vmax.f32 -0.99609375, %v105558 (stack84)
        %v105564 = vand.u32 2147483647, %v105562 (stack85)
        %vm105567 = vcmp.eq.f32.partialorder %v105564, 1.0 (stack86)
        %v105572 = vmul.f32 %v105562, inf (stack83)
        %v105574 = vxor.u32 %v105562, 2147483648 (stack87)
        %v105577 = vmul.f32 %v105562, %v105574 (stack83)
        %v105579 = vadd.f32 %v105577, 1.0 (stack88)
        %v105580 = vlog2.pop %v105579 (stack89)
        %v105581 = vmul.f32 %v105580, 0.6931472 (stack90)
        %v105582 = vmul.f32 -0.5, %v105577 (stack91)
        %v105583 = vadd.f32 %v105582, 1.0 (stack92)
        %v105584 = vmul.f32 %v105583, %v105577 (stack93)
        %v105585 = vand.u32 2147483647, %v105577 (stack94)
        %vm105586 = vcmp.lt.f32.partialorder %v105585, 0.0004427343 (stack95)
        %v105587 = vsel /*vm=*/%vm105586, /*on_true_vy=*/%v105584, /*on_false_vx=*/%v105581 (stack96)
        %v105588 = vxor.u32 %v105587, 2147483648 (stack87)
        %vm105591 = vcmp.lt.f32.partialorder %v105588, 5.0 (stack86)
        %v105596 = vsel /*vm=*/%vm105591, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v105600 = vsel /*vm=*/%vm105591, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v105604 = vsel /*vm=*/%vm105591, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v105608 = vsel /*vm=*/%vm105591, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v105612 = vsel /*vm=*/%vm105591, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v105616 = vsel /*vm=*/%vm105591, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v105620 = vsel /*vm=*/%vm105591, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v105624 = vsel /*vm=*/%vm105591, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v105628 = vsel /*vm=*/%vm105591, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v105632 = vadd.f32 %v105588, -2.5 (stack82)
        %v105634 = vrsqrt.pop %v105588 (stack97)
        %v105635 = vmul.f32 %v105588, %v105634 (stack98)
        %vm105636 = vcmp.eq.f32.partialorder %v105588, inf (stack99)
        %v105637 = vsel /*vm=*/%vm105636, /*on_true_vy=*/%v105588, /*on_false_vx=*/%v105635 (stack100)
        %vm105638 = vcmp.eq.f32.partialorder %v105588, 0.0 (stack101)
        %v105639 = vand.u32 %v105588, 2147483648 (stack102)
        %v105640 = vsel /*vm=*/%vm105638, /*on_true_vy=*/%v105639, /*on_false_vx=*/%v105637 (stack103)
        %v105643 = vadd.f32 %v105640, -3.0 (stack82)
        %v105647 = vsel /*vm=*/%vm105591, /*on_true_vy=*/%v105632, /*on_false_vx=*/%v105643 (stack72)
        %v105651 = vmul.f32 %v105628, %v105647 (stack83)
        %v105655 = vadd.f32 %v105624, %v105651 (stack82)
        %v105659 = vmul.f32 %v105655, %v105647 (stack83)
        %v105663 = vadd.f32 %v105620, %v105659 (stack82)
        %v105667 = vmul.f32 %v105663, %v105647 (stack83)
        %v105671 = vadd.f32 %v105616, %v105667 (stack82)
        %v105675 = vmul.f32 %v105671, %v105647 (stack83)
        %v105679 = vadd.f32 %v105612, %v105675 (stack82)
        %v105683 = vmul.f32 %v105679, %v105647 (stack83)
        %v105687 = vadd.f32 %v105608, %v105683 (stack82)
        %v105691 = vmul.f32 %v105687, %v105647 (stack83)
        %v105695 = vadd.f32 %v105604, %v105691 (stack82)
        %v105699 = vmul.f32 %v105695, %v105647 (stack83)
        %v105703 = vadd.f32 %v105600, %v105699 (stack82)
        %v105707 = vmul.f32 %v105703, %v105647 (stack83)
        %v105711 = vadd.f32 %v105596, %v105707 (stack82)
        %v105715 = vmul.f32 %v105711, %v105562 (stack83)
        %v105719 = vsel /*vm=*/%vm105567, /*on_true_vy=*/%v105572, /*on_false_vx=*/%v105715 (stack72)
        %v105723 = vmul.f32 %v105719, 1.4140625 (stack83)
        %s105725 = scalar_lea.vmem %s280, 240 [#allocation0] (stack107)
        %v105726 = vpack.c.bf16 0.0, %v105723 (stack104)
        %105727 = vst [vmem:[%s105725] sm:$0xf] /*vst_source=*/%v105726 (stack105)
        %v105730 = vadd.s32 %v1381, %v104805 (stack65)
        %s105732 = smul.u32 128, %s27 (stack66)
        %v105733 = vlaneseq (stack67)
        %v105734 = vand.u32 %v105733, 127 (stack68)
        %v105735 = vstv %s105732 (stack69)
        %v105736 = vadd.s32 %v105734, %v105735 (stack70)
        %v105740 = vadd.s32 %v105730, %v105736 (stack65)
        %vm105744 = vcmp.lt.u32.totalorder %v105740, %v105730 (stack71)
        %vm105749 = vcmp.lt.u32.totalorder %v105730, %v1381 (stack71)
        %v105754 = vadd.s32 %v1368, %v104788 (stack65)
        %v105758 = vadd.s32 %v105754, 1 (stack65)
        %v105762 = vsel /*vm=*/%vm105749, /*on_true_vy=*/%v105758, /*on_false_vx=*/%v105754 (stack72)
        %v105766 = vadd.s32 %v105762, 1 (stack65)
        %v105770 = vsel /*vm=*/%vm105744, /*on_true_vy=*/%v105766, /*on_false_vx=*/%v105762 (stack72)
        %v105775 = vadd.s32 %v105770, %v10 (stack65)
        %v105779 = vadd.s32 %v105740, %v9 (stack65)
        %v105783 = vadd.s32 %v105775, %v105779 (stack65)
        %v105785 = vshll.u32 %v105779, 13 (stack73)
        %v105786 = vshrl.u32 %v105779, 19 (stack74)
        %v105787 = vor.u32 %v105785, %v105786 (stack75)
        %v105788 = vxor.u32 %v105783, %v105787 (stack76)
        %v105791 = vadd.s32 %v105783, %v105788 (stack65)
        %v105793 = vshll.u32 %v105788, 15 (stack73)
        %v105794 = vshrl.u32 %v105788, 17 (stack74)
        %v105795 = vor.u32 %v105793, %v105794 (stack75)
        %v105796 = vxor.u32 %v105791, %v105795 (stack76)
        %v105799 = vadd.s32 %v105791, %v105796 (stack65)
        %v105801 = vshll.u32 %v105796, 26 (stack73)
        %v105802 = vshrl.u32 %v105796, 6 (stack74)
        %v105803 = vor.u32 %v105801, %v105802 (stack75)
        %v105804 = vxor.u32 %v105799, %v105803 (stack76)
        %v105807 = vadd.s32 %v105799, %v105804 (stack65)
        %v105811 = vadd.s32 %v105807, %v9 (stack65)
        %v105813 = vshll.u32 %v105804, 6 (stack73)
        %v105814 = vshrl.u32 %v105804, 26 (stack74)
        %v105815 = vor.u32 %v105813, %v105814 (stack75)
        %v105816 = vxor.u32 %v105807, %v105815 (stack76)
        %v105819 = vadd.s32 %v105816, %v8 (stack65)
        %v105823 = vadd.s32 %v105819, 1 (stack65)
        %v105827 = vadd.s32 %v105811, %v105823 (stack65)
        %v105829 = vshll.u32 %v105823, 17 (stack73)
        %v105830 = vshrl.u32 %v105823, 15 (stack74)
        %v105831 = vor.u32 %v105829, %v105830 (stack75)
        %v105832 = vxor.u32 %v105827, %v105831 (stack76)
        %v105835 = vadd.s32 %v105827, %v105832 (stack65)
        %v105837 = vshll.u32 %v105832, 29 (stack73)
        %v105838 = vshrl.u32 %v105832, 3 (stack74)
        %v105839 = vor.u32 %v105837, %v105838 (stack75)
        %v105840 = vxor.u32 %v105835, %v105839 (stack76)
        %v105843 = vadd.s32 %v105835, %v105840 (stack65)
        %v105845 = vshll.u32 %v105840, 16 (stack73)
        %v105846 = vshrl.u32 %v105840, 16 (stack74)
        %v105847 = vor.u32 %v105845, %v105846 (stack75)
        %v105848 = vxor.u32 %v105843, %v105847 (stack76)
        %v105851 = vadd.s32 %v105843, %v105848 (stack65)
        %v105855 = vadd.s32 %v105851, %v8 (stack65)
        %v105857 = vshll.u32 %v105848, 24 (stack73)
        %v105858 = vshrl.u32 %v105848, 8 (stack74)
        %v105859 = vor.u32 %v105857, %v105858 (stack75)
        %v105860 = vxor.u32 %v105851, %v105859 (stack76)
        %v105863 = vadd.s32 %v105860, %v10 (stack65)
        %v105867 = vadd.s32 %v105863, 2 (stack65)
        %v105871 = vadd.s32 %v105855, %v105867 (stack65)
        %v105873 = vshll.u32 %v105867, 13 (stack73)
        %v105874 = vshrl.u32 %v105867, 19 (stack74)
        %v105875 = vor.u32 %v105873, %v105874 (stack75)
        %v105876 = vxor.u32 %v105871, %v105875 (stack76)
        %v105879 = vadd.s32 %v105871, %v105876 (stack65)
        %v105881 = vshll.u32 %v105876, 15 (stack73)
        %v105882 = vshrl.u32 %v105876, 17 (stack74)
        %v105883 = vor.u32 %v105881, %v105882 (stack75)
        %v105884 = vxor.u32 %v105879, %v105883 (stack76)
        %v105887 = vadd.s32 %v105879, %v105884 (stack65)
        %v105889 = vshll.u32 %v105884, 26 (stack73)
        %v105890 = vshrl.u32 %v105884, 6 (stack74)
        %v105891 = vor.u32 %v105889, %v105890 (stack75)
        %v105892 = vxor.u32 %v105887, %v105891 (stack76)
        %v105895 = vadd.s32 %v105887, %v105892 (stack65)
        %v105899 = vadd.s32 %v105895, %v10 (stack65)
        %v105901 = vshll.u32 %v105892, 6 (stack73)
        %v105902 = vshrl.u32 %v105892, 26 (stack74)
        %v105903 = vor.u32 %v105901, %v105902 (stack75)
        %v105904 = vxor.u32 %v105895, %v105903 (stack76)
        %v105907 = vadd.s32 %v105904, %v9 (stack65)
        %v105911 = vadd.s32 %v105907, 3 (stack65)
        %v105915 = vadd.s32 %v105899, %v105911 (stack65)
        %v105917 = vshll.u32 %v105911, 17 (stack73)
        %v105918 = vshrl.u32 %v105911, 15 (stack74)
        %v105919 = vor.u32 %v105917, %v105918 (stack75)
        %v105920 = vxor.u32 %v105915, %v105919 (stack76)
        %v105923 = vadd.s32 %v105915, %v105920 (stack65)
        %v105925 = vshll.u32 %v105920, 29 (stack73)
        %v105926 = vshrl.u32 %v105920, 3 (stack74)
        %v105927 = vor.u32 %v105925, %v105926 (stack75)
        %v105928 = vxor.u32 %v105923, %v105927 (stack76)
        %v105931 = vadd.s32 %v105923, %v105928 (stack65)
        %v105933 = vshll.u32 %v105928, 16 (stack73)
        %v105934 = vshrl.u32 %v105928, 16 (stack74)
        %v105935 = vor.u32 %v105933, %v105934 (stack75)
        %v105936 = vxor.u32 %v105931, %v105935 (stack76)
        %v105939 = vadd.s32 %v105931, %v105936 (stack65)
        %v105943 = vadd.s32 %v105939, %v9 (stack65)
        %v105945 = vshll.u32 %v105936, 24 (stack73)
        %v105946 = vshrl.u32 %v105936, 8 (stack74)
        %v105947 = vor.u32 %v105945, %v105946 (stack75)
        %v105948 = vxor.u32 %v105939, %v105947 (stack76)
        %v105951 = vadd.s32 %v105948, %v8 (stack65)
        %v105955 = vadd.s32 %v105951, 4 (stack65)
        %v105959 = vadd.s32 %v105943, %v105955 (stack65)
        %v105961 = vshll.u32 %v105955, 13 (stack73)
        %v105962 = vshrl.u32 %v105955, 19 (stack74)
        %v105963 = vor.u32 %v105961, %v105962 (stack75)
        %v105964 = vxor.u32 %v105959, %v105963 (stack76)
        %v105967 = vadd.s32 %v105959, %v105964 (stack65)
        %v105969 = vshll.u32 %v105964, 15 (stack73)
        %v105970 = vshrl.u32 %v105964, 17 (stack74)
        %v105971 = vor.u32 %v105969, %v105970 (stack75)
        %v105972 = vxor.u32 %v105967, %v105971 (stack76)
        %v105975 = vadd.s32 %v105967, %v105972 (stack65)
        %v105977 = vshll.u32 %v105972, 26 (stack73)
        %v105978 = vshrl.u32 %v105972, 6 (stack74)
        %v105979 = vor.u32 %v105977, %v105978 (stack75)
        %v105980 = vxor.u32 %v105975, %v105979 (stack76)
        %v105983 = vadd.s32 %v105975, %v105980 (stack65)
        %v105987 = vadd.s32 %v105983, %v8 (stack65)
        %v105989 = vshll.u32 %v105980, 6 (stack73)
        %v105990 = vshrl.u32 %v105980, 26 (stack74)
        %v105991 = vor.u32 %v105989, %v105990 (stack75)
        %v105992 = vxor.u32 %v105983, %v105991 (stack76)
        %v105995 = vadd.s32 %v105992, %v10 (stack65)
        %v105999 = vadd.s32 %v105995, 5 (stack65)
        %v106001 = vxor.u32 %v105987, %v105999 (stack76)
        %v106002 = vand.u32.u8 %v106001, 255 (stack77)
        %v106003 = vand.u32 %v106002, 65535 (stack78)
        %v106004 = vshrl.u32 %v106003, 1 (stack79)
        %v106005 = vor.u32 %v106004, 16256 (stack75)
        %v106006 = vand.u32.u16 %v106005, 65535 (stack80)
        %v106007 = vunpack.i.l.bf16 %v106006 (stack81)
        %v106011 = vadd.f32 %v106007, -1.0 (stack82)
        %v106015 = vmul.f32 %v106011, 2.0 (stack83)
        %v106019 = vadd.f32 %v106015, -0.99609375 (stack82)
        %v106023 = vmax.f32 -0.99609375, %v106019 (stack84)
        %v106025 = vand.u32 2147483647, %v106023 (stack85)
        %vm106028 = vcmp.eq.f32.partialorder %v106025, 1.0 (stack86)
        %v106033 = vmul.f32 %v106023, inf (stack83)
        %v106035 = vxor.u32 %v106023, 2147483648 (stack87)
        %v106038 = vmul.f32 %v106023, %v106035 (stack83)
        %v106040 = vadd.f32 %v106038, 1.0 (stack88)
        %v106041 = vlog2.pop %v106040 (stack89)
        %v106042 = vmul.f32 %v106041, 0.6931472 (stack90)
        %v106043 = vmul.f32 -0.5, %v106038 (stack91)
        %v106044 = vadd.f32 %v106043, 1.0 (stack92)
        %v106045 = vmul.f32 %v106044, %v106038 (stack93)
        %v106046 = vand.u32 2147483647, %v106038 (stack94)
        %vm106047 = vcmp.lt.f32.partialorder %v106046, 0.0004427343 (stack95)
        %v106048 = vsel /*vm=*/%vm106047, /*on_true_vy=*/%v106045, /*on_false_vx=*/%v106042 (stack96)
        %v106049 = vxor.u32 %v106048, 2147483648 (stack87)
        %vm106052 = vcmp.lt.f32.partialorder %v106049, 5.0 (stack86)
        %v106057 = vsel /*vm=*/%vm106052, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v106061 = vsel /*vm=*/%vm106052, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v106065 = vsel /*vm=*/%vm106052, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v106069 = vsel /*vm=*/%vm106052, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v106073 = vsel /*vm=*/%vm106052, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v106077 = vsel /*vm=*/%vm106052, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v106081 = vsel /*vm=*/%vm106052, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v106085 = vsel /*vm=*/%vm106052, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v106089 = vsel /*vm=*/%vm106052, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v106093 = vadd.f32 %v106049, -2.5 (stack82)
        %v106095 = vrsqrt.pop %v106049 (stack97)
        %v106096 = vmul.f32 %v106049, %v106095 (stack98)
        %vm106097 = vcmp.eq.f32.partialorder %v106049, inf (stack99)
        %v106098 = vsel /*vm=*/%vm106097, /*on_true_vy=*/%v106049, /*on_false_vx=*/%v106096 (stack100)
        %vm106099 = vcmp.eq.f32.partialorder %v106049, 0.0 (stack101)
        %v106100 = vand.u32 %v106049, 2147483648 (stack102)
        %v106101 = vsel /*vm=*/%vm106099, /*on_true_vy=*/%v106100, /*on_false_vx=*/%v106098 (stack103)
        %v106104 = vadd.f32 %v106101, -3.0 (stack82)
        %v106108 = vsel /*vm=*/%vm106052, /*on_true_vy=*/%v106093, /*on_false_vx=*/%v106104 (stack72)
        %v106112 = vmul.f32 %v106089, %v106108 (stack83)
        %v106116 = vadd.f32 %v106085, %v106112 (stack82)
        %v106120 = vmul.f32 %v106116, %v106108 (stack83)
        %v106124 = vadd.f32 %v106081, %v106120 (stack82)
        %v106128 = vmul.f32 %v106124, %v106108 (stack83)
        %v106132 = vadd.f32 %v106077, %v106128 (stack82)
        %v106136 = vmul.f32 %v106132, %v106108 (stack83)
        %v106140 = vadd.f32 %v106073, %v106136 (stack82)
        %v106144 = vmul.f32 %v106140, %v106108 (stack83)
        %v106148 = vadd.f32 %v106069, %v106144 (stack82)
        %v106152 = vmul.f32 %v106148, %v106108 (stack83)
        %v106156 = vadd.f32 %v106065, %v106152 (stack82)
        %v106160 = vmul.f32 %v106156, %v106108 (stack83)
        %v106164 = vadd.f32 %v106061, %v106160 (stack82)
        %v106168 = vmul.f32 %v106164, %v106108 (stack83)
        %v106172 = vadd.f32 %v106057, %v106168 (stack82)
        %v106176 = vmul.f32 %v106172, %v106023 (stack83)
        %v106180 = vsel /*vm=*/%vm106028, /*on_true_vy=*/%v106033, /*on_false_vx=*/%v106176 (stack72)
        %v106184 = vmul.f32 %v106180, 1.4140625 (stack83)
        %s106186 = scalar_lea.vmem %s280, 368 [#allocation0] (stack107)
        %v106187 = vpack.c.bf16 0.0, %v106184 (stack104)
        %106188 = vst [vmem:[%s106186] sm:$0xf] /*vst_source=*/%v106187 (stack105)
        %v106191 = vadd.s32 %v1868, %v104805 (stack65)
        %s106193 = smul.u32 128, %s27 (stack66)
        %v106194 = vlaneseq (stack67)
        %v106195 = vand.u32 %v106194, 127 (stack68)
        %v106196 = vstv %s106193 (stack69)
        %v106197 = vadd.s32 %v106195, %v106196 (stack70)
        %v106201 = vadd.s32 %v106191, %v106197 (stack65)
        %vm106205 = vcmp.lt.u32.totalorder %v106201, %v106191 (stack71)
        %vm106210 = vcmp.lt.u32.totalorder %v106191, %v1868 (stack71)
        %v106215 = vadd.s32 %v1855, %v104788 (stack65)
        %v106219 = vadd.s32 %v106215, 1 (stack65)
        %v106223 = vsel /*vm=*/%vm106210, /*on_true_vy=*/%v106219, /*on_false_vx=*/%v106215 (stack72)
        %v106227 = vadd.s32 %v106223, 1 (stack65)
        %v106231 = vsel /*vm=*/%vm106205, /*on_true_vy=*/%v106227, /*on_false_vx=*/%v106223 (stack72)
        %v106236 = vadd.s32 %v106231, %v10 (stack65)
        %v106240 = vadd.s32 %v106201, %v9 (stack65)
        %v106244 = vadd.s32 %v106236, %v106240 (stack65)
        %v106246 = vshll.u32 %v106240, 13 (stack73)
        %v106247 = vshrl.u32 %v106240, 19 (stack74)
        %v106248 = vor.u32 %v106246, %v106247 (stack75)
        %v106249 = vxor.u32 %v106244, %v106248 (stack76)
        %v106252 = vadd.s32 %v106244, %v106249 (stack65)
        %v106254 = vshll.u32 %v106249, 15 (stack73)
        %v106255 = vshrl.u32 %v106249, 17 (stack74)
        %v106256 = vor.u32 %v106254, %v106255 (stack75)
        %v106257 = vxor.u32 %v106252, %v106256 (stack76)
        %v106260 = vadd.s32 %v106252, %v106257 (stack65)
        %v106262 = vshll.u32 %v106257, 26 (stack73)
        %v106263 = vshrl.u32 %v106257, 6 (stack74)
        %v106264 = vor.u32 %v106262, %v106263 (stack75)
        %v106265 = vxor.u32 %v106260, %v106264 (stack76)
        %v106268 = vadd.s32 %v106260, %v106265 (stack65)
        %v106272 = vadd.s32 %v106268, %v9 (stack65)
        %v106274 = vshll.u32 %v106265, 6 (stack73)
        %v106275 = vshrl.u32 %v106265, 26 (stack74)
        %v106276 = vor.u32 %v106274, %v106275 (stack75)
        %v106277 = vxor.u32 %v106268, %v106276 (stack76)
        %v106280 = vadd.s32 %v106277, %v8 (stack65)
        %v106284 = vadd.s32 %v106280, 1 (stack65)
        %v106288 = vadd.s32 %v106272, %v106284 (stack65)
        %v106290 = vshll.u32 %v106284, 17 (stack73)
        %v106291 = vshrl.u32 %v106284, 15 (stack74)
        %v106292 = vor.u32 %v106290, %v106291 (stack75)
        %v106293 = vxor.u32 %v106288, %v106292 (stack76)
        %v106296 = vadd.s32 %v106288, %v106293 (stack65)
        %v106298 = vshll.u32 %v106293, 29 (stack73)
        %v106299 = vshrl.u32 %v106293, 3 (stack74)
        %v106300 = vor.u32 %v106298, %v106299 (stack75)
        %v106301 = vxor.u32 %v106296, %v106300 (stack76)
        %v106304 = vadd.s32 %v106296, %v106301 (stack65)
        %v106306 = vshll.u32 %v106301, 16 (stack73)
        %v106307 = vshrl.u32 %v106301, 16 (stack74)
        %v106308 = vor.u32 %v106306, %v106307 (stack75)
        %v106309 = vxor.u32 %v106304, %v106308 (stack76)
        %v106312 = vadd.s32 %v106304, %v106309 (stack65)
        %v106316 = vadd.s32 %v106312, %v8 (stack65)
        %v106318 = vshll.u32 %v106309, 24 (stack73)
        %v106319 = vshrl.u32 %v106309, 8 (stack74)
        %v106320 = vor.u32 %v106318, %v106319 (stack75)
        %v106321 = vxor.u32 %v106312, %v106320 (stack76)
        %v106324 = vadd.s32 %v106321, %v10 (stack65)
        %v106328 = vadd.s32 %v106324, 2 (stack65)
        %v106332 = vadd.s32 %v106316, %v106328 (stack65)
        %v106334 = vshll.u32 %v106328, 13 (stack73)
        %v106335 = vshrl.u32 %v106328, 19 (stack74)
        %v106336 = vor.u32 %v106334, %v106335 (stack75)
        %v106337 = vxor.u32 %v106332, %v106336 (stack76)
        %v106340 = vadd.s32 %v106332, %v106337 (stack65)
        %v106342 = vshll.u32 %v106337, 15 (stack73)
        %v106343 = vshrl.u32 %v106337, 17 (stack74)
        %v106344 = vor.u32 %v106342, %v106343 (stack75)
        %v106345 = vxor.u32 %v106340, %v106344 (stack76)
        %v106348 = vadd.s32 %v106340, %v106345 (stack65)
        %v106350 = vshll.u32 %v106345, 26 (stack73)
        %v106351 = vshrl.u32 %v106345, 6 (stack74)
        %v106352 = vor.u32 %v106350, %v106351 (stack75)
        %v106353 = vxor.u32 %v106348, %v106352 (stack76)
        %v106356 = vadd.s32 %v106348, %v106353 (stack65)
        %v106360 = vadd.s32 %v106356, %v10 (stack65)
        %v106362 = vshll.u32 %v106353, 6 (stack73)
        %v106363 = vshrl.u32 %v106353, 26 (stack74)
        %v106364 = vor.u32 %v106362, %v106363 (stack75)
        %v106365 = vxor.u32 %v106356, %v106364 (stack76)
        %v106368 = vadd.s32 %v106365, %v9 (stack65)
        %v106372 = vadd.s32 %v106368, 3 (stack65)
        %v106376 = vadd.s32 %v106360, %v106372 (stack65)
        %v106378 = vshll.u32 %v106372, 17 (stack73)
        %v106379 = vshrl.u32 %v106372, 15 (stack74)
        %v106380 = vor.u32 %v106378, %v106379 (stack75)
        %v106381 = vxor.u32 %v106376, %v106380 (stack76)
        %v106384 = vadd.s32 %v106376, %v106381 (stack65)
        %v106386 = vshll.u32 %v106381, 29 (stack73)
        %v106387 = vshrl.u32 %v106381, 3 (stack74)
        %v106388 = vor.u32 %v106386, %v106387 (stack75)
        %v106389 = vxor.u32 %v106384, %v106388 (stack76)
        %v106392 = vadd.s32 %v106384, %v106389 (stack65)
        %v106394 = vshll.u32 %v106389, 16 (stack73)
        %v106395 = vshrl.u32 %v106389, 16 (stack74)
        %v106396 = vor.u32 %v106394, %v106395 (stack75)
        %v106397 = vxor.u32 %v106392, %v106396 (stack76)
        %v106400 = vadd.s32 %v106392, %v106397 (stack65)
        %v106404 = vadd.s32 %v106400, %v9 (stack65)
        %v106406 = vshll.u32 %v106397, 24 (stack73)
        %v106407 = vshrl.u32 %v106397, 8 (stack74)
        %v106408 = vor.u32 %v106406, %v106407 (stack75)
        %v106409 = vxor.u32 %v106400, %v106408 (stack76)
        %v106412 = vadd.s32 %v106409, %v8 (stack65)
        %v106416 = vadd.s32 %v106412, 4 (stack65)
        %v106420 = vadd.s32 %v106404, %v106416 (stack65)
        %v106422 = vshll.u32 %v106416, 13 (stack73)
        %v106423 = vshrl.u32 %v106416, 19 (stack74)
        %v106424 = vor.u32 %v106422, %v106423 (stack75)
        %v106425 = vxor.u32 %v106420, %v106424 (stack76)
        %v106428 = vadd.s32 %v106420, %v106425 (stack65)
        %v106430 = vshll.u32 %v106425, 15 (stack73)
        %v106431 = vshrl.u32 %v106425, 17 (stack74)
        %v106432 = vor.u32 %v106430, %v106431 (stack75)
        %v106433 = vxor.u32 %v106428, %v106432 (stack76)
        %v106436 = vadd.s32 %v106428, %v106433 (stack65)
        %v106438 = vshll.u32 %v106433, 26 (stack73)
        %v106439 = vshrl.u32 %v106433, 6 (stack74)
        %v106440 = vor.u32 %v106438, %v106439 (stack75)
        %v106441 = vxor.u32 %v106436, %v106440 (stack76)
        %v106444 = vadd.s32 %v106436, %v106441 (stack65)
        %v106448 = vadd.s32 %v106444, %v8 (stack65)
        %v106450 = vshll.u32 %v106441, 6 (stack73)
        %v106451 = vshrl.u32 %v106441, 26 (stack74)
        %v106452 = vor.u32 %v106450, %v106451 (stack75)
        %v106453 = vxor.u32 %v106444, %v106452 (stack76)
        %v106456 = vadd.s32 %v106453, %v10 (stack65)
        %v106460 = vadd.s32 %v106456, 5 (stack65)
        %v106462 = vxor.u32 %v106448, %v106460 (stack76)
        %v106463 = vand.u32.u8 %v106462, 255 (stack77)
        %v106464 = vand.u32 %v106463, 65535 (stack78)
        %v106465 = vshrl.u32 %v106464, 1 (stack79)
        %v106466 = vor.u32 %v106465, 16256 (stack75)
        %v106467 = vand.u32.u16 %v106466, 65535 (stack80)
        %v106468 = vunpack.i.l.bf16 %v106467 (stack81)
        %v106472 = vadd.f32 %v106468, -1.0 (stack82)
        %v106476 = vmul.f32 %v106472, 2.0 (stack83)
        %v106480 = vadd.f32 %v106476, -0.99609375 (stack82)
        %v106484 = vmax.f32 -0.99609375, %v106480 (stack84)
        %v106486 = vand.u32 2147483647, %v106484 (stack85)
        %vm106489 = vcmp.eq.f32.partialorder %v106486, 1.0 (stack86)
        %v106494 = vmul.f32 %v106484, inf (stack83)
        %v106496 = vxor.u32 %v106484, 2147483648 (stack87)
        %v106499 = vmul.f32 %v106484, %v106496 (stack83)
        %v106501 = vadd.f32 %v106499, 1.0 (stack88)
        %v106502 = vlog2.pop %v106501 (stack89)
        %v106503 = vmul.f32 %v106502, 0.6931472 (stack90)
        %v106504 = vmul.f32 -0.5, %v106499 (stack91)
        %v106505 = vadd.f32 %v106504, 1.0 (stack92)
        %v106506 = vmul.f32 %v106505, %v106499 (stack93)
        %v106507 = vand.u32 2147483647, %v106499 (stack94)
        %vm106508 = vcmp.lt.f32.partialorder %v106507, 0.0004427343 (stack95)
        %v106509 = vsel /*vm=*/%vm106508, /*on_true_vy=*/%v106506, /*on_false_vx=*/%v106503 (stack96)
        %v106510 = vxor.u32 %v106509, 2147483648 (stack87)
        %vm106513 = vcmp.lt.f32.partialorder %v106510, 5.0 (stack86)
        %v106518 = vsel /*vm=*/%vm106513, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v106522 = vsel /*vm=*/%vm106513, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v106526 = vsel /*vm=*/%vm106513, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v106530 = vsel /*vm=*/%vm106513, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v106534 = vsel /*vm=*/%vm106513, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v106538 = vsel /*vm=*/%vm106513, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v106542 = vsel /*vm=*/%vm106513, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v106546 = vsel /*vm=*/%vm106513, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v106550 = vsel /*vm=*/%vm106513, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v106554 = vadd.f32 %v106510, -2.5 (stack82)
        %v106556 = vrsqrt.pop %v106510 (stack97)
        %v106557 = vmul.f32 %v106510, %v106556 (stack98)
        %vm106558 = vcmp.eq.f32.partialorder %v106510, inf (stack99)
        %v106559 = vsel /*vm=*/%vm106558, /*on_true_vy=*/%v106510, /*on_false_vx=*/%v106557 (stack100)
        %vm106560 = vcmp.eq.f32.partialorder %v106510, 0.0 (stack101)
        %v106561 = vand.u32 %v106510, 2147483648 (stack102)
        %v106562 = vsel /*vm=*/%vm106560, /*on_true_vy=*/%v106561, /*on_false_vx=*/%v106559 (stack103)
        %v106565 = vadd.f32 %v106562, -3.0 (stack82)
        %v106569 = vsel /*vm=*/%vm106513, /*on_true_vy=*/%v106554, /*on_false_vx=*/%v106565 (stack72)
        %v106573 = vmul.f32 %v106550, %v106569 (stack83)
        %v106577 = vadd.f32 %v106546, %v106573 (stack82)
        %v106581 = vmul.f32 %v106577, %v106569 (stack83)
        %v106585 = vadd.f32 %v106542, %v106581 (stack82)
        %v106589 = vmul.f32 %v106585, %v106569 (stack83)
        %v106593 = vadd.f32 %v106538, %v106589 (stack82)
        %v106597 = vmul.f32 %v106593, %v106569 (stack83)
        %v106601 = vadd.f32 %v106534, %v106597 (stack82)
        %v106605 = vmul.f32 %v106601, %v106569 (stack83)
        %v106609 = vadd.f32 %v106530, %v106605 (stack82)
        %v106613 = vmul.f32 %v106609, %v106569 (stack83)
        %v106617 = vadd.f32 %v106526, %v106613 (stack82)
        %v106621 = vmul.f32 %v106617, %v106569 (stack83)
        %v106625 = vadd.f32 %v106522, %v106621 (stack82)
        %v106629 = vmul.f32 %v106625, %v106569 (stack83)
        %v106633 = vadd.f32 %v106518, %v106629 (stack82)
        %v106637 = vmul.f32 %v106633, %v106484 (stack83)
        %v106641 = vsel /*vm=*/%vm106489, /*on_true_vy=*/%v106494, /*on_false_vx=*/%v106637 (stack72)
        %v106645 = vmul.f32 %v106641, 1.4140625 (stack83)
        %s106647 = scalar_lea.vmem %s280, 496 [#allocation0] (stack107)
        %v106648 = vpack.c.bf16 0.0, %v106645 (stack104)
        %106649 = vst [vmem:[%s106647] sm:$0xf] /*vst_source=*/%v106648 (stack105)
        %v106652 = vadd.s32 %v2355, %v104805 (stack65)
        %s106654 = smul.u32 128, %s27 (stack66)
        %v106655 = vlaneseq (stack67)
        %v106656 = vand.u32 %v106655, 127 (stack68)
        %v106657 = vstv %s106654 (stack69)
        %v106658 = vadd.s32 %v106656, %v106657 (stack70)
        %v106662 = vadd.s32 %v106652, %v106658 (stack65)
        %vm106666 = vcmp.lt.u32.totalorder %v106662, %v106652 (stack71)
        %vm106671 = vcmp.lt.u32.totalorder %v106652, %v2355 (stack71)
        %v106676 = vadd.s32 %v2342, %v104788 (stack65)
        %v106680 = vadd.s32 %v106676, 1 (stack65)
        %v106684 = vsel /*vm=*/%vm106671, /*on_true_vy=*/%v106680, /*on_false_vx=*/%v106676 (stack72)
        %v106688 = vadd.s32 %v106684, 1 (stack65)
        %v106692 = vsel /*vm=*/%vm106666, /*on_true_vy=*/%v106688, /*on_false_vx=*/%v106684 (stack72)
        %v106697 = vadd.s32 %v106692, %v10 (stack65)
        %v106701 = vadd.s32 %v106662, %v9 (stack65)
        %v106705 = vadd.s32 %v106697, %v106701 (stack65)
        %v106707 = vshll.u32 %v106701, 13 (stack73)
        %v106708 = vshrl.u32 %v106701, 19 (stack74)
        %v106709 = vor.u32 %v106707, %v106708 (stack75)
        %v106710 = vxor.u32 %v106705, %v106709 (stack76)
        %v106713 = vadd.s32 %v106705, %v106710 (stack65)
        %v106715 = vshll.u32 %v106710, 15 (stack73)
        %v106716 = vshrl.u32 %v106710, 17 (stack74)
        %v106717 = vor.u32 %v106715, %v106716 (stack75)
        %v106718 = vxor.u32 %v106713, %v106717 (stack76)
        %v106721 = vadd.s32 %v106713, %v106718 (stack65)
        %v106723 = vshll.u32 %v106718, 26 (stack73)
        %v106724 = vshrl.u32 %v106718, 6 (stack74)
        %v106725 = vor.u32 %v106723, %v106724 (stack75)
        %v106726 = vxor.u32 %v106721, %v106725 (stack76)
        %v106729 = vadd.s32 %v106721, %v106726 (stack65)
        %v106733 = vadd.s32 %v106729, %v9 (stack65)
        %v106735 = vshll.u32 %v106726, 6 (stack73)
        %v106736 = vshrl.u32 %v106726, 26 (stack74)
        %v106737 = vor.u32 %v106735, %v106736 (stack75)
        %v106738 = vxor.u32 %v106729, %v106737 (stack76)
        %v106741 = vadd.s32 %v106738, %v8 (stack65)
        %v106745 = vadd.s32 %v106741, 1 (stack65)
        %v106749 = vadd.s32 %v106733, %v106745 (stack65)
        %v106751 = vshll.u32 %v106745, 17 (stack73)
        %v106752 = vshrl.u32 %v106745, 15 (stack74)
        %v106753 = vor.u32 %v106751, %v106752 (stack75)
        %v106754 = vxor.u32 %v106749, %v106753 (stack76)
        %v106757 = vadd.s32 %v106749, %v106754 (stack65)
        %v106759 = vshll.u32 %v106754, 29 (stack73)
        %v106760 = vshrl.u32 %v106754, 3 (stack74)
        %v106761 = vor.u32 %v106759, %v106760 (stack75)
        %v106762 = vxor.u32 %v106757, %v106761 (stack76)
        %v106765 = vadd.s32 %v106757, %v106762 (stack65)
        %v106767 = vshll.u32 %v106762, 16 (stack73)
        %v106768 = vshrl.u32 %v106762, 16 (stack74)
        %v106769 = vor.u32 %v106767, %v106768 (stack75)
        %v106770 = vxor.u32 %v106765, %v106769 (stack76)
        %v106773 = vadd.s32 %v106765, %v106770 (stack65)
        %v106777 = vadd.s32 %v106773, %v8 (stack65)
        %v106779 = vshll.u32 %v106770, 24 (stack73)
        %v106780 = vshrl.u32 %v106770, 8 (stack74)
        %v106781 = vor.u32 %v106779, %v106780 (stack75)
        %v106782 = vxor.u32 %v106773, %v106781 (stack76)
        %v106785 = vadd.s32 %v106782, %v10 (stack65)
        %v106789 = vadd.s32 %v106785, 2 (stack65)
        %v106793 = vadd.s32 %v106777, %v106789 (stack65)
        %v106795 = vshll.u32 %v106789, 13 (stack73)
        %v106796 = vshrl.u32 %v106789, 19 (stack74)
        %v106797 = vor.u32 %v106795, %v106796 (stack75)
        %v106798 = vxor.u32 %v106793, %v106797 (stack76)
        %v106801 = vadd.s32 %v106793, %v106798 (stack65)
        %v106803 = vshll.u32 %v106798, 15 (stack73)
        %v106804 = vshrl.u32 %v106798, 17 (stack74)
        %v106805 = vor.u32 %v106803, %v106804 (stack75)
        %v106806 = vxor.u32 %v106801, %v106805 (stack76)
        %v106809 = vadd.s32 %v106801, %v106806 (stack65)
        %v106811 = vshll.u32 %v106806, 26 (stack73)
        %v106812 = vshrl.u32 %v106806, 6 (stack74)
        %v106813 = vor.u32 %v106811, %v106812 (stack75)
        %v106814 = vxor.u32 %v106809, %v106813 (stack76)
        %v106817 = vadd.s32 %v106809, %v106814 (stack65)
        %v106821 = vadd.s32 %v106817, %v10 (stack65)
        %v106823 = vshll.u32 %v106814, 6 (stack73)
        %v106824 = vshrl.u32 %v106814, 26 (stack74)
        %v106825 = vor.u32 %v106823, %v106824 (stack75)
        %v106826 = vxor.u32 %v106817, %v106825 (stack76)
        %v106829 = vadd.s32 %v106826, %v9 (stack65)
        %v106833 = vadd.s32 %v106829, 3 (stack65)
        %v106837 = vadd.s32 %v106821, %v106833 (stack65)
        %v106839 = vshll.u32 %v106833, 17 (stack73)
        %v106840 = vshrl.u32 %v106833, 15 (stack74)
        %v106841 = vor.u32 %v106839, %v106840 (stack75)
        %v106842 = vxor.u32 %v106837, %v106841 (stack76)
        %v106845 = vadd.s32 %v106837, %v106842 (stack65)
        %v106847 = vshll.u32 %v106842, 29 (stack73)
        %v106848 = vshrl.u32 %v106842, 3 (stack74)
        %v106849 = vor.u32 %v106847, %v106848 (stack75)
        %v106850 = vxor.u32 %v106845, %v106849 (stack76)
        %v106853 = vadd.s32 %v106845, %v106850 (stack65)
        %v106855 = vshll.u32 %v106850, 16 (stack73)
        %v106856 = vshrl.u32 %v106850, 16 (stack74)
        %v106857 = vor.u32 %v106855, %v106856 (stack75)
        %v106858 = vxor.u32 %v106853, %v106857 (stack76)
        %v106861 = vadd.s32 %v106853, %v106858 (stack65)
        %v106865 = vadd.s32 %v106861, %v9 (stack65)
        %v106867 = vshll.u32 %v106858, 24 (stack73)
        %v106868 = vshrl.u32 %v106858, 8 (stack74)
        %v106869 = vor.u32 %v106867, %v106868 (stack75)
        %v106870 = vxor.u32 %v106861, %v106869 (stack76)
        %v106873 = vadd.s32 %v106870, %v8 (stack65)
        %v106877 = vadd.s32 %v106873, 4 (stack65)
        %v106881 = vadd.s32 %v106865, %v106877 (stack65)
        %v106883 = vshll.u32 %v106877, 13 (stack73)
        %v106884 = vshrl.u32 %v106877, 19 (stack74)
        %v106885 = vor.u32 %v106883, %v106884 (stack75)
        %v106886 = vxor.u32 %v106881, %v106885 (stack76)
        %v106889 = vadd.s32 %v106881, %v106886 (stack65)
        %v106891 = vshll.u32 %v106886, 15 (stack73)
        %v106892 = vshrl.u32 %v106886, 17 (stack74)
        %v106893 = vor.u32 %v106891, %v106892 (stack75)
        %v106894 = vxor.u32 %v106889, %v106893 (stack76)
        %v106897 = vadd.s32 %v106889, %v106894 (stack65)
        %v106899 = vshll.u32 %v106894, 26 (stack73)
        %v106900 = vshrl.u32 %v106894, 6 (stack74)
        %v106901 = vor.u32 %v106899, %v106900 (stack75)
        %v106902 = vxor.u32 %v106897, %v106901 (stack76)
        %v106905 = vadd.s32 %v106897, %v106902 (stack65)
        %v106909 = vadd.s32 %v106905, %v8 (stack65)
        %v106911 = vshll.u32 %v106902, 6 (stack73)
        %v106912 = vshrl.u32 %v106902, 26 (stack74)
        %v106913 = vor.u32 %v106911, %v106912 (stack75)
        %v106914 = vxor.u32 %v106905, %v106913 (stack76)
        %v106917 = vadd.s32 %v106914, %v10 (stack65)
        %v106921 = vadd.s32 %v106917, 5 (stack65)
        %v106923 = vxor.u32 %v106909, %v106921 (stack76)
        %v106924 = vand.u32.u8 %v106923, 255 (stack77)
        %v106925 = vand.u32 %v106924, 65535 (stack78)
        %v106926 = vshrl.u32 %v106925, 1 (stack79)
        %v106927 = vor.u32 %v106926, 16256 (stack75)
        %v106928 = vand.u32.u16 %v106927, 65535 (stack80)
        %v106929 = vunpack.i.l.bf16 %v106928 (stack81)
        %v106933 = vadd.f32 %v106929, -1.0 (stack82)
        %v106937 = vmul.f32 %v106933, 2.0 (stack83)
        %v106941 = vadd.f32 %v106937, -0.99609375 (stack82)
        %v106945 = vmax.f32 -0.99609375, %v106941 (stack84)
        %v106947 = vand.u32 2147483647, %v106945 (stack85)
        %vm106950 = vcmp.eq.f32.partialorder %v106947, 1.0 (stack86)
        %v106955 = vmul.f32 %v106945, inf (stack83)
        %v106957 = vxor.u32 %v106945, 2147483648 (stack87)
        %v106960 = vmul.f32 %v106945, %v106957 (stack83)
        %v106962 = vadd.f32 %v106960, 1.0 (stack88)
        %v106963 = vlog2.pop %v106962 (stack89)
        %v106964 = vmul.f32 %v106963, 0.6931472 (stack90)
        %v106965 = vmul.f32 -0.5, %v106960 (stack91)
        %v106966 = vadd.f32 %v106965, 1.0 (stack92)
        %v106967 = vmul.f32 %v106966, %v106960 (stack93)
        %v106968 = vand.u32 2147483647, %v106960 (stack94)
        %vm106969 = vcmp.lt.f32.partialorder %v106968, 0.0004427343 (stack95)
        %v106970 = vsel /*vm=*/%vm106969, /*on_true_vy=*/%v106967, /*on_false_vx=*/%v106964 (stack96)
        %v106971 = vxor.u32 %v106970, 2147483648 (stack87)
        %vm106974 = vcmp.lt.f32.partialorder %v106971, 5.0 (stack86)
        %v106979 = vsel /*vm=*/%vm106974, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v106983 = vsel /*vm=*/%vm106974, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v106987 = vsel /*vm=*/%vm106974, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v106991 = vsel /*vm=*/%vm106974, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v106995 = vsel /*vm=*/%vm106974, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v106999 = vsel /*vm=*/%vm106974, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v107003 = vsel /*vm=*/%vm106974, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v107007 = vsel /*vm=*/%vm106974, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v107011 = vsel /*vm=*/%vm106974, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v107015 = vadd.f32 %v106971, -2.5 (stack82)
        %v107017 = vrsqrt.pop %v106971 (stack97)
        %v107018 = vmul.f32 %v106971, %v107017 (stack98)
        %vm107019 = vcmp.eq.f32.partialorder %v106971, inf (stack99)
        %v107020 = vsel /*vm=*/%vm107019, /*on_true_vy=*/%v106971, /*on_false_vx=*/%v107018 (stack100)
        %vm107021 = vcmp.eq.f32.partialorder %v106971, 0.0 (stack101)
        %v107022 = vand.u32 %v106971, 2147483648 (stack102)
        %v107023 = vsel /*vm=*/%vm107021, /*on_true_vy=*/%v107022, /*on_false_vx=*/%v107020 (stack103)
        %v107026 = vadd.f32 %v107023, -3.0 (stack82)
        %v107030 = vsel /*vm=*/%vm106974, /*on_true_vy=*/%v107015, /*on_false_vx=*/%v107026 (stack72)
        %v107034 = vmul.f32 %v107011, %v107030 (stack83)
        %v107038 = vadd.f32 %v107007, %v107034 (stack82)
        %v107042 = vmul.f32 %v107038, %v107030 (stack83)
        %v107046 = vadd.f32 %v107003, %v107042 (stack82)
        %v107050 = vmul.f32 %v107046, %v107030 (stack83)
        %v107054 = vadd.f32 %v106999, %v107050 (stack82)
        %v107058 = vmul.f32 %v107054, %v107030 (stack83)
        %v107062 = vadd.f32 %v106995, %v107058 (stack82)
        %v107066 = vmul.f32 %v107062, %v107030 (stack83)
        %v107070 = vadd.f32 %v106991, %v107066 (stack82)
        %v107074 = vmul.f32 %v107070, %v107030 (stack83)
        %v107078 = vadd.f32 %v106987, %v107074 (stack82)
        %v107082 = vmul.f32 %v107078, %v107030 (stack83)
        %v107086 = vadd.f32 %v106983, %v107082 (stack82)
        %v107090 = vmul.f32 %v107086, %v107030 (stack83)
        %v107094 = vadd.f32 %v106979, %v107090 (stack82)
        %v107098 = vmul.f32 %v107094, %v106945 (stack83)
        %v107102 = vsel /*vm=*/%vm106950, /*on_true_vy=*/%v106955, /*on_false_vx=*/%v107098 (stack72)
        %v107106 = vmul.f32 %v107102, 1.4140625 (stack83)
        %s107108 = scalar_lea.vmem %s280, 624 [#allocation0] (stack107)
        %v107109 = vpack.c.bf16 0.0, %v107106 (stack104)
        %107110 = vst [vmem:[%s107108] sm:$0xf] /*vst_source=*/%v107109 (stack105)
        %v107113 = vadd.s32 %v2842, %v104805 (stack65)
        %s107115 = smul.u32 128, %s27 (stack66)
        %v107116 = vlaneseq (stack67)
        %v107117 = vand.u32 %v107116, 127 (stack68)
        %v107118 = vstv %s107115 (stack69)
        %v107119 = vadd.s32 %v107117, %v107118 (stack70)
        %v107123 = vadd.s32 %v107113, %v107119 (stack65)
        %vm107127 = vcmp.lt.u32.totalorder %v107123, %v107113 (stack71)
        %vm107132 = vcmp.lt.u32.totalorder %v107113, %v2842 (stack71)
        %v107137 = vadd.s32 %v2829, %v104788 (stack65)
        %v107141 = vadd.s32 %v107137, 1 (stack65)
        %v107145 = vsel /*vm=*/%vm107132, /*on_true_vy=*/%v107141, /*on_false_vx=*/%v107137 (stack72)
        %v107149 = vadd.s32 %v107145, 1 (stack65)
        %v107153 = vsel /*vm=*/%vm107127, /*on_true_vy=*/%v107149, /*on_false_vx=*/%v107145 (stack72)
        %v107158 = vadd.s32 %v107153, %v10 (stack65)
        %v107162 = vadd.s32 %v107123, %v9 (stack65)
        %v107166 = vadd.s32 %v107158, %v107162 (stack65)
        %v107168 = vshll.u32 %v107162, 13 (stack73)
        %v107169 = vshrl.u32 %v107162, 19 (stack74)
        %v107170 = vor.u32 %v107168, %v107169 (stack75)
        %v107171 = vxor.u32 %v107166, %v107170 (stack76)
        %v107174 = vadd.s32 %v107166, %v107171 (stack65)
        %v107176 = vshll.u32 %v107171, 15 (stack73)
        %v107177 = vshrl.u32 %v107171, 17 (stack74)
        %v107178 = vor.u32 %v107176, %v107177 (stack75)
        %v107179 = vxor.u32 %v107174, %v107178 (stack76)
        %v107182 = vadd.s32 %v107174, %v107179 (stack65)
        %v107184 = vshll.u32 %v107179, 26 (stack73)
        %v107185 = vshrl.u32 %v107179, 6 (stack74)
        %v107186 = vor.u32 %v107184, %v107185 (stack75)
        %v107187 = vxor.u32 %v107182, %v107186 (stack76)
        %v107190 = vadd.s32 %v107182, %v107187 (stack65)
        %v107194 = vadd.s32 %v107190, %v9 (stack65)
        %v107196 = vshll.u32 %v107187, 6 (stack73)
        %v107197 = vshrl.u32 %v107187, 26 (stack74)
        %v107198 = vor.u32 %v107196, %v107197 (stack75)
        %v107199 = vxor.u32 %v107190, %v107198 (stack76)
        %v107202 = vadd.s32 %v107199, %v8 (stack65)
        %v107206 = vadd.s32 %v107202, 1 (stack65)
        %v107210 = vadd.s32 %v107194, %v107206 (stack65)
        %v107212 = vshll.u32 %v107206, 17 (stack73)
        %v107213 = vshrl.u32 %v107206, 15 (stack74)
        %v107214 = vor.u32 %v107212, %v107213 (stack75)
        %v107215 = vxor.u32 %v107210, %v107214 (stack76)
        %v107218 = vadd.s32 %v107210, %v107215 (stack65)
        %v107220 = vshll.u32 %v107215, 29 (stack73)
        %v107221 = vshrl.u32 %v107215, 3 (stack74)
        %v107222 = vor.u32 %v107220, %v107221 (stack75)
        %v107223 = vxor.u32 %v107218, %v107222 (stack76)
        %v107226 = vadd.s32 %v107218, %v107223 (stack65)
        %v107228 = vshll.u32 %v107223, 16 (stack73)
        %v107229 = vshrl.u32 %v107223, 16 (stack74)
        %v107230 = vor.u32 %v107228, %v107229 (stack75)
        %v107231 = vxor.u32 %v107226, %v107230 (stack76)
        %v107234 = vadd.s32 %v107226, %v107231 (stack65)
        %v107238 = vadd.s32 %v107234, %v8 (stack65)
        %v107240 = vshll.u32 %v107231, 24 (stack73)
        %v107241 = vshrl.u32 %v107231, 8 (stack74)
        %v107242 = vor.u32 %v107240, %v107241 (stack75)
        %v107243 = vxor.u32 %v107234, %v107242 (stack76)
        %v107246 = vadd.s32 %v107243, %v10 (stack65)
        %v107250 = vadd.s32 %v107246, 2 (stack65)
        %v107254 = vadd.s32 %v107238, %v107250 (stack65)
        %v107256 = vshll.u32 %v107250, 13 (stack73)
        %v107257 = vshrl.u32 %v107250, 19 (stack74)
        %v107258 = vor.u32 %v107256, %v107257 (stack75)
        %v107259 = vxor.u32 %v107254, %v107258 (stack76)
        %v107262 = vadd.s32 %v107254, %v107259 (stack65)
        %v107264 = vshll.u32 %v107259, 15 (stack73)
        %v107265 = vshrl.u32 %v107259, 17 (stack74)
        %v107266 = vor.u32 %v107264, %v107265 (stack75)
        %v107267 = vxor.u32 %v107262, %v107266 (stack76)
        %v107270 = vadd.s32 %v107262, %v107267 (stack65)
        %v107272 = vshll.u32 %v107267, 26 (stack73)
        %v107273 = vshrl.u32 %v107267, 6 (stack74)
        %v107274 = vor.u32 %v107272, %v107273 (stack75)
        %v107275 = vxor.u32 %v107270, %v107274 (stack76)
        %v107278 = vadd.s32 %v107270, %v107275 (stack65)
        %v107282 = vadd.s32 %v107278, %v10 (stack65)
        %v107284 = vshll.u32 %v107275, 6 (stack73)
        %v107285 = vshrl.u32 %v107275, 26 (stack74)
        %v107286 = vor.u32 %v107284, %v107285 (stack75)
        %v107287 = vxor.u32 %v107278, %v107286 (stack76)
        %v107290 = vadd.s32 %v107287, %v9 (stack65)
        %v107294 = vadd.s32 %v107290, 3 (stack65)
        %v107298 = vadd.s32 %v107282, %v107294 (stack65)
        %v107300 = vshll.u32 %v107294, 17 (stack73)
        %v107301 = vshrl.u32 %v107294, 15 (stack74)
        %v107302 = vor.u32 %v107300, %v107301 (stack75)
        %v107303 = vxor.u32 %v107298, %v107302 (stack76)
        %v107306 = vadd.s32 %v107298, %v107303 (stack65)
        %v107308 = vshll.u32 %v107303, 29 (stack73)
        %v107309 = vshrl.u32 %v107303, 3 (stack74)
        %v107310 = vor.u32 %v107308, %v107309 (stack75)
        %v107311 = vxor.u32 %v107306, %v107310 (stack76)
        %v107314 = vadd.s32 %v107306, %v107311 (stack65)
        %v107316 = vshll.u32 %v107311, 16 (stack73)
        %v107317 = vshrl.u32 %v107311, 16 (stack74)
        %v107318 = vor.u32 %v107316, %v107317 (stack75)
        %v107319 = vxor.u32 %v107314, %v107318 (stack76)
        %v107322 = vadd.s32 %v107314, %v107319 (stack65)
        %v107326 = vadd.s32 %v107322, %v9 (stack65)
        %v107328 = vshll.u32 %v107319, 24 (stack73)
        %v107329 = vshrl.u32 %v107319, 8 (stack74)
        %v107330 = vor.u32 %v107328, %v107329 (stack75)
        %v107331 = vxor.u32 %v107322, %v107330 (stack76)
        %v107334 = vadd.s32 %v107331, %v8 (stack65)
        %v107338 = vadd.s32 %v107334, 4 (stack65)
        %v107342 = vadd.s32 %v107326, %v107338 (stack65)
        %v107344 = vshll.u32 %v107338, 13 (stack73)
        %v107345 = vshrl.u32 %v107338, 19 (stack74)
        %v107346 = vor.u32 %v107344, %v107345 (stack75)
        %v107347 = vxor.u32 %v107342, %v107346 (stack76)
        %v107350 = vadd.s32 %v107342, %v107347 (stack65)
        %v107352 = vshll.u32 %v107347, 15 (stack73)
        %v107353 = vshrl.u32 %v107347, 17 (stack74)
        %v107354 = vor.u32 %v107352, %v107353 (stack75)
        %v107355 = vxor.u32 %v107350, %v107354 (stack76)
        %v107358 = vadd.s32 %v107350, %v107355 (stack65)
        %v107360 = vshll.u32 %v107355, 26 (stack73)
        %v107361 = vshrl.u32 %v107355, 6 (stack74)
        %v107362 = vor.u32 %v107360, %v107361 (stack75)
        %v107363 = vxor.u32 %v107358, %v107362 (stack76)
        %v107366 = vadd.s32 %v107358, %v107363 (stack65)
        %v107370 = vadd.s32 %v107366, %v8 (stack65)
        %v107372 = vshll.u32 %v107363, 6 (stack73)
        %v107373 = vshrl.u32 %v107363, 26 (stack74)
        %v107374 = vor.u32 %v107372, %v107373 (stack75)
        %v107375 = vxor.u32 %v107366, %v107374 (stack76)
        %v107378 = vadd.s32 %v107375, %v10 (stack65)
        %v107382 = vadd.s32 %v107378, 5 (stack65)
        %v107384 = vxor.u32 %v107370, %v107382 (stack76)
        %v107385 = vand.u32.u8 %v107384, 255 (stack77)
        %v107386 = vand.u32 %v107385, 65535 (stack78)
        %v107387 = vshrl.u32 %v107386, 1 (stack79)
        %v107388 = vor.u32 %v107387, 16256 (stack75)
        %v107389 = vand.u32.u16 %v107388, 65535 (stack80)
        %v107390 = vunpack.i.l.bf16 %v107389 (stack81)
        %v107394 = vadd.f32 %v107390, -1.0 (stack82)
        %v107398 = vmul.f32 %v107394, 2.0 (stack83)
        %v107402 = vadd.f32 %v107398, -0.99609375 (stack82)
        %v107406 = vmax.f32 -0.99609375, %v107402 (stack84)
        %v107408 = vand.u32 2147483647, %v107406 (stack85)
        %vm107411 = vcmp.eq.f32.partialorder %v107408, 1.0 (stack86)
        %v107416 = vmul.f32 %v107406, inf (stack83)
        %v107418 = vxor.u32 %v107406, 2147483648 (stack87)
        %v107421 = vmul.f32 %v107406, %v107418 (stack83)
        %v107423 = vadd.f32 %v107421, 1.0 (stack88)
        %v107424 = vlog2.pop %v107423 (stack89)
        %v107425 = vmul.f32 %v107424, 0.6931472 (stack90)
        %v107426 = vmul.f32 -0.5, %v107421 (stack91)
        %v107427 = vadd.f32 %v107426, 1.0 (stack92)
        %v107428 = vmul.f32 %v107427, %v107421 (stack93)
        %v107429 = vand.u32 2147483647, %v107421 (stack94)
        %vm107430 = vcmp.lt.f32.partialorder %v107429, 0.0004427343 (stack95)
        %v107431 = vsel /*vm=*/%vm107430, /*on_true_vy=*/%v107428, /*on_false_vx=*/%v107425 (stack96)
        %v107432 = vxor.u32 %v107431, 2147483648 (stack87)
        %vm107435 = vcmp.lt.f32.partialorder %v107432, 5.0 (stack86)
        %v107440 = vsel /*vm=*/%vm107435, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v107444 = vsel /*vm=*/%vm107435, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v107448 = vsel /*vm=*/%vm107435, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v107452 = vsel /*vm=*/%vm107435, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v107456 = vsel /*vm=*/%vm107435, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v107460 = vsel /*vm=*/%vm107435, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v107464 = vsel /*vm=*/%vm107435, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v107468 = vsel /*vm=*/%vm107435, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v107472 = vsel /*vm=*/%vm107435, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v107476 = vadd.f32 %v107432, -2.5 (stack82)
        %v107478 = vrsqrt.pop %v107432 (stack97)
        %v107479 = vmul.f32 %v107432, %v107478 (stack98)
        %vm107480 = vcmp.eq.f32.partialorder %v107432, inf (stack99)
        %v107481 = vsel /*vm=*/%vm107480, /*on_true_vy=*/%v107432, /*on_false_vx=*/%v107479 (stack100)
        %vm107482 = vcmp.eq.f32.partialorder %v107432, 0.0 (stack101)
        %v107483 = vand.u32 %v107432, 2147483648 (stack102)
        %v107484 = vsel /*vm=*/%vm107482, /*on_true_vy=*/%v107483, /*on_false_vx=*/%v107481 (stack103)
        %v107487 = vadd.f32 %v107484, -3.0 (stack82)
        %v107491 = vsel /*vm=*/%vm107435, /*on_true_vy=*/%v107476, /*on_false_vx=*/%v107487 (stack72)
        %v107495 = vmul.f32 %v107472, %v107491 (stack83)
        %v107499 = vadd.f32 %v107468, %v107495 (stack82)
        %v107503 = vmul.f32 %v107499, %v107491 (stack83)
        %v107507 = vadd.f32 %v107464, %v107503 (stack82)
        %v107511 = vmul.f32 %v107507, %v107491 (stack83)
        %v107515 = vadd.f32 %v107460, %v107511 (stack82)
        %v107519 = vmul.f32 %v107515, %v107491 (stack83)
        %v107523 = vadd.f32 %v107456, %v107519 (stack82)
        %v107527 = vmul.f32 %v107523, %v107491 (stack83)
        %v107531 = vadd.f32 %v107452, %v107527 (stack82)
        %v107535 = vmul.f32 %v107531, %v107491 (stack83)
        %v107539 = vadd.f32 %v107448, %v107535 (stack82)
        %v107543 = vmul.f32 %v107539, %v107491 (stack83)
        %v107547 = vadd.f32 %v107444, %v107543 (stack82)
        %v107551 = vmul.f32 %v107547, %v107491 (stack83)
        %v107555 = vadd.f32 %v107440, %v107551 (stack82)
        %v107559 = vmul.f32 %v107555, %v107406 (stack83)
        %v107563 = vsel /*vm=*/%vm107411, /*on_true_vy=*/%v107416, /*on_false_vx=*/%v107559 (stack72)
        %v107567 = vmul.f32 %v107563, 1.4140625 (stack83)
        %s107569 = scalar_lea.vmem %s280, 752 [#allocation0] (stack107)
        %v107570 = vpack.c.bf16 0.0, %v107567 (stack104)
        %107571 = vst [vmem:[%s107569] sm:$0xf] /*vst_source=*/%v107570 (stack105)
        %v107574 = vadd.s32 %v3329, %v104805 (stack65)
        %s107576 = smul.u32 128, %s27 (stack66)
        %v107577 = vlaneseq (stack67)
        %v107578 = vand.u32 %v107577, 127 (stack68)
        %v107579 = vstv %s107576 (stack69)
        %v107580 = vadd.s32 %v107578, %v107579 (stack70)
        %v107584 = vadd.s32 %v107574, %v107580 (stack65)
        %vm107588 = vcmp.lt.u32.totalorder %v107584, %v107574 (stack71)
        %vm107593 = vcmp.lt.u32.totalorder %v107574, %v3329 (stack71)
        %v107598 = vadd.s32 %v3316, %v104788 (stack65)
        %v107602 = vadd.s32 %v107598, 1 (stack65)
        %v107606 = vsel /*vm=*/%vm107593, /*on_true_vy=*/%v107602, /*on_false_vx=*/%v107598 (stack72)
        %v107610 = vadd.s32 %v107606, 1 (stack65)
        %v107614 = vsel /*vm=*/%vm107588, /*on_true_vy=*/%v107610, /*on_false_vx=*/%v107606 (stack72)
        %v107619 = vadd.s32 %v107614, %v10 (stack65)
        %v107623 = vadd.s32 %v107584, %v9 (stack65)
        %v107627 = vadd.s32 %v107619, %v107623 (stack65)
        %v107629 = vshll.u32 %v107623, 13 (stack73)
        %v107630 = vshrl.u32 %v107623, 19 (stack74)
        %v107631 = vor.u32 %v107629, %v107630 (stack75)
        %v107632 = vxor.u32 %v107627, %v107631 (stack76)
        %v107635 = vadd.s32 %v107627, %v107632 (stack65)
        %v107637 = vshll.u32 %v107632, 15 (stack73)
        %v107638 = vshrl.u32 %v107632, 17 (stack74)
        %v107639 = vor.u32 %v107637, %v107638 (stack75)
        %v107640 = vxor.u32 %v107635, %v107639 (stack76)
        %v107643 = vadd.s32 %v107635, %v107640 (stack65)
        %v107645 = vshll.u32 %v107640, 26 (stack73)
        %v107646 = vshrl.u32 %v107640, 6 (stack74)
        %v107647 = vor.u32 %v107645, %v107646 (stack75)
        %v107648 = vxor.u32 %v107643, %v107647 (stack76)
        %v107651 = vadd.s32 %v107643, %v107648 (stack65)
        %v107655 = vadd.s32 %v107651, %v9 (stack65)
        %v107657 = vshll.u32 %v107648, 6 (stack73)
        %v107658 = vshrl.u32 %v107648, 26 (stack74)
        %v107659 = vor.u32 %v107657, %v107658 (stack75)
        %v107660 = vxor.u32 %v107651, %v107659 (stack76)
        %v107663 = vadd.s32 %v107660, %v8 (stack65)
        %v107667 = vadd.s32 %v107663, 1 (stack65)
        %v107671 = vadd.s32 %v107655, %v107667 (stack65)
        %v107673 = vshll.u32 %v107667, 17 (stack73)
        %v107674 = vshrl.u32 %v107667, 15 (stack74)
        %v107675 = vor.u32 %v107673, %v107674 (stack75)
        %v107676 = vxor.u32 %v107671, %v107675 (stack76)
        %v107679 = vadd.s32 %v107671, %v107676 (stack65)
        %v107681 = vshll.u32 %v107676, 29 (stack73)
        %v107682 = vshrl.u32 %v107676, 3 (stack74)
        %v107683 = vor.u32 %v107681, %v107682 (stack75)
        %v107684 = vxor.u32 %v107679, %v107683 (stack76)
        %v107687 = vadd.s32 %v107679, %v107684 (stack65)
        %v107689 = vshll.u32 %v107684, 16 (stack73)
        %v107690 = vshrl.u32 %v107684, 16 (stack74)
        %v107691 = vor.u32 %v107689, %v107690 (stack75)
        %v107692 = vxor.u32 %v107687, %v107691 (stack76)
        %v107695 = vadd.s32 %v107687, %v107692 (stack65)
        %v107699 = vadd.s32 %v107695, %v8 (stack65)
        %v107701 = vshll.u32 %v107692, 24 (stack73)
        %v107702 = vshrl.u32 %v107692, 8 (stack74)
        %v107703 = vor.u32 %v107701, %v107702 (stack75)
        %v107704 = vxor.u32 %v107695, %v107703 (stack76)
        %v107707 = vadd.s32 %v107704, %v10 (stack65)
        %v107711 = vadd.s32 %v107707, 2 (stack65)
        %v107715 = vadd.s32 %v107699, %v107711 (stack65)
        %v107717 = vshll.u32 %v107711, 13 (stack73)
        %v107718 = vshrl.u32 %v107711, 19 (stack74)
        %v107719 = vor.u32 %v107717, %v107718 (stack75)
        %v107720 = vxor.u32 %v107715, %v107719 (stack76)
        %v107723 = vadd.s32 %v107715, %v107720 (stack65)
        %v107725 = vshll.u32 %v107720, 15 (stack73)
        %v107726 = vshrl.u32 %v107720, 17 (stack74)
        %v107727 = vor.u32 %v107725, %v107726 (stack75)
        %v107728 = vxor.u32 %v107723, %v107727 (stack76)
        %v107731 = vadd.s32 %v107723, %v107728 (stack65)
        %v107733 = vshll.u32 %v107728, 26 (stack73)
        %v107734 = vshrl.u32 %v107728, 6 (stack74)
        %v107735 = vor.u32 %v107733, %v107734 (stack75)
        %v107736 = vxor.u32 %v107731, %v107735 (stack76)
        %v107739 = vadd.s32 %v107731, %v107736 (stack65)
        %v107743 = vadd.s32 %v107739, %v10 (stack65)
        %v107745 = vshll.u32 %v107736, 6 (stack73)
        %v107746 = vshrl.u32 %v107736, 26 (stack74)
        %v107747 = vor.u32 %v107745, %v107746 (stack75)
        %v107748 = vxor.u32 %v107739, %v107747 (stack76)
        %v107751 = vadd.s32 %v107748, %v9 (stack65)
        %v107755 = vadd.s32 %v107751, 3 (stack65)
        %v107759 = vadd.s32 %v107743, %v107755 (stack65)
        %v107761 = vshll.u32 %v107755, 17 (stack73)
        %v107762 = vshrl.u32 %v107755, 15 (stack74)
        %v107763 = vor.u32 %v107761, %v107762 (stack75)
        %v107764 = vxor.u32 %v107759, %v107763 (stack76)
        %v107767 = vadd.s32 %v107759, %v107764 (stack65)
        %v107769 = vshll.u32 %v107764, 29 (stack73)
        %v107770 = vshrl.u32 %v107764, 3 (stack74)
        %v107771 = vor.u32 %v107769, %v107770 (stack75)
        %v107772 = vxor.u32 %v107767, %v107771 (stack76)
        %v107775 = vadd.s32 %v107767, %v107772 (stack65)
        %v107777 = vshll.u32 %v107772, 16 (stack73)
        %v107778 = vshrl.u32 %v107772, 16 (stack74)
        %v107779 = vor.u32 %v107777, %v107778 (stack75)
        %v107780 = vxor.u32 %v107775, %v107779 (stack76)
        %v107783 = vadd.s32 %v107775, %v107780 (stack65)
        %v107787 = vadd.s32 %v107783, %v9 (stack65)
        %v107789 = vshll.u32 %v107780, 24 (stack73)
        %v107790 = vshrl.u32 %v107780, 8 (stack74)
        %v107791 = vor.u32 %v107789, %v107790 (stack75)
        %v107792 = vxor.u32 %v107783, %v107791 (stack76)
        %v107795 = vadd.s32 %v107792, %v8 (stack65)
        %v107799 = vadd.s32 %v107795, 4 (stack65)
        %v107803 = vadd.s32 %v107787, %v107799 (stack65)
        %v107805 = vshll.u32 %v107799, 13 (stack73)
        %v107806 = vshrl.u32 %v107799, 19 (stack74)
        %v107807 = vor.u32 %v107805, %v107806 (stack75)
        %v107808 = vxor.u32 %v107803, %v107807 (stack76)
        %v107811 = vadd.s32 %v107803, %v107808 (stack65)
        %v107813 = vshll.u32 %v107808, 15 (stack73)
        %v107814 = vshrl.u32 %v107808, 17 (stack74)
        %v107815 = vor.u32 %v107813, %v107814 (stack75)
        %v107816 = vxor.u32 %v107811, %v107815 (stack76)
        %v107819 = vadd.s32 %v107811, %v107816 (stack65)
        %v107821 = vshll.u32 %v107816, 26 (stack73)
        %v107822 = vshrl.u32 %v107816, 6 (stack74)
        %v107823 = vor.u32 %v107821, %v107822 (stack75)
        %v107824 = vxor.u32 %v107819, %v107823 (stack76)
        %v107827 = vadd.s32 %v107819, %v107824 (stack65)
        %v107831 = vadd.s32 %v107827, %v8 (stack65)
        %v107833 = vshll.u32 %v107824, 6 (stack73)
        %v107834 = vshrl.u32 %v107824, 26 (stack74)
        %v107835 = vor.u32 %v107833, %v107834 (stack75)
        %v107836 = vxor.u32 %v107827, %v107835 (stack76)
        %v107839 = vadd.s32 %v107836, %v10 (stack65)
        %v107843 = vadd.s32 %v107839, 5 (stack65)
        %v107845 = vxor.u32 %v107831, %v107843 (stack76)
        %v107846 = vand.u32.u8 %v107845, 255 (stack77)
        %v107847 = vand.u32 %v107846, 65535 (stack78)
        %v107848 = vshrl.u32 %v107847, 1 (stack79)
        %v107849 = vor.u32 %v107848, 16256 (stack75)
        %v107850 = vand.u32.u16 %v107849, 65535 (stack80)
        %v107851 = vunpack.i.l.bf16 %v107850 (stack81)
        %v107855 = vadd.f32 %v107851, -1.0 (stack82)
        %v107859 = vmul.f32 %v107855, 2.0 (stack83)
        %v107863 = vadd.f32 %v107859, -0.99609375 (stack82)
        %v107867 = vmax.f32 -0.99609375, %v107863 (stack84)
        %v107869 = vand.u32 2147483647, %v107867 (stack85)
        %vm107872 = vcmp.eq.f32.partialorder %v107869, 1.0 (stack86)
        %v107877 = vmul.f32 %v107867, inf (stack83)
        %v107879 = vxor.u32 %v107867, 2147483648 (stack87)
        %v107882 = vmul.f32 %v107867, %v107879 (stack83)
        %v107884 = vadd.f32 %v107882, 1.0 (stack88)
        %v107885 = vlog2.pop %v107884 (stack89)
        %v107886 = vmul.f32 %v107885, 0.6931472 (stack90)
        %v107887 = vmul.f32 -0.5, %v107882 (stack91)
        %v107888 = vadd.f32 %v107887, 1.0 (stack92)
        %v107889 = vmul.f32 %v107888, %v107882 (stack93)
        %v107890 = vand.u32 2147483647, %v107882 (stack94)
        %vm107891 = vcmp.lt.f32.partialorder %v107890, 0.0004427343 (stack95)
        %v107892 = vsel /*vm=*/%vm107891, /*on_true_vy=*/%v107889, /*on_false_vx=*/%v107886 (stack96)
        %v107893 = vxor.u32 %v107892, 2147483648 (stack87)
        %vm107896 = vcmp.lt.f32.partialorder %v107893, 5.0 (stack86)
        %v107901 = vsel /*vm=*/%vm107896, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v107905 = vsel /*vm=*/%vm107896, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v107909 = vsel /*vm=*/%vm107896, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v107913 = vsel /*vm=*/%vm107896, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v107917 = vsel /*vm=*/%vm107896, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v107921 = vsel /*vm=*/%vm107896, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v107925 = vsel /*vm=*/%vm107896, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v107929 = vsel /*vm=*/%vm107896, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v107933 = vsel /*vm=*/%vm107896, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v107937 = vadd.f32 %v107893, -2.5 (stack82)
        %v107939 = vrsqrt.pop %v107893 (stack97)
        %v107940 = vmul.f32 %v107893, %v107939 (stack98)
        %vm107941 = vcmp.eq.f32.partialorder %v107893, inf (stack99)
        %v107942 = vsel /*vm=*/%vm107941, /*on_true_vy=*/%v107893, /*on_false_vx=*/%v107940 (stack100)
        %vm107943 = vcmp.eq.f32.partialorder %v107893, 0.0 (stack101)
        %v107944 = vand.u32 %v107893, 2147483648 (stack102)
        %v107945 = vsel /*vm=*/%vm107943, /*on_true_vy=*/%v107944, /*on_false_vx=*/%v107942 (stack103)
        %v107948 = vadd.f32 %v107945, -3.0 (stack82)
        %v107952 = vsel /*vm=*/%vm107896, /*on_true_vy=*/%v107937, /*on_false_vx=*/%v107948 (stack72)
        %v107956 = vmul.f32 %v107933, %v107952 (stack83)
        %v107960 = vadd.f32 %v107929, %v107956 (stack82)
        %v107964 = vmul.f32 %v107960, %v107952 (stack83)
        %v107968 = vadd.f32 %v107925, %v107964 (stack82)
        %v107972 = vmul.f32 %v107968, %v107952 (stack83)
        %v107976 = vadd.f32 %v107921, %v107972 (stack82)
        %v107980 = vmul.f32 %v107976, %v107952 (stack83)
        %v107984 = vadd.f32 %v107917, %v107980 (stack82)
        %v107988 = vmul.f32 %v107984, %v107952 (stack83)
        %v107992 = vadd.f32 %v107913, %v107988 (stack82)
        %v107996 = vmul.f32 %v107992, %v107952 (stack83)
        %v108000 = vadd.f32 %v107909, %v107996 (stack82)
        %v108004 = vmul.f32 %v108000, %v107952 (stack83)
        %v108008 = vadd.f32 %v107905, %v108004 (stack82)
        %v108012 = vmul.f32 %v108008, %v107952 (stack83)
        %v108016 = vadd.f32 %v107901, %v108012 (stack82)
        %v108020 = vmul.f32 %v108016, %v107867 (stack83)
        %v108024 = vsel /*vm=*/%vm107872, /*on_true_vy=*/%v107877, /*on_false_vx=*/%v108020 (stack72)
        %v108028 = vmul.f32 %v108024, 1.4140625 (stack83)
        %s108030 = scalar_lea.vmem %s280, 880 [#allocation0] (stack107)
        %v108031 = vpack.c.bf16 0.0, %v108028 (stack104)
        %108032 = vst [vmem:[%s108030] sm:$0xf] /*vst_source=*/%v108031 (stack105)
        %v108035 = vadd.s32 %v3816, %v104805 (stack65)
        %s108037 = smul.u32 128, %s27 (stack66)
        %v108038 = vlaneseq (stack67)
        %v108039 = vand.u32 %v108038, 127 (stack68)
        %v108040 = vstv %s108037 (stack69)
        %v108041 = vadd.s32 %v108039, %v108040 (stack70)
        %v108045 = vadd.s32 %v108035, %v108041 (stack65)
        %vm108049 = vcmp.lt.u32.totalorder %v108045, %v108035 (stack71)
        %vm108054 = vcmp.lt.u32.totalorder %v108035, %v3816 (stack71)
        %v108059 = vadd.s32 %v3803, %v104788 (stack65)
        %v108063 = vadd.s32 %v108059, 1 (stack65)
        %v108067 = vsel /*vm=*/%vm108054, /*on_true_vy=*/%v108063, /*on_false_vx=*/%v108059 (stack72)
        %v108071 = vadd.s32 %v108067, 1 (stack65)
        %v108075 = vsel /*vm=*/%vm108049, /*on_true_vy=*/%v108071, /*on_false_vx=*/%v108067 (stack72)
        %v108080 = vadd.s32 %v108075, %v10 (stack65)
        %v108084 = vadd.s32 %v108045, %v9 (stack65)
        %v108088 = vadd.s32 %v108080, %v108084 (stack65)
        %v108090 = vshll.u32 %v108084, 13 (stack73)
        %v108091 = vshrl.u32 %v108084, 19 (stack74)
        %v108092 = vor.u32 %v108090, %v108091 (stack75)
        %v108093 = vxor.u32 %v108088, %v108092 (stack76)
        %v108096 = vadd.s32 %v108088, %v108093 (stack65)
        %v108098 = vshll.u32 %v108093, 15 (stack73)
        %v108099 = vshrl.u32 %v108093, 17 (stack74)
        %v108100 = vor.u32 %v108098, %v108099 (stack75)
        %v108101 = vxor.u32 %v108096, %v108100 (stack76)
        %v108104 = vadd.s32 %v108096, %v108101 (stack65)
        %v108106 = vshll.u32 %v108101, 26 (stack73)
        %v108107 = vshrl.u32 %v108101, 6 (stack74)
        %v108108 = vor.u32 %v108106, %v108107 (stack75)
        %v108109 = vxor.u32 %v108104, %v108108 (stack76)
        %v108112 = vadd.s32 %v108104, %v108109 (stack65)
        %v108116 = vadd.s32 %v108112, %v9 (stack65)
        %v108118 = vshll.u32 %v108109, 6 (stack73)
        %v108119 = vshrl.u32 %v108109, 26 (stack74)
        %v108120 = vor.u32 %v108118, %v108119 (stack75)
        %v108121 = vxor.u32 %v108112, %v108120 (stack76)
        %v108124 = vadd.s32 %v108121, %v8 (stack65)
        %v108128 = vadd.s32 %v108124, 1 (stack65)
        %v108132 = vadd.s32 %v108116, %v108128 (stack65)
        %v108134 = vshll.u32 %v108128, 17 (stack73)
        %v108135 = vshrl.u32 %v108128, 15 (stack74)
        %v108136 = vor.u32 %v108134, %v108135 (stack75)
        %v108137 = vxor.u32 %v108132, %v108136 (stack76)
        %v108140 = vadd.s32 %v108132, %v108137 (stack65)
        %v108142 = vshll.u32 %v108137, 29 (stack73)
        %v108143 = vshrl.u32 %v108137, 3 (stack74)
        %v108144 = vor.u32 %v108142, %v108143 (stack75)
        %v108145 = vxor.u32 %v108140, %v108144 (stack76)
        %v108148 = vadd.s32 %v108140, %v108145 (stack65)
        %v108150 = vshll.u32 %v108145, 16 (stack73)
        %v108151 = vshrl.u32 %v108145, 16 (stack74)
        %v108152 = vor.u32 %v108150, %v108151 (stack75)
        %v108153 = vxor.u32 %v108148, %v108152 (stack76)
        %v108156 = vadd.s32 %v108148, %v108153 (stack65)
        %v108160 = vadd.s32 %v108156, %v8 (stack65)
        %v108162 = vshll.u32 %v108153, 24 (stack73)
        %v108163 = vshrl.u32 %v108153, 8 (stack74)
        %v108164 = vor.u32 %v108162, %v108163 (stack75)
        %v108165 = vxor.u32 %v108156, %v108164 (stack76)
        %v108168 = vadd.s32 %v108165, %v10 (stack65)
        %v108172 = vadd.s32 %v108168, 2 (stack65)
        %v108176 = vadd.s32 %v108160, %v108172 (stack65)
        %v108178 = vshll.u32 %v108172, 13 (stack73)
        %v108179 = vshrl.u32 %v108172, 19 (stack74)
        %v108180 = vor.u32 %v108178, %v108179 (stack75)
        %v108181 = vxor.u32 %v108176, %v108180 (stack76)
        %v108184 = vadd.s32 %v108176, %v108181 (stack65)
        %v108186 = vshll.u32 %v108181, 15 (stack73)
        %v108187 = vshrl.u32 %v108181, 17 (stack74)
        %v108188 = vor.u32 %v108186, %v108187 (stack75)
        %v108189 = vxor.u32 %v108184, %v108188 (stack76)
        %v108192 = vadd.s32 %v108184, %v108189 (stack65)
        %v108194 = vshll.u32 %v108189, 26 (stack73)
        %v108195 = vshrl.u32 %v108189, 6 (stack74)
        %v108196 = vor.u32 %v108194, %v108195 (stack75)
        %v108197 = vxor.u32 %v108192, %v108196 (stack76)
        %v108200 = vadd.s32 %v108192, %v108197 (stack65)
        %v108204 = vadd.s32 %v108200, %v10 (stack65)
        %v108206 = vshll.u32 %v108197, 6 (stack73)
        %v108207 = vshrl.u32 %v108197, 26 (stack74)
        %v108208 = vor.u32 %v108206, %v108207 (stack75)
        %v108209 = vxor.u32 %v108200, %v108208 (stack76)
        %v108212 = vadd.s32 %v108209, %v9 (stack65)
        %v108216 = vadd.s32 %v108212, 3 (stack65)
        %v108220 = vadd.s32 %v108204, %v108216 (stack65)
        %v108222 = vshll.u32 %v108216, 17 (stack73)
        %v108223 = vshrl.u32 %v108216, 15 (stack74)
        %v108224 = vor.u32 %v108222, %v108223 (stack75)
        %v108225 = vxor.u32 %v108220, %v108224 (stack76)
        %v108228 = vadd.s32 %v108220, %v108225 (stack65)
        %v108230 = vshll.u32 %v108225, 29 (stack73)
        %v108231 = vshrl.u32 %v108225, 3 (stack74)
        %v108232 = vor.u32 %v108230, %v108231 (stack75)
        %v108233 = vxor.u32 %v108228, %v108232 (stack76)
        %v108236 = vadd.s32 %v108228, %v108233 (stack65)
        %v108238 = vshll.u32 %v108233, 16 (stack73)
        %v108239 = vshrl.u32 %v108233, 16 (stack74)
        %v108240 = vor.u32 %v108238, %v108239 (stack75)
        %v108241 = vxor.u32 %v108236, %v108240 (stack76)
        %v108244 = vadd.s32 %v108236, %v108241 (stack65)
        %v108248 = vadd.s32 %v108244, %v9 (stack65)
        %v108250 = vshll.u32 %v108241, 24 (stack73)
        %v108251 = vshrl.u32 %v108241, 8 (stack74)
        %v108252 = vor.u32 %v108250, %v108251 (stack75)
        %v108253 = vxor.u32 %v108244, %v108252 (stack76)
        %v108256 = vadd.s32 %v108253, %v8 (stack65)
        %v108260 = vadd.s32 %v108256, 4 (stack65)
        %v108264 = vadd.s32 %v108248, %v108260 (stack65)
        %v108266 = vshll.u32 %v108260, 13 (stack73)
        %v108267 = vshrl.u32 %v108260, 19 (stack74)
        %v108268 = vor.u32 %v108266, %v108267 (stack75)
        %v108269 = vxor.u32 %v108264, %v108268 (stack76)
        %v108272 = vadd.s32 %v108264, %v108269 (stack65)
        %v108274 = vshll.u32 %v108269, 15 (stack73)
        %v108275 = vshrl.u32 %v108269, 17 (stack74)
        %v108276 = vor.u32 %v108274, %v108275 (stack75)
        %v108277 = vxor.u32 %v108272, %v108276 (stack76)
        %v108280 = vadd.s32 %v108272, %v108277 (stack65)
        %v108282 = vshll.u32 %v108277, 26 (stack73)
        %v108283 = vshrl.u32 %v108277, 6 (stack74)
        %v108284 = vor.u32 %v108282, %v108283 (stack75)
        %v108285 = vxor.u32 %v108280, %v108284 (stack76)
        %v108288 = vadd.s32 %v108280, %v108285 (stack65)
        %v108292 = vadd.s32 %v108288, %v8 (stack65)
        %v108294 = vshll.u32 %v108285, 6 (stack73)
        %v108295 = vshrl.u32 %v108285, 26 (stack74)
        %v108296 = vor.u32 %v108294, %v108295 (stack75)
        %v108297 = vxor.u32 %v108288, %v108296 (stack76)
        %v108300 = vadd.s32 %v108297, %v10 (stack65)
        %v108304 = vadd.s32 %v108300, 5 (stack65)
        %v108306 = vxor.u32 %v108292, %v108304 (stack76)
        %v108307 = vand.u32.u8 %v108306, 255 (stack77)
        %v108308 = vand.u32 %v108307, 65535 (stack78)
        %v108309 = vshrl.u32 %v108308, 1 (stack79)
        %v108310 = vor.u32 %v108309, 16256 (stack75)
        %v108311 = vand.u32.u16 %v108310, 65535 (stack80)
        %v108312 = vunpack.i.l.bf16 %v108311 (stack81)
        %v108316 = vadd.f32 %v108312, -1.0 (stack82)
        %v108320 = vmul.f32 %v108316, 2.0 (stack83)
        %v108324 = vadd.f32 %v108320, -0.99609375 (stack82)
        %v108328 = vmax.f32 -0.99609375, %v108324 (stack84)
        %v108330 = vand.u32 2147483647, %v108328 (stack85)
        %vm108333 = vcmp.eq.f32.partialorder %v108330, 1.0 (stack86)
        %v108338 = vmul.f32 %v108328, inf (stack83)
        %v108340 = vxor.u32 %v108328, 2147483648 (stack87)
        %v108343 = vmul.f32 %v108328, %v108340 (stack83)
        %v108345 = vadd.f32 %v108343, 1.0 (stack88)
        %v108346 = vlog2.pop %v108345 (stack89)
        %v108347 = vmul.f32 %v108346, 0.6931472 (stack90)
        %v108348 = vmul.f32 -0.5, %v108343 (stack91)
        %v108349 = vadd.f32 %v108348, 1.0 (stack92)
        %v108350 = vmul.f32 %v108349, %v108343 (stack93)
        %v108351 = vand.u32 2147483647, %v108343 (stack94)
        %vm108352 = vcmp.lt.f32.partialorder %v108351, 0.0004427343 (stack95)
        %v108353 = vsel /*vm=*/%vm108352, /*on_true_vy=*/%v108350, /*on_false_vx=*/%v108347 (stack96)
        %v108354 = vxor.u32 %v108353, 2147483648 (stack87)
        %vm108357 = vcmp.lt.f32.partialorder %v108354, 5.0 (stack86)
        %v108362 = vsel /*vm=*/%vm108357, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v108366 = vsel /*vm=*/%vm108357, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v108370 = vsel /*vm=*/%vm108357, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v108374 = vsel /*vm=*/%vm108357, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v108378 = vsel /*vm=*/%vm108357, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v108382 = vsel /*vm=*/%vm108357, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v108386 = vsel /*vm=*/%vm108357, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v108390 = vsel /*vm=*/%vm108357, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v108394 = vsel /*vm=*/%vm108357, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v108398 = vadd.f32 %v108354, -2.5 (stack82)
        %v108400 = vrsqrt.pop %v108354 (stack97)
        %v108401 = vmul.f32 %v108354, %v108400 (stack98)
        %vm108402 = vcmp.eq.f32.partialorder %v108354, inf (stack99)
        %v108403 = vsel /*vm=*/%vm108402, /*on_true_vy=*/%v108354, /*on_false_vx=*/%v108401 (stack100)
        %vm108404 = vcmp.eq.f32.partialorder %v108354, 0.0 (stack101)
        %v108405 = vand.u32 %v108354, 2147483648 (stack102)
        %v108406 = vsel /*vm=*/%vm108404, /*on_true_vy=*/%v108405, /*on_false_vx=*/%v108403 (stack103)
        %v108409 = vadd.f32 %v108406, -3.0 (stack82)
        %v108413 = vsel /*vm=*/%vm108357, /*on_true_vy=*/%v108398, /*on_false_vx=*/%v108409 (stack72)
        %v108417 = vmul.f32 %v108394, %v108413 (stack83)
        %v108421 = vadd.f32 %v108390, %v108417 (stack82)
        %v108425 = vmul.f32 %v108421, %v108413 (stack83)
        %v108429 = vadd.f32 %v108386, %v108425 (stack82)
        %v108433 = vmul.f32 %v108429, %v108413 (stack83)
        %v108437 = vadd.f32 %v108382, %v108433 (stack82)
        %v108441 = vmul.f32 %v108437, %v108413 (stack83)
        %v108445 = vadd.f32 %v108378, %v108441 (stack82)
        %v108449 = vmul.f32 %v108445, %v108413 (stack83)
        %v108453 = vadd.f32 %v108374, %v108449 (stack82)
        %v108457 = vmul.f32 %v108453, %v108413 (stack83)
        %v108461 = vadd.f32 %v108370, %v108457 (stack82)
        %v108465 = vmul.f32 %v108461, %v108413 (stack83)
        %v108469 = vadd.f32 %v108366, %v108465 (stack82)
        %v108473 = vmul.f32 %v108469, %v108413 (stack83)
        %v108477 = vadd.f32 %v108362, %v108473 (stack82)
        %v108481 = vmul.f32 %v108477, %v108328 (stack83)
        %v108485 = vsel /*vm=*/%vm108333, /*on_true_vy=*/%v108338, /*on_false_vx=*/%v108481 (stack72)
        %v108489 = vmul.f32 %v108485, 1.4140625 (stack83)
        %s108491 = scalar_lea.vmem %s280, 1008 [#allocation0] (stack107)
        %v108492 = vpack.c.bf16 0.0, %v108489 (stack104)
        %108493 = vst [vmem:[%s108491] sm:$0xf] /*vst_source=*/%v108492 (stack105)
        %s108494 = sadd.s32 %s339, 232 (stack106)
        %s108495 = sshrl.u32 %s108494, 10 (stack49)
        %p108496 = scmp.lt.s32.totalorder 1, %s108495 (stack50)
        %s108497 = scalar_select /*predicate=*/%p108496, /*on_true=*/1, /*on_false=*/%s108495 (stack51)
        %s108498 = sand.u32 %s108494, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s108499 = sshrl.u32 %s108498, 7 (stack53)
        %s108500 = sand.u32 %s108498, 127 /* smod.u32 w/div 128 */ (stack54)
        %s108501 = smul.addr %s108497, 8 (stack55)
        %s108502 = scalar_lea.vmem %s3, %s108501 (stack56)
        %s108504 = scalar_lea.vmem %s108502, %s108499 (stack57)
        %v108505 = vld [vmem:[%s108504] ss:$0 sm:$0xff] (stack58)
        %s108506 = sand.u32 %s108500, 255 (stack59)
        %s108508 = sor.u32 256, %s108506 (stack60)
        %108509 = vbcast.lane.b32.xlu0 %v108505, %s108508 (stack61)
        %v108510 = vpop.permute.xlu0 %108509 (stack62)
        %s108511 = sadd.s32 %s347, 232 (stack106)
        %s108512 = sshrl.u32 %s108511, 10 (stack49)
        %p108513 = scmp.lt.s32.totalorder 1, %s108512 (stack50)
        %s108514 = scalar_select /*predicate=*/%p108513, /*on_true=*/1, /*on_false=*/%s108512 (stack51)
        %s108515 = sand.u32 %s108511, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s108516 = sshrl.u32 %s108515, 7 (stack53)
        %s108517 = sand.u32 %s108515, 127 /* smod.u32 w/div 128 */ (stack54)
        %s108518 = smul.addr %s108514, 8 (stack55)
        %s108519 = scalar_lea.vmem %s5, %s108518 (stack56)
        %s108521 = scalar_lea.vmem %s108519, %s108516 (stack57)
        %v108522 = vld [vmem:[%s108521] ss:$0 sm:$0xff] (stack58)
        %s108523 = sand.u32 %s108517, 255 (stack59)
        %s108525 = sor.u32 256, %s108523 (stack60)
        %108526 = vbcast.lane.b32.xlu0 %v108522, %s108525 (stack61)
        %v108527 = vpop.permute.xlu0 %108526 (stack62)
        %v108530 = vadd.s32 %v408, %v108527 (stack65)
        %s108532 = smul.u32 128, %s27 (stack66)
        %v108533 = vlaneseq (stack67)
        %v108534 = vand.u32 %v108533, 127 (stack68)
        %v108535 = vstv %s108532 (stack69)
        %v108536 = vadd.s32 %v108534, %v108535 (stack70)
        %v108540 = vadd.s32 %v108530, %v108536 (stack65)
        %vm108544 = vcmp.lt.u32.totalorder %v108540, %v108530 (stack71)
        %vm108549 = vcmp.lt.u32.totalorder %v108530, %v408 (stack71)
        %v108554 = vadd.s32 %v380, %v108510 (stack65)
        %v108558 = vadd.s32 %v108554, 1 (stack65)
        %v108562 = vsel /*vm=*/%vm108549, /*on_true_vy=*/%v108558, /*on_false_vx=*/%v108554 (stack72)
        %v108566 = vadd.s32 %v108562, 1 (stack65)
        %v108570 = vsel /*vm=*/%vm108544, /*on_true_vy=*/%v108566, /*on_false_vx=*/%v108562 (stack72)
        %v108575 = vadd.s32 %v108570, %v10 (stack65)
        %v108579 = vadd.s32 %v108540, %v9 (stack65)
        %v108583 = vadd.s32 %v108575, %v108579 (stack65)
        %v108585 = vshll.u32 %v108579, 13 (stack73)
        %v108586 = vshrl.u32 %v108579, 19 (stack74)
        %v108587 = vor.u32 %v108585, %v108586 (stack75)
        %v108588 = vxor.u32 %v108583, %v108587 (stack76)
        %v108591 = vadd.s32 %v108583, %v108588 (stack65)
        %v108593 = vshll.u32 %v108588, 15 (stack73)
        %v108594 = vshrl.u32 %v108588, 17 (stack74)
        %v108595 = vor.u32 %v108593, %v108594 (stack75)
        %v108596 = vxor.u32 %v108591, %v108595 (stack76)
        %v108599 = vadd.s32 %v108591, %v108596 (stack65)
        %v108601 = vshll.u32 %v108596, 26 (stack73)
        %v108602 = vshrl.u32 %v108596, 6 (stack74)
        %v108603 = vor.u32 %v108601, %v108602 (stack75)
        %v108604 = vxor.u32 %v108599, %v108603 (stack76)
        %v108607 = vadd.s32 %v108599, %v108604 (stack65)
        %v108611 = vadd.s32 %v108607, %v9 (stack65)
        %v108613 = vshll.u32 %v108604, 6 (stack73)
        %v108614 = vshrl.u32 %v108604, 26 (stack74)
        %v108615 = vor.u32 %v108613, %v108614 (stack75)
        %v108616 = vxor.u32 %v108607, %v108615 (stack76)
        %v108619 = vadd.s32 %v108616, %v8 (stack65)
        %v108623 = vadd.s32 %v108619, 1 (stack65)
        %v108627 = vadd.s32 %v108611, %v108623 (stack65)
        %v108629 = vshll.u32 %v108623, 17 (stack73)
        %v108630 = vshrl.u32 %v108623, 15 (stack74)
        %v108631 = vor.u32 %v108629, %v108630 (stack75)
        %v108632 = vxor.u32 %v108627, %v108631 (stack76)
        %v108635 = vadd.s32 %v108627, %v108632 (stack65)
        %v108637 = vshll.u32 %v108632, 29 (stack73)
        %v108638 = vshrl.u32 %v108632, 3 (stack74)
        %v108639 = vor.u32 %v108637, %v108638 (stack75)
        %v108640 = vxor.u32 %v108635, %v108639 (stack76)
        %v108643 = vadd.s32 %v108635, %v108640 (stack65)
        %v108645 = vshll.u32 %v108640, 16 (stack73)
        %v108646 = vshrl.u32 %v108640, 16 (stack74)
        %v108647 = vor.u32 %v108645, %v108646 (stack75)
        %v108648 = vxor.u32 %v108643, %v108647 (stack76)
        %v108651 = vadd.s32 %v108643, %v108648 (stack65)
        %v108655 = vadd.s32 %v108651, %v8 (stack65)
        %v108657 = vshll.u32 %v108648, 24 (stack73)
        %v108658 = vshrl.u32 %v108648, 8 (stack74)
        %v108659 = vor.u32 %v108657, %v108658 (stack75)
        %v108660 = vxor.u32 %v108651, %v108659 (stack76)
        %v108663 = vadd.s32 %v108660, %v10 (stack65)
        %v108667 = vadd.s32 %v108663, 2 (stack65)
        %v108671 = vadd.s32 %v108655, %v108667 (stack65)
        %v108673 = vshll.u32 %v108667, 13 (stack73)
        %v108674 = vshrl.u32 %v108667, 19 (stack74)
        %v108675 = vor.u32 %v108673, %v108674 (stack75)
        %v108676 = vxor.u32 %v108671, %v108675 (stack76)
        %v108679 = vadd.s32 %v108671, %v108676 (stack65)
        %v108681 = vshll.u32 %v108676, 15 (stack73)
        %v108682 = vshrl.u32 %v108676, 17 (stack74)
        %v108683 = vor.u32 %v108681, %v108682 (stack75)
        %v108684 = vxor.u32 %v108679, %v108683 (stack76)
        %v108687 = vadd.s32 %v108679, %v108684 (stack65)
        %v108689 = vshll.u32 %v108684, 26 (stack73)
        %v108690 = vshrl.u32 %v108684, 6 (stack74)
        %v108691 = vor.u32 %v108689, %v108690 (stack75)
        %v108692 = vxor.u32 %v108687, %v108691 (stack76)
        %v108695 = vadd.s32 %v108687, %v108692 (stack65)
        %v108699 = vadd.s32 %v108695, %v10 (stack65)
        %v108701 = vshll.u32 %v108692, 6 (stack73)
        %v108702 = vshrl.u32 %v108692, 26 (stack74)
        %v108703 = vor.u32 %v108701, %v108702 (stack75)
        %v108704 = vxor.u32 %v108695, %v108703 (stack76)
        %v108707 = vadd.s32 %v108704, %v9 (stack65)
        %v108711 = vadd.s32 %v108707, 3 (stack65)
        %v108715 = vadd.s32 %v108699, %v108711 (stack65)
        %v108717 = vshll.u32 %v108711, 17 (stack73)
        %v108718 = vshrl.u32 %v108711, 15 (stack74)
        %v108719 = vor.u32 %v108717, %v108718 (stack75)
        %v108720 = vxor.u32 %v108715, %v108719 (stack76)
        %v108723 = vadd.s32 %v108715, %v108720 (stack65)
        %v108725 = vshll.u32 %v108720, 29 (stack73)
        %v108726 = vshrl.u32 %v108720, 3 (stack74)
        %v108727 = vor.u32 %v108725, %v108726 (stack75)
        %v108728 = vxor.u32 %v108723, %v108727 (stack76)
        %v108731 = vadd.s32 %v108723, %v108728 (stack65)
        %v108733 = vshll.u32 %v108728, 16 (stack73)
        %v108734 = vshrl.u32 %v108728, 16 (stack74)
        %v108735 = vor.u32 %v108733, %v108734 (stack75)
        %v108736 = vxor.u32 %v108731, %v108735 (stack76)
        %v108739 = vadd.s32 %v108731, %v108736 (stack65)
        %v108743 = vadd.s32 %v108739, %v9 (stack65)
        %v108745 = vshll.u32 %v108736, 24 (stack73)
        %v108746 = vshrl.u32 %v108736, 8 (stack74)
        %v108747 = vor.u32 %v108745, %v108746 (stack75)
        %v108748 = vxor.u32 %v108739, %v108747 (stack76)
        %v108751 = vadd.s32 %v108748, %v8 (stack65)
        %v108755 = vadd.s32 %v108751, 4 (stack65)
        %v108759 = vadd.s32 %v108743, %v108755 (stack65)
        %v108761 = vshll.u32 %v108755, 13 (stack73)
        %v108762 = vshrl.u32 %v108755, 19 (stack74)
        %v108763 = vor.u32 %v108761, %v108762 (stack75)
        %v108764 = vxor.u32 %v108759, %v108763 (stack76)
        %v108767 = vadd.s32 %v108759, %v108764 (stack65)
        %v108769 = vshll.u32 %v108764, 15 (stack73)
        %v108770 = vshrl.u32 %v108764, 17 (stack74)
        %v108771 = vor.u32 %v108769, %v108770 (stack75)
        %v108772 = vxor.u32 %v108767, %v108771 (stack76)
        %v108775 = vadd.s32 %v108767, %v108772 (stack65)
        %v108777 = vshll.u32 %v108772, 26 (stack73)
        %v108778 = vshrl.u32 %v108772, 6 (stack74)
        %v108779 = vor.u32 %v108777, %v108778 (stack75)
        %v108780 = vxor.u32 %v108775, %v108779 (stack76)
        %v108783 = vadd.s32 %v108775, %v108780 (stack65)
        %v108787 = vadd.s32 %v108783, %v8 (stack65)
        %v108789 = vshll.u32 %v108780, 6 (stack73)
        %v108790 = vshrl.u32 %v108780, 26 (stack74)
        %v108791 = vor.u32 %v108789, %v108790 (stack75)
        %v108792 = vxor.u32 %v108783, %v108791 (stack76)
        %v108795 = vadd.s32 %v108792, %v10 (stack65)
        %v108799 = vadd.s32 %v108795, 5 (stack65)
        %v108801 = vxor.u32 %v108787, %v108799 (stack76)
        %v108802 = vand.u32.u8 %v108801, 255 (stack77)
        %v108803 = vand.u32 %v108802, 65535 (stack78)
        %v108804 = vshrl.u32 %v108803, 1 (stack79)
        %v108805 = vor.u32 %v108804, 16256 (stack75)
        %v108806 = vand.u32.u16 %v108805, 65535 (stack80)
        %v108807 = vunpack.i.l.bf16 %v108806 (stack81)
        %v108811 = vadd.f32 %v108807, -1.0 (stack82)
        %v108815 = vmul.f32 %v108811, 2.0 (stack83)
        %v108819 = vadd.f32 %v108815, -0.99609375 (stack82)
        %v108823 = vmax.f32 -0.99609375, %v108819 (stack84)
        %v108825 = vand.u32 2147483647, %v108823 (stack85)
        %vm108828 = vcmp.eq.f32.partialorder %v108825, 1.0 (stack86)
        %v108833 = vmul.f32 %v108823, inf (stack83)
        %v108835 = vxor.u32 %v108823, 2147483648 (stack87)
        %v108838 = vmul.f32 %v108823, %v108835 (stack83)
        %v108840 = vadd.f32 %v108838, 1.0 (stack88)
        %v108841 = vlog2.pop %v108840 (stack89)
        %v108842 = vmul.f32 %v108841, 0.6931472 (stack90)
        %v108843 = vmul.f32 -0.5, %v108838 (stack91)
        %v108844 = vadd.f32 %v108843, 1.0 (stack92)
        %v108845 = vmul.f32 %v108844, %v108838 (stack93)
        %v108846 = vand.u32 2147483647, %v108838 (stack94)
        %vm108847 = vcmp.lt.f32.partialorder %v108846, 0.0004427343 (stack95)
        %v108848 = vsel /*vm=*/%vm108847, /*on_true_vy=*/%v108845, /*on_false_vx=*/%v108842 (stack96)
        %v108849 = vxor.u32 %v108848, 2147483648 (stack87)
        %vm108852 = vcmp.lt.f32.partialorder %v108849, 5.0 (stack86)
        %v108857 = vsel /*vm=*/%vm108852, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v108861 = vsel /*vm=*/%vm108852, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v108865 = vsel /*vm=*/%vm108852, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v108869 = vsel /*vm=*/%vm108852, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v108873 = vsel /*vm=*/%vm108852, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v108877 = vsel /*vm=*/%vm108852, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v108881 = vsel /*vm=*/%vm108852, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v108885 = vsel /*vm=*/%vm108852, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v108889 = vsel /*vm=*/%vm108852, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v108893 = vadd.f32 %v108849, -2.5 (stack82)
        %v108895 = vrsqrt.pop %v108849 (stack97)
        %v108896 = vmul.f32 %v108849, %v108895 (stack98)
        %vm108897 = vcmp.eq.f32.partialorder %v108849, inf (stack99)
        %v108898 = vsel /*vm=*/%vm108897, /*on_true_vy=*/%v108849, /*on_false_vx=*/%v108896 (stack100)
        %vm108899 = vcmp.eq.f32.partialorder %v108849, 0.0 (stack101)
        %v108900 = vand.u32 %v108849, 2147483648 (stack102)
        %v108901 = vsel /*vm=*/%vm108899, /*on_true_vy=*/%v108900, /*on_false_vx=*/%v108898 (stack103)
        %v108904 = vadd.f32 %v108901, -3.0 (stack82)
        %v108908 = vsel /*vm=*/%vm108852, /*on_true_vy=*/%v108893, /*on_false_vx=*/%v108904 (stack72)
        %v108912 = vmul.f32 %v108889, %v108908 (stack83)
        %v108916 = vadd.f32 %v108885, %v108912 (stack82)
        %v108920 = vmul.f32 %v108916, %v108908 (stack83)
        %v108924 = vadd.f32 %v108881, %v108920 (stack82)
        %v108928 = vmul.f32 %v108924, %v108908 (stack83)
        %v108932 = vadd.f32 %v108877, %v108928 (stack82)
        %v108936 = vmul.f32 %v108932, %v108908 (stack83)
        %v108940 = vadd.f32 %v108873, %v108936 (stack82)
        %v108944 = vmul.f32 %v108940, %v108908 (stack83)
        %v108948 = vadd.f32 %v108869, %v108944 (stack82)
        %v108952 = vmul.f32 %v108948, %v108908 (stack83)
        %v108956 = vadd.f32 %v108865, %v108952 (stack82)
        %v108960 = vmul.f32 %v108956, %v108908 (stack83)
        %v108964 = vadd.f32 %v108861, %v108960 (stack82)
        %v108968 = vmul.f32 %v108964, %v108908 (stack83)
        %v108972 = vadd.f32 %v108857, %v108968 (stack82)
        %v108976 = vmul.f32 %v108972, %v108823 (stack83)
        %v108980 = vsel /*vm=*/%vm108828, /*on_true_vy=*/%v108833, /*on_false_vx=*/%v108976 (stack72)
        %v108984 = vmul.f32 %v108980, 1.4140625 (stack83)
        %s108986 = scalar_lea.vmem %s280, 116 [#allocation0] (stack107)
        %v108987 = vpack.c.bf16 0.0, %v108984 (stack104)
        %108988 = vst [vmem:[%s108986] sm:$0xf] /*vst_source=*/%v108987 (stack105)
        %v108991 = vadd.s32 %v894, %v108527 (stack65)
        %s108993 = smul.u32 128, %s27 (stack66)
        %v108994 = vlaneseq (stack67)
        %v108995 = vand.u32 %v108994, 127 (stack68)
        %v108996 = vstv %s108993 (stack69)
        %v108997 = vadd.s32 %v108995, %v108996 (stack70)
        %v109001 = vadd.s32 %v108991, %v108997 (stack65)
        %vm109005 = vcmp.lt.u32.totalorder %v109001, %v108991 (stack71)
        %vm109010 = vcmp.lt.u32.totalorder %v108991, %v894 (stack71)
        %v109015 = vadd.s32 %v881, %v108510 (stack65)
        %v109019 = vadd.s32 %v109015, 1 (stack65)
        %v109023 = vsel /*vm=*/%vm109010, /*on_true_vy=*/%v109019, /*on_false_vx=*/%v109015 (stack72)
        %v109027 = vadd.s32 %v109023, 1 (stack65)
        %v109031 = vsel /*vm=*/%vm109005, /*on_true_vy=*/%v109027, /*on_false_vx=*/%v109023 (stack72)
        %v109036 = vadd.s32 %v109031, %v10 (stack65)
        %v109040 = vadd.s32 %v109001, %v9 (stack65)
        %v109044 = vadd.s32 %v109036, %v109040 (stack65)
        %v109046 = vshll.u32 %v109040, 13 (stack73)
        %v109047 = vshrl.u32 %v109040, 19 (stack74)
        %v109048 = vor.u32 %v109046, %v109047 (stack75)
        %v109049 = vxor.u32 %v109044, %v109048 (stack76)
        %v109052 = vadd.s32 %v109044, %v109049 (stack65)
        %v109054 = vshll.u32 %v109049, 15 (stack73)
        %v109055 = vshrl.u32 %v109049, 17 (stack74)
        %v109056 = vor.u32 %v109054, %v109055 (stack75)
        %v109057 = vxor.u32 %v109052, %v109056 (stack76)
        %v109060 = vadd.s32 %v109052, %v109057 (stack65)
        %v109062 = vshll.u32 %v109057, 26 (stack73)
        %v109063 = vshrl.u32 %v109057, 6 (stack74)
        %v109064 = vor.u32 %v109062, %v109063 (stack75)
        %v109065 = vxor.u32 %v109060, %v109064 (stack76)
        %v109068 = vadd.s32 %v109060, %v109065 (stack65)
        %v109072 = vadd.s32 %v109068, %v9 (stack65)
        %v109074 = vshll.u32 %v109065, 6 (stack73)
        %v109075 = vshrl.u32 %v109065, 26 (stack74)
        %v109076 = vor.u32 %v109074, %v109075 (stack75)
        %v109077 = vxor.u32 %v109068, %v109076 (stack76)
        %v109080 = vadd.s32 %v109077, %v8 (stack65)
        %v109084 = vadd.s32 %v109080, 1 (stack65)
        %v109088 = vadd.s32 %v109072, %v109084 (stack65)
        %v109090 = vshll.u32 %v109084, 17 (stack73)
        %v109091 = vshrl.u32 %v109084, 15 (stack74)
        %v109092 = vor.u32 %v109090, %v109091 (stack75)
        %v109093 = vxor.u32 %v109088, %v109092 (stack76)
        %v109096 = vadd.s32 %v109088, %v109093 (stack65)
        %v109098 = vshll.u32 %v109093, 29 (stack73)
        %v109099 = vshrl.u32 %v109093, 3 (stack74)
        %v109100 = vor.u32 %v109098, %v109099 (stack75)
        %v109101 = vxor.u32 %v109096, %v109100 (stack76)
        %v109104 = vadd.s32 %v109096, %v109101 (stack65)
        %v109106 = vshll.u32 %v109101, 16 (stack73)
        %v109107 = vshrl.u32 %v109101, 16 (stack74)
        %v109108 = vor.u32 %v109106, %v109107 (stack75)
        %v109109 = vxor.u32 %v109104, %v109108 (stack76)
        %v109112 = vadd.s32 %v109104, %v109109 (stack65)
        %v109116 = vadd.s32 %v109112, %v8 (stack65)
        %v109118 = vshll.u32 %v109109, 24 (stack73)
        %v109119 = vshrl.u32 %v109109, 8 (stack74)
        %v109120 = vor.u32 %v109118, %v109119 (stack75)
        %v109121 = vxor.u32 %v109112, %v109120 (stack76)
        %v109124 = vadd.s32 %v109121, %v10 (stack65)
        %v109128 = vadd.s32 %v109124, 2 (stack65)
        %v109132 = vadd.s32 %v109116, %v109128 (stack65)
        %v109134 = vshll.u32 %v109128, 13 (stack73)
        %v109135 = vshrl.u32 %v109128, 19 (stack74)
        %v109136 = vor.u32 %v109134, %v109135 (stack75)
        %v109137 = vxor.u32 %v109132, %v109136 (stack76)
        %v109140 = vadd.s32 %v109132, %v109137 (stack65)
        %v109142 = vshll.u32 %v109137, 15 (stack73)
        %v109143 = vshrl.u32 %v109137, 17 (stack74)
        %v109144 = vor.u32 %v109142, %v109143 (stack75)
        %v109145 = vxor.u32 %v109140, %v109144 (stack76)
        %v109148 = vadd.s32 %v109140, %v109145 (stack65)
        %v109150 = vshll.u32 %v109145, 26 (stack73)
        %v109151 = vshrl.u32 %v109145, 6 (stack74)
        %v109152 = vor.u32 %v109150, %v109151 (stack75)
        %v109153 = vxor.u32 %v109148, %v109152 (stack76)
        %v109156 = vadd.s32 %v109148, %v109153 (stack65)
        %v109160 = vadd.s32 %v109156, %v10 (stack65)
        %v109162 = vshll.u32 %v109153, 6 (stack73)
        %v109163 = vshrl.u32 %v109153, 26 (stack74)
        %v109164 = vor.u32 %v109162, %v109163 (stack75)
        %v109165 = vxor.u32 %v109156, %v109164 (stack76)
        %v109168 = vadd.s32 %v109165, %v9 (stack65)
        %v109172 = vadd.s32 %v109168, 3 (stack65)
        %v109176 = vadd.s32 %v109160, %v109172 (stack65)
        %v109178 = vshll.u32 %v109172, 17 (stack73)
        %v109179 = vshrl.u32 %v109172, 15 (stack74)
        %v109180 = vor.u32 %v109178, %v109179 (stack75)
        %v109181 = vxor.u32 %v109176, %v109180 (stack76)
        %v109184 = vadd.s32 %v109176, %v109181 (stack65)
        %v109186 = vshll.u32 %v109181, 29 (stack73)
        %v109187 = vshrl.u32 %v109181, 3 (stack74)
        %v109188 = vor.u32 %v109186, %v109187 (stack75)
        %v109189 = vxor.u32 %v109184, %v109188 (stack76)
        %v109192 = vadd.s32 %v109184, %v109189 (stack65)
        %v109194 = vshll.u32 %v109189, 16 (stack73)
        %v109195 = vshrl.u32 %v109189, 16 (stack74)
        %v109196 = vor.u32 %v109194, %v109195 (stack75)
        %v109197 = vxor.u32 %v109192, %v109196 (stack76)
        %v109200 = vadd.s32 %v109192, %v109197 (stack65)
        %v109204 = vadd.s32 %v109200, %v9 (stack65)
        %v109206 = vshll.u32 %v109197, 24 (stack73)
        %v109207 = vshrl.u32 %v109197, 8 (stack74)
        %v109208 = vor.u32 %v109206, %v109207 (stack75)
        %v109209 = vxor.u32 %v109200, %v109208 (stack76)
        %v109212 = vadd.s32 %v109209, %v8 (stack65)
        %v109216 = vadd.s32 %v109212, 4 (stack65)
        %v109220 = vadd.s32 %v109204, %v109216 (stack65)
        %v109222 = vshll.u32 %v109216, 13 (stack73)
        %v109223 = vshrl.u32 %v109216, 19 (stack74)
        %v109224 = vor.u32 %v109222, %v109223 (stack75)
        %v109225 = vxor.u32 %v109220, %v109224 (stack76)
        %v109228 = vadd.s32 %v109220, %v109225 (stack65)
        %v109230 = vshll.u32 %v109225, 15 (stack73)
        %v109231 = vshrl.u32 %v109225, 17 (stack74)
        %v109232 = vor.u32 %v109230, %v109231 (stack75)
        %v109233 = vxor.u32 %v109228, %v109232 (stack76)
        %v109236 = vadd.s32 %v109228, %v109233 (stack65)
        %v109238 = vshll.u32 %v109233, 26 (stack73)
        %v109239 = vshrl.u32 %v109233, 6 (stack74)
        %v109240 = vor.u32 %v109238, %v109239 (stack75)
        %v109241 = vxor.u32 %v109236, %v109240 (stack76)
        %v109244 = vadd.s32 %v109236, %v109241 (stack65)
        %v109248 = vadd.s32 %v109244, %v8 (stack65)
        %v109250 = vshll.u32 %v109241, 6 (stack73)
        %v109251 = vshrl.u32 %v109241, 26 (stack74)
        %v109252 = vor.u32 %v109250, %v109251 (stack75)
        %v109253 = vxor.u32 %v109244, %v109252 (stack76)
        %v109256 = vadd.s32 %v109253, %v10 (stack65)
        %v109260 = vadd.s32 %v109256, 5 (stack65)
        %v109262 = vxor.u32 %v109248, %v109260 (stack76)
        %v109263 = vand.u32.u8 %v109262, 255 (stack77)
        %v109264 = vand.u32 %v109263, 65535 (stack78)
        %v109265 = vshrl.u32 %v109264, 1 (stack79)
        %v109266 = vor.u32 %v109265, 16256 (stack75)
        %v109267 = vand.u32.u16 %v109266, 65535 (stack80)
        %v109268 = vunpack.i.l.bf16 %v109267 (stack81)
        %v109272 = vadd.f32 %v109268, -1.0 (stack82)
        %v109276 = vmul.f32 %v109272, 2.0 (stack83)
        %v109280 = vadd.f32 %v109276, -0.99609375 (stack82)
        %v109284 = vmax.f32 -0.99609375, %v109280 (stack84)
        %v109286 = vand.u32 2147483647, %v109284 (stack85)
        %vm109289 = vcmp.eq.f32.partialorder %v109286, 1.0 (stack86)
        %v109294 = vmul.f32 %v109284, inf (stack83)
        %v109296 = vxor.u32 %v109284, 2147483648 (stack87)
        %v109299 = vmul.f32 %v109284, %v109296 (stack83)
        %v109301 = vadd.f32 %v109299, 1.0 (stack88)
        %v109302 = vlog2.pop %v109301 (stack89)
        %v109303 = vmul.f32 %v109302, 0.6931472 (stack90)
        %v109304 = vmul.f32 -0.5, %v109299 (stack91)
        %v109305 = vadd.f32 %v109304, 1.0 (stack92)
        %v109306 = vmul.f32 %v109305, %v109299 (stack93)
        %v109307 = vand.u32 2147483647, %v109299 (stack94)
        %vm109308 = vcmp.lt.f32.partialorder %v109307, 0.0004427343 (stack95)
        %v109309 = vsel /*vm=*/%vm109308, /*on_true_vy=*/%v109306, /*on_false_vx=*/%v109303 (stack96)
        %v109310 = vxor.u32 %v109309, 2147483648 (stack87)
        %vm109313 = vcmp.lt.f32.partialorder %v109310, 5.0 (stack86)
        %v109318 = vsel /*vm=*/%vm109313, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v109322 = vsel /*vm=*/%vm109313, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v109326 = vsel /*vm=*/%vm109313, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v109330 = vsel /*vm=*/%vm109313, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v109334 = vsel /*vm=*/%vm109313, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v109338 = vsel /*vm=*/%vm109313, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v109342 = vsel /*vm=*/%vm109313, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v109346 = vsel /*vm=*/%vm109313, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v109350 = vsel /*vm=*/%vm109313, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v109354 = vadd.f32 %v109310, -2.5 (stack82)
        %v109356 = vrsqrt.pop %v109310 (stack97)
        %v109357 = vmul.f32 %v109310, %v109356 (stack98)
        %vm109358 = vcmp.eq.f32.partialorder %v109310, inf (stack99)
        %v109359 = vsel /*vm=*/%vm109358, /*on_true_vy=*/%v109310, /*on_false_vx=*/%v109357 (stack100)
        %vm109360 = vcmp.eq.f32.partialorder %v109310, 0.0 (stack101)
        %v109361 = vand.u32 %v109310, 2147483648 (stack102)
        %v109362 = vsel /*vm=*/%vm109360, /*on_true_vy=*/%v109361, /*on_false_vx=*/%v109359 (stack103)
        %v109365 = vadd.f32 %v109362, -3.0 (stack82)
        %v109369 = vsel /*vm=*/%vm109313, /*on_true_vy=*/%v109354, /*on_false_vx=*/%v109365 (stack72)
        %v109373 = vmul.f32 %v109350, %v109369 (stack83)
        %v109377 = vadd.f32 %v109346, %v109373 (stack82)
        %v109381 = vmul.f32 %v109377, %v109369 (stack83)
        %v109385 = vadd.f32 %v109342, %v109381 (stack82)
        %v109389 = vmul.f32 %v109385, %v109369 (stack83)
        %v109393 = vadd.f32 %v109338, %v109389 (stack82)
        %v109397 = vmul.f32 %v109393, %v109369 (stack83)
        %v109401 = vadd.f32 %v109334, %v109397 (stack82)
        %v109405 = vmul.f32 %v109401, %v109369 (stack83)
        %v109409 = vadd.f32 %v109330, %v109405 (stack82)
        %v109413 = vmul.f32 %v109409, %v109369 (stack83)
        %v109417 = vadd.f32 %v109326, %v109413 (stack82)
        %v109421 = vmul.f32 %v109417, %v109369 (stack83)
        %v109425 = vadd.f32 %v109322, %v109421 (stack82)
        %v109429 = vmul.f32 %v109425, %v109369 (stack83)
        %v109433 = vadd.f32 %v109318, %v109429 (stack82)
        %v109437 = vmul.f32 %v109433, %v109284 (stack83)
        %v109441 = vsel /*vm=*/%vm109289, /*on_true_vy=*/%v109294, /*on_false_vx=*/%v109437 (stack72)
        %v109445 = vmul.f32 %v109441, 1.4140625 (stack83)
        %s109447 = scalar_lea.vmem %s280, 244 [#allocation0] (stack107)
        %v109448 = vpack.c.bf16 0.0, %v109445 (stack104)
        %109449 = vst [vmem:[%s109447] sm:$0xf] /*vst_source=*/%v109448 (stack105)
        %v109452 = vadd.s32 %v1381, %v108527 (stack65)
        %s109454 = smul.u32 128, %s27 (stack66)
        %v109455 = vlaneseq (stack67)
        %v109456 = vand.u32 %v109455, 127 (stack68)
        %v109457 = vstv %s109454 (stack69)
        %v109458 = vadd.s32 %v109456, %v109457 (stack70)
        %v109462 = vadd.s32 %v109452, %v109458 (stack65)
        %vm109466 = vcmp.lt.u32.totalorder %v109462, %v109452 (stack71)
        %vm109471 = vcmp.lt.u32.totalorder %v109452, %v1381 (stack71)
        %v109476 = vadd.s32 %v1368, %v108510 (stack65)
        %v109480 = vadd.s32 %v109476, 1 (stack65)
        %v109484 = vsel /*vm=*/%vm109471, /*on_true_vy=*/%v109480, /*on_false_vx=*/%v109476 (stack72)
        %v109488 = vadd.s32 %v109484, 1 (stack65)
        %v109492 = vsel /*vm=*/%vm109466, /*on_true_vy=*/%v109488, /*on_false_vx=*/%v109484 (stack72)
        %v109497 = vadd.s32 %v109492, %v10 (stack65)
        %v109501 = vadd.s32 %v109462, %v9 (stack65)
        %v109505 = vadd.s32 %v109497, %v109501 (stack65)
        %v109507 = vshll.u32 %v109501, 13 (stack73)
        %v109508 = vshrl.u32 %v109501, 19 (stack74)
        %v109509 = vor.u32 %v109507, %v109508 (stack75)
        %v109510 = vxor.u32 %v109505, %v109509 (stack76)
        %v109513 = vadd.s32 %v109505, %v109510 (stack65)
        %v109515 = vshll.u32 %v109510, 15 (stack73)
        %v109516 = vshrl.u32 %v109510, 17 (stack74)
        %v109517 = vor.u32 %v109515, %v109516 (stack75)
        %v109518 = vxor.u32 %v109513, %v109517 (stack76)
        %v109521 = vadd.s32 %v109513, %v109518 (stack65)
        %v109523 = vshll.u32 %v109518, 26 (stack73)
        %v109524 = vshrl.u32 %v109518, 6 (stack74)
        %v109525 = vor.u32 %v109523, %v109524 (stack75)
        %v109526 = vxor.u32 %v109521, %v109525 (stack76)
        %v109529 = vadd.s32 %v109521, %v109526 (stack65)
        %v109533 = vadd.s32 %v109529, %v9 (stack65)
        %v109535 = vshll.u32 %v109526, 6 (stack73)
        %v109536 = vshrl.u32 %v109526, 26 (stack74)
        %v109537 = vor.u32 %v109535, %v109536 (stack75)
        %v109538 = vxor.u32 %v109529, %v109537 (stack76)
        %v109541 = vadd.s32 %v109538, %v8 (stack65)
        %v109545 = vadd.s32 %v109541, 1 (stack65)
        %v109549 = vadd.s32 %v109533, %v109545 (stack65)
        %v109551 = vshll.u32 %v109545, 17 (stack73)
        %v109552 = vshrl.u32 %v109545, 15 (stack74)
        %v109553 = vor.u32 %v109551, %v109552 (stack75)
        %v109554 = vxor.u32 %v109549, %v109553 (stack76)
        %v109557 = vadd.s32 %v109549, %v109554 (stack65)
        %v109559 = vshll.u32 %v109554, 29 (stack73)
        %v109560 = vshrl.u32 %v109554, 3 (stack74)
        %v109561 = vor.u32 %v109559, %v109560 (stack75)
        %v109562 = vxor.u32 %v109557, %v109561 (stack76)
        %v109565 = vadd.s32 %v109557, %v109562 (stack65)
        %v109567 = vshll.u32 %v109562, 16 (stack73)
        %v109568 = vshrl.u32 %v109562, 16 (stack74)
        %v109569 = vor.u32 %v109567, %v109568 (stack75)
        %v109570 = vxor.u32 %v109565, %v109569 (stack76)
        %v109573 = vadd.s32 %v109565, %v109570 (stack65)
        %v109577 = vadd.s32 %v109573, %v8 (stack65)
        %v109579 = vshll.u32 %v109570, 24 (stack73)
        %v109580 = vshrl.u32 %v109570, 8 (stack74)
        %v109581 = vor.u32 %v109579, %v109580 (stack75)
        %v109582 = vxor.u32 %v109573, %v109581 (stack76)
        %v109585 = vadd.s32 %v109582, %v10 (stack65)
        %v109589 = vadd.s32 %v109585, 2 (stack65)
        %v109593 = vadd.s32 %v109577, %v109589 (stack65)
        %v109595 = vshll.u32 %v109589, 13 (stack73)
        %v109596 = vshrl.u32 %v109589, 19 (stack74)
        %v109597 = vor.u32 %v109595, %v109596 (stack75)
        %v109598 = vxor.u32 %v109593, %v109597 (stack76)
        %v109601 = vadd.s32 %v109593, %v109598 (stack65)
        %v109603 = vshll.u32 %v109598, 15 (stack73)
        %v109604 = vshrl.u32 %v109598, 17 (stack74)
        %v109605 = vor.u32 %v109603, %v109604 (stack75)
        %v109606 = vxor.u32 %v109601, %v109605 (stack76)
        %v109609 = vadd.s32 %v109601, %v109606 (stack65)
        %v109611 = vshll.u32 %v109606, 26 (stack73)
        %v109612 = vshrl.u32 %v109606, 6 (stack74)
        %v109613 = vor.u32 %v109611, %v109612 (stack75)
        %v109614 = vxor.u32 %v109609, %v109613 (stack76)
        %v109617 = vadd.s32 %v109609, %v109614 (stack65)
        %v109621 = vadd.s32 %v109617, %v10 (stack65)
        %v109623 = vshll.u32 %v109614, 6 (stack73)
        %v109624 = vshrl.u32 %v109614, 26 (stack74)
        %v109625 = vor.u32 %v109623, %v109624 (stack75)
        %v109626 = vxor.u32 %v109617, %v109625 (stack76)
        %v109629 = vadd.s32 %v109626, %v9 (stack65)
        %v109633 = vadd.s32 %v109629, 3 (stack65)
        %v109637 = vadd.s32 %v109621, %v109633 (stack65)
        %v109639 = vshll.u32 %v109633, 17 (stack73)
        %v109640 = vshrl.u32 %v109633, 15 (stack74)
        %v109641 = vor.u32 %v109639, %v109640 (stack75)
        %v109642 = vxor.u32 %v109637, %v109641 (stack76)
        %v109645 = vadd.s32 %v109637, %v109642 (stack65)
        %v109647 = vshll.u32 %v109642, 29 (stack73)
        %v109648 = vshrl.u32 %v109642, 3 (stack74)
        %v109649 = vor.u32 %v109647, %v109648 (stack75)
        %v109650 = vxor.u32 %v109645, %v109649 (stack76)
        %v109653 = vadd.s32 %v109645, %v109650 (stack65)
        %v109655 = vshll.u32 %v109650, 16 (stack73)
        %v109656 = vshrl.u32 %v109650, 16 (stack74)
        %v109657 = vor.u32 %v109655, %v109656 (stack75)
        %v109658 = vxor.u32 %v109653, %v109657 (stack76)
        %v109661 = vadd.s32 %v109653, %v109658 (stack65)
        %v109665 = vadd.s32 %v109661, %v9 (stack65)
        %v109667 = vshll.u32 %v109658, 24 (stack73)
        %v109668 = vshrl.u32 %v109658, 8 (stack74)
        %v109669 = vor.u32 %v109667, %v109668 (stack75)
        %v109670 = vxor.u32 %v109661, %v109669 (stack76)
        %v109673 = vadd.s32 %v109670, %v8 (stack65)
        %v109677 = vadd.s32 %v109673, 4 (stack65)
        %v109681 = vadd.s32 %v109665, %v109677 (stack65)
        %v109683 = vshll.u32 %v109677, 13 (stack73)
        %v109684 = vshrl.u32 %v109677, 19 (stack74)
        %v109685 = vor.u32 %v109683, %v109684 (stack75)
        %v109686 = vxor.u32 %v109681, %v109685 (stack76)
        %v109689 = vadd.s32 %v109681, %v109686 (stack65)
        %v109691 = vshll.u32 %v109686, 15 (stack73)
        %v109692 = vshrl.u32 %v109686, 17 (stack74)
        %v109693 = vor.u32 %v109691, %v109692 (stack75)
        %v109694 = vxor.u32 %v109689, %v109693 (stack76)
        %v109697 = vadd.s32 %v109689, %v109694 (stack65)
        %v109699 = vshll.u32 %v109694, 26 (stack73)
        %v109700 = vshrl.u32 %v109694, 6 (stack74)
        %v109701 = vor.u32 %v109699, %v109700 (stack75)
        %v109702 = vxor.u32 %v109697, %v109701 (stack76)
        %v109705 = vadd.s32 %v109697, %v109702 (stack65)
        %v109709 = vadd.s32 %v109705, %v8 (stack65)
        %v109711 = vshll.u32 %v109702, 6 (stack73)
        %v109712 = vshrl.u32 %v109702, 26 (stack74)
        %v109713 = vor.u32 %v109711, %v109712 (stack75)
        %v109714 = vxor.u32 %v109705, %v109713 (stack76)
        %v109717 = vadd.s32 %v109714, %v10 (stack65)
        %v109721 = vadd.s32 %v109717, 5 (stack65)
        %v109723 = vxor.u32 %v109709, %v109721 (stack76)
        %v109724 = vand.u32.u8 %v109723, 255 (stack77)
        %v109725 = vand.u32 %v109724, 65535 (stack78)
        %v109726 = vshrl.u32 %v109725, 1 (stack79)
        %v109727 = vor.u32 %v109726, 16256 (stack75)
        %v109728 = vand.u32.u16 %v109727, 65535 (stack80)
        %v109729 = vunpack.i.l.bf16 %v109728 (stack81)
        %v109733 = vadd.f32 %v109729, -1.0 (stack82)
        %v109737 = vmul.f32 %v109733, 2.0 (stack83)
        %v109741 = vadd.f32 %v109737, -0.99609375 (stack82)
        %v109745 = vmax.f32 -0.99609375, %v109741 (stack84)
        %v109747 = vand.u32 2147483647, %v109745 (stack85)
        %vm109750 = vcmp.eq.f32.partialorder %v109747, 1.0 (stack86)
        %v109755 = vmul.f32 %v109745, inf (stack83)
        %v109757 = vxor.u32 %v109745, 2147483648 (stack87)
        %v109760 = vmul.f32 %v109745, %v109757 (stack83)
        %v109762 = vadd.f32 %v109760, 1.0 (stack88)
        %v109763 = vlog2.pop %v109762 (stack89)
        %v109764 = vmul.f32 %v109763, 0.6931472 (stack90)
        %v109765 = vmul.f32 -0.5, %v109760 (stack91)
        %v109766 = vadd.f32 %v109765, 1.0 (stack92)
        %v109767 = vmul.f32 %v109766, %v109760 (stack93)
        %v109768 = vand.u32 2147483647, %v109760 (stack94)
        %vm109769 = vcmp.lt.f32.partialorder %v109768, 0.0004427343 (stack95)
        %v109770 = vsel /*vm=*/%vm109769, /*on_true_vy=*/%v109767, /*on_false_vx=*/%v109764 (stack96)
        %v109771 = vxor.u32 %v109770, 2147483648 (stack87)
        %vm109774 = vcmp.lt.f32.partialorder %v109771, 5.0 (stack86)
        %v109779 = vsel /*vm=*/%vm109774, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v109783 = vsel /*vm=*/%vm109774, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v109787 = vsel /*vm=*/%vm109774, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v109791 = vsel /*vm=*/%vm109774, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v109795 = vsel /*vm=*/%vm109774, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v109799 = vsel /*vm=*/%vm109774, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v109803 = vsel /*vm=*/%vm109774, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v109807 = vsel /*vm=*/%vm109774, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v109811 = vsel /*vm=*/%vm109774, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v109815 = vadd.f32 %v109771, -2.5 (stack82)
        %v109817 = vrsqrt.pop %v109771 (stack97)
        %v109818 = vmul.f32 %v109771, %v109817 (stack98)
        %vm109819 = vcmp.eq.f32.partialorder %v109771, inf (stack99)
        %v109820 = vsel /*vm=*/%vm109819, /*on_true_vy=*/%v109771, /*on_false_vx=*/%v109818 (stack100)
        %vm109821 = vcmp.eq.f32.partialorder %v109771, 0.0 (stack101)
        %v109822 = vand.u32 %v109771, 2147483648 (stack102)
        %v109823 = vsel /*vm=*/%vm109821, /*on_true_vy=*/%v109822, /*on_false_vx=*/%v109820 (stack103)
        %v109826 = vadd.f32 %v109823, -3.0 (stack82)
        %v109830 = vsel /*vm=*/%vm109774, /*on_true_vy=*/%v109815, /*on_false_vx=*/%v109826 (stack72)
        %v109834 = vmul.f32 %v109811, %v109830 (stack83)
        %v109838 = vadd.f32 %v109807, %v109834 (stack82)
        %v109842 = vmul.f32 %v109838, %v109830 (stack83)
        %v109846 = vadd.f32 %v109803, %v109842 (stack82)
        %v109850 = vmul.f32 %v109846, %v109830 (stack83)
        %v109854 = vadd.f32 %v109799, %v109850 (stack82)
        %v109858 = vmul.f32 %v109854, %v109830 (stack83)
        %v109862 = vadd.f32 %v109795, %v109858 (stack82)
        %v109866 = vmul.f32 %v109862, %v109830 (stack83)
        %v109870 = vadd.f32 %v109791, %v109866 (stack82)
        %v109874 = vmul.f32 %v109870, %v109830 (stack83)
        %v109878 = vadd.f32 %v109787, %v109874 (stack82)
        %v109882 = vmul.f32 %v109878, %v109830 (stack83)
        %v109886 = vadd.f32 %v109783, %v109882 (stack82)
        %v109890 = vmul.f32 %v109886, %v109830 (stack83)
        %v109894 = vadd.f32 %v109779, %v109890 (stack82)
        %v109898 = vmul.f32 %v109894, %v109745 (stack83)
        %v109902 = vsel /*vm=*/%vm109750, /*on_true_vy=*/%v109755, /*on_false_vx=*/%v109898 (stack72)
        %v109906 = vmul.f32 %v109902, 1.4140625 (stack83)
        %s109908 = scalar_lea.vmem %s280, 372 [#allocation0] (stack107)
        %v109909 = vpack.c.bf16 0.0, %v109906 (stack104)
        %109910 = vst [vmem:[%s109908] sm:$0xf] /*vst_source=*/%v109909 (stack105)
        %v109913 = vadd.s32 %v1868, %v108527 (stack65)
        %s109915 = smul.u32 128, %s27 (stack66)
        %v109916 = vlaneseq (stack67)
        %v109917 = vand.u32 %v109916, 127 (stack68)
        %v109918 = vstv %s109915 (stack69)
        %v109919 = vadd.s32 %v109917, %v109918 (stack70)
        %v109923 = vadd.s32 %v109913, %v109919 (stack65)
        %vm109927 = vcmp.lt.u32.totalorder %v109923, %v109913 (stack71)
        %vm109932 = vcmp.lt.u32.totalorder %v109913, %v1868 (stack71)
        %v109937 = vadd.s32 %v1855, %v108510 (stack65)
        %v109941 = vadd.s32 %v109937, 1 (stack65)
        %v109945 = vsel /*vm=*/%vm109932, /*on_true_vy=*/%v109941, /*on_false_vx=*/%v109937 (stack72)
        %v109949 = vadd.s32 %v109945, 1 (stack65)
        %v109953 = vsel /*vm=*/%vm109927, /*on_true_vy=*/%v109949, /*on_false_vx=*/%v109945 (stack72)
        %v109958 = vadd.s32 %v109953, %v10 (stack65)
        %v109962 = vadd.s32 %v109923, %v9 (stack65)
        %v109966 = vadd.s32 %v109958, %v109962 (stack65)
        %v109968 = vshll.u32 %v109962, 13 (stack73)
        %v109969 = vshrl.u32 %v109962, 19 (stack74)
        %v109970 = vor.u32 %v109968, %v109969 (stack75)
        %v109971 = vxor.u32 %v109966, %v109970 (stack76)
        %v109974 = vadd.s32 %v109966, %v109971 (stack65)
        %v109976 = vshll.u32 %v109971, 15 (stack73)
        %v109977 = vshrl.u32 %v109971, 17 (stack74)
        %v109978 = vor.u32 %v109976, %v109977 (stack75)
        %v109979 = vxor.u32 %v109974, %v109978 (stack76)
        %v109982 = vadd.s32 %v109974, %v109979 (stack65)
        %v109984 = vshll.u32 %v109979, 26 (stack73)
        %v109985 = vshrl.u32 %v109979, 6 (stack74)
        %v109986 = vor.u32 %v109984, %v109985 (stack75)
        %v109987 = vxor.u32 %v109982, %v109986 (stack76)
        %v109990 = vadd.s32 %v109982, %v109987 (stack65)
        %v109994 = vadd.s32 %v109990, %v9 (stack65)
        %v109996 = vshll.u32 %v109987, 6 (stack73)
        %v109997 = vshrl.u32 %v109987, 26 (stack74)
        %v109998 = vor.u32 %v109996, %v109997 (stack75)
        %v109999 = vxor.u32 %v109990, %v109998 (stack76)
        %v110002 = vadd.s32 %v109999, %v8 (stack65)
        %v110006 = vadd.s32 %v110002, 1 (stack65)
        %v110010 = vadd.s32 %v109994, %v110006 (stack65)
        %v110012 = vshll.u32 %v110006, 17 (stack73)
        %v110013 = vshrl.u32 %v110006, 15 (stack74)
        %v110014 = vor.u32 %v110012, %v110013 (stack75)
        %v110015 = vxor.u32 %v110010, %v110014 (stack76)
        %v110018 = vadd.s32 %v110010, %v110015 (stack65)
        %v110020 = vshll.u32 %v110015, 29 (stack73)
        %v110021 = vshrl.u32 %v110015, 3 (stack74)
        %v110022 = vor.u32 %v110020, %v110021 (stack75)
        %v110023 = vxor.u32 %v110018, %v110022 (stack76)
        %v110026 = vadd.s32 %v110018, %v110023 (stack65)
        %v110028 = vshll.u32 %v110023, 16 (stack73)
        %v110029 = vshrl.u32 %v110023, 16 (stack74)
        %v110030 = vor.u32 %v110028, %v110029 (stack75)
        %v110031 = vxor.u32 %v110026, %v110030 (stack76)
        %v110034 = vadd.s32 %v110026, %v110031 (stack65)
        %v110038 = vadd.s32 %v110034, %v8 (stack65)
        %v110040 = vshll.u32 %v110031, 24 (stack73)
        %v110041 = vshrl.u32 %v110031, 8 (stack74)
        %v110042 = vor.u32 %v110040, %v110041 (stack75)
        %v110043 = vxor.u32 %v110034, %v110042 (stack76)
        %v110046 = vadd.s32 %v110043, %v10 (stack65)
        %v110050 = vadd.s32 %v110046, 2 (stack65)
        %v110054 = vadd.s32 %v110038, %v110050 (stack65)
        %v110056 = vshll.u32 %v110050, 13 (stack73)
        %v110057 = vshrl.u32 %v110050, 19 (stack74)
        %v110058 = vor.u32 %v110056, %v110057 (stack75)
        %v110059 = vxor.u32 %v110054, %v110058 (stack76)
        %v110062 = vadd.s32 %v110054, %v110059 (stack65)
        %v110064 = vshll.u32 %v110059, 15 (stack73)
        %v110065 = vshrl.u32 %v110059, 17 (stack74)
        %v110066 = vor.u32 %v110064, %v110065 (stack75)
        %v110067 = vxor.u32 %v110062, %v110066 (stack76)
        %v110070 = vadd.s32 %v110062, %v110067 (stack65)
        %v110072 = vshll.u32 %v110067, 26 (stack73)
        %v110073 = vshrl.u32 %v110067, 6 (stack74)
        %v110074 = vor.u32 %v110072, %v110073 (stack75)
        %v110075 = vxor.u32 %v110070, %v110074 (stack76)
        %v110078 = vadd.s32 %v110070, %v110075 (stack65)
        %v110082 = vadd.s32 %v110078, %v10 (stack65)
        %v110084 = vshll.u32 %v110075, 6 (stack73)
        %v110085 = vshrl.u32 %v110075, 26 (stack74)
        %v110086 = vor.u32 %v110084, %v110085 (stack75)
        %v110087 = vxor.u32 %v110078, %v110086 (stack76)
        %v110090 = vadd.s32 %v110087, %v9 (stack65)
        %v110094 = vadd.s32 %v110090, 3 (stack65)
        %v110098 = vadd.s32 %v110082, %v110094 (stack65)
        %v110100 = vshll.u32 %v110094, 17 (stack73)
        %v110101 = vshrl.u32 %v110094, 15 (stack74)
        %v110102 = vor.u32 %v110100, %v110101 (stack75)
        %v110103 = vxor.u32 %v110098, %v110102 (stack76)
        %v110106 = vadd.s32 %v110098, %v110103 (stack65)
        %v110108 = vshll.u32 %v110103, 29 (stack73)
        %v110109 = vshrl.u32 %v110103, 3 (stack74)
        %v110110 = vor.u32 %v110108, %v110109 (stack75)
        %v110111 = vxor.u32 %v110106, %v110110 (stack76)
        %v110114 = vadd.s32 %v110106, %v110111 (stack65)
        %v110116 = vshll.u32 %v110111, 16 (stack73)
        %v110117 = vshrl.u32 %v110111, 16 (stack74)
        %v110118 = vor.u32 %v110116, %v110117 (stack75)
        %v110119 = vxor.u32 %v110114, %v110118 (stack76)
        %v110122 = vadd.s32 %v110114, %v110119 (stack65)
        %v110126 = vadd.s32 %v110122, %v9 (stack65)
        %v110128 = vshll.u32 %v110119, 24 (stack73)
        %v110129 = vshrl.u32 %v110119, 8 (stack74)
        %v110130 = vor.u32 %v110128, %v110129 (stack75)
        %v110131 = vxor.u32 %v110122, %v110130 (stack76)
        %v110134 = vadd.s32 %v110131, %v8 (stack65)
        %v110138 = vadd.s32 %v110134, 4 (stack65)
        %v110142 = vadd.s32 %v110126, %v110138 (stack65)
        %v110144 = vshll.u32 %v110138, 13 (stack73)
        %v110145 = vshrl.u32 %v110138, 19 (stack74)
        %v110146 = vor.u32 %v110144, %v110145 (stack75)
        %v110147 = vxor.u32 %v110142, %v110146 (stack76)
        %v110150 = vadd.s32 %v110142, %v110147 (stack65)
        %v110152 = vshll.u32 %v110147, 15 (stack73)
        %v110153 = vshrl.u32 %v110147, 17 (stack74)
        %v110154 = vor.u32 %v110152, %v110153 (stack75)
        %v110155 = vxor.u32 %v110150, %v110154 (stack76)
        %v110158 = vadd.s32 %v110150, %v110155 (stack65)
        %v110160 = vshll.u32 %v110155, 26 (stack73)
        %v110161 = vshrl.u32 %v110155, 6 (stack74)
        %v110162 = vor.u32 %v110160, %v110161 (stack75)
        %v110163 = vxor.u32 %v110158, %v110162 (stack76)
        %v110166 = vadd.s32 %v110158, %v110163 (stack65)
        %v110170 = vadd.s32 %v110166, %v8 (stack65)
        %v110172 = vshll.u32 %v110163, 6 (stack73)
        %v110173 = vshrl.u32 %v110163, 26 (stack74)
        %v110174 = vor.u32 %v110172, %v110173 (stack75)
        %v110175 = vxor.u32 %v110166, %v110174 (stack76)
        %v110178 = vadd.s32 %v110175, %v10 (stack65)
        %v110182 = vadd.s32 %v110178, 5 (stack65)
        %v110184 = vxor.u32 %v110170, %v110182 (stack76)
        %v110185 = vand.u32.u8 %v110184, 255 (stack77)
        %v110186 = vand.u32 %v110185, 65535 (stack78)
        %v110187 = vshrl.u32 %v110186, 1 (stack79)
        %v110188 = vor.u32 %v110187, 16256 (stack75)
        %v110189 = vand.u32.u16 %v110188, 65535 (stack80)
        %v110190 = vunpack.i.l.bf16 %v110189 (stack81)
        %v110194 = vadd.f32 %v110190, -1.0 (stack82)
        %v110198 = vmul.f32 %v110194, 2.0 (stack83)
        %v110202 = vadd.f32 %v110198, -0.99609375 (stack82)
        %v110206 = vmax.f32 -0.99609375, %v110202 (stack84)
        %v110208 = vand.u32 2147483647, %v110206 (stack85)
        %vm110211 = vcmp.eq.f32.partialorder %v110208, 1.0 (stack86)
        %v110216 = vmul.f32 %v110206, inf (stack83)
        %v110218 = vxor.u32 %v110206, 2147483648 (stack87)
        %v110221 = vmul.f32 %v110206, %v110218 (stack83)
        %v110223 = vadd.f32 %v110221, 1.0 (stack88)
        %v110224 = vlog2.pop %v110223 (stack89)
        %v110225 = vmul.f32 %v110224, 0.6931472 (stack90)
        %v110226 = vmul.f32 -0.5, %v110221 (stack91)
        %v110227 = vadd.f32 %v110226, 1.0 (stack92)
        %v110228 = vmul.f32 %v110227, %v110221 (stack93)
        %v110229 = vand.u32 2147483647, %v110221 (stack94)
        %vm110230 = vcmp.lt.f32.partialorder %v110229, 0.0004427343 (stack95)
        %v110231 = vsel /*vm=*/%vm110230, /*on_true_vy=*/%v110228, /*on_false_vx=*/%v110225 (stack96)
        %v110232 = vxor.u32 %v110231, 2147483648 (stack87)
        %vm110235 = vcmp.lt.f32.partialorder %v110232, 5.0 (stack86)
        %v110240 = vsel /*vm=*/%vm110235, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v110244 = vsel /*vm=*/%vm110235, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v110248 = vsel /*vm=*/%vm110235, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v110252 = vsel /*vm=*/%vm110235, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v110256 = vsel /*vm=*/%vm110235, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v110260 = vsel /*vm=*/%vm110235, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v110264 = vsel /*vm=*/%vm110235, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v110268 = vsel /*vm=*/%vm110235, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v110272 = vsel /*vm=*/%vm110235, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v110276 = vadd.f32 %v110232, -2.5 (stack82)
        %v110278 = vrsqrt.pop %v110232 (stack97)
        %v110279 = vmul.f32 %v110232, %v110278 (stack98)
        %vm110280 = vcmp.eq.f32.partialorder %v110232, inf (stack99)
        %v110281 = vsel /*vm=*/%vm110280, /*on_true_vy=*/%v110232, /*on_false_vx=*/%v110279 (stack100)
        %vm110282 = vcmp.eq.f32.partialorder %v110232, 0.0 (stack101)
        %v110283 = vand.u32 %v110232, 2147483648 (stack102)
        %v110284 = vsel /*vm=*/%vm110282, /*on_true_vy=*/%v110283, /*on_false_vx=*/%v110281 (stack103)
        %v110287 = vadd.f32 %v110284, -3.0 (stack82)
        %v110291 = vsel /*vm=*/%vm110235, /*on_true_vy=*/%v110276, /*on_false_vx=*/%v110287 (stack72)
        %v110295 = vmul.f32 %v110272, %v110291 (stack83)
        %v110299 = vadd.f32 %v110268, %v110295 (stack82)
        %v110303 = vmul.f32 %v110299, %v110291 (stack83)
        %v110307 = vadd.f32 %v110264, %v110303 (stack82)
        %v110311 = vmul.f32 %v110307, %v110291 (stack83)
        %v110315 = vadd.f32 %v110260, %v110311 (stack82)
        %v110319 = vmul.f32 %v110315, %v110291 (stack83)
        %v110323 = vadd.f32 %v110256, %v110319 (stack82)
        %v110327 = vmul.f32 %v110323, %v110291 (stack83)
        %v110331 = vadd.f32 %v110252, %v110327 (stack82)
        %v110335 = vmul.f32 %v110331, %v110291 (stack83)
        %v110339 = vadd.f32 %v110248, %v110335 (stack82)
        %v110343 = vmul.f32 %v110339, %v110291 (stack83)
        %v110347 = vadd.f32 %v110244, %v110343 (stack82)
        %v110351 = vmul.f32 %v110347, %v110291 (stack83)
        %v110355 = vadd.f32 %v110240, %v110351 (stack82)
        %v110359 = vmul.f32 %v110355, %v110206 (stack83)
        %v110363 = vsel /*vm=*/%vm110211, /*on_true_vy=*/%v110216, /*on_false_vx=*/%v110359 (stack72)
        %v110367 = vmul.f32 %v110363, 1.4140625 (stack83)
        %s110369 = scalar_lea.vmem %s280, 500 [#allocation0] (stack107)
        %v110370 = vpack.c.bf16 0.0, %v110367 (stack104)
        %110371 = vst [vmem:[%s110369] sm:$0xf] /*vst_source=*/%v110370 (stack105)
        %v110374 = vadd.s32 %v2355, %v108527 (stack65)
        %s110376 = smul.u32 128, %s27 (stack66)
        %v110377 = vlaneseq (stack67)
        %v110378 = vand.u32 %v110377, 127 (stack68)
        %v110379 = vstv %s110376 (stack69)
        %v110380 = vadd.s32 %v110378, %v110379 (stack70)
        %v110384 = vadd.s32 %v110374, %v110380 (stack65)
        %vm110388 = vcmp.lt.u32.totalorder %v110384, %v110374 (stack71)
        %vm110393 = vcmp.lt.u32.totalorder %v110374, %v2355 (stack71)
        %v110398 = vadd.s32 %v2342, %v108510 (stack65)
        %v110402 = vadd.s32 %v110398, 1 (stack65)
        %v110406 = vsel /*vm=*/%vm110393, /*on_true_vy=*/%v110402, /*on_false_vx=*/%v110398 (stack72)
        %v110410 = vadd.s32 %v110406, 1 (stack65)
        %v110414 = vsel /*vm=*/%vm110388, /*on_true_vy=*/%v110410, /*on_false_vx=*/%v110406 (stack72)
        %v110419 = vadd.s32 %v110414, %v10 (stack65)
        %v110423 = vadd.s32 %v110384, %v9 (stack65)
        %v110427 = vadd.s32 %v110419, %v110423 (stack65)
        %v110429 = vshll.u32 %v110423, 13 (stack73)
        %v110430 = vshrl.u32 %v110423, 19 (stack74)
        %v110431 = vor.u32 %v110429, %v110430 (stack75)
        %v110432 = vxor.u32 %v110427, %v110431 (stack76)
        %v110435 = vadd.s32 %v110427, %v110432 (stack65)
        %v110437 = vshll.u32 %v110432, 15 (stack73)
        %v110438 = vshrl.u32 %v110432, 17 (stack74)
        %v110439 = vor.u32 %v110437, %v110438 (stack75)
        %v110440 = vxor.u32 %v110435, %v110439 (stack76)
        %v110443 = vadd.s32 %v110435, %v110440 (stack65)
        %v110445 = vshll.u32 %v110440, 26 (stack73)
        %v110446 = vshrl.u32 %v110440, 6 (stack74)
        %v110447 = vor.u32 %v110445, %v110446 (stack75)
        %v110448 = vxor.u32 %v110443, %v110447 (stack76)
        %v110451 = vadd.s32 %v110443, %v110448 (stack65)
        %v110455 = vadd.s32 %v110451, %v9 (stack65)
        %v110457 = vshll.u32 %v110448, 6 (stack73)
        %v110458 = vshrl.u32 %v110448, 26 (stack74)
        %v110459 = vor.u32 %v110457, %v110458 (stack75)
        %v110460 = vxor.u32 %v110451, %v110459 (stack76)
        %v110463 = vadd.s32 %v110460, %v8 (stack65)
        %v110467 = vadd.s32 %v110463, 1 (stack65)
        %v110471 = vadd.s32 %v110455, %v110467 (stack65)
        %v110473 = vshll.u32 %v110467, 17 (stack73)
        %v110474 = vshrl.u32 %v110467, 15 (stack74)
        %v110475 = vor.u32 %v110473, %v110474 (stack75)
        %v110476 = vxor.u32 %v110471, %v110475 (stack76)
        %v110479 = vadd.s32 %v110471, %v110476 (stack65)
        %v110481 = vshll.u32 %v110476, 29 (stack73)
        %v110482 = vshrl.u32 %v110476, 3 (stack74)
        %v110483 = vor.u32 %v110481, %v110482 (stack75)
        %v110484 = vxor.u32 %v110479, %v110483 (stack76)
        %v110487 = vadd.s32 %v110479, %v110484 (stack65)
        %v110489 = vshll.u32 %v110484, 16 (stack73)
        %v110490 = vshrl.u32 %v110484, 16 (stack74)
        %v110491 = vor.u32 %v110489, %v110490 (stack75)
        %v110492 = vxor.u32 %v110487, %v110491 (stack76)
        %v110495 = vadd.s32 %v110487, %v110492 (stack65)
        %v110499 = vadd.s32 %v110495, %v8 (stack65)
        %v110501 = vshll.u32 %v110492, 24 (stack73)
        %v110502 = vshrl.u32 %v110492, 8 (stack74)
        %v110503 = vor.u32 %v110501, %v110502 (stack75)
        %v110504 = vxor.u32 %v110495, %v110503 (stack76)
        %v110507 = vadd.s32 %v110504, %v10 (stack65)
        %v110511 = vadd.s32 %v110507, 2 (stack65)
        %v110515 = vadd.s32 %v110499, %v110511 (stack65)
        %v110517 = vshll.u32 %v110511, 13 (stack73)
        %v110518 = vshrl.u32 %v110511, 19 (stack74)
        %v110519 = vor.u32 %v110517, %v110518 (stack75)
        %v110520 = vxor.u32 %v110515, %v110519 (stack76)
        %v110523 = vadd.s32 %v110515, %v110520 (stack65)
        %v110525 = vshll.u32 %v110520, 15 (stack73)
        %v110526 = vshrl.u32 %v110520, 17 (stack74)
        %v110527 = vor.u32 %v110525, %v110526 (stack75)
        %v110528 = vxor.u32 %v110523, %v110527 (stack76)
        %v110531 = vadd.s32 %v110523, %v110528 (stack65)
        %v110533 = vshll.u32 %v110528, 26 (stack73)
        %v110534 = vshrl.u32 %v110528, 6 (stack74)
        %v110535 = vor.u32 %v110533, %v110534 (stack75)
        %v110536 = vxor.u32 %v110531, %v110535 (stack76)
        %v110539 = vadd.s32 %v110531, %v110536 (stack65)
        %v110543 = vadd.s32 %v110539, %v10 (stack65)
        %v110545 = vshll.u32 %v110536, 6 (stack73)
        %v110546 = vshrl.u32 %v110536, 26 (stack74)
        %v110547 = vor.u32 %v110545, %v110546 (stack75)
        %v110548 = vxor.u32 %v110539, %v110547 (stack76)
        %v110551 = vadd.s32 %v110548, %v9 (stack65)
        %v110555 = vadd.s32 %v110551, 3 (stack65)
        %v110559 = vadd.s32 %v110543, %v110555 (stack65)
        %v110561 = vshll.u32 %v110555, 17 (stack73)
        %v110562 = vshrl.u32 %v110555, 15 (stack74)
        %v110563 = vor.u32 %v110561, %v110562 (stack75)
        %v110564 = vxor.u32 %v110559, %v110563 (stack76)
        %v110567 = vadd.s32 %v110559, %v110564 (stack65)
        %v110569 = vshll.u32 %v110564, 29 (stack73)
        %v110570 = vshrl.u32 %v110564, 3 (stack74)
        %v110571 = vor.u32 %v110569, %v110570 (stack75)
        %v110572 = vxor.u32 %v110567, %v110571 (stack76)
        %v110575 = vadd.s32 %v110567, %v110572 (stack65)
        %v110577 = vshll.u32 %v110572, 16 (stack73)
        %v110578 = vshrl.u32 %v110572, 16 (stack74)
        %v110579 = vor.u32 %v110577, %v110578 (stack75)
        %v110580 = vxor.u32 %v110575, %v110579 (stack76)
        %v110583 = vadd.s32 %v110575, %v110580 (stack65)
        %v110587 = vadd.s32 %v110583, %v9 (stack65)
        %v110589 = vshll.u32 %v110580, 24 (stack73)
        %v110590 = vshrl.u32 %v110580, 8 (stack74)
        %v110591 = vor.u32 %v110589, %v110590 (stack75)
        %v110592 = vxor.u32 %v110583, %v110591 (stack76)
        %v110595 = vadd.s32 %v110592, %v8 (stack65)
        %v110599 = vadd.s32 %v110595, 4 (stack65)
        %v110603 = vadd.s32 %v110587, %v110599 (stack65)
        %v110605 = vshll.u32 %v110599, 13 (stack73)
        %v110606 = vshrl.u32 %v110599, 19 (stack74)
        %v110607 = vor.u32 %v110605, %v110606 (stack75)
        %v110608 = vxor.u32 %v110603, %v110607 (stack76)
        %v110611 = vadd.s32 %v110603, %v110608 (stack65)
        %v110613 = vshll.u32 %v110608, 15 (stack73)
        %v110614 = vshrl.u32 %v110608, 17 (stack74)
        %v110615 = vor.u32 %v110613, %v110614 (stack75)
        %v110616 = vxor.u32 %v110611, %v110615 (stack76)
        %v110619 = vadd.s32 %v110611, %v110616 (stack65)
        %v110621 = vshll.u32 %v110616, 26 (stack73)
        %v110622 = vshrl.u32 %v110616, 6 (stack74)
        %v110623 = vor.u32 %v110621, %v110622 (stack75)
        %v110624 = vxor.u32 %v110619, %v110623 (stack76)
        %v110627 = vadd.s32 %v110619, %v110624 (stack65)
        %v110631 = vadd.s32 %v110627, %v8 (stack65)
        %v110633 = vshll.u32 %v110624, 6 (stack73)
        %v110634 = vshrl.u32 %v110624, 26 (stack74)
        %v110635 = vor.u32 %v110633, %v110634 (stack75)
        %v110636 = vxor.u32 %v110627, %v110635 (stack76)
        %v110639 = vadd.s32 %v110636, %v10 (stack65)
        %v110643 = vadd.s32 %v110639, 5 (stack65)
        %v110645 = vxor.u32 %v110631, %v110643 (stack76)
        %v110646 = vand.u32.u8 %v110645, 255 (stack77)
        %v110647 = vand.u32 %v110646, 65535 (stack78)
        %v110648 = vshrl.u32 %v110647, 1 (stack79)
        %v110649 = vor.u32 %v110648, 16256 (stack75)
        %v110650 = vand.u32.u16 %v110649, 65535 (stack80)
        %v110651 = vunpack.i.l.bf16 %v110650 (stack81)
        %v110655 = vadd.f32 %v110651, -1.0 (stack82)
        %v110659 = vmul.f32 %v110655, 2.0 (stack83)
        %v110663 = vadd.f32 %v110659, -0.99609375 (stack82)
        %v110667 = vmax.f32 -0.99609375, %v110663 (stack84)
        %v110669 = vand.u32 2147483647, %v110667 (stack85)
        %vm110672 = vcmp.eq.f32.partialorder %v110669, 1.0 (stack86)
        %v110677 = vmul.f32 %v110667, inf (stack83)
        %v110679 = vxor.u32 %v110667, 2147483648 (stack87)
        %v110682 = vmul.f32 %v110667, %v110679 (stack83)
        %v110684 = vadd.f32 %v110682, 1.0 (stack88)
        %v110685 = vlog2.pop %v110684 (stack89)
        %v110686 = vmul.f32 %v110685, 0.6931472 (stack90)
        %v110687 = vmul.f32 -0.5, %v110682 (stack91)
        %v110688 = vadd.f32 %v110687, 1.0 (stack92)
        %v110689 = vmul.f32 %v110688, %v110682 (stack93)
        %v110690 = vand.u32 2147483647, %v110682 (stack94)
        %vm110691 = vcmp.lt.f32.partialorder %v110690, 0.0004427343 (stack95)
        %v110692 = vsel /*vm=*/%vm110691, /*on_true_vy=*/%v110689, /*on_false_vx=*/%v110686 (stack96)
        %v110693 = vxor.u32 %v110692, 2147483648 (stack87)
        %vm110696 = vcmp.lt.f32.partialorder %v110693, 5.0 (stack86)
        %v110701 = vsel /*vm=*/%vm110696, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v110705 = vsel /*vm=*/%vm110696, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v110709 = vsel /*vm=*/%vm110696, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v110713 = vsel /*vm=*/%vm110696, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v110717 = vsel /*vm=*/%vm110696, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v110721 = vsel /*vm=*/%vm110696, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v110725 = vsel /*vm=*/%vm110696, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v110729 = vsel /*vm=*/%vm110696, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v110733 = vsel /*vm=*/%vm110696, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v110737 = vadd.f32 %v110693, -2.5 (stack82)
        %v110739 = vrsqrt.pop %v110693 (stack97)
        %v110740 = vmul.f32 %v110693, %v110739 (stack98)
        %vm110741 = vcmp.eq.f32.partialorder %v110693, inf (stack99)
        %v110742 = vsel /*vm=*/%vm110741, /*on_true_vy=*/%v110693, /*on_false_vx=*/%v110740 (stack100)
        %vm110743 = vcmp.eq.f32.partialorder %v110693, 0.0 (stack101)
        %v110744 = vand.u32 %v110693, 2147483648 (stack102)
        %v110745 = vsel /*vm=*/%vm110743, /*on_true_vy=*/%v110744, /*on_false_vx=*/%v110742 (stack103)
        %v110748 = vadd.f32 %v110745, -3.0 (stack82)
        %v110752 = vsel /*vm=*/%vm110696, /*on_true_vy=*/%v110737, /*on_false_vx=*/%v110748 (stack72)
        %v110756 = vmul.f32 %v110733, %v110752 (stack83)
        %v110760 = vadd.f32 %v110729, %v110756 (stack82)
        %v110764 = vmul.f32 %v110760, %v110752 (stack83)
        %v110768 = vadd.f32 %v110725, %v110764 (stack82)
        %v110772 = vmul.f32 %v110768, %v110752 (stack83)
        %v110776 = vadd.f32 %v110721, %v110772 (stack82)
        %v110780 = vmul.f32 %v110776, %v110752 (stack83)
        %v110784 = vadd.f32 %v110717, %v110780 (stack82)
        %v110788 = vmul.f32 %v110784, %v110752 (stack83)
        %v110792 = vadd.f32 %v110713, %v110788 (stack82)
        %v110796 = vmul.f32 %v110792, %v110752 (stack83)
        %v110800 = vadd.f32 %v110709, %v110796 (stack82)
        %v110804 = vmul.f32 %v110800, %v110752 (stack83)
        %v110808 = vadd.f32 %v110705, %v110804 (stack82)
        %v110812 = vmul.f32 %v110808, %v110752 (stack83)
        %v110816 = vadd.f32 %v110701, %v110812 (stack82)
        %v110820 = vmul.f32 %v110816, %v110667 (stack83)
        %v110824 = vsel /*vm=*/%vm110672, /*on_true_vy=*/%v110677, /*on_false_vx=*/%v110820 (stack72)
        %v110828 = vmul.f32 %v110824, 1.4140625 (stack83)
        %s110830 = scalar_lea.vmem %s280, 628 [#allocation0] (stack107)
        %v110831 = vpack.c.bf16 0.0, %v110828 (stack104)
        %110832 = vst [vmem:[%s110830] sm:$0xf] /*vst_source=*/%v110831 (stack105)
        %v110835 = vadd.s32 %v2842, %v108527 (stack65)
        %s110837 = smul.u32 128, %s27 (stack66)
        %v110838 = vlaneseq (stack67)
        %v110839 = vand.u32 %v110838, 127 (stack68)
        %v110840 = vstv %s110837 (stack69)
        %v110841 = vadd.s32 %v110839, %v110840 (stack70)
        %v110845 = vadd.s32 %v110835, %v110841 (stack65)
        %vm110849 = vcmp.lt.u32.totalorder %v110845, %v110835 (stack71)
        %vm110854 = vcmp.lt.u32.totalorder %v110835, %v2842 (stack71)
        %v110859 = vadd.s32 %v2829, %v108510 (stack65)
        %v110863 = vadd.s32 %v110859, 1 (stack65)
        %v110867 = vsel /*vm=*/%vm110854, /*on_true_vy=*/%v110863, /*on_false_vx=*/%v110859 (stack72)
        %v110871 = vadd.s32 %v110867, 1 (stack65)
        %v110875 = vsel /*vm=*/%vm110849, /*on_true_vy=*/%v110871, /*on_false_vx=*/%v110867 (stack72)
        %v110880 = vadd.s32 %v110875, %v10 (stack65)
        %v110884 = vadd.s32 %v110845, %v9 (stack65)
        %v110888 = vadd.s32 %v110880, %v110884 (stack65)
        %v110890 = vshll.u32 %v110884, 13 (stack73)
        %v110891 = vshrl.u32 %v110884, 19 (stack74)
        %v110892 = vor.u32 %v110890, %v110891 (stack75)
        %v110893 = vxor.u32 %v110888, %v110892 (stack76)
        %v110896 = vadd.s32 %v110888, %v110893 (stack65)
        %v110898 = vshll.u32 %v110893, 15 (stack73)
        %v110899 = vshrl.u32 %v110893, 17 (stack74)
        %v110900 = vor.u32 %v110898, %v110899 (stack75)
        %v110901 = vxor.u32 %v110896, %v110900 (stack76)
        %v110904 = vadd.s32 %v110896, %v110901 (stack65)
        %v110906 = vshll.u32 %v110901, 26 (stack73)
        %v110907 = vshrl.u32 %v110901, 6 (stack74)
        %v110908 = vor.u32 %v110906, %v110907 (stack75)
        %v110909 = vxor.u32 %v110904, %v110908 (stack76)
        %v110912 = vadd.s32 %v110904, %v110909 (stack65)
        %v110916 = vadd.s32 %v110912, %v9 (stack65)
        %v110918 = vshll.u32 %v110909, 6 (stack73)
        %v110919 = vshrl.u32 %v110909, 26 (stack74)
        %v110920 = vor.u32 %v110918, %v110919 (stack75)
        %v110921 = vxor.u32 %v110912, %v110920 (stack76)
        %v110924 = vadd.s32 %v110921, %v8 (stack65)
        %v110928 = vadd.s32 %v110924, 1 (stack65)
        %v110932 = vadd.s32 %v110916, %v110928 (stack65)
        %v110934 = vshll.u32 %v110928, 17 (stack73)
        %v110935 = vshrl.u32 %v110928, 15 (stack74)
        %v110936 = vor.u32 %v110934, %v110935 (stack75)
        %v110937 = vxor.u32 %v110932, %v110936 (stack76)
        %v110940 = vadd.s32 %v110932, %v110937 (stack65)
        %v110942 = vshll.u32 %v110937, 29 (stack73)
        %v110943 = vshrl.u32 %v110937, 3 (stack74)
        %v110944 = vor.u32 %v110942, %v110943 (stack75)
        %v110945 = vxor.u32 %v110940, %v110944 (stack76)
        %v110948 = vadd.s32 %v110940, %v110945 (stack65)
        %v110950 = vshll.u32 %v110945, 16 (stack73)
        %v110951 = vshrl.u32 %v110945, 16 (stack74)
        %v110952 = vor.u32 %v110950, %v110951 (stack75)
        %v110953 = vxor.u32 %v110948, %v110952 (stack76)
        %v110956 = vadd.s32 %v110948, %v110953 (stack65)
        %v110960 = vadd.s32 %v110956, %v8 (stack65)
        %v110962 = vshll.u32 %v110953, 24 (stack73)
        %v110963 = vshrl.u32 %v110953, 8 (stack74)
        %v110964 = vor.u32 %v110962, %v110963 (stack75)
        %v110965 = vxor.u32 %v110956, %v110964 (stack76)
        %v110968 = vadd.s32 %v110965, %v10 (stack65)
        %v110972 = vadd.s32 %v110968, 2 (stack65)
        %v110976 = vadd.s32 %v110960, %v110972 (stack65)
        %v110978 = vshll.u32 %v110972, 13 (stack73)
        %v110979 = vshrl.u32 %v110972, 19 (stack74)
        %v110980 = vor.u32 %v110978, %v110979 (stack75)
        %v110981 = vxor.u32 %v110976, %v110980 (stack76)
        %v110984 = vadd.s32 %v110976, %v110981 (stack65)
        %v110986 = vshll.u32 %v110981, 15 (stack73)
        %v110987 = vshrl.u32 %v110981, 17 (stack74)
        %v110988 = vor.u32 %v110986, %v110987 (stack75)
        %v110989 = vxor.u32 %v110984, %v110988 (stack76)
        %v110992 = vadd.s32 %v110984, %v110989 (stack65)
        %v110994 = vshll.u32 %v110989, 26 (stack73)
        %v110995 = vshrl.u32 %v110989, 6 (stack74)
        %v110996 = vor.u32 %v110994, %v110995 (stack75)
        %v110997 = vxor.u32 %v110992, %v110996 (stack76)
        %v111000 = vadd.s32 %v110992, %v110997 (stack65)
        %v111004 = vadd.s32 %v111000, %v10 (stack65)
        %v111006 = vshll.u32 %v110997, 6 (stack73)
        %v111007 = vshrl.u32 %v110997, 26 (stack74)
        %v111008 = vor.u32 %v111006, %v111007 (stack75)
        %v111009 = vxor.u32 %v111000, %v111008 (stack76)
        %v111012 = vadd.s32 %v111009, %v9 (stack65)
        %v111016 = vadd.s32 %v111012, 3 (stack65)
        %v111020 = vadd.s32 %v111004, %v111016 (stack65)
        %v111022 = vshll.u32 %v111016, 17 (stack73)
        %v111023 = vshrl.u32 %v111016, 15 (stack74)
        %v111024 = vor.u32 %v111022, %v111023 (stack75)
        %v111025 = vxor.u32 %v111020, %v111024 (stack76)
        %v111028 = vadd.s32 %v111020, %v111025 (stack65)
        %v111030 = vshll.u32 %v111025, 29 (stack73)
        %v111031 = vshrl.u32 %v111025, 3 (stack74)
        %v111032 = vor.u32 %v111030, %v111031 (stack75)
        %v111033 = vxor.u32 %v111028, %v111032 (stack76)
        %v111036 = vadd.s32 %v111028, %v111033 (stack65)
        %v111038 = vshll.u32 %v111033, 16 (stack73)
        %v111039 = vshrl.u32 %v111033, 16 (stack74)
        %v111040 = vor.u32 %v111038, %v111039 (stack75)
        %v111041 = vxor.u32 %v111036, %v111040 (stack76)
        %v111044 = vadd.s32 %v111036, %v111041 (stack65)
        %v111048 = vadd.s32 %v111044, %v9 (stack65)
        %v111050 = vshll.u32 %v111041, 24 (stack73)
        %v111051 = vshrl.u32 %v111041, 8 (stack74)
        %v111052 = vor.u32 %v111050, %v111051 (stack75)
        %v111053 = vxor.u32 %v111044, %v111052 (stack76)
        %v111056 = vadd.s32 %v111053, %v8 (stack65)
        %v111060 = vadd.s32 %v111056, 4 (stack65)
        %v111064 = vadd.s32 %v111048, %v111060 (stack65)
        %v111066 = vshll.u32 %v111060, 13 (stack73)
        %v111067 = vshrl.u32 %v111060, 19 (stack74)
        %v111068 = vor.u32 %v111066, %v111067 (stack75)
        %v111069 = vxor.u32 %v111064, %v111068 (stack76)
        %v111072 = vadd.s32 %v111064, %v111069 (stack65)
        %v111074 = vshll.u32 %v111069, 15 (stack73)
        %v111075 = vshrl.u32 %v111069, 17 (stack74)
        %v111076 = vor.u32 %v111074, %v111075 (stack75)
        %v111077 = vxor.u32 %v111072, %v111076 (stack76)
        %v111080 = vadd.s32 %v111072, %v111077 (stack65)
        %v111082 = vshll.u32 %v111077, 26 (stack73)
        %v111083 = vshrl.u32 %v111077, 6 (stack74)
        %v111084 = vor.u32 %v111082, %v111083 (stack75)
        %v111085 = vxor.u32 %v111080, %v111084 (stack76)
        %v111088 = vadd.s32 %v111080, %v111085 (stack65)
        %v111092 = vadd.s32 %v111088, %v8 (stack65)
        %v111094 = vshll.u32 %v111085, 6 (stack73)
        %v111095 = vshrl.u32 %v111085, 26 (stack74)
        %v111096 = vor.u32 %v111094, %v111095 (stack75)
        %v111097 = vxor.u32 %v111088, %v111096 (stack76)
        %v111100 = vadd.s32 %v111097, %v10 (stack65)
        %v111104 = vadd.s32 %v111100, 5 (stack65)
        %v111106 = vxor.u32 %v111092, %v111104 (stack76)
        %v111107 = vand.u32.u8 %v111106, 255 (stack77)
        %v111108 = vand.u32 %v111107, 65535 (stack78)
        %v111109 = vshrl.u32 %v111108, 1 (stack79)
        %v111110 = vor.u32 %v111109, 16256 (stack75)
        %v111111 = vand.u32.u16 %v111110, 65535 (stack80)
        %v111112 = vunpack.i.l.bf16 %v111111 (stack81)
        %v111116 = vadd.f32 %v111112, -1.0 (stack82)
        %v111120 = vmul.f32 %v111116, 2.0 (stack83)
        %v111124 = vadd.f32 %v111120, -0.99609375 (stack82)
        %v111128 = vmax.f32 -0.99609375, %v111124 (stack84)
        %v111130 = vand.u32 2147483647, %v111128 (stack85)
        %vm111133 = vcmp.eq.f32.partialorder %v111130, 1.0 (stack86)
        %v111138 = vmul.f32 %v111128, inf (stack83)
        %v111140 = vxor.u32 %v111128, 2147483648 (stack87)
        %v111143 = vmul.f32 %v111128, %v111140 (stack83)
        %v111145 = vadd.f32 %v111143, 1.0 (stack88)
        %v111146 = vlog2.pop %v111145 (stack89)
        %v111147 = vmul.f32 %v111146, 0.6931472 (stack90)
        %v111148 = vmul.f32 -0.5, %v111143 (stack91)
        %v111149 = vadd.f32 %v111148, 1.0 (stack92)
        %v111150 = vmul.f32 %v111149, %v111143 (stack93)
        %v111151 = vand.u32 2147483647, %v111143 (stack94)
        %vm111152 = vcmp.lt.f32.partialorder %v111151, 0.0004427343 (stack95)
        %v111153 = vsel /*vm=*/%vm111152, /*on_true_vy=*/%v111150, /*on_false_vx=*/%v111147 (stack96)
        %v111154 = vxor.u32 %v111153, 2147483648 (stack87)
        %vm111157 = vcmp.lt.f32.partialorder %v111154, 5.0 (stack86)
        %v111162 = vsel /*vm=*/%vm111157, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v111166 = vsel /*vm=*/%vm111157, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v111170 = vsel /*vm=*/%vm111157, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v111174 = vsel /*vm=*/%vm111157, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v111178 = vsel /*vm=*/%vm111157, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v111182 = vsel /*vm=*/%vm111157, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v111186 = vsel /*vm=*/%vm111157, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v111190 = vsel /*vm=*/%vm111157, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v111194 = vsel /*vm=*/%vm111157, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v111198 = vadd.f32 %v111154, -2.5 (stack82)
        %v111200 = vrsqrt.pop %v111154 (stack97)
        %v111201 = vmul.f32 %v111154, %v111200 (stack98)
        %vm111202 = vcmp.eq.f32.partialorder %v111154, inf (stack99)
        %v111203 = vsel /*vm=*/%vm111202, /*on_true_vy=*/%v111154, /*on_false_vx=*/%v111201 (stack100)
        %vm111204 = vcmp.eq.f32.partialorder %v111154, 0.0 (stack101)
        %v111205 = vand.u32 %v111154, 2147483648 (stack102)
        %v111206 = vsel /*vm=*/%vm111204, /*on_true_vy=*/%v111205, /*on_false_vx=*/%v111203 (stack103)
        %v111209 = vadd.f32 %v111206, -3.0 (stack82)
        %v111213 = vsel /*vm=*/%vm111157, /*on_true_vy=*/%v111198, /*on_false_vx=*/%v111209 (stack72)
        %v111217 = vmul.f32 %v111194, %v111213 (stack83)
        %v111221 = vadd.f32 %v111190, %v111217 (stack82)
        %v111225 = vmul.f32 %v111221, %v111213 (stack83)
        %v111229 = vadd.f32 %v111186, %v111225 (stack82)
        %v111233 = vmul.f32 %v111229, %v111213 (stack83)
        %v111237 = vadd.f32 %v111182, %v111233 (stack82)
        %v111241 = vmul.f32 %v111237, %v111213 (stack83)
        %v111245 = vadd.f32 %v111178, %v111241 (stack82)
        %v111249 = vmul.f32 %v111245, %v111213 (stack83)
        %v111253 = vadd.f32 %v111174, %v111249 (stack82)
        %v111257 = vmul.f32 %v111253, %v111213 (stack83)
        %v111261 = vadd.f32 %v111170, %v111257 (stack82)
        %v111265 = vmul.f32 %v111261, %v111213 (stack83)
        %v111269 = vadd.f32 %v111166, %v111265 (stack82)
        %v111273 = vmul.f32 %v111269, %v111213 (stack83)
        %v111277 = vadd.f32 %v111162, %v111273 (stack82)
        %v111281 = vmul.f32 %v111277, %v111128 (stack83)
        %v111285 = vsel /*vm=*/%vm111133, /*on_true_vy=*/%v111138, /*on_false_vx=*/%v111281 (stack72)
        %v111289 = vmul.f32 %v111285, 1.4140625 (stack83)
        %s111291 = scalar_lea.vmem %s280, 756 [#allocation0] (stack107)
        %v111292 = vpack.c.bf16 0.0, %v111289 (stack104)
        %111293 = vst [vmem:[%s111291] sm:$0xf] /*vst_source=*/%v111292 (stack105)
        %v111296 = vadd.s32 %v3329, %v108527 (stack65)
        %s111298 = smul.u32 128, %s27 (stack66)
        %v111299 = vlaneseq (stack67)
        %v111300 = vand.u32 %v111299, 127 (stack68)
        %v111301 = vstv %s111298 (stack69)
        %v111302 = vadd.s32 %v111300, %v111301 (stack70)
        %v111306 = vadd.s32 %v111296, %v111302 (stack65)
        %vm111310 = vcmp.lt.u32.totalorder %v111306, %v111296 (stack71)
        %vm111315 = vcmp.lt.u32.totalorder %v111296, %v3329 (stack71)
        %v111320 = vadd.s32 %v3316, %v108510 (stack65)
        %v111324 = vadd.s32 %v111320, 1 (stack65)
        %v111328 = vsel /*vm=*/%vm111315, /*on_true_vy=*/%v111324, /*on_false_vx=*/%v111320 (stack72)
        %v111332 = vadd.s32 %v111328, 1 (stack65)
        %v111336 = vsel /*vm=*/%vm111310, /*on_true_vy=*/%v111332, /*on_false_vx=*/%v111328 (stack72)
        %v111341 = vadd.s32 %v111336, %v10 (stack65)
        %v111345 = vadd.s32 %v111306, %v9 (stack65)
        %v111349 = vadd.s32 %v111341, %v111345 (stack65)
        %v111351 = vshll.u32 %v111345, 13 (stack73)
        %v111352 = vshrl.u32 %v111345, 19 (stack74)
        %v111353 = vor.u32 %v111351, %v111352 (stack75)
        %v111354 = vxor.u32 %v111349, %v111353 (stack76)
        %v111357 = vadd.s32 %v111349, %v111354 (stack65)
        %v111359 = vshll.u32 %v111354, 15 (stack73)
        %v111360 = vshrl.u32 %v111354, 17 (stack74)
        %v111361 = vor.u32 %v111359, %v111360 (stack75)
        %v111362 = vxor.u32 %v111357, %v111361 (stack76)
        %v111365 = vadd.s32 %v111357, %v111362 (stack65)
        %v111367 = vshll.u32 %v111362, 26 (stack73)
        %v111368 = vshrl.u32 %v111362, 6 (stack74)
        %v111369 = vor.u32 %v111367, %v111368 (stack75)
        %v111370 = vxor.u32 %v111365, %v111369 (stack76)
        %v111373 = vadd.s32 %v111365, %v111370 (stack65)
        %v111377 = vadd.s32 %v111373, %v9 (stack65)
        %v111379 = vshll.u32 %v111370, 6 (stack73)
        %v111380 = vshrl.u32 %v111370, 26 (stack74)
        %v111381 = vor.u32 %v111379, %v111380 (stack75)
        %v111382 = vxor.u32 %v111373, %v111381 (stack76)
        %v111385 = vadd.s32 %v111382, %v8 (stack65)
        %v111389 = vadd.s32 %v111385, 1 (stack65)
        %v111393 = vadd.s32 %v111377, %v111389 (stack65)
        %v111395 = vshll.u32 %v111389, 17 (stack73)
        %v111396 = vshrl.u32 %v111389, 15 (stack74)
        %v111397 = vor.u32 %v111395, %v111396 (stack75)
        %v111398 = vxor.u32 %v111393, %v111397 (stack76)
        %v111401 = vadd.s32 %v111393, %v111398 (stack65)
        %v111403 = vshll.u32 %v111398, 29 (stack73)
        %v111404 = vshrl.u32 %v111398, 3 (stack74)
        %v111405 = vor.u32 %v111403, %v111404 (stack75)
        %v111406 = vxor.u32 %v111401, %v111405 (stack76)
        %v111409 = vadd.s32 %v111401, %v111406 (stack65)
        %v111411 = vshll.u32 %v111406, 16 (stack73)
        %v111412 = vshrl.u32 %v111406, 16 (stack74)
        %v111413 = vor.u32 %v111411, %v111412 (stack75)
        %v111414 = vxor.u32 %v111409, %v111413 (stack76)
        %v111417 = vadd.s32 %v111409, %v111414 (stack65)
        %v111421 = vadd.s32 %v111417, %v8 (stack65)
        %v111423 = vshll.u32 %v111414, 24 (stack73)
        %v111424 = vshrl.u32 %v111414, 8 (stack74)
        %v111425 = vor.u32 %v111423, %v111424 (stack75)
        %v111426 = vxor.u32 %v111417, %v111425 (stack76)
        %v111429 = vadd.s32 %v111426, %v10 (stack65)
        %v111433 = vadd.s32 %v111429, 2 (stack65)
        %v111437 = vadd.s32 %v111421, %v111433 (stack65)
        %v111439 = vshll.u32 %v111433, 13 (stack73)
        %v111440 = vshrl.u32 %v111433, 19 (stack74)
        %v111441 = vor.u32 %v111439, %v111440 (stack75)
        %v111442 = vxor.u32 %v111437, %v111441 (stack76)
        %v111445 = vadd.s32 %v111437, %v111442 (stack65)
        %v111447 = vshll.u32 %v111442, 15 (stack73)
        %v111448 = vshrl.u32 %v111442, 17 (stack74)
        %v111449 = vor.u32 %v111447, %v111448 (stack75)
        %v111450 = vxor.u32 %v111445, %v111449 (stack76)
        %v111453 = vadd.s32 %v111445, %v111450 (stack65)
        %v111455 = vshll.u32 %v111450, 26 (stack73)
        %v111456 = vshrl.u32 %v111450, 6 (stack74)
        %v111457 = vor.u32 %v111455, %v111456 (stack75)
        %v111458 = vxor.u32 %v111453, %v111457 (stack76)
        %v111461 = vadd.s32 %v111453, %v111458 (stack65)
        %v111465 = vadd.s32 %v111461, %v10 (stack65)
        %v111467 = vshll.u32 %v111458, 6 (stack73)
        %v111468 = vshrl.u32 %v111458, 26 (stack74)
        %v111469 = vor.u32 %v111467, %v111468 (stack75)
        %v111470 = vxor.u32 %v111461, %v111469 (stack76)
        %v111473 = vadd.s32 %v111470, %v9 (stack65)
        %v111477 = vadd.s32 %v111473, 3 (stack65)
        %v111481 = vadd.s32 %v111465, %v111477 (stack65)
        %v111483 = vshll.u32 %v111477, 17 (stack73)
        %v111484 = vshrl.u32 %v111477, 15 (stack74)
        %v111485 = vor.u32 %v111483, %v111484 (stack75)
        %v111486 = vxor.u32 %v111481, %v111485 (stack76)
        %v111489 = vadd.s32 %v111481, %v111486 (stack65)
        %v111491 = vshll.u32 %v111486, 29 (stack73)
        %v111492 = vshrl.u32 %v111486, 3 (stack74)
        %v111493 = vor.u32 %v111491, %v111492 (stack75)
        %v111494 = vxor.u32 %v111489, %v111493 (stack76)
        %v111497 = vadd.s32 %v111489, %v111494 (stack65)
        %v111499 = vshll.u32 %v111494, 16 (stack73)
        %v111500 = vshrl.u32 %v111494, 16 (stack74)
        %v111501 = vor.u32 %v111499, %v111500 (stack75)
        %v111502 = vxor.u32 %v111497, %v111501 (stack76)
        %v111505 = vadd.s32 %v111497, %v111502 (stack65)
        %v111509 = vadd.s32 %v111505, %v9 (stack65)
        %v111511 = vshll.u32 %v111502, 24 (stack73)
        %v111512 = vshrl.u32 %v111502, 8 (stack74)
        %v111513 = vor.u32 %v111511, %v111512 (stack75)
        %v111514 = vxor.u32 %v111505, %v111513 (stack76)
        %v111517 = vadd.s32 %v111514, %v8 (stack65)
        %v111521 = vadd.s32 %v111517, 4 (stack65)
        %v111525 = vadd.s32 %v111509, %v111521 (stack65)
        %v111527 = vshll.u32 %v111521, 13 (stack73)
        %v111528 = vshrl.u32 %v111521, 19 (stack74)
        %v111529 = vor.u32 %v111527, %v111528 (stack75)
        %v111530 = vxor.u32 %v111525, %v111529 (stack76)
        %v111533 = vadd.s32 %v111525, %v111530 (stack65)
        %v111535 = vshll.u32 %v111530, 15 (stack73)
        %v111536 = vshrl.u32 %v111530, 17 (stack74)
        %v111537 = vor.u32 %v111535, %v111536 (stack75)
        %v111538 = vxor.u32 %v111533, %v111537 (stack76)
        %v111541 = vadd.s32 %v111533, %v111538 (stack65)
        %v111543 = vshll.u32 %v111538, 26 (stack73)
        %v111544 = vshrl.u32 %v111538, 6 (stack74)
        %v111545 = vor.u32 %v111543, %v111544 (stack75)
        %v111546 = vxor.u32 %v111541, %v111545 (stack76)
        %v111549 = vadd.s32 %v111541, %v111546 (stack65)
        %v111553 = vadd.s32 %v111549, %v8 (stack65)
        %v111555 = vshll.u32 %v111546, 6 (stack73)
        %v111556 = vshrl.u32 %v111546, 26 (stack74)
        %v111557 = vor.u32 %v111555, %v111556 (stack75)
        %v111558 = vxor.u32 %v111549, %v111557 (stack76)
        %v111561 = vadd.s32 %v111558, %v10 (stack65)
        %v111565 = vadd.s32 %v111561, 5 (stack65)
        %v111567 = vxor.u32 %v111553, %v111565 (stack76)
        %v111568 = vand.u32.u8 %v111567, 255 (stack77)
        %v111569 = vand.u32 %v111568, 65535 (stack78)
        %v111570 = vshrl.u32 %v111569, 1 (stack79)
        %v111571 = vor.u32 %v111570, 16256 (stack75)
        %v111572 = vand.u32.u16 %v111571, 65535 (stack80)
        %v111573 = vunpack.i.l.bf16 %v111572 (stack81)
        %v111577 = vadd.f32 %v111573, -1.0 (stack82)
        %v111581 = vmul.f32 %v111577, 2.0 (stack83)
        %v111585 = vadd.f32 %v111581, -0.99609375 (stack82)
        %v111589 = vmax.f32 -0.99609375, %v111585 (stack84)
        %v111591 = vand.u32 2147483647, %v111589 (stack85)
        %vm111594 = vcmp.eq.f32.partialorder %v111591, 1.0 (stack86)
        %v111599 = vmul.f32 %v111589, inf (stack83)
        %v111601 = vxor.u32 %v111589, 2147483648 (stack87)
        %v111604 = vmul.f32 %v111589, %v111601 (stack83)
        %v111606 = vadd.f32 %v111604, 1.0 (stack88)
        %v111607 = vlog2.pop %v111606 (stack89)
        %v111608 = vmul.f32 %v111607, 0.6931472 (stack90)
        %v111609 = vmul.f32 -0.5, %v111604 (stack91)
        %v111610 = vadd.f32 %v111609, 1.0 (stack92)
        %v111611 = vmul.f32 %v111610, %v111604 (stack93)
        %v111612 = vand.u32 2147483647, %v111604 (stack94)
        %vm111613 = vcmp.lt.f32.partialorder %v111612, 0.0004427343 (stack95)
        %v111614 = vsel /*vm=*/%vm111613, /*on_true_vy=*/%v111611, /*on_false_vx=*/%v111608 (stack96)
        %v111615 = vxor.u32 %v111614, 2147483648 (stack87)
        %vm111618 = vcmp.lt.f32.partialorder %v111615, 5.0 (stack86)
        %v111623 = vsel /*vm=*/%vm111618, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v111627 = vsel /*vm=*/%vm111618, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v111631 = vsel /*vm=*/%vm111618, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v111635 = vsel /*vm=*/%vm111618, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v111639 = vsel /*vm=*/%vm111618, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v111643 = vsel /*vm=*/%vm111618, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v111647 = vsel /*vm=*/%vm111618, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v111651 = vsel /*vm=*/%vm111618, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v111655 = vsel /*vm=*/%vm111618, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v111659 = vadd.f32 %v111615, -2.5 (stack82)
        %v111661 = vrsqrt.pop %v111615 (stack97)
        %v111662 = vmul.f32 %v111615, %v111661 (stack98)
        %vm111663 = vcmp.eq.f32.partialorder %v111615, inf (stack99)
        %v111664 = vsel /*vm=*/%vm111663, /*on_true_vy=*/%v111615, /*on_false_vx=*/%v111662 (stack100)
        %vm111665 = vcmp.eq.f32.partialorder %v111615, 0.0 (stack101)
        %v111666 = vand.u32 %v111615, 2147483648 (stack102)
        %v111667 = vsel /*vm=*/%vm111665, /*on_true_vy=*/%v111666, /*on_false_vx=*/%v111664 (stack103)
        %v111670 = vadd.f32 %v111667, -3.0 (stack82)
        %v111674 = vsel /*vm=*/%vm111618, /*on_true_vy=*/%v111659, /*on_false_vx=*/%v111670 (stack72)
        %v111678 = vmul.f32 %v111655, %v111674 (stack83)
        %v111682 = vadd.f32 %v111651, %v111678 (stack82)
        %v111686 = vmul.f32 %v111682, %v111674 (stack83)
        %v111690 = vadd.f32 %v111647, %v111686 (stack82)
        %v111694 = vmul.f32 %v111690, %v111674 (stack83)
        %v111698 = vadd.f32 %v111643, %v111694 (stack82)
        %v111702 = vmul.f32 %v111698, %v111674 (stack83)
        %v111706 = vadd.f32 %v111639, %v111702 (stack82)
        %v111710 = vmul.f32 %v111706, %v111674 (stack83)
        %v111714 = vadd.f32 %v111635, %v111710 (stack82)
        %v111718 = vmul.f32 %v111714, %v111674 (stack83)
        %v111722 = vadd.f32 %v111631, %v111718 (stack82)
        %v111726 = vmul.f32 %v111722, %v111674 (stack83)
        %v111730 = vadd.f32 %v111627, %v111726 (stack82)
        %v111734 = vmul.f32 %v111730, %v111674 (stack83)
        %v111738 = vadd.f32 %v111623, %v111734 (stack82)
        %v111742 = vmul.f32 %v111738, %v111589 (stack83)
        %v111746 = vsel /*vm=*/%vm111594, /*on_true_vy=*/%v111599, /*on_false_vx=*/%v111742 (stack72)
        %v111750 = vmul.f32 %v111746, 1.4140625 (stack83)
        %s111752 = scalar_lea.vmem %s280, 884 [#allocation0] (stack107)
        %v111753 = vpack.c.bf16 0.0, %v111750 (stack104)
        %111754 = vst [vmem:[%s111752] sm:$0xf] /*vst_source=*/%v111753 (stack105)
        %v111757 = vadd.s32 %v3816, %v108527 (stack65)
        %s111759 = smul.u32 128, %s27 (stack66)
        %v111760 = vlaneseq (stack67)
        %v111761 = vand.u32 %v111760, 127 (stack68)
        %v111762 = vstv %s111759 (stack69)
        %v111763 = vadd.s32 %v111761, %v111762 (stack70)
        %v111767 = vadd.s32 %v111757, %v111763 (stack65)
        %vm111771 = vcmp.lt.u32.totalorder %v111767, %v111757 (stack71)
        %vm111776 = vcmp.lt.u32.totalorder %v111757, %v3816 (stack71)
        %v111781 = vadd.s32 %v3803, %v108510 (stack65)
        %v111785 = vadd.s32 %v111781, 1 (stack65)
        %v111789 = vsel /*vm=*/%vm111776, /*on_true_vy=*/%v111785, /*on_false_vx=*/%v111781 (stack72)
        %v111793 = vadd.s32 %v111789, 1 (stack65)
        %v111797 = vsel /*vm=*/%vm111771, /*on_true_vy=*/%v111793, /*on_false_vx=*/%v111789 (stack72)
        %v111802 = vadd.s32 %v111797, %v10 (stack65)
        %v111806 = vadd.s32 %v111767, %v9 (stack65)
        %v111810 = vadd.s32 %v111802, %v111806 (stack65)
        %v111812 = vshll.u32 %v111806, 13 (stack73)
        %v111813 = vshrl.u32 %v111806, 19 (stack74)
        %v111814 = vor.u32 %v111812, %v111813 (stack75)
        %v111815 = vxor.u32 %v111810, %v111814 (stack76)
        %v111818 = vadd.s32 %v111810, %v111815 (stack65)
        %v111820 = vshll.u32 %v111815, 15 (stack73)
        %v111821 = vshrl.u32 %v111815, 17 (stack74)
        %v111822 = vor.u32 %v111820, %v111821 (stack75)
        %v111823 = vxor.u32 %v111818, %v111822 (stack76)
        %v111826 = vadd.s32 %v111818, %v111823 (stack65)
        %v111828 = vshll.u32 %v111823, 26 (stack73)
        %v111829 = vshrl.u32 %v111823, 6 (stack74)
        %v111830 = vor.u32 %v111828, %v111829 (stack75)
        %v111831 = vxor.u32 %v111826, %v111830 (stack76)
        %v111834 = vadd.s32 %v111826, %v111831 (stack65)
        %v111838 = vadd.s32 %v111834, %v9 (stack65)
        %v111840 = vshll.u32 %v111831, 6 (stack73)
        %v111841 = vshrl.u32 %v111831, 26 (stack74)
        %v111842 = vor.u32 %v111840, %v111841 (stack75)
        %v111843 = vxor.u32 %v111834, %v111842 (stack76)
        %v111846 = vadd.s32 %v111843, %v8 (stack65)
        %v111850 = vadd.s32 %v111846, 1 (stack65)
        %v111854 = vadd.s32 %v111838, %v111850 (stack65)
        %v111856 = vshll.u32 %v111850, 17 (stack73)
        %v111857 = vshrl.u32 %v111850, 15 (stack74)
        %v111858 = vor.u32 %v111856, %v111857 (stack75)
        %v111859 = vxor.u32 %v111854, %v111858 (stack76)
        %v111862 = vadd.s32 %v111854, %v111859 (stack65)
        %v111864 = vshll.u32 %v111859, 29 (stack73)
        %v111865 = vshrl.u32 %v111859, 3 (stack74)
        %v111866 = vor.u32 %v111864, %v111865 (stack75)
        %v111867 = vxor.u32 %v111862, %v111866 (stack76)
        %v111870 = vadd.s32 %v111862, %v111867 (stack65)
        %v111872 = vshll.u32 %v111867, 16 (stack73)
        %v111873 = vshrl.u32 %v111867, 16 (stack74)
        %v111874 = vor.u32 %v111872, %v111873 (stack75)
        %v111875 = vxor.u32 %v111870, %v111874 (stack76)
        %v111878 = vadd.s32 %v111870, %v111875 (stack65)
        %v111882 = vadd.s32 %v111878, %v8 (stack65)
        %v111884 = vshll.u32 %v111875, 24 (stack73)
        %v111885 = vshrl.u32 %v111875, 8 (stack74)
        %v111886 = vor.u32 %v111884, %v111885 (stack75)
        %v111887 = vxor.u32 %v111878, %v111886 (stack76)
        %v111890 = vadd.s32 %v111887, %v10 (stack65)
        %v111894 = vadd.s32 %v111890, 2 (stack65)
        %v111898 = vadd.s32 %v111882, %v111894 (stack65)
        %v111900 = vshll.u32 %v111894, 13 (stack73)
        %v111901 = vshrl.u32 %v111894, 19 (stack74)
        %v111902 = vor.u32 %v111900, %v111901 (stack75)
        %v111903 = vxor.u32 %v111898, %v111902 (stack76)
        %v111906 = vadd.s32 %v111898, %v111903 (stack65)
        %v111908 = vshll.u32 %v111903, 15 (stack73)
        %v111909 = vshrl.u32 %v111903, 17 (stack74)
        %v111910 = vor.u32 %v111908, %v111909 (stack75)
        %v111911 = vxor.u32 %v111906, %v111910 (stack76)
        %v111914 = vadd.s32 %v111906, %v111911 (stack65)
        %v111916 = vshll.u32 %v111911, 26 (stack73)
        %v111917 = vshrl.u32 %v111911, 6 (stack74)
        %v111918 = vor.u32 %v111916, %v111917 (stack75)
        %v111919 = vxor.u32 %v111914, %v111918 (stack76)
        %v111922 = vadd.s32 %v111914, %v111919 (stack65)
        %v111926 = vadd.s32 %v111922, %v10 (stack65)
        %v111928 = vshll.u32 %v111919, 6 (stack73)
        %v111929 = vshrl.u32 %v111919, 26 (stack74)
        %v111930 = vor.u32 %v111928, %v111929 (stack75)
        %v111931 = vxor.u32 %v111922, %v111930 (stack76)
        %v111934 = vadd.s32 %v111931, %v9 (stack65)
        %v111938 = vadd.s32 %v111934, 3 (stack65)
        %v111942 = vadd.s32 %v111926, %v111938 (stack65)
        %v111944 = vshll.u32 %v111938, 17 (stack73)
        %v111945 = vshrl.u32 %v111938, 15 (stack74)
        %v111946 = vor.u32 %v111944, %v111945 (stack75)
        %v111947 = vxor.u32 %v111942, %v111946 (stack76)
        %v111950 = vadd.s32 %v111942, %v111947 (stack65)
        %v111952 = vshll.u32 %v111947, 29 (stack73)
        %v111953 = vshrl.u32 %v111947, 3 (stack74)
        %v111954 = vor.u32 %v111952, %v111953 (stack75)
        %v111955 = vxor.u32 %v111950, %v111954 (stack76)
        %v111958 = vadd.s32 %v111950, %v111955 (stack65)
        %v111960 = vshll.u32 %v111955, 16 (stack73)
        %v111961 = vshrl.u32 %v111955, 16 (stack74)
        %v111962 = vor.u32 %v111960, %v111961 (stack75)
        %v111963 = vxor.u32 %v111958, %v111962 (stack76)
        %v111966 = vadd.s32 %v111958, %v111963 (stack65)
        %v111970 = vadd.s32 %v111966, %v9 (stack65)
        %v111972 = vshll.u32 %v111963, 24 (stack73)
        %v111973 = vshrl.u32 %v111963, 8 (stack74)
        %v111974 = vor.u32 %v111972, %v111973 (stack75)
        %v111975 = vxor.u32 %v111966, %v111974 (stack76)
        %v111978 = vadd.s32 %v111975, %v8 (stack65)
        %v111982 = vadd.s32 %v111978, 4 (stack65)
        %v111986 = vadd.s32 %v111970, %v111982 (stack65)
        %v111988 = vshll.u32 %v111982, 13 (stack73)
        %v111989 = vshrl.u32 %v111982, 19 (stack74)
        %v111990 = vor.u32 %v111988, %v111989 (stack75)
        %v111991 = vxor.u32 %v111986, %v111990 (stack76)
        %v111994 = vadd.s32 %v111986, %v111991 (stack65)
        %v111996 = vshll.u32 %v111991, 15 (stack73)
        %v111997 = vshrl.u32 %v111991, 17 (stack74)
        %v111998 = vor.u32 %v111996, %v111997 (stack75)
        %v111999 = vxor.u32 %v111994, %v111998 (stack76)
        %v112002 = vadd.s32 %v111994, %v111999 (stack65)
        %v112004 = vshll.u32 %v111999, 26 (stack73)
        %v112005 = vshrl.u32 %v111999, 6 (stack74)
        %v112006 = vor.u32 %v112004, %v112005 (stack75)
        %v112007 = vxor.u32 %v112002, %v112006 (stack76)
        %v112010 = vadd.s32 %v112002, %v112007 (stack65)
        %v112014 = vadd.s32 %v112010, %v8 (stack65)
        %v112016 = vshll.u32 %v112007, 6 (stack73)
        %v112017 = vshrl.u32 %v112007, 26 (stack74)
        %v112018 = vor.u32 %v112016, %v112017 (stack75)
        %v112019 = vxor.u32 %v112010, %v112018 (stack76)
        %v112022 = vadd.s32 %v112019, %v10 (stack65)
        %v112026 = vadd.s32 %v112022, 5 (stack65)
        %v112028 = vxor.u32 %v112014, %v112026 (stack76)
        %v112029 = vand.u32.u8 %v112028, 255 (stack77)
        %v112030 = vand.u32 %v112029, 65535 (stack78)
        %v112031 = vshrl.u32 %v112030, 1 (stack79)
        %v112032 = vor.u32 %v112031, 16256 (stack75)
        %v112033 = vand.u32.u16 %v112032, 65535 (stack80)
        %v112034 = vunpack.i.l.bf16 %v112033 (stack81)
        %v112038 = vadd.f32 %v112034, -1.0 (stack82)
        %v112042 = vmul.f32 %v112038, 2.0 (stack83)
        %v112046 = vadd.f32 %v112042, -0.99609375 (stack82)
        %v112050 = vmax.f32 -0.99609375, %v112046 (stack84)
        %v112052 = vand.u32 2147483647, %v112050 (stack85)
        %vm112055 = vcmp.eq.f32.partialorder %v112052, 1.0 (stack86)
        %v112060 = vmul.f32 %v112050, inf (stack83)
        %v112062 = vxor.u32 %v112050, 2147483648 (stack87)
        %v112065 = vmul.f32 %v112050, %v112062 (stack83)
        %v112067 = vadd.f32 %v112065, 1.0 (stack88)
        %v112068 = vlog2.pop %v112067 (stack89)
        %v112069 = vmul.f32 %v112068, 0.6931472 (stack90)
        %v112070 = vmul.f32 -0.5, %v112065 (stack91)
        %v112071 = vadd.f32 %v112070, 1.0 (stack92)
        %v112072 = vmul.f32 %v112071, %v112065 (stack93)
        %v112073 = vand.u32 2147483647, %v112065 (stack94)
        %vm112074 = vcmp.lt.f32.partialorder %v112073, 0.0004427343 (stack95)
        %v112075 = vsel /*vm=*/%vm112074, /*on_true_vy=*/%v112072, /*on_false_vx=*/%v112069 (stack96)
        %v112076 = vxor.u32 %v112075, 2147483648 (stack87)
        %vm112079 = vcmp.lt.f32.partialorder %v112076, 5.0 (stack86)
        %v112084 = vsel /*vm=*/%vm112079, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v112088 = vsel /*vm=*/%vm112079, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v112092 = vsel /*vm=*/%vm112079, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v112096 = vsel /*vm=*/%vm112079, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v112100 = vsel /*vm=*/%vm112079, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v112104 = vsel /*vm=*/%vm112079, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v112108 = vsel /*vm=*/%vm112079, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v112112 = vsel /*vm=*/%vm112079, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v112116 = vsel /*vm=*/%vm112079, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v112120 = vadd.f32 %v112076, -2.5 (stack82)
        %v112122 = vrsqrt.pop %v112076 (stack97)
        %v112123 = vmul.f32 %v112076, %v112122 (stack98)
        %vm112124 = vcmp.eq.f32.partialorder %v112076, inf (stack99)
        %v112125 = vsel /*vm=*/%vm112124, /*on_true_vy=*/%v112076, /*on_false_vx=*/%v112123 (stack100)
        %vm112126 = vcmp.eq.f32.partialorder %v112076, 0.0 (stack101)
        %v112127 = vand.u32 %v112076, 2147483648 (stack102)
        %v112128 = vsel /*vm=*/%vm112126, /*on_true_vy=*/%v112127, /*on_false_vx=*/%v112125 (stack103)
        %v112131 = vadd.f32 %v112128, -3.0 (stack82)
        %v112135 = vsel /*vm=*/%vm112079, /*on_true_vy=*/%v112120, /*on_false_vx=*/%v112131 (stack72)
        %v112139 = vmul.f32 %v112116, %v112135 (stack83)
        %v112143 = vadd.f32 %v112112, %v112139 (stack82)
        %v112147 = vmul.f32 %v112143, %v112135 (stack83)
        %v112151 = vadd.f32 %v112108, %v112147 (stack82)
        %v112155 = vmul.f32 %v112151, %v112135 (stack83)
        %v112159 = vadd.f32 %v112104, %v112155 (stack82)
        %v112163 = vmul.f32 %v112159, %v112135 (stack83)
        %v112167 = vadd.f32 %v112100, %v112163 (stack82)
        %v112171 = vmul.f32 %v112167, %v112135 (stack83)
        %v112175 = vadd.f32 %v112096, %v112171 (stack82)
        %v112179 = vmul.f32 %v112175, %v112135 (stack83)
        %v112183 = vadd.f32 %v112092, %v112179 (stack82)
        %v112187 = vmul.f32 %v112183, %v112135 (stack83)
        %v112191 = vadd.f32 %v112088, %v112187 (stack82)
        %v112195 = vmul.f32 %v112191, %v112135 (stack83)
        %v112199 = vadd.f32 %v112084, %v112195 (stack82)
        %v112203 = vmul.f32 %v112199, %v112050 (stack83)
        %v112207 = vsel /*vm=*/%vm112055, /*on_true_vy=*/%v112060, /*on_false_vx=*/%v112203 (stack72)
        %v112211 = vmul.f32 %v112207, 1.4140625 (stack83)
        %s112213 = scalar_lea.vmem %s280, 1012 [#allocation0] (stack107)
        %v112214 = vpack.c.bf16 0.0, %v112211 (stack104)
        %112215 = vst [vmem:[%s112213] sm:$0xf] /*vst_source=*/%v112214 (stack105)
        %s112216 = sadd.s32 %s339, 240 (stack106)
        %s112217 = sshrl.u32 %s112216, 10 (stack49)
        %p112218 = scmp.lt.s32.totalorder 1, %s112217 (stack50)
        %s112219 = scalar_select /*predicate=*/%p112218, /*on_true=*/1, /*on_false=*/%s112217 (stack51)
        %s112220 = sand.u32 %s112216, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s112221 = sshrl.u32 %s112220, 7 (stack53)
        %s112222 = sand.u32 %s112220, 127 /* smod.u32 w/div 128 */ (stack54)
        %s112223 = smul.addr %s112219, 8 (stack55)
        %s112224 = scalar_lea.vmem %s3, %s112223 (stack56)
        %s112226 = scalar_lea.vmem %s112224, %s112221 (stack57)
        %v112227 = vld [vmem:[%s112226] ss:$0 sm:$0xff] (stack58)
        %s112228 = sand.u32 %s112222, 255 (stack59)
        %s112230 = sor.u32 256, %s112228 (stack60)
        %112231 = vbcast.lane.b32.xlu0 %v112227, %s112230 (stack61)
        %v112232 = vpop.permute.xlu0 %112231 (stack62)
        %s112233 = sadd.s32 %s347, 240 (stack106)
        %s112234 = sshrl.u32 %s112233, 10 (stack49)
        %p112235 = scmp.lt.s32.totalorder 1, %s112234 (stack50)
        %s112236 = scalar_select /*predicate=*/%p112235, /*on_true=*/1, /*on_false=*/%s112234 (stack51)
        %s112237 = sand.u32 %s112233, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s112238 = sshrl.u32 %s112237, 7 (stack53)
        %s112239 = sand.u32 %s112237, 127 /* smod.u32 w/div 128 */ (stack54)
        %s112240 = smul.addr %s112236, 8 (stack55)
        %s112241 = scalar_lea.vmem %s5, %s112240 (stack56)
        %s112243 = scalar_lea.vmem %s112241, %s112238 (stack57)
        %v112244 = vld [vmem:[%s112243] ss:$0 sm:$0xff] (stack58)
        %s112245 = sand.u32 %s112239, 255 (stack59)
        %s112247 = sor.u32 256, %s112245 (stack60)
        %112248 = vbcast.lane.b32.xlu0 %v112244, %s112247 (stack61)
        %v112249 = vpop.permute.xlu0 %112248 (stack62)
        %v112252 = vadd.s32 %v408, %v112249 (stack65)
        %s112254 = smul.u32 128, %s27 (stack66)
        %v112255 = vlaneseq (stack67)
        %v112256 = vand.u32 %v112255, 127 (stack68)
        %v112257 = vstv %s112254 (stack69)
        %v112258 = vadd.s32 %v112256, %v112257 (stack70)
        %v112262 = vadd.s32 %v112252, %v112258 (stack65)
        %vm112266 = vcmp.lt.u32.totalorder %v112262, %v112252 (stack71)
        %vm112271 = vcmp.lt.u32.totalorder %v112252, %v408 (stack71)
        %v112276 = vadd.s32 %v380, %v112232 (stack65)
        %v112280 = vadd.s32 %v112276, 1 (stack65)
        %v112284 = vsel /*vm=*/%vm112271, /*on_true_vy=*/%v112280, /*on_false_vx=*/%v112276 (stack72)
        %v112288 = vadd.s32 %v112284, 1 (stack65)
        %v112292 = vsel /*vm=*/%vm112266, /*on_true_vy=*/%v112288, /*on_false_vx=*/%v112284 (stack72)
        %v112297 = vadd.s32 %v112292, %v10 (stack65)
        %v112301 = vadd.s32 %v112262, %v9 (stack65)
        %v112305 = vadd.s32 %v112297, %v112301 (stack65)
        %v112307 = vshll.u32 %v112301, 13 (stack73)
        %v112308 = vshrl.u32 %v112301, 19 (stack74)
        %v112309 = vor.u32 %v112307, %v112308 (stack75)
        %v112310 = vxor.u32 %v112305, %v112309 (stack76)
        %v112313 = vadd.s32 %v112305, %v112310 (stack65)
        %v112315 = vshll.u32 %v112310, 15 (stack73)
        %v112316 = vshrl.u32 %v112310, 17 (stack74)
        %v112317 = vor.u32 %v112315, %v112316 (stack75)
        %v112318 = vxor.u32 %v112313, %v112317 (stack76)
        %v112321 = vadd.s32 %v112313, %v112318 (stack65)
        %v112323 = vshll.u32 %v112318, 26 (stack73)
        %v112324 = vshrl.u32 %v112318, 6 (stack74)
        %v112325 = vor.u32 %v112323, %v112324 (stack75)
        %v112326 = vxor.u32 %v112321, %v112325 (stack76)
        %v112329 = vadd.s32 %v112321, %v112326 (stack65)
        %v112333 = vadd.s32 %v112329, %v9 (stack65)
        %v112335 = vshll.u32 %v112326, 6 (stack73)
        %v112336 = vshrl.u32 %v112326, 26 (stack74)
        %v112337 = vor.u32 %v112335, %v112336 (stack75)
        %v112338 = vxor.u32 %v112329, %v112337 (stack76)
        %v112341 = vadd.s32 %v112338, %v8 (stack65)
        %v112345 = vadd.s32 %v112341, 1 (stack65)
        %v112349 = vadd.s32 %v112333, %v112345 (stack65)
        %v112351 = vshll.u32 %v112345, 17 (stack73)
        %v112352 = vshrl.u32 %v112345, 15 (stack74)
        %v112353 = vor.u32 %v112351, %v112352 (stack75)
        %v112354 = vxor.u32 %v112349, %v112353 (stack76)
        %v112357 = vadd.s32 %v112349, %v112354 (stack65)
        %v112359 = vshll.u32 %v112354, 29 (stack73)
        %v112360 = vshrl.u32 %v112354, 3 (stack74)
        %v112361 = vor.u32 %v112359, %v112360 (stack75)
        %v112362 = vxor.u32 %v112357, %v112361 (stack76)
        %v112365 = vadd.s32 %v112357, %v112362 (stack65)
        %v112367 = vshll.u32 %v112362, 16 (stack73)
        %v112368 = vshrl.u32 %v112362, 16 (stack74)
        %v112369 = vor.u32 %v112367, %v112368 (stack75)
        %v112370 = vxor.u32 %v112365, %v112369 (stack76)
        %v112373 = vadd.s32 %v112365, %v112370 (stack65)
        %v112377 = vadd.s32 %v112373, %v8 (stack65)
        %v112379 = vshll.u32 %v112370, 24 (stack73)
        %v112380 = vshrl.u32 %v112370, 8 (stack74)
        %v112381 = vor.u32 %v112379, %v112380 (stack75)
        %v112382 = vxor.u32 %v112373, %v112381 (stack76)
        %v112385 = vadd.s32 %v112382, %v10 (stack65)
        %v112389 = vadd.s32 %v112385, 2 (stack65)
        %v112393 = vadd.s32 %v112377, %v112389 (stack65)
        %v112395 = vshll.u32 %v112389, 13 (stack73)
        %v112396 = vshrl.u32 %v112389, 19 (stack74)
        %v112397 = vor.u32 %v112395, %v112396 (stack75)
        %v112398 = vxor.u32 %v112393, %v112397 (stack76)
        %v112401 = vadd.s32 %v112393, %v112398 (stack65)
        %v112403 = vshll.u32 %v112398, 15 (stack73)
        %v112404 = vshrl.u32 %v112398, 17 (stack74)
        %v112405 = vor.u32 %v112403, %v112404 (stack75)
        %v112406 = vxor.u32 %v112401, %v112405 (stack76)
        %v112409 = vadd.s32 %v112401, %v112406 (stack65)
        %v112411 = vshll.u32 %v112406, 26 (stack73)
        %v112412 = vshrl.u32 %v112406, 6 (stack74)
        %v112413 = vor.u32 %v112411, %v112412 (stack75)
        %v112414 = vxor.u32 %v112409, %v112413 (stack76)
        %v112417 = vadd.s32 %v112409, %v112414 (stack65)
        %v112421 = vadd.s32 %v112417, %v10 (stack65)
        %v112423 = vshll.u32 %v112414, 6 (stack73)
        %v112424 = vshrl.u32 %v112414, 26 (stack74)
        %v112425 = vor.u32 %v112423, %v112424 (stack75)
        %v112426 = vxor.u32 %v112417, %v112425 (stack76)
        %v112429 = vadd.s32 %v112426, %v9 (stack65)
        %v112433 = vadd.s32 %v112429, 3 (stack65)
        %v112437 = vadd.s32 %v112421, %v112433 (stack65)
        %v112439 = vshll.u32 %v112433, 17 (stack73)
        %v112440 = vshrl.u32 %v112433, 15 (stack74)
        %v112441 = vor.u32 %v112439, %v112440 (stack75)
        %v112442 = vxor.u32 %v112437, %v112441 (stack76)
        %v112445 = vadd.s32 %v112437, %v112442 (stack65)
        %v112447 = vshll.u32 %v112442, 29 (stack73)
        %v112448 = vshrl.u32 %v112442, 3 (stack74)
        %v112449 = vor.u32 %v112447, %v112448 (stack75)
        %v112450 = vxor.u32 %v112445, %v112449 (stack76)
        %v112453 = vadd.s32 %v112445, %v112450 (stack65)
        %v112455 = vshll.u32 %v112450, 16 (stack73)
        %v112456 = vshrl.u32 %v112450, 16 (stack74)
        %v112457 = vor.u32 %v112455, %v112456 (stack75)
        %v112458 = vxor.u32 %v112453, %v112457 (stack76)
        %v112461 = vadd.s32 %v112453, %v112458 (stack65)
        %v112465 = vadd.s32 %v112461, %v9 (stack65)
        %v112467 = vshll.u32 %v112458, 24 (stack73)
        %v112468 = vshrl.u32 %v112458, 8 (stack74)
        %v112469 = vor.u32 %v112467, %v112468 (stack75)
        %v112470 = vxor.u32 %v112461, %v112469 (stack76)
        %v112473 = vadd.s32 %v112470, %v8 (stack65)
        %v112477 = vadd.s32 %v112473, 4 (stack65)
        %v112481 = vadd.s32 %v112465, %v112477 (stack65)
        %v112483 = vshll.u32 %v112477, 13 (stack73)
        %v112484 = vshrl.u32 %v112477, 19 (stack74)
        %v112485 = vor.u32 %v112483, %v112484 (stack75)
        %v112486 = vxor.u32 %v112481, %v112485 (stack76)
        %v112489 = vadd.s32 %v112481, %v112486 (stack65)
        %v112491 = vshll.u32 %v112486, 15 (stack73)
        %v112492 = vshrl.u32 %v112486, 17 (stack74)
        %v112493 = vor.u32 %v112491, %v112492 (stack75)
        %v112494 = vxor.u32 %v112489, %v112493 (stack76)
        %v112497 = vadd.s32 %v112489, %v112494 (stack65)
        %v112499 = vshll.u32 %v112494, 26 (stack73)
        %v112500 = vshrl.u32 %v112494, 6 (stack74)
        %v112501 = vor.u32 %v112499, %v112500 (stack75)
        %v112502 = vxor.u32 %v112497, %v112501 (stack76)
        %v112505 = vadd.s32 %v112497, %v112502 (stack65)
        %v112509 = vadd.s32 %v112505, %v8 (stack65)
        %v112511 = vshll.u32 %v112502, 6 (stack73)
        %v112512 = vshrl.u32 %v112502, 26 (stack74)
        %v112513 = vor.u32 %v112511, %v112512 (stack75)
        %v112514 = vxor.u32 %v112505, %v112513 (stack76)
        %v112517 = vadd.s32 %v112514, %v10 (stack65)
        %v112521 = vadd.s32 %v112517, 5 (stack65)
        %v112523 = vxor.u32 %v112509, %v112521 (stack76)
        %v112524 = vand.u32.u8 %v112523, 255 (stack77)
        %v112525 = vand.u32 %v112524, 65535 (stack78)
        %v112526 = vshrl.u32 %v112525, 1 (stack79)
        %v112527 = vor.u32 %v112526, 16256 (stack75)
        %v112528 = vand.u32.u16 %v112527, 65535 (stack80)
        %v112529 = vunpack.i.l.bf16 %v112528 (stack81)
        %v112533 = vadd.f32 %v112529, -1.0 (stack82)
        %v112537 = vmul.f32 %v112533, 2.0 (stack83)
        %v112541 = vadd.f32 %v112537, -0.99609375 (stack82)
        %v112545 = vmax.f32 -0.99609375, %v112541 (stack84)
        %v112547 = vand.u32 2147483647, %v112545 (stack85)
        %vm112550 = vcmp.eq.f32.partialorder %v112547, 1.0 (stack86)
        %v112555 = vmul.f32 %v112545, inf (stack83)
        %v112557 = vxor.u32 %v112545, 2147483648 (stack87)
        %v112560 = vmul.f32 %v112545, %v112557 (stack83)
        %v112562 = vadd.f32 %v112560, 1.0 (stack88)
        %v112563 = vlog2.pop %v112562 (stack89)
        %v112564 = vmul.f32 %v112563, 0.6931472 (stack90)
        %v112565 = vmul.f32 -0.5, %v112560 (stack91)
        %v112566 = vadd.f32 %v112565, 1.0 (stack92)
        %v112567 = vmul.f32 %v112566, %v112560 (stack93)
        %v112568 = vand.u32 2147483647, %v112560 (stack94)
        %vm112569 = vcmp.lt.f32.partialorder %v112568, 0.0004427343 (stack95)
        %v112570 = vsel /*vm=*/%vm112569, /*on_true_vy=*/%v112567, /*on_false_vx=*/%v112564 (stack96)
        %v112571 = vxor.u32 %v112570, 2147483648 (stack87)
        %vm112574 = vcmp.lt.f32.partialorder %v112571, 5.0 (stack86)
        %v112579 = vsel /*vm=*/%vm112574, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v112583 = vsel /*vm=*/%vm112574, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v112587 = vsel /*vm=*/%vm112574, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v112591 = vsel /*vm=*/%vm112574, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v112595 = vsel /*vm=*/%vm112574, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v112599 = vsel /*vm=*/%vm112574, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v112603 = vsel /*vm=*/%vm112574, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v112607 = vsel /*vm=*/%vm112574, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v112611 = vsel /*vm=*/%vm112574, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v112615 = vadd.f32 %v112571, -2.5 (stack82)
        %v112617 = vrsqrt.pop %v112571 (stack97)
        %v112618 = vmul.f32 %v112571, %v112617 (stack98)
        %vm112619 = vcmp.eq.f32.partialorder %v112571, inf (stack99)
        %v112620 = vsel /*vm=*/%vm112619, /*on_true_vy=*/%v112571, /*on_false_vx=*/%v112618 (stack100)
        %vm112621 = vcmp.eq.f32.partialorder %v112571, 0.0 (stack101)
        %v112622 = vand.u32 %v112571, 2147483648 (stack102)
        %v112623 = vsel /*vm=*/%vm112621, /*on_true_vy=*/%v112622, /*on_false_vx=*/%v112620 (stack103)
        %v112626 = vadd.f32 %v112623, -3.0 (stack82)
        %v112630 = vsel /*vm=*/%vm112574, /*on_true_vy=*/%v112615, /*on_false_vx=*/%v112626 (stack72)
        %v112634 = vmul.f32 %v112611, %v112630 (stack83)
        %v112638 = vadd.f32 %v112607, %v112634 (stack82)
        %v112642 = vmul.f32 %v112638, %v112630 (stack83)
        %v112646 = vadd.f32 %v112603, %v112642 (stack82)
        %v112650 = vmul.f32 %v112646, %v112630 (stack83)
        %v112654 = vadd.f32 %v112599, %v112650 (stack82)
        %v112658 = vmul.f32 %v112654, %v112630 (stack83)
        %v112662 = vadd.f32 %v112595, %v112658 (stack82)
        %v112666 = vmul.f32 %v112662, %v112630 (stack83)
        %v112670 = vadd.f32 %v112591, %v112666 (stack82)
        %v112674 = vmul.f32 %v112670, %v112630 (stack83)
        %v112678 = vadd.f32 %v112587, %v112674 (stack82)
        %v112682 = vmul.f32 %v112678, %v112630 (stack83)
        %v112686 = vadd.f32 %v112583, %v112682 (stack82)
        %v112690 = vmul.f32 %v112686, %v112630 (stack83)
        %v112694 = vadd.f32 %v112579, %v112690 (stack82)
        %v112698 = vmul.f32 %v112694, %v112545 (stack83)
        %v112702 = vsel /*vm=*/%vm112550, /*on_true_vy=*/%v112555, /*on_false_vx=*/%v112698 (stack72)
        %v112706 = vmul.f32 %v112702, 1.4140625 (stack83)
        %s112708 = scalar_lea.vmem %s280, 120 [#allocation0] (stack107)
        %v112709 = vpack.c.bf16 0.0, %v112706 (stack104)
        %112710 = vst [vmem:[%s112708] sm:$0xf] /*vst_source=*/%v112709 (stack105)
        %v112713 = vadd.s32 %v894, %v112249 (stack65)
        %s112715 = smul.u32 128, %s27 (stack66)
        %v112716 = vlaneseq (stack67)
        %v112717 = vand.u32 %v112716, 127 (stack68)
        %v112718 = vstv %s112715 (stack69)
        %v112719 = vadd.s32 %v112717, %v112718 (stack70)
        %v112723 = vadd.s32 %v112713, %v112719 (stack65)
        %vm112727 = vcmp.lt.u32.totalorder %v112723, %v112713 (stack71)
        %vm112732 = vcmp.lt.u32.totalorder %v112713, %v894 (stack71)
        %v112737 = vadd.s32 %v881, %v112232 (stack65)
        %v112741 = vadd.s32 %v112737, 1 (stack65)
        %v112745 = vsel /*vm=*/%vm112732, /*on_true_vy=*/%v112741, /*on_false_vx=*/%v112737 (stack72)
        %v112749 = vadd.s32 %v112745, 1 (stack65)
        %v112753 = vsel /*vm=*/%vm112727, /*on_true_vy=*/%v112749, /*on_false_vx=*/%v112745 (stack72)
        %v112758 = vadd.s32 %v112753, %v10 (stack65)
        %v112762 = vadd.s32 %v112723, %v9 (stack65)
        %v112766 = vadd.s32 %v112758, %v112762 (stack65)
        %v112768 = vshll.u32 %v112762, 13 (stack73)
        %v112769 = vshrl.u32 %v112762, 19 (stack74)
        %v112770 = vor.u32 %v112768, %v112769 (stack75)
        %v112771 = vxor.u32 %v112766, %v112770 (stack76)
        %v112774 = vadd.s32 %v112766, %v112771 (stack65)
        %v112776 = vshll.u32 %v112771, 15 (stack73)
        %v112777 = vshrl.u32 %v112771, 17 (stack74)
        %v112778 = vor.u32 %v112776, %v112777 (stack75)
        %v112779 = vxor.u32 %v112774, %v112778 (stack76)
        %v112782 = vadd.s32 %v112774, %v112779 (stack65)
        %v112784 = vshll.u32 %v112779, 26 (stack73)
        %v112785 = vshrl.u32 %v112779, 6 (stack74)
        %v112786 = vor.u32 %v112784, %v112785 (stack75)
        %v112787 = vxor.u32 %v112782, %v112786 (stack76)
        %v112790 = vadd.s32 %v112782, %v112787 (stack65)
        %v112794 = vadd.s32 %v112790, %v9 (stack65)
        %v112796 = vshll.u32 %v112787, 6 (stack73)
        %v112797 = vshrl.u32 %v112787, 26 (stack74)
        %v112798 = vor.u32 %v112796, %v112797 (stack75)
        %v112799 = vxor.u32 %v112790, %v112798 (stack76)
        %v112802 = vadd.s32 %v112799, %v8 (stack65)
        %v112806 = vadd.s32 %v112802, 1 (stack65)
        %v112810 = vadd.s32 %v112794, %v112806 (stack65)
        %v112812 = vshll.u32 %v112806, 17 (stack73)
        %v112813 = vshrl.u32 %v112806, 15 (stack74)
        %v112814 = vor.u32 %v112812, %v112813 (stack75)
        %v112815 = vxor.u32 %v112810, %v112814 (stack76)
        %v112818 = vadd.s32 %v112810, %v112815 (stack65)
        %v112820 = vshll.u32 %v112815, 29 (stack73)
        %v112821 = vshrl.u32 %v112815, 3 (stack74)
        %v112822 = vor.u32 %v112820, %v112821 (stack75)
        %v112823 = vxor.u32 %v112818, %v112822 (stack76)
        %v112826 = vadd.s32 %v112818, %v112823 (stack65)
        %v112828 = vshll.u32 %v112823, 16 (stack73)
        %v112829 = vshrl.u32 %v112823, 16 (stack74)
        %v112830 = vor.u32 %v112828, %v112829 (stack75)
        %v112831 = vxor.u32 %v112826, %v112830 (stack76)
        %v112834 = vadd.s32 %v112826, %v112831 (stack65)
        %v112838 = vadd.s32 %v112834, %v8 (stack65)
        %v112840 = vshll.u32 %v112831, 24 (stack73)
        %v112841 = vshrl.u32 %v112831, 8 (stack74)
        %v112842 = vor.u32 %v112840, %v112841 (stack75)
        %v112843 = vxor.u32 %v112834, %v112842 (stack76)
        %v112846 = vadd.s32 %v112843, %v10 (stack65)
        %v112850 = vadd.s32 %v112846, 2 (stack65)
        %v112854 = vadd.s32 %v112838, %v112850 (stack65)
        %v112856 = vshll.u32 %v112850, 13 (stack73)
        %v112857 = vshrl.u32 %v112850, 19 (stack74)
        %v112858 = vor.u32 %v112856, %v112857 (stack75)
        %v112859 = vxor.u32 %v112854, %v112858 (stack76)
        %v112862 = vadd.s32 %v112854, %v112859 (stack65)
        %v112864 = vshll.u32 %v112859, 15 (stack73)
        %v112865 = vshrl.u32 %v112859, 17 (stack74)
        %v112866 = vor.u32 %v112864, %v112865 (stack75)
        %v112867 = vxor.u32 %v112862, %v112866 (stack76)
        %v112870 = vadd.s32 %v112862, %v112867 (stack65)
        %v112872 = vshll.u32 %v112867, 26 (stack73)
        %v112873 = vshrl.u32 %v112867, 6 (stack74)
        %v112874 = vor.u32 %v112872, %v112873 (stack75)
        %v112875 = vxor.u32 %v112870, %v112874 (stack76)
        %v112878 = vadd.s32 %v112870, %v112875 (stack65)
        %v112882 = vadd.s32 %v112878, %v10 (stack65)
        %v112884 = vshll.u32 %v112875, 6 (stack73)
        %v112885 = vshrl.u32 %v112875, 26 (stack74)
        %v112886 = vor.u32 %v112884, %v112885 (stack75)
        %v112887 = vxor.u32 %v112878, %v112886 (stack76)
        %v112890 = vadd.s32 %v112887, %v9 (stack65)
        %v112894 = vadd.s32 %v112890, 3 (stack65)
        %v112898 = vadd.s32 %v112882, %v112894 (stack65)
        %v112900 = vshll.u32 %v112894, 17 (stack73)
        %v112901 = vshrl.u32 %v112894, 15 (stack74)
        %v112902 = vor.u32 %v112900, %v112901 (stack75)
        %v112903 = vxor.u32 %v112898, %v112902 (stack76)
        %v112906 = vadd.s32 %v112898, %v112903 (stack65)
        %v112908 = vshll.u32 %v112903, 29 (stack73)
        %v112909 = vshrl.u32 %v112903, 3 (stack74)
        %v112910 = vor.u32 %v112908, %v112909 (stack75)
        %v112911 = vxor.u32 %v112906, %v112910 (stack76)
        %v112914 = vadd.s32 %v112906, %v112911 (stack65)
        %v112916 = vshll.u32 %v112911, 16 (stack73)
        %v112917 = vshrl.u32 %v112911, 16 (stack74)
        %v112918 = vor.u32 %v112916, %v112917 (stack75)
        %v112919 = vxor.u32 %v112914, %v112918 (stack76)
        %v112922 = vadd.s32 %v112914, %v112919 (stack65)
        %v112926 = vadd.s32 %v112922, %v9 (stack65)
        %v112928 = vshll.u32 %v112919, 24 (stack73)
        %v112929 = vshrl.u32 %v112919, 8 (stack74)
        %v112930 = vor.u32 %v112928, %v112929 (stack75)
        %v112931 = vxor.u32 %v112922, %v112930 (stack76)
        %v112934 = vadd.s32 %v112931, %v8 (stack65)
        %v112938 = vadd.s32 %v112934, 4 (stack65)
        %v112942 = vadd.s32 %v112926, %v112938 (stack65)
        %v112944 = vshll.u32 %v112938, 13 (stack73)
        %v112945 = vshrl.u32 %v112938, 19 (stack74)
        %v112946 = vor.u32 %v112944, %v112945 (stack75)
        %v112947 = vxor.u32 %v112942, %v112946 (stack76)
        %v112950 = vadd.s32 %v112942, %v112947 (stack65)
        %v112952 = vshll.u32 %v112947, 15 (stack73)
        %v112953 = vshrl.u32 %v112947, 17 (stack74)
        %v112954 = vor.u32 %v112952, %v112953 (stack75)
        %v112955 = vxor.u32 %v112950, %v112954 (stack76)
        %v112958 = vadd.s32 %v112950, %v112955 (stack65)
        %v112960 = vshll.u32 %v112955, 26 (stack73)
        %v112961 = vshrl.u32 %v112955, 6 (stack74)
        %v112962 = vor.u32 %v112960, %v112961 (stack75)
        %v112963 = vxor.u32 %v112958, %v112962 (stack76)
        %v112966 = vadd.s32 %v112958, %v112963 (stack65)
        %v112970 = vadd.s32 %v112966, %v8 (stack65)
        %v112972 = vshll.u32 %v112963, 6 (stack73)
        %v112973 = vshrl.u32 %v112963, 26 (stack74)
        %v112974 = vor.u32 %v112972, %v112973 (stack75)
        %v112975 = vxor.u32 %v112966, %v112974 (stack76)
        %v112978 = vadd.s32 %v112975, %v10 (stack65)
        %v112982 = vadd.s32 %v112978, 5 (stack65)
        %v112984 = vxor.u32 %v112970, %v112982 (stack76)
        %v112985 = vand.u32.u8 %v112984, 255 (stack77)
        %v112986 = vand.u32 %v112985, 65535 (stack78)
        %v112987 = vshrl.u32 %v112986, 1 (stack79)
        %v112988 = vor.u32 %v112987, 16256 (stack75)
        %v112989 = vand.u32.u16 %v112988, 65535 (stack80)
        %v112990 = vunpack.i.l.bf16 %v112989 (stack81)
        %v112994 = vadd.f32 %v112990, -1.0 (stack82)
        %v112998 = vmul.f32 %v112994, 2.0 (stack83)
        %v113002 = vadd.f32 %v112998, -0.99609375 (stack82)
        %v113006 = vmax.f32 -0.99609375, %v113002 (stack84)
        %v113008 = vand.u32 2147483647, %v113006 (stack85)
        %vm113011 = vcmp.eq.f32.partialorder %v113008, 1.0 (stack86)
        %v113016 = vmul.f32 %v113006, inf (stack83)
        %v113018 = vxor.u32 %v113006, 2147483648 (stack87)
        %v113021 = vmul.f32 %v113006, %v113018 (stack83)
        %v113023 = vadd.f32 %v113021, 1.0 (stack88)
        %v113024 = vlog2.pop %v113023 (stack89)
        %v113025 = vmul.f32 %v113024, 0.6931472 (stack90)
        %v113026 = vmul.f32 -0.5, %v113021 (stack91)
        %v113027 = vadd.f32 %v113026, 1.0 (stack92)
        %v113028 = vmul.f32 %v113027, %v113021 (stack93)
        %v113029 = vand.u32 2147483647, %v113021 (stack94)
        %vm113030 = vcmp.lt.f32.partialorder %v113029, 0.0004427343 (stack95)
        %v113031 = vsel /*vm=*/%vm113030, /*on_true_vy=*/%v113028, /*on_false_vx=*/%v113025 (stack96)
        %v113032 = vxor.u32 %v113031, 2147483648 (stack87)
        %vm113035 = vcmp.lt.f32.partialorder %v113032, 5.0 (stack86)
        %v113040 = vsel /*vm=*/%vm113035, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v113044 = vsel /*vm=*/%vm113035, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v113048 = vsel /*vm=*/%vm113035, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v113052 = vsel /*vm=*/%vm113035, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v113056 = vsel /*vm=*/%vm113035, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v113060 = vsel /*vm=*/%vm113035, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v113064 = vsel /*vm=*/%vm113035, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v113068 = vsel /*vm=*/%vm113035, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v113072 = vsel /*vm=*/%vm113035, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v113076 = vadd.f32 %v113032, -2.5 (stack82)
        %v113078 = vrsqrt.pop %v113032 (stack97)
        %v113079 = vmul.f32 %v113032, %v113078 (stack98)
        %vm113080 = vcmp.eq.f32.partialorder %v113032, inf (stack99)
        %v113081 = vsel /*vm=*/%vm113080, /*on_true_vy=*/%v113032, /*on_false_vx=*/%v113079 (stack100)
        %vm113082 = vcmp.eq.f32.partialorder %v113032, 0.0 (stack101)
        %v113083 = vand.u32 %v113032, 2147483648 (stack102)
        %v113084 = vsel /*vm=*/%vm113082, /*on_true_vy=*/%v113083, /*on_false_vx=*/%v113081 (stack103)
        %v113087 = vadd.f32 %v113084, -3.0 (stack82)
        %v113091 = vsel /*vm=*/%vm113035, /*on_true_vy=*/%v113076, /*on_false_vx=*/%v113087 (stack72)
        %v113095 = vmul.f32 %v113072, %v113091 (stack83)
        %v113099 = vadd.f32 %v113068, %v113095 (stack82)
        %v113103 = vmul.f32 %v113099, %v113091 (stack83)
        %v113107 = vadd.f32 %v113064, %v113103 (stack82)
        %v113111 = vmul.f32 %v113107, %v113091 (stack83)
        %v113115 = vadd.f32 %v113060, %v113111 (stack82)
        %v113119 = vmul.f32 %v113115, %v113091 (stack83)
        %v113123 = vadd.f32 %v113056, %v113119 (stack82)
        %v113127 = vmul.f32 %v113123, %v113091 (stack83)
        %v113131 = vadd.f32 %v113052, %v113127 (stack82)
        %v113135 = vmul.f32 %v113131, %v113091 (stack83)
        %v113139 = vadd.f32 %v113048, %v113135 (stack82)
        %v113143 = vmul.f32 %v113139, %v113091 (stack83)
        %v113147 = vadd.f32 %v113044, %v113143 (stack82)
        %v113151 = vmul.f32 %v113147, %v113091 (stack83)
        %v113155 = vadd.f32 %v113040, %v113151 (stack82)
        %v113159 = vmul.f32 %v113155, %v113006 (stack83)
        %v113163 = vsel /*vm=*/%vm113011, /*on_true_vy=*/%v113016, /*on_false_vx=*/%v113159 (stack72)
        %v113167 = vmul.f32 %v113163, 1.4140625 (stack83)
        %s113169 = scalar_lea.vmem %s280, 248 [#allocation0] (stack107)
        %v113170 = vpack.c.bf16 0.0, %v113167 (stack104)
        %113171 = vst [vmem:[%s113169] sm:$0xf] /*vst_source=*/%v113170 (stack105)
        %v113174 = vadd.s32 %v1381, %v112249 (stack65)
        %s113176 = smul.u32 128, %s27 (stack66)
        %v113177 = vlaneseq (stack67)
        %v113178 = vand.u32 %v113177, 127 (stack68)
        %v113179 = vstv %s113176 (stack69)
        %v113180 = vadd.s32 %v113178, %v113179 (stack70)
        %v113184 = vadd.s32 %v113174, %v113180 (stack65)
        %vm113188 = vcmp.lt.u32.totalorder %v113184, %v113174 (stack71)
        %vm113193 = vcmp.lt.u32.totalorder %v113174, %v1381 (stack71)
        %v113198 = vadd.s32 %v1368, %v112232 (stack65)
        %v113202 = vadd.s32 %v113198, 1 (stack65)
        %v113206 = vsel /*vm=*/%vm113193, /*on_true_vy=*/%v113202, /*on_false_vx=*/%v113198 (stack72)
        %v113210 = vadd.s32 %v113206, 1 (stack65)
        %v113214 = vsel /*vm=*/%vm113188, /*on_true_vy=*/%v113210, /*on_false_vx=*/%v113206 (stack72)
        %v113219 = vadd.s32 %v113214, %v10 (stack65)
        %v113223 = vadd.s32 %v113184, %v9 (stack65)
        %v113227 = vadd.s32 %v113219, %v113223 (stack65)
        %v113229 = vshll.u32 %v113223, 13 (stack73)
        %v113230 = vshrl.u32 %v113223, 19 (stack74)
        %v113231 = vor.u32 %v113229, %v113230 (stack75)
        %v113232 = vxor.u32 %v113227, %v113231 (stack76)
        %v113235 = vadd.s32 %v113227, %v113232 (stack65)
        %v113237 = vshll.u32 %v113232, 15 (stack73)
        %v113238 = vshrl.u32 %v113232, 17 (stack74)
        %v113239 = vor.u32 %v113237, %v113238 (stack75)
        %v113240 = vxor.u32 %v113235, %v113239 (stack76)
        %v113243 = vadd.s32 %v113235, %v113240 (stack65)
        %v113245 = vshll.u32 %v113240, 26 (stack73)
        %v113246 = vshrl.u32 %v113240, 6 (stack74)
        %v113247 = vor.u32 %v113245, %v113246 (stack75)
        %v113248 = vxor.u32 %v113243, %v113247 (stack76)
        %v113251 = vadd.s32 %v113243, %v113248 (stack65)
        %v113255 = vadd.s32 %v113251, %v9 (stack65)
        %v113257 = vshll.u32 %v113248, 6 (stack73)
        %v113258 = vshrl.u32 %v113248, 26 (stack74)
        %v113259 = vor.u32 %v113257, %v113258 (stack75)
        %v113260 = vxor.u32 %v113251, %v113259 (stack76)
        %v113263 = vadd.s32 %v113260, %v8 (stack65)
        %v113267 = vadd.s32 %v113263, 1 (stack65)
        %v113271 = vadd.s32 %v113255, %v113267 (stack65)
        %v113273 = vshll.u32 %v113267, 17 (stack73)
        %v113274 = vshrl.u32 %v113267, 15 (stack74)
        %v113275 = vor.u32 %v113273, %v113274 (stack75)
        %v113276 = vxor.u32 %v113271, %v113275 (stack76)
        %v113279 = vadd.s32 %v113271, %v113276 (stack65)
        %v113281 = vshll.u32 %v113276, 29 (stack73)
        %v113282 = vshrl.u32 %v113276, 3 (stack74)
        %v113283 = vor.u32 %v113281, %v113282 (stack75)
        %v113284 = vxor.u32 %v113279, %v113283 (stack76)
        %v113287 = vadd.s32 %v113279, %v113284 (stack65)
        %v113289 = vshll.u32 %v113284, 16 (stack73)
        %v113290 = vshrl.u32 %v113284, 16 (stack74)
        %v113291 = vor.u32 %v113289, %v113290 (stack75)
        %v113292 = vxor.u32 %v113287, %v113291 (stack76)
        %v113295 = vadd.s32 %v113287, %v113292 (stack65)
        %v113299 = vadd.s32 %v113295, %v8 (stack65)
        %v113301 = vshll.u32 %v113292, 24 (stack73)
        %v113302 = vshrl.u32 %v113292, 8 (stack74)
        %v113303 = vor.u32 %v113301, %v113302 (stack75)
        %v113304 = vxor.u32 %v113295, %v113303 (stack76)
        %v113307 = vadd.s32 %v113304, %v10 (stack65)
        %v113311 = vadd.s32 %v113307, 2 (stack65)
        %v113315 = vadd.s32 %v113299, %v113311 (stack65)
        %v113317 = vshll.u32 %v113311, 13 (stack73)
        %v113318 = vshrl.u32 %v113311, 19 (stack74)
        %v113319 = vor.u32 %v113317, %v113318 (stack75)
        %v113320 = vxor.u32 %v113315, %v113319 (stack76)
        %v113323 = vadd.s32 %v113315, %v113320 (stack65)
        %v113325 = vshll.u32 %v113320, 15 (stack73)
        %v113326 = vshrl.u32 %v113320, 17 (stack74)
        %v113327 = vor.u32 %v113325, %v113326 (stack75)
        %v113328 = vxor.u32 %v113323, %v113327 (stack76)
        %v113331 = vadd.s32 %v113323, %v113328 (stack65)
        %v113333 = vshll.u32 %v113328, 26 (stack73)
        %v113334 = vshrl.u32 %v113328, 6 (stack74)
        %v113335 = vor.u32 %v113333, %v113334 (stack75)
        %v113336 = vxor.u32 %v113331, %v113335 (stack76)
        %v113339 = vadd.s32 %v113331, %v113336 (stack65)
        %v113343 = vadd.s32 %v113339, %v10 (stack65)
        %v113345 = vshll.u32 %v113336, 6 (stack73)
        %v113346 = vshrl.u32 %v113336, 26 (stack74)
        %v113347 = vor.u32 %v113345, %v113346 (stack75)
        %v113348 = vxor.u32 %v113339, %v113347 (stack76)
        %v113351 = vadd.s32 %v113348, %v9 (stack65)
        %v113355 = vadd.s32 %v113351, 3 (stack65)
        %v113359 = vadd.s32 %v113343, %v113355 (stack65)
        %v113361 = vshll.u32 %v113355, 17 (stack73)
        %v113362 = vshrl.u32 %v113355, 15 (stack74)
        %v113363 = vor.u32 %v113361, %v113362 (stack75)
        %v113364 = vxor.u32 %v113359, %v113363 (stack76)
        %v113367 = vadd.s32 %v113359, %v113364 (stack65)
        %v113369 = vshll.u32 %v113364, 29 (stack73)
        %v113370 = vshrl.u32 %v113364, 3 (stack74)
        %v113371 = vor.u32 %v113369, %v113370 (stack75)
        %v113372 = vxor.u32 %v113367, %v113371 (stack76)
        %v113375 = vadd.s32 %v113367, %v113372 (stack65)
        %v113377 = vshll.u32 %v113372, 16 (stack73)
        %v113378 = vshrl.u32 %v113372, 16 (stack74)
        %v113379 = vor.u32 %v113377, %v113378 (stack75)
        %v113380 = vxor.u32 %v113375, %v113379 (stack76)
        %v113383 = vadd.s32 %v113375, %v113380 (stack65)
        %v113387 = vadd.s32 %v113383, %v9 (stack65)
        %v113389 = vshll.u32 %v113380, 24 (stack73)
        %v113390 = vshrl.u32 %v113380, 8 (stack74)
        %v113391 = vor.u32 %v113389, %v113390 (stack75)
        %v113392 = vxor.u32 %v113383, %v113391 (stack76)
        %v113395 = vadd.s32 %v113392, %v8 (stack65)
        %v113399 = vadd.s32 %v113395, 4 (stack65)
        %v113403 = vadd.s32 %v113387, %v113399 (stack65)
        %v113405 = vshll.u32 %v113399, 13 (stack73)
        %v113406 = vshrl.u32 %v113399, 19 (stack74)
        %v113407 = vor.u32 %v113405, %v113406 (stack75)
        %v113408 = vxor.u32 %v113403, %v113407 (stack76)
        %v113411 = vadd.s32 %v113403, %v113408 (stack65)
        %v113413 = vshll.u32 %v113408, 15 (stack73)
        %v113414 = vshrl.u32 %v113408, 17 (stack74)
        %v113415 = vor.u32 %v113413, %v113414 (stack75)
        %v113416 = vxor.u32 %v113411, %v113415 (stack76)
        %v113419 = vadd.s32 %v113411, %v113416 (stack65)
        %v113421 = vshll.u32 %v113416, 26 (stack73)
        %v113422 = vshrl.u32 %v113416, 6 (stack74)
        %v113423 = vor.u32 %v113421, %v113422 (stack75)
        %v113424 = vxor.u32 %v113419, %v113423 (stack76)
        %v113427 = vadd.s32 %v113419, %v113424 (stack65)
        %v113431 = vadd.s32 %v113427, %v8 (stack65)
        %v113433 = vshll.u32 %v113424, 6 (stack73)
        %v113434 = vshrl.u32 %v113424, 26 (stack74)
        %v113435 = vor.u32 %v113433, %v113434 (stack75)
        %v113436 = vxor.u32 %v113427, %v113435 (stack76)
        %v113439 = vadd.s32 %v113436, %v10 (stack65)
        %v113443 = vadd.s32 %v113439, 5 (stack65)
        %v113445 = vxor.u32 %v113431, %v113443 (stack76)
        %v113446 = vand.u32.u8 %v113445, 255 (stack77)
        %v113447 = vand.u32 %v113446, 65535 (stack78)
        %v113448 = vshrl.u32 %v113447, 1 (stack79)
        %v113449 = vor.u32 %v113448, 16256 (stack75)
        %v113450 = vand.u32.u16 %v113449, 65535 (stack80)
        %v113451 = vunpack.i.l.bf16 %v113450 (stack81)
        %v113455 = vadd.f32 %v113451, -1.0 (stack82)
        %v113459 = vmul.f32 %v113455, 2.0 (stack83)
        %v113463 = vadd.f32 %v113459, -0.99609375 (stack82)
        %v113467 = vmax.f32 -0.99609375, %v113463 (stack84)
        %v113469 = vand.u32 2147483647, %v113467 (stack85)
        %vm113472 = vcmp.eq.f32.partialorder %v113469, 1.0 (stack86)
        %v113477 = vmul.f32 %v113467, inf (stack83)
        %v113479 = vxor.u32 %v113467, 2147483648 (stack87)
        %v113482 = vmul.f32 %v113467, %v113479 (stack83)
        %v113484 = vadd.f32 %v113482, 1.0 (stack88)
        %v113485 = vlog2.pop %v113484 (stack89)
        %v113486 = vmul.f32 %v113485, 0.6931472 (stack90)
        %v113487 = vmul.f32 -0.5, %v113482 (stack91)
        %v113488 = vadd.f32 %v113487, 1.0 (stack92)
        %v113489 = vmul.f32 %v113488, %v113482 (stack93)
        %v113490 = vand.u32 2147483647, %v113482 (stack94)
        %vm113491 = vcmp.lt.f32.partialorder %v113490, 0.0004427343 (stack95)
        %v113492 = vsel /*vm=*/%vm113491, /*on_true_vy=*/%v113489, /*on_false_vx=*/%v113486 (stack96)
        %v113493 = vxor.u32 %v113492, 2147483648 (stack87)
        %vm113496 = vcmp.lt.f32.partialorder %v113493, 5.0 (stack86)
        %v113501 = vsel /*vm=*/%vm113496, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v113505 = vsel /*vm=*/%vm113496, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v113509 = vsel /*vm=*/%vm113496, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v113513 = vsel /*vm=*/%vm113496, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v113517 = vsel /*vm=*/%vm113496, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v113521 = vsel /*vm=*/%vm113496, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v113525 = vsel /*vm=*/%vm113496, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v113529 = vsel /*vm=*/%vm113496, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v113533 = vsel /*vm=*/%vm113496, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v113537 = vadd.f32 %v113493, -2.5 (stack82)
        %v113539 = vrsqrt.pop %v113493 (stack97)
        %v113540 = vmul.f32 %v113493, %v113539 (stack98)
        %vm113541 = vcmp.eq.f32.partialorder %v113493, inf (stack99)
        %v113542 = vsel /*vm=*/%vm113541, /*on_true_vy=*/%v113493, /*on_false_vx=*/%v113540 (stack100)
        %vm113543 = vcmp.eq.f32.partialorder %v113493, 0.0 (stack101)
        %v113544 = vand.u32 %v113493, 2147483648 (stack102)
        %v113545 = vsel /*vm=*/%vm113543, /*on_true_vy=*/%v113544, /*on_false_vx=*/%v113542 (stack103)
        %v113548 = vadd.f32 %v113545, -3.0 (stack82)
        %v113552 = vsel /*vm=*/%vm113496, /*on_true_vy=*/%v113537, /*on_false_vx=*/%v113548 (stack72)
        %v113556 = vmul.f32 %v113533, %v113552 (stack83)
        %v113560 = vadd.f32 %v113529, %v113556 (stack82)
        %v113564 = vmul.f32 %v113560, %v113552 (stack83)
        %v113568 = vadd.f32 %v113525, %v113564 (stack82)
        %v113572 = vmul.f32 %v113568, %v113552 (stack83)
        %v113576 = vadd.f32 %v113521, %v113572 (stack82)
        %v113580 = vmul.f32 %v113576, %v113552 (stack83)
        %v113584 = vadd.f32 %v113517, %v113580 (stack82)
        %v113588 = vmul.f32 %v113584, %v113552 (stack83)
        %v113592 = vadd.f32 %v113513, %v113588 (stack82)
        %v113596 = vmul.f32 %v113592, %v113552 (stack83)
        %v113600 = vadd.f32 %v113509, %v113596 (stack82)
        %v113604 = vmul.f32 %v113600, %v113552 (stack83)
        %v113608 = vadd.f32 %v113505, %v113604 (stack82)
        %v113612 = vmul.f32 %v113608, %v113552 (stack83)
        %v113616 = vadd.f32 %v113501, %v113612 (stack82)
        %v113620 = vmul.f32 %v113616, %v113467 (stack83)
        %v113624 = vsel /*vm=*/%vm113472, /*on_true_vy=*/%v113477, /*on_false_vx=*/%v113620 (stack72)
        %v113628 = vmul.f32 %v113624, 1.4140625 (stack83)
        %s113630 = scalar_lea.vmem %s280, 376 [#allocation0] (stack107)
        %v113631 = vpack.c.bf16 0.0, %v113628 (stack104)
        %113632 = vst [vmem:[%s113630] sm:$0xf] /*vst_source=*/%v113631 (stack105)
        %v113635 = vadd.s32 %v1868, %v112249 (stack65)
        %s113637 = smul.u32 128, %s27 (stack66)
        %v113638 = vlaneseq (stack67)
        %v113639 = vand.u32 %v113638, 127 (stack68)
        %v113640 = vstv %s113637 (stack69)
        %v113641 = vadd.s32 %v113639, %v113640 (stack70)
        %v113645 = vadd.s32 %v113635, %v113641 (stack65)
        %vm113649 = vcmp.lt.u32.totalorder %v113645, %v113635 (stack71)
        %vm113654 = vcmp.lt.u32.totalorder %v113635, %v1868 (stack71)
        %v113659 = vadd.s32 %v1855, %v112232 (stack65)
        %v113663 = vadd.s32 %v113659, 1 (stack65)
        %v113667 = vsel /*vm=*/%vm113654, /*on_true_vy=*/%v113663, /*on_false_vx=*/%v113659 (stack72)
        %v113671 = vadd.s32 %v113667, 1 (stack65)
        %v113675 = vsel /*vm=*/%vm113649, /*on_true_vy=*/%v113671, /*on_false_vx=*/%v113667 (stack72)
        %v113680 = vadd.s32 %v113675, %v10 (stack65)
        %v113684 = vadd.s32 %v113645, %v9 (stack65)
        %v113688 = vadd.s32 %v113680, %v113684 (stack65)
        %v113690 = vshll.u32 %v113684, 13 (stack73)
        %v113691 = vshrl.u32 %v113684, 19 (stack74)
        %v113692 = vor.u32 %v113690, %v113691 (stack75)
        %v113693 = vxor.u32 %v113688, %v113692 (stack76)
        %v113696 = vadd.s32 %v113688, %v113693 (stack65)
        %v113698 = vshll.u32 %v113693, 15 (stack73)
        %v113699 = vshrl.u32 %v113693, 17 (stack74)
        %v113700 = vor.u32 %v113698, %v113699 (stack75)
        %v113701 = vxor.u32 %v113696, %v113700 (stack76)
        %v113704 = vadd.s32 %v113696, %v113701 (stack65)
        %v113706 = vshll.u32 %v113701, 26 (stack73)
        %v113707 = vshrl.u32 %v113701, 6 (stack74)
        %v113708 = vor.u32 %v113706, %v113707 (stack75)
        %v113709 = vxor.u32 %v113704, %v113708 (stack76)
        %v113712 = vadd.s32 %v113704, %v113709 (stack65)
        %v113716 = vadd.s32 %v113712, %v9 (stack65)
        %v113718 = vshll.u32 %v113709, 6 (stack73)
        %v113719 = vshrl.u32 %v113709, 26 (stack74)
        %v113720 = vor.u32 %v113718, %v113719 (stack75)
        %v113721 = vxor.u32 %v113712, %v113720 (stack76)
        %v113724 = vadd.s32 %v113721, %v8 (stack65)
        %v113728 = vadd.s32 %v113724, 1 (stack65)
        %v113732 = vadd.s32 %v113716, %v113728 (stack65)
        %v113734 = vshll.u32 %v113728, 17 (stack73)
        %v113735 = vshrl.u32 %v113728, 15 (stack74)
        %v113736 = vor.u32 %v113734, %v113735 (stack75)
        %v113737 = vxor.u32 %v113732, %v113736 (stack76)
        %v113740 = vadd.s32 %v113732, %v113737 (stack65)
        %v113742 = vshll.u32 %v113737, 29 (stack73)
        %v113743 = vshrl.u32 %v113737, 3 (stack74)
        %v113744 = vor.u32 %v113742, %v113743 (stack75)
        %v113745 = vxor.u32 %v113740, %v113744 (stack76)
        %v113748 = vadd.s32 %v113740, %v113745 (stack65)
        %v113750 = vshll.u32 %v113745, 16 (stack73)
        %v113751 = vshrl.u32 %v113745, 16 (stack74)
        %v113752 = vor.u32 %v113750, %v113751 (stack75)
        %v113753 = vxor.u32 %v113748, %v113752 (stack76)
        %v113756 = vadd.s32 %v113748, %v113753 (stack65)
        %v113760 = vadd.s32 %v113756, %v8 (stack65)
        %v113762 = vshll.u32 %v113753, 24 (stack73)
        %v113763 = vshrl.u32 %v113753, 8 (stack74)
        %v113764 = vor.u32 %v113762, %v113763 (stack75)
        %v113765 = vxor.u32 %v113756, %v113764 (stack76)
        %v113768 = vadd.s32 %v113765, %v10 (stack65)
        %v113772 = vadd.s32 %v113768, 2 (stack65)
        %v113776 = vadd.s32 %v113760, %v113772 (stack65)
        %v113778 = vshll.u32 %v113772, 13 (stack73)
        %v113779 = vshrl.u32 %v113772, 19 (stack74)
        %v113780 = vor.u32 %v113778, %v113779 (stack75)
        %v113781 = vxor.u32 %v113776, %v113780 (stack76)
        %v113784 = vadd.s32 %v113776, %v113781 (stack65)
        %v113786 = vshll.u32 %v113781, 15 (stack73)
        %v113787 = vshrl.u32 %v113781, 17 (stack74)
        %v113788 = vor.u32 %v113786, %v113787 (stack75)
        %v113789 = vxor.u32 %v113784, %v113788 (stack76)
        %v113792 = vadd.s32 %v113784, %v113789 (stack65)
        %v113794 = vshll.u32 %v113789, 26 (stack73)
        %v113795 = vshrl.u32 %v113789, 6 (stack74)
        %v113796 = vor.u32 %v113794, %v113795 (stack75)
        %v113797 = vxor.u32 %v113792, %v113796 (stack76)
        %v113800 = vadd.s32 %v113792, %v113797 (stack65)
        %v113804 = vadd.s32 %v113800, %v10 (stack65)
        %v113806 = vshll.u32 %v113797, 6 (stack73)
        %v113807 = vshrl.u32 %v113797, 26 (stack74)
        %v113808 = vor.u32 %v113806, %v113807 (stack75)
        %v113809 = vxor.u32 %v113800, %v113808 (stack76)
        %v113812 = vadd.s32 %v113809, %v9 (stack65)
        %v113816 = vadd.s32 %v113812, 3 (stack65)
        %v113820 = vadd.s32 %v113804, %v113816 (stack65)
        %v113822 = vshll.u32 %v113816, 17 (stack73)
        %v113823 = vshrl.u32 %v113816, 15 (stack74)
        %v113824 = vor.u32 %v113822, %v113823 (stack75)
        %v113825 = vxor.u32 %v113820, %v113824 (stack76)
        %v113828 = vadd.s32 %v113820, %v113825 (stack65)
        %v113830 = vshll.u32 %v113825, 29 (stack73)
        %v113831 = vshrl.u32 %v113825, 3 (stack74)
        %v113832 = vor.u32 %v113830, %v113831 (stack75)
        %v113833 = vxor.u32 %v113828, %v113832 (stack76)
        %v113836 = vadd.s32 %v113828, %v113833 (stack65)
        %v113838 = vshll.u32 %v113833, 16 (stack73)
        %v113839 = vshrl.u32 %v113833, 16 (stack74)
        %v113840 = vor.u32 %v113838, %v113839 (stack75)
        %v113841 = vxor.u32 %v113836, %v113840 (stack76)
        %v113844 = vadd.s32 %v113836, %v113841 (stack65)
        %v113848 = vadd.s32 %v113844, %v9 (stack65)
        %v113850 = vshll.u32 %v113841, 24 (stack73)
        %v113851 = vshrl.u32 %v113841, 8 (stack74)
        %v113852 = vor.u32 %v113850, %v113851 (stack75)
        %v113853 = vxor.u32 %v113844, %v113852 (stack76)
        %v113856 = vadd.s32 %v113853, %v8 (stack65)
        %v113860 = vadd.s32 %v113856, 4 (stack65)
        %v113864 = vadd.s32 %v113848, %v113860 (stack65)
        %v113866 = vshll.u32 %v113860, 13 (stack73)
        %v113867 = vshrl.u32 %v113860, 19 (stack74)
        %v113868 = vor.u32 %v113866, %v113867 (stack75)
        %v113869 = vxor.u32 %v113864, %v113868 (stack76)
        %v113872 = vadd.s32 %v113864, %v113869 (stack65)
        %v113874 = vshll.u32 %v113869, 15 (stack73)
        %v113875 = vshrl.u32 %v113869, 17 (stack74)
        %v113876 = vor.u32 %v113874, %v113875 (stack75)
        %v113877 = vxor.u32 %v113872, %v113876 (stack76)
        %v113880 = vadd.s32 %v113872, %v113877 (stack65)
        %v113882 = vshll.u32 %v113877, 26 (stack73)
        %v113883 = vshrl.u32 %v113877, 6 (stack74)
        %v113884 = vor.u32 %v113882, %v113883 (stack75)
        %v113885 = vxor.u32 %v113880, %v113884 (stack76)
        %v113888 = vadd.s32 %v113880, %v113885 (stack65)
        %v113892 = vadd.s32 %v113888, %v8 (stack65)
        %v113894 = vshll.u32 %v113885, 6 (stack73)
        %v113895 = vshrl.u32 %v113885, 26 (stack74)
        %v113896 = vor.u32 %v113894, %v113895 (stack75)
        %v113897 = vxor.u32 %v113888, %v113896 (stack76)
        %v113900 = vadd.s32 %v113897, %v10 (stack65)
        %v113904 = vadd.s32 %v113900, 5 (stack65)
        %v113906 = vxor.u32 %v113892, %v113904 (stack76)
        %v113907 = vand.u32.u8 %v113906, 255 (stack77)
        %v113908 = vand.u32 %v113907, 65535 (stack78)
        %v113909 = vshrl.u32 %v113908, 1 (stack79)
        %v113910 = vor.u32 %v113909, 16256 (stack75)
        %v113911 = vand.u32.u16 %v113910, 65535 (stack80)
        %v113912 = vunpack.i.l.bf16 %v113911 (stack81)
        %v113916 = vadd.f32 %v113912, -1.0 (stack82)
        %v113920 = vmul.f32 %v113916, 2.0 (stack83)
        %v113924 = vadd.f32 %v113920, -0.99609375 (stack82)
        %v113928 = vmax.f32 -0.99609375, %v113924 (stack84)
        %v113930 = vand.u32 2147483647, %v113928 (stack85)
        %vm113933 = vcmp.eq.f32.partialorder %v113930, 1.0 (stack86)
        %v113938 = vmul.f32 %v113928, inf (stack83)
        %v113940 = vxor.u32 %v113928, 2147483648 (stack87)
        %v113943 = vmul.f32 %v113928, %v113940 (stack83)
        %v113945 = vadd.f32 %v113943, 1.0 (stack88)
        %v113946 = vlog2.pop %v113945 (stack89)
        %v113947 = vmul.f32 %v113946, 0.6931472 (stack90)
        %v113948 = vmul.f32 -0.5, %v113943 (stack91)
        %v113949 = vadd.f32 %v113948, 1.0 (stack92)
        %v113950 = vmul.f32 %v113949, %v113943 (stack93)
        %v113951 = vand.u32 2147483647, %v113943 (stack94)
        %vm113952 = vcmp.lt.f32.partialorder %v113951, 0.0004427343 (stack95)
        %v113953 = vsel /*vm=*/%vm113952, /*on_true_vy=*/%v113950, /*on_false_vx=*/%v113947 (stack96)
        %v113954 = vxor.u32 %v113953, 2147483648 (stack87)
        %vm113957 = vcmp.lt.f32.partialorder %v113954, 5.0 (stack86)
        %v113962 = vsel /*vm=*/%vm113957, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v113966 = vsel /*vm=*/%vm113957, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v113970 = vsel /*vm=*/%vm113957, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v113974 = vsel /*vm=*/%vm113957, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v113978 = vsel /*vm=*/%vm113957, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v113982 = vsel /*vm=*/%vm113957, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v113986 = vsel /*vm=*/%vm113957, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v113990 = vsel /*vm=*/%vm113957, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v113994 = vsel /*vm=*/%vm113957, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v113998 = vadd.f32 %v113954, -2.5 (stack82)
        %v114000 = vrsqrt.pop %v113954 (stack97)
        %v114001 = vmul.f32 %v113954, %v114000 (stack98)
        %vm114002 = vcmp.eq.f32.partialorder %v113954, inf (stack99)
        %v114003 = vsel /*vm=*/%vm114002, /*on_true_vy=*/%v113954, /*on_false_vx=*/%v114001 (stack100)
        %vm114004 = vcmp.eq.f32.partialorder %v113954, 0.0 (stack101)
        %v114005 = vand.u32 %v113954, 2147483648 (stack102)
        %v114006 = vsel /*vm=*/%vm114004, /*on_true_vy=*/%v114005, /*on_false_vx=*/%v114003 (stack103)
        %v114009 = vadd.f32 %v114006, -3.0 (stack82)
        %v114013 = vsel /*vm=*/%vm113957, /*on_true_vy=*/%v113998, /*on_false_vx=*/%v114009 (stack72)
        %v114017 = vmul.f32 %v113994, %v114013 (stack83)
        %v114021 = vadd.f32 %v113990, %v114017 (stack82)
        %v114025 = vmul.f32 %v114021, %v114013 (stack83)
        %v114029 = vadd.f32 %v113986, %v114025 (stack82)
        %v114033 = vmul.f32 %v114029, %v114013 (stack83)
        %v114037 = vadd.f32 %v113982, %v114033 (stack82)
        %v114041 = vmul.f32 %v114037, %v114013 (stack83)
        %v114045 = vadd.f32 %v113978, %v114041 (stack82)
        %v114049 = vmul.f32 %v114045, %v114013 (stack83)
        %v114053 = vadd.f32 %v113974, %v114049 (stack82)
        %v114057 = vmul.f32 %v114053, %v114013 (stack83)
        %v114061 = vadd.f32 %v113970, %v114057 (stack82)
        %v114065 = vmul.f32 %v114061, %v114013 (stack83)
        %v114069 = vadd.f32 %v113966, %v114065 (stack82)
        %v114073 = vmul.f32 %v114069, %v114013 (stack83)
        %v114077 = vadd.f32 %v113962, %v114073 (stack82)
        %v114081 = vmul.f32 %v114077, %v113928 (stack83)
        %v114085 = vsel /*vm=*/%vm113933, /*on_true_vy=*/%v113938, /*on_false_vx=*/%v114081 (stack72)
        %v114089 = vmul.f32 %v114085, 1.4140625 (stack83)
        %s114091 = scalar_lea.vmem %s280, 504 [#allocation0] (stack107)
        %v114092 = vpack.c.bf16 0.0, %v114089 (stack104)
        %114093 = vst [vmem:[%s114091] sm:$0xf] /*vst_source=*/%v114092 (stack105)
        %v114096 = vadd.s32 %v2355, %v112249 (stack65)
        %s114098 = smul.u32 128, %s27 (stack66)
        %v114099 = vlaneseq (stack67)
        %v114100 = vand.u32 %v114099, 127 (stack68)
        %v114101 = vstv %s114098 (stack69)
        %v114102 = vadd.s32 %v114100, %v114101 (stack70)
        %v114106 = vadd.s32 %v114096, %v114102 (stack65)
        %vm114110 = vcmp.lt.u32.totalorder %v114106, %v114096 (stack71)
        %vm114115 = vcmp.lt.u32.totalorder %v114096, %v2355 (stack71)
        %v114120 = vadd.s32 %v2342, %v112232 (stack65)
        %v114124 = vadd.s32 %v114120, 1 (stack65)
        %v114128 = vsel /*vm=*/%vm114115, /*on_true_vy=*/%v114124, /*on_false_vx=*/%v114120 (stack72)
        %v114132 = vadd.s32 %v114128, 1 (stack65)
        %v114136 = vsel /*vm=*/%vm114110, /*on_true_vy=*/%v114132, /*on_false_vx=*/%v114128 (stack72)
        %v114141 = vadd.s32 %v114136, %v10 (stack65)
        %v114145 = vadd.s32 %v114106, %v9 (stack65)
        %v114149 = vadd.s32 %v114141, %v114145 (stack65)
        %v114151 = vshll.u32 %v114145, 13 (stack73)
        %v114152 = vshrl.u32 %v114145, 19 (stack74)
        %v114153 = vor.u32 %v114151, %v114152 (stack75)
        %v114154 = vxor.u32 %v114149, %v114153 (stack76)
        %v114157 = vadd.s32 %v114149, %v114154 (stack65)
        %v114159 = vshll.u32 %v114154, 15 (stack73)
        %v114160 = vshrl.u32 %v114154, 17 (stack74)
        %v114161 = vor.u32 %v114159, %v114160 (stack75)
        %v114162 = vxor.u32 %v114157, %v114161 (stack76)
        %v114165 = vadd.s32 %v114157, %v114162 (stack65)
        %v114167 = vshll.u32 %v114162, 26 (stack73)
        %v114168 = vshrl.u32 %v114162, 6 (stack74)
        %v114169 = vor.u32 %v114167, %v114168 (stack75)
        %v114170 = vxor.u32 %v114165, %v114169 (stack76)
        %v114173 = vadd.s32 %v114165, %v114170 (stack65)
        %v114177 = vadd.s32 %v114173, %v9 (stack65)
        %v114179 = vshll.u32 %v114170, 6 (stack73)
        %v114180 = vshrl.u32 %v114170, 26 (stack74)
        %v114181 = vor.u32 %v114179, %v114180 (stack75)
        %v114182 = vxor.u32 %v114173, %v114181 (stack76)
        %v114185 = vadd.s32 %v114182, %v8 (stack65)
        %v114189 = vadd.s32 %v114185, 1 (stack65)
        %v114193 = vadd.s32 %v114177, %v114189 (stack65)
        %v114195 = vshll.u32 %v114189, 17 (stack73)
        %v114196 = vshrl.u32 %v114189, 15 (stack74)
        %v114197 = vor.u32 %v114195, %v114196 (stack75)
        %v114198 = vxor.u32 %v114193, %v114197 (stack76)
        %v114201 = vadd.s32 %v114193, %v114198 (stack65)
        %v114203 = vshll.u32 %v114198, 29 (stack73)
        %v114204 = vshrl.u32 %v114198, 3 (stack74)
        %v114205 = vor.u32 %v114203, %v114204 (stack75)
        %v114206 = vxor.u32 %v114201, %v114205 (stack76)
        %v114209 = vadd.s32 %v114201, %v114206 (stack65)
        %v114211 = vshll.u32 %v114206, 16 (stack73)
        %v114212 = vshrl.u32 %v114206, 16 (stack74)
        %v114213 = vor.u32 %v114211, %v114212 (stack75)
        %v114214 = vxor.u32 %v114209, %v114213 (stack76)
        %v114217 = vadd.s32 %v114209, %v114214 (stack65)
        %v114221 = vadd.s32 %v114217, %v8 (stack65)
        %v114223 = vshll.u32 %v114214, 24 (stack73)
        %v114224 = vshrl.u32 %v114214, 8 (stack74)
        %v114225 = vor.u32 %v114223, %v114224 (stack75)
        %v114226 = vxor.u32 %v114217, %v114225 (stack76)
        %v114229 = vadd.s32 %v114226, %v10 (stack65)
        %v114233 = vadd.s32 %v114229, 2 (stack65)
        %v114237 = vadd.s32 %v114221, %v114233 (stack65)
        %v114239 = vshll.u32 %v114233, 13 (stack73)
        %v114240 = vshrl.u32 %v114233, 19 (stack74)
        %v114241 = vor.u32 %v114239, %v114240 (stack75)
        %v114242 = vxor.u32 %v114237, %v114241 (stack76)
        %v114245 = vadd.s32 %v114237, %v114242 (stack65)
        %v114247 = vshll.u32 %v114242, 15 (stack73)
        %v114248 = vshrl.u32 %v114242, 17 (stack74)
        %v114249 = vor.u32 %v114247, %v114248 (stack75)
        %v114250 = vxor.u32 %v114245, %v114249 (stack76)
        %v114253 = vadd.s32 %v114245, %v114250 (stack65)
        %v114255 = vshll.u32 %v114250, 26 (stack73)
        %v114256 = vshrl.u32 %v114250, 6 (stack74)
        %v114257 = vor.u32 %v114255, %v114256 (stack75)
        %v114258 = vxor.u32 %v114253, %v114257 (stack76)
        %v114261 = vadd.s32 %v114253, %v114258 (stack65)
        %v114265 = vadd.s32 %v114261, %v10 (stack65)
        %v114267 = vshll.u32 %v114258, 6 (stack73)
        %v114268 = vshrl.u32 %v114258, 26 (stack74)
        %v114269 = vor.u32 %v114267, %v114268 (stack75)
        %v114270 = vxor.u32 %v114261, %v114269 (stack76)
        %v114273 = vadd.s32 %v114270, %v9 (stack65)
        %v114277 = vadd.s32 %v114273, 3 (stack65)
        %v114281 = vadd.s32 %v114265, %v114277 (stack65)
        %v114283 = vshll.u32 %v114277, 17 (stack73)
        %v114284 = vshrl.u32 %v114277, 15 (stack74)
        %v114285 = vor.u32 %v114283, %v114284 (stack75)
        %v114286 = vxor.u32 %v114281, %v114285 (stack76)
        %v114289 = vadd.s32 %v114281, %v114286 (stack65)
        %v114291 = vshll.u32 %v114286, 29 (stack73)
        %v114292 = vshrl.u32 %v114286, 3 (stack74)
        %v114293 = vor.u32 %v114291, %v114292 (stack75)
        %v114294 = vxor.u32 %v114289, %v114293 (stack76)
        %v114297 = vadd.s32 %v114289, %v114294 (stack65)
        %v114299 = vshll.u32 %v114294, 16 (stack73)
        %v114300 = vshrl.u32 %v114294, 16 (stack74)
        %v114301 = vor.u32 %v114299, %v114300 (stack75)
        %v114302 = vxor.u32 %v114297, %v114301 (stack76)
        %v114305 = vadd.s32 %v114297, %v114302 (stack65)
        %v114309 = vadd.s32 %v114305, %v9 (stack65)
        %v114311 = vshll.u32 %v114302, 24 (stack73)
        %v114312 = vshrl.u32 %v114302, 8 (stack74)
        %v114313 = vor.u32 %v114311, %v114312 (stack75)
        %v114314 = vxor.u32 %v114305, %v114313 (stack76)
        %v114317 = vadd.s32 %v114314, %v8 (stack65)
        %v114321 = vadd.s32 %v114317, 4 (stack65)
        %v114325 = vadd.s32 %v114309, %v114321 (stack65)
        %v114327 = vshll.u32 %v114321, 13 (stack73)
        %v114328 = vshrl.u32 %v114321, 19 (stack74)
        %v114329 = vor.u32 %v114327, %v114328 (stack75)
        %v114330 = vxor.u32 %v114325, %v114329 (stack76)
        %v114333 = vadd.s32 %v114325, %v114330 (stack65)
        %v114335 = vshll.u32 %v114330, 15 (stack73)
        %v114336 = vshrl.u32 %v114330, 17 (stack74)
        %v114337 = vor.u32 %v114335, %v114336 (stack75)
        %v114338 = vxor.u32 %v114333, %v114337 (stack76)
        %v114341 = vadd.s32 %v114333, %v114338 (stack65)
        %v114343 = vshll.u32 %v114338, 26 (stack73)
        %v114344 = vshrl.u32 %v114338, 6 (stack74)
        %v114345 = vor.u32 %v114343, %v114344 (stack75)
        %v114346 = vxor.u32 %v114341, %v114345 (stack76)
        %v114349 = vadd.s32 %v114341, %v114346 (stack65)
        %v114353 = vadd.s32 %v114349, %v8 (stack65)
        %v114355 = vshll.u32 %v114346, 6 (stack73)
        %v114356 = vshrl.u32 %v114346, 26 (stack74)
        %v114357 = vor.u32 %v114355, %v114356 (stack75)
        %v114358 = vxor.u32 %v114349, %v114357 (stack76)
        %v114361 = vadd.s32 %v114358, %v10 (stack65)
        %v114365 = vadd.s32 %v114361, 5 (stack65)
        %v114367 = vxor.u32 %v114353, %v114365 (stack76)
        %v114368 = vand.u32.u8 %v114367, 255 (stack77)
        %v114369 = vand.u32 %v114368, 65535 (stack78)
        %v114370 = vshrl.u32 %v114369, 1 (stack79)
        %v114371 = vor.u32 %v114370, 16256 (stack75)
        %v114372 = vand.u32.u16 %v114371, 65535 (stack80)
        %v114373 = vunpack.i.l.bf16 %v114372 (stack81)
        %v114377 = vadd.f32 %v114373, -1.0 (stack82)
        %v114381 = vmul.f32 %v114377, 2.0 (stack83)
        %v114385 = vadd.f32 %v114381, -0.99609375 (stack82)
        %v114389 = vmax.f32 -0.99609375, %v114385 (stack84)
        %v114391 = vand.u32 2147483647, %v114389 (stack85)
        %vm114394 = vcmp.eq.f32.partialorder %v114391, 1.0 (stack86)
        %v114399 = vmul.f32 %v114389, inf (stack83)
        %v114401 = vxor.u32 %v114389, 2147483648 (stack87)
        %v114404 = vmul.f32 %v114389, %v114401 (stack83)
        %v114406 = vadd.f32 %v114404, 1.0 (stack88)
        %v114407 = vlog2.pop %v114406 (stack89)
        %v114408 = vmul.f32 %v114407, 0.6931472 (stack90)
        %v114409 = vmul.f32 -0.5, %v114404 (stack91)
        %v114410 = vadd.f32 %v114409, 1.0 (stack92)
        %v114411 = vmul.f32 %v114410, %v114404 (stack93)
        %v114412 = vand.u32 2147483647, %v114404 (stack94)
        %vm114413 = vcmp.lt.f32.partialorder %v114412, 0.0004427343 (stack95)
        %v114414 = vsel /*vm=*/%vm114413, /*on_true_vy=*/%v114411, /*on_false_vx=*/%v114408 (stack96)
        %v114415 = vxor.u32 %v114414, 2147483648 (stack87)
        %vm114418 = vcmp.lt.f32.partialorder %v114415, 5.0 (stack86)
        %v114423 = vsel /*vm=*/%vm114418, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v114427 = vsel /*vm=*/%vm114418, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v114431 = vsel /*vm=*/%vm114418, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v114435 = vsel /*vm=*/%vm114418, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v114439 = vsel /*vm=*/%vm114418, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v114443 = vsel /*vm=*/%vm114418, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v114447 = vsel /*vm=*/%vm114418, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v114451 = vsel /*vm=*/%vm114418, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v114455 = vsel /*vm=*/%vm114418, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v114459 = vadd.f32 %v114415, -2.5 (stack82)
        %v114461 = vrsqrt.pop %v114415 (stack97)
        %v114462 = vmul.f32 %v114415, %v114461 (stack98)
        %vm114463 = vcmp.eq.f32.partialorder %v114415, inf (stack99)
        %v114464 = vsel /*vm=*/%vm114463, /*on_true_vy=*/%v114415, /*on_false_vx=*/%v114462 (stack100)
        %vm114465 = vcmp.eq.f32.partialorder %v114415, 0.0 (stack101)
        %v114466 = vand.u32 %v114415, 2147483648 (stack102)
        %v114467 = vsel /*vm=*/%vm114465, /*on_true_vy=*/%v114466, /*on_false_vx=*/%v114464 (stack103)
        %v114470 = vadd.f32 %v114467, -3.0 (stack82)
        %v114474 = vsel /*vm=*/%vm114418, /*on_true_vy=*/%v114459, /*on_false_vx=*/%v114470 (stack72)
        %v114478 = vmul.f32 %v114455, %v114474 (stack83)
        %v114482 = vadd.f32 %v114451, %v114478 (stack82)
        %v114486 = vmul.f32 %v114482, %v114474 (stack83)
        %v114490 = vadd.f32 %v114447, %v114486 (stack82)
        %v114494 = vmul.f32 %v114490, %v114474 (stack83)
        %v114498 = vadd.f32 %v114443, %v114494 (stack82)
        %v114502 = vmul.f32 %v114498, %v114474 (stack83)
        %v114506 = vadd.f32 %v114439, %v114502 (stack82)
        %v114510 = vmul.f32 %v114506, %v114474 (stack83)
        %v114514 = vadd.f32 %v114435, %v114510 (stack82)
        %v114518 = vmul.f32 %v114514, %v114474 (stack83)
        %v114522 = vadd.f32 %v114431, %v114518 (stack82)
        %v114526 = vmul.f32 %v114522, %v114474 (stack83)
        %v114530 = vadd.f32 %v114427, %v114526 (stack82)
        %v114534 = vmul.f32 %v114530, %v114474 (stack83)
        %v114538 = vadd.f32 %v114423, %v114534 (stack82)
        %v114542 = vmul.f32 %v114538, %v114389 (stack83)
        %v114546 = vsel /*vm=*/%vm114394, /*on_true_vy=*/%v114399, /*on_false_vx=*/%v114542 (stack72)
        %v114550 = vmul.f32 %v114546, 1.4140625 (stack83)
        %s114552 = scalar_lea.vmem %s280, 632 [#allocation0] (stack107)
        %v114553 = vpack.c.bf16 0.0, %v114550 (stack104)
        %114554 = vst [vmem:[%s114552] sm:$0xf] /*vst_source=*/%v114553 (stack105)
        %v114557 = vadd.s32 %v2842, %v112249 (stack65)
        %s114559 = smul.u32 128, %s27 (stack66)
        %v114560 = vlaneseq (stack67)
        %v114561 = vand.u32 %v114560, 127 (stack68)
        %v114562 = vstv %s114559 (stack69)
        %v114563 = vadd.s32 %v114561, %v114562 (stack70)
        %v114567 = vadd.s32 %v114557, %v114563 (stack65)
        %vm114571 = vcmp.lt.u32.totalorder %v114567, %v114557 (stack71)
        %vm114576 = vcmp.lt.u32.totalorder %v114557, %v2842 (stack71)
        %v114581 = vadd.s32 %v2829, %v112232 (stack65)
        %v114585 = vadd.s32 %v114581, 1 (stack65)
        %v114589 = vsel /*vm=*/%vm114576, /*on_true_vy=*/%v114585, /*on_false_vx=*/%v114581 (stack72)
        %v114593 = vadd.s32 %v114589, 1 (stack65)
        %v114597 = vsel /*vm=*/%vm114571, /*on_true_vy=*/%v114593, /*on_false_vx=*/%v114589 (stack72)
        %v114602 = vadd.s32 %v114597, %v10 (stack65)
        %v114606 = vadd.s32 %v114567, %v9 (stack65)
        %v114610 = vadd.s32 %v114602, %v114606 (stack65)
        %v114612 = vshll.u32 %v114606, 13 (stack73)
        %v114613 = vshrl.u32 %v114606, 19 (stack74)
        %v114614 = vor.u32 %v114612, %v114613 (stack75)
        %v114615 = vxor.u32 %v114610, %v114614 (stack76)
        %v114618 = vadd.s32 %v114610, %v114615 (stack65)
        %v114620 = vshll.u32 %v114615, 15 (stack73)
        %v114621 = vshrl.u32 %v114615, 17 (stack74)
        %v114622 = vor.u32 %v114620, %v114621 (stack75)
        %v114623 = vxor.u32 %v114618, %v114622 (stack76)
        %v114626 = vadd.s32 %v114618, %v114623 (stack65)
        %v114628 = vshll.u32 %v114623, 26 (stack73)
        %v114629 = vshrl.u32 %v114623, 6 (stack74)
        %v114630 = vor.u32 %v114628, %v114629 (stack75)
        %v114631 = vxor.u32 %v114626, %v114630 (stack76)
        %v114634 = vadd.s32 %v114626, %v114631 (stack65)
        %v114638 = vadd.s32 %v114634, %v9 (stack65)
        %v114640 = vshll.u32 %v114631, 6 (stack73)
        %v114641 = vshrl.u32 %v114631, 26 (stack74)
        %v114642 = vor.u32 %v114640, %v114641 (stack75)
        %v114643 = vxor.u32 %v114634, %v114642 (stack76)
        %v114646 = vadd.s32 %v114643, %v8 (stack65)
        %v114650 = vadd.s32 %v114646, 1 (stack65)
        %v114654 = vadd.s32 %v114638, %v114650 (stack65)
        %v114656 = vshll.u32 %v114650, 17 (stack73)
        %v114657 = vshrl.u32 %v114650, 15 (stack74)
        %v114658 = vor.u32 %v114656, %v114657 (stack75)
        %v114659 = vxor.u32 %v114654, %v114658 (stack76)
        %v114662 = vadd.s32 %v114654, %v114659 (stack65)
        %v114664 = vshll.u32 %v114659, 29 (stack73)
        %v114665 = vshrl.u32 %v114659, 3 (stack74)
        %v114666 = vor.u32 %v114664, %v114665 (stack75)
        %v114667 = vxor.u32 %v114662, %v114666 (stack76)
        %v114670 = vadd.s32 %v114662, %v114667 (stack65)
        %v114672 = vshll.u32 %v114667, 16 (stack73)
        %v114673 = vshrl.u32 %v114667, 16 (stack74)
        %v114674 = vor.u32 %v114672, %v114673 (stack75)
        %v114675 = vxor.u32 %v114670, %v114674 (stack76)
        %v114678 = vadd.s32 %v114670, %v114675 (stack65)
        %v114682 = vadd.s32 %v114678, %v8 (stack65)
        %v114684 = vshll.u32 %v114675, 24 (stack73)
        %v114685 = vshrl.u32 %v114675, 8 (stack74)
        %v114686 = vor.u32 %v114684, %v114685 (stack75)
        %v114687 = vxor.u32 %v114678, %v114686 (stack76)
        %v114690 = vadd.s32 %v114687, %v10 (stack65)
        %v114694 = vadd.s32 %v114690, 2 (stack65)
        %v114698 = vadd.s32 %v114682, %v114694 (stack65)
        %v114700 = vshll.u32 %v114694, 13 (stack73)
        %v114701 = vshrl.u32 %v114694, 19 (stack74)
        %v114702 = vor.u32 %v114700, %v114701 (stack75)
        %v114703 = vxor.u32 %v114698, %v114702 (stack76)
        %v114706 = vadd.s32 %v114698, %v114703 (stack65)
        %v114708 = vshll.u32 %v114703, 15 (stack73)
        %v114709 = vshrl.u32 %v114703, 17 (stack74)
        %v114710 = vor.u32 %v114708, %v114709 (stack75)
        %v114711 = vxor.u32 %v114706, %v114710 (stack76)
        %v114714 = vadd.s32 %v114706, %v114711 (stack65)
        %v114716 = vshll.u32 %v114711, 26 (stack73)
        %v114717 = vshrl.u32 %v114711, 6 (stack74)
        %v114718 = vor.u32 %v114716, %v114717 (stack75)
        %v114719 = vxor.u32 %v114714, %v114718 (stack76)
        %v114722 = vadd.s32 %v114714, %v114719 (stack65)
        %v114726 = vadd.s32 %v114722, %v10 (stack65)
        %v114728 = vshll.u32 %v114719, 6 (stack73)
        %v114729 = vshrl.u32 %v114719, 26 (stack74)
        %v114730 = vor.u32 %v114728, %v114729 (stack75)
        %v114731 = vxor.u32 %v114722, %v114730 (stack76)
        %v114734 = vadd.s32 %v114731, %v9 (stack65)
        %v114738 = vadd.s32 %v114734, 3 (stack65)
        %v114742 = vadd.s32 %v114726, %v114738 (stack65)
        %v114744 = vshll.u32 %v114738, 17 (stack73)
        %v114745 = vshrl.u32 %v114738, 15 (stack74)
        %v114746 = vor.u32 %v114744, %v114745 (stack75)
        %v114747 = vxor.u32 %v114742, %v114746 (stack76)
        %v114750 = vadd.s32 %v114742, %v114747 (stack65)
        %v114752 = vshll.u32 %v114747, 29 (stack73)
        %v114753 = vshrl.u32 %v114747, 3 (stack74)
        %v114754 = vor.u32 %v114752, %v114753 (stack75)
        %v114755 = vxor.u32 %v114750, %v114754 (stack76)
        %v114758 = vadd.s32 %v114750, %v114755 (stack65)
        %v114760 = vshll.u32 %v114755, 16 (stack73)
        %v114761 = vshrl.u32 %v114755, 16 (stack74)
        %v114762 = vor.u32 %v114760, %v114761 (stack75)
        %v114763 = vxor.u32 %v114758, %v114762 (stack76)
        %v114766 = vadd.s32 %v114758, %v114763 (stack65)
        %v114770 = vadd.s32 %v114766, %v9 (stack65)
        %v114772 = vshll.u32 %v114763, 24 (stack73)
        %v114773 = vshrl.u32 %v114763, 8 (stack74)
        %v114774 = vor.u32 %v114772, %v114773 (stack75)
        %v114775 = vxor.u32 %v114766, %v114774 (stack76)
        %v114778 = vadd.s32 %v114775, %v8 (stack65)
        %v114782 = vadd.s32 %v114778, 4 (stack65)
        %v114786 = vadd.s32 %v114770, %v114782 (stack65)
        %v114788 = vshll.u32 %v114782, 13 (stack73)
        %v114789 = vshrl.u32 %v114782, 19 (stack74)
        %v114790 = vor.u32 %v114788, %v114789 (stack75)
        %v114791 = vxor.u32 %v114786, %v114790 (stack76)
        %v114794 = vadd.s32 %v114786, %v114791 (stack65)
        %v114796 = vshll.u32 %v114791, 15 (stack73)
        %v114797 = vshrl.u32 %v114791, 17 (stack74)
        %v114798 = vor.u32 %v114796, %v114797 (stack75)
        %v114799 = vxor.u32 %v114794, %v114798 (stack76)
        %v114802 = vadd.s32 %v114794, %v114799 (stack65)
        %v114804 = vshll.u32 %v114799, 26 (stack73)
        %v114805 = vshrl.u32 %v114799, 6 (stack74)
        %v114806 = vor.u32 %v114804, %v114805 (stack75)
        %v114807 = vxor.u32 %v114802, %v114806 (stack76)
        %v114810 = vadd.s32 %v114802, %v114807 (stack65)
        %v114814 = vadd.s32 %v114810, %v8 (stack65)
        %v114816 = vshll.u32 %v114807, 6 (stack73)
        %v114817 = vshrl.u32 %v114807, 26 (stack74)
        %v114818 = vor.u32 %v114816, %v114817 (stack75)
        %v114819 = vxor.u32 %v114810, %v114818 (stack76)
        %v114822 = vadd.s32 %v114819, %v10 (stack65)
        %v114826 = vadd.s32 %v114822, 5 (stack65)
        %v114828 = vxor.u32 %v114814, %v114826 (stack76)
        %v114829 = vand.u32.u8 %v114828, 255 (stack77)
        %v114830 = vand.u32 %v114829, 65535 (stack78)
        %v114831 = vshrl.u32 %v114830, 1 (stack79)
        %v114832 = vor.u32 %v114831, 16256 (stack75)
        %v114833 = vand.u32.u16 %v114832, 65535 (stack80)
        %v114834 = vunpack.i.l.bf16 %v114833 (stack81)
        %v114838 = vadd.f32 %v114834, -1.0 (stack82)
        %v114842 = vmul.f32 %v114838, 2.0 (stack83)
        %v114846 = vadd.f32 %v114842, -0.99609375 (stack82)
        %v114850 = vmax.f32 -0.99609375, %v114846 (stack84)
        %v114852 = vand.u32 2147483647, %v114850 (stack85)
        %vm114855 = vcmp.eq.f32.partialorder %v114852, 1.0 (stack86)
        %v114860 = vmul.f32 %v114850, inf (stack83)
        %v114862 = vxor.u32 %v114850, 2147483648 (stack87)
        %v114865 = vmul.f32 %v114850, %v114862 (stack83)
        %v114867 = vadd.f32 %v114865, 1.0 (stack88)
        %v114868 = vlog2.pop %v114867 (stack89)
        %v114869 = vmul.f32 %v114868, 0.6931472 (stack90)
        %v114870 = vmul.f32 -0.5, %v114865 (stack91)
        %v114871 = vadd.f32 %v114870, 1.0 (stack92)
        %v114872 = vmul.f32 %v114871, %v114865 (stack93)
        %v114873 = vand.u32 2147483647, %v114865 (stack94)
        %vm114874 = vcmp.lt.f32.partialorder %v114873, 0.0004427343 (stack95)
        %v114875 = vsel /*vm=*/%vm114874, /*on_true_vy=*/%v114872, /*on_false_vx=*/%v114869 (stack96)
        %v114876 = vxor.u32 %v114875, 2147483648 (stack87)
        %vm114879 = vcmp.lt.f32.partialorder %v114876, 5.0 (stack86)
        %v114884 = vsel /*vm=*/%vm114879, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v114888 = vsel /*vm=*/%vm114879, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v114892 = vsel /*vm=*/%vm114879, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v114896 = vsel /*vm=*/%vm114879, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v114900 = vsel /*vm=*/%vm114879, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v114904 = vsel /*vm=*/%vm114879, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v114908 = vsel /*vm=*/%vm114879, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v114912 = vsel /*vm=*/%vm114879, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v114916 = vsel /*vm=*/%vm114879, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v114920 = vadd.f32 %v114876, -2.5 (stack82)
        %v114922 = vrsqrt.pop %v114876 (stack97)
        %v114923 = vmul.f32 %v114876, %v114922 (stack98)
        %vm114924 = vcmp.eq.f32.partialorder %v114876, inf (stack99)
        %v114925 = vsel /*vm=*/%vm114924, /*on_true_vy=*/%v114876, /*on_false_vx=*/%v114923 (stack100)
        %vm114926 = vcmp.eq.f32.partialorder %v114876, 0.0 (stack101)
        %v114927 = vand.u32 %v114876, 2147483648 (stack102)
        %v114928 = vsel /*vm=*/%vm114926, /*on_true_vy=*/%v114927, /*on_false_vx=*/%v114925 (stack103)
        %v114931 = vadd.f32 %v114928, -3.0 (stack82)
        %v114935 = vsel /*vm=*/%vm114879, /*on_true_vy=*/%v114920, /*on_false_vx=*/%v114931 (stack72)
        %v114939 = vmul.f32 %v114916, %v114935 (stack83)
        %v114943 = vadd.f32 %v114912, %v114939 (stack82)
        %v114947 = vmul.f32 %v114943, %v114935 (stack83)
        %v114951 = vadd.f32 %v114908, %v114947 (stack82)
        %v114955 = vmul.f32 %v114951, %v114935 (stack83)
        %v114959 = vadd.f32 %v114904, %v114955 (stack82)
        %v114963 = vmul.f32 %v114959, %v114935 (stack83)
        %v114967 = vadd.f32 %v114900, %v114963 (stack82)
        %v114971 = vmul.f32 %v114967, %v114935 (stack83)
        %v114975 = vadd.f32 %v114896, %v114971 (stack82)
        %v114979 = vmul.f32 %v114975, %v114935 (stack83)
        %v114983 = vadd.f32 %v114892, %v114979 (stack82)
        %v114987 = vmul.f32 %v114983, %v114935 (stack83)
        %v114991 = vadd.f32 %v114888, %v114987 (stack82)
        %v114995 = vmul.f32 %v114991, %v114935 (stack83)
        %v114999 = vadd.f32 %v114884, %v114995 (stack82)
        %v115003 = vmul.f32 %v114999, %v114850 (stack83)
        %v115007 = vsel /*vm=*/%vm114855, /*on_true_vy=*/%v114860, /*on_false_vx=*/%v115003 (stack72)
        %v115011 = vmul.f32 %v115007, 1.4140625 (stack83)
        %s115013 = scalar_lea.vmem %s280, 760 [#allocation0] (stack107)
        %v115014 = vpack.c.bf16 0.0, %v115011 (stack104)
        %115015 = vst [vmem:[%s115013] sm:$0xf] /*vst_source=*/%v115014 (stack105)
        %v115018 = vadd.s32 %v3329, %v112249 (stack65)
        %s115020 = smul.u32 128, %s27 (stack66)
        %v115021 = vlaneseq (stack67)
        %v115022 = vand.u32 %v115021, 127 (stack68)
        %v115023 = vstv %s115020 (stack69)
        %v115024 = vadd.s32 %v115022, %v115023 (stack70)
        %v115028 = vadd.s32 %v115018, %v115024 (stack65)
        %vm115032 = vcmp.lt.u32.totalorder %v115028, %v115018 (stack71)
        %vm115037 = vcmp.lt.u32.totalorder %v115018, %v3329 (stack71)
        %v115042 = vadd.s32 %v3316, %v112232 (stack65)
        %v115046 = vadd.s32 %v115042, 1 (stack65)
        %v115050 = vsel /*vm=*/%vm115037, /*on_true_vy=*/%v115046, /*on_false_vx=*/%v115042 (stack72)
        %v115054 = vadd.s32 %v115050, 1 (stack65)
        %v115058 = vsel /*vm=*/%vm115032, /*on_true_vy=*/%v115054, /*on_false_vx=*/%v115050 (stack72)
        %v115063 = vadd.s32 %v115058, %v10 (stack65)
        %v115067 = vadd.s32 %v115028, %v9 (stack65)
        %v115071 = vadd.s32 %v115063, %v115067 (stack65)
        %v115073 = vshll.u32 %v115067, 13 (stack73)
        %v115074 = vshrl.u32 %v115067, 19 (stack74)
        %v115075 = vor.u32 %v115073, %v115074 (stack75)
        %v115076 = vxor.u32 %v115071, %v115075 (stack76)
        %v115079 = vadd.s32 %v115071, %v115076 (stack65)
        %v115081 = vshll.u32 %v115076, 15 (stack73)
        %v115082 = vshrl.u32 %v115076, 17 (stack74)
        %v115083 = vor.u32 %v115081, %v115082 (stack75)
        %v115084 = vxor.u32 %v115079, %v115083 (stack76)
        %v115087 = vadd.s32 %v115079, %v115084 (stack65)
        %v115089 = vshll.u32 %v115084, 26 (stack73)
        %v115090 = vshrl.u32 %v115084, 6 (stack74)
        %v115091 = vor.u32 %v115089, %v115090 (stack75)
        %v115092 = vxor.u32 %v115087, %v115091 (stack76)
        %v115095 = vadd.s32 %v115087, %v115092 (stack65)
        %v115099 = vadd.s32 %v115095, %v9 (stack65)
        %v115101 = vshll.u32 %v115092, 6 (stack73)
        %v115102 = vshrl.u32 %v115092, 26 (stack74)
        %v115103 = vor.u32 %v115101, %v115102 (stack75)
        %v115104 = vxor.u32 %v115095, %v115103 (stack76)
        %v115107 = vadd.s32 %v115104, %v8 (stack65)
        %v115111 = vadd.s32 %v115107, 1 (stack65)
        %v115115 = vadd.s32 %v115099, %v115111 (stack65)
        %v115117 = vshll.u32 %v115111, 17 (stack73)
        %v115118 = vshrl.u32 %v115111, 15 (stack74)
        %v115119 = vor.u32 %v115117, %v115118 (stack75)
        %v115120 = vxor.u32 %v115115, %v115119 (stack76)
        %v115123 = vadd.s32 %v115115, %v115120 (stack65)
        %v115125 = vshll.u32 %v115120, 29 (stack73)
        %v115126 = vshrl.u32 %v115120, 3 (stack74)
        %v115127 = vor.u32 %v115125, %v115126 (stack75)
        %v115128 = vxor.u32 %v115123, %v115127 (stack76)
        %v115131 = vadd.s32 %v115123, %v115128 (stack65)
        %v115133 = vshll.u32 %v115128, 16 (stack73)
        %v115134 = vshrl.u32 %v115128, 16 (stack74)
        %v115135 = vor.u32 %v115133, %v115134 (stack75)
        %v115136 = vxor.u32 %v115131, %v115135 (stack76)
        %v115139 = vadd.s32 %v115131, %v115136 (stack65)
        %v115143 = vadd.s32 %v115139, %v8 (stack65)
        %v115145 = vshll.u32 %v115136, 24 (stack73)
        %v115146 = vshrl.u32 %v115136, 8 (stack74)
        %v115147 = vor.u32 %v115145, %v115146 (stack75)
        %v115148 = vxor.u32 %v115139, %v115147 (stack76)
        %v115151 = vadd.s32 %v115148, %v10 (stack65)
        %v115155 = vadd.s32 %v115151, 2 (stack65)
        %v115159 = vadd.s32 %v115143, %v115155 (stack65)
        %v115161 = vshll.u32 %v115155, 13 (stack73)
        %v115162 = vshrl.u32 %v115155, 19 (stack74)
        %v115163 = vor.u32 %v115161, %v115162 (stack75)
        %v115164 = vxor.u32 %v115159, %v115163 (stack76)
        %v115167 = vadd.s32 %v115159, %v115164 (stack65)
        %v115169 = vshll.u32 %v115164, 15 (stack73)
        %v115170 = vshrl.u32 %v115164, 17 (stack74)
        %v115171 = vor.u32 %v115169, %v115170 (stack75)
        %v115172 = vxor.u32 %v115167, %v115171 (stack76)
        %v115175 = vadd.s32 %v115167, %v115172 (stack65)
        %v115177 = vshll.u32 %v115172, 26 (stack73)
        %v115178 = vshrl.u32 %v115172, 6 (stack74)
        %v115179 = vor.u32 %v115177, %v115178 (stack75)
        %v115180 = vxor.u32 %v115175, %v115179 (stack76)
        %v115183 = vadd.s32 %v115175, %v115180 (stack65)
        %v115187 = vadd.s32 %v115183, %v10 (stack65)
        %v115189 = vshll.u32 %v115180, 6 (stack73)
        %v115190 = vshrl.u32 %v115180, 26 (stack74)
        %v115191 = vor.u32 %v115189, %v115190 (stack75)
        %v115192 = vxor.u32 %v115183, %v115191 (stack76)
        %v115195 = vadd.s32 %v115192, %v9 (stack65)
        %v115199 = vadd.s32 %v115195, 3 (stack65)
        %v115203 = vadd.s32 %v115187, %v115199 (stack65)
        %v115205 = vshll.u32 %v115199, 17 (stack73)
        %v115206 = vshrl.u32 %v115199, 15 (stack74)
        %v115207 = vor.u32 %v115205, %v115206 (stack75)
        %v115208 = vxor.u32 %v115203, %v115207 (stack76)
        %v115211 = vadd.s32 %v115203, %v115208 (stack65)
        %v115213 = vshll.u32 %v115208, 29 (stack73)
        %v115214 = vshrl.u32 %v115208, 3 (stack74)
        %v115215 = vor.u32 %v115213, %v115214 (stack75)
        %v115216 = vxor.u32 %v115211, %v115215 (stack76)
        %v115219 = vadd.s32 %v115211, %v115216 (stack65)
        %v115221 = vshll.u32 %v115216, 16 (stack73)
        %v115222 = vshrl.u32 %v115216, 16 (stack74)
        %v115223 = vor.u32 %v115221, %v115222 (stack75)
        %v115224 = vxor.u32 %v115219, %v115223 (stack76)
        %v115227 = vadd.s32 %v115219, %v115224 (stack65)
        %v115231 = vadd.s32 %v115227, %v9 (stack65)
        %v115233 = vshll.u32 %v115224, 24 (stack73)
        %v115234 = vshrl.u32 %v115224, 8 (stack74)
        %v115235 = vor.u32 %v115233, %v115234 (stack75)
        %v115236 = vxor.u32 %v115227, %v115235 (stack76)
        %v115239 = vadd.s32 %v115236, %v8 (stack65)
        %v115243 = vadd.s32 %v115239, 4 (stack65)
        %v115247 = vadd.s32 %v115231, %v115243 (stack65)
        %v115249 = vshll.u32 %v115243, 13 (stack73)
        %v115250 = vshrl.u32 %v115243, 19 (stack74)
        %v115251 = vor.u32 %v115249, %v115250 (stack75)
        %v115252 = vxor.u32 %v115247, %v115251 (stack76)
        %v115255 = vadd.s32 %v115247, %v115252 (stack65)
        %v115257 = vshll.u32 %v115252, 15 (stack73)
        %v115258 = vshrl.u32 %v115252, 17 (stack74)
        %v115259 = vor.u32 %v115257, %v115258 (stack75)
        %v115260 = vxor.u32 %v115255, %v115259 (stack76)
        %v115263 = vadd.s32 %v115255, %v115260 (stack65)
        %v115265 = vshll.u32 %v115260, 26 (stack73)
        %v115266 = vshrl.u32 %v115260, 6 (stack74)
        %v115267 = vor.u32 %v115265, %v115266 (stack75)
        %v115268 = vxor.u32 %v115263, %v115267 (stack76)
        %v115271 = vadd.s32 %v115263, %v115268 (stack65)
        %v115275 = vadd.s32 %v115271, %v8 (stack65)
        %v115277 = vshll.u32 %v115268, 6 (stack73)
        %v115278 = vshrl.u32 %v115268, 26 (stack74)
        %v115279 = vor.u32 %v115277, %v115278 (stack75)
        %v115280 = vxor.u32 %v115271, %v115279 (stack76)
        %v115283 = vadd.s32 %v115280, %v10 (stack65)
        %v115287 = vadd.s32 %v115283, 5 (stack65)
        %v115289 = vxor.u32 %v115275, %v115287 (stack76)
        %v115290 = vand.u32.u8 %v115289, 255 (stack77)
        %v115291 = vand.u32 %v115290, 65535 (stack78)
        %v115292 = vshrl.u32 %v115291, 1 (stack79)
        %v115293 = vor.u32 %v115292, 16256 (stack75)
        %v115294 = vand.u32.u16 %v115293, 65535 (stack80)
        %v115295 = vunpack.i.l.bf16 %v115294 (stack81)
        %v115299 = vadd.f32 %v115295, -1.0 (stack82)
        %v115303 = vmul.f32 %v115299, 2.0 (stack83)
        %v115307 = vadd.f32 %v115303, -0.99609375 (stack82)
        %v115311 = vmax.f32 -0.99609375, %v115307 (stack84)
        %v115313 = vand.u32 2147483647, %v115311 (stack85)
        %vm115316 = vcmp.eq.f32.partialorder %v115313, 1.0 (stack86)
        %v115321 = vmul.f32 %v115311, inf (stack83)
        %v115323 = vxor.u32 %v115311, 2147483648 (stack87)
        %v115326 = vmul.f32 %v115311, %v115323 (stack83)
        %v115328 = vadd.f32 %v115326, 1.0 (stack88)
        %v115329 = vlog2.pop %v115328 (stack89)
        %v115330 = vmul.f32 %v115329, 0.6931472 (stack90)
        %v115331 = vmul.f32 -0.5, %v115326 (stack91)
        %v115332 = vadd.f32 %v115331, 1.0 (stack92)
        %v115333 = vmul.f32 %v115332, %v115326 (stack93)
        %v115334 = vand.u32 2147483647, %v115326 (stack94)
        %vm115335 = vcmp.lt.f32.partialorder %v115334, 0.0004427343 (stack95)
        %v115336 = vsel /*vm=*/%vm115335, /*on_true_vy=*/%v115333, /*on_false_vx=*/%v115330 (stack96)
        %v115337 = vxor.u32 %v115336, 2147483648 (stack87)
        %vm115340 = vcmp.lt.f32.partialorder %v115337, 5.0 (stack86)
        %v115345 = vsel /*vm=*/%vm115340, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v115349 = vsel /*vm=*/%vm115340, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v115353 = vsel /*vm=*/%vm115340, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v115357 = vsel /*vm=*/%vm115340, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v115361 = vsel /*vm=*/%vm115340, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v115365 = vsel /*vm=*/%vm115340, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v115369 = vsel /*vm=*/%vm115340, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v115373 = vsel /*vm=*/%vm115340, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v115377 = vsel /*vm=*/%vm115340, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v115381 = vadd.f32 %v115337, -2.5 (stack82)
        %v115383 = vrsqrt.pop %v115337 (stack97)
        %v115384 = vmul.f32 %v115337, %v115383 (stack98)
        %vm115385 = vcmp.eq.f32.partialorder %v115337, inf (stack99)
        %v115386 = vsel /*vm=*/%vm115385, /*on_true_vy=*/%v115337, /*on_false_vx=*/%v115384 (stack100)
        %vm115387 = vcmp.eq.f32.partialorder %v115337, 0.0 (stack101)
        %v115388 = vand.u32 %v115337, 2147483648 (stack102)
        %v115389 = vsel /*vm=*/%vm115387, /*on_true_vy=*/%v115388, /*on_false_vx=*/%v115386 (stack103)
        %v115392 = vadd.f32 %v115389, -3.0 (stack82)
        %v115396 = vsel /*vm=*/%vm115340, /*on_true_vy=*/%v115381, /*on_false_vx=*/%v115392 (stack72)
        %v115400 = vmul.f32 %v115377, %v115396 (stack83)
        %v115404 = vadd.f32 %v115373, %v115400 (stack82)
        %v115408 = vmul.f32 %v115404, %v115396 (stack83)
        %v115412 = vadd.f32 %v115369, %v115408 (stack82)
        %v115416 = vmul.f32 %v115412, %v115396 (stack83)
        %v115420 = vadd.f32 %v115365, %v115416 (stack82)
        %v115424 = vmul.f32 %v115420, %v115396 (stack83)
        %v115428 = vadd.f32 %v115361, %v115424 (stack82)
        %v115432 = vmul.f32 %v115428, %v115396 (stack83)
        %v115436 = vadd.f32 %v115357, %v115432 (stack82)
        %v115440 = vmul.f32 %v115436, %v115396 (stack83)
        %v115444 = vadd.f32 %v115353, %v115440 (stack82)
        %v115448 = vmul.f32 %v115444, %v115396 (stack83)
        %v115452 = vadd.f32 %v115349, %v115448 (stack82)
        %v115456 = vmul.f32 %v115452, %v115396 (stack83)
        %v115460 = vadd.f32 %v115345, %v115456 (stack82)
        %v115464 = vmul.f32 %v115460, %v115311 (stack83)
        %v115468 = vsel /*vm=*/%vm115316, /*on_true_vy=*/%v115321, /*on_false_vx=*/%v115464 (stack72)
        %v115472 = vmul.f32 %v115468, 1.4140625 (stack83)
        %s115474 = scalar_lea.vmem %s280, 888 [#allocation0] (stack107)
        %v115475 = vpack.c.bf16 0.0, %v115472 (stack104)
        %115476 = vst [vmem:[%s115474] sm:$0xf] /*vst_source=*/%v115475 (stack105)
        %v115479 = vadd.s32 %v3816, %v112249 (stack65)
        %s115481 = smul.u32 128, %s27 (stack66)
        %v115482 = vlaneseq (stack67)
        %v115483 = vand.u32 %v115482, 127 (stack68)
        %v115484 = vstv %s115481 (stack69)
        %v115485 = vadd.s32 %v115483, %v115484 (stack70)
        %v115489 = vadd.s32 %v115479, %v115485 (stack65)
        %vm115493 = vcmp.lt.u32.totalorder %v115489, %v115479 (stack71)
        %vm115498 = vcmp.lt.u32.totalorder %v115479, %v3816 (stack71)
        %v115503 = vadd.s32 %v3803, %v112232 (stack65)
        %v115507 = vadd.s32 %v115503, 1 (stack65)
        %v115511 = vsel /*vm=*/%vm115498, /*on_true_vy=*/%v115507, /*on_false_vx=*/%v115503 (stack72)
        %v115515 = vadd.s32 %v115511, 1 (stack65)
        %v115519 = vsel /*vm=*/%vm115493, /*on_true_vy=*/%v115515, /*on_false_vx=*/%v115511 (stack72)
        %v115524 = vadd.s32 %v115519, %v10 (stack65)
        %v115528 = vadd.s32 %v115489, %v9 (stack65)
        %v115532 = vadd.s32 %v115524, %v115528 (stack65)
        %v115534 = vshll.u32 %v115528, 13 (stack73)
        %v115535 = vshrl.u32 %v115528, 19 (stack74)
        %v115536 = vor.u32 %v115534, %v115535 (stack75)
        %v115537 = vxor.u32 %v115532, %v115536 (stack76)
        %v115540 = vadd.s32 %v115532, %v115537 (stack65)
        %v115542 = vshll.u32 %v115537, 15 (stack73)
        %v115543 = vshrl.u32 %v115537, 17 (stack74)
        %v115544 = vor.u32 %v115542, %v115543 (stack75)
        %v115545 = vxor.u32 %v115540, %v115544 (stack76)
        %v115548 = vadd.s32 %v115540, %v115545 (stack65)
        %v115550 = vshll.u32 %v115545, 26 (stack73)
        %v115551 = vshrl.u32 %v115545, 6 (stack74)
        %v115552 = vor.u32 %v115550, %v115551 (stack75)
        %v115553 = vxor.u32 %v115548, %v115552 (stack76)
        %v115556 = vadd.s32 %v115548, %v115553 (stack65)
        %v115560 = vadd.s32 %v115556, %v9 (stack65)
        %v115562 = vshll.u32 %v115553, 6 (stack73)
        %v115563 = vshrl.u32 %v115553, 26 (stack74)
        %v115564 = vor.u32 %v115562, %v115563 (stack75)
        %v115565 = vxor.u32 %v115556, %v115564 (stack76)
        %v115568 = vadd.s32 %v115565, %v8 (stack65)
        %v115572 = vadd.s32 %v115568, 1 (stack65)
        %v115576 = vadd.s32 %v115560, %v115572 (stack65)
        %v115578 = vshll.u32 %v115572, 17 (stack73)
        %v115579 = vshrl.u32 %v115572, 15 (stack74)
        %v115580 = vor.u32 %v115578, %v115579 (stack75)
        %v115581 = vxor.u32 %v115576, %v115580 (stack76)
        %v115584 = vadd.s32 %v115576, %v115581 (stack65)
        %v115586 = vshll.u32 %v115581, 29 (stack73)
        %v115587 = vshrl.u32 %v115581, 3 (stack74)
        %v115588 = vor.u32 %v115586, %v115587 (stack75)
        %v115589 = vxor.u32 %v115584, %v115588 (stack76)
        %v115592 = vadd.s32 %v115584, %v115589 (stack65)
        %v115594 = vshll.u32 %v115589, 16 (stack73)
        %v115595 = vshrl.u32 %v115589, 16 (stack74)
        %v115596 = vor.u32 %v115594, %v115595 (stack75)
        %v115597 = vxor.u32 %v115592, %v115596 (stack76)
        %v115600 = vadd.s32 %v115592, %v115597 (stack65)
        %v115604 = vadd.s32 %v115600, %v8 (stack65)
        %v115606 = vshll.u32 %v115597, 24 (stack73)
        %v115607 = vshrl.u32 %v115597, 8 (stack74)
        %v115608 = vor.u32 %v115606, %v115607 (stack75)
        %v115609 = vxor.u32 %v115600, %v115608 (stack76)
        %v115612 = vadd.s32 %v115609, %v10 (stack65)
        %v115616 = vadd.s32 %v115612, 2 (stack65)
        %v115620 = vadd.s32 %v115604, %v115616 (stack65)
        %v115622 = vshll.u32 %v115616, 13 (stack73)
        %v115623 = vshrl.u32 %v115616, 19 (stack74)
        %v115624 = vor.u32 %v115622, %v115623 (stack75)
        %v115625 = vxor.u32 %v115620, %v115624 (stack76)
        %v115628 = vadd.s32 %v115620, %v115625 (stack65)
        %v115630 = vshll.u32 %v115625, 15 (stack73)
        %v115631 = vshrl.u32 %v115625, 17 (stack74)
        %v115632 = vor.u32 %v115630, %v115631 (stack75)
        %v115633 = vxor.u32 %v115628, %v115632 (stack76)
        %v115636 = vadd.s32 %v115628, %v115633 (stack65)
        %v115638 = vshll.u32 %v115633, 26 (stack73)
        %v115639 = vshrl.u32 %v115633, 6 (stack74)
        %v115640 = vor.u32 %v115638, %v115639 (stack75)
        %v115641 = vxor.u32 %v115636, %v115640 (stack76)
        %v115644 = vadd.s32 %v115636, %v115641 (stack65)
        %v115648 = vadd.s32 %v115644, %v10 (stack65)
        %v115650 = vshll.u32 %v115641, 6 (stack73)
        %v115651 = vshrl.u32 %v115641, 26 (stack74)
        %v115652 = vor.u32 %v115650, %v115651 (stack75)
        %v115653 = vxor.u32 %v115644, %v115652 (stack76)
        %v115656 = vadd.s32 %v115653, %v9 (stack65)
        %v115660 = vadd.s32 %v115656, 3 (stack65)
        %v115664 = vadd.s32 %v115648, %v115660 (stack65)
        %v115666 = vshll.u32 %v115660, 17 (stack73)
        %v115667 = vshrl.u32 %v115660, 15 (stack74)
        %v115668 = vor.u32 %v115666, %v115667 (stack75)
        %v115669 = vxor.u32 %v115664, %v115668 (stack76)
        %v115672 = vadd.s32 %v115664, %v115669 (stack65)
        %v115674 = vshll.u32 %v115669, 29 (stack73)
        %v115675 = vshrl.u32 %v115669, 3 (stack74)
        %v115676 = vor.u32 %v115674, %v115675 (stack75)
        %v115677 = vxor.u32 %v115672, %v115676 (stack76)
        %v115680 = vadd.s32 %v115672, %v115677 (stack65)
        %v115682 = vshll.u32 %v115677, 16 (stack73)
        %v115683 = vshrl.u32 %v115677, 16 (stack74)
        %v115684 = vor.u32 %v115682, %v115683 (stack75)
        %v115685 = vxor.u32 %v115680, %v115684 (stack76)
        %v115688 = vadd.s32 %v115680, %v115685 (stack65)
        %v115692 = vadd.s32 %v115688, %v9 (stack65)
        %v115694 = vshll.u32 %v115685, 24 (stack73)
        %v115695 = vshrl.u32 %v115685, 8 (stack74)
        %v115696 = vor.u32 %v115694, %v115695 (stack75)
        %v115697 = vxor.u32 %v115688, %v115696 (stack76)
        %v115700 = vadd.s32 %v115697, %v8 (stack65)
        %v115704 = vadd.s32 %v115700, 4 (stack65)
        %v115708 = vadd.s32 %v115692, %v115704 (stack65)
        %v115710 = vshll.u32 %v115704, 13 (stack73)
        %v115711 = vshrl.u32 %v115704, 19 (stack74)
        %v115712 = vor.u32 %v115710, %v115711 (stack75)
        %v115713 = vxor.u32 %v115708, %v115712 (stack76)
        %v115716 = vadd.s32 %v115708, %v115713 (stack65)
        %v115718 = vshll.u32 %v115713, 15 (stack73)
        %v115719 = vshrl.u32 %v115713, 17 (stack74)
        %v115720 = vor.u32 %v115718, %v115719 (stack75)
        %v115721 = vxor.u32 %v115716, %v115720 (stack76)
        %v115724 = vadd.s32 %v115716, %v115721 (stack65)
        %v115726 = vshll.u32 %v115721, 26 (stack73)
        %v115727 = vshrl.u32 %v115721, 6 (stack74)
        %v115728 = vor.u32 %v115726, %v115727 (stack75)
        %v115729 = vxor.u32 %v115724, %v115728 (stack76)
        %v115732 = vadd.s32 %v115724, %v115729 (stack65)
        %v115736 = vadd.s32 %v115732, %v8 (stack65)
        %v115738 = vshll.u32 %v115729, 6 (stack73)
        %v115739 = vshrl.u32 %v115729, 26 (stack74)
        %v115740 = vor.u32 %v115738, %v115739 (stack75)
        %v115741 = vxor.u32 %v115732, %v115740 (stack76)
        %v115744 = vadd.s32 %v115741, %v10 (stack65)
        %v115748 = vadd.s32 %v115744, 5 (stack65)
        %v115750 = vxor.u32 %v115736, %v115748 (stack76)
        %v115751 = vand.u32.u8 %v115750, 255 (stack77)
        %v115752 = vand.u32 %v115751, 65535 (stack78)
        %v115753 = vshrl.u32 %v115752, 1 (stack79)
        %v115754 = vor.u32 %v115753, 16256 (stack75)
        %v115755 = vand.u32.u16 %v115754, 65535 (stack80)
        %v115756 = vunpack.i.l.bf16 %v115755 (stack81)
        %v115760 = vadd.f32 %v115756, -1.0 (stack82)
        %v115764 = vmul.f32 %v115760, 2.0 (stack83)
        %v115768 = vadd.f32 %v115764, -0.99609375 (stack82)
        %v115772 = vmax.f32 -0.99609375, %v115768 (stack84)
        %v115774 = vand.u32 2147483647, %v115772 (stack85)
        %vm115777 = vcmp.eq.f32.partialorder %v115774, 1.0 (stack86)
        %v115782 = vmul.f32 %v115772, inf (stack83)
        %v115784 = vxor.u32 %v115772, 2147483648 (stack87)
        %v115787 = vmul.f32 %v115772, %v115784 (stack83)
        %v115789 = vadd.f32 %v115787, 1.0 (stack88)
        %v115790 = vlog2.pop %v115789 (stack89)
        %v115791 = vmul.f32 %v115790, 0.6931472 (stack90)
        %v115792 = vmul.f32 -0.5, %v115787 (stack91)
        %v115793 = vadd.f32 %v115792, 1.0 (stack92)
        %v115794 = vmul.f32 %v115793, %v115787 (stack93)
        %v115795 = vand.u32 2147483647, %v115787 (stack94)
        %vm115796 = vcmp.lt.f32.partialorder %v115795, 0.0004427343 (stack95)
        %v115797 = vsel /*vm=*/%vm115796, /*on_true_vy=*/%v115794, /*on_false_vx=*/%v115791 (stack96)
        %v115798 = vxor.u32 %v115797, 2147483648 (stack87)
        %vm115801 = vcmp.lt.f32.partialorder %v115798, 5.0 (stack86)
        %v115806 = vsel /*vm=*/%vm115801, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v115810 = vsel /*vm=*/%vm115801, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v115814 = vsel /*vm=*/%vm115801, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v115818 = vsel /*vm=*/%vm115801, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v115822 = vsel /*vm=*/%vm115801, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v115826 = vsel /*vm=*/%vm115801, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v115830 = vsel /*vm=*/%vm115801, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v115834 = vsel /*vm=*/%vm115801, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v115838 = vsel /*vm=*/%vm115801, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v115842 = vadd.f32 %v115798, -2.5 (stack82)
        %v115844 = vrsqrt.pop %v115798 (stack97)
        %v115845 = vmul.f32 %v115798, %v115844 (stack98)
        %vm115846 = vcmp.eq.f32.partialorder %v115798, inf (stack99)
        %v115847 = vsel /*vm=*/%vm115846, /*on_true_vy=*/%v115798, /*on_false_vx=*/%v115845 (stack100)
        %vm115848 = vcmp.eq.f32.partialorder %v115798, 0.0 (stack101)
        %v115849 = vand.u32 %v115798, 2147483648 (stack102)
        %v115850 = vsel /*vm=*/%vm115848, /*on_true_vy=*/%v115849, /*on_false_vx=*/%v115847 (stack103)
        %v115853 = vadd.f32 %v115850, -3.0 (stack82)
        %v115857 = vsel /*vm=*/%vm115801, /*on_true_vy=*/%v115842, /*on_false_vx=*/%v115853 (stack72)
        %v115861 = vmul.f32 %v115838, %v115857 (stack83)
        %v115865 = vadd.f32 %v115834, %v115861 (stack82)
        %v115869 = vmul.f32 %v115865, %v115857 (stack83)
        %v115873 = vadd.f32 %v115830, %v115869 (stack82)
        %v115877 = vmul.f32 %v115873, %v115857 (stack83)
        %v115881 = vadd.f32 %v115826, %v115877 (stack82)
        %v115885 = vmul.f32 %v115881, %v115857 (stack83)
        %v115889 = vadd.f32 %v115822, %v115885 (stack82)
        %v115893 = vmul.f32 %v115889, %v115857 (stack83)
        %v115897 = vadd.f32 %v115818, %v115893 (stack82)
        %v115901 = vmul.f32 %v115897, %v115857 (stack83)
        %v115905 = vadd.f32 %v115814, %v115901 (stack82)
        %v115909 = vmul.f32 %v115905, %v115857 (stack83)
        %v115913 = vadd.f32 %v115810, %v115909 (stack82)
        %v115917 = vmul.f32 %v115913, %v115857 (stack83)
        %v115921 = vadd.f32 %v115806, %v115917 (stack82)
        %v115925 = vmul.f32 %v115921, %v115772 (stack83)
        %v115929 = vsel /*vm=*/%vm115777, /*on_true_vy=*/%v115782, /*on_false_vx=*/%v115925 (stack72)
        %v115933 = vmul.f32 %v115929, 1.4140625 (stack83)
        %s115935 = scalar_lea.vmem %s280, 1016 [#allocation0] (stack107)
        %v115936 = vpack.c.bf16 0.0, %v115933 (stack104)
        %115937 = vst [vmem:[%s115935] sm:$0xf] /*vst_source=*/%v115936 (stack105)
        %s115938 = sadd.s32 %s339, 248 (stack106)
        %s115939 = sshrl.u32 %s115938, 10 (stack49)
        %p115940 = scmp.lt.s32.totalorder 1, %s115939 (stack50)
        %s115941 = scalar_select /*predicate=*/%p115940, /*on_true=*/1, /*on_false=*/%s115939 (stack51)
        %s115942 = sand.u32 %s115938, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s115943 = sshrl.u32 %s115942, 7 (stack53)
        %s115944 = sand.u32 %s115942, 127 /* smod.u32 w/div 128 */ (stack54)
        %s115945 = smul.addr %s115941, 8 (stack55)
        %s115946 = scalar_lea.vmem %s3, %s115945 (stack56)
        %s115948 = scalar_lea.vmem %s115946, %s115943 (stack57)
        %v115949 = vld [vmem:[%s115948] ss:$0 sm:$0xff] (stack58)
        %s115950 = sand.u32 %s115944, 255 (stack59)
        %s115952 = sor.u32 256, %s115950 (stack60)
        %115953 = vbcast.lane.b32.xlu0 %v115949, %s115952 (stack61)
        %v115954 = vpop.permute.xlu0 %115953 (stack62)
        %s115955 = sadd.s32 %s347, 248 (stack106)
        %s115956 = sshrl.u32 %s115955, 10 (stack49)
        %p115957 = scmp.lt.s32.totalorder 1, %s115956 (stack50)
        %s115958 = scalar_select /*predicate=*/%p115957, /*on_true=*/1, /*on_false=*/%s115956 (stack51)
        %s115959 = sand.u32 %s115955, 1023 /* smod.u32 w/div 1024 */ (stack52)
        %s115960 = sshrl.u32 %s115959, 7 (stack53)
        %s115961 = sand.u32 %s115959, 127 /* smod.u32 w/div 128 */ (stack54)
        %s115962 = smul.addr %s115958, 8 (stack55)
        %s115963 = scalar_lea.vmem %s5, %s115962 (stack56)
        %s115965 = scalar_lea.vmem %s115963, %s115960 (stack57)
        %v115966 = vld [vmem:[%s115965] ss:$0 sm:$0xff] (stack58)
        %s115967 = sand.u32 %s115961, 255 (stack59)
        %s115969 = sor.u32 256, %s115967 (stack60)
        %115970 = vbcast.lane.b32.xlu0 %v115966, %s115969 (stack61)
        %v115971 = vpop.permute.xlu0 %115970 (stack62)
        %v115974 = vadd.s32 %v408, %v115971 (stack65)
        %s115976 = smul.u32 128, %s27 (stack66)
        %v115977 = vlaneseq (stack67)
        %v115978 = vand.u32 %v115977, 127 (stack68)
        %v115979 = vstv %s115976 (stack69)
        %v115980 = vadd.s32 %v115978, %v115979 (stack70)
        %v115984 = vadd.s32 %v115974, %v115980 (stack65)
        %vm115988 = vcmp.lt.u32.totalorder %v115984, %v115974 (stack71)
        %vm115993 = vcmp.lt.u32.totalorder %v115974, %v408 (stack71)
        %v115998 = vadd.s32 %v380, %v115954 (stack65)
        %v116002 = vadd.s32 %v115998, 1 (stack65)
        %v116006 = vsel /*vm=*/%vm115993, /*on_true_vy=*/%v116002, /*on_false_vx=*/%v115998 (stack72)
        %v116010 = vadd.s32 %v116006, 1 (stack65)
        %v116014 = vsel /*vm=*/%vm115988, /*on_true_vy=*/%v116010, /*on_false_vx=*/%v116006 (stack72)
        %v116019 = vadd.s32 %v116014, %v10 (stack65)
        %v116023 = vadd.s32 %v115984, %v9 (stack65)
        %v116027 = vadd.s32 %v116019, %v116023 (stack65)
        %v116029 = vshll.u32 %v116023, 13 (stack73)
        %v116030 = vshrl.u32 %v116023, 19 (stack74)
        %v116031 = vor.u32 %v116029, %v116030 (stack75)
        %v116032 = vxor.u32 %v116027, %v116031 (stack76)
        %v116035 = vadd.s32 %v116027, %v116032 (stack65)
        %v116037 = vshll.u32 %v116032, 15 (stack73)
        %v116038 = vshrl.u32 %v116032, 17 (stack74)
        %v116039 = vor.u32 %v116037, %v116038 (stack75)
        %v116040 = vxor.u32 %v116035, %v116039 (stack76)
        %v116043 = vadd.s32 %v116035, %v116040 (stack65)
        %v116045 = vshll.u32 %v116040, 26 (stack73)
        %v116046 = vshrl.u32 %v116040, 6 (stack74)
        %v116047 = vor.u32 %v116045, %v116046 (stack75)
        %v116048 = vxor.u32 %v116043, %v116047 (stack76)
        %v116051 = vadd.s32 %v116043, %v116048 (stack65)
        %v116055 = vadd.s32 %v116051, %v9 (stack65)
        %v116057 = vshll.u32 %v116048, 6 (stack73)
        %v116058 = vshrl.u32 %v116048, 26 (stack74)
        %v116059 = vor.u32 %v116057, %v116058 (stack75)
        %v116060 = vxor.u32 %v116051, %v116059 (stack76)
        %v116063 = vadd.s32 %v116060, %v8 (stack65)
        %v116067 = vadd.s32 %v116063, 1 (stack65)
        %v116071 = vadd.s32 %v116055, %v116067 (stack65)
        %v116073 = vshll.u32 %v116067, 17 (stack73)
        %v116074 = vshrl.u32 %v116067, 15 (stack74)
        %v116075 = vor.u32 %v116073, %v116074 (stack75)
        %v116076 = vxor.u32 %v116071, %v116075 (stack76)
        %v116079 = vadd.s32 %v116071, %v116076 (stack65)
        %v116081 = vshll.u32 %v116076, 29 (stack73)
        %v116082 = vshrl.u32 %v116076, 3 (stack74)
        %v116083 = vor.u32 %v116081, %v116082 (stack75)
        %v116084 = vxor.u32 %v116079, %v116083 (stack76)
        %v116087 = vadd.s32 %v116079, %v116084 (stack65)
        %v116089 = vshll.u32 %v116084, 16 (stack73)
        %v116090 = vshrl.u32 %v116084, 16 (stack74)
        %v116091 = vor.u32 %v116089, %v116090 (stack75)
        %v116092 = vxor.u32 %v116087, %v116091 (stack76)
        %v116095 = vadd.s32 %v116087, %v116092 (stack65)
        %v116099 = vadd.s32 %v116095, %v8 (stack65)
        %v116101 = vshll.u32 %v116092, 24 (stack73)
        %v116102 = vshrl.u32 %v116092, 8 (stack74)
        %v116103 = vor.u32 %v116101, %v116102 (stack75)
        %v116104 = vxor.u32 %v116095, %v116103 (stack76)
        %v116107 = vadd.s32 %v116104, %v10 (stack65)
        %v116111 = vadd.s32 %v116107, 2 (stack65)
        %v116115 = vadd.s32 %v116099, %v116111 (stack65)
        %v116117 = vshll.u32 %v116111, 13 (stack73)
        %v116118 = vshrl.u32 %v116111, 19 (stack74)
        %v116119 = vor.u32 %v116117, %v116118 (stack75)
        %v116120 = vxor.u32 %v116115, %v116119 (stack76)
        %v116123 = vadd.s32 %v116115, %v116120 (stack65)
        %v116125 = vshll.u32 %v116120, 15 (stack73)
        %v116126 = vshrl.u32 %v116120, 17 (stack74)
        %v116127 = vor.u32 %v116125, %v116126 (stack75)
        %v116128 = vxor.u32 %v116123, %v116127 (stack76)
        %v116131 = vadd.s32 %v116123, %v116128 (stack65)
        %v116133 = vshll.u32 %v116128, 26 (stack73)
        %v116134 = vshrl.u32 %v116128, 6 (stack74)
        %v116135 = vor.u32 %v116133, %v116134 (stack75)
        %v116136 = vxor.u32 %v116131, %v116135 (stack76)
        %v116139 = vadd.s32 %v116131, %v116136 (stack65)
        %v116143 = vadd.s32 %v116139, %v10 (stack65)
        %v116145 = vshll.u32 %v116136, 6 (stack73)
        %v116146 = vshrl.u32 %v116136, 26 (stack74)
        %v116147 = vor.u32 %v116145, %v116146 (stack75)
        %v116148 = vxor.u32 %v116139, %v116147 (stack76)
        %v116151 = vadd.s32 %v116148, %v9 (stack65)
        %v116155 = vadd.s32 %v116151, 3 (stack65)
        %v116159 = vadd.s32 %v116143, %v116155 (stack65)
        %v116161 = vshll.u32 %v116155, 17 (stack73)
        %v116162 = vshrl.u32 %v116155, 15 (stack74)
        %v116163 = vor.u32 %v116161, %v116162 (stack75)
        %v116164 = vxor.u32 %v116159, %v116163 (stack76)
        %v116167 = vadd.s32 %v116159, %v116164 (stack65)
        %v116169 = vshll.u32 %v116164, 29 (stack73)
        %v116170 = vshrl.u32 %v116164, 3 (stack74)
        %v116171 = vor.u32 %v116169, %v116170 (stack75)
        %v116172 = vxor.u32 %v116167, %v116171 (stack76)
        %v116175 = vadd.s32 %v116167, %v116172 (stack65)
        %v116177 = vshll.u32 %v116172, 16 (stack73)
        %v116178 = vshrl.u32 %v116172, 16 (stack74)
        %v116179 = vor.u32 %v116177, %v116178 (stack75)
        %v116180 = vxor.u32 %v116175, %v116179 (stack76)
        %v116183 = vadd.s32 %v116175, %v116180 (stack65)
        %v116187 = vadd.s32 %v116183, %v9 (stack65)
        %v116189 = vshll.u32 %v116180, 24 (stack73)
        %v116190 = vshrl.u32 %v116180, 8 (stack74)
        %v116191 = vor.u32 %v116189, %v116190 (stack75)
        %v116192 = vxor.u32 %v116183, %v116191 (stack76)
        %v116195 = vadd.s32 %v116192, %v8 (stack65)
        %v116199 = vadd.s32 %v116195, 4 (stack65)
        %v116203 = vadd.s32 %v116187, %v116199 (stack65)
        %v116205 = vshll.u32 %v116199, 13 (stack73)
        %v116206 = vshrl.u32 %v116199, 19 (stack74)
        %v116207 = vor.u32 %v116205, %v116206 (stack75)
        %v116208 = vxor.u32 %v116203, %v116207 (stack76)
        %v116211 = vadd.s32 %v116203, %v116208 (stack65)
        %v116213 = vshll.u32 %v116208, 15 (stack73)
        %v116214 = vshrl.u32 %v116208, 17 (stack74)
        %v116215 = vor.u32 %v116213, %v116214 (stack75)
        %v116216 = vxor.u32 %v116211, %v116215 (stack76)
        %v116219 = vadd.s32 %v116211, %v116216 (stack65)
        %v116221 = vshll.u32 %v116216, 26 (stack73)
        %v116222 = vshrl.u32 %v116216, 6 (stack74)
        %v116223 = vor.u32 %v116221, %v116222 (stack75)
        %v116224 = vxor.u32 %v116219, %v116223 (stack76)
        %v116227 = vadd.s32 %v116219, %v116224 (stack65)
        %v116231 = vadd.s32 %v116227, %v8 (stack65)
        %v116233 = vshll.u32 %v116224, 6 (stack73)
        %v116234 = vshrl.u32 %v116224, 26 (stack74)
        %v116235 = vor.u32 %v116233, %v116234 (stack75)
        %v116236 = vxor.u32 %v116227, %v116235 (stack76)
        %v116239 = vadd.s32 %v116236, %v10 (stack65)
        %v116243 = vadd.s32 %v116239, 5 (stack65)
        %v116245 = vxor.u32 %v116231, %v116243 (stack76)
        %v116246 = vand.u32.u8 %v116245, 255 (stack77)
        %v116247 = vand.u32 %v116246, 65535 (stack78)
        %v116248 = vshrl.u32 %v116247, 1 (stack79)
        %v116249 = vor.u32 %v116248, 16256 (stack75)
        %v116250 = vand.u32.u16 %v116249, 65535 (stack80)
        %v116251 = vunpack.i.l.bf16 %v116250 (stack81)
        %v116255 = vadd.f32 %v116251, -1.0 (stack82)
        %v116259 = vmul.f32 %v116255, 2.0 (stack83)
        %v116263 = vadd.f32 %v116259, -0.99609375 (stack82)
        %v116267 = vmax.f32 -0.99609375, %v116263 (stack84)
        %v116269 = vand.u32 2147483647, %v116267 (stack85)
        %vm116272 = vcmp.eq.f32.partialorder %v116269, 1.0 (stack86)
        %v116277 = vmul.f32 %v116267, inf (stack83)
        %v116279 = vxor.u32 %v116267, 2147483648 (stack87)
        %v116282 = vmul.f32 %v116267, %v116279 (stack83)
        %v116284 = vadd.f32 %v116282, 1.0 (stack88)
        %v116285 = vlog2.pop %v116284 (stack89)
        %v116286 = vmul.f32 %v116285, 0.6931472 (stack90)
        %v116287 = vmul.f32 -0.5, %v116282 (stack91)
        %v116288 = vadd.f32 %v116287, 1.0 (stack92)
        %v116289 = vmul.f32 %v116288, %v116282 (stack93)
        %v116290 = vand.u32 2147483647, %v116282 (stack94)
        %vm116291 = vcmp.lt.f32.partialorder %v116290, 0.0004427343 (stack95)
        %v116292 = vsel /*vm=*/%vm116291, /*on_true_vy=*/%v116289, /*on_false_vx=*/%v116286 (stack96)
        %v116293 = vxor.u32 %v116292, 2147483648 (stack87)
        %vm116296 = vcmp.lt.f32.partialorder %v116293, 5.0 (stack86)
        %v116301 = vsel /*vm=*/%vm116296, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v116305 = vsel /*vm=*/%vm116296, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v116309 = vsel /*vm=*/%vm116296, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v116313 = vsel /*vm=*/%vm116296, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v116317 = vsel /*vm=*/%vm116296, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v116321 = vsel /*vm=*/%vm116296, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v116325 = vsel /*vm=*/%vm116296, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v116329 = vsel /*vm=*/%vm116296, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v116333 = vsel /*vm=*/%vm116296, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v116337 = vadd.f32 %v116293, -2.5 (stack82)
        %v116339 = vrsqrt.pop %v116293 (stack97)
        %v116340 = vmul.f32 %v116293, %v116339 (stack98)
        %vm116341 = vcmp.eq.f32.partialorder %v116293, inf (stack99)
        %v116342 = vsel /*vm=*/%vm116341, /*on_true_vy=*/%v116293, /*on_false_vx=*/%v116340 (stack100)
        %vm116343 = vcmp.eq.f32.partialorder %v116293, 0.0 (stack101)
        %v116344 = vand.u32 %v116293, 2147483648 (stack102)
        %v116345 = vsel /*vm=*/%vm116343, /*on_true_vy=*/%v116344, /*on_false_vx=*/%v116342 (stack103)
        %v116348 = vadd.f32 %v116345, -3.0 (stack82)
        %v116352 = vsel /*vm=*/%vm116296, /*on_true_vy=*/%v116337, /*on_false_vx=*/%v116348 (stack72)
        %v116356 = vmul.f32 %v116333, %v116352 (stack83)
        %v116360 = vadd.f32 %v116329, %v116356 (stack82)
        %v116364 = vmul.f32 %v116360, %v116352 (stack83)
        %v116368 = vadd.f32 %v116325, %v116364 (stack82)
        %v116372 = vmul.f32 %v116368, %v116352 (stack83)
        %v116376 = vadd.f32 %v116321, %v116372 (stack82)
        %v116380 = vmul.f32 %v116376, %v116352 (stack83)
        %v116384 = vadd.f32 %v116317, %v116380 (stack82)
        %v116388 = vmul.f32 %v116384, %v116352 (stack83)
        %v116392 = vadd.f32 %v116313, %v116388 (stack82)
        %v116396 = vmul.f32 %v116392, %v116352 (stack83)
        %v116400 = vadd.f32 %v116309, %v116396 (stack82)
        %v116404 = vmul.f32 %v116400, %v116352 (stack83)
        %v116408 = vadd.f32 %v116305, %v116404 (stack82)
        %v116412 = vmul.f32 %v116408, %v116352 (stack83)
        %v116416 = vadd.f32 %v116301, %v116412 (stack82)
        %v116420 = vmul.f32 %v116416, %v116267 (stack83)
        %v116424 = vsel /*vm=*/%vm116272, /*on_true_vy=*/%v116277, /*on_false_vx=*/%v116420 (stack72)
        %v116428 = vmul.f32 %v116424, 1.4140625 (stack83)
        %s116430 = scalar_lea.vmem %s280, 124 [#allocation0] (stack107)
        %v116431 = vpack.c.bf16 0.0, %v116428 (stack104)
        %116432 = vst [vmem:[%s116430] sm:$0xf] /*vst_source=*/%v116431 (stack105)
        %v116435 = vadd.s32 %v894, %v115971 (stack65)
        %s116437 = smul.u32 128, %s27 (stack66)
        %v116438 = vlaneseq (stack67)
        %v116439 = vand.u32 %v116438, 127 (stack68)
        %v116440 = vstv %s116437 (stack69)
        %v116441 = vadd.s32 %v116439, %v116440 (stack70)
        %v116445 = vadd.s32 %v116435, %v116441 (stack65)
        %vm116449 = vcmp.lt.u32.totalorder %v116445, %v116435 (stack71)
        %vm116454 = vcmp.lt.u32.totalorder %v116435, %v894 (stack71)
        %v116459 = vadd.s32 %v881, %v115954 (stack65)
        %v116463 = vadd.s32 %v116459, 1 (stack65)
        %v116467 = vsel /*vm=*/%vm116454, /*on_true_vy=*/%v116463, /*on_false_vx=*/%v116459 (stack72)
        %v116471 = vadd.s32 %v116467, 1 (stack65)
        %v116475 = vsel /*vm=*/%vm116449, /*on_true_vy=*/%v116471, /*on_false_vx=*/%v116467 (stack72)
        %v116480 = vadd.s32 %v116475, %v10 (stack65)
        %v116484 = vadd.s32 %v116445, %v9 (stack65)
        %v116488 = vadd.s32 %v116480, %v116484 (stack65)
        %v116490 = vshll.u32 %v116484, 13 (stack73)
        %v116491 = vshrl.u32 %v116484, 19 (stack74)
        %v116492 = vor.u32 %v116490, %v116491 (stack75)
        %v116493 = vxor.u32 %v116488, %v116492 (stack76)
        %v116496 = vadd.s32 %v116488, %v116493 (stack65)
        %v116498 = vshll.u32 %v116493, 15 (stack73)
        %v116499 = vshrl.u32 %v116493, 17 (stack74)
        %v116500 = vor.u32 %v116498, %v116499 (stack75)
        %v116501 = vxor.u32 %v116496, %v116500 (stack76)
        %v116504 = vadd.s32 %v116496, %v116501 (stack65)
        %v116506 = vshll.u32 %v116501, 26 (stack73)
        %v116507 = vshrl.u32 %v116501, 6 (stack74)
        %v116508 = vor.u32 %v116506, %v116507 (stack75)
        %v116509 = vxor.u32 %v116504, %v116508 (stack76)
        %v116512 = vadd.s32 %v116504, %v116509 (stack65)
        %v116516 = vadd.s32 %v116512, %v9 (stack65)
        %v116518 = vshll.u32 %v116509, 6 (stack73)
        %v116519 = vshrl.u32 %v116509, 26 (stack74)
        %v116520 = vor.u32 %v116518, %v116519 (stack75)
        %v116521 = vxor.u32 %v116512, %v116520 (stack76)
        %v116524 = vadd.s32 %v116521, %v8 (stack65)
        %v116528 = vadd.s32 %v116524, 1 (stack65)
        %v116532 = vadd.s32 %v116516, %v116528 (stack65)
        %v116534 = vshll.u32 %v116528, 17 (stack73)
        %v116535 = vshrl.u32 %v116528, 15 (stack74)
        %v116536 = vor.u32 %v116534, %v116535 (stack75)
        %v116537 = vxor.u32 %v116532, %v116536 (stack76)
        %v116540 = vadd.s32 %v116532, %v116537 (stack65)
        %v116542 = vshll.u32 %v116537, 29 (stack73)
        %v116543 = vshrl.u32 %v116537, 3 (stack74)
        %v116544 = vor.u32 %v116542, %v116543 (stack75)
        %v116545 = vxor.u32 %v116540, %v116544 (stack76)
        %v116548 = vadd.s32 %v116540, %v116545 (stack65)
        %v116550 = vshll.u32 %v116545, 16 (stack73)
        %v116551 = vshrl.u32 %v116545, 16 (stack74)
        %v116552 = vor.u32 %v116550, %v116551 (stack75)
        %v116553 = vxor.u32 %v116548, %v116552 (stack76)
        %v116556 = vadd.s32 %v116548, %v116553 (stack65)
        %v116560 = vadd.s32 %v116556, %v8 (stack65)
        %v116562 = vshll.u32 %v116553, 24 (stack73)
        %v116563 = vshrl.u32 %v116553, 8 (stack74)
        %v116564 = vor.u32 %v116562, %v116563 (stack75)
        %v116565 = vxor.u32 %v116556, %v116564 (stack76)
        %v116568 = vadd.s32 %v116565, %v10 (stack65)
        %v116572 = vadd.s32 %v116568, 2 (stack65)
        %v116576 = vadd.s32 %v116560, %v116572 (stack65)
        %v116578 = vshll.u32 %v116572, 13 (stack73)
        %v116579 = vshrl.u32 %v116572, 19 (stack74)
        %v116580 = vor.u32 %v116578, %v116579 (stack75)
        %v116581 = vxor.u32 %v116576, %v116580 (stack76)
        %v116584 = vadd.s32 %v116576, %v116581 (stack65)
        %v116586 = vshll.u32 %v116581, 15 (stack73)
        %v116587 = vshrl.u32 %v116581, 17 (stack74)
        %v116588 = vor.u32 %v116586, %v116587 (stack75)
        %v116589 = vxor.u32 %v116584, %v116588 (stack76)
        %v116592 = vadd.s32 %v116584, %v116589 (stack65)
        %v116594 = vshll.u32 %v116589, 26 (stack73)
        %v116595 = vshrl.u32 %v116589, 6 (stack74)
        %v116596 = vor.u32 %v116594, %v116595 (stack75)
        %v116597 = vxor.u32 %v116592, %v116596 (stack76)
        %v116600 = vadd.s32 %v116592, %v116597 (stack65)
        %v116604 = vadd.s32 %v116600, %v10 (stack65)
        %v116606 = vshll.u32 %v116597, 6 (stack73)
        %v116607 = vshrl.u32 %v116597, 26 (stack74)
        %v116608 = vor.u32 %v116606, %v116607 (stack75)
        %v116609 = vxor.u32 %v116600, %v116608 (stack76)
        %v116612 = vadd.s32 %v116609, %v9 (stack65)
        %v116616 = vadd.s32 %v116612, 3 (stack65)
        %v116620 = vadd.s32 %v116604, %v116616 (stack65)
        %v116622 = vshll.u32 %v116616, 17 (stack73)
        %v116623 = vshrl.u32 %v116616, 15 (stack74)
        %v116624 = vor.u32 %v116622, %v116623 (stack75)
        %v116625 = vxor.u32 %v116620, %v116624 (stack76)
        %v116628 = vadd.s32 %v116620, %v116625 (stack65)
        %v116630 = vshll.u32 %v116625, 29 (stack73)
        %v116631 = vshrl.u32 %v116625, 3 (stack74)
        %v116632 = vor.u32 %v116630, %v116631 (stack75)
        %v116633 = vxor.u32 %v116628, %v116632 (stack76)
        %v116636 = vadd.s32 %v116628, %v116633 (stack65)
        %v116638 = vshll.u32 %v116633, 16 (stack73)
        %v116639 = vshrl.u32 %v116633, 16 (stack74)
        %v116640 = vor.u32 %v116638, %v116639 (stack75)
        %v116641 = vxor.u32 %v116636, %v116640 (stack76)
        %v116644 = vadd.s32 %v116636, %v116641 (stack65)
        %v116648 = vadd.s32 %v116644, %v9 (stack65)
        %v116650 = vshll.u32 %v116641, 24 (stack73)
        %v116651 = vshrl.u32 %v116641, 8 (stack74)
        %v116652 = vor.u32 %v116650, %v116651 (stack75)
        %v116653 = vxor.u32 %v116644, %v116652 (stack76)
        %v116656 = vadd.s32 %v116653, %v8 (stack65)
        %v116660 = vadd.s32 %v116656, 4 (stack65)
        %v116664 = vadd.s32 %v116648, %v116660 (stack65)
        %v116666 = vshll.u32 %v116660, 13 (stack73)
        %v116667 = vshrl.u32 %v116660, 19 (stack74)
        %v116668 = vor.u32 %v116666, %v116667 (stack75)
        %v116669 = vxor.u32 %v116664, %v116668 (stack76)
        %v116672 = vadd.s32 %v116664, %v116669 (stack65)
        %v116674 = vshll.u32 %v116669, 15 (stack73)
        %v116675 = vshrl.u32 %v116669, 17 (stack74)
        %v116676 = vor.u32 %v116674, %v116675 (stack75)
        %v116677 = vxor.u32 %v116672, %v116676 (stack76)
        %v116680 = vadd.s32 %v116672, %v116677 (stack65)
        %v116682 = vshll.u32 %v116677, 26 (stack73)
        %v116683 = vshrl.u32 %v116677, 6 (stack74)
        %v116684 = vor.u32 %v116682, %v116683 (stack75)
        %v116685 = vxor.u32 %v116680, %v116684 (stack76)
        %v116688 = vadd.s32 %v116680, %v116685 (stack65)
        %v116692 = vadd.s32 %v116688, %v8 (stack65)
        %v116694 = vshll.u32 %v116685, 6 (stack73)
        %v116695 = vshrl.u32 %v116685, 26 (stack74)
        %v116696 = vor.u32 %v116694, %v116695 (stack75)
        %v116697 = vxor.u32 %v116688, %v116696 (stack76)
        %v116700 = vadd.s32 %v116697, %v10 (stack65)
        %v116704 = vadd.s32 %v116700, 5 (stack65)
        %v116706 = vxor.u32 %v116692, %v116704 (stack76)
        %v116707 = vand.u32.u8 %v116706, 255 (stack77)
        %v116708 = vand.u32 %v116707, 65535 (stack78)
        %v116709 = vshrl.u32 %v116708, 1 (stack79)
        %v116710 = vor.u32 %v116709, 16256 (stack75)
        %v116711 = vand.u32.u16 %v116710, 65535 (stack80)
        %v116712 = vunpack.i.l.bf16 %v116711 (stack81)
        %v116716 = vadd.f32 %v116712, -1.0 (stack82)
        %v116720 = vmul.f32 %v116716, 2.0 (stack83)
        %v116724 = vadd.f32 %v116720, -0.99609375 (stack82)
        %v116728 = vmax.f32 -0.99609375, %v116724 (stack84)
        %v116730 = vand.u32 2147483647, %v116728 (stack85)
        %vm116733 = vcmp.eq.f32.partialorder %v116730, 1.0 (stack86)
        %v116738 = vmul.f32 %v116728, inf (stack83)
        %v116740 = vxor.u32 %v116728, 2147483648 (stack87)
        %v116743 = vmul.f32 %v116728, %v116740 (stack83)
        %v116745 = vadd.f32 %v116743, 1.0 (stack88)
        %v116746 = vlog2.pop %v116745 (stack89)
        %v116747 = vmul.f32 %v116746, 0.6931472 (stack90)
        %v116748 = vmul.f32 -0.5, %v116743 (stack91)
        %v116749 = vadd.f32 %v116748, 1.0 (stack92)
        %v116750 = vmul.f32 %v116749, %v116743 (stack93)
        %v116751 = vand.u32 2147483647, %v116743 (stack94)
        %vm116752 = vcmp.lt.f32.partialorder %v116751, 0.0004427343 (stack95)
        %v116753 = vsel /*vm=*/%vm116752, /*on_true_vy=*/%v116750, /*on_false_vx=*/%v116747 (stack96)
        %v116754 = vxor.u32 %v116753, 2147483648 (stack87)
        %vm116757 = vcmp.lt.f32.partialorder %v116754, 5.0 (stack86)
        %v116762 = vsel /*vm=*/%vm116757, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v116766 = vsel /*vm=*/%vm116757, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v116770 = vsel /*vm=*/%vm116757, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v116774 = vsel /*vm=*/%vm116757, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v116778 = vsel /*vm=*/%vm116757, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v116782 = vsel /*vm=*/%vm116757, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v116786 = vsel /*vm=*/%vm116757, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v116790 = vsel /*vm=*/%vm116757, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v116794 = vsel /*vm=*/%vm116757, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v116798 = vadd.f32 %v116754, -2.5 (stack82)
        %v116800 = vrsqrt.pop %v116754 (stack97)
        %v116801 = vmul.f32 %v116754, %v116800 (stack98)
        %vm116802 = vcmp.eq.f32.partialorder %v116754, inf (stack99)
        %v116803 = vsel /*vm=*/%vm116802, /*on_true_vy=*/%v116754, /*on_false_vx=*/%v116801 (stack100)
        %vm116804 = vcmp.eq.f32.partialorder %v116754, 0.0 (stack101)
        %v116805 = vand.u32 %v116754, 2147483648 (stack102)
        %v116806 = vsel /*vm=*/%vm116804, /*on_true_vy=*/%v116805, /*on_false_vx=*/%v116803 (stack103)
        %v116809 = vadd.f32 %v116806, -3.0 (stack82)
        %v116813 = vsel /*vm=*/%vm116757, /*on_true_vy=*/%v116798, /*on_false_vx=*/%v116809 (stack72)
        %v116817 = vmul.f32 %v116794, %v116813 (stack83)
        %v116821 = vadd.f32 %v116790, %v116817 (stack82)
        %v116825 = vmul.f32 %v116821, %v116813 (stack83)
        %v116829 = vadd.f32 %v116786, %v116825 (stack82)
        %v116833 = vmul.f32 %v116829, %v116813 (stack83)
        %v116837 = vadd.f32 %v116782, %v116833 (stack82)
        %v116841 = vmul.f32 %v116837, %v116813 (stack83)
        %v116845 = vadd.f32 %v116778, %v116841 (stack82)
        %v116849 = vmul.f32 %v116845, %v116813 (stack83)
        %v116853 = vadd.f32 %v116774, %v116849 (stack82)
        %v116857 = vmul.f32 %v116853, %v116813 (stack83)
        %v116861 = vadd.f32 %v116770, %v116857 (stack82)
        %v116865 = vmul.f32 %v116861, %v116813 (stack83)
        %v116869 = vadd.f32 %v116766, %v116865 (stack82)
        %v116873 = vmul.f32 %v116869, %v116813 (stack83)
        %v116877 = vadd.f32 %v116762, %v116873 (stack82)
        %v116881 = vmul.f32 %v116877, %v116728 (stack83)
        %v116885 = vsel /*vm=*/%vm116733, /*on_true_vy=*/%v116738, /*on_false_vx=*/%v116881 (stack72)
        %v116889 = vmul.f32 %v116885, 1.4140625 (stack83)
        %s116891 = scalar_lea.vmem %s280, 252 [#allocation0] (stack107)
        %v116892 = vpack.c.bf16 0.0, %v116889 (stack104)
        %116893 = vst [vmem:[%s116891] sm:$0xf] /*vst_source=*/%v116892 (stack105)
        %v116896 = vadd.s32 %v1381, %v115971 (stack65)
        %s116898 = smul.u32 128, %s27 (stack66)
        %v116899 = vlaneseq (stack67)
        %v116900 = vand.u32 %v116899, 127 (stack68)
        %v116901 = vstv %s116898 (stack69)
        %v116902 = vadd.s32 %v116900, %v116901 (stack70)
        %v116906 = vadd.s32 %v116896, %v116902 (stack65)
        %vm116910 = vcmp.lt.u32.totalorder %v116906, %v116896 (stack71)
        %vm116915 = vcmp.lt.u32.totalorder %v116896, %v1381 (stack71)
        %v116920 = vadd.s32 %v1368, %v115954 (stack65)
        %v116924 = vadd.s32 %v116920, 1 (stack65)
        %v116928 = vsel /*vm=*/%vm116915, /*on_true_vy=*/%v116924, /*on_false_vx=*/%v116920 (stack72)
        %v116932 = vadd.s32 %v116928, 1 (stack65)
        %v116936 = vsel /*vm=*/%vm116910, /*on_true_vy=*/%v116932, /*on_false_vx=*/%v116928 (stack72)
        %v116941 = vadd.s32 %v116936, %v10 (stack65)
        %v116945 = vadd.s32 %v116906, %v9 (stack65)
        %v116949 = vadd.s32 %v116941, %v116945 (stack65)
        %v116951 = vshll.u32 %v116945, 13 (stack73)
        %v116952 = vshrl.u32 %v116945, 19 (stack74)
        %v116953 = vor.u32 %v116951, %v116952 (stack75)
        %v116954 = vxor.u32 %v116949, %v116953 (stack76)
        %v116957 = vadd.s32 %v116949, %v116954 (stack65)
        %v116959 = vshll.u32 %v116954, 15 (stack73)
        %v116960 = vshrl.u32 %v116954, 17 (stack74)
        %v116961 = vor.u32 %v116959, %v116960 (stack75)
        %v116962 = vxor.u32 %v116957, %v116961 (stack76)
        %v116965 = vadd.s32 %v116957, %v116962 (stack65)
        %v116967 = vshll.u32 %v116962, 26 (stack73)
        %v116968 = vshrl.u32 %v116962, 6 (stack74)
        %v116969 = vor.u32 %v116967, %v116968 (stack75)
        %v116970 = vxor.u32 %v116965, %v116969 (stack76)
        %v116973 = vadd.s32 %v116965, %v116970 (stack65)
        %v116977 = vadd.s32 %v116973, %v9 (stack65)
        %v116979 = vshll.u32 %v116970, 6 (stack73)
        %v116980 = vshrl.u32 %v116970, 26 (stack74)
        %v116981 = vor.u32 %v116979, %v116980 (stack75)
        %v116982 = vxor.u32 %v116973, %v116981 (stack76)
        %v116985 = vadd.s32 %v116982, %v8 (stack65)
        %v116989 = vadd.s32 %v116985, 1 (stack65)
        %v116993 = vadd.s32 %v116977, %v116989 (stack65)
        %v116995 = vshll.u32 %v116989, 17 (stack73)
        %v116996 = vshrl.u32 %v116989, 15 (stack74)
        %v116997 = vor.u32 %v116995, %v116996 (stack75)
        %v116998 = vxor.u32 %v116993, %v116997 (stack76)
        %v117001 = vadd.s32 %v116993, %v116998 (stack65)
        %v117003 = vshll.u32 %v116998, 29 (stack73)
        %v117004 = vshrl.u32 %v116998, 3 (stack74)
        %v117005 = vor.u32 %v117003, %v117004 (stack75)
        %v117006 = vxor.u32 %v117001, %v117005 (stack76)
        %v117009 = vadd.s32 %v117001, %v117006 (stack65)
        %v117011 = vshll.u32 %v117006, 16 (stack73)
        %v117012 = vshrl.u32 %v117006, 16 (stack74)
        %v117013 = vor.u32 %v117011, %v117012 (stack75)
        %v117014 = vxor.u32 %v117009, %v117013 (stack76)
        %v117017 = vadd.s32 %v117009, %v117014 (stack65)
        %v117021 = vadd.s32 %v117017, %v8 (stack65)
        %v117023 = vshll.u32 %v117014, 24 (stack73)
        %v117024 = vshrl.u32 %v117014, 8 (stack74)
        %v117025 = vor.u32 %v117023, %v117024 (stack75)
        %v117026 = vxor.u32 %v117017, %v117025 (stack76)
        %v117029 = vadd.s32 %v117026, %v10 (stack65)
        %v117033 = vadd.s32 %v117029, 2 (stack65)
        %v117037 = vadd.s32 %v117021, %v117033 (stack65)
        %v117039 = vshll.u32 %v117033, 13 (stack73)
        %v117040 = vshrl.u32 %v117033, 19 (stack74)
        %v117041 = vor.u32 %v117039, %v117040 (stack75)
        %v117042 = vxor.u32 %v117037, %v117041 (stack76)
        %v117045 = vadd.s32 %v117037, %v117042 (stack65)
        %v117047 = vshll.u32 %v117042, 15 (stack73)
        %v117048 = vshrl.u32 %v117042, 17 (stack74)
        %v117049 = vor.u32 %v117047, %v117048 (stack75)
        %v117050 = vxor.u32 %v117045, %v117049 (stack76)
        %v117053 = vadd.s32 %v117045, %v117050 (stack65)
        %v117055 = vshll.u32 %v117050, 26 (stack73)
        %v117056 = vshrl.u32 %v117050, 6 (stack74)
        %v117057 = vor.u32 %v117055, %v117056 (stack75)
        %v117058 = vxor.u32 %v117053, %v117057 (stack76)
        %v117061 = vadd.s32 %v117053, %v117058 (stack65)
        %v117065 = vadd.s32 %v117061, %v10 (stack65)
        %v117067 = vshll.u32 %v117058, 6 (stack73)
        %v117068 = vshrl.u32 %v117058, 26 (stack74)
        %v117069 = vor.u32 %v117067, %v117068 (stack75)
        %v117070 = vxor.u32 %v117061, %v117069 (stack76)
        %v117073 = vadd.s32 %v117070, %v9 (stack65)
        %v117077 = vadd.s32 %v117073, 3 (stack65)
        %v117081 = vadd.s32 %v117065, %v117077 (stack65)
        %v117083 = vshll.u32 %v117077, 17 (stack73)
        %v117084 = vshrl.u32 %v117077, 15 (stack74)
        %v117085 = vor.u32 %v117083, %v117084 (stack75)
        %v117086 = vxor.u32 %v117081, %v117085 (stack76)
        %v117089 = vadd.s32 %v117081, %v117086 (stack65)
        %v117091 = vshll.u32 %v117086, 29 (stack73)
        %v117092 = vshrl.u32 %v117086, 3 (stack74)
        %v117093 = vor.u32 %v117091, %v117092 (stack75)
        %v117094 = vxor.u32 %v117089, %v117093 (stack76)
        %v117097 = vadd.s32 %v117089, %v117094 (stack65)
        %v117099 = vshll.u32 %v117094, 16 (stack73)
        %v117100 = vshrl.u32 %v117094, 16 (stack74)
        %v117101 = vor.u32 %v117099, %v117100 (stack75)
        %v117102 = vxor.u32 %v117097, %v117101 (stack76)
        %v117105 = vadd.s32 %v117097, %v117102 (stack65)
        %v117109 = vadd.s32 %v117105, %v9 (stack65)
        %v117111 = vshll.u32 %v117102, 24 (stack73)
        %v117112 = vshrl.u32 %v117102, 8 (stack74)
        %v117113 = vor.u32 %v117111, %v117112 (stack75)
        %v117114 = vxor.u32 %v117105, %v117113 (stack76)
        %v117117 = vadd.s32 %v117114, %v8 (stack65)
        %v117121 = vadd.s32 %v117117, 4 (stack65)
        %v117125 = vadd.s32 %v117109, %v117121 (stack65)
        %v117127 = vshll.u32 %v117121, 13 (stack73)
        %v117128 = vshrl.u32 %v117121, 19 (stack74)
        %v117129 = vor.u32 %v117127, %v117128 (stack75)
        %v117130 = vxor.u32 %v117125, %v117129 (stack76)
        %v117133 = vadd.s32 %v117125, %v117130 (stack65)
        %v117135 = vshll.u32 %v117130, 15 (stack73)
        %v117136 = vshrl.u32 %v117130, 17 (stack74)
        %v117137 = vor.u32 %v117135, %v117136 (stack75)
        %v117138 = vxor.u32 %v117133, %v117137 (stack76)
        %v117141 = vadd.s32 %v117133, %v117138 (stack65)
        %v117143 = vshll.u32 %v117138, 26 (stack73)
        %v117144 = vshrl.u32 %v117138, 6 (stack74)
        %v117145 = vor.u32 %v117143, %v117144 (stack75)
        %v117146 = vxor.u32 %v117141, %v117145 (stack76)
        %v117149 = vadd.s32 %v117141, %v117146 (stack65)
        %v117153 = vadd.s32 %v117149, %v8 (stack65)
        %v117155 = vshll.u32 %v117146, 6 (stack73)
        %v117156 = vshrl.u32 %v117146, 26 (stack74)
        %v117157 = vor.u32 %v117155, %v117156 (stack75)
        %v117158 = vxor.u32 %v117149, %v117157 (stack76)
        %v117161 = vadd.s32 %v117158, %v10 (stack65)
        %v117165 = vadd.s32 %v117161, 5 (stack65)
        %v117167 = vxor.u32 %v117153, %v117165 (stack76)
        %v117168 = vand.u32.u8 %v117167, 255 (stack77)
        %v117169 = vand.u32 %v117168, 65535 (stack78)
        %v117170 = vshrl.u32 %v117169, 1 (stack79)
        %v117171 = vor.u32 %v117170, 16256 (stack75)
        %v117172 = vand.u32.u16 %v117171, 65535 (stack80)
        %v117173 = vunpack.i.l.bf16 %v117172 (stack81)
        %v117177 = vadd.f32 %v117173, -1.0 (stack82)
        %v117181 = vmul.f32 %v117177, 2.0 (stack83)
        %v117185 = vadd.f32 %v117181, -0.99609375 (stack82)
        %v117189 = vmax.f32 -0.99609375, %v117185 (stack84)
        %v117191 = vand.u32 2147483647, %v117189 (stack85)
        %vm117194 = vcmp.eq.f32.partialorder %v117191, 1.0 (stack86)
        %v117199 = vmul.f32 %v117189, inf (stack83)
        %v117201 = vxor.u32 %v117189, 2147483648 (stack87)
        %v117204 = vmul.f32 %v117189, %v117201 (stack83)
        %v117206 = vadd.f32 %v117204, 1.0 (stack88)
        %v117207 = vlog2.pop %v117206 (stack89)
        %v117208 = vmul.f32 %v117207, 0.6931472 (stack90)
        %v117209 = vmul.f32 -0.5, %v117204 (stack91)
        %v117210 = vadd.f32 %v117209, 1.0 (stack92)
        %v117211 = vmul.f32 %v117210, %v117204 (stack93)
        %v117212 = vand.u32 2147483647, %v117204 (stack94)
        %vm117213 = vcmp.lt.f32.partialorder %v117212, 0.0004427343 (stack95)
        %v117214 = vsel /*vm=*/%vm117213, /*on_true_vy=*/%v117211, /*on_false_vx=*/%v117208 (stack96)
        %v117215 = vxor.u32 %v117214, 2147483648 (stack87)
        %vm117218 = vcmp.lt.f32.partialorder %v117215, 5.0 (stack86)
        %v117223 = vsel /*vm=*/%vm117218, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v117227 = vsel /*vm=*/%vm117218, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v117231 = vsel /*vm=*/%vm117218, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v117235 = vsel /*vm=*/%vm117218, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v117239 = vsel /*vm=*/%vm117218, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v117243 = vsel /*vm=*/%vm117218, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v117247 = vsel /*vm=*/%vm117218, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v117251 = vsel /*vm=*/%vm117218, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v117255 = vsel /*vm=*/%vm117218, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v117259 = vadd.f32 %v117215, -2.5 (stack82)
        %v117261 = vrsqrt.pop %v117215 (stack97)
        %v117262 = vmul.f32 %v117215, %v117261 (stack98)
        %vm117263 = vcmp.eq.f32.partialorder %v117215, inf (stack99)
        %v117264 = vsel /*vm=*/%vm117263, /*on_true_vy=*/%v117215, /*on_false_vx=*/%v117262 (stack100)
        %vm117265 = vcmp.eq.f32.partialorder %v117215, 0.0 (stack101)
        %v117266 = vand.u32 %v117215, 2147483648 (stack102)
        %v117267 = vsel /*vm=*/%vm117265, /*on_true_vy=*/%v117266, /*on_false_vx=*/%v117264 (stack103)
        %v117270 = vadd.f32 %v117267, -3.0 (stack82)
        %v117274 = vsel /*vm=*/%vm117218, /*on_true_vy=*/%v117259, /*on_false_vx=*/%v117270 (stack72)
        %v117278 = vmul.f32 %v117255, %v117274 (stack83)
        %v117282 = vadd.f32 %v117251, %v117278 (stack82)
        %v117286 = vmul.f32 %v117282, %v117274 (stack83)
        %v117290 = vadd.f32 %v117247, %v117286 (stack82)
        %v117294 = vmul.f32 %v117290, %v117274 (stack83)
        %v117298 = vadd.f32 %v117243, %v117294 (stack82)
        %v117302 = vmul.f32 %v117298, %v117274 (stack83)
        %v117306 = vadd.f32 %v117239, %v117302 (stack82)
        %v117310 = vmul.f32 %v117306, %v117274 (stack83)
        %v117314 = vadd.f32 %v117235, %v117310 (stack82)
        %v117318 = vmul.f32 %v117314, %v117274 (stack83)
        %v117322 = vadd.f32 %v117231, %v117318 (stack82)
        %v117326 = vmul.f32 %v117322, %v117274 (stack83)
        %v117330 = vadd.f32 %v117227, %v117326 (stack82)
        %v117334 = vmul.f32 %v117330, %v117274 (stack83)
        %v117338 = vadd.f32 %v117223, %v117334 (stack82)
        %v117342 = vmul.f32 %v117338, %v117189 (stack83)
        %v117346 = vsel /*vm=*/%vm117194, /*on_true_vy=*/%v117199, /*on_false_vx=*/%v117342 (stack72)
        %v117350 = vmul.f32 %v117346, 1.4140625 (stack83)
        %s117352 = scalar_lea.vmem %s280, 380 [#allocation0] (stack107)
        %v117353 = vpack.c.bf16 0.0, %v117350 (stack104)
        %117354 = vst [vmem:[%s117352] sm:$0xf] /*vst_source=*/%v117353 (stack105)
        %v117357 = vadd.s32 %v1868, %v115971 (stack65)
        %s117359 = smul.u32 128, %s27 (stack66)
        %v117360 = vlaneseq (stack67)
        %v117361 = vand.u32 %v117360, 127 (stack68)
        %v117362 = vstv %s117359 (stack69)
        %v117363 = vadd.s32 %v117361, %v117362 (stack70)
        %v117367 = vadd.s32 %v117357, %v117363 (stack65)
        %vm117371 = vcmp.lt.u32.totalorder %v117367, %v117357 (stack71)
        %vm117376 = vcmp.lt.u32.totalorder %v117357, %v1868 (stack71)
        %v117381 = vadd.s32 %v1855, %v115954 (stack65)
        %v117385 = vadd.s32 %v117381, 1 (stack65)
        %v117389 = vsel /*vm=*/%vm117376, /*on_true_vy=*/%v117385, /*on_false_vx=*/%v117381 (stack72)
        %v117393 = vadd.s32 %v117389, 1 (stack65)
        %v117397 = vsel /*vm=*/%vm117371, /*on_true_vy=*/%v117393, /*on_false_vx=*/%v117389 (stack72)
        %v117402 = vadd.s32 %v117397, %v10 (stack65)
        %v117406 = vadd.s32 %v117367, %v9 (stack65)
        %v117410 = vadd.s32 %v117402, %v117406 (stack65)
        %v117412 = vshll.u32 %v117406, 13 (stack73)
        %v117413 = vshrl.u32 %v117406, 19 (stack74)
        %v117414 = vor.u32 %v117412, %v117413 (stack75)
        %v117415 = vxor.u32 %v117410, %v117414 (stack76)
        %v117418 = vadd.s32 %v117410, %v117415 (stack65)
        %v117420 = vshll.u32 %v117415, 15 (stack73)
        %v117421 = vshrl.u32 %v117415, 17 (stack74)
        %v117422 = vor.u32 %v117420, %v117421 (stack75)
        %v117423 = vxor.u32 %v117418, %v117422 (stack76)
        %v117426 = vadd.s32 %v117418, %v117423 (stack65)
        %v117428 = vshll.u32 %v117423, 26 (stack73)
        %v117429 = vshrl.u32 %v117423, 6 (stack74)
        %v117430 = vor.u32 %v117428, %v117429 (stack75)
        %v117431 = vxor.u32 %v117426, %v117430 (stack76)
        %v117434 = vadd.s32 %v117426, %v117431 (stack65)
        %v117438 = vadd.s32 %v117434, %v9 (stack65)
        %v117440 = vshll.u32 %v117431, 6 (stack73)
        %v117441 = vshrl.u32 %v117431, 26 (stack74)
        %v117442 = vor.u32 %v117440, %v117441 (stack75)
        %v117443 = vxor.u32 %v117434, %v117442 (stack76)
        %v117446 = vadd.s32 %v117443, %v8 (stack65)
        %v117450 = vadd.s32 %v117446, 1 (stack65)
        %v117454 = vadd.s32 %v117438, %v117450 (stack65)
        %v117456 = vshll.u32 %v117450, 17 (stack73)
        %v117457 = vshrl.u32 %v117450, 15 (stack74)
        %v117458 = vor.u32 %v117456, %v117457 (stack75)
        %v117459 = vxor.u32 %v117454, %v117458 (stack76)
        %v117462 = vadd.s32 %v117454, %v117459 (stack65)
        %v117464 = vshll.u32 %v117459, 29 (stack73)
        %v117465 = vshrl.u32 %v117459, 3 (stack74)
        %v117466 = vor.u32 %v117464, %v117465 (stack75)
        %v117467 = vxor.u32 %v117462, %v117466 (stack76)
        %v117470 = vadd.s32 %v117462, %v117467 (stack65)
        %v117472 = vshll.u32 %v117467, 16 (stack73)
        %v117473 = vshrl.u32 %v117467, 16 (stack74)
        %v117474 = vor.u32 %v117472, %v117473 (stack75)
        %v117475 = vxor.u32 %v117470, %v117474 (stack76)
        %v117478 = vadd.s32 %v117470, %v117475 (stack65)
        %v117482 = vadd.s32 %v117478, %v8 (stack65)
        %v117484 = vshll.u32 %v117475, 24 (stack73)
        %v117485 = vshrl.u32 %v117475, 8 (stack74)
        %v117486 = vor.u32 %v117484, %v117485 (stack75)
        %v117487 = vxor.u32 %v117478, %v117486 (stack76)
        %v117490 = vadd.s32 %v117487, %v10 (stack65)
        %v117494 = vadd.s32 %v117490, 2 (stack65)
        %v117498 = vadd.s32 %v117482, %v117494 (stack65)
        %v117500 = vshll.u32 %v117494, 13 (stack73)
        %v117501 = vshrl.u32 %v117494, 19 (stack74)
        %v117502 = vor.u32 %v117500, %v117501 (stack75)
        %v117503 = vxor.u32 %v117498, %v117502 (stack76)
        %v117506 = vadd.s32 %v117498, %v117503 (stack65)
        %v117508 = vshll.u32 %v117503, 15 (stack73)
        %v117509 = vshrl.u32 %v117503, 17 (stack74)
        %v117510 = vor.u32 %v117508, %v117509 (stack75)
        %v117511 = vxor.u32 %v117506, %v117510 (stack76)
        %v117514 = vadd.s32 %v117506, %v117511 (stack65)
        %v117516 = vshll.u32 %v117511, 26 (stack73)
        %v117517 = vshrl.u32 %v117511, 6 (stack74)
        %v117518 = vor.u32 %v117516, %v117517 (stack75)
        %v117519 = vxor.u32 %v117514, %v117518 (stack76)
        %v117522 = vadd.s32 %v117514, %v117519 (stack65)
        %v117526 = vadd.s32 %v117522, %v10 (stack65)
        %v117528 = vshll.u32 %v117519, 6 (stack73)
        %v117529 = vshrl.u32 %v117519, 26 (stack74)
        %v117530 = vor.u32 %v117528, %v117529 (stack75)
        %v117531 = vxor.u32 %v117522, %v117530 (stack76)
        %v117534 = vadd.s32 %v117531, %v9 (stack65)
        %v117538 = vadd.s32 %v117534, 3 (stack65)
        %v117542 = vadd.s32 %v117526, %v117538 (stack65)
        %v117544 = vshll.u32 %v117538, 17 (stack73)
        %v117545 = vshrl.u32 %v117538, 15 (stack74)
        %v117546 = vor.u32 %v117544, %v117545 (stack75)
        %v117547 = vxor.u32 %v117542, %v117546 (stack76)
        %v117550 = vadd.s32 %v117542, %v117547 (stack65)
        %v117552 = vshll.u32 %v117547, 29 (stack73)
        %v117553 = vshrl.u32 %v117547, 3 (stack74)
        %v117554 = vor.u32 %v117552, %v117553 (stack75)
        %v117555 = vxor.u32 %v117550, %v117554 (stack76)
        %v117558 = vadd.s32 %v117550, %v117555 (stack65)
        %v117560 = vshll.u32 %v117555, 16 (stack73)
        %v117561 = vshrl.u32 %v117555, 16 (stack74)
        %v117562 = vor.u32 %v117560, %v117561 (stack75)
        %v117563 = vxor.u32 %v117558, %v117562 (stack76)
        %v117566 = vadd.s32 %v117558, %v117563 (stack65)
        %v117570 = vadd.s32 %v117566, %v9 (stack65)
        %v117572 = vshll.u32 %v117563, 24 (stack73)
        %v117573 = vshrl.u32 %v117563, 8 (stack74)
        %v117574 = vor.u32 %v117572, %v117573 (stack75)
        %v117575 = vxor.u32 %v117566, %v117574 (stack76)
        %v117578 = vadd.s32 %v117575, %v8 (stack65)
        %v117582 = vadd.s32 %v117578, 4 (stack65)
        %v117586 = vadd.s32 %v117570, %v117582 (stack65)
        %v117588 = vshll.u32 %v117582, 13 (stack73)
        %v117589 = vshrl.u32 %v117582, 19 (stack74)
        %v117590 = vor.u32 %v117588, %v117589 (stack75)
        %v117591 = vxor.u32 %v117586, %v117590 (stack76)
        %v117594 = vadd.s32 %v117586, %v117591 (stack65)
        %v117596 = vshll.u32 %v117591, 15 (stack73)
        %v117597 = vshrl.u32 %v117591, 17 (stack74)
        %v117598 = vor.u32 %v117596, %v117597 (stack75)
        %v117599 = vxor.u32 %v117594, %v117598 (stack76)
        %v117602 = vadd.s32 %v117594, %v117599 (stack65)
        %v117604 = vshll.u32 %v117599, 26 (stack73)
        %v117605 = vshrl.u32 %v117599, 6 (stack74)
        %v117606 = vor.u32 %v117604, %v117605 (stack75)
        %v117607 = vxor.u32 %v117602, %v117606 (stack76)
        %v117610 = vadd.s32 %v117602, %v117607 (stack65)
        %v117614 = vadd.s32 %v117610, %v8 (stack65)
        %v117616 = vshll.u32 %v117607, 6 (stack73)
        %v117617 = vshrl.u32 %v117607, 26 (stack74)
        %v117618 = vor.u32 %v117616, %v117617 (stack75)
        %v117619 = vxor.u32 %v117610, %v117618 (stack76)
        %v117622 = vadd.s32 %v117619, %v10 (stack65)
        %v117626 = vadd.s32 %v117622, 5 (stack65)
        %v117628 = vxor.u32 %v117614, %v117626 (stack76)
        %v117629 = vand.u32.u8 %v117628, 255 (stack77)
        %v117630 = vand.u32 %v117629, 65535 (stack78)
        %v117631 = vshrl.u32 %v117630, 1 (stack79)
        %v117632 = vor.u32 %v117631, 16256 (stack75)
        %v117633 = vand.u32.u16 %v117632, 65535 (stack80)
        %v117634 = vunpack.i.l.bf16 %v117633 (stack81)
        %v117638 = vadd.f32 %v117634, -1.0 (stack82)
        %v117642 = vmul.f32 %v117638, 2.0 (stack83)
        %v117646 = vadd.f32 %v117642, -0.99609375 (stack82)
        %v117650 = vmax.f32 -0.99609375, %v117646 (stack84)
        %v117652 = vand.u32 2147483647, %v117650 (stack85)
        %vm117655 = vcmp.eq.f32.partialorder %v117652, 1.0 (stack86)
        %v117660 = vmul.f32 %v117650, inf (stack83)
        %v117662 = vxor.u32 %v117650, 2147483648 (stack87)
        %v117665 = vmul.f32 %v117650, %v117662 (stack83)
        %v117667 = vadd.f32 %v117665, 1.0 (stack88)
        %v117668 = vlog2.pop %v117667 (stack89)
        %v117669 = vmul.f32 %v117668, 0.6931472 (stack90)
        %v117670 = vmul.f32 -0.5, %v117665 (stack91)
        %v117671 = vadd.f32 %v117670, 1.0 (stack92)
        %v117672 = vmul.f32 %v117671, %v117665 (stack93)
        %v117673 = vand.u32 2147483647, %v117665 (stack94)
        %vm117674 = vcmp.lt.f32.partialorder %v117673, 0.0004427343 (stack95)
        %v117675 = vsel /*vm=*/%vm117674, /*on_true_vy=*/%v117672, /*on_false_vx=*/%v117669 (stack96)
        %v117676 = vxor.u32 %v117675, 2147483648 (stack87)
        %vm117679 = vcmp.lt.f32.partialorder %v117676, 5.0 (stack86)
        %v117684 = vsel /*vm=*/%vm117679, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v117688 = vsel /*vm=*/%vm117679, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v117692 = vsel /*vm=*/%vm117679, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v117696 = vsel /*vm=*/%vm117679, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v117700 = vsel /*vm=*/%vm117679, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v117704 = vsel /*vm=*/%vm117679, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v117708 = vsel /*vm=*/%vm117679, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v117712 = vsel /*vm=*/%vm117679, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v117716 = vsel /*vm=*/%vm117679, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v117720 = vadd.f32 %v117676, -2.5 (stack82)
        %v117722 = vrsqrt.pop %v117676 (stack97)
        %v117723 = vmul.f32 %v117676, %v117722 (stack98)
        %vm117724 = vcmp.eq.f32.partialorder %v117676, inf (stack99)
        %v117725 = vsel /*vm=*/%vm117724, /*on_true_vy=*/%v117676, /*on_false_vx=*/%v117723 (stack100)
        %vm117726 = vcmp.eq.f32.partialorder %v117676, 0.0 (stack101)
        %v117727 = vand.u32 %v117676, 2147483648 (stack102)
        %v117728 = vsel /*vm=*/%vm117726, /*on_true_vy=*/%v117727, /*on_false_vx=*/%v117725 (stack103)
        %v117731 = vadd.f32 %v117728, -3.0 (stack82)
        %v117735 = vsel /*vm=*/%vm117679, /*on_true_vy=*/%v117720, /*on_false_vx=*/%v117731 (stack72)
        %v117739 = vmul.f32 %v117716, %v117735 (stack83)
        %v117743 = vadd.f32 %v117712, %v117739 (stack82)
        %v117747 = vmul.f32 %v117743, %v117735 (stack83)
        %v117751 = vadd.f32 %v117708, %v117747 (stack82)
        %v117755 = vmul.f32 %v117751, %v117735 (stack83)
        %v117759 = vadd.f32 %v117704, %v117755 (stack82)
        %v117763 = vmul.f32 %v117759, %v117735 (stack83)
        %v117767 = vadd.f32 %v117700, %v117763 (stack82)
        %v117771 = vmul.f32 %v117767, %v117735 (stack83)
        %v117775 = vadd.f32 %v117696, %v117771 (stack82)
        %v117779 = vmul.f32 %v117775, %v117735 (stack83)
        %v117783 = vadd.f32 %v117692, %v117779 (stack82)
        %v117787 = vmul.f32 %v117783, %v117735 (stack83)
        %v117791 = vadd.f32 %v117688, %v117787 (stack82)
        %v117795 = vmul.f32 %v117791, %v117735 (stack83)
        %v117799 = vadd.f32 %v117684, %v117795 (stack82)
        %v117803 = vmul.f32 %v117799, %v117650 (stack83)
        %v117807 = vsel /*vm=*/%vm117655, /*on_true_vy=*/%v117660, /*on_false_vx=*/%v117803 (stack72)
        %v117811 = vmul.f32 %v117807, 1.4140625 (stack83)
        %s117813 = scalar_lea.vmem %s280, 508 [#allocation0] (stack107)
        %v117814 = vpack.c.bf16 0.0, %v117811 (stack104)
        %117815 = vst [vmem:[%s117813] sm:$0xf] /*vst_source=*/%v117814 (stack105)
        %v117818 = vadd.s32 %v2355, %v115971 (stack65)
        %s117820 = smul.u32 128, %s27 (stack66)
        %v117821 = vlaneseq (stack67)
        %v117822 = vand.u32 %v117821, 127 (stack68)
        %v117823 = vstv %s117820 (stack69)
        %v117824 = vadd.s32 %v117822, %v117823 (stack70)
        %v117828 = vadd.s32 %v117818, %v117824 (stack65)
        %vm117832 = vcmp.lt.u32.totalorder %v117828, %v117818 (stack71)
        %vm117837 = vcmp.lt.u32.totalorder %v117818, %v2355 (stack71)
        %v117842 = vadd.s32 %v2342, %v115954 (stack65)
        %v117846 = vadd.s32 %v117842, 1 (stack65)
        %v117850 = vsel /*vm=*/%vm117837, /*on_true_vy=*/%v117846, /*on_false_vx=*/%v117842 (stack72)
        %v117854 = vadd.s32 %v117850, 1 (stack65)
        %v117858 = vsel /*vm=*/%vm117832, /*on_true_vy=*/%v117854, /*on_false_vx=*/%v117850 (stack72)
        %v117863 = vadd.s32 %v117858, %v10 (stack65)
        %v117867 = vadd.s32 %v117828, %v9 (stack65)
        %v117871 = vadd.s32 %v117863, %v117867 (stack65)
        %v117873 = vshll.u32 %v117867, 13 (stack73)
        %v117874 = vshrl.u32 %v117867, 19 (stack74)
        %v117875 = vor.u32 %v117873, %v117874 (stack75)
        %v117876 = vxor.u32 %v117871, %v117875 (stack76)
        %v117879 = vadd.s32 %v117871, %v117876 (stack65)
        %v117881 = vshll.u32 %v117876, 15 (stack73)
        %v117882 = vshrl.u32 %v117876, 17 (stack74)
        %v117883 = vor.u32 %v117881, %v117882 (stack75)
        %v117884 = vxor.u32 %v117879, %v117883 (stack76)
        %v117887 = vadd.s32 %v117879, %v117884 (stack65)
        %v117889 = vshll.u32 %v117884, 26 (stack73)
        %v117890 = vshrl.u32 %v117884, 6 (stack74)
        %v117891 = vor.u32 %v117889, %v117890 (stack75)
        %v117892 = vxor.u32 %v117887, %v117891 (stack76)
        %v117895 = vadd.s32 %v117887, %v117892 (stack65)
        %v117899 = vadd.s32 %v117895, %v9 (stack65)
        %v117901 = vshll.u32 %v117892, 6 (stack73)
        %v117902 = vshrl.u32 %v117892, 26 (stack74)
        %v117903 = vor.u32 %v117901, %v117902 (stack75)
        %v117904 = vxor.u32 %v117895, %v117903 (stack76)
        %v117907 = vadd.s32 %v117904, %v8 (stack65)
        %v117911 = vadd.s32 %v117907, 1 (stack65)
        %v117915 = vadd.s32 %v117899, %v117911 (stack65)
        %v117917 = vshll.u32 %v117911, 17 (stack73)
        %v117918 = vshrl.u32 %v117911, 15 (stack74)
        %v117919 = vor.u32 %v117917, %v117918 (stack75)
        %v117920 = vxor.u32 %v117915, %v117919 (stack76)
        %v117923 = vadd.s32 %v117915, %v117920 (stack65)
        %v117925 = vshll.u32 %v117920, 29 (stack73)
        %v117926 = vshrl.u32 %v117920, 3 (stack74)
        %v117927 = vor.u32 %v117925, %v117926 (stack75)
        %v117928 = vxor.u32 %v117923, %v117927 (stack76)
        %v117931 = vadd.s32 %v117923, %v117928 (stack65)
        %v117933 = vshll.u32 %v117928, 16 (stack73)
        %v117934 = vshrl.u32 %v117928, 16 (stack74)
        %v117935 = vor.u32 %v117933, %v117934 (stack75)
        %v117936 = vxor.u32 %v117931, %v117935 (stack76)
        %v117939 = vadd.s32 %v117931, %v117936 (stack65)
        %v117943 = vadd.s32 %v117939, %v8 (stack65)
        %v117945 = vshll.u32 %v117936, 24 (stack73)
        %v117946 = vshrl.u32 %v117936, 8 (stack74)
        %v117947 = vor.u32 %v117945, %v117946 (stack75)
        %v117948 = vxor.u32 %v117939, %v117947 (stack76)
        %v117951 = vadd.s32 %v117948, %v10 (stack65)
        %v117955 = vadd.s32 %v117951, 2 (stack65)
        %v117959 = vadd.s32 %v117943, %v117955 (stack65)
        %v117961 = vshll.u32 %v117955, 13 (stack73)
        %v117962 = vshrl.u32 %v117955, 19 (stack74)
        %v117963 = vor.u32 %v117961, %v117962 (stack75)
        %v117964 = vxor.u32 %v117959, %v117963 (stack76)
        %v117967 = vadd.s32 %v117959, %v117964 (stack65)
        %v117969 = vshll.u32 %v117964, 15 (stack73)
        %v117970 = vshrl.u32 %v117964, 17 (stack74)
        %v117971 = vor.u32 %v117969, %v117970 (stack75)
        %v117972 = vxor.u32 %v117967, %v117971 (stack76)
        %v117975 = vadd.s32 %v117967, %v117972 (stack65)
        %v117977 = vshll.u32 %v117972, 26 (stack73)
        %v117978 = vshrl.u32 %v117972, 6 (stack74)
        %v117979 = vor.u32 %v117977, %v117978 (stack75)
        %v117980 = vxor.u32 %v117975, %v117979 (stack76)
        %v117983 = vadd.s32 %v117975, %v117980 (stack65)
        %v117987 = vadd.s32 %v117983, %v10 (stack65)
        %v117989 = vshll.u32 %v117980, 6 (stack73)
        %v117990 = vshrl.u32 %v117980, 26 (stack74)
        %v117991 = vor.u32 %v117989, %v117990 (stack75)
        %v117992 = vxor.u32 %v117983, %v117991 (stack76)
        %v117995 = vadd.s32 %v117992, %v9 (stack65)
        %v117999 = vadd.s32 %v117995, 3 (stack65)
        %v118003 = vadd.s32 %v117987, %v117999 (stack65)
        %v118005 = vshll.u32 %v117999, 17 (stack73)
        %v118006 = vshrl.u32 %v117999, 15 (stack74)
        %v118007 = vor.u32 %v118005, %v118006 (stack75)
        %v118008 = vxor.u32 %v118003, %v118007 (stack76)
        %v118011 = vadd.s32 %v118003, %v118008 (stack65)
        %v118013 = vshll.u32 %v118008, 29 (stack73)
        %v118014 = vshrl.u32 %v118008, 3 (stack74)
        %v118015 = vor.u32 %v118013, %v118014 (stack75)
        %v118016 = vxor.u32 %v118011, %v118015 (stack76)
        %v118019 = vadd.s32 %v118011, %v118016 (stack65)
        %v118021 = vshll.u32 %v118016, 16 (stack73)
        %v118022 = vshrl.u32 %v118016, 16 (stack74)
        %v118023 = vor.u32 %v118021, %v118022 (stack75)
        %v118024 = vxor.u32 %v118019, %v118023 (stack76)
        %v118027 = vadd.s32 %v118019, %v118024 (stack65)
        %v118031 = vadd.s32 %v118027, %v9 (stack65)
        %v118033 = vshll.u32 %v118024, 24 (stack73)
        %v118034 = vshrl.u32 %v118024, 8 (stack74)
        %v118035 = vor.u32 %v118033, %v118034 (stack75)
        %v118036 = vxor.u32 %v118027, %v118035 (stack76)
        %v118039 = vadd.s32 %v118036, %v8 (stack65)
        %v118043 = vadd.s32 %v118039, 4 (stack65)
        %v118047 = vadd.s32 %v118031, %v118043 (stack65)
        %v118049 = vshll.u32 %v118043, 13 (stack73)
        %v118050 = vshrl.u32 %v118043, 19 (stack74)
        %v118051 = vor.u32 %v118049, %v118050 (stack75)
        %v118052 = vxor.u32 %v118047, %v118051 (stack76)
        %v118055 = vadd.s32 %v118047, %v118052 (stack65)
        %v118057 = vshll.u32 %v118052, 15 (stack73)
        %v118058 = vshrl.u32 %v118052, 17 (stack74)
        %v118059 = vor.u32 %v118057, %v118058 (stack75)
        %v118060 = vxor.u32 %v118055, %v118059 (stack76)
        %v118063 = vadd.s32 %v118055, %v118060 (stack65)
        %v118065 = vshll.u32 %v118060, 26 (stack73)
        %v118066 = vshrl.u32 %v118060, 6 (stack74)
        %v118067 = vor.u32 %v118065, %v118066 (stack75)
        %v118068 = vxor.u32 %v118063, %v118067 (stack76)
        %v118071 = vadd.s32 %v118063, %v118068 (stack65)
        %v118075 = vadd.s32 %v118071, %v8 (stack65)
        %v118077 = vshll.u32 %v118068, 6 (stack73)
        %v118078 = vshrl.u32 %v118068, 26 (stack74)
        %v118079 = vor.u32 %v118077, %v118078 (stack75)
        %v118080 = vxor.u32 %v118071, %v118079 (stack76)
        %v118083 = vadd.s32 %v118080, %v10 (stack65)
        %v118087 = vadd.s32 %v118083, 5 (stack65)
        %v118089 = vxor.u32 %v118075, %v118087 (stack76)
        %v118090 = vand.u32.u8 %v118089, 255 (stack77)
        %v118091 = vand.u32 %v118090, 65535 (stack78)
        %v118092 = vshrl.u32 %v118091, 1 (stack79)
        %v118093 = vor.u32 %v118092, 16256 (stack75)
        %v118094 = vand.u32.u16 %v118093, 65535 (stack80)
        %v118095 = vunpack.i.l.bf16 %v118094 (stack81)
        %v118099 = vadd.f32 %v118095, -1.0 (stack82)
        %v118103 = vmul.f32 %v118099, 2.0 (stack83)
        %v118107 = vadd.f32 %v118103, -0.99609375 (stack82)
        %v118111 = vmax.f32 -0.99609375, %v118107 (stack84)
        %v118113 = vand.u32 2147483647, %v118111 (stack85)
        %vm118116 = vcmp.eq.f32.partialorder %v118113, 1.0 (stack86)
        %v118121 = vmul.f32 %v118111, inf (stack83)
        %v118123 = vxor.u32 %v118111, 2147483648 (stack87)
        %v118126 = vmul.f32 %v118111, %v118123 (stack83)
        %v118128 = vadd.f32 %v118126, 1.0 (stack88)
        %v118129 = vlog2.pop %v118128 (stack89)
        %v118130 = vmul.f32 %v118129, 0.6931472 (stack90)
        %v118131 = vmul.f32 -0.5, %v118126 (stack91)
        %v118132 = vadd.f32 %v118131, 1.0 (stack92)
        %v118133 = vmul.f32 %v118132, %v118126 (stack93)
        %v118134 = vand.u32 2147483647, %v118126 (stack94)
        %vm118135 = vcmp.lt.f32.partialorder %v118134, 0.0004427343 (stack95)
        %v118136 = vsel /*vm=*/%vm118135, /*on_true_vy=*/%v118133, /*on_false_vx=*/%v118130 (stack96)
        %v118137 = vxor.u32 %v118136, 2147483648 (stack87)
        %vm118140 = vcmp.lt.f32.partialorder %v118137, 5.0 (stack86)
        %v118145 = vsel /*vm=*/%vm118140, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v118149 = vsel /*vm=*/%vm118140, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v118153 = vsel /*vm=*/%vm118140, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v118157 = vsel /*vm=*/%vm118140, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v118161 = vsel /*vm=*/%vm118140, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v118165 = vsel /*vm=*/%vm118140, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v118169 = vsel /*vm=*/%vm118140, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v118173 = vsel /*vm=*/%vm118140, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v118177 = vsel /*vm=*/%vm118140, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v118181 = vadd.f32 %v118137, -2.5 (stack82)
        %v118183 = vrsqrt.pop %v118137 (stack97)
        %v118184 = vmul.f32 %v118137, %v118183 (stack98)
        %vm118185 = vcmp.eq.f32.partialorder %v118137, inf (stack99)
        %v118186 = vsel /*vm=*/%vm118185, /*on_true_vy=*/%v118137, /*on_false_vx=*/%v118184 (stack100)
        %vm118187 = vcmp.eq.f32.partialorder %v118137, 0.0 (stack101)
        %v118188 = vand.u32 %v118137, 2147483648 (stack102)
        %v118189 = vsel /*vm=*/%vm118187, /*on_true_vy=*/%v118188, /*on_false_vx=*/%v118186 (stack103)
        %v118192 = vadd.f32 %v118189, -3.0 (stack82)
        %v118196 = vsel /*vm=*/%vm118140, /*on_true_vy=*/%v118181, /*on_false_vx=*/%v118192 (stack72)
        %v118200 = vmul.f32 %v118177, %v118196 (stack83)
        %v118204 = vadd.f32 %v118173, %v118200 (stack82)
        %v118208 = vmul.f32 %v118204, %v118196 (stack83)
        %v118212 = vadd.f32 %v118169, %v118208 (stack82)
        %v118216 = vmul.f32 %v118212, %v118196 (stack83)
        %v118220 = vadd.f32 %v118165, %v118216 (stack82)
        %v118224 = vmul.f32 %v118220, %v118196 (stack83)
        %v118228 = vadd.f32 %v118161, %v118224 (stack82)
        %v118232 = vmul.f32 %v118228, %v118196 (stack83)
        %v118236 = vadd.f32 %v118157, %v118232 (stack82)
        %v118240 = vmul.f32 %v118236, %v118196 (stack83)
        %v118244 = vadd.f32 %v118153, %v118240 (stack82)
        %v118248 = vmul.f32 %v118244, %v118196 (stack83)
        %v118252 = vadd.f32 %v118149, %v118248 (stack82)
        %v118256 = vmul.f32 %v118252, %v118196 (stack83)
        %v118260 = vadd.f32 %v118145, %v118256 (stack82)
        %v118264 = vmul.f32 %v118260, %v118111 (stack83)
        %v118268 = vsel /*vm=*/%vm118116, /*on_true_vy=*/%v118121, /*on_false_vx=*/%v118264 (stack72)
        %v118272 = vmul.f32 %v118268, 1.4140625 (stack83)
        %s118274 = scalar_lea.vmem %s280, 636 [#allocation0] (stack107)
        %v118275 = vpack.c.bf16 0.0, %v118272 (stack104)
        %118276 = vst [vmem:[%s118274] sm:$0xf] /*vst_source=*/%v118275 (stack105)
        %v118279 = vadd.s32 %v2842, %v115971 (stack65)
        %s118281 = smul.u32 128, %s27 (stack66)
        %v118282 = vlaneseq (stack67)
        %v118283 = vand.u32 %v118282, 127 (stack68)
        %v118284 = vstv %s118281 (stack69)
        %v118285 = vadd.s32 %v118283, %v118284 (stack70)
        %v118289 = vadd.s32 %v118279, %v118285 (stack65)
        %vm118293 = vcmp.lt.u32.totalorder %v118289, %v118279 (stack71)
        %vm118298 = vcmp.lt.u32.totalorder %v118279, %v2842 (stack71)
        %v118303 = vadd.s32 %v2829, %v115954 (stack65)
        %v118307 = vadd.s32 %v118303, 1 (stack65)
        %v118311 = vsel /*vm=*/%vm118298, /*on_true_vy=*/%v118307, /*on_false_vx=*/%v118303 (stack72)
        %v118315 = vadd.s32 %v118311, 1 (stack65)
        %v118319 = vsel /*vm=*/%vm118293, /*on_true_vy=*/%v118315, /*on_false_vx=*/%v118311 (stack72)
        %v118324 = vadd.s32 %v118319, %v10 (stack65)
        %v118328 = vadd.s32 %v118289, %v9 (stack65)
        %v118332 = vadd.s32 %v118324, %v118328 (stack65)
        %v118334 = vshll.u32 %v118328, 13 (stack73)
        %v118335 = vshrl.u32 %v118328, 19 (stack74)
        %v118336 = vor.u32 %v118334, %v118335 (stack75)
        %v118337 = vxor.u32 %v118332, %v118336 (stack76)
        %v118340 = vadd.s32 %v118332, %v118337 (stack65)
        %v118342 = vshll.u32 %v118337, 15 (stack73)
        %v118343 = vshrl.u32 %v118337, 17 (stack74)
        %v118344 = vor.u32 %v118342, %v118343 (stack75)
        %v118345 = vxor.u32 %v118340, %v118344 (stack76)
        %v118348 = vadd.s32 %v118340, %v118345 (stack65)
        %v118350 = vshll.u32 %v118345, 26 (stack73)
        %v118351 = vshrl.u32 %v118345, 6 (stack74)
        %v118352 = vor.u32 %v118350, %v118351 (stack75)
        %v118353 = vxor.u32 %v118348, %v118352 (stack76)
        %v118356 = vadd.s32 %v118348, %v118353 (stack65)
        %v118360 = vadd.s32 %v118356, %v9 (stack65)
        %v118362 = vshll.u32 %v118353, 6 (stack73)
        %v118363 = vshrl.u32 %v118353, 26 (stack74)
        %v118364 = vor.u32 %v118362, %v118363 (stack75)
        %v118365 = vxor.u32 %v118356, %v118364 (stack76)
        %v118368 = vadd.s32 %v118365, %v8 (stack65)
        %v118372 = vadd.s32 %v118368, 1 (stack65)
        %v118376 = vadd.s32 %v118360, %v118372 (stack65)
        %v118378 = vshll.u32 %v118372, 17 (stack73)
        %v118379 = vshrl.u32 %v118372, 15 (stack74)
        %v118380 = vor.u32 %v118378, %v118379 (stack75)
        %v118381 = vxor.u32 %v118376, %v118380 (stack76)
        %v118384 = vadd.s32 %v118376, %v118381 (stack65)
        %v118386 = vshll.u32 %v118381, 29 (stack73)
        %v118387 = vshrl.u32 %v118381, 3 (stack74)
        %v118388 = vor.u32 %v118386, %v118387 (stack75)
        %v118389 = vxor.u32 %v118384, %v118388 (stack76)
        %v118392 = vadd.s32 %v118384, %v118389 (stack65)
        %v118394 = vshll.u32 %v118389, 16 (stack73)
        %v118395 = vshrl.u32 %v118389, 16 (stack74)
        %v118396 = vor.u32 %v118394, %v118395 (stack75)
        %v118397 = vxor.u32 %v118392, %v118396 (stack76)
        %v118400 = vadd.s32 %v118392, %v118397 (stack65)
        %v118404 = vadd.s32 %v118400, %v8 (stack65)
        %v118406 = vshll.u32 %v118397, 24 (stack73)
        %v118407 = vshrl.u32 %v118397, 8 (stack74)
        %v118408 = vor.u32 %v118406, %v118407 (stack75)
        %v118409 = vxor.u32 %v118400, %v118408 (stack76)
        %v118412 = vadd.s32 %v118409, %v10 (stack65)
        %v118416 = vadd.s32 %v118412, 2 (stack65)
        %v118420 = vadd.s32 %v118404, %v118416 (stack65)
        %v118422 = vshll.u32 %v118416, 13 (stack73)
        %v118423 = vshrl.u32 %v118416, 19 (stack74)
        %v118424 = vor.u32 %v118422, %v118423 (stack75)
        %v118425 = vxor.u32 %v118420, %v118424 (stack76)
        %v118428 = vadd.s32 %v118420, %v118425 (stack65)
        %v118430 = vshll.u32 %v118425, 15 (stack73)
        %v118431 = vshrl.u32 %v118425, 17 (stack74)
        %v118432 = vor.u32 %v118430, %v118431 (stack75)
        %v118433 = vxor.u32 %v118428, %v118432 (stack76)
        %v118436 = vadd.s32 %v118428, %v118433 (stack65)
        %v118438 = vshll.u32 %v118433, 26 (stack73)
        %v118439 = vshrl.u32 %v118433, 6 (stack74)
        %v118440 = vor.u32 %v118438, %v118439 (stack75)
        %v118441 = vxor.u32 %v118436, %v118440 (stack76)
        %v118444 = vadd.s32 %v118436, %v118441 (stack65)
        %v118448 = vadd.s32 %v118444, %v10 (stack65)
        %v118450 = vshll.u32 %v118441, 6 (stack73)
        %v118451 = vshrl.u32 %v118441, 26 (stack74)
        %v118452 = vor.u32 %v118450, %v118451 (stack75)
        %v118453 = vxor.u32 %v118444, %v118452 (stack76)
        %v118456 = vadd.s32 %v118453, %v9 (stack65)
        %v118460 = vadd.s32 %v118456, 3 (stack65)
        %v118464 = vadd.s32 %v118448, %v118460 (stack65)
        %v118466 = vshll.u32 %v118460, 17 (stack73)
        %v118467 = vshrl.u32 %v118460, 15 (stack74)
        %v118468 = vor.u32 %v118466, %v118467 (stack75)
        %v118469 = vxor.u32 %v118464, %v118468 (stack76)
        %v118472 = vadd.s32 %v118464, %v118469 (stack65)
        %v118474 = vshll.u32 %v118469, 29 (stack73)
        %v118475 = vshrl.u32 %v118469, 3 (stack74)
        %v118476 = vor.u32 %v118474, %v118475 (stack75)
        %v118477 = vxor.u32 %v118472, %v118476 (stack76)
        %v118480 = vadd.s32 %v118472, %v118477 (stack65)
        %v118482 = vshll.u32 %v118477, 16 (stack73)
        %v118483 = vshrl.u32 %v118477, 16 (stack74)
        %v118484 = vor.u32 %v118482, %v118483 (stack75)
        %v118485 = vxor.u32 %v118480, %v118484 (stack76)
        %v118488 = vadd.s32 %v118480, %v118485 (stack65)
        %v118492 = vadd.s32 %v118488, %v9 (stack65)
        %v118494 = vshll.u32 %v118485, 24 (stack73)
        %v118495 = vshrl.u32 %v118485, 8 (stack74)
        %v118496 = vor.u32 %v118494, %v118495 (stack75)
        %v118497 = vxor.u32 %v118488, %v118496 (stack76)
        %v118500 = vadd.s32 %v118497, %v8 (stack65)
        %v118504 = vadd.s32 %v118500, 4 (stack65)
        %v118508 = vadd.s32 %v118492, %v118504 (stack65)
        %v118510 = vshll.u32 %v118504, 13 (stack73)
        %v118511 = vshrl.u32 %v118504, 19 (stack74)
        %v118512 = vor.u32 %v118510, %v118511 (stack75)
        %v118513 = vxor.u32 %v118508, %v118512 (stack76)
        %v118516 = vadd.s32 %v118508, %v118513 (stack65)
        %v118518 = vshll.u32 %v118513, 15 (stack73)
        %v118519 = vshrl.u32 %v118513, 17 (stack74)
        %v118520 = vor.u32 %v118518, %v118519 (stack75)
        %v118521 = vxor.u32 %v118516, %v118520 (stack76)
        %v118524 = vadd.s32 %v118516, %v118521 (stack65)
        %v118526 = vshll.u32 %v118521, 26 (stack73)
        %v118527 = vshrl.u32 %v118521, 6 (stack74)
        %v118528 = vor.u32 %v118526, %v118527 (stack75)
        %v118529 = vxor.u32 %v118524, %v118528 (stack76)
        %v118532 = vadd.s32 %v118524, %v118529 (stack65)
        %v118536 = vadd.s32 %v118532, %v8 (stack65)
        %v118538 = vshll.u32 %v118529, 6 (stack73)
        %v118539 = vshrl.u32 %v118529, 26 (stack74)
        %v118540 = vor.u32 %v118538, %v118539 (stack75)
        %v118541 = vxor.u32 %v118532, %v118540 (stack76)
        %v118544 = vadd.s32 %v118541, %v10 (stack65)
        %v118548 = vadd.s32 %v118544, 5 (stack65)
        %v118550 = vxor.u32 %v118536, %v118548 (stack76)
        %v118551 = vand.u32.u8 %v118550, 255 (stack77)
        %v118552 = vand.u32 %v118551, 65535 (stack78)
        %v118553 = vshrl.u32 %v118552, 1 (stack79)
        %v118554 = vor.u32 %v118553, 16256 (stack75)
        %v118555 = vand.u32.u16 %v118554, 65535 (stack80)
        %v118556 = vunpack.i.l.bf16 %v118555 (stack81)
        %v118560 = vadd.f32 %v118556, -1.0 (stack82)
        %v118564 = vmul.f32 %v118560, 2.0 (stack83)
        %v118568 = vadd.f32 %v118564, -0.99609375 (stack82)
        %v118572 = vmax.f32 -0.99609375, %v118568 (stack84)
        %v118574 = vand.u32 2147483647, %v118572 (stack85)
        %vm118577 = vcmp.eq.f32.partialorder %v118574, 1.0 (stack86)
        %v118582 = vmul.f32 %v118572, inf (stack83)
        %v118584 = vxor.u32 %v118572, 2147483648 (stack87)
        %v118587 = vmul.f32 %v118572, %v118584 (stack83)
        %v118589 = vadd.f32 %v118587, 1.0 (stack88)
        %v118590 = vlog2.pop %v118589 (stack89)
        %v118591 = vmul.f32 %v118590, 0.6931472 (stack90)
        %v118592 = vmul.f32 -0.5, %v118587 (stack91)
        %v118593 = vadd.f32 %v118592, 1.0 (stack92)
        %v118594 = vmul.f32 %v118593, %v118587 (stack93)
        %v118595 = vand.u32 2147483647, %v118587 (stack94)
        %vm118596 = vcmp.lt.f32.partialorder %v118595, 0.0004427343 (stack95)
        %v118597 = vsel /*vm=*/%vm118596, /*on_true_vy=*/%v118594, /*on_false_vx=*/%v118591 (stack96)
        %v118598 = vxor.u32 %v118597, 2147483648 (stack87)
        %vm118601 = vcmp.lt.f32.partialorder %v118598, 5.0 (stack86)
        %v118606 = vsel /*vm=*/%vm118601, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v118610 = vsel /*vm=*/%vm118601, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v118614 = vsel /*vm=*/%vm118601, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v118618 = vsel /*vm=*/%vm118601, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v118622 = vsel /*vm=*/%vm118601, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v118626 = vsel /*vm=*/%vm118601, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v118630 = vsel /*vm=*/%vm118601, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v118634 = vsel /*vm=*/%vm118601, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v118638 = vsel /*vm=*/%vm118601, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v118642 = vadd.f32 %v118598, -2.5 (stack82)
        %v118644 = vrsqrt.pop %v118598 (stack97)
        %v118645 = vmul.f32 %v118598, %v118644 (stack98)
        %vm118646 = vcmp.eq.f32.partialorder %v118598, inf (stack99)
        %v118647 = vsel /*vm=*/%vm118646, /*on_true_vy=*/%v118598, /*on_false_vx=*/%v118645 (stack100)
        %vm118648 = vcmp.eq.f32.partialorder %v118598, 0.0 (stack101)
        %v118649 = vand.u32 %v118598, 2147483648 (stack102)
        %v118650 = vsel /*vm=*/%vm118648, /*on_true_vy=*/%v118649, /*on_false_vx=*/%v118647 (stack103)
        %v118653 = vadd.f32 %v118650, -3.0 (stack82)
        %v118657 = vsel /*vm=*/%vm118601, /*on_true_vy=*/%v118642, /*on_false_vx=*/%v118653 (stack72)
        %v118661 = vmul.f32 %v118638, %v118657 (stack83)
        %v118665 = vadd.f32 %v118634, %v118661 (stack82)
        %v118669 = vmul.f32 %v118665, %v118657 (stack83)
        %v118673 = vadd.f32 %v118630, %v118669 (stack82)
        %v118677 = vmul.f32 %v118673, %v118657 (stack83)
        %v118681 = vadd.f32 %v118626, %v118677 (stack82)
        %v118685 = vmul.f32 %v118681, %v118657 (stack83)
        %v118689 = vadd.f32 %v118622, %v118685 (stack82)
        %v118693 = vmul.f32 %v118689, %v118657 (stack83)
        %v118697 = vadd.f32 %v118618, %v118693 (stack82)
        %v118701 = vmul.f32 %v118697, %v118657 (stack83)
        %v118705 = vadd.f32 %v118614, %v118701 (stack82)
        %v118709 = vmul.f32 %v118705, %v118657 (stack83)
        %v118713 = vadd.f32 %v118610, %v118709 (stack82)
        %v118717 = vmul.f32 %v118713, %v118657 (stack83)
        %v118721 = vadd.f32 %v118606, %v118717 (stack82)
        %v118725 = vmul.f32 %v118721, %v118572 (stack83)
        %v118729 = vsel /*vm=*/%vm118577, /*on_true_vy=*/%v118582, /*on_false_vx=*/%v118725 (stack72)
        %v118733 = vmul.f32 %v118729, 1.4140625 (stack83)
        %s118735 = scalar_lea.vmem %s280, 764 [#allocation0] (stack107)
        %v118736 = vpack.c.bf16 0.0, %v118733 (stack104)
        %118737 = vst [vmem:[%s118735] sm:$0xf] /*vst_source=*/%v118736 (stack105)
        %v118740 = vadd.s32 %v3329, %v115971 (stack65)
        %s118742 = smul.u32 128, %s27 (stack66)
        %v118743 = vlaneseq (stack67)
        %v118744 = vand.u32 %v118743, 127 (stack68)
        %v118745 = vstv %s118742 (stack69)
        %v118746 = vadd.s32 %v118744, %v118745 (stack70)
        %v118750 = vadd.s32 %v118740, %v118746 (stack65)
        %vm118754 = vcmp.lt.u32.totalorder %v118750, %v118740 (stack71)
        %vm118759 = vcmp.lt.u32.totalorder %v118740, %v3329 (stack71)
        %v118764 = vadd.s32 %v3316, %v115954 (stack65)
        %v118768 = vadd.s32 %v118764, 1 (stack65)
        %v118772 = vsel /*vm=*/%vm118759, /*on_true_vy=*/%v118768, /*on_false_vx=*/%v118764 (stack72)
        %v118776 = vadd.s32 %v118772, 1 (stack65)
        %v118780 = vsel /*vm=*/%vm118754, /*on_true_vy=*/%v118776, /*on_false_vx=*/%v118772 (stack72)
        %v118785 = vadd.s32 %v118780, %v10 (stack65)
        %v118789 = vadd.s32 %v118750, %v9 (stack65)
        %v118793 = vadd.s32 %v118785, %v118789 (stack65)
        %v118795 = vshll.u32 %v118789, 13 (stack73)
        %v118796 = vshrl.u32 %v118789, 19 (stack74)
        %v118797 = vor.u32 %v118795, %v118796 (stack75)
        %v118798 = vxor.u32 %v118793, %v118797 (stack76)
        %v118801 = vadd.s32 %v118793, %v118798 (stack65)
        %v118803 = vshll.u32 %v118798, 15 (stack73)
        %v118804 = vshrl.u32 %v118798, 17 (stack74)
        %v118805 = vor.u32 %v118803, %v118804 (stack75)
        %v118806 = vxor.u32 %v118801, %v118805 (stack76)
        %v118809 = vadd.s32 %v118801, %v118806 (stack65)
        %v118811 = vshll.u32 %v118806, 26 (stack73)
        %v118812 = vshrl.u32 %v118806, 6 (stack74)
        %v118813 = vor.u32 %v118811, %v118812 (stack75)
        %v118814 = vxor.u32 %v118809, %v118813 (stack76)
        %v118817 = vadd.s32 %v118809, %v118814 (stack65)
        %v118821 = vadd.s32 %v118817, %v9 (stack65)
        %v118823 = vshll.u32 %v118814, 6 (stack73)
        %v118824 = vshrl.u32 %v118814, 26 (stack74)
        %v118825 = vor.u32 %v118823, %v118824 (stack75)
        %v118826 = vxor.u32 %v118817, %v118825 (stack76)
        %v118829 = vadd.s32 %v118826, %v8 (stack65)
        %v118833 = vadd.s32 %v118829, 1 (stack65)
        %v118837 = vadd.s32 %v118821, %v118833 (stack65)
        %v118839 = vshll.u32 %v118833, 17 (stack73)
        %v118840 = vshrl.u32 %v118833, 15 (stack74)
        %v118841 = vor.u32 %v118839, %v118840 (stack75)
        %v118842 = vxor.u32 %v118837, %v118841 (stack76)
        %v118845 = vadd.s32 %v118837, %v118842 (stack65)
        %v118847 = vshll.u32 %v118842, 29 (stack73)
        %v118848 = vshrl.u32 %v118842, 3 (stack74)
        %v118849 = vor.u32 %v118847, %v118848 (stack75)
        %v118850 = vxor.u32 %v118845, %v118849 (stack76)
        %v118853 = vadd.s32 %v118845, %v118850 (stack65)
        %v118855 = vshll.u32 %v118850, 16 (stack73)
        %v118856 = vshrl.u32 %v118850, 16 (stack74)
        %v118857 = vor.u32 %v118855, %v118856 (stack75)
        %v118858 = vxor.u32 %v118853, %v118857 (stack76)
        %v118861 = vadd.s32 %v118853, %v118858 (stack65)
        %v118865 = vadd.s32 %v118861, %v8 (stack65)
        %v118867 = vshll.u32 %v118858, 24 (stack73)
        %v118868 = vshrl.u32 %v118858, 8 (stack74)
        %v118869 = vor.u32 %v118867, %v118868 (stack75)
        %v118870 = vxor.u32 %v118861, %v118869 (stack76)
        %v118873 = vadd.s32 %v118870, %v10 (stack65)
        %v118877 = vadd.s32 %v118873, 2 (stack65)
        %v118881 = vadd.s32 %v118865, %v118877 (stack65)
        %v118883 = vshll.u32 %v118877, 13 (stack73)
        %v118884 = vshrl.u32 %v118877, 19 (stack74)
        %v118885 = vor.u32 %v118883, %v118884 (stack75)
        %v118886 = vxor.u32 %v118881, %v118885 (stack76)
        %v118889 = vadd.s32 %v118881, %v118886 (stack65)
        %v118891 = vshll.u32 %v118886, 15 (stack73)
        %v118892 = vshrl.u32 %v118886, 17 (stack74)
        %v118893 = vor.u32 %v118891, %v118892 (stack75)
        %v118894 = vxor.u32 %v118889, %v118893 (stack76)
        %v118897 = vadd.s32 %v118889, %v118894 (stack65)
        %v118899 = vshll.u32 %v118894, 26 (stack73)
        %v118900 = vshrl.u32 %v118894, 6 (stack74)
        %v118901 = vor.u32 %v118899, %v118900 (stack75)
        %v118902 = vxor.u32 %v118897, %v118901 (stack76)
        %v118905 = vadd.s32 %v118897, %v118902 (stack65)
        %v118909 = vadd.s32 %v118905, %v10 (stack65)
        %v118911 = vshll.u32 %v118902, 6 (stack73)
        %v118912 = vshrl.u32 %v118902, 26 (stack74)
        %v118913 = vor.u32 %v118911, %v118912 (stack75)
        %v118914 = vxor.u32 %v118905, %v118913 (stack76)
        %v118917 = vadd.s32 %v118914, %v9 (stack65)
        %v118921 = vadd.s32 %v118917, 3 (stack65)
        %v118925 = vadd.s32 %v118909, %v118921 (stack65)
        %v118927 = vshll.u32 %v118921, 17 (stack73)
        %v118928 = vshrl.u32 %v118921, 15 (stack74)
        %v118929 = vor.u32 %v118927, %v118928 (stack75)
        %v118930 = vxor.u32 %v118925, %v118929 (stack76)
        %v118933 = vadd.s32 %v118925, %v118930 (stack65)
        %v118935 = vshll.u32 %v118930, 29 (stack73)
        %v118936 = vshrl.u32 %v118930, 3 (stack74)
        %v118937 = vor.u32 %v118935, %v118936 (stack75)
        %v118938 = vxor.u32 %v118933, %v118937 (stack76)
        %v118941 = vadd.s32 %v118933, %v118938 (stack65)
        %v118943 = vshll.u32 %v118938, 16 (stack73)
        %v118944 = vshrl.u32 %v118938, 16 (stack74)
        %v118945 = vor.u32 %v118943, %v118944 (stack75)
        %v118946 = vxor.u32 %v118941, %v118945 (stack76)
        %v118949 = vadd.s32 %v118941, %v118946 (stack65)
        %v118953 = vadd.s32 %v118949, %v9 (stack65)
        %v118955 = vshll.u32 %v118946, 24 (stack73)
        %v118956 = vshrl.u32 %v118946, 8 (stack74)
        %v118957 = vor.u32 %v118955, %v118956 (stack75)
        %v118958 = vxor.u32 %v118949, %v118957 (stack76)
        %v118961 = vadd.s32 %v118958, %v8 (stack65)
        %v118965 = vadd.s32 %v118961, 4 (stack65)
        %v118969 = vadd.s32 %v118953, %v118965 (stack65)
        %v118971 = vshll.u32 %v118965, 13 (stack73)
        %v118972 = vshrl.u32 %v118965, 19 (stack74)
        %v118973 = vor.u32 %v118971, %v118972 (stack75)
        %v118974 = vxor.u32 %v118969, %v118973 (stack76)
        %v118977 = vadd.s32 %v118969, %v118974 (stack65)
        %v118979 = vshll.u32 %v118974, 15 (stack73)
        %v118980 = vshrl.u32 %v118974, 17 (stack74)
        %v118981 = vor.u32 %v118979, %v118980 (stack75)
        %v118982 = vxor.u32 %v118977, %v118981 (stack76)
        %v118985 = vadd.s32 %v118977, %v118982 (stack65)
        %v118987 = vshll.u32 %v118982, 26 (stack73)
        %v118988 = vshrl.u32 %v118982, 6 (stack74)
        %v118989 = vor.u32 %v118987, %v118988 (stack75)
        %v118990 = vxor.u32 %v118985, %v118989 (stack76)
        %v118993 = vadd.s32 %v118985, %v118990 (stack65)
        %v118997 = vadd.s32 %v118993, %v8 (stack65)
        %v118999 = vshll.u32 %v118990, 6 (stack73)
        %v119000 = vshrl.u32 %v118990, 26 (stack74)
        %v119001 = vor.u32 %v118999, %v119000 (stack75)
        %v119002 = vxor.u32 %v118993, %v119001 (stack76)
        %v119005 = vadd.s32 %v119002, %v10 (stack65)
        %v119009 = vadd.s32 %v119005, 5 (stack65)
        %v119011 = vxor.u32 %v118997, %v119009 (stack76)
        %v119012 = vand.u32.u8 %v119011, 255 (stack77)
        %v119013 = vand.u32 %v119012, 65535 (stack78)
        %v119014 = vshrl.u32 %v119013, 1 (stack79)
        %v119015 = vor.u32 %v119014, 16256 (stack75)
        %v119016 = vand.u32.u16 %v119015, 65535 (stack80)
        %v119017 = vunpack.i.l.bf16 %v119016 (stack81)
        %v119021 = vadd.f32 %v119017, -1.0 (stack82)
        %v119025 = vmul.f32 %v119021, 2.0 (stack83)
        %v119029 = vadd.f32 %v119025, -0.99609375 (stack82)
        %v119033 = vmax.f32 -0.99609375, %v119029 (stack84)
        %v119035 = vand.u32 2147483647, %v119033 (stack85)
        %vm119038 = vcmp.eq.f32.partialorder %v119035, 1.0 (stack86)
        %v119043 = vmul.f32 %v119033, inf (stack83)
        %v119045 = vxor.u32 %v119033, 2147483648 (stack87)
        %v119048 = vmul.f32 %v119033, %v119045 (stack83)
        %v119050 = vadd.f32 %v119048, 1.0 (stack88)
        %v119051 = vlog2.pop %v119050 (stack89)
        %v119052 = vmul.f32 %v119051, 0.6931472 (stack90)
        %v119053 = vmul.f32 -0.5, %v119048 (stack91)
        %v119054 = vadd.f32 %v119053, 1.0 (stack92)
        %v119055 = vmul.f32 %v119054, %v119048 (stack93)
        %v119056 = vand.u32 2147483647, %v119048 (stack94)
        %vm119057 = vcmp.lt.f32.partialorder %v119056, 0.0004427343 (stack95)
        %v119058 = vsel /*vm=*/%vm119057, /*on_true_vy=*/%v119055, /*on_false_vx=*/%v119052 (stack96)
        %v119059 = vxor.u32 %v119058, 2147483648 (stack87)
        %vm119062 = vcmp.lt.f32.partialorder %v119059, 5.0 (stack86)
        %v119067 = vsel /*vm=*/%vm119062, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v119071 = vsel /*vm=*/%vm119062, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v119075 = vsel /*vm=*/%vm119062, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v119079 = vsel /*vm=*/%vm119062, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v119083 = vsel /*vm=*/%vm119062, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v119087 = vsel /*vm=*/%vm119062, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v119091 = vsel /*vm=*/%vm119062, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v119095 = vsel /*vm=*/%vm119062, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v119099 = vsel /*vm=*/%vm119062, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v119103 = vadd.f32 %v119059, -2.5 (stack82)
        %v119105 = vrsqrt.pop %v119059 (stack97)
        %v119106 = vmul.f32 %v119059, %v119105 (stack98)
        %vm119107 = vcmp.eq.f32.partialorder %v119059, inf (stack99)
        %v119108 = vsel /*vm=*/%vm119107, /*on_true_vy=*/%v119059, /*on_false_vx=*/%v119106 (stack100)
        %vm119109 = vcmp.eq.f32.partialorder %v119059, 0.0 (stack101)
        %v119110 = vand.u32 %v119059, 2147483648 (stack102)
        %v119111 = vsel /*vm=*/%vm119109, /*on_true_vy=*/%v119110, /*on_false_vx=*/%v119108 (stack103)
        %v119114 = vadd.f32 %v119111, -3.0 (stack82)
        %v119118 = vsel /*vm=*/%vm119062, /*on_true_vy=*/%v119103, /*on_false_vx=*/%v119114 (stack72)
        %v119122 = vmul.f32 %v119099, %v119118 (stack83)
        %v119126 = vadd.f32 %v119095, %v119122 (stack82)
        %v119130 = vmul.f32 %v119126, %v119118 (stack83)
        %v119134 = vadd.f32 %v119091, %v119130 (stack82)
        %v119138 = vmul.f32 %v119134, %v119118 (stack83)
        %v119142 = vadd.f32 %v119087, %v119138 (stack82)
        %v119146 = vmul.f32 %v119142, %v119118 (stack83)
        %v119150 = vadd.f32 %v119083, %v119146 (stack82)
        %v119154 = vmul.f32 %v119150, %v119118 (stack83)
        %v119158 = vadd.f32 %v119079, %v119154 (stack82)
        %v119162 = vmul.f32 %v119158, %v119118 (stack83)
        %v119166 = vadd.f32 %v119075, %v119162 (stack82)
        %v119170 = vmul.f32 %v119166, %v119118 (stack83)
        %v119174 = vadd.f32 %v119071, %v119170 (stack82)
        %v119178 = vmul.f32 %v119174, %v119118 (stack83)
        %v119182 = vadd.f32 %v119067, %v119178 (stack82)
        %v119186 = vmul.f32 %v119182, %v119033 (stack83)
        %v119190 = vsel /*vm=*/%vm119038, /*on_true_vy=*/%v119043, /*on_false_vx=*/%v119186 (stack72)
        %v119194 = vmul.f32 %v119190, 1.4140625 (stack83)
        %s119196 = scalar_lea.vmem %s280, 892 [#allocation0] (stack107)
        %v119197 = vpack.c.bf16 0.0, %v119194 (stack104)
        %119198 = vst [vmem:[%s119196] sm:$0xf] /*vst_source=*/%v119197 (stack105)
        %v119201 = vadd.s32 %v3816, %v115971 (stack65)
        %s119203 = smul.u32 128, %s27 (stack66)
        %v119204 = vlaneseq (stack67)
        %v119205 = vand.u32 %v119204, 127 (stack68)
        %v119206 = vstv %s119203 (stack69)
        %v119207 = vadd.s32 %v119205, %v119206 (stack70)
        %v119211 = vadd.s32 %v119201, %v119207 (stack65)
        %vm119215 = vcmp.lt.u32.totalorder %v119211, %v119201 (stack71)
        %vm119220 = vcmp.lt.u32.totalorder %v119201, %v3816 (stack71)
        %v119225 = vadd.s32 %v3803, %v115954 (stack65)
        %v119229 = vadd.s32 %v119225, 1 (stack65)
        %v119233 = vsel /*vm=*/%vm119220, /*on_true_vy=*/%v119229, /*on_false_vx=*/%v119225 (stack72)
        %v119237 = vadd.s32 %v119233, 1 (stack65)
        %v119241 = vsel /*vm=*/%vm119215, /*on_true_vy=*/%v119237, /*on_false_vx=*/%v119233 (stack72)
        %v119246 = vadd.s32 %v119241, %v10 (stack65)
        %v119250 = vadd.s32 %v119211, %v9 (stack65)
        %v119254 = vadd.s32 %v119246, %v119250 (stack65)
        %v119256 = vshll.u32 %v119250, 13 (stack73)
        %v119257 = vshrl.u32 %v119250, 19 (stack74)
        %v119258 = vor.u32 %v119256, %v119257 (stack75)
        %v119259 = vxor.u32 %v119254, %v119258 (stack76)
        %v119262 = vadd.s32 %v119254, %v119259 (stack65)
        %v119264 = vshll.u32 %v119259, 15 (stack73)
        %v119265 = vshrl.u32 %v119259, 17 (stack74)
        %v119266 = vor.u32 %v119264, %v119265 (stack75)
        %v119267 = vxor.u32 %v119262, %v119266 (stack76)
        %v119270 = vadd.s32 %v119262, %v119267 (stack65)
        %v119272 = vshll.u32 %v119267, 26 (stack73)
        %v119273 = vshrl.u32 %v119267, 6 (stack74)
        %v119274 = vor.u32 %v119272, %v119273 (stack75)
        %v119275 = vxor.u32 %v119270, %v119274 (stack76)
        %v119278 = vadd.s32 %v119270, %v119275 (stack65)
        %v119282 = vadd.s32 %v119278, %v9 (stack65)
        %v119284 = vshll.u32 %v119275, 6 (stack73)
        %v119285 = vshrl.u32 %v119275, 26 (stack74)
        %v119286 = vor.u32 %v119284, %v119285 (stack75)
        %v119287 = vxor.u32 %v119278, %v119286 (stack76)
        %v119290 = vadd.s32 %v119287, %v8 (stack65)
        %v119294 = vadd.s32 %v119290, 1 (stack65)
        %v119298 = vadd.s32 %v119282, %v119294 (stack65)
        %v119300 = vshll.u32 %v119294, 17 (stack73)
        %v119301 = vshrl.u32 %v119294, 15 (stack74)
        %v119302 = vor.u32 %v119300, %v119301 (stack75)
        %v119303 = vxor.u32 %v119298, %v119302 (stack76)
        %v119306 = vadd.s32 %v119298, %v119303 (stack65)
        %v119308 = vshll.u32 %v119303, 29 (stack73)
        %v119309 = vshrl.u32 %v119303, 3 (stack74)
        %v119310 = vor.u32 %v119308, %v119309 (stack75)
        %v119311 = vxor.u32 %v119306, %v119310 (stack76)
        %v119314 = vadd.s32 %v119306, %v119311 (stack65)
        %v119316 = vshll.u32 %v119311, 16 (stack73)
        %v119317 = vshrl.u32 %v119311, 16 (stack74)
        %v119318 = vor.u32 %v119316, %v119317 (stack75)
        %v119319 = vxor.u32 %v119314, %v119318 (stack76)
        %v119322 = vadd.s32 %v119314, %v119319 (stack65)
        %v119326 = vadd.s32 %v119322, %v8 (stack65)
        %v119328 = vshll.u32 %v119319, 24 (stack73)
        %v119329 = vshrl.u32 %v119319, 8 (stack74)
        %v119330 = vor.u32 %v119328, %v119329 (stack75)
        %v119331 = vxor.u32 %v119322, %v119330 (stack76)
        %v119334 = vadd.s32 %v119331, %v10 (stack65)
        %v119338 = vadd.s32 %v119334, 2 (stack65)
        %v119342 = vadd.s32 %v119326, %v119338 (stack65)
        %v119344 = vshll.u32 %v119338, 13 (stack73)
        %v119345 = vshrl.u32 %v119338, 19 (stack74)
        %v119346 = vor.u32 %v119344, %v119345 (stack75)
        %v119347 = vxor.u32 %v119342, %v119346 (stack76)
        %v119350 = vadd.s32 %v119342, %v119347 (stack65)
        %v119352 = vshll.u32 %v119347, 15 (stack73)
        %v119353 = vshrl.u32 %v119347, 17 (stack74)
        %v119354 = vor.u32 %v119352, %v119353 (stack75)
        %v119355 = vxor.u32 %v119350, %v119354 (stack76)
        %v119358 = vadd.s32 %v119350, %v119355 (stack65)
        %v119360 = vshll.u32 %v119355, 26 (stack73)
        %v119361 = vshrl.u32 %v119355, 6 (stack74)
        %v119362 = vor.u32 %v119360, %v119361 (stack75)
        %v119363 = vxor.u32 %v119358, %v119362 (stack76)
        %v119366 = vadd.s32 %v119358, %v119363 (stack65)
        %v119370 = vadd.s32 %v119366, %v10 (stack65)
        %v119372 = vshll.u32 %v119363, 6 (stack73)
        %v119373 = vshrl.u32 %v119363, 26 (stack74)
        %v119374 = vor.u32 %v119372, %v119373 (stack75)
        %v119375 = vxor.u32 %v119366, %v119374 (stack76)
        %v119378 = vadd.s32 %v119375, %v9 (stack65)
        %v119382 = vadd.s32 %v119378, 3 (stack65)
        %v119386 = vadd.s32 %v119370, %v119382 (stack65)
        %v119388 = vshll.u32 %v119382, 17 (stack73)
        %v119389 = vshrl.u32 %v119382, 15 (stack74)
        %v119390 = vor.u32 %v119388, %v119389 (stack75)
        %v119391 = vxor.u32 %v119386, %v119390 (stack76)
        %v119394 = vadd.s32 %v119386, %v119391 (stack65)
        %v119396 = vshll.u32 %v119391, 29 (stack73)
        %v119397 = vshrl.u32 %v119391, 3 (stack74)
        %v119398 = vor.u32 %v119396, %v119397 (stack75)
        %v119399 = vxor.u32 %v119394, %v119398 (stack76)
        %v119402 = vadd.s32 %v119394, %v119399 (stack65)
        %v119404 = vshll.u32 %v119399, 16 (stack73)
        %v119405 = vshrl.u32 %v119399, 16 (stack74)
        %v119406 = vor.u32 %v119404, %v119405 (stack75)
        %v119407 = vxor.u32 %v119402, %v119406 (stack76)
        %v119410 = vadd.s32 %v119402, %v119407 (stack65)
        %v119414 = vadd.s32 %v119410, %v9 (stack65)
        %v119416 = vshll.u32 %v119407, 24 (stack73)
        %v119417 = vshrl.u32 %v119407, 8 (stack74)
        %v119418 = vor.u32 %v119416, %v119417 (stack75)
        %v119419 = vxor.u32 %v119410, %v119418 (stack76)
        %v119422 = vadd.s32 %v119419, %v8 (stack65)
        %v119426 = vadd.s32 %v119422, 4 (stack65)
        %v119430 = vadd.s32 %v119414, %v119426 (stack65)
        %v119432 = vshll.u32 %v119426, 13 (stack73)
        %v119433 = vshrl.u32 %v119426, 19 (stack74)
        %v119434 = vor.u32 %v119432, %v119433 (stack75)
        %v119435 = vxor.u32 %v119430, %v119434 (stack76)
        %v119438 = vadd.s32 %v119430, %v119435 (stack65)
        %v119440 = vshll.u32 %v119435, 15 (stack73)
        %v119441 = vshrl.u32 %v119435, 17 (stack74)
        %v119442 = vor.u32 %v119440, %v119441 (stack75)
        %v119443 = vxor.u32 %v119438, %v119442 (stack76)
        %v119446 = vadd.s32 %v119438, %v119443 (stack65)
        %v119448 = vshll.u32 %v119443, 26 (stack73)
        %v119449 = vshrl.u32 %v119443, 6 (stack74)
        %v119450 = vor.u32 %v119448, %v119449 (stack75)
        %v119451 = vxor.u32 %v119446, %v119450 (stack76)
        %v119454 = vadd.s32 %v119446, %v119451 (stack65)
        %v119458 = vadd.s32 %v119454, %v8 (stack65)
        %v119460 = vshll.u32 %v119451, 6 (stack73)
        %v119461 = vshrl.u32 %v119451, 26 (stack74)
        %v119462 = vor.u32 %v119460, %v119461 (stack75)
        %v119463 = vxor.u32 %v119454, %v119462 (stack76)
        %v119466 = vadd.s32 %v119463, %v10 (stack65)
        %v119470 = vadd.s32 %v119466, 5 (stack65)
        %v119472 = vxor.u32 %v119458, %v119470 (stack76)
        %v119473 = vand.u32.u8 %v119472, 255 (stack77)
        %v119474 = vand.u32 %v119473, 65535 (stack78)
        %v119475 = vshrl.u32 %v119474, 1 (stack79)
        %v119476 = vor.u32 %v119475, 16256 (stack75)
        %v119477 = vand.u32.u16 %v119476, 65535 (stack80)
        %v119478 = vunpack.i.l.bf16 %v119477 (stack81)
        %v119482 = vadd.f32 %v119478, -1.0 (stack82)
        %v119486 = vmul.f32 %v119482, 2.0 (stack83)
        %v119490 = vadd.f32 %v119486, -0.99609375 (stack82)
        %v119494 = vmax.f32 -0.99609375, %v119490 (stack84)
        %v119496 = vand.u32 2147483647, %v119494 (stack85)
        %vm119499 = vcmp.eq.f32.partialorder %v119496, 1.0 (stack86)
        %v119504 = vmul.f32 %v119494, inf (stack83)
        %v119506 = vxor.u32 %v119494, 2147483648 (stack87)
        %v119509 = vmul.f32 %v119494, %v119506 (stack83)
        %v119511 = vadd.f32 %v119509, 1.0 (stack88)
        %v119512 = vlog2.pop %v119511 (stack89)
        %v119513 = vmul.f32 %v119512, 0.6931472 (stack90)
        %v119514 = vmul.f32 -0.5, %v119509 (stack91)
        %v119515 = vadd.f32 %v119514, 1.0 (stack92)
        %v119516 = vmul.f32 %v119515, %v119509 (stack93)
        %v119517 = vand.u32 2147483647, %v119509 (stack94)
        %vm119518 = vcmp.lt.f32.partialorder %v119517, 0.0004427343 (stack95)
        %v119519 = vsel /*vm=*/%vm119518, /*on_true_vy=*/%v119516, /*on_false_vx=*/%v119513 (stack96)
        %v119520 = vxor.u32 %v119519, 2147483648 (stack87)
        %vm119523 = vcmp.lt.f32.partialorder %v119520, 5.0 (stack86)
        %v119528 = vsel /*vm=*/%vm119523, /*on_true_vy=*/1.5014094, /*on_false_vx=*/2.8329768 (stack72)
        %v119532 = vsel /*vm=*/%vm119523, /*on_true_vy=*/0.24664073, /*on_false_vx=*/1.001674 (stack72)
        %v119536 = vsel /*vm=*/%vm119523, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/0.0094388705 (stack72)
        %v119540 = vsel /*vm=*/%vm119523, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/-0.0076224613 (stack72)
        %v119544 = vsel /*vm=*/%vm119523, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/0.0057395077 (stack72)
        %v119548 = vsel /*vm=*/%vm119523, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/-0.0036734284 (stack72)
        %v119552 = vsel /*vm=*/%vm119523, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/0.0013493432 (stack72)
        %v119556 = vsel /*vm=*/%vm119523, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/0.00010095056 (stack72)
        %v119560 = vsel /*vm=*/%vm119523, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/-0.00020021426 (stack72)
        %v119564 = vadd.f32 %v119520, -2.5 (stack82)
        %v119566 = vrsqrt.pop %v119520 (stack97)
        %v119567 = vmul.f32 %v119520, %v119566 (stack98)
        %vm119568 = vcmp.eq.f32.partialorder %v119520, inf (stack99)
        %v119569 = vsel /*vm=*/%vm119568, /*on_true_vy=*/%v119520, /*on_false_vx=*/%v119567 (stack100)
        %vm119570 = vcmp.eq.f32.partialorder %v119520, 0.0 (stack101)
        %v119571 = vand.u32 %v119520, 2147483648 (stack102)
        %v119572 = vsel /*vm=*/%vm119570, /*on_true_vy=*/%v119571, /*on_false_vx=*/%v119569 (stack103)
        %v119575 = vadd.f32 %v119572, -3.0 (stack82)
        %v119579 = vsel /*vm=*/%vm119523, /*on_true_vy=*/%v119564, /*on_false_vx=*/%v119575 (stack72)
        %v119583 = vmul.f32 %v119560, %v119579 (stack83)
        %v119587 = vadd.f32 %v119556, %v119583 (stack82)
        %v119591 = vmul.f32 %v119587, %v119579 (stack83)
        %v119595 = vadd.f32 %v119552, %v119591 (stack82)
        %v119599 = vmul.f32 %v119595, %v119579 (stack83)
        %v119603 = vadd.f32 %v119548, %v119599 (stack82)
        %v119607 = vmul.f32 %v119603, %v119579 (stack83)
        %v119611 = vadd.f32 %v119544, %v119607 (stack82)
        %v119615 = vmul.f32 %v119611, %v119579 (stack83)
        %v119619 = vadd.f32 %v119540, %v119615 (stack82)
        %v119623 = vmul.f32 %v119619, %v119579 (stack83)
        %v119627 = vadd.f32 %v119536, %v119623 (stack82)
        %v119631 = vmul.f32 %v119627, %v119579 (stack83)
        %v119635 = vadd.f32 %v119532, %v119631 (stack82)
        %v119639 = vmul.f32 %v119635, %v119579 (stack83)
        %v119643 = vadd.f32 %v119528, %v119639 (stack82)
        %v119647 = vmul.f32 %v119643, %v119494 (stack83)
        %v119651 = vsel /*vm=*/%vm119499, /*on_true_vy=*/%v119504, /*on_false_vx=*/%v119647 (stack72)
        %v119655 = vmul.f32 %v119651, 1.4140625 (stack83)
        %s119657 = scalar_lea.vmem %s280, 1020 [#allocation0] (stack107)
        %v119658 = vpack.c.bf16 0.0, %v119655 (stack104)
        %119659 = vst [vmem:[%s119657] sm:$0xf] /*vst_source=*/%v119658 (stack105)
        %s119660 = sand.u32 %s237, 1 /* smod.u32 w/div 2 */ (stack108)
        %s119661 = scalar_lea.sflag [#allocation2], %s119660 (stack109)
        %s119662 = sand.u32 %s237, 1 /* smod.u32 w/div 2 */ (stack110)
        %s119663 = smul.addr %s119662, 1024 (stack111)
        %s119664 = scalar_lea.vmem [#allocation0], %s119663 (stack112)
        %s119665 = smul.u32 8, %s25 (stack113)
        %s119666 = smul.u32 32, %s26 (stack113)
        %s119668 = ssub.s32 16384, 16384 (stack114)
        %119669 = vsyncadd %s119661, %s119668 (stack115)
        %s119670 = sadd.s32 %s27, %s119666 (stack116)
        %s119671 = smul.addr %s119665, 256 (stack117)
        %s119672 = sadd.s32 %s119670, %s119671 (stack116)
        %s119673 = smul.addr %s119672, 64 (stack118)
        %s119674 = scalar_lea.hbm %s7, %s119673 (stack119)
        %s119675 = sshll.u32 %s119664, 4 (stack120)
        %s119676 = int_to_ptr.vmem [resolvable:$true] %s119675 (stack121)
        %119681 = dma.vmem_to_hbm [thread:$0]  /*vmem=*/%s119676, /*size_in_granules=*/16384, /*hbm=*/%s119674, /*dst_syncflagno=*/%s119661, /*src_stride=*/2048, /*dst_stride=*/16384, /*steps_per_stride=*/128 /* 
base_bounds: (8, 256, 1)
dynamic_base_bounds: (8, 256, 1)
window_bounds: (8, 32, 1)
iteration_bounds: (1, 8, 1)
strides: (8, 32, 1)
pad_low: (0, 0, 0)
pad_high: (0, 0, 0)
element_size_in_bytes: 2048 */ (stack122)
      $region36: #{fusion.21} parent=5 // pred_fallthru
        _
      %p119682 = scmp.le.s32.totalorder 2, %s15 (stack123)
      // Predicated region
      $region37: #{fusion.21} parent=5 // pred_check
        %p119683 = pneg %p119682 (stack124)
      $region38: #{fusion.21} parent=5 // pred_check_branch
        %119685 = sbr.rel (%p119683) target = $region40 (stack125)
      $region39: #{fusion.21} parent=5 // pred_region
        %s119686 = ssub.s32 %s15, 2 (stack126)
        %s119687 = sand.u32 %s119686, 1 /* smod.u32 w/div 2 */ (stack127)
        %s119688 = scalar_lea.sflag [#allocation2], %s119687 (stack128)
        %119692 = dma.done %s119688, 16384 /* pipeline-emitter-dma-wait */ (stack129)
      $region40: #{fusion.21} parent=5 // pred_fallthru
        _
    $region6: #{fusion.21} parent=1 // loop_footer
      %s19 = sadd.s32 1, %s15 (stack130)
    $region7: #{fusion.21} parent=1 // loop_footer_branch
      %14 = sbr.rel target = $region3 (stack131)
    $region8: #{fusion.21} parent=1 // loop_exit
      _
    %119693 = vsyncpa [#allocation2], 1 (stack132)
    %s119694 = scalar_lea.sflag [#allocation2], 1 (stack133)
    %119695 = vsyncpa %s119694, 1 (stack132)

stack0
    @     0x787f0f52271e  (unknown)
    @     0x787f0f5226ce  (unknown)
    @     0x787f0f526cec  (unknown)
    @     0x787f0f526fe5  (unknown)
    @     0x787f0b748189  (unknown)
    @     0x787f0b74581b  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack1
    @     0x787f0f4f5115  (unknown)
    @     0x787f0f557ab8  (unknown)
    @     0x787f09c9a1eb  (unknown)
    @     0x787f09c972c3  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack2
    @     0x787f0f4f5115  (unknown)
    @     0x787f0f557ab8  (unknown)
    @     0x787f09c9a1eb  (unknown)
    @     0x787f09c9733c  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack3
    @     0x787f0f4df370  (unknown)
    @     0x787f0f52b3aa  (unknown)
    @     0x787f0f315bce  (unknown)
    @     0x787f0b72ea4a  (unknown)
    @     0x787f0b74d53c  (unknown)
    @     0x787f0b74b642  (unknown)
    @     0x787f0b75005f  (unknown)
    @     0x787f0b74af97  (unknown)
    @     0x787f0b74e1fb  (unknown)
    @     0x787f0b74d898  (unknown)
    @     0x787f0b74511f  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack4
    @     0x787f0f52271e  (unknown)
    @     0x787f0f5226ce  (unknown)
    @     0x787f0f526cec  (unknown)
    @     0x787f0f32b5e0  (unknown)
    @     0x787f0f32485a  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack5
    @     0x787f0f52271e  (unknown)
    @     0x787f0f5226ce  (unknown)
    @     0x787f0f526cec  (unknown)
    @     0x787f0f527060  (unknown)
    @     0x787f0f32edb3  (unknown)
    @     0x787f0f32b263  (unknown)
    @     0x787f0f32485a  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack6
    @     0x787f0f52271e  (unknown)
    @     0x787f0f5226ce  (unknown)
    @     0x787f0f526cec  (unknown)
    @     0x787f0f527060  (unknown)
    @     0x787f0f32edef  (unknown)
    @     0x787f0f32b263  (unknown)
    @     0x787f0f32485a  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack7
    @     0x787f0f4ea266  (unknown)
    @     0x787f0f55872d  (unknown)
    @     0x787f0f324e23  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack8
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0f324e0e  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack9
    @     0x787f0f52a35f  (unknown)
    @     0x787f0f506669  (unknown)
    @     0x787f0f529dfb  (unknown)
    @     0x787f0f3274d3  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack10
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f52a459  (unknown)
    @     0x787f0f5066e6  (unknown)
    @     0x787f0f529dfb  (unknown)
    @     0x787f0f3274d3  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack11
    @     0x787f0f52a35f  (unknown)
    @     0x787f0f3228b8  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack12
    @     0x787f0f52a35f  (unknown)
    @     0x787f0f32315c  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack13
    @     0x787f0f52a35f  (unknown)
    @     0x787f0f55562f  (unknown)
    @     0x787f0f323235  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack14
    @     0x787f0f4f426f  (unknown)
    @     0x787f0f57a997  (unknown)
    @     0x787f0f506719  (unknown)
    @     0x787f0f529dfb  (unknown)
    @     0x787f0f3274d3  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack15
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53d2ca  (unknown)
    @     0x787f0f529008  (unknown)
    @     0x787f0f4828c4  (unknown)
    @     0x787f0f3229bc  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack16
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f52a459  (unknown)
    @     0x787f0f4828ec  (unknown)
    @     0x787f0f3229bc  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack17
    @     0x787f0f4e38b7  (unknown)
    @     0x787f0f532edc  (unknown)
    @     0x787f0f48290a  (unknown)
    @     0x787f0f3229bc  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack18
    @     0x787f0f4e38b7  (unknown)
    @     0x787f0f532edc  (unknown)
    @     0x787f0f4828d5  (unknown)
    @     0x787f0f3229bc  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack19
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53dd92  (unknown)
    @     0x787f0b767651  (unknown)
    @     0x787f0b7673a5  (unknown)
    @     0x787f0f323012  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack20
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f56964f  (unknown)
    @     0x787f0b767693  (unknown)
    @     0x787f0b7673a5  (unknown)
    @     0x787f0f323012  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack21
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53dd92  (unknown)
    @     0x787f0b767651  (unknown)
    @     0x787f0b7673a5  (unknown)
    @     0x787f0f32304b  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack22
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f56964f  (unknown)
    @     0x787f0b767693  (unknown)
    @     0x787f0b7673a5  (unknown)
    @     0x787f0f32304b  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack23
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53da6a  (unknown)
    @     0x787f0f5266c8  (unknown)
    @     0x787f0f3230dd  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack24
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f528533  (unknown)
    @     0x787f0f323116  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack25
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53d2ca  (unknown)
    @     0x787f0f529008  (unknown)
    @     0x787f0f3231aa  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack26
    @     0x787f0f4e38b7  (unknown)
    @     0x787f0f532edc  (unknown)
    @     0x787f0f3231bb  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack27
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f52f479  (unknown)
    @     0x787f0f3234bc  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack28
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f528533  (unknown)
    @     0x787f0f3234f4  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack29
    @     0x787f0f4de266  (unknown)
    @     0x787f0f52eb04  (unknown)
    @     0x787f0f323502  (unknown)
    @     0x787f0f327519  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack30
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f565cf9  (unknown)
    @     0x787f0f32e779  (unknown)
    @     0x787f0f32756f  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack31
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f52f4f9  (unknown)
    @     0x787f0f32e7f7  (unknown)
    @     0x787f0f32756f  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack32
    @     0x787f0f4de766  (unknown)
    @     0x787f0f52eb6d  (unknown)
    @     0x787f0f32e80b  (unknown)
    @     0x787f0f32756f  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack33
    @     0x787f0f4f426f  (unknown)
    @     0x787f0f57a997  (unknown)
    @     0x787f0f519b3c  (unknown)
    @     0x787f0f555576  (unknown)
    @     0x787f0f32e79f  (unknown)
    @     0x787f0f32756f  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack34
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f52f4f9  (unknown)
    @     0x787f0f32e7f7  (unknown)
    @     0x787f0f32789d  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack35
    @     0x787f0f4de55e  (unknown)
    @     0x787f0f52ebf2  (unknown)
    @     0x787f0f519b18  (unknown)
    @     0x787f0f555576  (unknown)
    @     0x787f0f32e79f  (unknown)
    @     0x787f0f32789d  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack36
    @     0x787f0f4f426f  (unknown)
    @     0x787f0f57a997  (unknown)
    @     0x787f0f519b3c  (unknown)
    @     0x787f0f555576  (unknown)
    @     0x787f0f32e79f  (unknown)
    @     0x787f0f32789d  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack37
    @     0x787f0f4de55e  (unknown)
    @     0x787f0f52ebf2  (unknown)
    @     0x787f0f519b18  (unknown)
    @     0x787f0f555576  (unknown)
    @     0x787f0f32bc71  (unknown)
    @     0x787f0f327a78  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack38
    @     0x787f0f4f426f  (unknown)
    @     0x787f0f57a997  (unknown)
    @     0x787f0f519b3c  (unknown)
    @     0x787f0f555576  (unknown)
    @     0x787f0f32bc71  (unknown)
    @     0x787f0f327a78  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack39
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f565cf9  (unknown)
    @     0x787f0f32e779  (unknown)
    @     0x787f0f327af8  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack40
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f52f4f9  (unknown)
    @     0x787f0f32e7f7  (unknown)
    @     0x787f0f327af8  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack41
    @     0x787f0f4de766  (unknown)
    @     0x787f0f52eb6d  (unknown)
    @     0x787f0f32e80b  (unknown)
    @     0x787f0f327af8  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack42
    @     0x787f0f4f426f  (unknown)
    @     0x787f0f57a997  (unknown)
    @     0x787f0f519b3c  (unknown)
    @     0x787f0f555576  (unknown)
    @     0x787f0f32e79f  (unknown)
    @     0x787f0f327af8  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack43
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53da6a  (unknown)
    @     0x787f0f5266c8  (unknown)
    @     0x787f0f327b27  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack44
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f530505  (unknown)
    @     0x787f0f32f97e  (unknown)
    @     0x787f0f32cdf1  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack45
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53dd92  (unknown)
    @     0x787f0f545f5c  (unknown)
    @     0x787f0f32fce7  (unknown)
    @     0x787f0f32cdf1  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack46
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0f32fce7  (unknown)
    @     0x787f0f32cdf1  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack47
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53dd92  (unknown)
    @     0x787f0b77d96c  (unknown)
    @     0x787f0b757234  (unknown)
    @     0x787f0b76e38a  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack48
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53dd92  (unknown)
    @     0x787f0b77da82  (unknown)
    @     0x787f0b757234  (unknown)
    @     0x787f0b76e38a  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack49
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f56964f  (unknown)
    @     0x787f0b77cd4f  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack50
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f567291  (unknown)
    @     0x787f0b77cf32  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack51
    @     0x787f0f4e38b7  (unknown)
    @     0x787f0f532edc  (unknown)
    @     0x787f0f5672bd  (unknown)
    @     0x787f0b77cf32  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack52
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f530505  (unknown)
    @     0x787f0b77cf49  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack53
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f56964f  (unknown)
    @     0x787f0f527cd6  (unknown)
    @     0x787f0b77d02b  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack54
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f530505  (unknown)
    @     0x787f0f527ce7  (unknown)
    @     0x787f0b77d02b  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack55
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53dd92  (unknown)
    @     0x787f0f545f5c  (unknown)
    @     0x787f0b77d0d6  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack56
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0b77d0d6  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack57
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0b77d1f8  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack58
    @     0x787f0f4ebbdf  (unknown)
    @     0x787f0f549909  (unknown)
    @     0x787f0b77d205  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack59
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53a225  (unknown)
    @     0x787f0f53a847  (unknown)
    @     0x787f0c4bf709  (unknown)
    @     0x787f0b77d329  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack60
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53a2f1  (unknown)
    @     0x787f0f53a847  (unknown)
    @     0x787f0c4bf709  (unknown)
    @     0x787f0b77d329  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack61
    @     0x787f0f4e307c  (unknown)
    @     0x787f0f53a85f  (unknown)
    @     0x787f0c4bf709  (unknown)
    @     0x787f0b77d329  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack62
    @     0x787f0f4e2e33  (unknown)
    @     0x787f0f539d88  (unknown)
    @     0x787f0c4bf716  (unknown)
    @     0x787f0b77d329  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack63
    @     0x787f0f4e307c  (unknown)
    @     0x787f0f53a85f  (unknown)
    @     0x787f0f312f7f  (unknown)
    @     0x787f0b77d284  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack64
    @     0x787f0f4e2e33  (unknown)
    @     0x787f0f539d88  (unknown)
    @     0x787f0f312f8c  (unknown)
    @     0x787f0b77d284  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack65
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f5691eb  (unknown)
    @     0x787f0c15ce44  (unknown)
    @     0x787f0f557bb7  (unknown)
    @     0x787f0c158ac3  (unknown)
    @     0x787f1136d73e  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack66
    @     0x787f0f4df554  (unknown)
    @     0x787f0f53e02e  (unknown)
    @     0x787f0f5292e6  (unknown)
    @     0x787f0b7637c7  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack67
    @     0x787f0f4de147  (unknown)
    @     0x787f0f52b798  (unknown)
    @     0x787f0f30a807  (unknown)
    @     0x787f0b7637de  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack68
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f52b807  (unknown)
    @     0x787f0f30a807  (unknown)
    @     0x787f0b7637de  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack69
    @     0x787f0f4df370  (unknown)
    @     0x787f0f52b3aa  (unknown)
    @     0x787f0b7637ec  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack70
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f52cc7e  (unknown)
    @     0x787f0b7637fa  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack71
    @     0x787f0f4e8df6  (unknown)
    @     0x787f0f568b6e  (unknown)
    @     0x787f0f56827a  (unknown)
    @     0x787f0c15bf00  (unknown)
    @     0x787f0f557bb7  (unknown)
    @     0x787f0c158ac3  (unknown)
    @     0x787f1136d73e  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack72
    @     0x787f0f4e7674  (unknown)
    @     0x787f0f52e22d  (unknown)
    @     0x787f0c15bb18  (unknown)
    @     0x787f0f557bb7  (unknown)
    @     0x787f0c158ac3  (unknown)
    @     0x787f1136d73e  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack73
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f52cb80  (unknown)
    @     0x787f0c157250  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack74
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f52c03e  (unknown)
    @     0x787f0c15759b  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack75
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f52d8e9  (unknown)
    @     0x787f0c156f76  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack76
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f565769  (unknown)
    @     0x787f0c157036  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack77
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f52b94b  (unknown)
    @     0x787f0c158583  (unknown)
    @     0x787f0c155364  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack78
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f52b94b  (unknown)
    @     0x787f0c157622  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack79
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f52c03e  (unknown)
    @     0x787f0c157640  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack80
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f52b94b  (unknown)
    @     0x787f0c158583  (unknown)
    @     0x787f0c155a1d  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack81
    @     0x787f0f4e111d  (unknown)
    @     0x787f0f5350eb  (unknown)
    @     0x787f0c155b03  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack82
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f564248  (unknown)
    @     0x787f0c15cdb6  (unknown)
    @     0x787f0f557bb7  (unknown)
    @     0x787f0c158ac3  (unknown)
    @     0x787f1136d73e  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack83
    @     0x787f0f4eb946  (unknown)
    @     0x787f0f534643  (unknown)
    @     0x787f0c15caf7  (unknown)
    @     0x787f0f557bb7  (unknown)
    @     0x787f0c158ac3  (unknown)
    @     0x787f1136d73e  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack84
    @     0x787f0f4e7a26  (unknown)
    @     0x787f0f565643  (unknown)
    @     0x787f0c15c675  (unknown)
    @     0x787f0f557bb7  (unknown)
    @     0x787f0c158ac3  (unknown)
    @     0x787f1136d73e  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack85
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f531cdf  (unknown)
    @     0x787f0c152d8e  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack86
    @     0x787f0f4e8df6  (unknown)
    @     0x787f0f568b6e  (unknown)
    @     0x787f0f56825c  (unknown)
    @     0x787f0c15bf00  (unknown)
    @     0x787f0f557bb7  (unknown)
    @     0x787f0c158ac3  (unknown)
    @     0x787f1136d73e  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack87
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f534cdd  (unknown)
    @     0x787f0c1523d0  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack88
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f563f30  (unknown)
    @     0x787f0f541ddc  (unknown)
    @     0x787f0c153eb3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack89
    @     0x787f0f4e46f5  (unknown)
    @     0x787f0f563375  (unknown)
    @     0x787f0f563f59  (unknown)
    @     0x787f0f541ddc  (unknown)
    @     0x787f0c153eb3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack90
    @     0x787f0f4eb946  (unknown)
    @     0x787f0f564143  (unknown)
    @     0x787f0f541ddc  (unknown)
    @     0x787f0c153eb3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack91
    @     0x787f0f4eb946  (unknown)
    @     0x787f0f563fcd  (unknown)
    @     0x787f0f541ddc  (unknown)
    @     0x787f0c153eb3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack92
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f56400b  (unknown)
    @     0x787f0f541ddc  (unknown)
    @     0x787f0c153eb3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack93
    @     0x787f0f4eb946  (unknown)
    @     0x787f0f564044  (unknown)
    @     0x787f0f541ddc  (unknown)
    @     0x787f0c153eb3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack94
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f56409b  (unknown)
    @     0x787f0f541ddc  (unknown)
    @     0x787f0c153eb3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack95
    @     0x787f0f4e8df6  (unknown)
    @     0x787f0f568b6e  (unknown)
    @     0x787f0f5640df  (unknown)
    @     0x787f0f541ddc  (unknown)
    @     0x787f0c153eb3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack96
    @     0x787f0f4e7674  (unknown)
    @     0x787f0f564112  (unknown)
    @     0x787f0f541ddc  (unknown)
    @     0x787f0c153eb3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack97
    @     0x787f0f4e46f5  (unknown)
    @     0x787f0f5645c5  (unknown)
    @     0x787f0f5432eb  (unknown)
    @     0x787f0c1565f3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack98
    @     0x787f0f4eb946  (unknown)
    @     0x787f0f54336b  (unknown)
    @     0x787f0c1565f3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack99
    @     0x787f0f4e8df6  (unknown)
    @     0x787f0f568b6e  (unknown)
    @     0x787f0f5433ac  (unknown)
    @     0x787f0c1565f3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack100
    @     0x787f0f4e7674  (unknown)
    @     0x787f0f5433de  (unknown)
    @     0x787f0c1565f3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack101
    @     0x787f0f4e8df6  (unknown)
    @     0x787f0f568b6e  (unknown)
    @     0x787f0f54341b  (unknown)
    @     0x787f0c1565f3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack102
    @     0x787f0f4e0085  (unknown)
    @     0x787f0f531bf5  (unknown)
    @     0x787f0f54343d  (unknown)
    @     0x787f0c1565f3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack103
    @     0x787f0f4e7674  (unknown)
    @     0x787f0f54346c  (unknown)
    @     0x787f0c1565f3  (unknown)
    @     0x787f1136d715  (unknown)
    @     0x787f0b754004  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack104
    @     0x787f0f4e0a1d  (unknown)
    @     0x787f0f570e15  (unknown)
    @     0x787f0f57137e  (unknown)
    @     0x787f0b785933  (unknown)
    @     0x787f0b784874  (unknown)
    @     0x787f0b784256  (unknown)
    @     0x787f0b754fe2  (unknown)
    @     0x787f0b76ea43  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack105
    @     0x787f0f4edfa8  (unknown)
    @     0x787f0f4edc1b  (unknown)
    @     0x787f0f577bfc  (unknown)
    @     0x787f0b783feb  (unknown)
    @     0x787f0b78594f  (unknown)
    @     0x787f0b784874  (unknown)
    @     0x787f0b784256  (unknown)
    @     0x787f0b754fe2  (unknown)
    @     0x787f0b76ea43  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack106
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53d2ca  (unknown)
    @     0x787f0f529008  (unknown)
    @     0x787f0b77cd3d  (unknown)
    @     0x787f0b75282d  (unknown)
    @     0x787f0b753d2d  (unknown)
    @     0x787f0b76e88d  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack107
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0f5459ae  (unknown)
    @     0x787f0f475227  (unknown)
    @     0x787f0b78420b  (unknown)
    @     0x787f0b754fe2  (unknown)
    @     0x787f0b76ea43  (unknown)
    @     0x787f0f333a09  (unknown)
    @     0x787f0f330612  (unknown)
    @     0x787f0f32d5bb  (unknown)
    @     0x787f0f327eff  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack108
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f530505  (unknown)
    @     0x787f0f32f91c  (unknown)
    @     0x787f0f32db2b  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack109
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0f32f95e  (unknown)
    @     0x787f0f32db2b  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack110
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f530505  (unknown)
    @     0x787f0f32f97e  (unknown)
    @     0x787f0f32db2b  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack111
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53dd92  (unknown)
    @     0x787f0f545f5c  (unknown)
    @     0x787f0f32fce7  (unknown)
    @     0x787f0f32db2b  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack112
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0f32fce7  (unknown)
    @     0x787f0f32db2b  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack113
    @     0x787f0f4df554  (unknown)
    @     0x787f0f53e02e  (unknown)
    @     0x787f0f5292e6  (unknown)
    @     0x787f0f470e7b  (unknown)
    @     0x787f0f471482  (unknown)
    @     0x787f0f334d9c  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack114
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53da6a  (unknown)
    @     0x787f0f5266c8  (unknown)
    @     0x787f0f53dbe7  (unknown)
    @     0x787f0f472ab8  (unknown)
    @     0x787f0f473b7f  (unknown)
    @     0x787f0f334dc9  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack115
    @     0x787f0f4e9ccd  (unknown)
    @     0x787f0f559af0  (unknown)
    @     0x787f0f472acc  (unknown)
    @     0x787f0f473b7f  (unknown)
    @     0x787f0f334dc9  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack116
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53d2ca  (unknown)
    @     0x787f0f529008  (unknown)
    @     0x787f0f482324  (unknown)
    @     0x787f0f47254a  (unknown)
    @     0x787f0f472c0b  (unknown)
    @     0x787f0f473b7f  (unknown)
    @     0x787f0f334dc9  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack117
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53dd92  (unknown)
    @     0x787f0f4825a3  (unknown)
    @     0x787f0f47254a  (unknown)
    @     0x787f0f472c0b  (unknown)
    @     0x787f0f473b7f  (unknown)
    @     0x787f0f334dc9  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack118
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53dd92  (unknown)
    @     0x787f0f545f5c  (unknown)
    @     0x787f0f4725c5  (unknown)
    @     0x787f0f472c0b  (unknown)
    @     0x787f0f473b7f  (unknown)
    @     0x787f0f334dc9  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack119
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0f4725c5  (unknown)
    @     0x787f0f472c0b  (unknown)
    @     0x787f0f473b7f  (unknown)
    @     0x787f0f334dc9  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack120
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f555df0  (unknown)
    @     0x787f0f54cbc7  (unknown)
    @     0x787f0f54f8ab  (unknown)
    @     0x787f0f479fa4  (unknown)
    @     0x787f0f47b136  (unknown)
    @     0x787f0f4737ce  (unknown)
    @     0x787f0f473b7f  (unknown)
    @     0x787f0f334dc9  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack121
    @     0x787f0f4df9d1  (unknown)
    @     0x787f0f52763d  (unknown)
    @     0x787f0f54cbd7  (unknown)
    @     0x787f0f54f8ab  (unknown)
    @     0x787f0f479fa4  (unknown)
    @     0x787f0f47b136  (unknown)
    @     0x787f0f4737ce  (unknown)
    @     0x787f0f473b7f  (unknown)
    @     0x787f0f334dc9  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack122
    @     0x787f0f4f13ba  (unknown)
    @     0x787f0f54fbb4  (unknown)
    @     0x787f0f479fa4  (unknown)
    @     0x787f0f47b136  (unknown)
    @     0x787f0f4737ce  (unknown)
    @     0x787f0f473b7f  (unknown)
    @     0x787f0f334dc9  (unknown)
    @     0x787f0f32e178  (unknown)
    @     0x787f0f32810b  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack123
    @     0x787f0f4e363c  (unknown)
    @     0x787f0f565a16  (unknown)
    @     0x787f0f565cf9  (unknown)
    @     0x787f0f32e779  (unknown)
    @     0x787f0f3283d7  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack124
    @     0x787f0f4de55e  (unknown)
    @     0x787f0f52ebf2  (unknown)
    @     0x787f0f519b18  (unknown)
    @     0x787f0f555576  (unknown)
    @     0x787f0f32e79f  (unknown)
    @     0x787f0f3283d7  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack125
    @     0x787f0f4f426f  (unknown)
    @     0x787f0f57a997  (unknown)
    @     0x787f0f519b3c  (unknown)
    @     0x787f0f555576  (unknown)
    @     0x787f0f32e79f  (unknown)
    @     0x787f0f3283d7  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack126
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53da6a  (unknown)
    @     0x787f0f5266c8  (unknown)
    @     0x787f0f328406  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack127
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f530505  (unknown)
    @     0x787f0f32f91c  (unknown)
    @     0x787f0f32e659  (unknown)
    @     0x787f0f3285d0  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack128
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0f32f95e  (unknown)
    @     0x787f0f32e659  (unknown)
    @     0x787f0f3285d0  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack129
    @     0x787f0f4f31b6  (unknown)
    @     0x787f0f556c30  (unknown)
    @     0x787f0f3351d6  (unknown)
    @     0x787f0f32e66a  (unknown)
    @     0x787f0f3285d0  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack130
    @     0x787f0f4df3d6  (unknown)
    @     0x787f0f53d2ca  (unknown)
    @     0x787f0f529008  (unknown)
    @     0x787f0f506741  (unknown)
    @     0x787f0f529dfb  (unknown)
    @     0x787f0f3274d3  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack131
    @     0x787f0f4f426f  (unknown)
    @     0x787f0f57a997  (unknown)
    @     0x787f0f506542  (unknown)
    @     0x787f0f5063d2  (unknown)
    @     0x787f0f529de0  (unknown)
    @     0x787f0f3274d3  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack132
    @     0x787f0f4ea266  (unknown)
    @     0x787f0f55872d  (unknown)
    @     0x787f0f328938  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

stack133
    @     0x787f0f4df5ff  (unknown)
    @     0x787f0f545c3a  (unknown)
    @     0x787f0f32892d  (unknown)
    @     0x787f0b7457a9  (unknown)
    @     0x787f09c95163  (unknown)
    @     0x787f09c97c13  (unknown)
    @     0x787f09c9b71d  (unknown)
    @     0x787f120c67e5  (unknown)
    @     0x787f120cbb16  (unknown)
    @     0x787f120d49d2  (unknown)
    @     0x787f12341c23  (unknown)
    @     0x787f98294ac3  (unknown)

