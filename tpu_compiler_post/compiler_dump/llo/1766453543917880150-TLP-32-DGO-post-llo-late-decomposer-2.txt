
%s184 = sld [smem:[#allocation14]]
%s185 = sand.u32 134217727, %s184
%s186 = sor.u32 4026531840, %s185

%187 = vtrace %s186

%s178 = sld [smem:[#allocation11]]

%179 = vtrace %s178

%s180 = sld [smem:[#allocation12]]

%181 = vtrace %s180

%s182 = sld [smem:[#allocation13]]

%183 = vtrace %s182

%v165 = vlaneseq
%v166 = vshrl.u32 %v165, 7
%v167 = vshrl.u32 %v166, 1
%v168 = vand.u32 1, %v166
%v169 = vshll.u32 %v168, 2
%v170 = vadd.s32 %v169, %v167
%v176 = vsub.s32 %v167, %v166
%v171 = vsub.s32 %v170, %v166
%172 = vsetiar.raw.iar0 %v171 /* EvenOdd Store IAR initialization */
%177 = vsetiar.raw.iar1 %v176 /* EvenOdd Load IAR initialization */
%s45 = sld [smem:[#allocation8]]

%p188 = scmp.eq.s32.totalorder %s45, 0

%49 = sbr.rel (%p188) target = $region29

%v54 = vand.u32 127, %v165
%s62 = sxor.u32 2925155241, %s45
%v58 = vxor.u32 1135663077, %v54
%s63 = smul.u32 2223506493, %s62
%v59 = vmul.u32 2925155241, %v58
%v60 = vshrl.u32 %v59, 16
%s64 = sshrl.u32 %s63, 16
%v61 = vxor.u32 %v60, %v59
%s65 = sxor.u32 %s64, %s63
%v66 = vxor.u32 2223506493, %v61
%v67 = vmul.u32 1519409121, %v66
%s70 = smul.u32 3389127133, %s65
%v68 = vshrl.u32 %v67, 16
%v69 = vxor.u32 %v68, %v67
%v72 = vstv %s70
%v71 = vmul.u32 1232336661, %v69
%v73 = vsub.s32 %v72, %v71
%v74 = vshrl.u32 %v73, 16
%v75 = vxor.u32 %v74, %v73
%v76 = vxor.u32 1519409121, %v75
%v77 = vmul.u32 2449846741, %v76
%v78 = vshrl.u32 %v77, 16
%v79 = vxor.u32 %v78, %v77
%v80 = vmul.u32 3389127133, %v61
%v81 = vmul.u32 1232336661, %v79
%v82 = vsub.s32 %v80, %v81
%v83 = vshrl.u32 %v82, 16
%v84 = vxor.u32 %v83, %v82
%v89 = vxor.u32 2925155241, %v75
%v85 = vxor.u32 1135663077, %v84
%v86 = vmul.u32 2925155241, %v85
%v90 = vmul.u32 2223506493, %v89
%v87 = vshrl.u32 %v86, 16
%v88 = vxor.u32 %v87, %v86
%v91 = vshrl.u32 %v90, 16
%v93 = vxor.u32 2223506493, %v88
%v92 = vxor.u32 %v91, %v90
%v94 = vmul.u32 1519409121, %v93
%v95 = vshrl.u32 %v94, 16
%v96 = vxor.u32 %v95, %v94
%v97 = vmul.u32 3389127133, %v92
%v98 = vmul.u32 1232336661, %v96
%v99 = vsub.s32 %v97, %v98
%v100 = vshrl.u32 %v99, 16
%v101 = vxor.u32 %v100, %v99
%v102 = vxor.u32 1519409121, %v101
%v103 = vmul.u32 2449846741, %v102
%v104 = vshrl.u32 %v103, 16
%v105 = vxor.u32 %v104, %v103
%v106 = vmul.u32 3389127133, %v88
%v119 = vxor.u32 1179257497, %v101
%v107 = vmul.u32 1232336661, %v105
%v108 = vsub.s32 %v106, %v107
%v120 = vmul.u32 2174555301, %v119
%v135 = vxor.u32 3546938817, %v101
%v109 = vshrl.u32 %v108, 16
%v123 = vxor.u32 461070425, %v101
%v139 = vxor.u32 728804945, %v101
%v110 = vxor.u32 %v109, %v108
%v121 = vshrl.u32 %v120, 16
%v136 = vmul.u32 1343633581, %v135
%v111 = vxor.u32 2337405405, %v110
%v115 = vxor.u32 747796405, %v110
%v127 = vxor.u32 2174555301, %v110
%v124 = vmul.u32 702470093, %v123
%v131 = vxor.u32 702470093, %v110
%v140 = vmul.u32 1920080165, %v139
%v112 = vmul.u32 1179257497, %v111
%v116 = vmul.u32 461070425, %v115
%v128 = vmul.u32 3546938817, %v127
%v132 = vmul.u32 728804945, %v131
%v113 = vshrl.u32 %v112, 16
%v122 = vxor.u32 %v121, %v120
%v137 = vshrl.u32 %v136, 16
%v129 = vshrl.u32 %v128, 16
%v114 = vxor.u32 %v113, %v112
%v125 = vshrl.u32 %v124, 16
%v117 = vshrl.u32 %v116, 16
%v130 = vxor.u32 %v129, %v128
%v133 = vshrl.u32 %v132, 16
%v141 = vshrl.u32 %v140, 16
%v138 = vxor.u32 %v137, %v136
%v143 = vor.u32 %v122, %v114
%v144 = vor.u32 %v143, %v130
%v118 = vxor.u32 %v117, %v116
%v126 = vxor.u32 %v125, %v124
%v134 = vxor.u32 %v133, %v132
%v142 = vxor.u32 %v141, %v140
%v145 = vor.u32 %v144, %v138
%vm189 = vcmp.eq.s32.totalorder %v166, 1
%vm146 = vcmp.eq.s32.totalorder %v145, 0
%vm190 = vcmp.eq.s32.totalorder %v166, 2
%vm191 = vcmp.eq.s32.totalorder %v166, 3
%v156 = vsel /*vm=*/%vm146, /*on_true_vy=*/%v118, /*on_false_vx=*/%v114
%v157 = vsel /*vm=*/%vm146, /*on_true_vy=*/%v126, /*on_false_vx=*/%v122
%v159 = vsel /*vm=*/%vm146, /*on_true_vy=*/%v134, /*on_false_vx=*/%v130
%v161 = vsel /*vm=*/%vm146, /*on_true_vy=*/%v142, /*on_false_vx=*/%v138
%v158 = vsel /*vm=*/%vm189, /*on_true_vy=*/%v157, /*on_false_vx=*/%v156
%v160 = vsel /*vm=*/%vm190, /*on_true_vy=*/%v159, /*on_false_vx=*/%v158
%v162 = vsel /*vm=*/%vm191, /*on_true_vy=*/%v161, /*on_false_vx=*/%v160
%163 = setrngseed %v162 /* Rng seed initialization */
%v164 = vrng /* Rng seed initialization */

%40 = vsettm 1

%s193 = smov 2147483646 /* materialized constant */

%39 = vsettm %s193

%37 = vtrace 2415919103

%0 = vtrace 2952790016

%1 = vtrace 3221225472

%s2 = sld [smem:[#allocation0]]

%p192 = scmp.ne.s32.totalorder %s2, 1

%6 = sbr.rel (%p192) target = $region4

%s7 = sld [smem:[#allocation2]]
%s8 = int_to_ptr.hbm [resolvable:$false] %s7

%s9 = scalar_parameter_address 0

%10 = compiler-scheduling-barrier

%12 = compiler-scheduling-barrier

%22 = vtrace 2147483648

%13 = compiler-scheduling-barrier

%15 = vsyncpa [#allocation6], 0
%s194 = smov [#allocation5] /* materialized constant */
%s16 = sshll.u32 %s194, 4
%s17 = int_to_ptr.vmem [resolvable:$true] %s16
%19 = dma.hbm_to_vmem [thread:$1]  /*hbm=*/%s9, /*size_in_granules=*/16, /*vmem=*/%s17, /*dst_syncflagno=*/[#allocation6]

%21 = compiler-scheduling-barrier

%23 = vtrace 2415919104

%28 = vtrace 2147483649

%24 = compiler-scheduling-barrier

%25 = dma.done [#allocation6], 16 /* local-dma-wait */

%26 = vsyncpa [#allocation6], 1

%27 = compiler-scheduling-barrier

%29 = vtrace 2415919105

%30 = compiler-scheduling-barrier

%31 = compiler-scheduling-barrier

%35 = vtrace 2147483650

%32 = compiler-scheduling-barrier

%s195 = smov [#allocation5] /* materialized constant */

%33 = inlined_call %s195, %s8 /* %copy.1 = copy(%bitcast.1) */

%34 = compiler-scheduling-barrier

%36 = vtrace 2415919106

%38 = vtrace 2684354559

%s196 = smov 2147483647 /* materialized constant */

%41 = vsettm %s196

%42 = vdelay 1

%43 = sfence

%s197 = smov 0 /* materialized constant */
%44 = sst [smem:[#allocation7]] %s197
