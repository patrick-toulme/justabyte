= control target key start
LH: loop header
LB: loop body
LE: loop exit
PB: predicated region body
PF: predicated region fallthrough
CT: control target
= control target key end

     0   :  { %v19_v3 = vlaneseq  ;;  %v164_v13 = vmov -inf /* materialized constant */  ;;  %s192_s1 = inlined_call_operand.vmem [shape: f32[16], index: 1, kind: input, shape index: {}] /* operand 1 */  ;;  %s193_s0 = inlined_call_operand.vmem [shape: f32[16,64], index: 0, kind: input, shape index: {}] /* operand 0 */  ;;  %s194_s2 = inlined_call_operand.vmem [shape: f32[16], index: 2, kind: output, shape index: {}] /* operand 2 */ } /* entry bundle: %fusion.5 = fusion(%get-tuple-element.1, %add_sqrt_fusion) */
   0x1   :  { %v13_v0 = vld [vmem:[%s192_s1] ss:$0 sm:$0xff]  ;;  %v158_v8 = vld [vmem:[%s193_s0 + $0x8] sm:$0xff]  ;;  %9 = vst [vmem:[#allocation0] sm:$0xff] /*vst_source=*/%v164_v13 }
   0x2   :  { %15 = vbcast.lane.b32.xlu0 %v13_v0, 256  ;;  %v20_v4 = vand.u32 127, %v19_v3  ;;  %v12_v5 = vld [vmem:[%s193_s0] sm:$0xff] }
   0x3   :  { %vm21_vm0 = vcmp.lt.s32.totalorder %v20_v4, 64 }
   0x4   :  { %28 = vbcast.lane.b32.xlu0 %v13_v0, 264 }
   0x5   :  { %v16_v1 = vpop.permute.xlu0 %15 }
   0x6   :  { %160 = vrcp.f32 %v16_v1 }
   0x7   :  { %v29_v2 = vpop.permute.xlu0 %28 }
   0x8   :  { %162 = vrcp.f32 %v29_v2 }
   0x9   :  { %v161_v6 = vpop.eup %160 }
   0xa   :  { %v18_v7 = vmul.f32 %v161_v6, %v12_v5 }
   0xb   :  { %v22_v9 = vsel /*vm=*/%vm21_vm0, /*on_true_vy=*/%v18_v7, /*on_false_vx=*/-inf }
   0xc   :  { %v163_v10 = vpop.eup %162  ;;  %52 = vxpose.xlu1.b32.start [1/2] (short) /*vx=*/%v22_v9, /*width=*/128 }
   0xd   :  { %v31_v11 = vmul.f32 %v163_v10, %v158_v8 }
   0xe   :  { %v35_v12 = vsel /*vm=*/%vm21_vm0, /*on_true_vy=*/%v31_v11, /*on_false_vx=*/-inf }
   0xf   :  { %53 = vxpose.xlu1.b32.end [2/2] (short) /*vx=*/%v35_v12, /*width=*/128 }
  0x10   :  { %v54_v14 = vpop.trf.xlu1 }
  0x11   :  { %v55_v15 = vpop.trf.xlu1 }
  0x12   :  { %v76_v16 = vmax.f32 %v54_v14, %v55_v15 }
  0x13   :  { %v56_v17 = vpop.trf.xlu1 }
  0x14   :  { %v80_v18 = vmax.f32 %v76_v16, %v56_v17 }
  0x15   :  { %v57_v19 = vpop.trf.xlu1 }
  0x16   :  { %v84_v20 = vmax.f32 %v80_v18, %v57_v19 }
  0x17   :  { %v58_v21 = vpop.trf.xlu1 }
  0x18   :  { %v88_v22 = vmax.f32 %v84_v20, %v58_v21 }
  0x19   :  { %v59_v23 = vpop.trf.xlu1 }
  0x1a   :  { %v92_v24 = vmax.f32 %v88_v22, %v59_v23 }
  0x1b   :  { %v60_v25 = vpop.trf.xlu1 }
  0x1c   :  { %v96_v26 = vmax.f32 %v92_v24, %v60_v25 }
  0x1d   :  { %v61_v27 = vpop.trf.xlu1 }
  0x1e   :  { %v100_v28 = vmax.f32 %v96_v26, %v61_v27 }
  0x1f   :  { %v62_v29 = vpop.trf.xlu1 }
  0x20   :  { %v104_v30 = vmax.f32 %v100_v28, %v62_v29 }
  0x21   :  { %v63_v31 = vpop.trf.xlu1 }
  0x22   :  { %v108_v32 = vmax.f32 %v104_v30, %v63_v31 }
  0x23   :  { %v64_v33 = vpop.trf.xlu1 }
  0x24   :  { %v112_v34 = vmax.f32 %v108_v32, %v64_v33 }
  0x25   :  { %v65_v35 = vpop.trf.xlu1 }
  0x26   :  { %v116_v36 = vmax.f32 %v112_v34, %v65_v35 }
  0x27   :  { %v66_v37 = vpop.trf.xlu1 }
  0x28   :  { %v120_v38 = vmax.f32 %v116_v36, %v66_v37 }
  0x29   :  { %v67_v39 = vpop.trf.xlu1 }
  0x2a   :  { %v124_v40 = vmax.f32 %v120_v38, %v67_v39 }
  0x2b   :  { %v68_v41 = vpop.trf.xlu1 }
  0x2c   :  { %v128_v42 = vmax.f32 %v124_v40, %v68_v41 }
  0x2d   :  { %v69_v43 = vpop.trf.xlu1 }
  0x2e   :  { %v132_v44 = vmax.f32 %v128_v42, %v69_v43 }
  0x2f   :  { %v134_v45 = vrot.slane %v132_v44, 4 }
  0x30   :  { %v137_v46 = vmax.f32 %v132_v44, %v134_v45 }
  0x31   :  { %v139_v47 = vrot.slane %v137_v46, 2 }
  0x32   :  { %v142_v48 = vmax.f32 %v137_v46, %v139_v47 }
  0x33   :  { %v144_v49 = vrot.slane %v142_v48, 1 }
  0x34   :  { %v147_v50 = vmax.f32 %v142_v48, %v144_v49 }
  0x35   :  { %149 = vst [vmem:[#allocation0] sm:$0x1] /*vst_source=*/%v147_v50 }
  0x36   :  { %v154_v51 = vld [vmem:[#allocation0] sm:$0x1] }
  0x37   :  { %157 = vst [vmem:[%s194_s2] sm:$0x1] /*vst_source=*/%v154_v51 } /* exit bundle: %fusion.5 = fusion(%get-tuple-element.1, %add_sqrt_fusion) */
