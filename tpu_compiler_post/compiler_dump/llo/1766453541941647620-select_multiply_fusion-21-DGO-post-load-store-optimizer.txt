
%s0 = inlined_call_operand.vmem [shape: u32[64], index: 0, kind: input, shape index: {}] /* operand 0 */
%s1 = inlined_call_operand.vmem [shape: u32[64], index: 1, kind: input, shape index: {}] /* operand 1 */
%s2 = inlined_call_operand.vmem [shape: u32[64], index: 2, kind: input, shape index: {}] /* operand 2 */
%s3 = inlined_call_operand.<no memory space> [shape: u32[], index: 3, kind: input, shape index: {}] /* operand 3 */
%s4 = inlined_call_operand.<no memory space> [shape: u32[], index: 4, kind: input, shape index: {}] /* operand 4 */
%s5 = inlined_call_operand.<no memory space> [shape: u32[], index: 5, kind: input, shape index: {}] /* operand 5 */
%s6 = inlined_call_operand.hbm [shape: f32[64,64], index: 6, kind: output, shape index: {}] /* operand 6 */
%v7 = vstv %s3
%v8 = vstv %s4
%v9 = vstv %s5

%v911 = vld [vmem:[%s2] ss:$0 sm:$0xff]
%914 = vbcast.lane.b32.xlu1 %v911, 272
%27 = vbcast.lane.b32.xlu0 %v911, 256
%1358 = vbcast.lane.b32.xlu1 %v911, 280
%470 = vbcast.lane.b32.xlu0 %v911, 264
%2246 = vbcast.lane.b32.xlu1 %v911, 296
%1802 = vbcast.lane.b32.xlu0 %v911, 288
%3134 = vbcast.lane.b32.xlu1 %v911, 312
%2690 = vbcast.lane.b32.xlu0 %v911, 304
%v21 = vld [vmem:[%s1] ss:$0 sm:$0xff]
%23 = vbcast.lane.b32.xlu1 %v21, 256
%v17 = vld [vmem:[%s0] ss:$0 sm:$0xff]
%19 = vbcast.lane.b32.xlu0 %v17, 256
%465 = vbcast.lane.b32.xlu1 %v21, 264
%460 = vbcast.lane.b32.xlu0 %v17, 264
%909 = vbcast.lane.b32.xlu1 %v21, 272
%904 = vbcast.lane.b32.xlu0 %v17, 272
%1353 = vbcast.lane.b32.xlu1 %v21, 280
%1348 = vbcast.lane.b32.xlu0 %v17, 280
%1797 = vbcast.lane.b32.xlu1 %v21, 288
%1792 = vbcast.lane.b32.xlu0 %v17, 288
%2241 = vbcast.lane.b32.xlu1 %v21, 296
%2236 = vbcast.lane.b32.xlu0 %v17, 296
%2685 = vbcast.lane.b32.xlu1 %v21, 304
%2680 = vbcast.lane.b32.xlu0 %v17, 304
%3129 = vbcast.lane.b32.xlu1 %v21, 312
%3124 = vbcast.lane.b32.xlu0 %v17, 312
%v915 = vpop.permute.xlu1 %914
%v29 = vlaneseq
%v30 = vand.u32 127, %v29
%v922 = vadd.s32 %v915, %v30
%vm926 = vcmp.lt.u32.totalorder %v922, %v915
%v940 = vadd.s32 %v922, %v8
%v946 = vshll.u32 %v940, 13
%v947 = vshrl.u32 %v940, 19
%v28 = vpop.permute.xlu0 %27
%v35 = vadd.s32 %v30, %v28
%vm39 = vcmp.lt.u32.totalorder %v35, %v28
%v53 = vadd.s32 %v35, %v8
%v59 = vshll.u32 %v53, 13
%v60 = vshrl.u32 %v53, 19
%v1359 = vpop.permute.xlu1 %1358
%v1366 = vadd.s32 %v1359, %v30
%vm1370 = vcmp.lt.u32.totalorder %v1366, %v1359
%v1384 = vadd.s32 %v1366, %v8
%v1390 = vshll.u32 %v1384, 13
%v1391 = vshrl.u32 %v1384, 19
%v471 = vpop.permute.xlu0 %470
%v478 = vadd.s32 %v471, %v30
%vm482 = vcmp.lt.u32.totalorder %v478, %v471
%v496 = vadd.s32 %v478, %v8
%v502 = vshll.u32 %v496, 13
%v503 = vshrl.u32 %v496, 19
%v2247 = vpop.permute.xlu1 %2246
%v2254 = vadd.s32 %v2247, %v30
%vm2258 = vcmp.lt.u32.totalorder %v2254, %v2247
%v2272 = vadd.s32 %v2254, %v8
%v2278 = vshll.u32 %v2272, 13
%v2279 = vshrl.u32 %v2272, 19
%v1803 = vpop.permute.xlu0 %1802
%v1810 = vadd.s32 %v1803, %v30
%vm1814 = vcmp.lt.u32.totalorder %v1810, %v1803
%v1828 = vadd.s32 %v1810, %v8
%v1834 = vshll.u32 %v1828, 13
%v1835 = vshrl.u32 %v1828, 19
%v3135 = vpop.permute.xlu1 %3134
%v3142 = vadd.s32 %v3135, %v30
%vm3146 = vcmp.lt.u32.totalorder %v3142, %v3135
%v3160 = vadd.s32 %v3142, %v8
%v3166 = vshll.u32 %v3160, 13
%v3167 = vshrl.u32 %v3160, 19
%v2691 = vpop.permute.xlu0 %2690
%v2698 = vadd.s32 %v2691, %v30
%vm2702 = vcmp.lt.u32.totalorder %v2698, %v2691
%v2716 = vadd.s32 %v2698, %v8
%v2722 = vshll.u32 %v2716, 13
%v2723 = vshrl.u32 %v2716, 19
%v61 = vor.u32 %v60, %v59
%v24 = vpop.permute.xlu1 %23
%v20 = vpop.permute.xlu0 %19
%v44 = vsel /*vm=*/%vm39, /*on_true_vy=*/%v20, /*on_false_vx=*/%v24
%v504 = vor.u32 %v503, %v502
%v466 = vpop.permute.xlu1 %465
%v49 = vadd.s32 %v44, %v9
%v461 = vpop.permute.xlu0 %460
%v57 = vadd.s32 %v53, %v49
%v487 = vsel /*vm=*/%vm482, /*on_true_vy=*/%v461, /*on_false_vx=*/%v466
%v948 = vor.u32 %v947, %v946
%v910 = vpop.permute.xlu1 %909
%v62 = vxor.u32 %v61, %v57
%v492 = vadd.s32 %v487, %v9
%v905 = vpop.permute.xlu0 %904
%v65 = vadd.s32 %v62, %v57
%v67 = vshll.u32 %v62, 15
%v68 = vshrl.u32 %v62, 17
%v500 = vadd.s32 %v496, %v492
%v931 = vsel /*vm=*/%vm926, /*on_true_vy=*/%v905, /*on_false_vx=*/%v910
%v1392 = vor.u32 %v1391, %v1390
%v1354 = vpop.permute.xlu1 %1353
%v69 = vor.u32 %v68, %v67
%v505 = vxor.u32 %v504, %v500
%v936 = vadd.s32 %v931, %v9
%v1349 = vpop.permute.xlu0 %1348
%v70 = vxor.u32 %v69, %v65
%v508 = vadd.s32 %v505, %v500
%v510 = vshll.u32 %v505, 15
%v511 = vshrl.u32 %v505, 17
%v944 = vadd.s32 %v940, %v936
%v1375 = vsel /*vm=*/%vm1370, /*on_true_vy=*/%v1349, /*on_false_vx=*/%v1354
%v1836 = vor.u32 %v1835, %v1834
%v1798 = vpop.permute.xlu1 %1797
%v73 = vadd.s32 %v70, %v65
%v75 = vshll.u32 %v70, 26
%v76 = vshrl.u32 %v70, 6
%v512 = vor.u32 %v511, %v510
%v949 = vxor.u32 %v948, %v944
%v1380 = vadd.s32 %v1375, %v9
%v1793 = vpop.permute.xlu0 %1792
%v77 = vor.u32 %v76, %v75
%v513 = vxor.u32 %v512, %v508
%v952 = vadd.s32 %v949, %v944
%v954 = vshll.u32 %v949, 15
%v955 = vshrl.u32 %v949, 17
%v1388 = vadd.s32 %v1384, %v1380
%v1819 = vsel /*vm=*/%vm1814, /*on_true_vy=*/%v1793, /*on_false_vx=*/%v1798
%v2280 = vor.u32 %v2279, %v2278
%v2242 = vpop.permute.xlu1 %2241
%v78 = vxor.u32 %v77, %v73
%v516 = vadd.s32 %v513, %v508
%v518 = vshll.u32 %v513, 26
%v519 = vshrl.u32 %v513, 6
%v956 = vor.u32 %v955, %v954
%v1393 = vxor.u32 %v1392, %v1388
%v1824 = vadd.s32 %v1819, %v9
%v2237 = vpop.permute.xlu0 %2236
%v81 = vadd.s32 %v78, %v73
%v87 = vshll.u32 %v78, 6
%v88 = vshrl.u32 %v78, 26
%v520 = vor.u32 %v519, %v518
%v957 = vxor.u32 %v956, %v952
%v1396 = vadd.s32 %v1393, %v1388
%v1398 = vshll.u32 %v1393, 15
%v1399 = vshrl.u32 %v1393, 17
%v1832 = vadd.s32 %v1828, %v1824
%v2263 = vsel /*vm=*/%vm2258, /*on_true_vy=*/%v2237, /*on_false_vx=*/%v2242
%v2724 = vor.u32 %v2723, %v2722
%v2686 = vpop.permute.xlu1 %2685
%v85 = vadd.s32 %v81, %v8
%v89 = vor.u32 %v88, %v87
%v521 = vxor.u32 %v520, %v516
%v960 = vadd.s32 %v957, %v952
%v962 = vshll.u32 %v957, 26
%v963 = vshrl.u32 %v957, 6
%v1400 = vor.u32 %v1399, %v1398
%v1837 = vxor.u32 %v1836, %v1832
%v2268 = vadd.s32 %v2263, %v9
%v2681 = vpop.permute.xlu0 %2680
%v90 = vxor.u32 %v89, %v81
%v524 = vadd.s32 %v521, %v516
%v530 = vshll.u32 %v521, 6
%v531 = vshrl.u32 %v521, 26
%v964 = vor.u32 %v963, %v962
%v1401 = vxor.u32 %v1400, %v1396
%v1840 = vadd.s32 %v1837, %v1832
%v1842 = vshll.u32 %v1837, 15
%v1843 = vshrl.u32 %v1837, 17
%v2276 = vadd.s32 %v2272, %v2268
%v2707 = vsel /*vm=*/%vm2702, /*on_true_vy=*/%v2681, /*on_false_vx=*/%v2686
%v3168 = vor.u32 %v3167, %v3166
%v3130 = vpop.permute.xlu1 %3129
%v93 = vadd.s32 %v90, %v7
%v528 = vadd.s32 %v524, %v8
%v532 = vor.u32 %v531, %v530
%v965 = vxor.u32 %v964, %v960
%v1404 = vadd.s32 %v1401, %v1396
%v1406 = vshll.u32 %v1401, 26
%v1407 = vshrl.u32 %v1401, 6
%v1844 = vor.u32 %v1843, %v1842
%v2281 = vxor.u32 %v2280, %v2276
%v2712 = vadd.s32 %v2707, %v9
%v3125 = vpop.permute.xlu0 %3124
%v97 = vadd.s32 1, %v93
%v533 = vxor.u32 %v532, %v524
%v968 = vadd.s32 %v965, %v960
%v974 = vshll.u32 %v965, 6
%v975 = vshrl.u32 %v965, 26
%v1408 = vor.u32 %v1407, %v1406
%v1845 = vxor.u32 %v1844, %v1840
%v2284 = vadd.s32 %v2281, %v2276
%v2286 = vshll.u32 %v2281, 15
%v2287 = vshrl.u32 %v2281, 17
%v2720 = vadd.s32 %v2716, %v2712
%v3151 = vsel /*vm=*/%vm3146, /*on_true_vy=*/%v3125, /*on_false_vx=*/%v3130
%10 = vsyncpa [#allocation2], 0
%v101 = vadd.s32 %v97, %v85
%v103 = vshll.u32 %v97, 17
%v104 = vshrl.u32 %v97, 15
%v105 = vor.u32 %v104, %v103
%v106 = vxor.u32 %v105, %v101
%v109 = vadd.s32 %v106, %v101
%v111 = vshll.u32 %v106, 29
%v112 = vshrl.u32 %v106, 3
%v113 = vor.u32 %v112, %v111
%v114 = vxor.u32 %v113, %v109
%v117 = vadd.s32 %v114, %v109
%v119 = vshll.u32 %v114, 16
%v120 = vshrl.u32 %v114, 16
%v121 = vor.u32 %v120, %v119
%v122 = vxor.u32 %v121, %v117
%v125 = vadd.s32 %v122, %v117
%v129 = vadd.s32 %v125, %v7
%v131 = vshll.u32 %v122, 24
%v132 = vshrl.u32 %v122, 8
%v133 = vor.u32 %v132, %v131
%v134 = vxor.u32 %v133, %v125
%v137 = vadd.s32 %v134, %v9
%v141 = vadd.s32 2, %v137
%v145 = vadd.s32 %v141, %v129
%v147 = vshll.u32 %v141, 13
%v148 = vshrl.u32 %v141, 19
%v149 = vor.u32 %v148, %v147
%v150 = vxor.u32 %v149, %v145
%v153 = vadd.s32 %v150, %v145
%v155 = vshll.u32 %v150, 15
%v156 = vshrl.u32 %v150, 17
%v157 = vor.u32 %v156, %v155
%v158 = vxor.u32 %v157, %v153
%v161 = vadd.s32 %v158, %v153
%v163 = vshll.u32 %v158, 26
%v164 = vshrl.u32 %v158, 6
%v165 = vor.u32 %v164, %v163
%v166 = vxor.u32 %v165, %v161
%v169 = vadd.s32 %v166, %v161
%v173 = vadd.s32 %v169, %v9
%v175 = vshll.u32 %v166, 6
%v176 = vshrl.u32 %v166, 26
%v177 = vor.u32 %v176, %v175
%v178 = vxor.u32 %v177, %v169
%v181 = vadd.s32 %v178, %v8
%v185 = vadd.s32 3, %v181
%v189 = vadd.s32 %v185, %v173
%v191 = vshll.u32 %v185, 17
%v192 = vshrl.u32 %v185, 15
%v193 = vor.u32 %v192, %v191
%v194 = vxor.u32 %v193, %v189
%v197 = vadd.s32 %v194, %v189
%v199 = vshll.u32 %v194, 29
%v200 = vshrl.u32 %v194, 3
%v201 = vor.u32 %v200, %v199
%v202 = vxor.u32 %v201, %v197
%v205 = vadd.s32 %v202, %v197
%v207 = vshll.u32 %v202, 16
%v208 = vshrl.u32 %v202, 16
%v209 = vor.u32 %v208, %v207
%v210 = vxor.u32 %v209, %v205
%v213 = vadd.s32 %v210, %v205
%v217 = vadd.s32 %v213, %v8
%v219 = vshll.u32 %v210, 24
%v220 = vshrl.u32 %v210, 8
%v221 = vor.u32 %v220, %v219
%v222 = vxor.u32 %v221, %v213
%v225 = vadd.s32 %v222, %v7
%v229 = vadd.s32 4, %v225
%v233 = vadd.s32 %v229, %v217
%v235 = vshll.u32 %v229, 13
%v236 = vshrl.u32 %v229, 19
%v237 = vor.u32 %v236, %v235
%v238 = vxor.u32 %v237, %v233
%v241 = vadd.s32 %v238, %v233
%v243 = vshll.u32 %v238, 15
%v244 = vshrl.u32 %v238, 17
%v245 = vor.u32 %v244, %v243
%v246 = vxor.u32 %v245, %v241
%v249 = vadd.s32 %v246, %v241
%v251 = vshll.u32 %v246, 26
%v252 = vshrl.u32 %v246, 6
%v253 = vor.u32 %v252, %v251
%v254 = vxor.u32 %v253, %v249
%v257 = vadd.s32 %v254, %v249
%v261 = vadd.s32 %v257, %v7
%v263 = vshll.u32 %v254, 6
%v264 = vshrl.u32 %v254, 26
%v265 = vor.u32 %v264, %v263
%v266 = vxor.u32 %v265, %v257
%v269 = vadd.s32 %v266, %v9
%v273 = vadd.s32 5, %v269
%v275 = vxor.u32 %v273, %v261
%v276 = vshrl.u32 %v275, 9
%v277 = vor.u32 1065353216, %v276
%v281 = vadd.f32 -1.0, %v277
%v285 = vmul.f32 2.0, %v281
%v289 = vadd.f32 -0.99999994, %v285
%v293 = vmax.f32 %v289, -0.99999994
%v295 = vand.u32 2147483647, %v293
%vm298 = vcmp.eq.f32.partialorder %v295, 1.0
%v303 = vmul.f32 inf, %v293
%v305 = vxor.u32 2147483648, %v293
%v308 = vmul.f32 %v305, %v293
%v310 = vadd.f32 1.0, %v308
%v311 = vlog2.pop %v310
%v312 = vmul.f32 0.6931472, %v311
%v313 = vmul.f32 -0.5, %v308
%v314 = vadd.f32 1.0, %v313
%v315 = vmul.f32 %v314, %v308
%v316 = vand.u32 2147483647, %v308
%vm317 = vcmp.lt.f32.partialorder %v316, 0.0004427343
%v318 = vsel /*vm=*/%vm317, /*on_true_vy=*/%v315, /*on_false_vx=*/%v312
%v319 = vxor.u32 2147483648, %v318
%vm322 = vcmp.lt.f32.partialorder %v319, 5.0
%v3575 = vmov 2.8329768 /* materialized constant */
%v327 = vsel /*vm=*/%vm322, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v3575
%v3576 = vmov 1.001674 /* materialized constant */
%v331 = vsel /*vm=*/%vm322, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v3576
%v3577 = vmov 0.0094388705 /* materialized constant */
%v335 = vsel /*vm=*/%vm322, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v3577
%v3578 = vmov -0.0076224613 /* materialized constant */
%v339 = vsel /*vm=*/%vm322, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v3578
%v3579 = vmov 0.0057395077 /* materialized constant */
%v343 = vsel /*vm=*/%vm322, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v3579
%v3580 = vmov -0.0036734284 /* materialized constant */
%v347 = vsel /*vm=*/%vm322, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v3580
%v3581 = vmov 0.0013493432 /* materialized constant */
%v351 = vsel /*vm=*/%vm322, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v3581
%v3582 = vmov 0.00010095056 /* materialized constant */
%v355 = vsel /*vm=*/%vm322, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v3582
%v3583 = vmov -0.00020021426 /* materialized constant */
%v359 = vsel /*vm=*/%vm322, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v3583
%v363 = vadd.f32 -2.5, %v319
%v365 = vrsqrt.pop %v319
%v366 = vmul.f32 %v365, %v319
%vm367 = vcmp.eq.f32.partialorder %v319, inf
%v368 = vsel /*vm=*/%vm367, /*on_true_vy=*/%v319, /*on_false_vx=*/%v366
%vm369 = vcmp.eq.f32.partialorder %v319, 0.0
%v370 = vand.u32 2147483648, %v319
%v371 = vsel /*vm=*/%vm369, /*on_true_vy=*/%v370, /*on_false_vx=*/%v368
%v374 = vadd.f32 -3.0, %v371
%v378 = vsel /*vm=*/%vm322, /*on_true_vy=*/%v363, /*on_false_vx=*/%v374
%v382 = vmul.f32 %v378, %v359
%v386 = vadd.f32 %v382, %v355
%v390 = vmul.f32 %v386, %v378
%v394 = vadd.f32 %v390, %v351
%v398 = vmul.f32 %v394, %v378
%v402 = vadd.f32 %v398, %v347
%v406 = vmul.f32 %v402, %v378
%v410 = vadd.f32 %v406, %v343
%v414 = vmul.f32 %v410, %v378
%v418 = vadd.f32 %v414, %v339
%v422 = vmul.f32 %v418, %v378
%v426 = vadd.f32 %v422, %v335
%v430 = vmul.f32 %v426, %v378
%v434 = vadd.f32 %v430, %v331
%v438 = vmul.f32 %v434, %v378
%v442 = vadd.f32 %v438, %v327
%v446 = vmul.f32 %v442, %v293
%v450 = vsel /*vm=*/%vm298, /*on_true_vy=*/%v303, /*on_false_vx=*/%v446
%v454 = vmul.f32 1.4142135, %v450
%456 = vst [vmem:[#allocation0] sm:$0xff] /*vst_source=*/%v454
%v536 = vadd.s32 %v533, %v7
%v540 = vadd.s32 1, %v536
%v544 = vadd.s32 %v540, %v528
%v546 = vshll.u32 %v540, 17
%v547 = vshrl.u32 %v540, 15
%v548 = vor.u32 %v547, %v546
%v549 = vxor.u32 %v548, %v544
%v552 = vadd.s32 %v549, %v544
%v554 = vshll.u32 %v549, 29
%v555 = vshrl.u32 %v549, 3
%v556 = vor.u32 %v555, %v554
%v557 = vxor.u32 %v556, %v552
%v560 = vadd.s32 %v557, %v552
%v562 = vshll.u32 %v557, 16
%v563 = vshrl.u32 %v557, 16
%v564 = vor.u32 %v563, %v562
%v565 = vxor.u32 %v564, %v560
%v568 = vadd.s32 %v565, %v560
%v572 = vadd.s32 %v568, %v7
%v574 = vshll.u32 %v565, 24
%v575 = vshrl.u32 %v565, 8
%v576 = vor.u32 %v575, %v574
%v577 = vxor.u32 %v576, %v568
%v580 = vadd.s32 %v577, %v9
%v584 = vadd.s32 2, %v580
%v588 = vadd.s32 %v584, %v572
%v590 = vshll.u32 %v584, 13
%v591 = vshrl.u32 %v584, 19
%v592 = vor.u32 %v591, %v590
%v593 = vxor.u32 %v592, %v588
%v596 = vadd.s32 %v593, %v588
%v598 = vshll.u32 %v593, 15
%v599 = vshrl.u32 %v593, 17
%v600 = vor.u32 %v599, %v598
%v601 = vxor.u32 %v600, %v596
%v604 = vadd.s32 %v601, %v596
%v606 = vshll.u32 %v601, 26
%v607 = vshrl.u32 %v601, 6
%v608 = vor.u32 %v607, %v606
%v609 = vxor.u32 %v608, %v604
%v612 = vadd.s32 %v609, %v604
%v616 = vadd.s32 %v612, %v9
%v618 = vshll.u32 %v609, 6
%v619 = vshrl.u32 %v609, 26
%v620 = vor.u32 %v619, %v618
%v621 = vxor.u32 %v620, %v612
%v624 = vadd.s32 %v621, %v8
%v628 = vadd.s32 3, %v624
%v632 = vadd.s32 %v628, %v616
%v634 = vshll.u32 %v628, 17
%v635 = vshrl.u32 %v628, 15
%v636 = vor.u32 %v635, %v634
%v637 = vxor.u32 %v636, %v632
%v640 = vadd.s32 %v637, %v632
%v642 = vshll.u32 %v637, 29
%v643 = vshrl.u32 %v637, 3
%v644 = vor.u32 %v643, %v642
%v645 = vxor.u32 %v644, %v640
%v648 = vadd.s32 %v645, %v640
%v650 = vshll.u32 %v645, 16
%v651 = vshrl.u32 %v645, 16
%v652 = vor.u32 %v651, %v650
%v653 = vxor.u32 %v652, %v648
%v656 = vadd.s32 %v653, %v648
%v660 = vadd.s32 %v656, %v8
%v662 = vshll.u32 %v653, 24
%v663 = vshrl.u32 %v653, 8
%v664 = vor.u32 %v663, %v662
%v665 = vxor.u32 %v664, %v656
%v668 = vadd.s32 %v665, %v7
%v672 = vadd.s32 4, %v668
%v676 = vadd.s32 %v672, %v660
%v678 = vshll.u32 %v672, 13
%v679 = vshrl.u32 %v672, 19
%v680 = vor.u32 %v679, %v678
%v681 = vxor.u32 %v680, %v676
%v684 = vadd.s32 %v681, %v676
%v686 = vshll.u32 %v681, 15
%v687 = vshrl.u32 %v681, 17
%v688 = vor.u32 %v687, %v686
%v689 = vxor.u32 %v688, %v684
%v692 = vadd.s32 %v689, %v684
%v694 = vshll.u32 %v689, 26
%v695 = vshrl.u32 %v689, 6
%v696 = vor.u32 %v695, %v694
%v697 = vxor.u32 %v696, %v692
%v700 = vadd.s32 %v697, %v692
%v704 = vadd.s32 %v700, %v7
%v706 = vshll.u32 %v697, 6
%v707 = vshrl.u32 %v697, 26
%v708 = vor.u32 %v707, %v706
%v709 = vxor.u32 %v708, %v700
%v712 = vadd.s32 %v709, %v9
%v716 = vadd.s32 5, %v712
%v718 = vxor.u32 %v716, %v704
%v719 = vshrl.u32 %v718, 9
%v720 = vor.u32 1065353216, %v719
%v724 = vadd.f32 -1.0, %v720
%v728 = vmul.f32 2.0, %v724
%v732 = vadd.f32 -0.99999994, %v728
%v736 = vmax.f32 %v732, -0.99999994
%v738 = vand.u32 2147483647, %v736
%vm741 = vcmp.eq.f32.partialorder %v738, 1.0
%v746 = vmul.f32 inf, %v736
%v748 = vxor.u32 2147483648, %v736
%v751 = vmul.f32 %v748, %v736
%v753 = vadd.f32 1.0, %v751
%v754 = vlog2.pop %v753
%v755 = vmul.f32 0.6931472, %v754
%v756 = vmul.f32 -0.5, %v751
%v757 = vadd.f32 1.0, %v756
%v758 = vmul.f32 %v757, %v751
%v759 = vand.u32 2147483647, %v751
%vm760 = vcmp.lt.f32.partialorder %v759, 0.0004427343
%v761 = vsel /*vm=*/%vm760, /*on_true_vy=*/%v758, /*on_false_vx=*/%v755
%v762 = vxor.u32 2147483648, %v761
%vm765 = vcmp.lt.f32.partialorder %v762, 5.0
%v770 = vsel /*vm=*/%vm765, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v3575
%v774 = vsel /*vm=*/%vm765, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v3576
%v778 = vsel /*vm=*/%vm765, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v3577
%v782 = vsel /*vm=*/%vm765, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v3578
%v786 = vsel /*vm=*/%vm765, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v3579
%v790 = vsel /*vm=*/%vm765, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v3580
%v794 = vsel /*vm=*/%vm765, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v3581
%v798 = vsel /*vm=*/%vm765, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v3582
%v802 = vsel /*vm=*/%vm765, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v3583
%v806 = vadd.f32 -2.5, %v762
%v808 = vrsqrt.pop %v762
%v809 = vmul.f32 %v808, %v762
%vm810 = vcmp.eq.f32.partialorder %v762, inf
%v811 = vsel /*vm=*/%vm810, /*on_true_vy=*/%v762, /*on_false_vx=*/%v809
%vm812 = vcmp.eq.f32.partialorder %v762, 0.0
%v813 = vand.u32 2147483648, %v762
%v814 = vsel /*vm=*/%vm812, /*on_true_vy=*/%v813, /*on_false_vx=*/%v811
%v817 = vadd.f32 -3.0, %v814
%v821 = vsel /*vm=*/%vm765, /*on_true_vy=*/%v806, /*on_false_vx=*/%v817
%v825 = vmul.f32 %v821, %v802
%v829 = vadd.f32 %v825, %v798
%v833 = vmul.f32 %v829, %v821
%v837 = vadd.f32 %v833, %v794
%v841 = vmul.f32 %v837, %v821
%v845 = vadd.f32 %v841, %v790
%v849 = vmul.f32 %v845, %v821
%v853 = vadd.f32 %v849, %v786
%v857 = vmul.f32 %v853, %v821
%v861 = vadd.f32 %v857, %v782
%v865 = vmul.f32 %v861, %v821
%v869 = vadd.f32 %v865, %v778
%v873 = vmul.f32 %v869, %v821
%v877 = vadd.f32 %v873, %v774
%v881 = vmul.f32 %v877, %v821
%v885 = vadd.f32 %v881, %v770
%v889 = vmul.f32 %v885, %v736
%v893 = vsel /*vm=*/%vm741, /*on_true_vy=*/%v746, /*on_false_vx=*/%v889
%v897 = vmul.f32 1.4142135, %v893
%900 = vst [vmem:[#allocation0 + $0x8] sm:$0xff] /*vst_source=*/%v897
%v972 = vadd.s32 %v968, %v8
%v976 = vor.u32 %v975, %v974
%v977 = vxor.u32 %v976, %v968
%v980 = vadd.s32 %v977, %v7
%v984 = vadd.s32 1, %v980
%v988 = vadd.s32 %v984, %v972
%v990 = vshll.u32 %v984, 17
%v991 = vshrl.u32 %v984, 15
%v992 = vor.u32 %v991, %v990
%v993 = vxor.u32 %v992, %v988
%v996 = vadd.s32 %v993, %v988
%v998 = vshll.u32 %v993, 29
%v999 = vshrl.u32 %v993, 3
%v1000 = vor.u32 %v999, %v998
%v1001 = vxor.u32 %v1000, %v996
%v1004 = vadd.s32 %v1001, %v996
%v1006 = vshll.u32 %v1001, 16
%v1007 = vshrl.u32 %v1001, 16
%v1008 = vor.u32 %v1007, %v1006
%v1009 = vxor.u32 %v1008, %v1004
%v1012 = vadd.s32 %v1009, %v1004
%v1016 = vadd.s32 %v1012, %v7
%v1018 = vshll.u32 %v1009, 24
%v1019 = vshrl.u32 %v1009, 8
%v1020 = vor.u32 %v1019, %v1018
%v1021 = vxor.u32 %v1020, %v1012
%v1024 = vadd.s32 %v1021, %v9
%v1028 = vadd.s32 2, %v1024
%v1032 = vadd.s32 %v1028, %v1016
%v1034 = vshll.u32 %v1028, 13
%v1035 = vshrl.u32 %v1028, 19
%v1036 = vor.u32 %v1035, %v1034
%v1037 = vxor.u32 %v1036, %v1032
%v1040 = vadd.s32 %v1037, %v1032
%v1042 = vshll.u32 %v1037, 15
%v1043 = vshrl.u32 %v1037, 17
%v1044 = vor.u32 %v1043, %v1042
%v1045 = vxor.u32 %v1044, %v1040
%v1048 = vadd.s32 %v1045, %v1040
%v1050 = vshll.u32 %v1045, 26
%v1051 = vshrl.u32 %v1045, 6
%v1052 = vor.u32 %v1051, %v1050
%v1053 = vxor.u32 %v1052, %v1048
%v1056 = vadd.s32 %v1053, %v1048
%v1060 = vadd.s32 %v1056, %v9
%v1062 = vshll.u32 %v1053, 6
%v1063 = vshrl.u32 %v1053, 26
%v1064 = vor.u32 %v1063, %v1062
%v1065 = vxor.u32 %v1064, %v1056
%v1068 = vadd.s32 %v1065, %v8
%v1072 = vadd.s32 3, %v1068
%v1076 = vadd.s32 %v1072, %v1060
%v1078 = vshll.u32 %v1072, 17
%v1079 = vshrl.u32 %v1072, 15
%v1080 = vor.u32 %v1079, %v1078
%v1081 = vxor.u32 %v1080, %v1076
%v1084 = vadd.s32 %v1081, %v1076
%v1086 = vshll.u32 %v1081, 29
%v1087 = vshrl.u32 %v1081, 3
%v1088 = vor.u32 %v1087, %v1086
%v1089 = vxor.u32 %v1088, %v1084
%v1092 = vadd.s32 %v1089, %v1084
%v1094 = vshll.u32 %v1089, 16
%v1095 = vshrl.u32 %v1089, 16
%v1096 = vor.u32 %v1095, %v1094
%v1097 = vxor.u32 %v1096, %v1092
%v1100 = vadd.s32 %v1097, %v1092
%v1104 = vadd.s32 %v1100, %v8
%v1106 = vshll.u32 %v1097, 24
%v1107 = vshrl.u32 %v1097, 8
%v1108 = vor.u32 %v1107, %v1106
%v1109 = vxor.u32 %v1108, %v1100
%v1112 = vadd.s32 %v1109, %v7
%v1116 = vadd.s32 4, %v1112
%v1120 = vadd.s32 %v1116, %v1104
%v1122 = vshll.u32 %v1116, 13
%v1123 = vshrl.u32 %v1116, 19
%v1124 = vor.u32 %v1123, %v1122
%v1125 = vxor.u32 %v1124, %v1120
%v1128 = vadd.s32 %v1125, %v1120
%v1130 = vshll.u32 %v1125, 15
%v1131 = vshrl.u32 %v1125, 17
%v1132 = vor.u32 %v1131, %v1130
%v1133 = vxor.u32 %v1132, %v1128
%v1136 = vadd.s32 %v1133, %v1128
%v1138 = vshll.u32 %v1133, 26
%v1139 = vshrl.u32 %v1133, 6
%v1140 = vor.u32 %v1139, %v1138
%v1141 = vxor.u32 %v1140, %v1136
%v1144 = vadd.s32 %v1141, %v1136
%v1148 = vadd.s32 %v1144, %v7
%v1150 = vshll.u32 %v1141, 6
%v1151 = vshrl.u32 %v1141, 26
%v1152 = vor.u32 %v1151, %v1150
%v1153 = vxor.u32 %v1152, %v1144
%v1156 = vadd.s32 %v1153, %v9
%v1160 = vadd.s32 5, %v1156
%v1162 = vxor.u32 %v1160, %v1148
%v1163 = vshrl.u32 %v1162, 9
%v1164 = vor.u32 1065353216, %v1163
%v1168 = vadd.f32 -1.0, %v1164
%v1172 = vmul.f32 2.0, %v1168
%v1176 = vadd.f32 -0.99999994, %v1172
%v1180 = vmax.f32 %v1176, -0.99999994
%v1182 = vand.u32 2147483647, %v1180
%vm1185 = vcmp.eq.f32.partialorder %v1182, 1.0
%v1190 = vmul.f32 inf, %v1180
%v1192 = vxor.u32 2147483648, %v1180
%v1195 = vmul.f32 %v1192, %v1180
%v1197 = vadd.f32 1.0, %v1195
%v1198 = vlog2.pop %v1197
%v1199 = vmul.f32 0.6931472, %v1198
%v1200 = vmul.f32 -0.5, %v1195
%v1201 = vadd.f32 1.0, %v1200
%v1202 = vmul.f32 %v1201, %v1195
%v1203 = vand.u32 2147483647, %v1195
%vm1204 = vcmp.lt.f32.partialorder %v1203, 0.0004427343
%v1205 = vsel /*vm=*/%vm1204, /*on_true_vy=*/%v1202, /*on_false_vx=*/%v1199
%v1206 = vxor.u32 2147483648, %v1205
%vm1209 = vcmp.lt.f32.partialorder %v1206, 5.0
%v1214 = vsel /*vm=*/%vm1209, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v3575
%v1218 = vsel /*vm=*/%vm1209, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v3576
%v1222 = vsel /*vm=*/%vm1209, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v3577
%v1226 = vsel /*vm=*/%vm1209, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v3578
%v1230 = vsel /*vm=*/%vm1209, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v3579
%v1234 = vsel /*vm=*/%vm1209, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v3580
%v1238 = vsel /*vm=*/%vm1209, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v3581
%v1242 = vsel /*vm=*/%vm1209, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v3582
%v1246 = vsel /*vm=*/%vm1209, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v3583
%v1250 = vadd.f32 -2.5, %v1206
%v1252 = vrsqrt.pop %v1206
%v1253 = vmul.f32 %v1252, %v1206
%vm1254 = vcmp.eq.f32.partialorder %v1206, inf
%v1255 = vsel /*vm=*/%vm1254, /*on_true_vy=*/%v1206, /*on_false_vx=*/%v1253
%vm1256 = vcmp.eq.f32.partialorder %v1206, 0.0
%v1257 = vand.u32 2147483648, %v1206
%v1258 = vsel /*vm=*/%vm1256, /*on_true_vy=*/%v1257, /*on_false_vx=*/%v1255
%v1261 = vadd.f32 -3.0, %v1258
%v1265 = vsel /*vm=*/%vm1209, /*on_true_vy=*/%v1250, /*on_false_vx=*/%v1261
%v1269 = vmul.f32 %v1265, %v1246
%v1273 = vadd.f32 %v1269, %v1242
%v1277 = vmul.f32 %v1273, %v1265
%v1281 = vadd.f32 %v1277, %v1238
%v1285 = vmul.f32 %v1281, %v1265
%v1289 = vadd.f32 %v1285, %v1234
%v1293 = vmul.f32 %v1289, %v1265
%v1297 = vadd.f32 %v1293, %v1230
%v1301 = vmul.f32 %v1297, %v1265
%v1305 = vadd.f32 %v1301, %v1226
%v1309 = vmul.f32 %v1305, %v1265
%v1313 = vadd.f32 %v1309, %v1222
%v1317 = vmul.f32 %v1313, %v1265
%v1321 = vadd.f32 %v1317, %v1218
%v1325 = vmul.f32 %v1321, %v1265
%v1329 = vadd.f32 %v1325, %v1214
%v1333 = vmul.f32 %v1329, %v1180
%v1337 = vsel /*vm=*/%vm1185, /*on_true_vy=*/%v1190, /*on_false_vx=*/%v1333
%v1341 = vmul.f32 1.4142135, %v1337
%1344 = vst [vmem:[#allocation0 + $0x10] sm:$0xff] /*vst_source=*/%v1341
%v1409 = vxor.u32 %v1408, %v1404
%v1412 = vadd.s32 %v1409, %v1404
%v1416 = vadd.s32 %v1412, %v8
%v1418 = vshll.u32 %v1409, 6
%v1419 = vshrl.u32 %v1409, 26
%v1420 = vor.u32 %v1419, %v1418
%v1421 = vxor.u32 %v1420, %v1412
%v1424 = vadd.s32 %v1421, %v7
%v1428 = vadd.s32 1, %v1424
%v1432 = vadd.s32 %v1428, %v1416
%v1434 = vshll.u32 %v1428, 17
%v1435 = vshrl.u32 %v1428, 15
%v1436 = vor.u32 %v1435, %v1434
%v1437 = vxor.u32 %v1436, %v1432
%v1440 = vadd.s32 %v1437, %v1432
%v1442 = vshll.u32 %v1437, 29
%v1443 = vshrl.u32 %v1437, 3
%v1444 = vor.u32 %v1443, %v1442
%v1445 = vxor.u32 %v1444, %v1440
%v1448 = vadd.s32 %v1445, %v1440
%v1450 = vshll.u32 %v1445, 16
%v1451 = vshrl.u32 %v1445, 16
%v1452 = vor.u32 %v1451, %v1450
%v1453 = vxor.u32 %v1452, %v1448
%v1456 = vadd.s32 %v1453, %v1448
%v1460 = vadd.s32 %v1456, %v7
%v1462 = vshll.u32 %v1453, 24
%v1463 = vshrl.u32 %v1453, 8
%v1464 = vor.u32 %v1463, %v1462
%v1465 = vxor.u32 %v1464, %v1456
%v1468 = vadd.s32 %v1465, %v9
%v1472 = vadd.s32 2, %v1468
%v1476 = vadd.s32 %v1472, %v1460
%v1478 = vshll.u32 %v1472, 13
%v1479 = vshrl.u32 %v1472, 19
%v1480 = vor.u32 %v1479, %v1478
%v1481 = vxor.u32 %v1480, %v1476
%v1484 = vadd.s32 %v1481, %v1476
%v1486 = vshll.u32 %v1481, 15
%v1487 = vshrl.u32 %v1481, 17
%v1488 = vor.u32 %v1487, %v1486
%v1489 = vxor.u32 %v1488, %v1484
%v1492 = vadd.s32 %v1489, %v1484
%v1494 = vshll.u32 %v1489, 26
%v1495 = vshrl.u32 %v1489, 6
%v1496 = vor.u32 %v1495, %v1494
%v1497 = vxor.u32 %v1496, %v1492
%v1500 = vadd.s32 %v1497, %v1492
%v1504 = vadd.s32 %v1500, %v9
%v1506 = vshll.u32 %v1497, 6
%v1507 = vshrl.u32 %v1497, 26
%v1508 = vor.u32 %v1507, %v1506
%v1509 = vxor.u32 %v1508, %v1500
%v1512 = vadd.s32 %v1509, %v8
%v1516 = vadd.s32 3, %v1512
%v1520 = vadd.s32 %v1516, %v1504
%v1522 = vshll.u32 %v1516, 17
%v1523 = vshrl.u32 %v1516, 15
%v1524 = vor.u32 %v1523, %v1522
%v1525 = vxor.u32 %v1524, %v1520
%v1528 = vadd.s32 %v1525, %v1520
%v1530 = vshll.u32 %v1525, 29
%v1531 = vshrl.u32 %v1525, 3
%v1532 = vor.u32 %v1531, %v1530
%v1533 = vxor.u32 %v1532, %v1528
%v1536 = vadd.s32 %v1533, %v1528
%v1538 = vshll.u32 %v1533, 16
%v1539 = vshrl.u32 %v1533, 16
%v1540 = vor.u32 %v1539, %v1538
%v1541 = vxor.u32 %v1540, %v1536
%v1544 = vadd.s32 %v1541, %v1536
%v1548 = vadd.s32 %v1544, %v8
%v1550 = vshll.u32 %v1541, 24
%v1551 = vshrl.u32 %v1541, 8
%v1552 = vor.u32 %v1551, %v1550
%v1553 = vxor.u32 %v1552, %v1544
%v1556 = vadd.s32 %v1553, %v7
%v1560 = vadd.s32 4, %v1556
%v1564 = vadd.s32 %v1560, %v1548
%v1566 = vshll.u32 %v1560, 13
%v1567 = vshrl.u32 %v1560, 19
%v1568 = vor.u32 %v1567, %v1566
%v1569 = vxor.u32 %v1568, %v1564
%v1572 = vadd.s32 %v1569, %v1564
%v1574 = vshll.u32 %v1569, 15
%v1575 = vshrl.u32 %v1569, 17
%v1576 = vor.u32 %v1575, %v1574
%v1577 = vxor.u32 %v1576, %v1572
%v1580 = vadd.s32 %v1577, %v1572
%v1582 = vshll.u32 %v1577, 26
%v1583 = vshrl.u32 %v1577, 6
%v1584 = vor.u32 %v1583, %v1582
%v1585 = vxor.u32 %v1584, %v1580
%v1588 = vadd.s32 %v1585, %v1580
%v1592 = vadd.s32 %v1588, %v7
%v1594 = vshll.u32 %v1585, 6
%v1595 = vshrl.u32 %v1585, 26
%v1596 = vor.u32 %v1595, %v1594
%v1597 = vxor.u32 %v1596, %v1588
%v1600 = vadd.s32 %v1597, %v9
%v1604 = vadd.s32 5, %v1600
%v1606 = vxor.u32 %v1604, %v1592
%v1607 = vshrl.u32 %v1606, 9
%v1608 = vor.u32 1065353216, %v1607
%v1612 = vadd.f32 -1.0, %v1608
%v1616 = vmul.f32 2.0, %v1612
%v1620 = vadd.f32 -0.99999994, %v1616
%v1624 = vmax.f32 %v1620, -0.99999994
%v1626 = vand.u32 2147483647, %v1624
%vm1629 = vcmp.eq.f32.partialorder %v1626, 1.0
%v1634 = vmul.f32 inf, %v1624
%v1636 = vxor.u32 2147483648, %v1624
%v1639 = vmul.f32 %v1636, %v1624
%v1641 = vadd.f32 1.0, %v1639
%v1642 = vlog2.pop %v1641
%v1643 = vmul.f32 0.6931472, %v1642
%v1644 = vmul.f32 -0.5, %v1639
%v1645 = vadd.f32 1.0, %v1644
%v1646 = vmul.f32 %v1645, %v1639
%v1647 = vand.u32 2147483647, %v1639
%vm1648 = vcmp.lt.f32.partialorder %v1647, 0.0004427343
%v1649 = vsel /*vm=*/%vm1648, /*on_true_vy=*/%v1646, /*on_false_vx=*/%v1643
%v1650 = vxor.u32 2147483648, %v1649
%vm1653 = vcmp.lt.f32.partialorder %v1650, 5.0
%v1658 = vsel /*vm=*/%vm1653, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v3575
%v1662 = vsel /*vm=*/%vm1653, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v3576
%v1666 = vsel /*vm=*/%vm1653, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v3577
%v1670 = vsel /*vm=*/%vm1653, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v3578
%v1674 = vsel /*vm=*/%vm1653, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v3579
%v1678 = vsel /*vm=*/%vm1653, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v3580
%v1682 = vsel /*vm=*/%vm1653, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v3581
%v1686 = vsel /*vm=*/%vm1653, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v3582
%v1690 = vsel /*vm=*/%vm1653, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v3583
%v1694 = vadd.f32 -2.5, %v1650
%v1696 = vrsqrt.pop %v1650
%v1697 = vmul.f32 %v1696, %v1650
%vm1698 = vcmp.eq.f32.partialorder %v1650, inf
%v1699 = vsel /*vm=*/%vm1698, /*on_true_vy=*/%v1650, /*on_false_vx=*/%v1697
%vm1700 = vcmp.eq.f32.partialorder %v1650, 0.0
%v1701 = vand.u32 2147483648, %v1650
%v1702 = vsel /*vm=*/%vm1700, /*on_true_vy=*/%v1701, /*on_false_vx=*/%v1699
%v1705 = vadd.f32 -3.0, %v1702
%v1709 = vsel /*vm=*/%vm1653, /*on_true_vy=*/%v1694, /*on_false_vx=*/%v1705
%v1713 = vmul.f32 %v1709, %v1690
%v1717 = vadd.f32 %v1713, %v1686
%v1721 = vmul.f32 %v1717, %v1709
%v1725 = vadd.f32 %v1721, %v1682
%v1729 = vmul.f32 %v1725, %v1709
%v1733 = vadd.f32 %v1729, %v1678
%v1737 = vmul.f32 %v1733, %v1709
%v1741 = vadd.f32 %v1737, %v1674
%v1745 = vmul.f32 %v1741, %v1709
%v1749 = vadd.f32 %v1745, %v1670
%v1753 = vmul.f32 %v1749, %v1709
%v1757 = vadd.f32 %v1753, %v1666
%v1761 = vmul.f32 %v1757, %v1709
%v1765 = vadd.f32 %v1761, %v1662
%v1769 = vmul.f32 %v1765, %v1709
%v1773 = vadd.f32 %v1769, %v1658
%v1777 = vmul.f32 %v1773, %v1624
%v1781 = vsel /*vm=*/%vm1629, /*on_true_vy=*/%v1634, /*on_false_vx=*/%v1777
%v1785 = vmul.f32 1.4142135, %v1781
%1788 = vst [vmem:[#allocation0 + $0x18] sm:$0xff] /*vst_source=*/%v1785
%v1848 = vadd.s32 %v1845, %v1840
%v1850 = vshll.u32 %v1845, 26
%v1851 = vshrl.u32 %v1845, 6
%v1852 = vor.u32 %v1851, %v1850
%v1853 = vxor.u32 %v1852, %v1848
%v1856 = vadd.s32 %v1853, %v1848
%v1860 = vadd.s32 %v1856, %v8
%v1862 = vshll.u32 %v1853, 6
%v1863 = vshrl.u32 %v1853, 26
%v1864 = vor.u32 %v1863, %v1862
%v1865 = vxor.u32 %v1864, %v1856
%v1868 = vadd.s32 %v1865, %v7
%v1872 = vadd.s32 1, %v1868
%v1876 = vadd.s32 %v1872, %v1860
%v1878 = vshll.u32 %v1872, 17
%v1879 = vshrl.u32 %v1872, 15
%v1880 = vor.u32 %v1879, %v1878
%v1881 = vxor.u32 %v1880, %v1876
%v1884 = vadd.s32 %v1881, %v1876
%v1886 = vshll.u32 %v1881, 29
%v1887 = vshrl.u32 %v1881, 3
%v1888 = vor.u32 %v1887, %v1886
%v1889 = vxor.u32 %v1888, %v1884
%v1892 = vadd.s32 %v1889, %v1884
%v1894 = vshll.u32 %v1889, 16
%v1895 = vshrl.u32 %v1889, 16
%v1896 = vor.u32 %v1895, %v1894
%v1897 = vxor.u32 %v1896, %v1892
%v1900 = vadd.s32 %v1897, %v1892
%v1904 = vadd.s32 %v1900, %v7
%v1906 = vshll.u32 %v1897, 24
%v1907 = vshrl.u32 %v1897, 8
%v1908 = vor.u32 %v1907, %v1906
%v1909 = vxor.u32 %v1908, %v1900
%v1912 = vadd.s32 %v1909, %v9
%v1916 = vadd.s32 2, %v1912
%v1920 = vadd.s32 %v1916, %v1904
%v1922 = vshll.u32 %v1916, 13
%v1923 = vshrl.u32 %v1916, 19
%v1924 = vor.u32 %v1923, %v1922
%v1925 = vxor.u32 %v1924, %v1920
%v1928 = vadd.s32 %v1925, %v1920
%v1930 = vshll.u32 %v1925, 15
%v1931 = vshrl.u32 %v1925, 17
%v1932 = vor.u32 %v1931, %v1930
%v1933 = vxor.u32 %v1932, %v1928
%v1936 = vadd.s32 %v1933, %v1928
%v1938 = vshll.u32 %v1933, 26
%v1939 = vshrl.u32 %v1933, 6
%v1940 = vor.u32 %v1939, %v1938
%v1941 = vxor.u32 %v1940, %v1936
%v1944 = vadd.s32 %v1941, %v1936
%v1948 = vadd.s32 %v1944, %v9
%v1950 = vshll.u32 %v1941, 6
%v1951 = vshrl.u32 %v1941, 26
%v1952 = vor.u32 %v1951, %v1950
%v1953 = vxor.u32 %v1952, %v1944
%v1956 = vadd.s32 %v1953, %v8
%v1960 = vadd.s32 3, %v1956
%v1964 = vadd.s32 %v1960, %v1948
%v1966 = vshll.u32 %v1960, 17
%v1967 = vshrl.u32 %v1960, 15
%v1968 = vor.u32 %v1967, %v1966
%v1969 = vxor.u32 %v1968, %v1964
%v1972 = vadd.s32 %v1969, %v1964
%v1974 = vshll.u32 %v1969, 29
%v1975 = vshrl.u32 %v1969, 3
%v1976 = vor.u32 %v1975, %v1974
%v1977 = vxor.u32 %v1976, %v1972
%v1980 = vadd.s32 %v1977, %v1972
%v1982 = vshll.u32 %v1977, 16
%v1983 = vshrl.u32 %v1977, 16
%v1984 = vor.u32 %v1983, %v1982
%v1985 = vxor.u32 %v1984, %v1980
%v1988 = vadd.s32 %v1985, %v1980
%v1992 = vadd.s32 %v1988, %v8
%v1994 = vshll.u32 %v1985, 24
%v1995 = vshrl.u32 %v1985, 8
%v1996 = vor.u32 %v1995, %v1994
%v1997 = vxor.u32 %v1996, %v1988
%v2000 = vadd.s32 %v1997, %v7
%v2004 = vadd.s32 4, %v2000
%v2008 = vadd.s32 %v2004, %v1992
%v2010 = vshll.u32 %v2004, 13
%v2011 = vshrl.u32 %v2004, 19
%v2012 = vor.u32 %v2011, %v2010
%v2013 = vxor.u32 %v2012, %v2008
%v2016 = vadd.s32 %v2013, %v2008
%v2018 = vshll.u32 %v2013, 15
%v2019 = vshrl.u32 %v2013, 17
%v2020 = vor.u32 %v2019, %v2018
%v2021 = vxor.u32 %v2020, %v2016
%v2024 = vadd.s32 %v2021, %v2016
%v2026 = vshll.u32 %v2021, 26
%v2027 = vshrl.u32 %v2021, 6
%v2028 = vor.u32 %v2027, %v2026
%v2029 = vxor.u32 %v2028, %v2024
%v2032 = vadd.s32 %v2029, %v2024
%v2036 = vadd.s32 %v2032, %v7
%v2038 = vshll.u32 %v2029, 6
%v2039 = vshrl.u32 %v2029, 26
%v2040 = vor.u32 %v2039, %v2038
%v2041 = vxor.u32 %v2040, %v2032
%v2044 = vadd.s32 %v2041, %v9
%v2048 = vadd.s32 5, %v2044
%v2050 = vxor.u32 %v2048, %v2036
%v2051 = vshrl.u32 %v2050, 9
%v2052 = vor.u32 1065353216, %v2051
%v2056 = vadd.f32 -1.0, %v2052
%v2060 = vmul.f32 2.0, %v2056
%v2064 = vadd.f32 -0.99999994, %v2060
%v2068 = vmax.f32 %v2064, -0.99999994
%v2070 = vand.u32 2147483647, %v2068
%vm2073 = vcmp.eq.f32.partialorder %v2070, 1.0
%v2078 = vmul.f32 inf, %v2068
%v2080 = vxor.u32 2147483648, %v2068
%v2083 = vmul.f32 %v2080, %v2068
%v2085 = vadd.f32 1.0, %v2083
%v2086 = vlog2.pop %v2085
%v2087 = vmul.f32 0.6931472, %v2086
%v2088 = vmul.f32 -0.5, %v2083
%v2089 = vadd.f32 1.0, %v2088
%v2090 = vmul.f32 %v2089, %v2083
%v2091 = vand.u32 2147483647, %v2083
%vm2092 = vcmp.lt.f32.partialorder %v2091, 0.0004427343
%v2093 = vsel /*vm=*/%vm2092, /*on_true_vy=*/%v2090, /*on_false_vx=*/%v2087
%v2094 = vxor.u32 2147483648, %v2093
%vm2097 = vcmp.lt.f32.partialorder %v2094, 5.0
%v2102 = vsel /*vm=*/%vm2097, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v3575
%v2106 = vsel /*vm=*/%vm2097, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v3576
%v2110 = vsel /*vm=*/%vm2097, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v3577
%v2114 = vsel /*vm=*/%vm2097, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v3578
%v2118 = vsel /*vm=*/%vm2097, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v3579
%v2122 = vsel /*vm=*/%vm2097, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v3580
%v2126 = vsel /*vm=*/%vm2097, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v3581
%v2130 = vsel /*vm=*/%vm2097, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v3582
%v2134 = vsel /*vm=*/%vm2097, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v3583
%v2138 = vadd.f32 -2.5, %v2094
%v2140 = vrsqrt.pop %v2094
%v2141 = vmul.f32 %v2140, %v2094
%vm2142 = vcmp.eq.f32.partialorder %v2094, inf
%v2143 = vsel /*vm=*/%vm2142, /*on_true_vy=*/%v2094, /*on_false_vx=*/%v2141
%vm2144 = vcmp.eq.f32.partialorder %v2094, 0.0
%v2145 = vand.u32 2147483648, %v2094
%v2146 = vsel /*vm=*/%vm2144, /*on_true_vy=*/%v2145, /*on_false_vx=*/%v2143
%v2149 = vadd.f32 -3.0, %v2146
%v2153 = vsel /*vm=*/%vm2097, /*on_true_vy=*/%v2138, /*on_false_vx=*/%v2149
%v2157 = vmul.f32 %v2153, %v2134
%v2161 = vadd.f32 %v2157, %v2130
%v2165 = vmul.f32 %v2161, %v2153
%v2169 = vadd.f32 %v2165, %v2126
%v2173 = vmul.f32 %v2169, %v2153
%v2177 = vadd.f32 %v2173, %v2122
%v2181 = vmul.f32 %v2177, %v2153
%v2185 = vadd.f32 %v2181, %v2118
%v2189 = vmul.f32 %v2185, %v2153
%v2193 = vadd.f32 %v2189, %v2114
%v2197 = vmul.f32 %v2193, %v2153
%v2201 = vadd.f32 %v2197, %v2110
%v2205 = vmul.f32 %v2201, %v2153
%v2209 = vadd.f32 %v2205, %v2106
%v2213 = vmul.f32 %v2209, %v2153
%v2217 = vadd.f32 %v2213, %v2102
%v2221 = vmul.f32 %v2217, %v2068
%v2225 = vsel /*vm=*/%vm2073, /*on_true_vy=*/%v2078, /*on_false_vx=*/%v2221
%v2229 = vmul.f32 1.4142135, %v2225
%2232 = vst [vmem:[#allocation0 + $0x20] sm:$0xff] /*vst_source=*/%v2229
%v2288 = vor.u32 %v2287, %v2286
%v2289 = vxor.u32 %v2288, %v2284
%v2292 = vadd.s32 %v2289, %v2284
%v2294 = vshll.u32 %v2289, 26
%v2295 = vshrl.u32 %v2289, 6
%v2296 = vor.u32 %v2295, %v2294
%v2297 = vxor.u32 %v2296, %v2292
%v2300 = vadd.s32 %v2297, %v2292
%v2304 = vadd.s32 %v2300, %v8
%v2306 = vshll.u32 %v2297, 6
%v2307 = vshrl.u32 %v2297, 26
%v2308 = vor.u32 %v2307, %v2306
%v2309 = vxor.u32 %v2308, %v2300
%v2312 = vadd.s32 %v2309, %v7
%v2316 = vadd.s32 1, %v2312
%v2320 = vadd.s32 %v2316, %v2304
%v2322 = vshll.u32 %v2316, 17
%v2323 = vshrl.u32 %v2316, 15
%v2324 = vor.u32 %v2323, %v2322
%v2325 = vxor.u32 %v2324, %v2320
%v2328 = vadd.s32 %v2325, %v2320
%v2330 = vshll.u32 %v2325, 29
%v2331 = vshrl.u32 %v2325, 3
%v2332 = vor.u32 %v2331, %v2330
%v2333 = vxor.u32 %v2332, %v2328
%v2336 = vadd.s32 %v2333, %v2328
%v2338 = vshll.u32 %v2333, 16
%v2339 = vshrl.u32 %v2333, 16
%v2340 = vor.u32 %v2339, %v2338
%v2341 = vxor.u32 %v2340, %v2336
%v2344 = vadd.s32 %v2341, %v2336
%v2348 = vadd.s32 %v2344, %v7
%v2350 = vshll.u32 %v2341, 24
%v2351 = vshrl.u32 %v2341, 8
%v2352 = vor.u32 %v2351, %v2350
%v2353 = vxor.u32 %v2352, %v2344
%v2356 = vadd.s32 %v2353, %v9
%v2360 = vadd.s32 2, %v2356
%v2364 = vadd.s32 %v2360, %v2348
%v2366 = vshll.u32 %v2360, 13
%v2367 = vshrl.u32 %v2360, 19
%v2368 = vor.u32 %v2367, %v2366
%v2369 = vxor.u32 %v2368, %v2364
%v2372 = vadd.s32 %v2369, %v2364
%v2374 = vshll.u32 %v2369, 15
%v2375 = vshrl.u32 %v2369, 17
%v2376 = vor.u32 %v2375, %v2374
%v2377 = vxor.u32 %v2376, %v2372
%v2380 = vadd.s32 %v2377, %v2372
%v2382 = vshll.u32 %v2377, 26
%v2383 = vshrl.u32 %v2377, 6
%v2384 = vor.u32 %v2383, %v2382
%v2385 = vxor.u32 %v2384, %v2380
%v2388 = vadd.s32 %v2385, %v2380
%v2392 = vadd.s32 %v2388, %v9
%v2394 = vshll.u32 %v2385, 6
%v2395 = vshrl.u32 %v2385, 26
%v2396 = vor.u32 %v2395, %v2394
%v2397 = vxor.u32 %v2396, %v2388
%v2400 = vadd.s32 %v2397, %v8
%v2404 = vadd.s32 3, %v2400
%v2408 = vadd.s32 %v2404, %v2392
%v2410 = vshll.u32 %v2404, 17
%v2411 = vshrl.u32 %v2404, 15
%v2412 = vor.u32 %v2411, %v2410
%v2413 = vxor.u32 %v2412, %v2408
%v2416 = vadd.s32 %v2413, %v2408
%v2418 = vshll.u32 %v2413, 29
%v2419 = vshrl.u32 %v2413, 3
%v2420 = vor.u32 %v2419, %v2418
%v2421 = vxor.u32 %v2420, %v2416
%v2424 = vadd.s32 %v2421, %v2416
%v2426 = vshll.u32 %v2421, 16
%v2427 = vshrl.u32 %v2421, 16
%v2428 = vor.u32 %v2427, %v2426
%v2429 = vxor.u32 %v2428, %v2424
%v2432 = vadd.s32 %v2429, %v2424
%v2436 = vadd.s32 %v2432, %v8
%v2438 = vshll.u32 %v2429, 24
%v2439 = vshrl.u32 %v2429, 8
%v2440 = vor.u32 %v2439, %v2438
%v2441 = vxor.u32 %v2440, %v2432
%v2444 = vadd.s32 %v2441, %v7
%v2448 = vadd.s32 4, %v2444
%v2452 = vadd.s32 %v2448, %v2436
%v2454 = vshll.u32 %v2448, 13
%v2455 = vshrl.u32 %v2448, 19
%v2456 = vor.u32 %v2455, %v2454
%v2457 = vxor.u32 %v2456, %v2452
%v2460 = vadd.s32 %v2457, %v2452
%v2462 = vshll.u32 %v2457, 15
%v2463 = vshrl.u32 %v2457, 17
%v2464 = vor.u32 %v2463, %v2462
%v2465 = vxor.u32 %v2464, %v2460
%v2468 = vadd.s32 %v2465, %v2460
%v2470 = vshll.u32 %v2465, 26
%v2471 = vshrl.u32 %v2465, 6
%v2472 = vor.u32 %v2471, %v2470
%v2473 = vxor.u32 %v2472, %v2468
%v2476 = vadd.s32 %v2473, %v2468
%v2480 = vadd.s32 %v2476, %v7
%v2482 = vshll.u32 %v2473, 6
%v2483 = vshrl.u32 %v2473, 26
%v2484 = vor.u32 %v2483, %v2482
%v2485 = vxor.u32 %v2484, %v2476
%v2488 = vadd.s32 %v2485, %v9
%v2492 = vadd.s32 5, %v2488
%v2494 = vxor.u32 %v2492, %v2480
%v2495 = vshrl.u32 %v2494, 9
%v2496 = vor.u32 1065353216, %v2495
%v2500 = vadd.f32 -1.0, %v2496
%v2504 = vmul.f32 2.0, %v2500
%v2508 = vadd.f32 -0.99999994, %v2504
%v2512 = vmax.f32 %v2508, -0.99999994
%v2514 = vand.u32 2147483647, %v2512
%vm2517 = vcmp.eq.f32.partialorder %v2514, 1.0
%v2522 = vmul.f32 inf, %v2512
%v2524 = vxor.u32 2147483648, %v2512
%v2527 = vmul.f32 %v2524, %v2512
%v2529 = vadd.f32 1.0, %v2527
%v2530 = vlog2.pop %v2529
%v2531 = vmul.f32 0.6931472, %v2530
%v2532 = vmul.f32 -0.5, %v2527
%v2533 = vadd.f32 1.0, %v2532
%v2534 = vmul.f32 %v2533, %v2527
%v2535 = vand.u32 2147483647, %v2527
%vm2536 = vcmp.lt.f32.partialorder %v2535, 0.0004427343
%v2537 = vsel /*vm=*/%vm2536, /*on_true_vy=*/%v2534, /*on_false_vx=*/%v2531
%v2538 = vxor.u32 2147483648, %v2537
%vm2541 = vcmp.lt.f32.partialorder %v2538, 5.0
%v2546 = vsel /*vm=*/%vm2541, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v3575
%v2550 = vsel /*vm=*/%vm2541, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v3576
%v2554 = vsel /*vm=*/%vm2541, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v3577
%v2558 = vsel /*vm=*/%vm2541, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v3578
%v2562 = vsel /*vm=*/%vm2541, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v3579
%v2566 = vsel /*vm=*/%vm2541, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v3580
%v2570 = vsel /*vm=*/%vm2541, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v3581
%v2574 = vsel /*vm=*/%vm2541, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v3582
%v2578 = vsel /*vm=*/%vm2541, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v3583
%v2582 = vadd.f32 -2.5, %v2538
%v2584 = vrsqrt.pop %v2538
%v2585 = vmul.f32 %v2584, %v2538
%vm2586 = vcmp.eq.f32.partialorder %v2538, inf
%v2587 = vsel /*vm=*/%vm2586, /*on_true_vy=*/%v2538, /*on_false_vx=*/%v2585
%vm2588 = vcmp.eq.f32.partialorder %v2538, 0.0
%v2589 = vand.u32 2147483648, %v2538
%v2590 = vsel /*vm=*/%vm2588, /*on_true_vy=*/%v2589, /*on_false_vx=*/%v2587
%v2593 = vadd.f32 -3.0, %v2590
%v2597 = vsel /*vm=*/%vm2541, /*on_true_vy=*/%v2582, /*on_false_vx=*/%v2593
%v2601 = vmul.f32 %v2597, %v2578
%v2605 = vadd.f32 %v2601, %v2574
%v2609 = vmul.f32 %v2605, %v2597
%v2613 = vadd.f32 %v2609, %v2570
%v2617 = vmul.f32 %v2613, %v2597
%v2621 = vadd.f32 %v2617, %v2566
%v2625 = vmul.f32 %v2621, %v2597
%v2629 = vadd.f32 %v2625, %v2562
%v2633 = vmul.f32 %v2629, %v2597
%v2637 = vadd.f32 %v2633, %v2558
%v2641 = vmul.f32 %v2637, %v2597
%v2645 = vadd.f32 %v2641, %v2554
%v2649 = vmul.f32 %v2645, %v2597
%v2653 = vadd.f32 %v2649, %v2550
%v2657 = vmul.f32 %v2653, %v2597
%v2661 = vadd.f32 %v2657, %v2546
%v2665 = vmul.f32 %v2661, %v2512
%v2669 = vsel /*vm=*/%vm2517, /*on_true_vy=*/%v2522, /*on_false_vx=*/%v2665
%v2673 = vmul.f32 1.4142135, %v2669
%2676 = vst [vmem:[#allocation0 + $0x28] sm:$0xff] /*vst_source=*/%v2673
%v2725 = vxor.u32 %v2724, %v2720
%v2728 = vadd.s32 %v2725, %v2720
%v2730 = vshll.u32 %v2725, 15
%v2731 = vshrl.u32 %v2725, 17
%v2732 = vor.u32 %v2731, %v2730
%v2733 = vxor.u32 %v2732, %v2728
%v2736 = vadd.s32 %v2733, %v2728
%v2738 = vshll.u32 %v2733, 26
%v2739 = vshrl.u32 %v2733, 6
%v2740 = vor.u32 %v2739, %v2738
%v2741 = vxor.u32 %v2740, %v2736
%v2744 = vadd.s32 %v2741, %v2736
%v2748 = vadd.s32 %v2744, %v8
%v2750 = vshll.u32 %v2741, 6
%v2751 = vshrl.u32 %v2741, 26
%v2752 = vor.u32 %v2751, %v2750
%v2753 = vxor.u32 %v2752, %v2744
%v2756 = vadd.s32 %v2753, %v7
%v2760 = vadd.s32 1, %v2756
%v2764 = vadd.s32 %v2760, %v2748
%v2766 = vshll.u32 %v2760, 17
%v2767 = vshrl.u32 %v2760, 15
%v2768 = vor.u32 %v2767, %v2766
%v2769 = vxor.u32 %v2768, %v2764
%v2772 = vadd.s32 %v2769, %v2764
%v2774 = vshll.u32 %v2769, 29
%v2775 = vshrl.u32 %v2769, 3
%v2776 = vor.u32 %v2775, %v2774
%v2777 = vxor.u32 %v2776, %v2772
%v2780 = vadd.s32 %v2777, %v2772
%v2782 = vshll.u32 %v2777, 16
%v2783 = vshrl.u32 %v2777, 16
%v2784 = vor.u32 %v2783, %v2782
%v2785 = vxor.u32 %v2784, %v2780
%v2788 = vadd.s32 %v2785, %v2780
%v2792 = vadd.s32 %v2788, %v7
%v2794 = vshll.u32 %v2785, 24
%v2795 = vshrl.u32 %v2785, 8
%v2796 = vor.u32 %v2795, %v2794
%v2797 = vxor.u32 %v2796, %v2788
%v2800 = vadd.s32 %v2797, %v9
%v2804 = vadd.s32 2, %v2800
%v2808 = vadd.s32 %v2804, %v2792
%v2810 = vshll.u32 %v2804, 13
%v2811 = vshrl.u32 %v2804, 19
%v2812 = vor.u32 %v2811, %v2810
%v2813 = vxor.u32 %v2812, %v2808
%v2816 = vadd.s32 %v2813, %v2808
%v2818 = vshll.u32 %v2813, 15
%v2819 = vshrl.u32 %v2813, 17
%v2820 = vor.u32 %v2819, %v2818
%v2821 = vxor.u32 %v2820, %v2816
%v2824 = vadd.s32 %v2821, %v2816
%v2826 = vshll.u32 %v2821, 26
%v2827 = vshrl.u32 %v2821, 6
%v2828 = vor.u32 %v2827, %v2826
%v2829 = vxor.u32 %v2828, %v2824
%v2832 = vadd.s32 %v2829, %v2824
%v2836 = vadd.s32 %v2832, %v9
%v2838 = vshll.u32 %v2829, 6
%v2839 = vshrl.u32 %v2829, 26
%v2840 = vor.u32 %v2839, %v2838
%v2841 = vxor.u32 %v2840, %v2832
%v2844 = vadd.s32 %v2841, %v8
%v2848 = vadd.s32 3, %v2844
%v2852 = vadd.s32 %v2848, %v2836
%v2854 = vshll.u32 %v2848, 17
%v2855 = vshrl.u32 %v2848, 15
%v2856 = vor.u32 %v2855, %v2854
%v2857 = vxor.u32 %v2856, %v2852
%v2860 = vadd.s32 %v2857, %v2852
%v2862 = vshll.u32 %v2857, 29
%v2863 = vshrl.u32 %v2857, 3
%v2864 = vor.u32 %v2863, %v2862
%v2865 = vxor.u32 %v2864, %v2860
%v2868 = vadd.s32 %v2865, %v2860
%v2870 = vshll.u32 %v2865, 16
%v2871 = vshrl.u32 %v2865, 16
%v2872 = vor.u32 %v2871, %v2870
%v2873 = vxor.u32 %v2872, %v2868
%v2876 = vadd.s32 %v2873, %v2868
%v2880 = vadd.s32 %v2876, %v8
%v2882 = vshll.u32 %v2873, 24
%v2883 = vshrl.u32 %v2873, 8
%v2884 = vor.u32 %v2883, %v2882
%v2885 = vxor.u32 %v2884, %v2876
%v2888 = vadd.s32 %v2885, %v7
%v2892 = vadd.s32 4, %v2888
%v2896 = vadd.s32 %v2892, %v2880
%v2898 = vshll.u32 %v2892, 13
%v2899 = vshrl.u32 %v2892, 19
%v2900 = vor.u32 %v2899, %v2898
%v2901 = vxor.u32 %v2900, %v2896
%v2904 = vadd.s32 %v2901, %v2896
%v2906 = vshll.u32 %v2901, 15
%v2907 = vshrl.u32 %v2901, 17
%v2908 = vor.u32 %v2907, %v2906
%v2909 = vxor.u32 %v2908, %v2904
%v2912 = vadd.s32 %v2909, %v2904
%v2914 = vshll.u32 %v2909, 26
%v2915 = vshrl.u32 %v2909, 6
%v2916 = vor.u32 %v2915, %v2914
%v2917 = vxor.u32 %v2916, %v2912
%v2920 = vadd.s32 %v2917, %v2912
%v2924 = vadd.s32 %v2920, %v7
%v2926 = vshll.u32 %v2917, 6
%v2927 = vshrl.u32 %v2917, 26
%v2928 = vor.u32 %v2927, %v2926
%v2929 = vxor.u32 %v2928, %v2920
%v2932 = vadd.s32 %v2929, %v9
%v2936 = vadd.s32 5, %v2932
%v2938 = vxor.u32 %v2936, %v2924
%v2939 = vshrl.u32 %v2938, 9
%v2940 = vor.u32 1065353216, %v2939
%v2944 = vadd.f32 -1.0, %v2940
%v2948 = vmul.f32 2.0, %v2944
%v2952 = vadd.f32 -0.99999994, %v2948
%v2956 = vmax.f32 %v2952, -0.99999994
%v2958 = vand.u32 2147483647, %v2956
%vm2961 = vcmp.eq.f32.partialorder %v2958, 1.0
%v2966 = vmul.f32 inf, %v2956
%v2968 = vxor.u32 2147483648, %v2956
%v2971 = vmul.f32 %v2968, %v2956
%v2973 = vadd.f32 1.0, %v2971
%v2974 = vlog2.pop %v2973
%v2975 = vmul.f32 0.6931472, %v2974
%v2976 = vmul.f32 -0.5, %v2971
%v2977 = vadd.f32 1.0, %v2976
%v2978 = vmul.f32 %v2977, %v2971
%v2979 = vand.u32 2147483647, %v2971
%vm2980 = vcmp.lt.f32.partialorder %v2979, 0.0004427343
%v2981 = vsel /*vm=*/%vm2980, /*on_true_vy=*/%v2978, /*on_false_vx=*/%v2975
%v2982 = vxor.u32 2147483648, %v2981
%vm2985 = vcmp.lt.f32.partialorder %v2982, 5.0
%v2990 = vsel /*vm=*/%vm2985, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v3575
%v2994 = vsel /*vm=*/%vm2985, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v3576
%v2998 = vsel /*vm=*/%vm2985, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v3577
%v3002 = vsel /*vm=*/%vm2985, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v3578
%v3006 = vsel /*vm=*/%vm2985, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v3579
%v3010 = vsel /*vm=*/%vm2985, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v3580
%v3014 = vsel /*vm=*/%vm2985, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v3581
%v3018 = vsel /*vm=*/%vm2985, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v3582
%v3022 = vsel /*vm=*/%vm2985, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v3583
%v3026 = vadd.f32 -2.5, %v2982
%v3028 = vrsqrt.pop %v2982
%v3029 = vmul.f32 %v3028, %v2982
%vm3030 = vcmp.eq.f32.partialorder %v2982, inf
%v3031 = vsel /*vm=*/%vm3030, /*on_true_vy=*/%v2982, /*on_false_vx=*/%v3029
%vm3032 = vcmp.eq.f32.partialorder %v2982, 0.0
%v3033 = vand.u32 2147483648, %v2982
%v3034 = vsel /*vm=*/%vm3032, /*on_true_vy=*/%v3033, /*on_false_vx=*/%v3031
%v3037 = vadd.f32 -3.0, %v3034
%v3041 = vsel /*vm=*/%vm2985, /*on_true_vy=*/%v3026, /*on_false_vx=*/%v3037
%v3045 = vmul.f32 %v3041, %v3022
%v3049 = vadd.f32 %v3045, %v3018
%v3053 = vmul.f32 %v3049, %v3041
%v3057 = vadd.f32 %v3053, %v3014
%v3061 = vmul.f32 %v3057, %v3041
%v3065 = vadd.f32 %v3061, %v3010
%v3069 = vmul.f32 %v3065, %v3041
%v3073 = vadd.f32 %v3069, %v3006
%v3077 = vmul.f32 %v3073, %v3041
%v3081 = vadd.f32 %v3077, %v3002
%v3085 = vmul.f32 %v3081, %v3041
%v3089 = vadd.f32 %v3085, %v2998
%v3093 = vmul.f32 %v3089, %v3041
%v3097 = vadd.f32 %v3093, %v2994
%v3101 = vmul.f32 %v3097, %v3041
%v3105 = vadd.f32 %v3101, %v2990
%v3109 = vmul.f32 %v3105, %v2956
%v3113 = vsel /*vm=*/%vm2961, /*on_true_vy=*/%v2966, /*on_false_vx=*/%v3109
%v3117 = vmul.f32 1.4142135, %v3113
%3120 = vst [vmem:[#allocation0 + $0x30] sm:$0xff] /*vst_source=*/%v3117
%v3156 = vadd.s32 %v3151, %v9
%v3164 = vadd.s32 %v3160, %v3156
%v3169 = vxor.u32 %v3168, %v3164
%v3172 = vadd.s32 %v3169, %v3164
%v3174 = vshll.u32 %v3169, 15
%v3175 = vshrl.u32 %v3169, 17
%v3176 = vor.u32 %v3175, %v3174
%v3177 = vxor.u32 %v3176, %v3172
%v3180 = vadd.s32 %v3177, %v3172
%v3182 = vshll.u32 %v3177, 26
%v3183 = vshrl.u32 %v3177, 6
%v3184 = vor.u32 %v3183, %v3182
%v3185 = vxor.u32 %v3184, %v3180
%v3188 = vadd.s32 %v3185, %v3180
%v3192 = vadd.s32 %v3188, %v8
%v3194 = vshll.u32 %v3185, 6
%v3195 = vshrl.u32 %v3185, 26
%v3196 = vor.u32 %v3195, %v3194
%v3197 = vxor.u32 %v3196, %v3188
%v3200 = vadd.s32 %v3197, %v7
%v3204 = vadd.s32 1, %v3200
%v3208 = vadd.s32 %v3204, %v3192
%v3210 = vshll.u32 %v3204, 17
%v3211 = vshrl.u32 %v3204, 15
%v3212 = vor.u32 %v3211, %v3210
%v3213 = vxor.u32 %v3212, %v3208
%v3216 = vadd.s32 %v3213, %v3208
%v3218 = vshll.u32 %v3213, 29
%v3219 = vshrl.u32 %v3213, 3
%v3220 = vor.u32 %v3219, %v3218
%v3221 = vxor.u32 %v3220, %v3216
%v3224 = vadd.s32 %v3221, %v3216
%v3226 = vshll.u32 %v3221, 16
%v3227 = vshrl.u32 %v3221, 16
%v3228 = vor.u32 %v3227, %v3226
%v3229 = vxor.u32 %v3228, %v3224
%v3232 = vadd.s32 %v3229, %v3224
%v3236 = vadd.s32 %v3232, %v7
%v3238 = vshll.u32 %v3229, 24
%v3239 = vshrl.u32 %v3229, 8
%v3240 = vor.u32 %v3239, %v3238
%v3241 = vxor.u32 %v3240, %v3232
%v3244 = vadd.s32 %v3241, %v9
%v3248 = vadd.s32 2, %v3244
%v3252 = vadd.s32 %v3248, %v3236
%v3254 = vshll.u32 %v3248, 13
%v3255 = vshrl.u32 %v3248, 19
%v3256 = vor.u32 %v3255, %v3254
%v3257 = vxor.u32 %v3256, %v3252
%v3260 = vadd.s32 %v3257, %v3252
%v3262 = vshll.u32 %v3257, 15
%v3263 = vshrl.u32 %v3257, 17
%v3264 = vor.u32 %v3263, %v3262
%v3265 = vxor.u32 %v3264, %v3260
%v3268 = vadd.s32 %v3265, %v3260
%v3270 = vshll.u32 %v3265, 26
%v3271 = vshrl.u32 %v3265, 6
%v3272 = vor.u32 %v3271, %v3270
%v3273 = vxor.u32 %v3272, %v3268
%v3276 = vadd.s32 %v3273, %v3268
%v3280 = vadd.s32 %v3276, %v9
%v3282 = vshll.u32 %v3273, 6
%v3283 = vshrl.u32 %v3273, 26
%v3284 = vor.u32 %v3283, %v3282
%v3285 = vxor.u32 %v3284, %v3276
%v3288 = vadd.s32 %v3285, %v8
%v3292 = vadd.s32 3, %v3288
%v3296 = vadd.s32 %v3292, %v3280
%v3298 = vshll.u32 %v3292, 17
%v3299 = vshrl.u32 %v3292, 15
%v3300 = vor.u32 %v3299, %v3298
%v3301 = vxor.u32 %v3300, %v3296
%v3304 = vadd.s32 %v3301, %v3296
%v3306 = vshll.u32 %v3301, 29
%v3307 = vshrl.u32 %v3301, 3
%v3308 = vor.u32 %v3307, %v3306
%v3309 = vxor.u32 %v3308, %v3304
%v3312 = vadd.s32 %v3309, %v3304
%v3314 = vshll.u32 %v3309, 16
%v3315 = vshrl.u32 %v3309, 16
%v3316 = vor.u32 %v3315, %v3314
%v3317 = vxor.u32 %v3316, %v3312
%v3320 = vadd.s32 %v3317, %v3312
%v3324 = vadd.s32 %v3320, %v8
%v3326 = vshll.u32 %v3317, 24
%v3327 = vshrl.u32 %v3317, 8
%v3328 = vor.u32 %v3327, %v3326
%v3329 = vxor.u32 %v3328, %v3320
%v3332 = vadd.s32 %v3329, %v7
%v3336 = vadd.s32 4, %v3332
%v3340 = vadd.s32 %v3336, %v3324
%v3342 = vshll.u32 %v3336, 13
%v3343 = vshrl.u32 %v3336, 19
%v3344 = vor.u32 %v3343, %v3342
%v3345 = vxor.u32 %v3344, %v3340
%v3348 = vadd.s32 %v3345, %v3340
%v3350 = vshll.u32 %v3345, 15
%v3351 = vshrl.u32 %v3345, 17
%v3352 = vor.u32 %v3351, %v3350
%v3353 = vxor.u32 %v3352, %v3348
%v3356 = vadd.s32 %v3353, %v3348
%v3358 = vshll.u32 %v3353, 26
%v3359 = vshrl.u32 %v3353, 6
%v3360 = vor.u32 %v3359, %v3358
%v3361 = vxor.u32 %v3360, %v3356
%v3364 = vadd.s32 %v3361, %v3356
%v3368 = vadd.s32 %v3364, %v7
%v3370 = vshll.u32 %v3361, 6
%v3371 = vshrl.u32 %v3361, 26
%v3372 = vor.u32 %v3371, %v3370
%v3373 = vxor.u32 %v3372, %v3364
%v3376 = vadd.s32 %v3373, %v9
%v3380 = vadd.s32 5, %v3376
%v3382 = vxor.u32 %v3380, %v3368
%v3383 = vshrl.u32 %v3382, 9
%v3384 = vor.u32 1065353216, %v3383
%v3388 = vadd.f32 -1.0, %v3384
%v3392 = vmul.f32 2.0, %v3388
%v3396 = vadd.f32 -0.99999994, %v3392
%v3400 = vmax.f32 %v3396, -0.99999994
%v3402 = vand.u32 2147483647, %v3400
%vm3405 = vcmp.eq.f32.partialorder %v3402, 1.0
%v3410 = vmul.f32 inf, %v3400
%v3412 = vxor.u32 2147483648, %v3400
%v3415 = vmul.f32 %v3412, %v3400
%v3417 = vadd.f32 1.0, %v3415
%v3418 = vlog2.pop %v3417
%v3419 = vmul.f32 0.6931472, %v3418
%v3420 = vmul.f32 -0.5, %v3415
%v3421 = vadd.f32 1.0, %v3420
%v3422 = vmul.f32 %v3421, %v3415
%v3423 = vand.u32 2147483647, %v3415
%vm3424 = vcmp.lt.f32.partialorder %v3423, 0.0004427343
%v3425 = vsel /*vm=*/%vm3424, /*on_true_vy=*/%v3422, /*on_false_vx=*/%v3419
%v3426 = vxor.u32 2147483648, %v3425
%vm3429 = vcmp.lt.f32.partialorder %v3426, 5.0
%v3434 = vsel /*vm=*/%vm3429, /*on_true_vy=*/1.5014094, /*on_false_vx=*/%v3575
%v3438 = vsel /*vm=*/%vm3429, /*on_true_vy=*/0.24664073, /*on_false_vx=*/%v3576
%v3442 = vsel /*vm=*/%vm3429, /*on_true_vy=*/-0.0041776816, /*on_false_vx=*/%v3577
%v3446 = vsel /*vm=*/%vm3429, /*on_true_vy=*/-0.001253725, /*on_false_vx=*/%v3578
%v3450 = vsel /*vm=*/%vm3429, /*on_true_vy=*/0.00021858087, /*on_false_vx=*/%v3579
%v3454 = vsel /*vm=*/%vm3429, /*on_true_vy=*/-4.3915065e-06, /*on_false_vx=*/%v3580
%v3458 = vsel /*vm=*/%vm3429, /*on_true_vy=*/-3.5233877e-06, /*on_false_vx=*/%v3581
%v3462 = vsel /*vm=*/%vm3429, /*on_true_vy=*/3.4327394e-07, /*on_false_vx=*/%v3582
%v3466 = vsel /*vm=*/%vm3429, /*on_true_vy=*/2.8102264e-08, /*on_false_vx=*/%v3583
%v3470 = vadd.f32 -2.5, %v3426
%v3472 = vrsqrt.pop %v3426
%v3473 = vmul.f32 %v3472, %v3426
%vm3474 = vcmp.eq.f32.partialorder %v3426, inf
%v3475 = vsel /*vm=*/%vm3474, /*on_true_vy=*/%v3426, /*on_false_vx=*/%v3473
%vm3476 = vcmp.eq.f32.partialorder %v3426, 0.0
%v3477 = vand.u32 2147483648, %v3426
%v3478 = vsel /*vm=*/%vm3476, /*on_true_vy=*/%v3477, /*on_false_vx=*/%v3475
%v3481 = vadd.f32 -3.0, %v3478
%v3485 = vsel /*vm=*/%vm3429, /*on_true_vy=*/%v3470, /*on_false_vx=*/%v3481
%v3489 = vmul.f32 %v3485, %v3466
%v3493 = vadd.f32 %v3489, %v3462
%v3497 = vmul.f32 %v3493, %v3485
%v3501 = vadd.f32 %v3497, %v3458
%v3505 = vmul.f32 %v3501, %v3485
%v3509 = vadd.f32 %v3505, %v3454
%v3513 = vmul.f32 %v3509, %v3485
%v3517 = vadd.f32 %v3513, %v3450
%v3521 = vmul.f32 %v3517, %v3485
%v3525 = vadd.f32 %v3521, %v3446
%v3529 = vmul.f32 %v3525, %v3485
%v3533 = vadd.f32 %v3529, %v3442
%v3537 = vmul.f32 %v3533, %v3485
%v3541 = vadd.f32 %v3537, %v3438
%v3545 = vmul.f32 %v3541, %v3485
%v3549 = vadd.f32 %v3545, %v3434
%v3553 = vmul.f32 %v3549, %v3400
%v3557 = vsel /*vm=*/%vm3405, /*on_true_vy=*/%v3410, /*on_false_vx=*/%v3553
%v3561 = vmul.f32 1.4142135, %v3557
%3564 = vst [vmem:[#allocation0 + $0x38] sm:$0xff] /*vst_source=*/%v3561
%s3584 = smov [#allocation0] /* materialized constant */
%s3569 = sshll.u32 %s3584, 4
%s3570 = int_to_ptr.vmem [resolvable:$true] %s3569
%3572 = dma.vmem_to_hbm [thread:$0]  /*vmem=*/%s3570, /*size_in_granules=*/1024, /*hbm=*/%s6, /*dst_syncflagno=*/[#allocation2] /* 
base_bounds: (8, 1)
dynamic_base_bounds: (8, 1)
window_bounds: (8, 1)
iteration_bounds: (1, 1)
strides: (8, 1)
pad_low: (0, 0)
pad_high: (0, 0)
element_size_in_bytes: 4096 */

%3573 = dma.done [#allocation2], 1024 /* pipeline-emitter-dma-wait */

%3574 = vsyncpa [#allocation2], 1
